Authors,Title,Year,Source title,Cited by,DOI,Link,Abstract,Author Keywords,Index Keywords,Document Type,Source,EID
"Goferman S., Zelnik-Manor L., Tal A.","Context-aware saliency detection",2012,"IEEE Transactions on Pattern Analysis and Machine Intelligence",592,10.1109/TPAMI.2011.272,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865331032&doi=10.1109%2fTPAMI.2011.272&partnerID=40&md5=5838929f9239333c2d7e8e16b24753d0","We propose a new type of saliency-context-aware saliency-which aims at detecting the image regions that represent the scene. This definition differs from previous definitions whose goal is to either identify fixation points or detect the dominant object. In accordance with our saliency definition, we present a detection algorithm which is based on four principles observed in the psychological literature. The benefits of the proposed approach are evaluated in two applications where the context of the dominant objects is just as essential as the objects themselves. In image retargeting, we demonstrate that using our saliency prevents distortions in the important regions. In summarization, we show that our saliency helps to produce compact, appealing, and informative summaries. © 1979-2012 IEEE.","context aware.; Image saliency; visual saliency","Context-Aware; Detection algorithm; Fixation point; Image regions; Image retargeting; Image saliency; Saliency detection; Visual saliency; Artificial intelligence; Computer vision; algorithm; animal; article; bird; computer graphics; fish; human; image processing; methodology; vision; Algorithms; Animals; Birds; Computer Graphics; Fishes; Humans; Image Processing, Computer-Assisted; Visual Perception",Article,Scopus,2-s2.0-84865331032
"Akay B., Karaboga D.","A modified Artificial Bee Colony algorithm for real-parameter optimization",2012,"Information Sciences",456,10.1016/j.ins.2010.07.015,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857832525&doi=10.1016%2fj.ins.2010.07.015&partnerID=40&md5=33e2702b60aa2d71393f2f134cdc6f24","Swarm intelligence is a research field that models the collective intelligence in swarms of insects or animals. Many algorithms that simulates these models have been proposed in order to solve a wide range of problems. The Artificial Bee Colony algorithm is one of the most recent swarm intelligence based algorithms which simulates the foraging behaviour of honey bee colonies. In this work, modified versions of the Artificial Bee Colony algorithm are introduced and applied for efficiently solving real-parameter optimization problems. © 2010 Elsevier Inc. All rights reserved.","Artificial Bee Colony algorithm; Real-parameter optimization; Self-organization; Swarm intelligence","Artificial bee colony algorithms; Collective intelligences; Foraging behaviours; Honey bee; Real-parameter optimization; Research fields; Self-organization; Swarm Intelligence; Animals; Artificial intelligence; Optimization; Evolutionary algorithms",Article,Scopus,2-s2.0-84857832525
"Alexe B., Deselaers T., Ferrari V.","Measuring the objectness of image windows",2012,"IEEE Transactions on Pattern Analysis and Machine Intelligence",424,10.1109/TPAMI.2012.28,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866688216&doi=10.1109%2fTPAMI.2012.28&partnerID=40&md5=189f50fa552c5bf574d8d1dccf054ba3","We present a generic objectness measure, quantifying how likely it is for an image window to contain an object of any class. We explicitly train it to distinguish objects with a well-defined boundary in space, such as cows and telephones, from amorphous background elements, such as grass and road. The measure combines in a Bayesian framework several image cues measuring characteristics of objects, such as appearing different from their surroundings and having a closed boundary. These include an innovative cue to measure the closed boundary characteristic. In experiments on the challenging PASCAL VOC 07 dataset, we show this new cue to outperform a state-of-the-art saliency measure, and the combined objectness measure to perform better than any cue alone. We also compare to interest point operators, a HOG detector, and three recent works aiming at automatic object segmentation. Finally, we present two applications of objectness. In the first, we sample a small numberof windows according to their objectness probability and give an algorithm to employ them as location priors for modern class-specific object detectors. As we show experimentally, this greatly reduces the number of windows evaluated by the expensive class-specific model. In the second application, we use objectness as a complementary score in addition to the class-specific model, which leads to fewer false positives. As shown in several recent papers, objectness can act as a valuable focus of attention mechanism in many other applications operating on image windows, including weakly supervised learning of object categories, unsupervised pixelwise segmentation, and object tracking in video. Computing objectness is very efficient and takes only about 4 sec. per image. © 2012 IEEE.","object detection; object recognition; Objectness measure","Bayesian frameworks; Closed boundary; Data sets; False positive; Focus of Attention; Image window; Object categories; Object Detection; Object detectors; Object segmentation; Object Tracking; Objectness measure; Saliency measure; Weakly supervised learning; Object recognition; Image segmentation; algorithm; article; artificial intelligence; automated pattern recognition; Bayes theorem; computer assisted diagnosis; image enhancement; image subtraction; methodology; reproducibility; sensitivity and specificity; three dimensional imaging; Algorithms; Artificial Intelligence; Bayes Theorem; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-84866688216
"Saad M.A., Bovik A.C., Charrier C.","Blind image quality assessment: A natural scene statistics approach in the DCT domain",2012,"IEEE Transactions on Image Processing",415,10.1109/TIP.2012.2191563,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864127797&doi=10.1109%2fTIP.2012.2191563&partnerID=40&md5=70dd6cebda0efcb3fc7cfb68eaeba755","We develop an efficient general-purpose blind/no-reference image quality assessment (IQA) algorithm using a natural scene statistics (NSS) model of discrete cosine transform (DCT) coefficients. The algorithm is computationally appealing, given the availability of platforms optimized for DCT computation. The approach relies on a simple Bayesian inference model to predict image quality scores given certain extracted features. The features are based on an NSS model of the image DCT coefficients. The estimated parameters of the model are utilized to form features that are indicative of perceptual quality. These features are used in a simple Bayesian inference approach to predict quality scores. The resulting algorithm, which we name BLIINDS-II, requires minimal training and adopts a simple probabilistic model for score prediction. Given the extracted features from a test image, the quality score that maximizes the probability of the empirically determined inference model is chosen as the predicted quality score of that image. When tested on the LIVE IQA database, BLIINDS-II is shown to correlate highly with human judgments of quality, at a level that is competitive with the popular SSIM index. © 1992-2012 IEEE.","Discrete cosine transform (DCT); generalized Gaussian density; natural scene statistics; no-reference image quality assessment","Bayesian inference; Bayesian inference model; DCT coefficients; DCT domain; Discrete cosine transform coefficients; Estimated parameter; Form features; Generalized Gaussian density; Human judgments; Image quality assessment; Inference models; Minimal training; Natural scene statistics; No-reference image quality assessments; Perceptual quality; Probabilistic models; Test images; Algorithms; Bayesian networks; Discrete cosine transforms; Forecasting; Inference engines; Quality control; Image quality; algorithm; artificial intelligence; automated pattern recognition; computer assisted diagnosis; image enhancement; procedures; reproducibility; sensitivity and specificity; single blind procedure; statistical analysis; three dimensional imaging; article; automated pattern recognition; computer assisted diagnosis; methodology; three dimensional imaging; Algorithms; Artificial Intelligence; Data Interpretation, Statistical; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Single-Blind Method; Algorithms; Artificial Intelligence; Data Interpretation, Statistical; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Single-Blind Method",Article,Scopus,2-s2.0-84864127797
"Peng Y., Ganesh A., Wright J., Xu W., Ma Y.","RASL: Robust alignment by sparse and low-rank decomposition for linearly correlated images",2012,"IEEE Transactions on Pattern Analysis and Machine Intelligence",292,10.1109/TPAMI.2011.282,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866665730&doi=10.1109%2fTPAMI.2011.282&partnerID=40&md5=2aa11b9f54fe2563ab09d78449275880","This paper studies the problem of simultaneously aligning a batch of linearly correlated images despite gross corruption (such as occlusion). Our method seeks an optimal set of image domain transformations such that the matrix of transformed images can be decomposed as the sum of a sparse matrix of errors and a low-rank matrix of recovered aligned images. We reduce this extremely challenging optimization problem to a sequence of convex programs that minimize the sum of\ell-1-norm and nuclear norm of the two component matrices, which can be efficiently solved by scalable convex optimization techniques. We verify the efficacy of the proposed robust alignment algorithm with extensive experiments on both controlled and uncontrolled real data, demonstrating higher accuracy and efficiency than existing methods over a wide range of realistic misalignments and corruptions. © 2012 IEEE.","Batch image alignment; low-rank matrix; occlusion and corruption; robust principal component analysis; sparse errors","Aligned images; Alignment algorithms; Convex optimization techniques; Convex programs; Image alignment; Image domain; Low-rank decomposition; Low-rank matrices; occlusion and corruption; Optimal sets; Optimization problems; Robust principal component analysis; Sparse matrices; Two-component; Alignment; Crime; Errors; Optimization; Principal component analysis; Linear transformations; algorithm; article; artifact; artificial intelligence; automated pattern recognition; image enhancement; image subtraction; methodology; reproducibility; sensitivity and specificity; statistical model; statistics; three dimensional imaging; Algorithms; Artifacts; Artificial Intelligence; Image Enhancement; Imaging, Three-Dimensional; Linear Models; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Statistics as Topic; Subtraction Technique",Article,Scopus,2-s2.0-84866665730
"Li X., Yao X.","Cooperatively coevolving particle swarms for large scale optimization",2012,"IEEE Transactions on Evolutionary Computation",287,10.1109/TEVC.2011.2112662,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859719176&doi=10.1109%2fTEVC.2011.2112662&partnerID=40&md5=e43df835eec9fdac2877bcaded0bf0b7","This paper presents a new cooperative coevolving particle swarm optimization (CCPSO) algorithm in an attempt to address the issue of scaling up particle swarm optimization (PSO) algorithms in solving large-scale optimization problems (up to 2000 real-valued variables). The proposed CCPSO2 builds on the success of an early CCPSO that employs an effective variable grouping technique random grouping. CCPSO2 adopts a new PSO position update rule that relies on Cauchy and Gaussian distributions to sample new points in the search space, and a scheme to dynamically determine the coevolving subcomponent sizes of the variables. On high-dimensional problems (ranging from 100 to 2000 variables), the performance of CCPSO2 compared favorably against a state-of-the-art evolutionary algorithm sep-CMA-ES, two existing PSO algorithms, and a cooperative coevolving differential evolution algorithm. In particular, CCPSO2 performed significantly better than sep-CMA-ES and two existing PSO algorithms on more complex multimodal problems (which more closely resemble real-world problems), though not as well as the existing algorithms on unimodal functions. Our experimental results and analysis suggest that CCPSO2 is a highly competitive optimization algorithm for solving large-scale and complex multimodal optimization problems. © 2011 IEEE.","Cooperative coevolution; evolutionary algorithms; large-scale optimization; particle swarm optimization; swarm intelligence","Cooperative co-evolution; Differential evolution algorithms; Effective variables; Grouping technique; High-dimensional problems; Large-scale optimization; Multimodal optimization problems; Multimodal problems; Optimization algorithms; Particle swarm; Particle swarm optimization algorithm; PSO algorithms; Random grouping; Real-valued variables; Real-world problem; Scaling-up; Search spaces; Swarm Intelligence; Unimodal functions; Algorithms; Artificial intelligence; Evolutionary algorithms; Particle swarm optimization (PSO)",Article,Scopus,2-s2.0-84859719176
"Hinton G.E.","A practical guide to training restricted boltzmann machines",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",249,10.1007/978-3-642-35289-8-32,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872506495&doi=10.1007%2f978-3-642-35289-8-32&partnerID=40&md5=bb680a2a7b62095c7f83cb7c9aa75b4e","Restricted Boltzmann machines (RBMs) have been used as generative models of many different types of data. RBMs are usually trained using the contrastive divergence learning procedure. This requires a certain amount of practical experience to decide how to set the values of numerical meta-parameters. Over the last few years, the machine learning group at the University of Toronto has acquired considerable expertise at training RBMs and this guide is an attempt to share this expertise with other machine learning researchers. © Springer-Verlag Berlin Heidelberg 2012.",,"Contrastive divergence; Generative model; Learning procedures; Practical experience; Practical guide; Restricted boltzmann machine; University of Toronto; Artificial intelligence; Learning systems",Article,Scopus,2-s2.0-84872506495
"Guha T., Ward R.K.","Learning sparse representations for human action recognition",2012,"IEEE Transactions on Pattern Analysis and Machine Intelligence",186,10.1109/TPAMI.2011.253,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862649818&doi=10.1109%2fTPAMI.2011.253&partnerID=40&md5=55f1de81a73f1797298910cf3d2058b7","This paper explores the effectiveness of sparse representations obtained by learning a set of overcomplete basis (dictionary) in the context of action recognition in videos. Although this work concentrates on recognizing human movementsphysical actions as well as facial expressionsthe proposed approach is fairly general and can be used to address other classification problems. In order to model human actions, three overcomplete dictionary learning frameworks are investigated. An overcomplete dictionary is constructed using a set of spatio-temporal descriptors (extracted from the video sequences) in such a way that each descriptor is represented by some linear combination of a small number of dictionary elements. This leads to a more compact and richer representation of the video sequences compared to the existing methods that involve clustering and vector quantization. For each framework, a novel classification algorithm is proposed. Additionally, this work also presents the idea of a new local spatio-temporal feature that is distinctive, scale invariant, and fast to compute. The proposed approach repeatedly achieves state-of-the-art results on several public data sets containing various physical actions and facial expressions. © 2012 IEEE.","Action recognition; dictionary learning; expression recognition; orthogonal matching pursuit; overcomplete; sparse representation; spatio-temporal descriptors","Action recognition; Descriptors; Dictionary learning; Expression recognition; Orthogonal matching pursuit; Over-complete; Sparse representation; Gesture recognition; Image recognition; Vector quantization; Video recording; Motion estimation; algorithm; article; artificial intelligence; automated pattern recognition; dancing; facial expression; factual database; human; image processing; methodology; movement (physiology); nomenclature; sport; theoretical model; videorecording; Algorithms; Artificial Intelligence; Dancing; Databases, Factual; Facial Expression; Humans; Image Processing, Computer-Assisted; Models, Theoretical; Movement; Pattern Recognition, Automated; Sports; Terminology as Topic; Video Recording",Article,Scopus,2-s2.0-84862649818
"Stallkamp J., Schlipsing M., Salmen J., Igel C.","Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition",2012,"Neural Networks",179,10.1016/j.neunet.2012.02.016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861783004&doi=10.1016%2fj.neunet.2012.02.016&partnerID=40&md5=98f868ae3ed59f2b9150da01e84c5ef5","Traffic signs are characterized by a wide variability in their visual appearance in real-world environments. For example, changes of illumination, varying weather conditions and partial occlusions impact the perception of road signs. In practice, a large number of different sign classes needs to be recognized with very high accuracy. Traffic signs have been designed to be easily readable for humans, who perform very well at this task. For computer systems, however, classifying traffic signs still seems to pose a challenging pattern recognition problem. Both image processing and machine learning algorithms are continuously refined to improve on this task. But little systematic comparison of such systems exist. What is the status quo? Do today's algorithms reach human performance? For assessing the performance of state-of-the-art machine learning algorithms, we present a publicly available traffic sign dataset with more than 50,000 images of German road signs in 43 classes. The data was considered in the second stage of the German Traffic Sign Recognition Benchmark held at IJCNN 2011. The results of this competition are reported and the best-performing algorithms are briefly described. Convolutional neural networks (CNNs) showed particularly high classification accuracies in the competition. We measured the performance of human subjects on the same data-and the CNNs outperformed the human test persons. © 2012 Elsevier Ltd.","Benchmarking; Convolutional neural networks; Machine learning; Traffic sign recognition","Classification accuracy; Convolutional neural network; Data sets; Human performance; Human subjects; Partial occlusions; Pattern recognition problems; Real world environments; Road signs; Status quo; Traffic sign recognition; Visual appearance; Weather conditions; Benchmarking; Image processing; Learning systems; Neural networks; Pattern recognition; Roads and streets; Traffic signs; Visualization; Learning algorithms; article; artificial neural network; classification algorithm; computer interface; discriminant analysis; human; human computer interaction; image analysis; image display; image processing; learning algorithm; online system; priority journal; quality control; traffic sign recognition; Algorithms; Artifacts; Artificial Intelligence; Automobile Driving; Benchmarking; Computer Graphics; Computers; Data Collection; Databases, Factual; Germany; Humans; Image Processing, Computer-Assisted; Pattern Recognition, Automated; Psychomotor Performance; Trees",Article,Scopus,2-s2.0-84861783004
"Zhang K., Gao X., Tao D., Li X.","Single image super-resolution with non-local means and steering kernel regression",2012,"IEEE Transactions on Image Processing",175,10.1109/TIP.2012.2208977,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867853644&doi=10.1109%2fTIP.2012.2208977&partnerID=40&md5=cb9ba38da37f5574bde0b970599a1ef6","Image super-resolution (SR) reconstruction is essentially an ill-posed problem, so it is important to design an effective prior. For this purpose, we propose a novel image SR method by learning both non-local and local regularization priors from a given low-resolution image. The non-local prior takes advantage of the redundancy of similar patches in natural images, while the local prior assumes that a target pixel can be estimated by a weighted average of its neighbors. Based on the above considerations, we utilize the non-local means filter to learn a non-local prior and the steering kernel regression to learn a local prior. By assembling the two complementary regularization terms, we propose a maximum a posteriori probability framework for SR recovery. Thorough experimental results suggest that the proposed SR method can reconstruct higher quality results both quantitatively and perceptually. © 2012 IEEE.","Image super-resolution; non-local means; regularization prior; self-similarity; steering kernel regression","Image super-resolution; Kernel regression; Nonlocal; regularization prior; Self-similarities; Probability distributions; Regression analysis; Optical resolving power; algorithm; animal; article; artificial intelligence; automated pattern recognition; factual database; human; image processing; methodology; regression analysis; reproducibility; Algorithms; Animals; Artificial Intelligence; Databases, Factual; Humans; Image Processing, Computer-Assisted; Pattern Recognition, Automated; Regression Analysis; Reproducibility of Results",Article,Scopus,2-s2.0-84867853644
"Zhang D., Shen D.","Multi-modal multi-task learning for joint prediction of multiple regression and classification variables in Alzheimer's disease",2012,"NeuroImage",173,10.1016/j.neuroimage.2011.09.069,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-83055184373&doi=10.1016%2fj.neuroimage.2011.09.069&partnerID=40&md5=e9fca7ce61d6ff0402a5f8310a735eac","Many machine learning and pattern classification methods have been applied to the diagnosis of Alzheimer's disease (AD) and its prodromal stage, i.e., mild cognitive impairment (MCI). Recently, rather than predicting categorical variables as in classification, several pattern regression methods have also been used to estimate continuous clinical variables from brain images. However, most existing regression methods focus on estimating multiple clinical variables separately and thus cannot utilize the intrinsic useful correlation information among different clinical variables. On the other hand, in those regression methods, only a single modality of data (usually only the structural MRI) is often used, without considering the complementary information that can be provided by different modalities. In this paper, we propose a general methodology, namely multi-modal multi-task (M3T) learning, to jointly predict multiple variables from multi-modal data. Here, the variables include not only the clinical variables used for regression but also the categorical variable used for classification, with different tasks corresponding to prediction of different variables. Specifically, our method contains two key components, i.e., (1) a multi-task feature selection which selects the common subset of relevant features for multiple variables from each modality, and (2) a multi-modal support vector machine which fuses the above-selected features from all modalities to predict multiple (regression and classification) variables. To validate our method, we perform two sets of experiments on ADNI baseline MRI, FDG-PET, and cerebrospinal fluid (CSF) data from 45 AD patients, 91 MCI patients, and 50 healthy controls (HC). In the first set of experiments, we estimate two clinical variables such as Mini Mental State Examination (MMSE) and Alzheimer's Disease Assessment Scale-Cognitive Subscale (ADAS-Cog), as well as one categorical variable (with value of 'AD', 'MCI' or 'HC'), from the baseline MRI, FDG-PET, and CSF data. In the second set of experiments, we predict the 2-year changes of MMSE and ADAS-Cog scores and also the conversion of MCI to AD from the baseline MRI, FDG-PET, and CSF data. The results on both sets of experiments demonstrate that our proposed M3T learning scheme can achieve better performance on both regression and classification tasks than the conventional learning methods. © 2011 Elsevier Inc.","ADAS-Cog; Alzheimer's disease (AD); MCI conversion; MMSE; Multi-modal multi-task (M3T) learning; Multi-modality; Multi-task feature selection","adult; aged; Alzheimer disease; Alzheimer Disease Assessment Scale Cognitive Subscale; article; categorical variable; cerebrospinal fluid examination; classification; classification variable; clinical article; clinical variable; controlled study; data base; female; human; machine learning; male; mathematical variable; methodology; mini mental state examination; multi modal multi task learning; multiple regression; named inventories, questionnaires and rating scales; neuroimaging; nuclear magnetic resonance imaging; positron emission tomography; prediction; priority journal; regression analysis; scoring system; support vector machine; Aged; Aged, 80 and over; Algorithms; Alzheimer Disease; Artificial Intelligence; Female; Humans; Image Interpretation, Computer-Assisted; Male; Middle Aged; Neuroimaging; Pattern Recognition, Automated; Regression Analysis; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-83055184373
"Li G., Niu P., Xiao X.","Development and investigation of efficient artificial bee colony algorithm for numerical function optimization",2012,"Applied Soft Computing Journal",163,10.1016/j.asoc.2011.08.040,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-81155148313&doi=10.1016%2fj.asoc.2011.08.040&partnerID=40&md5=38a0c08da9ec732287389f6e489a5929","Artificial bee colony algorithm (ABC), which is inspired by the foraging behavior of honey bee swarm, is a biological-inspired optimization. It shows more effective than genetic algorithm (GA), particle swarm optimization (PSO) and ant colony optimization (ACO). However, ABC is good at exploration but poor at exploitation, and its convergence speed is also an issue in some cases. For these insufficiencies, we propose an improved ABC algorithm called I-ABC. In I-ABC, the best-so-far solution, inertia weight and acceleration coefficients are introduced to modify the search process. Inertia weight and acceleration coefficients are defined as functions of the fitness. In addition, to further balance search processes, the modification forms of the employed bees and the onlooker ones are different in the second acceleration coefficient. Experiments show that, for most functions, the I-ABC has a faster convergence speed and better performances than each of ABC and the gbest-guided ABC (GABC). But I-ABC could not still substantially achieve the best solution for all optimization problems. In a few cases, it could not find better results than ABC or GABC. In order to inherit the bright sides of ABC, GABC and I-ABC, a high-efficiency hybrid ABC algorithm, which is called PS-ABC, is proposed. PS-ABC owns the abilities of prediction and selection. Results show that PS-ABC has a faster convergence speed like I-ABC and better search ability than other relevant methods for almost all functions. © 2011 Elsevier B.V. All rights reserved.","Artificial bee colony; Biological-inspired optimization; Gbest-guided ABC; Optimization","Acceleration coefficients; Ant-colony optimization; Artificial bee colonies; Convergence speed; Faster convergence; Foraging behaviors; Gbest-guided ABC; Honey bee; Inertia weight; Numerical function optimization; Optimization problems; Search process; Artificial intelligence; Convergence of numerical methods; Genetic algorithms; Optimization; Particle swarm optimization (PSO)",Article,Scopus,2-s2.0-81155148313
"Garner A.R., Rowland D.C., Hwang S.Y., Baumgaertel K., Roth B.L., Kentros C., Mayford M.","Generation of a synthetic memory trace",2012,"Science",156,10.1126/science.1214985,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858769326&doi=10.1126%2fscience.1214985&partnerID=40&md5=ac849ca4d373885c2293b150c9a76777","We investigated the effect of activating a competing, artificially generated, neural representation on encoding of contextual fear memory in mice. We used a c-fos-based transgenic approach to introduce the hM3D q DREADD receptor (designer receptor exclusively activated by designer drug) into neurons naturally activated by sensory experience. Neural activity could then be specifically and inducibly increased in the hM 3Dq-expressing neurons by an exogenous ligand. When an ensemble of neurons for one context (ctxA) was artificially activated during conditioning in a distinct second context (ctxB), mice formed a hybrid memory representation. Reactivation of the artificially stimulated network within the conditioning context was required for retrieval of the memory, and the memory was specific for the spatial pattern of neurons artificially activated during learning. Similar stimulation impaired recall when not part of the initial conditioning.",,"protein c fos; brain; drug; hybrid; learning; ligand; memory; neurology; rodent; spatial analysis; animal experiment; article; artificial intelligence; cell activity; fear; memory consolidation; mouse; nerve cell; nerve cell network; nonhuman; priority journal; sensory feedback; sensory stimulation; spatial memory; working memory; Mus",Article,Scopus,2-s2.0-84858769326
"Heinrich M.P., Jenkinson M., Bhushan M., Matin T., Gleeson F.V., Brady S.M., Schnabel J.A.","MIND: Modality independent neighbourhood descriptor for multi-modal deformable registration",2012,"Medical Image Analysis",153,10.1016/j.media.2012.05.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866457193&doi=10.1016%2fj.media.2012.05.008&partnerID=40&md5=fe983e31132db352d69ed1f3305f36c6","Deformable registration of images obtained from different modalities remains a challenging task in medical image analysis. This paper addresses this important problem and proposes a modality independent neighbourhood descriptor (MIND) for both linear and deformable multi-modal registration. Based on the similarity of small image patches within one image, it aims to extract the distinctive structure in a local neighbourhood, which is preserved across modalities. The descriptor is based on the concept of image self-similarity, which has been introduced for non-local means filtering for image denoising. It is able to distinguish between different types of features such as corners, edges and homogeneously textured regions. MIND is robust to the most considerable differences between modalities: non-functional intensity relations, image noise and non-uniform bias fields. The multi-dimensional descriptor can be efficiently computed in a dense fashion across the whole image and provides point-wise local similarity across modalities based on the absolute or squared difference between descriptors, making it applicable for a wide range of transformation models and optimisation algorithms. We use the sum of squared differences of the MIND representations of the images as a similarity metric within a symmetric non-parametric Gauss-Newton registration framework. In principle, MIND would be applicable to the registration of arbitrary modalities. In this work, we apply and validate it for the registration of clinical 3D thoracic CT scans between inhale and exhale as well as the alignment of 3D CT and MRI scans. Experimental results show the advantages of MIND over state-of-the-art techniques such as conditional mutual information and entropy images, with respect to clinically annotated landmark locations. © 2012 Elsevier B.V.","Multi-modal similarity metric; Non-local means; Non-rigid registration; Pulmonary images; Self-similarity","Nonlocal; Nonrigid registration; Pulmonary images; Self-similarities; Similarity metrics; Algorithms; Computerized tomography; Deformation; Three dimensional; algorithm; article; computer assisted tomography; conceptual framework; controlled study; digital filtering; entropy; exhalation; human; image analysis; information processing; inhalation; instrument validation; modality independent neighbourhood descriptor; nonparametric test; nuclear magnetic resonance imaging; priority journal; process optimization; thorax; three dimensional imaging; Algorithms; Artificial Intelligence; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique; Tomography, X-Ray Computed",Article,Scopus,2-s2.0-84866457193
"Li C., Yang S., Nguyen T.T.","A self-learning particle swarm optimizer for global optimization problems",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",148,10.1109/TSMCB.2011.2171946,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861191799&doi=10.1109%2fTSMCB.2011.2171946&partnerID=40&md5=4c0e0e5208a621684bb947c02c1fc618","Particle swarm optimization (PSO) has been shown as an effective tool for solving global optimization problems. So far, most PSO algorithms use a single learning pattern for all particles, which means that all particles in a swarm use the same strategy. This monotonic learning pattern may cause the lack of intelligence for a particular particle, which makes it unable to deal with different complex situations. This paper presents a novel algorithm, called self-learning particle swarm optimizer (SLPSO), for global optimization problems. In SLPSO, each particle has a set of four strategies to cope with different situations in the search space. The cooperation of the four strategies is implemented by an adaptive learning framework at the individual level, which can enable a particle to choose the optimal strategy according to its own local fitness landscape. The experimental study on a set of 45 test functions and two real-world problems show that SLPSO has a superior performance in comparison with several other peer algorithms. © 2012 IEEE.","Global optimization problem; operator adaptation; particle swarm optimization (PSO); self-learning particle swarm optimizer (SLPSO); topology adaptation","Adaptive learning; Effective tool; Experimental studies; Global optimization problems; Learning patterns; Local fitness; Monotonic learning; Novel algorithm; operator adaptation; Optimal strategies; Particle swarm optimizers; PSO algorithms; Real-world problem; Search spaces; Self-learning; Test functions; Topology adaptation; Algorithms; Global optimization; Problem solving; Particle swarm optimization (PSO); algorithm; article; artificial intelligence; automated pattern recognition; computer simulation; decision support system; methodology; theoretical model; Algorithms; Artificial Intelligence; Computer Simulation; Decision Support Techniques; Models, Theoretical; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84861191799
"Kumar A., Zhou Y.","Human identification using finger images",2012,"IEEE Transactions on Image Processing",147,10.1109/TIP.2011.2171697,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859062048&doi=10.1109%2fTIP.2011.2171697&partnerID=40&md5=4b5a6890bf7ec9a961e3d4be9365cb0f","This paper presents a new approach to improve the performance of finger-vein identification systems presented in the literature. The proposed system simultaneously acquires the finger-vein and low-resolution fingerprint images and combines these two evidences using a novel score-level combination strategy. We examine the previously proposed finger-vein identification approaches and develop a new approach that illustrates it superiority over prior published efforts. The utility of low-resolution fingerprint images acquired from a webcam is examined to ascertain the matching performance from such images. We develop and investigate two new score-level combinations, i.e., holistic and nonlinear fusion, and comparatively evaluate them with more popular score-level fusion approaches to ascertain their effectiveness in the proposed system. The rigorous experimental results presented on the database of 6264 images from 156 subjects illustrate significant improvement in the performance, i.e., both from the authentication and recognition experiments. © 2011 IEEE.","finger-vein recognition; Fingerprint recognition; fusion; hand biometrics; multispectral finger identification; personal identification; vascular biometrics","Finger-vein recognition; Fingerprint Recognition; Multi-spectral; Personal identification; Vascular biometrics; Fusion reactions; Image processing; Mathematical models; Biometrics; article; artificial intelligence; automated pattern recognition; biometry; computer assisted diagnosis; dermatoglyphics; finger; histology; human; image enhancement; image subtraction; methodology; reproducibility; sensitivity and specificity; vascularization; vein; Artificial Intelligence; Biometry; Dermatoglyphics; Fingers; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique; Veins",Article,Scopus,2-s2.0-84859062048
"Wee C.-Y., Yap P.-T., Zhang D., Denny K., Browndyke J.N., Potter G.G., Welsh-Bohmer K.A., Wang L., Shen D.","Identification of MCI individuals using structural and functional connectivity networks",2012,"NeuroImage",146,10.1016/j.neuroimage.2011.10.015,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855453290&doi=10.1016%2fj.neuroimage.2011.10.015&partnerID=40&md5=c3467d99f2689d05105ec100e45d0d30","Different imaging modalities provide essential complementary information that can be used to enhance our understanding of brain disorders. This study focuses on integrating multiple imaging modalities to identify individuals at risk for mild cognitive impairment (MCI). MCI, often an early stage of Alzheimer's disease (AD), is difficult to diagnose due to its very mild or insignificant symptoms of cognitive impairment. Recent emergence of brain network analysis has made characterization of neurological disorders at a whole-brain connectivity level possible, thus providing new avenues for brain diseases classification. Employing multiple-kernel Support Vector Machines (SVMs), we attempt to integrate information from diffusion tensor imaging (DTI) and resting-state functional magnetic resonance imaging (rs-fMRI) for improving classification performance. Our results indicate that the multimodality classification approach yields statistically significant improvement in accuracy over using each modality independently. The classification accuracy obtained by the proposed method is 96.3%, which is an increase of at least 7.4% from the single modality-based methods and the direct data fusion method. A cross-validation estimation of the generalization performance gives an area of 0.953 under the receiver operating characteristic (ROC) curve, indicating excellent diagnostic power. The multimodality classification approach hence allows more accurate early detection of brain abnormalities with greater sensitivity. © 2011 Elsevier Inc.","Alzheimer's disease (AD); Brain network analysis multiple-kernel Support Vector Machines (SVMs); Diffusion tensor imaging (DTI); Mild cognitive impairment (MCI); Multimodality representation; Resting-state functional magnetic resonance imaging (rs-fMRI)","adult; Alzheimer disease; article; clinical article; controlled study; diagnostic accuracy; diffusion tensor imaging; disease classification; female; functional magnetic resonance imaging; human; image analysis; image processing; male; mild cognitive impairment; priority journal; receiver operating characteristic; risk assessment; support vector machine; symptom; validation process; Aged; Algorithms; Artificial Intelligence; Brain; Cluster Analysis; Cohort Studies; Data Interpretation, Statistical; Diffusion Tensor Imaging; Educational Status; Female; Humans; Image Processing, Computer-Assisted; Linear Models; Magnetic Resonance Imaging; Male; Middle Aged; Mild Cognitive Impairment; Nerve Net; Neural Pathways; Neuropsychological Tests; Nonlinear Dynamics; Reproducibility of Results; ROC Curve; Support Vector Machines",Article,Scopus,2-s2.0-84855453290
"Valstar M.F., Pantic M.","Fully automatic recognition of the temporal phases of facial actions",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",144,10.1109/TSMCB.2011.2163710,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856294719&doi=10.1109%2fTSMCB.2011.2163710&partnerID=40&md5=b7e3f390839f2353954f6d81df1f3b61","Past work on automatic analysis of facial expressions has focused mostly on detecting prototypic expressions of basic emotions like happiness and anger. The method proposed here enables the detection of a much larger range of facial behavior by recognizing facial muscle actions [action units (AUs)] that compound expressions. AUs are agnostic, leaving the inference about conveyed intent to higher order decision making (e.g., emotion recognition). The proposed fully automatic method not only allows the recognition of 22 AUs but also explicitly models their temporal characteristics (i.e., sequences of temporal segments: neutral, onset, apex, and offset). To do so, it uses a facial point detector based on Gabor-feature-based boosted classifiers to automatically localize 20 facial fiducial points. These points are tracked through a sequence of images using a method called particle filtering with factorized likelihoods. To encode AUs and their temporal activation models based on the tracking data, it applies a combination of GentleBoost, support vector machines, and hidden Markov models. We attain an average AU recognition rate of 95.3% when tested on a benchmark set of deliberately displayed facial expressions and 72% when tested on spontaneous expressions. © 2011 IEEE.","Facial expression analysis; GentleBoost; particle filtering; spatiotemporal facial behavior analysis; support vector machine (SVM)","Facial expression analysis; GentleBoost; particle filtering; spatiotemporal facial behavior analysis; Support vector; Hidden Markov models; Signal filtering and prediction; Support vector machines; Face recognition; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; facial expression; human; image subtraction; methodology; photography; videorecording; Artificial Intelligence; Facial Expression; Humans; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Photography; Subtraction Technique; Video Recording",Article,Scopus,2-s2.0-84856294719
"Lewko A., Waters B.","New proof methods for attribute-based encryption: Achieving full security through selective techniques",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",141,10.1007/978-3-642-32009-5_12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865479255&doi=10.1007%2f978-3-642-32009-5_12&partnerID=40&md5=41c06717b4fe45b6d5fdf01c2887a79b","We develop a new methodology for utilizing the prior techniques to prove selective security for functional encryption systems as a direct ingredient in devising proofs of full security. This deepens the relationship between the selective and full security models and provides a path for transferring the best qualities of selectively secure systems to fully secure systems. In particular, we present a Ciphertext-Policy Attribute-Based Encryption scheme that is proven fully secure while matching the efficiency of the state of the art selectively secure systems. © 2012 International Association for Cryptologic Research.",,"Encryption schemes; Encryption system; Proof methods; Secure system; Security model; State of the art; Artificial intelligence; Cryptography",Conference Paper,Scopus,2-s2.0-84865479255
"Ge W., Collins R.T., Ruback R.B.","Vision-based analysis of small groups in pedestrian crowds",2012,"IEEE Transactions on Pattern Analysis and Machine Intelligence",139,10.1109/TPAMI.2011.176,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859193194&doi=10.1109%2fTPAMI.2011.176&partnerID=40&md5=88404aca2967d70db257dff9c220ab2b","Building upon state-of-the-art algorithms for pedestrian detection and multi-object tracking, and inspired by sociological models of human collective behavior, we automatically detect small groups of individuals who are traveling together. These groups are discovered by bottom-up hierarchical clustering using a generalized, symmetric Hausdorff distance defined with respect to pairwise proximity and velocity. We validate our results quantitatively and qualitatively on videos of real-world pedestrian scenes. Where human-coded ground truth is available, we find substantial statistical agreement between our results and the human-perceived small group structure of the crowd. Results from our automated crowd analysis also reveal interesting patterns governing the shape of pedestrian groups. These discoveries complement current research in crowd dynamics, and may provide insights to improve evacuation planning and real-time situation awareness during public disturbances. © 2012 IEEE.","crowd dynamics; Pedestrian detection and tracking; pedestrian groups","Collective behavior; Crowd analysis; Crowd dynamics; Evacuation planning; Ground truth; Hausdorff distance; Hier-archical clustering; Multi-object tracking; Pedestrian detection; Pedestrian detection and tracking; pedestrian groups; Situation awareness; Small groups; Sociological models; State-of-the-art algorithms; Vision based; Dynamics; Tracking (position); Behavioral research; algorithm; article; artificial intelligence; automated pattern recognition; cluster analysis; crowding; human; image processing; methodology; social behavior; videorecording; walking; Algorithms; Artificial Intelligence; Cluster Analysis; Crowding; Humans; Image Processing, Computer-Assisted; Pattern Recognition, Automated; Social Behavior; Video Recording; Walking",Article,Scopus,2-s2.0-84859193194
"Xu Q., Yu H.Y., Mou X.Q., Zhang L., Hsieh J., Wang G.","Low-dose X-ray CT reconstruction via dictionary learning",2012,"IEEE Transactions on Medical Imaging",134,10.1109/TMI.2012.2195669,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867561335&doi=10.1109%2fTMI.2012.2195669&partnerID=40&md5=988924d82e8f3c4b1fb8ffead15a0b96","Although diagnostic medical imaging provides enormous benefits in the early detection and accuracy diagnosis of various diseases, there are growing concerns on the potential side effect of radiation induced genetic, cancerous and other diseases. How to reduce radiation dose whilemaintaining the diagnostic performance is a major challenge in the computed tomography (CT) field. Inspired by the compressive sensing theory, the sparse constraint in terms of total variation (TV) minimization has already led to promising results for low-doseCT reconstruction.Compared to the discrete gradient transform used in the TV method, dictionary learning is proven to be an effective way for sparse representation. On the other hand, it is important to consider the statistical property of projection data in the low-dose CT case. Recently, we have developed a dictionary learning based approach for lowdose X-ray CT. In this paper, we present this method in detail and evaluate it in experiments. In our method, the sparse constraint in terms of a redundant dictionary is incorporated into an objective function in a statistical iterative reconstruction framework. The dictionary can be either predetermined before an image reconstruction task or adaptively defined during the reconstruction process. An alternating minimization scheme is developed to minimize the objective function. Our approach is evaluated with lowdose X-ray projections collected in animal and human CT studies, and the improvement associated with dictionary learning is quantified relative to filtered backprojection and TV-based reconstructions. The results show that the proposed approach might produce better images with lower noise and more detailed structural features in our selected cases. However, there is no proof that this is true for all kinds of structures. © 2012 IEEE.","Compressive sensing (CS); Computed tomography (CT); Dictionary learning; Low-dose CT; Sparse representation; Statistical iterative reconstruction","Compressive sensing; Computed Tomography; Dictionary learning; Low-dose CT; Sparse representation; Statistical iterative reconstruction; Constraint theory; Diagnosis; Image reconstruction; Iterative methods; Medical imaging; X rays; Computerized tomography; algorithm; animal; article; artificial intelligence; computer assisted tomography; computer simulation; human; image processing; image quality; instrumentation; lung; methodology; radiography; sheep; thorax radiography; Algorithms; Animals; Artificial Intelligence; Computer Simulation; Humans; Image Processing, Computer-Assisted; Lung; Phantoms, Imaging; Radiography, Thoracic; Sheep; Tomography, X-Ray Computed",Article,Scopus,2-s2.0-84867561335
"Yip K.Y., Cheng C., Bhardwaj N., Brown J.B., Leng J., Kundaje A., Rozowsky J., Birney E., Bickel P., Snyder M., Gerstein M.","Classification of human genomic regions based on experimentally determined binding sites of more than 100 transcription-related factors",2012,"Genome Biology",131,10.1186/gb-2012-13-9-r48,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865754452&doi=10.1186%2fgb-2012-13-9-r48&partnerID=40&md5=5ae402cf9d2806f229113af4231b3072","Background: Transcription factors function by binding different classes of regulatory elements. The Encyclopedia of DNA Elements (ENCODE) project has recently produced binding data for more than 100 transcription factors from about 500 ChIP-seq experiments in multiple cell types. While this large amount of data creates a valuable resource, it is nonetheless overwhelmingly complex and simultaneously incomplete since it covers only a small fraction of all human transcription factors.Results: As part of the consortium effort in providing a concise abstraction of the data for facilitating various types of downstream analyses, we constructed statistical models that capture the genomic features of three paired types of regions by machine-learning methods: firstly, regions with active or inactive binding; secondly, those with extremely high or low degrees of co-binding, termed HOT and LOT regions; and finally, regulatory modules proximal or distal to genes. From the distal regulatory modules, we developed computational pipelines to identify potential enhancers, many of which were validated experimentally. We further associated the predicted enhancers with potential target transcripts and the transcription factors involved. For HOT regions, we found a significant fraction of transcription factor binding without clear sequence motifs and showed that this observation could be related to strong DNA accessibility of these regions.Conclusions: Overall, the three pairs of regions exhibit intricate differences in chromosomal locations, chromatin features, factors that bind them, and cell-type specificity. Our machine learning approach enables us to identify features potentially general to all transcription factors, including those not included in the data. © 2012 Yip et al.; licensee BioMed Central Ltd.",,"cohesin; DNA directed RNA polymerase III; E1A associated p300 protein; protein RAD21; protein Smc3; RNA polymerase II; transcription factor; transcription factor CTCF; transcription factor E2F4; transcription factor GATA 1; transcription factor GATA 2; transcription factor IIIB; transcription initiation complex subunit BDP1; transcription initiation complex subunit BRF1; transcription initiation complex subunit BRF2; unclassified drug; transcription factor; article; base pairing; binding active region; binding inactive region; binding site; cell specificity; chromatin immunoprecipitation; chromatin structure; chromosomal localization; DNA binding; embryo; enhancer region; gene activity; gene control; gene distal regulatory module; gene expression; gene function; gene location; gene mapping; genetic association; genetic transcription; genome analysis; genomic region classification; high occupancy of transcription factor; histone modification; human; low occupancy of transcription factor; machine learning; mouse; nonhuman; promoter proximal regulatory module; protein DNA interaction; validation process; artificial intelligence; biological model; DNA sequence; human genome; metabolism; methodology; nucleotide motif; open reading frame; regulatory sequence; statistical model; Artificial Intelligence; Binding Sites; Genome, Human; Humans; Models, Genetic; Models, Statistical; Nucleotide Motifs; Open Reading Frames; Regulatory Sequences, Nucleic Acid; Sequence Analysis, DNA; Transcription Factors",Article,Scopus,2-s2.0-84865754452
"Chaoui H., Sicard P.","Adaptive fuzzy logic control of permanent magnet synchronous machines with nonlinear friction",2012,"IEEE Transactions on Industrial Electronics",131,10.1109/TIE.2011.2148678,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054811061&doi=10.1109%2fTIE.2011.2148678&partnerID=40&md5=a73c90a6e473b93c1427a620b5a3d029","In this paper, an adaptive fuzzy control scheme is introduced for permanent magnet synchronous machines (PMSMs). The adaptive control strategy consists of a Lyapunov stability-based fuzzy speed controller that capitalizes on the machine's inverse model to achieve accurate tracking with unknown nonlinear system dynamics. As such, robustness to modeling and parametric uncertainties is achieved. Moreover, no explicit currents loop regulation is needed, which simplifies the control structure and unlike other control strategies, no a priori offline training, weights initialization, parameters knowledge, voltage, or current transducer is required. The system's convergence and stability are proved by Lyapunov stability theory, which yields an improved performance. Simulation results for different situations highlight the performance of the proposed controller in transient, steady-state, and standstill conditions. Furthermore, the adaptive fuzzy systems inherent parallelism makes them a good candidate for implementation in real-time PMSM drive systems. © 2011 IEEE.","Artificial intelligence; fuzzy logic; Lyapunov stability; neurofuzzy control; speed control; synchronous machine","Accurate tracking; Adaptive control strategy; Adaptive fuzzy control; Adaptive fuzzy logic control; Adaptive fuzzy system; Control strategies; Control structure; Convergence and stability; Current transducer; Inherent parallelism; Inverse models; Lyapunov; Lyapunov stability; Lyapunov stability theory; Neurofuzzy control; Non-linear system dynamics; Nonlinear friction; Off-line training; Parametric uncertainties; Permanent magnet synchronous machines; PMSM drives; Simulation result; Speed controller; Synchronous machine; AC motors; Control system stability; Controllers; Fuzzy control; Fuzzy logic; Fuzzy systems; Magnetic devices; Permanent magnets; Real time systems; Speed control; Stability; Synchronous machinery; Adaptive control systems",Article,Scopus,2-s2.0-80054811061
"Chan A.B., Vasconcelos N.","Counting people with low-level features and bayesian regression",2012,"IEEE Transactions on Image Processing",130,10.1109/TIP.2011.2172800,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859031876&doi=10.1109%2fTIP.2011.2172800&partnerID=40&md5=e6e752feb991af182d2f76dfe1ca3f71","An approach to the problem of estimating the size of inhomogeneous crowds, which are composed of pedestrians that travel in different directions, without using explicit object segmentation or tracking is proposed. Instead, the crowd is segmented into components of homogeneous motion, using the mixture of dynamic-texture motion model. A set of holistic low-level features is extracted from each segmented region, and a function that maps features into estimates of the number of people per segment is learned with Bayesian regression. Two Bayesian regression models are examined. The first is a combination of Gaussian process regression with a compound kernel, which accounts for both the global and local trends of the count mapping but is limited by the real-valued outputs that do not match the discrete counts. We address this limitation with a second model, which is based on a Bayesian treatment of Poisson regression that introduces a prior distribution on the linear weights of the model. Since exact inference is analytically intractable, a closed-form approximation is derived that is computationally efficient and kernelizable, enabling the representation of nonlinear functions. An approximate marginal likelihood is also derived for kernel hyperparameter learning. The two regression-based crowd counting methods are evaluated on a large pedestrian data set, containing very distinct camera views, pedestrian traffic, and outliers, such as bikes or skateboarders. Experimental results show that regression-based counts are accurate regardless of the crowd size, outperforming the count estimates produced by state-of-the-art pedestrian detectors. Results on 2 h of video demonstrate the efficiency and robustness of the regression-based crowd size estimation over long periods of time. © 2011 IEEE.","Bayesian regression; crowd analysis; Gaussian processes; Poisson regression; surveillance","Bayesian regression; Camera view; Closed form approximations; Computationally efficient; Crowd analysis; Data sets; Exact inference; Explicit objects; Gaussian process regression; Gaussian Processes; Hyper-parameter; Low-level features; Marginal likelihood; Motion models; Nonlinear functions; Number of peoples; Pedestrian traffic; Poisson regression; Prior distribution; Segmented regions; Size estimation; Estimation; Gaussian distribution; Gaussian noise (electronic); Poisson distribution; Space surveillance; Regression analysis; article; artificial intelligence; automated pattern recognition; Bayes theorem; biometry; computer assisted diagnosis; human; image enhancement; methodology; photography; population density; population research; regression analysis; whole body imaging; Artificial Intelligence; Bayes Theorem; Biometry; Censuses; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Photography; Population Density; Regression Analysis; Whole Body Imaging",Article,Scopus,2-s2.0-84859031876
"Su Y., Huang J.","Cooperative output regulation with application to multi-agent consensus under switching network",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",128,10.1109/TSMCB.2011.2179981,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862817364&doi=10.1109%2fTSMCB.2011.2179981&partnerID=40&md5=5d4ce498e8fa8f59ad2717ad0fbfca28","In this paper, we consider the cooperative output regulation of linear multi-agent systems under switching network. The problem can be viewed as a generalization of the leader-following consensus problem of multi-agent systems. Due to the limited information exchanges of different subsystems, the problem cannot be solved by the decentralized approach and is not allowed to be solved by the centralized control. By devising a distributed observer network, we can solve the problem by both dynamic state feedback control and dynamic measurement output feedback control. As an application of our main result, we show that a special case of our results leads to the solution of the leader-following consensus problem of linear multi-agent systems. © 2012 IEEE.","Cooperative control; linear systems; multi-agent systems; output regulation; switching network topology","Centralized control; Co-operative control; Consensus problems; Decentralized approach; Dynamic measurement; Dynamic state; Leader following; Limited information; Multi agent system (MAS); Observer network; Output feedback controls; Output regulation; Electric network topology; Feedback control; Linear systems; Multi agent systems; Observability; State feedback; Switching networks; Problem solving; algorithm; article; artificial intelligence; automated pattern recognition; computer simulation; decision support system; methodology; signal processing; theoretical model; Algorithms; Artificial Intelligence; Computer Simulation; Decision Support Techniques; Models, Theoretical; Pattern Recognition, Automated; Signal Processing, Computer-Assisted",Article,Scopus,2-s2.0-84862817364
"Tako A.A., Robinson S.","The application of discrete event simulation and system dynamics in the logistics and supply chain context",2012,"Decision Support Systems",127,10.1016/j.dss.2011.11.015,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857653614&doi=10.1016%2fj.dss.2011.11.015&partnerID=40&md5=925a627dabd2dee0e534402ad72e64c4","Discrete event simulation (DES) and system dynamics (SD) are two modelling approaches widely used as decision support tools in logistics and supply chain management (LSCM). A widely held belief exists that SD is mostly used to model problems at a strategic level, whereas DES is used at an operational/tactical level. This paper explores the application of DES and SD as decision support systems (DSS) for LSCM by looking at the nature and level of issues modelled. Peer reviewed journal papers that use these modelling approaches to study supply chains, published between 1996 and 2006 are reviewed. A total of 127 journal articles are analysed to identify the frequency with which the two simulation approaches are used as modelling tools for DSS in LSCM. Our findings suggest that DES has been used more frequently to model supply chains, with the exception of the bullwhip effect, which is mostly modelled using SD. Based on the most commonly used modelling approach, issues in LSCM are categorised into four groups: the DES domain, the SD domain, the common domain and the less common domain. The study furthermore suggests that in terms of the level of decision making involved, strategic or operational/tactical, there is no difference in the use of either DES or SD. The results of this study inform the existing literature about the use of DES and SD as DSS tools in LSCM. © 2011 Elsevier B.V. All rights reserved.","Comparison of methods; Discrete-event simulation; Logistics and supply chain management; Simulation modelling; System dynamics","Bullwhip effects; Comparison of methods; Decision support tools; Discrete events; Four-group; Journal articles; Journal paper; Logistics and supply chain management; Model problems; Modelling tools; Simulation approach; Strategic level; System Dynamics; Artificial intelligence; Computer simulation; Decision support systems; Discrete event simulation; Supply chain management; System theory",Article,Scopus,2-s2.0-84857653614
"Kolter J.Z., Jaakkola T.","Approximate inference in additive factorial HMMs with application to energy disaggregation",2012,"Journal of Machine Learning Research",121,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883098797&partnerID=40&md5=756cdb5e741908e54a3f416d2996cb46","This paper considers additive factorial hidden Markov models, an extension to HMMs where the state factors into multiple independent chains, and the output is an additive function of all the hidden states. Although such models are very powerful, accurate inference is unfortunately difficult: exact inference is not computationally tractable, and existing approximate inference techniques are highly susceptible to local optima. In this paper we propose an alternative inference method for such models, which exploits their additive structure by 1) looking at the observed difference signal of the observation, 2) incorporating a ""robust"" mixture component that can account for unmodeled observations, and 3) constraining the posterior to allow at most one hidden state to change at a time. Combining these elements we develop a convex formulation of approximate inference that is computationally efficient, has no issues of local optima, and which performs much better than existing approaches in practice. The method is motivated by the problem of energy disaggregation, the task of taking a whole home electricity signal and decomposing it into its component appliances; applied to this task, our algorithm achieves state-of-the-art performance, and is able to separate many appliances almost perfectly using just the total aggregate signal. © Copyright 2012 by the authors.",,"Artificial intelligence; Markov processes; Signal processing; Accurate inference; Additive function; Approximate inference; Computationally efficient; Inference methods; Mixture components; State-of-the-art performance; Total aggregates; Hidden Markov models",Conference Paper,Scopus,2-s2.0-84883098797
"Toĝan V.","Design of planar steel frames using Teaching-Learning Based Optimization",2012,"Engineering Structures",118,10.1016/j.engstruct.2011.08.035,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155189617&doi=10.1016%2fj.engstruct.2011.08.035&partnerID=40&md5=d4fff3aaceecf03f48f437432b45c635","This paper presents a design procedure employing a Teaching-Learning Based Optimization (TLBO) technique for discrete optimization of planar steel frames. TLBO is a nature-inspired search method that has been developed recently. It simulates the social interaction between the teacher and the learners in a class, which is summarized as teaching-learning process. The design algorithm aims to obtain minimum weight frames subjected to strength and displacement requirements imposed by the American Institute for Steel Construction (AISC) Load and Resistance Factor Design (LRFD). Designs are obtained selecting appropriate W-shaped sections from a standard set of steel sections specified by the AISC. Several frame examples from the literature are examined to verify the suitability of the design procedure and to demonstrate the effectiveness and robustness of the TLBO creating of an optimal design for frame structures. The results of the TLBO are compared to those of the genetic algorithm (GA), the ant colony optimization (ACO), the harmony search (HS) and the improved ant colony optimization (IACO) and they shows that TLBO is a powerful search and applicable optimization method for the problem of engineering design applications. © 2011 Elsevier Ltd.","Optimization frames; Structural design; Teaching-Learning Based Optimization","American institute for steel constructions; Ant-colony optimization; Design procedure; Discrete optimization; Engineering design; Frame structure; Harmony search; Improved ant colony optimization; Load and resistance factor designs; Minimum weight; Optimal design; Optimization method; Planar steel frames; Search method; Social interactions; Steel sections; Teaching-learning; Teaching-learning process; Artificial intelligence; Genetic algorithms; Hydraulic structures; Learning systems; Steel construction; Structural design; Structural frames; Teaching; Structural optimization; algorithm; design; optimization; steel structure; structural response",Article,Scopus,2-s2.0-80155189617
"Cheng C., Yang H., King I., Lyu M.R.","Fused matrix factorization with geographical and social influence in location-based social networks",2012,"Proceedings of the National Conference on Artificial Intelligence",116,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868276527&partnerID=40&md5=a7aa92d481ee3e6f64e229dd70153b8e","Recently, location-based social networks (LBSNs), such as Gowalla, Foursquare, Facebook, and Brightkite, etc., have attracted millions of users to share their social friendship and their locations via check-ins. The available check-in information makes it possible to mine users' preference on locations and to provide favorite recommendations. Personalized Point-of-interest (POI) recommendation is a significant task in LBSNs since it can help targeted users explore their surroundings as well as help third-party developers to provide personalized services. To solve this task, matrix factorization is a promising tool due to its success in recommender systems. However, previously proposed matrix factorization (MF) methods do not explore geographical influence, e.g., multi-center check-in property, which yields suboptimal solutions for the recommendation. In this paper, to the best of our knowledge, we are the first to fuse MF with geographical and social influence for POI recommendation in LBSNs. We first capture the geographical influence via modeling the probability of a user's check-in on a location as a Multi-center Gaussian Model (MGM). Next, we include social information and fuse the geographical influence into a generalized matrix factorization framework. Our solution to POI recommendation is efficient and scales linearly with the number of observations. Finally, we conduct thorough experiments on a large-scale real-world LBSNs dataset and demonstrate that the fused matrix factorization framework with MGM utilizes the distance information sufficiently and outperforms other state-of-the-art methods significantly. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Check-in; Data sets; Distance information; Facebook; Gaussian model; Generalized matrix; Location based; Matrix factorizations; Personalized service; Social influence; Social information; Social Networks; State-of-the-art methods; Suboptimal solution; Artificial intelligence; Social networking (online); Economic and social effects",Conference Paper,Scopus,2-s2.0-84868276527
"Jin X., Krishnan R., Sandhu R.","A unified attribute-based access control model covering DAC, MAC and RBAC",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",113,10.1007/978-3-642-31540-4_4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864355928&doi=10.1007%2f978-3-642-31540-4_4&partnerID=40&md5=b1d1ab7971947b1f941f41ad20f9e5e2","Recently, there has been considerable interest in attribute based access control (ABAC) to overcome the limitations of the dominant access control models (i.e, discretionary-DAC, mandatory-MAC and role based-RBAC) while unifying their advantages. Although some proposals for ABAC have been published, and even implemented and standardized, there is no consensus on precisely what is meant by ABAC or the required features of ABAC. There is no widely accepted ABAC model as there are for DAC, MAC and RBAC. This paper takes a step towards this end by constructing an ABAC model that has ""just sufficient"" features to be ""easily and naturally"" configured to do DAC, MAC and RBAC. For this purpose we understand DAC to mean owner-controlled access control lists, MAC to mean lattice-based access control with tranquility and RBAC to mean flat and hierarchical RBAC. Our central contribution is to take a first cut at establishing formal connections between the three successful classical models and desired ABAC models. © 2012 IFIP International Federation for Information Processing.","ABAC; Attribute; DAC; MAC; RBAC; XACML","ABAC; Attribute; DAC; MAC; RBAC; XACML; Artificial intelligence; Access control",Conference Paper,Scopus,2-s2.0-84864355928
"Ardagna D., Di Nitto E., Mohagheghi P., Mosser S., Ballagny C., D'Andria F., Casale G., Matthews P., Nechifor C.-S., Petcu D., Gericke A., Sheridan C.","MODAClouds: A model-driven approach for the design and execution of applications on multiple clouds",2012,"2012 4th International Workshop on Modeling in Software Engineering, MiSE 2012 - Proceedings",112,10.1109/MISE.2012.6226014,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864997201&doi=10.1109%2fMISE.2012.6226014&partnerID=40&md5=8d75e7cfcd76d928b2579cf6761e04cb","Cloud computing is emerging as a major trend in the ICT industry. While most of the attention of the research community is focused on considering the perspective of the Cloud providers, offering mechanisms to support scaling of resources and interoperability and federation between Clouds, the perspective of developers and operators willing to choose the Cloud without being strictly bound to a specific solution is mostly neglected. We argue that Model-Driven Development can be helpful in this context as it would allow developers to design software systems in a cloud-agnostic way and to be supported by model transformation techniques into the process of instantiating the system into specific, possibly, multiple Clouds. The MODAClouds (MOdel-Driven Approach for the design and execution of applications on multiple Clouds) approach we present here is based on these principles and aims at supporting system developers and operators in exploiting multiple Clouds for the same system and in migrating (part of) their systems from Cloud to Cloud as needed. MODAClouds offers a quality-driven design, development and operation method and features a Decision Support System to enable risk analysis for the selection of Cloud providers and for the evaluation of the Cloud adoption impact on internal business processes. Furthermore, MODAClouds offers a run-time environment for observing the system under execution and for enabling a feedback loop with the design environment. This allows system developers to react to performance fluctuations and to re-deploy applications on different Clouds on the long term. © 2012 IEEE.","Cloud computing; model-driven development; performance; portability","Business Process; Cloud providers; Design environment; Design softwares; Feed-back loop; ICT industries; Model driven approach; Model driven development; Model transformation technique; Operation methods; performance; Research communities; Runtime environments; Supporting systems; System developers; Artificial intelligence; Cloud computing; Computer software portability; Decision support systems; Software engineering; Design",Conference Paper,Scopus,2-s2.0-84864997201
"Chai W.K., He D., Psaras I., Pavlou G.","Cache ""less for more"" in information-centric networks",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",112,10.1007/978-3-642-30045-5_3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861660771&doi=10.1007%2f978-3-642-30045-5_3&partnerID=40&md5=f9eb3debedfd51e5f6117a4fa4ab0f92","Ubiquitous in-network caching is one of the key aspects of information-centric networking (ICN) which has recently received widespread research interest. In one of the key relevant proposals known as Networking Named Content (NNC), the premise is that leveraging in-network caching to store content in every node it traverses along the delivery path can enhance content delivery. We question such indiscriminate universal caching strategy and investigate whether caching less can actually achieve more. Specifically, we investigate if caching only in a subset of node(s) along the content delivery path can achieve better performance in terms of cache and server hit rates. In this paper, we first study the behavior of NNC's ubiquitous caching and observe that even naïve random caching at one intermediate node within the delivery path can achieve similar and, under certain conditions, even better caching gain. We propose a centrality-based caching algorithm by exploiting the concept of (ego network) betweenness centrality to improve the caching gain and eliminate the uncertainty in the performance of the simplistic random caching strategy. Our results suggest that our solution can consistently achieve better gain across both synthetic and real network topologies that have different structural properties. © 2012 IFIP International Federation for Information Processing.","betweenness centrality; caching; Information-centric networking","Betweenness centrality; caching; Caching algorithm; Caching strategy; Content delivery; Delivery path; Hit rate; Information-centric networking; Intermediate node; Real networks; Artificial intelligence; Electric network topology",Conference Paper,Scopus,2-s2.0-84861660771
"Lewko A.","Tools for simulating features of composite order bilinear groups in the prime order setting",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",109,10.1007/978-3-642-29011-4_20,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859952520&doi=10.1007%2f978-3-642-29011-4_20&partnerID=40&md5=40608ace93c65c7556e3d75825fa055f","In this paper, we explore a general methodology for converting composite order pairing-based cryptosystems into the prime order setting. We employ the dual pairing vector space approach initiated by Okamoto and Takashima and formulate versatile tools in this framework that can be used to translate composite order schemes for which the prior techniques of Freeman were insufficient. Our techniques are typically applicable for composite order schemes relying on the canceling property and proven secure from variants of the subgroup decision assumption, and will result in prime order schemes that are proven secure from the decisional linear assumption. As an instructive example, we obtain a translation of the Lewko-Waters composite order IBE scheme. This provides a close analog of the Boneh-Boyen IBE scheme that is proven fully secure from the decisional linear assumption. In the full version of this paper, we also provide a translation of the Lewko-Waters unbounded HIBE scheme. © 2012 International Association for Cryptologic Research.",,"General methodologies; Okamoto; Pairing based cryptosystems; Prime orders; Versatile tools; Bilinear groups; Fully secure; General methodologies; Okamoto; Pairing based cryptosystems; Prime orders; Versatile tools; Artificial intelligence; Vector spaces; Cryptography; Cryptography",Conference Paper,Scopus,2-s2.0-84859952520
"Chen X., Udupa J.K., Bagci U., Zhuge Y., Yao J.","Medical image segmentation by combining graph cuts and oriented active appearance models",2012,"IEEE Transactions on Image Processing",107,10.1109/TIP.2012.2186306,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859068489&doi=10.1109%2fTIP.2012.2186306&partnerID=40&md5=7f09220e66d7058dfa0e2cdedc8f52fd","In this paper, we propose a novel method based on a strategic combination of the active appearance model (AAM), live wire (LW), and graph cuts (GCs) for abdominal 3-D organ segmentation. The proposed method consists of three main parts: model building, object recognition, and delineation. In the model building part, we construct the AAM and train the LW cost function and GC parameters. In the recognition part, a novel algorithm is proposed for improving the conventional AAM matching method, which effectively combines the AAM and LW methods, resulting in the oriented AAM (OAAM). A multiobject strategy is utilized to help in object initialization. We employ a pseudo-3-D initialization strategy and segment the organs slice by slice via a multiobject OAAM method. For the object delineation part, a 3-D shape-constrained GC method is proposed. The object shape generated from the initialization step is integrated into the GC cost computation, and an iterative GC-OAAM method is used for object delineation. The proposed method was tested in segmenting the liver, kidneys, and spleen on a clinical CT data set and also on the MICCAI 2007 Grand Challenge liver data set. The results show the following: 1) The overall segmentation accuracy of true positive volume fraction TPVF > 94.3% and false positive volume fraction FPVF < 0.2% can be achieved; 2) the initialization performance can be improved by combining the AAM and LW; 3) the multiobject strategy greatly facilitates initialization; 4) compared with the traditional 3-D AAM method, the pseudo-3-D OAAM method achieves comparable performance while running 12 times faster; and 5) the performance of the proposed method is comparable to state-of-the-art liver segmentation algorithm. The executable version of the 3-D shape-constrained GC method with a user interface can be downloaded from http://xinjianchen.wordpress.com/research/. © 2011 IEEE.","Active appearance model (AAM); graph cut (GC); live wire (LW); object segmentation","Active appearance models; Clinical ct; Data sets; False positive; Grand Challenge; Graph cut; Initialization step; Live wire; Liver segmentation; Matching methods; Medical image segmentation; Multiobject; Novel algorithm; Object delineation; Object initialization; Object segmentation; Object shape; Organ segmentation; Segmentation accuracy; True positive; Algorithms; Graphic methods; Image segmentation; Medical image processing; Model buildings; Object recognition; User interfaces; Wire; Three dimensional; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; human; image enhancement; methodology; reproducibility; sensitivity and specificity; three dimensional imaging; Algorithms; Artificial Intelligence; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84859068489
"Wei G., Zhao X.","Some dependent aggregation operators with 2-tuple linguistic information and their application to multiple attribute group decision making",2012,"Expert Systems with Applications",107,10.1016/j.eswa.2011.11.120,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855900986&doi=10.1016%2fj.eswa.2011.11.120&partnerID=40&md5=44f37e50d8ca393fffd8785ba176fd0d","We investigate the multiple attribute group decision making (MAGDM) problems in which the attribute values take the form of 2-tuple linguistic information. Motivated by the ideal of dependent aggregation [Xu, Z. S. (2006). Dependent OWA operators. Lecture Notes in Artificial Intelligence, 3885, 172-178], in this paper, we develop some dependent 2-tuple linguistic aggregation operators: the dependent 2-tuple ordered weighted averaging (D2TOWA) operator and the dependent 2-tuple ordered weighted geometric (D2TOWG) operator, in which the associated weights only depend on the aggregated 2-tuple linguistic arguments and can relieve the influence of unfair 2-tuple linguistic arguments on the aggregated results by assigning low weights to those ""false"" and ""biased"" ones and then apply them to develop some approaches for multiple attribute group decision making with 2-tuples linguistic information. Finally, some illustrative examples are given to verify the developed approach and to demonstrate its practicality and effectiveness. © 2011 Elsevier Ltd. All rights reserved.","2-Tuple linguistic information; Dependent 2-tuple ordered weighted averaging (D2TOWA) operator; Multiple attribute group decision making (MAGDM); Power average operator; The dependent 2-tuple ordered weighted geometric (D2TOWG) operator","2-Tuple; 2-tuple linguistic informations; 2-tuples; Aggregation operator; Attribute values; Illustrative examples; Lecture Notes; Linguistic information; Low weight; Multiple attribute group; Ordered weighted averaging; OWA operators; Power average operator; Artificial intelligence; Decision making; Mathematical operators; Statistical methods; Linguistics",Article,Scopus,2-s2.0-84855900986
"Dimitrov S., Sinanovic S., Haas H.","Clipping noise in OFDM-based optical wireless communication systems",2012,"IEEE Transactions on Communications",106,10.1109/TCOMM.2012.022712.100493,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860005378&doi=10.1109%2fTCOMM.2012.022712.100493&partnerID=40&md5=1673f114032144d04d1a394ecf1d4c46","In this paper, the impact of clipping noise on optical wireless communication (OWC) systems employing orthogonal frequency division multiplexing (OFDM) is investigated. The two existing optical OFDM (O-OFDM) transmission schemes, asymmetrically clipped optical OFDM (ACO-OFDM) and direct-current- biased optical OFDM (DCO-OFDM), are studied. Time domain signal clipping generally results from direct current (DC) biasing and/or from physical limitations of the transmitter front-end. These include insufficient forward biasing and the maximum power driving limit of the emitter. The clipping noise can be modeled according to the Bussgang theorem and the central limit theorem (CLT) as attenuation of the data-carrying subcarriers at the receiver and addition of zero-mean complex-valued Gaussian noise. Analytical expressions for the attenuation factor and the clipping noise variance are determined in closed-form and employed in the derivation of the electrical signal-to-noise ratio (SNR). The validity of the model is verified through a Monte Carlo bit-error ratio (BER) simulation. Finally, the BER performance of ACO-OFDM with DCO-OFDM is compared for different clipping levels and multi-level quadrature amplitude modulation (M-QAM) schemes. © 2012 IEEE.","Gaussian processes; non-linear distortion; OFDM; optical devices; Wireless communication","Analytical expressions; Attenuation factors; BER performance; Bit error ratio (BER); Bussgang theorem; Central Limit Theorem; Clipping levels; Clipping noise; Closed form; Direct current; Gaussian Processes; Maximum power; MONTE CARLO; Multi-level; Optical OFDM; Optical wireless communication systems; Orthogonal frequency division multiplexing (OFDM); Physical limitations; Signaltonoise ratio (SNR); Sub-carriers; Time-domain signal; Transmission schemes; Wireless communications; Artificial intelligence; Computer simulation; DC power transmission; Gaussian noise (electronic); Monte Carlo methods; Optical communication; Optical devices; Quadrature amplitude modulation; Signal to noise ratio; Wireless telecommunication systems; Orthogonal frequency division multiplexing",Article,Scopus,2-s2.0-84860005378
"Waltman L., Van Eck N.J.","The inconsistency of the h-index",2012,"Journal of the American Society for Information Science and Technology",104,10.1002/asi.21678,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857371655&doi=10.1002%2fasi.21678&partnerID=40&md5=3aa54e50215ff17094080d39b5588102","The h-index is a popular bibliometric indicator for assessing individual scientists. We criticize the h-index from a theoretical point of view. We argue that for the purpose of measuring the overall scientific impact of a scientist (or some other unit of analysis), the h-index behaves in a counterintuitive way. In certain cases, the mechanism used by the h-index to aggregate publication and citation statistics into a single number leads to inconsistencies in the way in which scientists are ranked. Our conclusion is that the h-index cannot be considered an appropriate indicator of a scientist's overall scientific impact. Based on recent theoretical insights, we discuss what kind of indicators can be used as an alternative to the h-index.We pay special attention to the highly cited publications indicator. This indicator has a lot in common with the h-index, but unlike the h-index it does not produce inconsistent rankings. © 2011 ASIS&T.",,"H indices; Theoretical points; Unit of analysis; Artificial intelligence; Software engineering; Indexing (of information)",Article,Scopus,2-s2.0-84857371655
"Valenti R., Gevers T.","Accurate eye center location through invariant isocentric patterns",2012,"IEEE Transactions on Pattern Analysis and Machine Intelligence",102,10.1109/TPAMI.2011.251,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865605498&doi=10.1109%2fTPAMI.2011.251&partnerID=40&md5=ca6a056b9ff5c740bdecff6877b641ae","Locating the center of the eyes allows for valuable information to be captured and used in a wide range of applications. Accurate eye center location can be determined using commercial eye-gaze trackers, but additional constraints and expensive hardware make these existing solutions unattractive and impossible to use on standard (i.e., visible wavelength), low-resolution images of eyes. Systems based solely on appearance are proposed in the literature, but their accuracy does not allow us to accurately locate and distinguish eye centers movements in these low-resolution settings. Our aim is to bridge this gap by locating the center of the eye within the area of the pupil on low-resolution images taken from a webcam or a similar device. The proposed method makes use of isophote properties to gain invariance to linear lighting changes (contrast and brightness), to achieve in-plane rotational invariance, and to keep low-computational costs. To further gain scale invariance, the approach is applied to a scale space pyramid. In this paper, we extensively test our approach for its robustness to changes in illumination, head pose, scale, occlusion, and eye rotation. We demonstrate that our system can achieve a significant improvement in accuracy over state-of-the-art techniques for eye center location in standard low-resolution imagery. © 2012 IEEE.","Eye center location; facial features detection.; isophotes","Center locations; Expensive hardware; Eye-gaze; Facial features detection; Head pose; Isophotes; Low resolution images; Low-resolution imagery; Rotational invariances; Scale invariance; Scale spaces; Visible wavelengths; Artificial intelligence; Computer vision; Eye movements; algorithm; article; artificial intelligence; automated pattern recognition; eye movement; face; factual database; histology; human; illumination; image processing; iris; methodology; physiology; pupil; Algorithms; Artificial Intelligence; Databases, Factual; Eye Movements; Face; Humans; Image Processing, Computer-Assisted; Iris; Lighting; Pattern Recognition, Automated; Pupil",Article,Scopus,2-s2.0-84865605498
"Lucchi A., Smith K., Achanta R., Knott G., Fua P.","Supervoxel-based segmentation of mitochondria in em image stacks with learned shape features",2012,"IEEE Transactions on Medical Imaging",101,10.1109/TMI.2011.2171705,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856735789&doi=10.1109%2fTMI.2011.2171705&partnerID=40&md5=c134cbdc0a573c0b871eed19389cb08d","It is becoming increasingly clear that mitochondria play an important role in neural function. Recent studies show mitochondrial morphology to be crucial to cellular physiology and synaptic function and a link between mitochondrial defects and neuro-degenerative diseases is strongly suspected. Electron microscopy (EM), with its very high resolution in all three directions, is one of the key tools to look more closely into these issues but the huge amounts of data it produces make automated analysis necessary. State-of-the-art computer vision algorithms designed to operate on natural 2-D images tend to perform poorly when applied to EM data for a number of reasons. First, the sheer size of a typical EM volume renders most modern segmentation schemes intractable. Furthermore, most approaches ignore important shape cues, relying only on local statistics that easily become confused when confronted with noise and textures inherent in the data. Finally, the conventional assumption that strong image gradients always correspond to object boundaries is violated by the clutter of distracting membranes. In this work, we propose an automated graph partitioning scheme that addresses these issues. It reduces the computational complexity by operating on supervoxels instead of voxels, incorporates shape features capable of describing the 3-D shape of the target objects, and learns to recognize the distinctive appearance of true boundaries. Our experiments demonstrate that our approach is able to segment mitochondria at a performance level close to that of a human annotator, and outperforms a state-of-the-art 3-D segmentation technique. © 2011 IEEE.","Electron microscopy (EM); mitochondria; segmentation; shape features; supervoxels","3-D shape; 3D segmentation; Automated analysis; Cellular physiology; Computer vision algorithms; Graph Partitioning; Image gradients; Image stacks; Local statistics; Mitochondrial defects; Mitochondrial morphology; Neural functions; Object boundaries; Performance level; Segmentation scheme; Shape features; Sheer size; supervoxels; Target object; Very high resolution; Electron microscopes; Image segmentation; Mitochondria; Three dimensional; algorithm; anatomy; article; artificial intelligence; automated pattern recognition; brain; computer assisted diagnosis; electron microscopy; human; image enhancement; methodology; mitochondrion; reproducibility; sensitivity and specificity; three dimensional imaging; ultrastructure; Algorithms; Anatomy, Cross-Sectional; Artificial Intelligence; Brain; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Microscopy, Electron; Mitochondria; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84856735789
"Waltman L., Van Eck N.J.","A new methodology for constructing a publication-level classification system of science",2012,"Journal of the American Society for Information Science and Technology",100,10.1002/asi.22748,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870498391&doi=10.1002%2fasi.22748&partnerID=40&md5=0dcc8ba8f73817486ba78590f1cd1504","Classifying journals or publications into research areas is an essential element of many bibliometric analyses. Classification usually takes place at the level of journals, where the Web of Science subject categories are the most popular classification system. However, journal-level classification systems have two important limitations: They offer only a limited amount of detail, and they have difficulties with multidisciplinary journals. To avoid these limitations, we introduce a new methodology for constructing classification systems at the level of individual publications. In the proposed methodology, publications are clustered into research areas based on citation relations. The methodology is able to deal with very large numbers of publications. We present an application in which a classification system is produced that includes almost 10 million publications. Based on an extensive analysis of this classification system, we discuss the strengths and the limitations of the proposed methodology. Important strengths are the transparency and relative simplicity of the methodology and its fairly modest computing and memory requirements. The main limitation of the methodology is its exclusive reliance on direct citation relations between publications. The accuracy of the methodology can probably be increased by also taking into account other types of relations-for instance, based on bibliographic coupling. © 2012 ASIS&T.","bibliometrics; citation analysis","Bibliographic couplings; Bibliometric analysis; Bibliometrics; Citation analysis; Classification system; Essential elements; Memory requirements; Web of Science; Artificial intelligence; Software engineering; Publishing",Article,Scopus,2-s2.0-84870498391
"Yin Y.H., Fan Y.J., Xu L.D.","EMG and EPP-integrated human-machine interface between the paralyzed and rehabilitation exoskeleton",2012,"IEEE Transactions on Information Technology in Biomedicine",100,10.1109/TITB.2011.2178034,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862013966&doi=10.1109%2fTITB.2011.2178034&partnerID=40&md5=3f42c74f08e2ba67fd5bf8e31bf9d34e","Although a lower extremity exoskeleton shows great prospect in the rehabilitation of the lower limb, it has not yet been widely applied to the clinical rehabilitation of the paralyzed. This is partly caused by insufficient information interactions between the paralyzed and existing exoskeleton that cannot meet the requirements of harmonious control. In this research, a bidirectional human-machine interface including a neurofuzzy controller and an extended physiological proprioception (EPP) feedback system is developed by imitating the biological closed-loop control system of human body. The neurofuzzy controller is built to decode human motion in advance by the fusion of the fuzzy electromyographic signals reflecting human motion intention and the precise proprioception providing joint angular feedback information. It transmits control information from human to exoskeleton, while the EPP feedback system based on haptic stimuli transmits motion information of the exoskeleton back to the human. Joint angle and torque information are transmitted in the form of air pressure to the human body. The real-time bidirectional human-machine interface can help a patient with lower limb paralysis to control the exoskeleton with his/her healthy side and simultaneously perceive motion on the paralyzed side by EPP. The interface rebuilds a closed-loop motion control system for paralyzed patients and realizes harmonious control of the human-machine system. © 2012 IEEE.","E-health care; exoskeleton robot; harmonious control; health care enterprise systems; information interaction","Air pressures; Closed-loop; Control information; E-health care; Electromyographic signal; Enterprise system; Exoskeleton robots; Extended physiological proprioception; Feed back information; Feedback systems; Haptic stimuli; Human bodies; Human Machine Interface; Human motion intention; Human motions; Human-machine systems; Information interaction; Joint angle; Lower extremity exoskeletons; Lower limb; Motion information; Neuro-fuzzy controller; Paralyzed patients; Atmospheric pressure; Health care; Man machine systems; Controllers; adult; algorithm; article; artificial intelligence; artificial neural network; computer interface; electromyography; fuzzy logic; human; instrumentation; limb prosthesis; male; man machine interaction; methodology; paraplegia; physiology; proprioception; robotics; signal processing; Adult; Algorithms; Artificial Intelligence; Artificial Limbs; Electromyography; Fuzzy Logic; Humans; Male; Man-Machine Systems; Neural Networks (Computer); Paraplegia; Proprioception; Robotics; Signal Processing, Computer-Assisted; User-Computer Interface",Article,Scopus,2-s2.0-84862013966
"Liu C., Gong S., Loy C.C., Lin X.","Person re-identification: What features are important?",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",99,10.1007/978-3-642-33863-2_39,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867698801&doi=10.1007%2f978-3-642-33863-2_39&partnerID=40&md5=d29a91eed80717b8ad6b2e0b8916d0b9","State-of-the-art person re-identification methods seek robust person matching through combining various feature types. Often, these features are implicitly assigned with a single vector of global weights, which are assumed to be universally good for all individuals, independent to their different appearances. In this study, we show that certain features play more important role than others under different circumstances. Consequently, we propose a novel unsupervised approach for learning a bottom-up feature importance, so features extracted from different individuals are weighted adaptively driven by their unique and inherent appearance attributes. Extensive experiments on two public datasets demonstrate that attribute-sensitive feature importance facilitates more accurate person matching when it is fused together with global weights obtained using existing methods. © 2012 Springer-Verlag.",,"Data sets; Feature types; Single vectors; Unsupervised approaches; Artificial intelligence; Computer vision",Conference Paper,Scopus,2-s2.0-84867698801
"Kumar S., Mohri M., Talwalkar A.","Sampling methods for the Nyström method",2012,"Journal of Machine Learning Research",99,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860672647&partnerID=40&md5=d444a015e00fbb1162520b91ef7b486e","The Nyström method is an efficient technique to generate low-rank matrix approximations and is used in several large-scale learning applications. A key aspect of this method is the procedure according to which columns are sampled from the original matrix. In this work, we explore the efficacy of a variety of fixed and adaptive sampling schemes. We also propose a family of ensemble- based sampling algorithms for the Nyström method. We report results of extensive experiments that provide a detailed comparison of various fixed and adaptive sampling techniques, and demonstrate the performance improvement associated with the ensemble Nyström method when used in conjunction with either fixed or adaptive sampling schemes. Corroborating these empirical findings, we present a theoretical analysis of the Nyström method, providing novel error bounds guaranteeing a better convergence rate of the ensemble Nyström method in comparison to the standard Nyström method. © 2012 Sanjiv Kumar, Mehryar Mohri and Ameet Talwalkar.","Ensemble methods; Large-scale learning; Low-rank approximation; Nyström method","Adaptive sampling; Convergence rates; Empirical findings; Ensemble methods; Error bound; Large-scale learning; Low rank approximations; Low-rank matrices; M method; Performance improvements; Sampling algorithm; Sampling method; Artificial intelligence; Software engineering; Error analysis",Article,Scopus,2-s2.0-84860672647
"Raykar V.C., Yu S.","Eliminating spammers and ranking annotators for crowdsourced labeling tasks",2012,"Journal of Machine Learning Research",97,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857856268&partnerID=40&md5=0d39bab6ce2b11ce689c96cbaa712275","With the advent of crowdsourcing services it has become quite cheap and reasonably effective to get a data set labeled by multiple annotators in a short amount of time. Various methods have been proposed to estimate the consensus labels by correcting for the bias of annotators with different kinds of expertise. Since we do not have control over the quality of the annotators, very often the annotations can be dominated by spammers, defined as annotators who assign labels randomly without actually looking at the instance. Spammers can make the cost of acquiring labels very expensive and can potentially degrade the quality of the final consensus labels. In this paper we propose an empirical Bayesian algorithm called SpEM that iteratively eliminates the spammers and estimates the consensus labels based only on the good annotators. The algorithm is motivated by defining a spammer score that can be used to rank the annotators. Experiments on simulated and real data show that the proposed approach is better than (or as good as) the earlier approaches in terms of the accuracy and uses a significantly smaller number of annotators. © 2012 Vikas C. Raykar and Shipeng Yu.","Crowdsourcing; Multiple annotators; Ranking annotators; Spammers","Bayesian algorithms; Crowdsourcing; Data sets; Multiple annotators; Ranking annotators; Spammers; Artificial intelligence; Software engineering; Algorithms",Article,Scopus,2-s2.0-84857856268
"Lozano G.A., Larivière V., Gingras Y.","The weakening relationship between the impact factor and papers' citations in the digital age",2012,"Journal of the American Society for Information Science and Technology",95,10.1002/asi.22731,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868215130&doi=10.1002%2fasi.22731&partnerID=40&md5=e234db49f5317469b193cb6240076cc9","Historically, papers have been physically bound to the journal in which they were published; but in the digital age papers are available individually, no longer tied to their respective journals. Hence, papers now can be read and cited based on their own merits, independently of the journal's physical availability, reputation, or impact factor (IF). We compare the strength of the relationship between journals' IFs and the actual citations received by their respective papers from 1902 to 2009. Throughout most of the 20th century, papers' citation rates were increasingly linked to their respective journals' IFs. However, since 1990, the advent of the digital age, the relation between IFs and paper citations has been weakening. This began first in physics, a field that was quick to make the transition into the electronic domain. Furthermore, since 1990 the overall proportion of highly cited papers coming from highly cited journals has been decreasing and, of these highly cited papers, the proportion not coming from highly cited journals has been increasing. Should this pattern continue, it might bring an end to the use of the IF as a way to evaluate the quality of journals, papers, and researchers. © 2012 ASIS&T.","bibliometrics; impact factor","20th century; Bibliometrics; Cited papers; Digital age; Electronic domains; Impact factor; Paper citations; Artificial intelligence; Software engineering; Paper",Article,Scopus,2-s2.0-84868215130
"Zhang S., Zhan Y., Metaxas D.N.","Deformable segmentation via sparse representation and dictionary learning",2012,"Medical Image Analysis",94,10.1016/j.media.2012.07.007,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866450569&doi=10.1016%2fj.media.2012.07.007&partnerID=40&md5=1e3d93ce13db58ce7f0e1d1b247f7416","""Shape"" and ""appearance"", the two pillars of a deformable model, complement each other in object segmentation. In many medical imaging applications, while the low-level appearance information is weak or mis-leading, shape priors play a more important role to guide a correct segmentation, thanks to the strong shape characteristics of biological structures. Recently a novel shape prior modeling method has been proposed based on sparse learning theory. Instead of learning a generative shape model, shape priors are incorporated on-the-fly through the sparse shape composition (SSC). SSC is robust to non-Gaussian errors and still preserves individual shape characteristics even when such characteristics is not statistically significant. Although it seems straightforward to incorporate SSC into a deformable segmentation framework as shape priors, the large-scale sparse optimization of SSC has low runtime efficiency, which cannot satisfy clinical requirements. In this paper, we design two strategies to decrease the computational complexity of SSC, making a robust, accurate and efficient deformable segmentation system. (1) When the shape repository contains a large number of instances, which is often the case in 2D problems, K-SVD is used to learn a more compact but still informative shape dictionary. (2) If the derived shape instance has a large number of vertices, which often appears in 3D problems, an affinity propagation method is used to partition the surface into small sub-regions, on which the sparse shape composition is performed locally. Both strategies dramatically decrease the scale of the sparse optimization problem and hence speed up the algorithm. Our method is applied on a diverse set of biomedical image analysis problems. Compared to the original SSC, these two newly-proposed modules not only significant reduce the computational complexity, but also improve the overall accuracy. © 2012 Elsevier B.V.","Dictionary learning; Mesh partitioning; Segmentation; Shape prior; Sparse representation","3-D problems; Affinity propagation; Biological structures; Biomedical image analysis; Deformable models; Deformable segmentation; Dictionary learning; Imaging applications; Learning Theory; Mesh partitioning; Non-Gaussian; Object segmentation; On-the-fly; Optimization problems; Run-time efficiency; Shape characteristics; Shape model; Shape priors; Sparse representation; Sub-regions; Computational complexity; Image segmentation; Medical imaging; Optimization; Three dimensional; article; biology; book; cerebellum; diagnostic accuracy; diagnostic imaging; image analysis; learning; learning algorithm; nonhuman; priority journal; Algorithms; Artificial Intelligence; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Information Storage and Retrieval; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-84866450569
"Dembczynski K., Waegeman W., Cheng W., Hüllermeier E.","On label dependence and loss minimization in multi-label classification",2012,"Machine Learning",94,10.1007/s10994-012-5285-8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865223006&doi=10.1007%2fs10994-012-5285-8&partnerID=40&md5=237b6c28fd5fcfa961391706967576a2","Most of the multi-label classification (MLC) methods proposed in recent years intended to exploit, in one way or the other, dependencies between the class labels. Comparing to simple binary relevance learning as a baseline, any gain in performance is normally explained by the fact that this method is ignoring such dependencies. Without questioning the correctness of such studies, one has to admit that a blanket explanation of that kind is hiding many subtle details, and indeed, the underlying mechanisms and true reasons for the improvements reported in experimental studies are rarely laid bare. Rather than proposing yet another MLC algorithm, the aim of this paper is to elaborate more closely on the idea of exploiting label dependence, thereby contributing to a better understanding of MLC. Adopting a statistical perspective, we claim that two types of label dependence should be distinguished, namely conditional and marginal dependence. Subsequently, we present three scenarios in which the exploitation of one of these types of dependence may boost the predictive performance of a classifier. In this regard, a close connection with loss minimization is established, showing that the benefit of exploiting label dependence does also depend on the type of loss to be minimized. Concrete theoretical results are presented for two repre-sentative loss functions, namely the Hamming loss and the subset 0/1 loss. In addition, we give an overview of state-of-the-art decomposition algorithms for MLC and we try to reveal the reasons for their effectiveness. Our conclusions are supported by carefully designed experiments on synthetic and benchmark data. © The Author(s) 2012.","Label dependence; Loss functions; Multi-label classification","Benchmark data; Class labels; Decomposition algorithm; Designed experiments; Experimental studies; Loss functions; Loss minimization; Multi-label; Predictive performance; Relevance learning; Theoretical result; Underlying mechanism; Artificial intelligence; Software engineering; Algorithms",Article,Scopus,2-s2.0-84865223006
"Lukoševičius M.","A practical guide to applying echo state networks",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",93,10.1007/978-3-642-35289-8-36,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872502995&doi=10.1007%2f978-3-642-35289-8-36&partnerID=40&md5=5550885e46ee81bec093088eeca8c4c1","Reservoir computing has emerged in the last decade as an alternative to gradient descent methods for training recurrent neural networks. Echo State Network (ESN) is one of the key reservoir computing ""flavors"". While being practical, conceptually simple, and easy to implement, ESNs require some experience and insight to achieve the hailed good performance in many tasks. Here we present practical techniques and recommendations for successfully applying ESNs, as well as some more advanced application-specific modifications. © Springer-Verlag Berlin Heidelberg 2012.",,"Echo state networks; Gradient Descent method; Practical guide; Reservoir Computing; Artificial intelligence; Recurrent neural networks",Article,Scopus,2-s2.0-84872502995
"Nielsen J.B., Nordholt P.S., Orlandi C., Burra S.S.","A new approach to practical active-secure two-party computation",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",93,10.1007/978-3-642-32009-5_40,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865506559&doi=10.1007%2f978-3-642-32009-5_40&partnerID=40&md5=9cb3e1c34c45b2c8d500ceaa0ae07aa1","We propose a new approach to practical two-party computation secure against an active adversary. All prior practical protocols were based on Yao's garbled circuits. We use an OT-based approach and get efficiency via OT extension in the random oracle model. To get a practical protocol we introduce a number of novel techniques for relating the outputs and inputs of OTs in a larger construction. We also report on an implementation of this approach, that shows that our protocol is more efficient than any previous one: For big enough circuits, we can evaluate more than 20000 Boolean gates per second. As an example, evaluating one oblivious AES encryption (∼ 34000 gates) takes 64 seconds, but when repeating the task 27 times it only takes less than 3 seconds per instance. © 2012 International Association for Cryptologic Research.",,"Active adversary; AES encryption; Boolean gate; Garbled circuits; Novel techniques; Random Oracle model; Two-party computation; Artificial intelligence; Cryptography",Conference Paper,Scopus,2-s2.0-84865506559
"Lan T., Wang Y., Yang W., Robinovitch S.N., Mori G.","Discriminative latent models for recognizing contextual group activities",2012,"IEEE Transactions on Pattern Analysis and Machine Intelligence",92,10.1109/TPAMI.2011.228,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862638723&doi=10.1109%2fTPAMI.2011.228&partnerID=40&md5=91cf94302031384b59aa6a998892fad2","In this paper, we go beyond recognizing the actions of individuals and focus on group activities. This is motivated from the observation that human actions are rarely performed in isolation; the contextual information of what other people in the scene are doing provides a useful cue for understanding high-level activities. We propose a novel framework for recognizing group activities which jointly captures the group activity, the individual person actions, and the interactions among them. Two types of contextual information, group-person interaction and person-person interaction, are explored in a latent variable framework. In particular, we propose three different approaches to model the person-person interaction. One approach is to explore the structures of person-person interaction. Differently from most of the previous latent structured models, which assume a predefined structure for the hidden layer, e.g., a tree structure, we treat the structure of the hidden layer as a latent variable and implicitly infer it during learning and inference. The second approach explores person-person interaction in the feature level. We introduce a new feature representation called the action context (AC) descriptor. The AC descriptor encodes information about not only the action of an individual person in the video, but also the behavior of other people nearby. The third approach combines the above two. Our experimental results demonstrate the benefit of using contextual information for disambiguating group activities. © 2012 IEEE.","context; Group activity recognition; latent structured models","context; Contextual information; Descriptors; Feature level; Feature representation; Group activities; Hidden layers; Human actions; Latent models; Latent variable; Structured model; Tree structures; Image recognition; Trees (mathematics); Motion estimation; algorithm; article; artificial intelligence; behavior; classification; daily life activity; discriminant analysis; falling; human; human relation; image processing; methodology; nursing home; receiver operating characteristic; social behavior; theoretical model; videorecording; Accidental Falls; Activities of Daily Living; Algorithms; Artificial Intelligence; Discriminant Analysis; Humans; Image Processing, Computer-Assisted; Interpersonal Relations; Models, Theoretical; Nursing Homes; ROC Curve; Social Behavior; Spatial Behavior; Video Recording",Article,Scopus,2-s2.0-84862638723
"Zeng Z., Zheng W.X.","Multistability of neural networks with time-varying delays and concave-convex characteristics",2012,"IEEE Transactions on Neural Networks and Learning Systems",91,10.1109/TNNLS.2011.2179311,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867337265&doi=10.1109%2fTNNLS.2011.2179311&partnerID=40&md5=dea52810816df16340d936829401ef0c","In this paper, stability of multiple equilibria of neural networks with time-varying delays and concave-convex characteristics is formulated and studied. Some sufficient conditions are obtained to ensure that an n-neuron neural network with concave-convex characteristics can have a fixed point located in the appointed region. By means of an appropriate partition of the n-dimensional state space, when nonlinear activation functions of an n-neuron neural network are concave or convex in 2k+2m-1 intervals, this neural network can have (2k+2m-1)n equilibrium points. This result can be applied to the multiobjective optimal control and associative memory. In particular, several succinct criteria are given to ascertain multistability of cellular neural networks. These stability conditions are the improvement and extension of the existing stability results in the literature. A numerical example is given to illustrate the theoretical findings via computer simulations. © 2012 IEEE.","Attractive set; concave-convex characteristics; fixed point; multistability; neural networks; time-varying delays","Attractive set; concave-convex characteristics; Fixed points; Multi stabilities; Time-varying delay; Associative processing; Cellular neural networks; Computer simulation; Neural networks; Time varying control systems; algorithm; artificial intelligence; artificial neural network; automated pattern recognition; computer simulation; nonlinear system; procedures; statistical model; time; Algorithms; Artificial Intelligence; Computer Simulation; Models, Statistical; Neural Networks (Computer); Nonlinear Dynamics; Pattern Recognition, Automated; Time Factors",Article,Scopus,2-s2.0-84867337265
"Song L., Smola A., Gretton A., Bedo J., Borgwardt K.","Feature selection via dependence maximization",2012,"Journal of Machine Learning Research",91,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862024860&partnerID=40&md5=a8dc5a748d1b0ff5e9086d559eaeab20","We introduce a framework for feature selection based on dependence maximization between the selected features and the labels of an estimation problem, using the Hilbert-Schmidt Independence Criterion. The key idea is that good features should be highly dependent on the labels. Our approach leads to a greedy procedure for feature selection. We show that a number of existing feature selectors are special cases of this framework. Experiments on both artificial and real-world data show that our feature selector works well in practice. © 2012 Le Song, Alex Smola, Arthur Gretton, Justin Bedo and Karsten Borgwardt.","Feature selection; Hilbert space embedding of distribution; Hilbert-Schmidt Independence Criterion; Independence measure; Kernel methods","Estimation problem; Hilbert space embedding of distribution; Hilbert-Schmidt Independence Criterion; Independence measure; Kernel methods; Real world data; Artificial intelligence; Feature extraction; Software engineering",Article,Scopus,2-s2.0-84862024860
"Coron J.-S., Naccache D., Tibouchi M.","Public key compression and modulus switching for fully homomorphic encryption over the integers",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",91,10.1007/978-3-642-29011-4_27,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859990706&doi=10.1007%2f978-3-642-29011-4_27&partnerID=40&md5=3d9dc38c7317e163abb17b80abdd31b3","We describe a compression technique that reduces the public key size of van Dijk, Gentry, Halevi and Vaikuntanathan's (DGHV) fully homomorphic scheme over the integers from Õ (λ 7) Õ (λ 5) to . Our variant remains semantically secure, but in the random oracle model. We obtain an implementation of the full scheme with a 10.1 MB public key instead of 802 MB using similar parameters as in [7]. Additionally we show how to extend the quadratic encryption technique of [7] to higher degrees, to obtain a shorter public-key for the basic scheme. This paper also describes a new modulus switching technique for the DGHV scheme that enables to use the new FHE framework without bootstrapping from Brakerski, Gentry and Vaikuntanathan with the DGHV scheme. Finally we describe an improved attack against the Approximate GCD Problem on which the DGHV scheme is based, with complexity Õ(2 p)instead of Õ(2 3p/2). © 2012 International Association for Cryptologic Research.",,"Approximate gcd; Compression techniques; Encryption technique; Fully homomorphic encryption; Homomorphic scheme; Public keys; Random Oracle model; Switching techniques; Approximate gcd; Compression techniques; Encryption technique; Fully homomorphic encryption; Homomorphic scheme; Public keys; Random Oracle model; Switching techniques; Image quality; Artificial intelligence; Computers; Cryptography; Cryptography",Conference Paper,Scopus,2-s2.0-84859990706
"Mao Y., Kohler E., Morris R.","Cache craftiness for fast multicore key-value storage",2012,"EuroSys'12 - Proceedings of the EuroSys 2012 Conference",90,10.1145/2168836.2168855,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860592643&doi=10.1145%2f2168836.2168855&partnerID=40&md5=2235d108ec3d700be8763cb0d2dba112","We present Masstree, a fast key-value database designed for SMP machines. Masstree keeps all data in memory. Its main data structure is a trie-like concatenation of B +-trees, each of which handles a fixed-length slice of a variable-length key. This structure effectively handles arbitrary-length possiblybinary keys, including keys with long shared prefixes. B +-tree fanout was chosen to minimize total DRAM delay when descending the tree and prefetching each tree node. Lookups use optimistic concurrency control, a read-copy-update-like technique, and do not write shared data structures; updates lock only affected nodes. Logging and checkpointing provide consistency and durability. Though some of these ideas appear elsewhere, Masstree is the first to combine them. We discuss design variants and their consequences. On a 16-core machine, with logging enabled and queries arriving over a network, Masstree executes more than six million simple queries per second. This performance is comparable to that of memcached, a non-persistent hash table server, and higher (often much higher) than that of VoltDB, MongoDB, and Redis. © 2012 ACM.","In-memory; Key-value; Multicore; Persistent","Check pointing; Fan-out; Hash table; In-memory; Key-value; Lookups; Multi core; Optimistic concurrency; Persistent; Prefetching; Shared data structure; Tree nodes; Concurrency control; Data structures; Forestry; Dynamic random access storage; Algorithms; Artificial Intelligence; Data Bases; Problem Solving",Conference Paper,Scopus,2-s2.0-84860592643
"Putha R., Quadrifoglio L., Zechman E.","Comparing Ant Colony Optimization and Genetic Algorithm Approaches for Solving Traffic Signal Coordination under Oversaturation Conditions",2012,"Computer-Aided Civil and Infrastructure Engineering",90,10.1111/j.1467-8667.2010.00715.x,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-83055179798&doi=10.1111%2fj.1467-8667.2010.00715.x&partnerID=40&md5=849601e9921f87e4e76f34afff29b0c2","This article proposes to solve the oversaturated network traffic signal coordination problem using the Ant Colony Optimization (ACO) algorithm. The traffic networks used are discrete time models which use green times at all the intersections throughout the considered period of oversaturation as the decision variables. The ACO algorithm finds intelligent timing plans which take care of dissipation of queues and removal of blockages as opposed to the sole cost minimization usually performed for undersaturation conditions. Two scenarios are considered and results are rigorously compared with solutions obtained using the genetic algorithm (GA), traditionally employed to solve oversaturated conditions. ACO is shown to be consistently more effective for a larger number of trials and to provide more reliable solutions. Further, as a master-slave parallelism is possible for the nature of ACO algorithm, its implementation is suggested to reduce the overall execution time allowing the opportunity to solve real-time signal control systems. © 2011Computer-Aided Civil and Infrastructure Engineering.",,"ACO algorithms; Ant Colony Optimization algorithms; Ant-colony optimization; Coordination problems; Cost minimization; Decision variables; Discrete-time model; Master-slave; Network traffic; Overall execution; Oversaturation; Real-time signals; Timing plans; Traffic networks; Undersaturation; Artificial intelligence; Optimization; Real time systems; Traffic signals; Genetic algorithms; genetic algorithm; numerical model; optimization; traffic congestion; traffic management",Article,Scopus,2-s2.0-83055179798
"Zhang R., Lan Y., Huang G.-B., Xu Z.-B.","Universal approximation of extreme learning machine with adaptive growth of hidden nodes",2012,"IEEE Transactions on Neural Networks and Learning Systems",89,10.1109/TNNLS.2011.2178124,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867839878&doi=10.1109%2fTNNLS.2011.2178124&partnerID=40&md5=b93ceaa3b6f9edec2463ec1d5163b994","Extreme learning machines (ELMs) have been proposed for generalized single-hidden-layer feedforward networks which need not be neuron-like and perform well in both regression and classification applications. In this brief, we propose an ELM with adaptive growth of hidden nodes (AG-ELM), which provides a new approach for the automated design of networks. Different from other incremental ELMs (I-ELMs) whose existing hidden nodes are frozen when the new hidden nodes are added one by one, in AG-ELM the number of hidden nodes is determined in an adaptive way in the sense that the existing networks may be replaced by newly generated networks which have fewer hidden nodes and better generalization performance. We then prove that such an AG-ELM using Lebesgue p-integrable hidden activation functions can approximate any Lebesgue p-integrable function on a compact input set. Simulation results demonstrate and verify that this new approach can achieve a more compact network architecture than the I-ELM. © 2012 IEEE.","Extreme learning machine (ELM); feedforward neural networks; growing algorithm; incremental learning; universal approximation","Activation functions; Adaptive growth; Automated design; Extreme learning machine; Feed-forward network; Generalization performance; Incremental learning; Universal approximation; Approximation algorithms; Feedforward neural networks; Knowledge acquisition; Learning systems; Network architecture; Network layers; algorithm; artificial intelligence; artificial neural network; automated pattern recognition; computer simulation; feedback system; nonlinear system; procedures; statistical model; Algorithms; Artificial Intelligence; Computer Simulation; Feedback; Models, Statistical; Neural Networks (Computer); Nonlinear Dynamics; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84867839878
"Ji R., Yao H., Liu W., Sun X., Tian Q.","Task-dependent visual-codebook compression",2012,"IEEE Transactions on Image Processing",89,10.1109/TIP.2011.2176950,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859073393&doi=10.1109%2fTIP.2011.2176950&partnerID=40&md5=9b7d8d3049bf310bff2d676df609924e","A visual codebook serves as a fundamental component in many state-of-the-art computer vision systems. Most existing codebooks are built based on quantizing local feature descriptors extracted from training images. Subsequently, each image is represented as a high-dimensional bag-of-words histogram. Such highly redundant image description lacks efficiency in both storage and retrieval, in which only a few bins are nonzero and distributed sparsely. Furthermore, most existing codebooks are built based solely on the visual statistics of local descriptors, without considering the supervise labels coming from the subsequent recognition or classification tasks. In this paper, we propose a task-dependent codebook compression framework to handle the above two problems. First, we propose to learn a compression function to map an originally high-dimensional codebook into a compact codebook while maintaining its visual discriminability. This is achieved by a codeword sparse coding scheme with Lasso regression, which minimizes the descriptor distortions of training images after codebook compression. Second, we propose to adapt our codebook compression to the subsequent recognition or classification tasks. This is achieved by introducing a label constraint kernel (LCK) into our compression loss function. In particular, our LCK can model heterogeneous kinds of supervision, i.e., (partial) category labels, correlative semantic annotations, and image query logs. We validated our codebook compression in three computer vision tasks: 1) object recognition in PASCAL Visual Object Class 07; 2) near-duplicate image retrieval in UKBench; and 3) web image search in a collection of 0.5 million Flickr photographs. Our compressed codebook has shown superior performances over several state-of-the-art supervised and unsupervised codebooks. © 2011 IEEE.","Image retrieval; indexing; local feature; object classification; supervised quantization; visual codebook","Bag of words; Classification tasks; Codebooks; Codeword; Compression functions; Computer vision system; Descriptors; Discriminability; Fundamental component; High-dimensional; Highly redundant image; Image query; Local descriptors; Local feature; Loss functions; Near-duplicate image retrieval; Object classification; Semantic annotations; Sparse coding; Storage and retrievals; supervised quantization; Training image; Visual objects; Web image search; Computer vision; Image coding; Indexing (of information); Object recognition; Photography; Semantics; Image retrieval; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; hospital information system; image enhancement; image subtraction; information processing; methodology; Algorithms; Artificial Intelligence; Data Compression; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Radiology Information Systems; Subtraction Technique",Article,Scopus,2-s2.0-84859073393
"Yao B., Fei-Fei L.","Recognizing human-object interactions in still images by modeling the mutual context of objects and human poses",2012,"IEEE Transactions on Pattern Analysis and Machine Intelligence",85,10.1109/TPAMI.2012.67,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865593256&doi=10.1109%2fTPAMI.2012.67&partnerID=40&md5=094640ec0854d21f494568460ce124a9","Detecting objects in cluttered scenes and estimating articulated human body parts from 2D images are two challenging problems in computer vision. The difficulty is particularly pronounced in activities involving human-object interactions (e.g., playing tennis), where the relevant objects tend to be small or only partially visible and the human body parts are often self-occluded. We observe, however, that objects and human poses can serve as mutual context to each other-recognizing one facilitates the recognition of the other. In this paper, we propose a mutual context model to jointly model objects and human poses in human-object interaction activities. In our approach, object detection provides a strong prior for better human pose estimation, while human pose estimation improves the accuracy of detecting the objects that interact with the human. On a six-class sports data set and a 24-class people interacting with musical instruments data set, we show that our mutual context model outperforms state of the art in detecting very difficult objects and estimating human poses, as well as classifying human-object interaction activities. © 2012 IEEE.","action recognition; conditional random field; human pose estimation; Mutual context; object detection","Action recognition; Conditional random field; Human pose estimations; Mutual context; Object Detection; Computer vision; Image recognition; Object recognition; SportS; Motion estimation; algorithm; article; artificial intelligence; automated pattern recognition; body posture; factual database; human; human activities; image processing; methodology; physiology; theoretical model; Algorithms; Artificial Intelligence; Databases, Factual; Human Activities; Humans; Image Processing, Computer-Assisted; Models, Theoretical; Pattern Recognition, Automated; Posture",Article,Scopus,2-s2.0-84865593256
"Brunet D., Vrscay E.R., Wang Z.","On the mathematical properties of the structural similarity index",2012,"IEEE Transactions on Image Processing",85,10.1109/TIP.2011.2173206,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859013796&doi=10.1109%2fTIP.2011.2173206&partnerID=40&md5=3271f65a1288360425b800747c85a7c0","Since its introduction in 2004, the structural similarity (SSIM) index has gained widespread popularity as a tool to assess the quality of images and to evaluate the performance of image processing algorithms and systems. There has been also a growing interest of using SSIM as an objective function in optimization problems in a variety of image processing applications. One major issue that could strongly impede the progress of such efforts is the lack of understanding of the mathematical properties of the SSIM measure. For example, some highly desirable properties such as convexity and triangular inequality that are possessed by the mean squared error may not hold. In this paper, we first construct a series of normalized and generalized (vector-valued) metrics based on the important ingredients of SSIM. We then show that such modified measures are valid distance metrics and have many useful properties, among which the most significant ones include quasi-convexity, a region of convexity around the minimizer, and distance preservation under orthogonal or unitary transformations. The groundwork laid here extends the potentials of SSIM in both theoretical development and practical applications. © 2011 IEEE.","Cone metrics; normalized metrics; perceptually optimized algorithms and methods; quality metrics and assessment tools; quasi-convexity and convexity; structural similarity (SSIM) index","Normalized metrics; Optimized algorithms; quality metrics and assessment tools; Quasi-convexity; Structural similarity; Mathematical models; Image processing; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; image enhancement; image subtraction; methodology; reproducibility; sensitivity and specificity; signal processing; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Signal Processing, Computer-Assisted; Subtraction Technique",Article,Scopus,2-s2.0-84859013796
"Hassanzadeh E., Zarghami M., Hassanzadeh Y.","Determining the Main Factors in Declining the Urmia Lake Level by Using System Dynamics Modeling",2012,"Water Resources Management",85,10.1007/s11269-011-9909-8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-82955233090&doi=10.1007%2fs11269-011-9909-8&partnerID=40&md5=6170f153314e2a02cdb42a17a6e59ca3","Urmia Lake in Iran is the second largest saline lake in the world. This ecosystem is the home for different species. Due to various socio-economical and ecological criteria, Urmia Lake has important role in the Northwestern part of the country but it has faced many problems in recent years. Because of droughts, overuse of surface water resources and dam constructions, water level has decreased in such a way that one quarter of the lake has changed to saline area in the last 10 years. The purpose of this research is to determine the main factors which reduce the lake's water level. To this end, a simulation model, based on system dynamics method, is developed for the Urmia Lake basin to estimate the lake's level. After successful verification of the model, results show that (among the proposed factors) changes in inflows due to the climate change and overuse of surface water resources is the main factor for 65% of the effect, constructing four dams is responsible for 25% of the problem, and less precipitation on lake has 10% effect on decreasing the lake's level in the recent years. In the future, the model also can be used by managers as a decision support system to find the effects of building new dams or other infrastructures. © 2011 Springer Science+Business Media B.V.","Climate change; Integrated lake management; Simulation; Surface water overuse; System dynamics","Dam construction; Integrated lake management; Lake levels; Saline lake; Simulation; Simulation model; System Dynamics; System dynamics modeling; Artificial intelligence; Climate change; Climate models; Computer simulation; Decision support systems; Ecology; Hydraulic structures; Lakes; Saline water; System theory; Water levels; Surface water resources; climate change; decision support system; lake water; precipitation intensity; water level; water resource; Iran; Lake Urmia",Article,Scopus,2-s2.0-82955233090
"El-Abd M.","Performance assessment of foraging algorithms vs. evolutionary algorithms",2012,"Information Sciences",85,10.1016/j.ins.2011.09.005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80055056236&doi=10.1016%2fj.ins.2011.09.005&partnerID=40&md5=17e2d2d2dcd799de1ce2263cb26ca40d","The class of foraging algorithms is a relatively new field based on mimicking the foraging behavior of animals, insects, birds or fish in order to develop efficient optimization algorithms. The artificial bee colony (ABC), the bees algorithm (BA), ant colony optimization (ACO), and bacterial foraging optimization algorithms (BFOA) are examples of this class to name a few. This work provides a complete performance assessment of the four mentioned algorithms in comparison to the widely known differential evolution (DE), genetic algorithms (GAs), harmony search (HS), and particle swarm optimization (PSO) algorithms when applied to the problem of unconstrained nonlinear continuous function optimization. To the best of our knowledge, most of the work conducted so far using foraging algorithms has been tested on classical functions. This work provides the comparison using the well-known CEC05 benchmark functions based on the solution reached, the success rate, and the performance rate. © 2011 Elsevier Inc. All rights reserved.","Evolutionary algorithms; Evolutionary optimization; Foraging algorithms; Performance comparison","Ant-colony optimization; Artificial bee colonies; Bacterial foraging optimization algorithms; Bees algorithms; Benchmark functions; Continuous function optimization; Differential evolution; Evolutionary optimization; Foraging algorithms; Foraging behaviors; Harmony search; Optimization algorithms; Particle swarm optimization algorithm; Performance assessment; Performance comparison; Artificial intelligence; Biology; Constrained optimization; Fish; Particle swarm optimization (PSO); Algorithms",Article,Scopus,2-s2.0-80055056236
"Kiran M.S., Özceylan E., Gündüz M., Paksoy T.","A novel hybrid approach based on Particle Swarm Optimization and Ant Colony Algorithm to forecast energy demand of Turkey",2012,"Energy Conversion and Management",85,10.1016/j.enconman.2011.08.004,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053171957&doi=10.1016%2fj.enconman.2011.08.004&partnerID=40&md5=6238d4e6b0684cd50c3140ca368d02d4","This paper proposes a new hybrid method (HAP) for estimating energy demand of Turkey using Particle Swarm Optimization (PSO) and Ant Colony Optimization (ACO). Proposed energy demand model (HAPE) is the first model which integrates two mentioned meta-heuristic techniques. While, PSO, developed for solving continuous optimization problems, is a population based stochastic technique; ACO, simulating behaviors between nest and food source of real ants, is generally used for discrete optimizations. Hybrid method based PSO and ACO is developed to estimate energy demand using gross domestic product (GDP), population, import and export. HAPE is developed in two forms which are linear (HAPEL) and quadratic (HAPEQ). The future energy demand is estimated under different scenarios. In order to show the accuracy of the algorithm, a comparison is made with ACO and PSO which are developed for the same problem. According to obtained results, relative estimation errors of the HAPE model are the lowest of them and quadratic form (HAPEQ) provides better-fit solutions due to fluctuations of the socio-economic indicators. © 2011 Elsevier Ltd. All rights reserved.","Ant Colony Optimization; Energy demand; Estimation; Hybrid meta-heuristic; Particle Swarm Optimization; Turkey","Ant colony algorithms; Ant-colony optimization; Continuous optimization problems; Discrete optimization; Energy demand models; Energy demands; Food sources; Gross domestic products; Hybrid approach; Hybrid meta-heuristic; Hybrid method; Meta-heuristic techniques; Quadratic form; Relative estimation; Socio-economics; Stochastic techniques; Turkey; Algorithms; Artificial intelligence; Economics; Energy management; Estimation; Heuristic methods; Number theory; Particle swarm optimization (PSO)",Article,Scopus,2-s2.0-80053171957
"Di lena P., Nagata K., Baldi P.","Deep architectures for protein contact map prediction",2012,"Bioinformatics",84,10.1093/bioinformatics/bts475,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867316765&doi=10.1093%2fbioinformatics%2fbts475&partnerID=40&md5=95584715b75968bb2d2990369ca58491","Motivation: Residue-residue contact prediction is important for protein structure prediction and other applications. However, the accuracy of current contact predictors often barely exceeds 20% on long-range contacts, falling short of the level required for ab initio structure prediction.Results: Here, we develop a novel machine learning approach for contact map prediction using three steps of increasing resolution. First, we use 2D recursive neural networks to predict coarse contacts and orientations between secondary structure elements. Second, we use an energy-based method to align secondary structure elements and predict contact probabilities between residues in contacting alpha-helices or strands. Third, we use a deep neural network architecture to organize and progressively refine the prediction of contacts, integrating information over both space and time. We train the architecture on a large set of non-redundant proteins and test it on a large set of non-homologous domains, as well as on the set of protein domains used for contact prediction in the two most recent CASP8 and CASP9 experiments. For long-range contacts, the accuracy of the new CMAPpro predictor is close to 30%, a significant increase over existing approaches. © The Author 2012. Published by Oxford University Press. All rights reserved.",,"protein; algorithm; article; artificial intelligence; artificial neural network; biology; chemistry; methodology; protein secondary structure; protein tertiary structure; Algorithms; Artificial Intelligence; Computational Biology; Neural Networks (Computer); Protein Structure, Secondary; Protein Structure, Tertiary; Proteins",Article,Scopus,2-s2.0-84867316765
"Mohemmed A., Schliebs S., Matsuda S., Kasabov N.","Span: Spike pattern association neuron for learning spatio-temporal spike patterns",2012,"International Journal of Neural Systems",84,10.1142/S0129065712500128,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864362117&doi=10.1142%2fS0129065712500128&partnerID=40&md5=9cddebffb7ae228fecaac42360130162","Spiking Neural Networks (SNN) were shown to be suitable tools for the processing of spatio-temporal information. However, due to their inherent complexity, the formulation of efficient supervised learning algorithms for SNN is difficult and remains an important problem in the research area. This article presents SPAN - a spiking neuron that is able to learn associations of arbitrary spike trains in a supervised fashion allowing the processing of spatio-temporal information encoded in the precise timing of spikes. The idea of the proposed algorithm is to transform spike trains during the learning phase into analog signals so that common mathematical operations can be performed on them. Using this conversion, it is possible to apply the well-known WidrowHoff rule directly to the transformed spike trains in order to adjust the synaptic weights and to achieve a desired input/output spike behavior of the neuron. In the presented experimental analysis, the proposed learning algorithm is evaluated regarding its learning capabilities, its memory capacity, its robustness to noisy stimuli and its classification performance. Differences and similarities of SPAN regarding two related algorithms, ReSuMe and Chronotron, are discussed. © 2012 World Scientific Publishing Company.","learning; spike pattern association; Spiking Neural Network; temporal coding","Analog signals; Classification performance; Experimental analysis; Inherent complexity; Input/output; learning; Learning capabilities; Learning phase; Mathematical operations; Memory capacity; Spatio-temporal; Spatiotemporal information; Spike patterns; Spike train; Spiking neural networks; Spiking neuron; Synaptic weight; Temporal coding; Intelligent agents; Mathematical transformations; Neural networks; Neurons; Learning algorithms; action potential; algorithm; animal; article; artificial intelligence; biological model; learning; nerve cell; nerve cell network; physiology; time; Action Potentials; Algorithms; Animals; Artificial Intelligence; Association Learning; Models, Neurological; Nerve Net; Neurons; Time Factors",Article,Scopus,2-s2.0-84864362117
"Hayat M., Khan A.","Discriminating outer membrane proteins with fuzzy K-nearest neighbor algorithms based on the general form of Chou's PseAAC",2012,"Protein and Peptide Letters",84,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858833657&partnerID=40&md5=5ea6eaf141eb06ba4644fa71a5a3b4e4","Outer membrane proteins (OMPs) play important roles in cell biology. In addition, OMPs are targeted by multiple drugs. The identification of OMPs from genomic sequences and successful prediction of their secondary and tertiary structures is a challenging task due to short membrane-spanning regions with high variation in properties. Therefore, an effective and accurate silico method for discrimination of OMPs from their primary sequences is needed. In this paper, we have analyzed the performance of various machine learning mechanisms for discriminating OMPs such as: Genetic Programming, K-nearest Neighbor, and Fuzzy K-nearest Neighbor (Fuzzy K-NN) in conjunction with discrete methods such as: Amino acid composition, Amphiphilic Pseudo amino acid composition, Split amino acid composition (SAAC), and hybrid versions of these methods. The performance of the classifiers is evaluated by two datasets using 5-fold crossvalidation. After the simulation, we have observed that Fuzzy K-NN using SAAC based-features makes it quite effective in discriminating OMPs. Fuzzy K-NN achieves the highest success rates of 99.00% accuracy for discriminating OMPs from non-OMPs and 98.77% and 98.28% accuracies from α-helix membrane and globular proteins, respectively on dataset1. While on dataset2, Fuzzy K-NN achieves 99.55%, 99.90%, and 99.81% accuracies for discriminating OMPs from non-OMPs, α-helix membrane, and globular proteins, respectively. It is observed that the classification performance of our proposed method is satisfactory and is better than the existing methods. Thus, it might be an effective tool for high throughput innovation of OMPs. © 2012 Bentham Science Publishers.","AAC; Am-PseAAC; Fuzzy K-NN; Genetic programming; K-nearest neighbor; SAAC","globular protein; outer membrane protein; accuracy; alpha helix; amino acid composition; amino acid sequence; article; classification algorithm; computer model; computer simulation; controlled study; data analysis; fuzzy system; intermethod comparison; k nearest neighbor; mathematical computing; mathematical model; prediction; process development; reliability; sensitivity and specificity; structure activity relation; validation process; Algorithms; Artificial Intelligence; Computational Biology; Databases, Protein; Membrane Proteins; Sequence Analysis, Protein",Article,Scopus,2-s2.0-84858833657
"Kang L.-W., Lin C.-W., Fu Y.-H.","Automatic single-image-based rain streaks removal via image decomposition",2012,"IEEE Transactions on Image Processing",84,10.1109/TIP.2011.2179057,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859079535&doi=10.1109%2fTIP.2011.2179057&partnerID=40&md5=0ac9b7bb3ee0821d9f997dbb20e85e03","Rain removal from a video is a challenging problem and has been recently investigated extensively. Nevertheless, the problem of rain removal from a single image was rarely studied in the literature, where no temporal information among successive images can be exploited, making the problem very challenging. In this paper, we propose a single-image-based rain removal framework via properly formulating rain removal as an image decomposition problem based on morphological component analysis. Instead of directly applying a conventional image decomposition technique, the proposed method first decomposes an image into the low- and high-frequency (HF) parts using a bilateral filter. The HF part is then decomposed into a rain component and a nonrain component by performing dictionary learning and sparse coding. As a result, the rain component can be successfully removed from the image while preserving most original image details. Experimental results demonstrate the efficacy of the proposed algorithm. © 2011 IEEE.","Dictionary learning; image decomposition; morphological component analysis (MCA); rain removal; sparse representation","Bilateral filters; Component analysis; Dictionary learning; High frequency HF; Image decomposition; Original images; Single images; Sparse coding; Sparse representation; Temporal information; Algorithms; Rain; rain; algorithm; article; artifact; artificial intelligence; automated pattern recognition; computer assisted diagnosis; image enhancement; image subtraction; information retrieval; methodology; photography; reproducibility; sensitivity and specificity; Algorithms; Artifacts; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Information Storage and Retrieval; Pattern Recognition, Automated; Photography; Rain; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-84859079535
"Prest A., Schmid C., Ferrari V.","Weakly supervised learning of interactions between humans and objects",2012,"IEEE Transactions on Pattern Analysis and Machine Intelligence",84,10.1109/TPAMI.2011.158,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856142160&doi=10.1109%2fTPAMI.2011.158&partnerID=40&md5=210d8d26d598e1d9668376e4c8bdf4d4","We introduce a weakly supervised approach for learning human actions modeled as interactions between humans and objects. Our approach is human-centric: We first localize a human in the image and then determine the object relevant for the action and its spatial relation with the human. The model is learned automatically from a set of still images annotated only with the action label. Our approach relies on a human detector to initialize the model learning. For robustness to various degrees of visibility, we build a detector that learns to combine a set of existing part detectors. Starting from humans detected in a set of images depicting the action, our approach determines the action object and its spatial relation to the human. Its final output is a probabilistic model of the human-object interaction, i.e., the spatial relation between the human and the object. We present an extensive experimental evaluation on the sports action data set from [1], the PASCAL Action 2010 data set [2], and a new human-object interaction data set. © 2012 IEEE.","Action recognition; object detection; weakly supervised learning","Action recognition; Data sets; Experimental evaluation; Human actions; Human-centric; Human-object interaction; Model learning; Object Detection; Probabilistic models; Spatial relations; Still images; weakly supervised learning; Detectors; Image recognition; Motion estimation; Supervised learning; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; human; methodology; statistical model; Algorithms; Artificial Intelligence; Humans; Image Interpretation, Computer-Assisted; Models, Statistical; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84856142160
"Bora T.C., Coelho L.D.S., Lebensztajn L.","Bat-inspired optimization approach for the brushless DC wheel motor problem",2012,"IEEE Transactions on Magnetics",83,10.1109/TMAG.2011.2176108,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856405561&doi=10.1109%2fTMAG.2011.2176108&partnerID=40&md5=cf6dc31b5c32e9b010e3ca56e3a5ebbf","This paper presents a metaheuristic algorithm inspired in evolutionary computation and swarm intelligence concepts and fundamentals of echolocation of micro bats. The aim is to optimize the mono and multiobjective optimization problems related to the brushless DC wheel motor problems, which has 5 design parameters and 6 constraints for the mono-objective problem and 2 objectives, 5 design parameters, and 5 constraints for multiobjective version. Furthermore, results are compared with other optimization approaches proposed in the recent literature, showing the feasibility of this newly introduced technique to high nonlinear problems in electromagnetics. © 2012 IEEE.","Brushless machines; evolutionary computation; optimization","5-design; Brushless DC wheel motor; Brushless machines; Electromagnetics; Evolutionary computations; Meta heuristic algorithm; Multi objective; Multi-objective optimization problem; Nonlinear problems; Optimization approach; Swarm Intelligence; Artificial intelligence; Cellular automata; Mammals; Optimization; Wheels; Multiobjective optimization",Conference Paper,Scopus,2-s2.0-84856405561
"Cambria E., Havasi C., Hussain A.","SenticNet 2: A semantic and affective resource for opinion mining and sentiment analysis",2012,"Proceedings of the 25th International Florida Artificial Intelligence Research Society Conference, FLAIRS-25",82,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864992296&partnerID=40&md5=6f08261ca48797a055e5531c7c217035","Web 2.0 has changed the ways people communicate, collaborate, and express their opinions and sentiments. But despite social data on the Web being perfectly suitable for human consumption, they remain hardly accessible to machines. To bridge the cognitive and affective gap between word-level natural language data and the concept-level sentiments conveyed by them, we developed SenticNet 2, a publicly available semantic and affective resource for opinion mining and sentiment analysis. SenticNet 2 is built by means of sentic computing, a new paradigm that exploits both AI and Semantic Web techniques to better recognize, interpret, and process natural language opinions. By providing the semantics and sentics (that is, the cognitive and affective information) associated with over 14,000 concepts, SenticNet 2 represents one of the most comprehensive semantic resources for the development of affect-sensitive applications in fields such as social data mining, multimodal affective HCI, and social media marketing. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Human consumption; In-field; Multi-modal; Natural languages; Opinion mining; Semantic resources; Semantic-Web techniques; Sentiment analysis; Social data mining; Social media; Web 2.0; Artificial intelligence; Semantics; Data mining",Conference Paper,Scopus,2-s2.0-84864992296
"Liu Q., Chen E., Xiong H., Ding C.H.Q., Chen J.","Enhancing collaborative filtering by user interest expansion via personalized ranking",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",82,10.1109/TSMCB.2011.2163711,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856254249&doi=10.1109%2fTSMCB.2011.2163711&partnerID=40&md5=77ab69e158e78ed6ca73c2277ebece11","Recommender systems suggest a few items from many possible choices to the users by understanding their past behaviors. In these systems, the user behaviors are influenced by the hidden interests of the users. Learning to leverage the information about user interests is often critical for making better recommendations. However, existing collaborative-filtering-based recommender systems are usually focused on exploiting the information about the user's interaction with the systems; the information about latent user interests is largely underexplored. To that end, inspired by the topic models, in this paper, we propose a novel collaborative-filtering-based recommender system by user interest expansion via personalized ranking, named iExpand. The goal is to build an item-oriented model-based collaborative-filtering framework. The iExpand method introduces a three-layer, user-interests-item, representation scheme, which leads to more accurate ranking recommendation results with less computation cost and helps the understanding of the interactions among users, items, and user interests. Moreover, iExpand strategically deals with many issues that exist in traditional collaborative-filtering approaches, such as the overspecialization problem and the cold-start problem. Finally, we evaluate iExpand on three benchmark data sets, and experimental results show that iExpand can lead to better ranking performance than state-of-The-art methods with a significant margin. © 2011 IEEE.","Collaborative filtering; latent Dirichlet allocation (LDA); personalized ranking; recommender systems; topic model","Benchmark data; Cold start problems; Collaborative filtering; Computation costs; latent Dirichlet allocation (LDA); Overspecialization; personalized ranking; Ranking performance; Representation schemes; State-of-the-art methods; Three-layer; Topic model; User behaviors; User interests; Benchmarking; Recommender systems; Statistics; Behavioral research; algorithm; article; artificial intelligence; automated pattern recognition; computer simulation; decision support system; methodology; signal processing; theoretical model; Algorithms; Artificial Intelligence; Computer Simulation; Decision Support Techniques; Models, Theoretical; Pattern Recognition, Automated; Signal Processing, Computer-Assisted",Article,Scopus,2-s2.0-84856254249
"Carlson T., Demiris Y.","Collaborative control for a robotic wheelchair: Evaluation of performance, attention, and workload",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",82,10.1109/TSMCB.2011.2181833,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861200566&doi=10.1109%2fTSMCB.2011.2181833&partnerID=40&md5=9245619052930af74d6b2321a1f7aea6","Powered wheelchair users often struggle to drive safely and effectively and, in more critical cases, can only get around when accompanied by an assistant. To address these issues, we propose a collaborative control mechanism that assists users as and when they require help. The system uses a multiple-hypothesis method to predict the driver's intentions and, if necessary, adjusts the control signals to achieve the desired goal safely. The main emphasis of this paper is on a comprehensive evaluation, where we not only look at the system performance but also, perhaps more importantly, characterize the user performance in an experiment that combines eye tracking with a secondary task. Without assistance, participants experienced multiple collisions while driving around the predefined route. Conversely, when they were assisted by the collaborative controller, not only did they drive more safely but also they were able to pay less attention to their driving, resulting in a reduced cognitive workload. We discuss the importance of these results and their implications for other applications of shared control, such as brain-machine interfaces, where it could be used to compensate for both the low frequency and the low resolution of the user input. © 2012 IEEE.","Collision avoidance; human factors; human robot interaction; intelligent robots; rehabilitation robotics; wheelchairs","Brain machine interface; Cognitive workloads; Collaborative control; Comprehensive evaluation; Control signal; Critical case; Eye-tracking; Low frequency; Low resolution; Multiple collisions; Other applications; Powered wheel chairs; Rehabilitation robotics; Robotic wheelchairs; Shared control; System use; User input; User performance; Collision avoidance; Human engineering; Human robot interaction; Intelligent robots; Wheelchairs; Robotics; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted therapy; human; instrumentation; male; methodology; physiology; psychophysiology; robotics; task performance; wheelchair; workload; Algorithms; Artificial Intelligence; Biofeedback, Psychology; Humans; Male; Pattern Recognition, Automated; Robotics; Task Performance and Analysis; Therapy, Computer-Assisted; Wheelchairs; Workload",Article,Scopus,2-s2.0-84861200566
"Matsuda H., Mizumura S., Nemoto K., Yamashita F., Imabayashi E., Sato N., Asada T.","Automatic voxel-based morphometry of structural MRI by SPM8 plus diffeomorphic anatomic registration through exponentiated lie algebra improves the diagnosis of probable Alzheimer disease",2012,"American Journal of Neuroradiology",81,10.3174/ajnr.A2935,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862536219&doi=10.3174%2fajnr.A2935&partnerID=40&md5=672402a538127fc26a1d9c60293477f0","BACKGROUND AND PURPOSE: The necessity for structural MRI is greater than ever to both diagnose AD in its early stage and objectively evaluate its progression. We propose a new VBM-based software program for automatic detection of early specific atrophy in AD. MATERIALS AND METHODS: A target VOI was determined by group comparison of 30 patients with very mild AD and 40 age-matched healthy controls by using SPM. Then this target VOI was incorporated into a newly developed automated software program independently running on a Windows PC for VBM by using SPM8 plus DARTEL. ROC analysis was performed for discrimination of 116 other patients with AD with very mild stage (n = 45), mild stage (n = 30) and moderate-to-advanced stages (n = 41) from 40 other age-matched healthy controls by using a z score map in the target VOI. RESULTS: Medial temporal structures involving the entire region of the entorhinal cortex, hippocampus, and amygdala showed significant atrophy in the patients with very mild AD and were determined as a target VOI. When we used the severity score of atrophy in this target VOI, 91.6%, 95.8%, and 98.2% accuracies were obtained in the very mild AD, mild AD, and moderate-to-severe AD groups, respectively. In the very mild AD group, a high specificity of 97.5% with a sensitivity of 86.4% was obtained, and age at onset of AD did not influence this accuracy. CONCLUSIONS: This software program with application of SPM8 plus DARTEL to VBM provides a high performance for AD diagnosis by using MRI.",,"adult; aged; Alzheimer disease; amygdaloid nucleus; article; automation; brain atrophy; brain region; brain size; computer program; controlled study; diagnostic accuracy; disease severity; entorhinal cortex; female; follow up; gray matter; hippocampus; human; major clinical study; male; mathematics; nuclear magnetic resonance imaging; retrospective study; sensitivity and specificity; voxel based morphometry; Aged; Aged, 80 and over; Algorithms; Alzheimer Disease; Artificial Intelligence; Brain; Female; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Male; Middle Aged; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Software; Software Validation",Article,Scopus,2-s2.0-84862536219
"Song Y., Crowcroft J., Zhang J.","Automatic epileptic seizure detection in EEGs based on optimized sample entropy and extreme learning machine",2012,"Journal of Neuroscience Methods",79,10.1016/j.jneumeth.2012.07.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865313417&doi=10.1016%2fj.jneumeth.2012.07.003&partnerID=40&md5=90da3391f6383e43bbf367a5931ea453","Epilepsy is one of the most common neurological disorders - approximately one in every 100 people worldwide are suffering from it. The electroencephalogram (EEG) is the most common source of information used to monitor, diagnose and manage neurological disorders related to epilepsy. Large amounts of data are produced by EEG monitoring devices, and analysis by visual inspection of long recordings of EEG in order to find traces of epilepsy is not routinely possible. Therefore, automated detection of epilepsy has been a goal of many researchers for a long time. This paper presents a novel method for automatic epileptic seizure detection. An optimized sample entropy (O-SampEn) algorithm is proposed and combined with extreme learning machine (ELM) to identify the EEG signals regarding the existence of seizure or not. To the knowledge of the authors, there exists no similar work in the literature. A public dataset was utilized for evaluating the proposed method. Results show that the proposed epilepsy detection approach achieves not only high detection accuracy but also a very fast computation speed, which demonstrates its huge potential for the real-time detection of epileptic seizures. © 2012 Elsevier B.V.","Electroencephalogram (EEG); Epileptic seizure detection; Extreme learning machine (ELM); Optimized sample entropy (O-SampEn)","article; automation; clinical article; controlled study; diagnostic accuracy; electroencephalogram; entropy; epilepsy; human; machine learning; priority journal; receiver operating characteristic; seizure; Algorithms; Artificial Intelligence; Automatic Data Processing; Brain Mapping; Brain Waves; Electroencephalography; Entropy; Epilepsy; Humans",Article,Scopus,2-s2.0-84865313417
"Xiang S., Nie F., Meng G., Pan C., Zhang C.","Discriminative least squares regression for multiclass classification and feature selection",2012,"IEEE Transactions on Neural Networks and Learning Systems",78,10.1109/TNNLS.2012.2212721,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875878163&doi=10.1109%2fTNNLS.2012.2212721&partnerID=40&md5=c528784474b69efac26b25dd2ccefc86","This paper presents a framework of discriminative least squares regression (LSR) for multiclass classification and feature selection. The core idea is to enlarge the distance between different classes under the conceptual framework of LSR. First, a technique called ε-dragging is introduced to force the regression targets of different classes moving along opposite directions such that the distances between classes can be enlarged. Then, the ε-draggings are integrated into the LSR model for multiclass classification. Our learning framework, referred to as discriminative LSR, has a compact model form, where there is no need to train two-class machines that are independent of each other. With its compact form, this model can be naturally extended for feature selection. This goal is achieved in terms of L2,1 norm of matrix, generating a sparse learning model for feature selection. The model for multiclass classification and its extension for feature selection are finally solved elegantly and efficiently. Experimental evaluation over a range of benchmark datasets indicates the validity of our method. © 2012 IEEE.","Feature selection; least squares regression; multiclass classification; sparse learning","Benchmark datasets; Conceptual frameworks; Different class; Experimental evaluation; Learning frameworks; Least squares regression; Multi-class classification; sparse learning; Artificial intelligence; Computer networks; Feature extraction; Regression analysis",Article,Scopus,2-s2.0-84875878163
"Hatamlou A., Abdullah S., Nezamabadi-Pour H.","A combined approach for clustering based on K-means and gravitational search algorithms",2012,"Swarm and Evolutionary Computation",78,10.1016/j.swevo.2012.02.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864614741&doi=10.1016%2fj.swevo.2012.02.003&partnerID=40&md5=04e943ec0d945ec1050245d048d9e1ed","Clustering is an attractive and important task in data mining that is used in many applications. Clustering refers to grouping together data objects so that objects within a cluster are similar to one another, while objects in different clusters are dissimilar. K-means is a simple and efficient algorithm that is widely used for data clustering. However, its performance depends on the initial state of centroids and may trap in local optima. The gravitational search algorithm (GSA) is one effective method for searching problem space to find a near optimal solution. In this paper, we present a hybrid data clustering algorithm based on GSA and k-means (GSA-KM), which uses the advantages of both algorithms. The GSA-KM algorithm helps the k-means algorithm to escape from local optima and also increases the convergence speed of the GSA algorithm. We compared the performance of GSA-KM with other well-known algorithms, including k-means, genetic algorithm (GA), simulated annealing (SA), ant colony optimization (ACO), honey bee mating optimization (HBMO), particle swarm optimization (PSO) and gravitational search algorithm (GSA). Five real and standard datasets from the UCI repository have been used to demonstrate the results of the algorithms. The experimental results are encouraging in terms of the quality of the solutions and the convergence speed of the proposed algorithm. © 2012 Elsevier B.V. All rights reserved.","Clustering; Gravitational search algorithm; K-means","Ant Colony Optimization (ACO); Clustering; Convergence speed; Data clustering; Data clustering algorithm; Data objects; Data sets; Gravitational search algorithms; Honey-bee mating optimization; Initial state; K-means; k-Means algorithm; Local optima; Near-optimal solutions; Problem space; Simple and efficient algorithms; UCI repository; Artificial intelligence; Genetic algorithms; Learning algorithms; Particle swarm optimization (PSO); Simulated annealing; Clustering algorithms",Article,Scopus,2-s2.0-84864614741
"Arteta C., Lempitsky V., Noble J.A., Zisserman A.","Learning to detect cells using non-overlapping extremal regions",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",78,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872579310&partnerID=40&md5=d5bb734a5b9409e2839e7901e8858643","Cell detection in microscopy images is an important step in the automation of cell based-experiments. We propose a machine learning-based cell detection method applicable to different modalities. The method consists of three steps: first, a set of candidate cell-like regions is identified. Then, each candidate region is evaluated using a statistical model of the cell appearance. Finally, dynamic programming picks a set of non-overlapping regions that match the model. The cell model requires few images with simple dot annotation for training and can be learned within a structured SVM framework. In the reported experiments, state-of-the-art cell detection accuracy is achieved for H&E-stained histology, fluorescence, and phase-contrast images. © Springer-Verlag Berlin Heidelberg 2012.",,"Cells; Dynamic programming; Education; Medical computing; Medical imaging; Cell detection; Cell-based; Microscopy images; Overlapping regions; Phase-contrast image; State of the art; Statistical modeling; Structured SVM; Cytology; algorithm; article; artificial intelligence; automated pattern recognition; cell size; computer program; computer simulation; HeLa cell; human; image processing; methodology; microscopy; phase contrast microscopy; reproducibility; statistical model; support vector machine; Algorithms; Artificial Intelligence; Cell Size; Computer Simulation; HeLa Cells; Humans; Image Processing, Computer-Assisted; Microscopy; Microscopy, Phase-Contrast; Models, Statistical; Pattern Recognition, Automated; Reproducibility of Results; Software; Support Vector Machines",Conference Paper,Scopus,2-s2.0-84872579310
"Weston J., Ratle F., Mobahi H., Collobert R.","Deep learning via semi-supervised embedding",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",77,10.1007/978-3-642-35289-8-34,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872553130&doi=10.1007%2f978-3-642-35289-8-34&partnerID=40&md5=a5a77f1376ad3db0b8ba476b086f972a","We show how nonlinear semi-supervised embedding algorithms popular for use with ""shallow"" learning techniques such as kernel methods can be easily applied to deep multi-layer architectures, either as a regularizer at the output layer, or on each layer of the architecture. Compared to standard supervised backpropagation this can give significant gains. This trick provides a simple alternative to existing approaches to semi-supervised deep learning whilst yielding competitive error rates compared to those methods, and existing shallow semi-supervised techniques. © Springer-Verlag Berlin Heidelberg 2012.",,"Deep learning; Embedding algorithms; Error rate; Kernel methods; Learning techniques; Output layer; Regularizer; Semi-supervised; Artificial intelligence",Article,Scopus,2-s2.0-84872553130
"Parson O., Ghosh S., Weal M., Rogers A.","Non-intrusive load monitoring using prior models of general appliance types",2012,"Proceedings of the National Conference on Artificial Intelligence",77,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868292476&partnerID=40&md5=67549e8e6b126b1c02604fe300fa19ac","Non-intrusive appliance load monitoring is the process of disaggregating a household's total electricity consumption into its contributing appliances. In this paper we propose an approach by which individual appliances can be iteratively separated from an aggregate load. Unlike existing approaches, our approach does not require training data to be collected by sub-metering individual appliances, nor does it assume complete knowledge of the appliances present in the household. Instead, we propose an approach in which prior models of general appliance types are tuned to specific appliance instances using only signatures extracted from the aggregate load. The tuned appliance models are then used to estimate each appliance's load, which is subsequently subtracted from the aggregate load. This process is applied iteratively until all appliances for which prior behaviour models are known have been disaggregated. We evaluate the accuracy of our approach using the REDD data set, and show the disaggregation performance when using our training approach is comparable to when sub-metered training data is used. We also present a deployment of our system as a live application and demonstrate the potential for personalised energy saving feedback. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Aggregate load; Behaviour models; Data sets; Disaggregation; Electricity-consumption; Non-intrusive appliance load monitoring; Nonintrusive load monitoring; Training data; Aggregates; Artificial intelligence; Electric load management; Iterative methods; Equipment",Conference Paper,Scopus,2-s2.0-84868292476
"Hu X., Mordohai P.","A quantitative evaluation of confidence measures for stereo vision",2012,"IEEE Transactions on Pattern Analysis and Machine Intelligence",77,10.1109/TPAMI.2012.46,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865584091&doi=10.1109%2fTPAMI.2012.46&partnerID=40&md5=720197be1fa1cea249fe3fdd37e01aff","We present an extensive evaluation of 17 confidence measures for stereo matching that compares the most widely used measures as well as several novel techniques proposed here. We begin by categorizing these methods according to which aspects of stereo cost estimation they take into account and then assess their strengths and weaknesses. The evaluation is conducted using a winner-take-all framework on binocular and multibaseline datasets with ground truth. It measures the capability of each confidence method to rank depth estimates according to their likelihood for being correct, to detect occluded pixels, and to generate low-error depth maps by selecting among multiple hypotheses for each pixel. Our work was motivated by the observation that such an evaluation is missing from the rapidly maturing stereo literature and that our findings would be helpful to researchers in binocular and multiview stereo. © 2012 IEEE.","3D reconstruction; confidence; correspondence; distinctiveness; Stereo vision","3D reconstruction; confidence; Confidence Measure; correspondence; Cost estimations; Data sets; Depth Map; distinctiveness; Ground truth; Multi-baseline; Multi-view stereo; Multiple hypothesis; Novel techniques; Quantitative evaluation; Stereo matching; Winner take alls; Binoculars; Pixels; Stereo vision; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; image enhancement; image subtraction; methodology; reproducibility; sensitivity and specificity; three dimensional imaging; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-84865584091
"Wang C., Zhang J., Wang L., Pu J., Yuan X.","Human identification using temporal information preserving gait template",2012,"IEEE Transactions on Pattern Analysis and Machine Intelligence",77,10.1109/TPAMI.2011.260,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866278142&doi=10.1109%2fTPAMI.2011.260&partnerID=40&md5=a783e0920f868a3b4a1ad43832aefb9d","Gait Energy Image (GEI) is an efficient template for human identification by gait. However, such a template loses temporal information in a gait sequence, which is critical to the performance of gait recognition. To address this issue, we develop a novel temporal template, named Chrono-Gait Image (CGI), in this paper. The proposed CGI template first extracts the contour in each gait frame, followed by encoding each of the gait contour images in the same gait sequence with a multichannel mapping function and compositing them to a single CGI. To make the templates robust to a complex surrounding environment, we also propose CGI-based real and synthetic temporal information preserving templates by using different gait periods and contour distortion techniques. Extensive experiments on three benchmark gait databases indicate that, compared with the recently published gait recognition approaches, our CGI-based temporal information preserving approach achieves competitive performance in gait recognition with robustness and efficiency. © 2012 IEEE.","biometric authentication; Computer vision; gait recognition; pattern recognition","Biometric authentication; Compositing; Contour image; Distortion techniques; Gait energy images; Gait period; Gait recognition; Gait sequences; Human identification; Mapping functions; Multi-channel; Surrounding environment; Temporal information; Temporal templates; Biometrics; Computer vision; Image coding; Information use; Pattern recognition; Gait analysis; algorithm; article; artificial intelligence; automated pattern recognition; biometry; computer assisted diagnosis; gait; human; image enhancement; image subtraction; information retrieval; methodology; physiology; reproducibility; sensitivity and specificity; three dimensional imaging; whole body imaging; Algorithms; Artificial Intelligence; Biometry; Gait; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Information Storage and Retrieval; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique; Whole Body Imaging",Article,Scopus,2-s2.0-84866278142
"Yi C., Tian Y.","Localizing text in scene images by boundary clustering, stroke segmentation, and string fragment classification",2012,"IEEE Transactions on Image Processing",77,10.1109/TIP.2012.2199327,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865421190&doi=10.1109%2fTIP.2012.2199327&partnerID=40&md5=98f98bb0ff739c68c547001820a22888","In this paper, we propose a novel framework to extract text regions from scene images with complex backgrounds and multiple text appearances. This framework consists of three main steps: boundary clustering (BC), stroke segmentation, and string fragment classification. In BC, we propose a new bigram-color-uniformity-based method to model both text and attachment surface, and cluster edge pixels based on color pairs and spatial positions into boundary layers. Then, stroke segmentation is performed at each boundary layer by color assignment to extract character candidates. We propose two algorithms to combine the structural analysis of text stroke with color assignment and filter out background interferences. Further, we design a robust string fragment classification based on Gabor-based text features. The features are obtained from feature maps of gradient, stroke distribution, and stroke width. The proposed framework of text localization is evaluated on scene images, born-digital images, broadcast video images, and images of handheld objects captured by blind persons. Experimental results on respective datasets demonstrate that the framework outperforms state-of-the-art localization algorithms. © 1992-2012 IEEE.","Bigram color uniformity; boundary clustering (BC); color assignment; Gabor-based text features; string fragment classification; stroke segmentation; text localization","boundary clustering (BC); Color assignments; Color uniformity; Fragment classification; Text feature; Text localization; Algorithms; Boundary layers; Color; Image segmentation; Character recognition; algorithm; article; artificial intelligence; automated pattern recognition; cluster analysis; human; image processing; methodology; patient; self help; videorecording; writing; Algorithms; Artificial Intelligence; Cluster Analysis; Humans; Image Processing, Computer-Assisted; Pattern Recognition, Automated; Self-Help Devices; Video Recording; Visually Impaired Persons; Writing",Article,Scopus,2-s2.0-84865421190
"Yu J., Liu D., Tao D., Seah H.S.","On combining multiple features for cartoon character retrieval and clip synthesis",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",77,10.1109/TSMCB.2012.2192108,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866523030&doi=10.1109%2fTSMCB.2012.2192108&partnerID=40&md5=076212da6cbd2c62ec48f03f7e942d32","How do we retrieve cartoon characters accurately? Or how to synthesize new cartoon clips smoothly and efficiently from the cartoon library? Both questions are important for animators and cartoon enthusiasts to design and create new cartoons by utilizing existing cartoon materials. The first key issue to answer those questions is to find a proper representation that describes the cartoon character effectively. In this paper, we consider multiple features from different views, i.e., color histogram, Hausdorff edge feature, and skeleton feature, to represent cartoon characters with different colors, shapes, and gestures. Each visual feature reflects a unique characteristic of a cartoon character, and they are complementary to each other for retrieval and synthesis. However, how to combine the three visual features is the second key issue of our application. By simply concatenating them into a long vector, it will end up with the so-called curse of dimensionality, let alone their heterogeneity embedded in different visual feature spaces. Here, we introduce a semisupervised multiview subspace learning (semi-MSL) algorithm, to encode different features in a unified space. Specifically, under the patch alignment framework, semi-MSL uses the discriminative information from labeled cartoon characters in the construction of local patches where the manifold structure revealed by unlabeled cartoon characters is utilized to capture the geometric distribution. The experimental evaluations based on both cartoon character retrieval and clip synthesis demonstrate the effectiveness of the proposed method for cartoon application. Moreover, additional results of content-based image retrieval on benchmark data suggest the generality of semi-MSL for other applications. © 1996-2012 IEEE.","Cartoon character; cartoon clip; multiview subspace learning; retrieval; synthesis; visual features","Cartoon characters; cartoon clip; retrieval; Subspace learning; Visual feature; Animation; Probability distributions; Synthesis (chemical); Vector spaces; Vectors; Benchmarking; algorithm; art; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; data base; information retrieval; methodology; Algorithms; Artificial Intelligence; Cartoons as Topic; Database Management Systems; Image Interpretation, Computer-Assisted; Information Storage and Retrieval; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84866523030
"Kashan M.H., Nahavandi N., Kashan A.H.","DisABC: A new artificial bee colony algorithm for binary optimization",2012,"Applied Soft Computing Journal",77,10.1016/j.asoc.2011.08.038,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-81155126173&doi=10.1016%2fj.asoc.2011.08.038&partnerID=40&md5=be1a578b622b7647c22728680ed283c4","Artificial bee colony (ABC) algorithm is one of the recently proposed swarm intelligence based algorithms for continuous optimization. Therefore it is not possible to use the original ABC algorithm directly to optimize binary structured problems. In this paper we introduce a new version of ABC, called DisABC, which is particularly designed for binary optimization. DisABC uses a new differential expression, which employs a measure of dissimilarity between binary vectors in place of the vector subtraction operator typically used in the original ABC algorithm. Such an expression helps to maintain the major characteristics of the original one and is respondent to the structure of binary optimization problems, too. Similar to original ABC algorithm, DisABC's differential expression works in continuous space while its consequence is used in a two-phase heuristic to construct a complete solution in binary space. Effectiveness of DisABC algorithm is tested on solving the uncapacitated facility location problem (UFLP). A set of 15 benchmark test problem instances of UFLP are adopted from OR-Library and solved by the proposed algorithm. Results are compared with two other state of the art binary optimization algorithms, i.e., binDE and PSO algorithms, in terms of three quality indices. Comparisons indicate that DisABC performs very well and can be regarded as a promising method for solving wide class of binary optimization problems. © 2011 Elsevier B.V. All rights reserved.","Artificial bee colony algorithm; Binary optimization; Dissimilarity measure of binary structures; Swarm intelligence; Uncapacitated facility location problem","Artificial bee colonies; Binary optimization; Dissimilarity measures; Swarm intelligence; Uncapacitated facility location problem; Artificial intelligence; Benchmarking; Cellular automata; Location; Particle swarm optimization (PSO); Structural optimization; Algorithms",Article,Scopus,2-s2.0-81155126173
"Fernando N., Hong Y., Viterbo E.","Flip-OFDM for unipolar communication systems",2012,"IEEE Transactions on Communications",76,10.1109/TCOMM.2012.082712.110812,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871644541&doi=10.1109%2fTCOMM.2012.082712.110812&partnerID=40&md5=98ccb218eb9e288065d850db2dd037a8","Unipolar communications systems can transmit information using only real and positive signals. This includes a variety of physical channels ranging from optical (fiber or free-space), to RF wireless using amplitude modulation with non-coherent reception, to baseband single wire communications. Unipolar OFDM techniques can efficiently compensate frequency selective channel distortion in unipolar communication systems. One of the leading example of unipolar OFDM is asymmetric clipped optical OFDM (ACO-OFDM) originally proposed for optical communications. Flip-OFDM is an alternative approach that was proposed in a patent, but its performance and full potentials have never been investigated in the literature. In this paper, we first compare Flip-OFDM and ACO-OFDM, and show that both techniques have the same performance but different complexities. In particular, Flip-OFDM offers 50% saving in hardware complexity at the receiver over ACO-OFDM. We then propose a new detection scheme, which enables to reduce the noise at the Flip-OFDM receiver by almost 3dB. The analytical performance of the noise filtering schemes is supported by the simulation results. © 2012 IEEE.","detection; non-coherent communications; OFDM; optical communications; unipolar baseband communications","Alternative approach; Analytical performance; Base bands; Baseband communication; Communications systems; Detection scheme; Frequency selective channel; Hardware complexity; Noise filtering; Non-coherent; Optical OFDM; Physical channels; Positive signals; RF wireless; Single wires; Artificial intelligence; Communication systems; Error detection; Optical communication; Optical fiber communication; Orthogonal frequency division multiplexing",Article,Scopus,2-s2.0-84871644541
"Liu M., Wu Y., Chen Y., Sun J., Zhao Z., Chen X.-W., Matheny M.E., Xu H.","Large-scale prediction of adverse drug reactions using chemical, biological, and phenotypic properties of drugs",2012,"Journal of the American Medical Informatics Association",76,10.1136/amiajnl-2011-000699,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863548955&doi=10.1136%2famiajnl-2011-000699&partnerID=40&md5=a56ca7c6023a85dcb54231def46c1532","Dementia with Lewy bodies was first recognized as a separate entity about 30 years ago. The prevalence varies from 0% to 5% in the general population, and this disease accounts for 0% to 30.5% of all dementia cases. Dementia with Lewy bodies is considered the second most common cause of degenerative dementia after Alzheimer's disease. The disease is characterized by alpha-synuclein immunoreactive protein deposits in both neurons and glial cells. The protein deposits are especially prominent in dopaminergic neurons, where they can be detected using conventional histological stains, such as hematoxylin and eosin, and are commonly referred to as Lewy bodies. The diagnosis of dementia with Lewy bodies is based on the presence of dementia as well as 2 of the following 3 core diagnostic features: 1) fluctuating cognition, 2) visual hallucinations, and 3) movement disorder. Diagnostic tests include laboratory data, structural and functional imaging, and electroencephalography. Differential diagnosis of dementia with Lewy bodies focuses on other later life dementia syndromes, other parkinsonian diseases (Parkinson's disease, progressive supranuclear palsy, corticobasal degeneration), and primary psychiatric illnesses. There is type 1b evidence to support treatment with cholinesterase inhibitors. Glutamatergic and dopaminergic therapies are used as well. Standard neuroleptics are contraindicated, and atypical agents should be used cautiously. Nonpharmacologic measures - therapeutic environment, psychological and social support, physical activity, behavioral management strategies, caregivers' education and support, and different services - could be suggested.",,"cerivastatin; protein; rofecoxib; adverse drug reaction; algorithm; article; Bayesian learning; biological activity; conceptual framework; controlled study; drug structure; drug surveillance program; drug targeting; heart infarction; human; intermethod comparison; k nearest neighbor; logistic regression analysis; machine learning; model; phenotype; prediction; random forest; rhabdomyolysis; support vector machine; validation study; Algorithms; Area Under Curve; Artificial Intelligence; Bayes Theorem; Drug Toxicity; Humans; Logistic Models; Pharmaceutical Preparations; ROC Curve; Support Vector Machines",Article,Scopus,2-s2.0-84863548955
"Riaño D., Real F., López-Vallverdú J.A., Campana F., Ercolani S., Mecocci P., Annicchiarico R., Caltagirone C.","An ontology-based personalization of health-care knowledge to support clinical decisions for chronically ill patients",2012,"Journal of Biomedical Informatics",75,10.1016/j.jbi.2011.12.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861847757&doi=10.1016%2fj.jbi.2011.12.008&partnerID=40&md5=9fab3c2e4a55e63b38c90ae1ac5e2e6d","Chronically ill patients are complex health care cases that require the coordinated interaction of multiple professionals. A correct intervention of these sort of patients entails the accurate analysis of the conditions of each concrete patient and the adaptation of evidence-based standard intervention plans to these conditions. There are some other clinical circumstances such as wrong diagnoses, unobserved comorbidities, missing information, unobserved related diseases or prevention, whose detection depends on the capacities of deduction of the professionals involved.In this paper, we introduce an ontology for the care of chronically ill patients and implement two personalization processes and a decision support tool. The first personalization process adapts the contents of the ontology to the particularities observed in the health-care record of a given concrete patient, automatically providing a personalized ontology containing only the clinical information that is relevant for health-care professionals to manage that patient. The second personalization process uses the personalized ontology of a patient to automatically transform intervention plans describing health-care general treatments into individual intervention plans. For comorbid patients, this process concludes with the semi-automatic integration of several individual plans into a single personalized plan. Finally, the ontology is also used as the knowledge base of a decision support tool that helps health-care professionals to detect anomalous circumstances such as wrong diagnoses, unobserved comorbidities, missing information, unobserved related diseases, or preventive actions.Seven health-care centers participating in the K4CARE project, together with the group SAGESA and the Local Health System in the town of Pollenza have served as the validation platform for these two processes and tool. Health-care professionals participating in the evaluation agree about the average quality 84% (5.9/7.0) and utility 90% (6.3/7.0) of the tools and also about the correct reasoning of the decision support tool, according to clinical standards. © 2012 Elsevier Inc..","Health care personalization; Medical decision support systems; Ontologies","Accurate analysis; Clinical decision; Clinical information; Coordinated interaction; Decision support tools; Health systems; Knowledge base; Medical decision support system; Missing information; Ontology-based; Personalizations; Semi-automatics; Artificial intelligence; Coordination reactions; Decision support systems; Diagnosis; Hospital data processing; Knowledge based systems; Ontology; Quality control; Patient treatment; article; chronic patient; clinical decision making; comorbidity; decision support system; diagnostic error; health care; health care personnel; human; individualization; knowledge; knowledge base; medical information system; patient care; priority journal; standard; Chronic Disease; Decision Support Systems, Clinical; Delivery of Health Care; Health Personnel; Humans; Individualized Medicine",Article,Scopus,2-s2.0-84861847757
"Wu Y., Cheng J., Wang J., Lu H., Wang J., Ling H., Blasch E., Bai L.","Real-time probabilistic covariance tracking with efficient model update",2012,"IEEE Transactions on Image Processing",75,10.1109/TIP.2011.2182521,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860208938&doi=10.1109%2fTIP.2011.2182521&partnerID=40&md5=882425dfab59fccedd173996c53a6658","The recently proposed covariance region descriptor has been proven robust and versatile for a modest computational cost. The covariance matrix enables efficient fusion of different types of features, where the spatial and statistical properties, as well as their correlation, are characterized. The similarity between two covariance descriptors is measured on Riemannian manifolds. Based on the same metric but with a probabilistic framework, we propose a novel tracking approach on Riemannian manifolds with a novel incremental covariance tensor learning (ICTL). To address the appearance variations, ICTL incrementally learns a low-dimensional covariance tensor representation and efficiently adapts online to appearance changes of the target with only O(1) computational complexity, resulting in a real-time performance. The covariance-based representation and the ICTL are then combined with the particle filter framework to allow better handling of background clutter, as well as the temporary occlusions. We test the proposed probabilistic ICTL tracker on numerous benchmark sequences involving different types of challenges including occlusions and variations in illumination, scale, and pose. The proposed approach demonstrates excellent real-time performance, both qualitatively and quantitatively, in comparison with several previously proposed trackers. © 1992-2012 IEEE.","Covariance descriptor; incremental learning; model update; particle filter; Riemannian manifolds; visual tracking","Covariance descriptor; Incremental learning; Model updates; Particle filter; Riemannian manifold; Visual Tracking; Covariance matrix; Nonlinear filtering; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; computer system; image enhancement; methodology; reproducibility; sensitivity and specificity; Algorithms; Artificial Intelligence; Computer Systems; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84860208938
"Tai F., Lin H.-T.","Multilabel classification with principal label space transformation",2012,"Neural Computation",74,10.1162/NECO_a_00320,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867116137&doi=10.1162%2fNECO_a_00320&partnerID=40&md5=c94e73fc20db8ab99e551298f3603a51","We consider a hypercube view to perceive the label space of multilabel classification problems geometrically. The view allows us not only to unify many existing multilabel classification approaches but also design a novel algorithm, principal label space transformation (PLST), that captures key correlations between labels before learning. The simple and efficient PLST relies on only singular value decomposition as the key step.We derive the theoretical guarantee of PLST and evaluate its empirical performance using real-world data sets. Experimental results demonstrate that PLST is faster than the traditional binary relevance approach and is superior to the modern compressive sensing approach in terms of both accuracy and efficiency. © 2012 Massachusetts Institute of Technology.",,"algorithm; article; artificial intelligence; automated pattern recognition; biology; classification; human; learning; statistical analysis; Algorithms; Artificial Intelligence; Classification; Computational Biology; Data Interpretation, Statistical; Humans; Learning; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84867116137
"Dagdougui H., Minciardi R., Ouammi A., Robba M., Sacile R.","Modeling and optimization of a hybrid system for the energy supply of a ""green"" building",2012,"Energy Conversion and Management",74,10.1016/j.enconman.2012.05.017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866528382&doi=10.1016%2fj.enconman.2012.05.017&partnerID=40&md5=a2d9ef1da2086bf0253570bdc70a45d6","Renewable energy sources (RES) are an ""indigenous"" environmental option, economically competitive with conventional power generation where good wind and solar resources are available. Hybrid systems can help in improving the economic and environmental sustainability of renewable energy systems to fulfill the energy demand. The aim of this paper is to present a dynamic model able to integrate different RES and one storage device to feed a ""Green"" building for its thermal and electrical energy needs in a sustainable way. The system model is embedded in a dynamic decision model and is used to optimize a quite complex hybrid system connected to the grid which can exploit different renewable energy sources. A Model Predictive Control (MPC) is adopted to find the optimal solution. The optimization model has been applied to a case study where electric energy is also used to pump water for domestic use. Optimal results are reported for two main cases: the presence/absence of the energy storage system. © 2012 Elsevier Ltd. All rights reserved.","Decision support systems; Green building; Optimization; Renewable energy systems; Smart grid","Conventional power; Domestic use; Dynamic decision models; Electric energies; Electrical energy; Energy demands; Energy storage systems; Energy supplies; Environmental sustainability; Green buildings; Modeling and optimization; Optimal results; Optimal solutions; Optimization models; Presence/absence; Renewable energy source; Renewable energy systems; Smart grid; Solar resources; System models; Artificial intelligence; Decision support systems; Energy management; Hybrid systems; Model predictive control; Natural resources; Optimization; Predictive control systems; Virtual storage; Renewable energy resources",Conference Paper,Scopus,2-s2.0-84866528382
"Wang S.-J., Yang J., Sun M.-F., Peng X.-J., Sun M.-M., Zhou C.-G.","Sparse tensor discriminant color space for face verification",2012,"IEEE Transactions on Neural Networks and Learning Systems",74,10.1109/TNNLS.2012.2191620,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866609797&doi=10.1109%2fTNNLS.2012.2191620&partnerID=40&md5=09e520a92ff2f74c4ce1b2982fbf3a31","As one of the fundamental features, color provides useful information and plays an important role for face recognition. Generally, the choice of a color space is different for different visual tasks. How can a color space be sought for the specific face recognition problem? To address this problem, we propose a sparse tensor discriminant color space (STDCS) model that represents a color image as a third-order tensor in this paper. The model cannot only keep the underlying spatial structure of color images but also enhance robustness and give intuitionistic or semantic interpretation. STDCS transforms the eigenvalue problem to a series of regression problems. Then one spare color space transformation matrix and two sparse discriminant projection matrices are obtained by applying lasso or elastic net on the regression problems. The experiments on three color face databases, AR, Georgia Tech, and Labeled Faces in the Wild face databases, show that both the performance and the robustness of the proposed method outperform those of the state-of-the-art TDCS model. © 2012 IEEE.","Color images; discriminant information; face recognition; sparse representation; tensor subspace","Color images; Color space transformation; Discriminant informations; Fundamental features; Semantic interpretation; Sparse representation; Tensor subspaces; Third-order tensors; Color image processing; Eigenvalues and eigenfunctions; Face recognition; Linear transformations; Semantics; Space applications; Tensors; Vision; Color; anatomy and histology; artificial intelligence; automated pattern recognition; biometry; color; colorimetry; computer assisted diagnosis; face; human; procedures; reproducibility; sensitivity and specificity; Artificial Intelligence; Biometry; Color; Colorimetry; Face; Humans; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84866609797
"Li Z., Yang Y., Liu J., Zhou X., Lu H.","Unsupervised feature selection using nonnegative spectral analysis",2012,"Proceedings of the National Conference on Artificial Intelligence",74,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868284545&partnerID=40&md5=c47083b1aef5a5c14089b62641c6f199","In this paper, a new unsupervised learning algorithm, namely Nonnegative Discriminative Feature Selection (NDFS), is proposed. To exploit the discriminative information in unsupervised scenarios, we perform spectral clustering to learn the cluster labels of the input samples, during which the feature selection is performed simultaneously. The joint learning of the cluster labels and feature selection matrix enables NDFS to select the most discriminative features. To learn more accurate cluster labels, a nonnegative constraint is explicitly imposed to the class indicators. To reduce the redundant or even noisy features, ℓ 2,1-norm minimization constraint is added into the objective function, which guarantees the feature selection matrix sparse in rows. Our algorithm exploits the discriminative information and feature correlation simultaneously to select a better feature subset. A simple yet efficient iterative algorithm is designed to optimize the proposed objective function. Experimental results on different real world datasets demonstrate the encouraging performance of our algorithm over the state-of-the-arts. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Discriminative features; Feature correlation; Feature subset; Input sample; Iterative algorithm; Objective functions; Real-world datasets; Spectral clustering; Unsupervised feature selection; Learning algorithms; Spectrum analysis; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868284545
"Rocktäschel T., Weidlich M., Leser U.","Chemspot: A hybrid system for chemical named entity recognition",2012,"Bioinformatics",74,10.1093/bioinformatics/bts183,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863506694&doi=10.1093%2fbioinformatics%2fbts183&partnerID=40&md5=47f41d419419250a2f9d1a0c8a406460","Motivation: The accurate identification of chemicals in text is important for many applications, including computer-assisted reconstruction of metabolic networks or retrieval of information about substances in drug development. But due to the diversity of naming conventions and traditions for such molecules, this task is highly complex and should be supported by computational tools.Results: We present ChemSpot, a named entity recognition (NER) tool for identifying mentions of chemicals in natural language texts, including trivial names, drugs, abbreviations, molecular formulas and International Union of Pure and Applied Chemistry entities. Since the different classes of relevant entities have rather different naming characteristics, ChemSpot uses a hybrid approach combining a Conditional Random Field with a dictionary. It achieves an F1 measure of 68.1% on the SCAI corpus, outperforming the only other freely available chemical NER tool, OSCAR4, by 10.8 percentage points. © The Author 2012. Published by Oxford University Press. All rights reserved.",,"drug; article; artificial intelligence; biology; book; classification; computer program; information retrieval; methodology; natural language processing; nomenclature; Artificial Intelligence; Computational Biology; Dictionaries, Chemical; Information Storage and Retrieval; Natural Language Processing; Pharmaceutical Preparations; Software; Terminology as Topic",Article,Scopus,2-s2.0-84863506694
"Tang J., Zha Z.-J., Tao D., Chua T.-S.","Semantic-gap-oriented active learning for multilabel image annotation",2012,"IEEE Transactions on Image Processing",74,10.1109/TIP.2011.2180916,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859075572&doi=10.1109%2fTIP.2011.2180916&partnerID=40&md5=946bbe24feb7a7c7698a9dc3fe869b22","User interaction is an effective way to handle the semantic gap problem in image annotation. To minimize user effort in the interactions, many active learning methods were proposed. These methods treat the semantic concepts individually or correlatively. However, they still neglect the key motivation of user feedback: to tackle the semantic gap. The size of the semantic gap of each concept is an important factor that affects the performance of user feedback. User should pay more efforts to the concepts with large semantic gaps, and vice versa. In this paper, we propose a semantic-gap-oriented active learning method, which incorporates the semantic gap measure into the information-minimization- based sample selection strategy. The basic learning model used in the active learning framework is an extended multilabel version of the sparse-graph-based semisupervised learning method that incorporates the semantic correlation. Extensive experiments conducted on two benchmark image data sets demonstrated the importance of bringing the semantic gap measure into the active learning process. © 2011 IEEE.","Active learning; image annotation; multilabel; semantic gap; sparse graph","Active Learning; Image annotation; Multi-label; semantic gap; Sparse graphs; Image analysis; Learning systems; Semantics; algorithm; artificial intelligence; automated pattern recognition; computer assisted diagnosis; documentation; image enhancement; image subtraction; information retrieval; letter; methodology; semantics; Algorithms; Artificial Intelligence; Documentation; Image Enhancement; Image Interpretation, Computer-Assisted; Information Storage and Retrieval; Pattern Recognition, Automated; Semantics; Subtraction Technique",Article,Scopus,2-s2.0-84859075572
"Ciornei I., Kyriakides E.","A GA-API solution for the economic dispatch of generation in power system operation",2012,"IEEE Transactions on Power Systems",74,10.1109/TPWRS.2011.2168833,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856295481&doi=10.1109%2fTPWRS.2011.2168833&partnerID=40&md5=7c7944f38477d8b3983d8f920f9f912a","This work proposes a novel heuristic-hybrid optimization method designed to solve the nonconvex economic dispatch problem in power systems. Due to the fast computational capabilities of the proposed algorithm, it is envisioned that it becomes an operations tool for both the generation companies and the TSO/ISO. The methodology proposed improves the overall search capability of two powerful heuristic optimization algorithms: a special class of ant colony optimization called API and a real coded genetic algorithm (RCGA). The proposed algorithm, entitled GAAPI, is a relatively simple but robust algorithm, which combines the downhill behavior of API (a key characteristic of optimization algorithms) and a good spreading in the solution space of the GA search strategy (a guarantee to avoid being trapped in local optima). The feasibility of the proposed method is first tested on a number of well-known complex test functions, as well as on four different power test systems having different sizes and complexities. The results are analyzed in terms of both quality of the solution and the computational efficiency; it is shown that the proposed GAAPI algorithm is capable of obtaining highly robust, quality solutions in a reasonable computational time, compared to a number of similar algorithms proposed in the literature. © 2006 IEEE.","ant colony optimization; API; economic dispatch; genetic algorithm; global optimization; hybrid models; nonconvex optimization; power system operation; robust search","Ant-colony optimization; API; economic dispatch; Hybrid model; Nonconvex optimization; Power system operations; robust search; Artificial intelligence; Computational efficiency; Constrained optimization; Global optimization; Heuristic algorithms; Heuristic methods; Power transmission; Genetic algorithms",Article,Scopus,2-s2.0-84856295481
"Gorbunov S., Vaikuntanathan V., Wee H.","Functional encryption with bounded collusions via multi-party computation",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",73,10.1007/978-3-642-32009-5_11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865477351&doi=10.1007%2f978-3-642-32009-5_11&partnerID=40&md5=68748fd7556447db6f0dfa067441b358","We construct functional encryption schemes for polynomial-time computable functions secure against an a-priori bounded polynomial number of collusions. Our constructions require only semantically secure public-key encryption schemes and pseudorandom generators computable by small-depth circuits (known to be implied by most concrete intractability assumptions). For certain special cases such as predicate encryption schemes with public index, the construction requires only semantically secure encryption schemes. Along the way, we show a ""bootstrapping theorem"" that builds a q-query functional encryption scheme for arbitrary functions starting from a q-query functional encryption scheme for bounded-degree functions. All our constructions rely heavily on techniques from secure multi-party computation and randomized encodings. Our constructions are secure under a strong simulation-based definition of functional encryption. © 2012 International Association for Cryptologic Research.",,"Arbitrary functions; Encodings; Encryption schemes; Multiparty computation; Polynomial number; Polynomial-time computable functions; Pseudorandom generators; Public-key encryption scheme; Secure multi-party computation; Artificial intelligence; Public key cryptography",Conference Paper,Scopus,2-s2.0-84865477351
"Cheng H.-Y., Weng C.-C., Chen Y.-Y.","Vehicle detection in aerial surveillance using dynamic bayesian networks",2012,"IEEE Transactions on Image Processing",73,10.1109/TIP.2011.2172798,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859072436&doi=10.1109%2fTIP.2011.2172798&partnerID=40&md5=573c35956391d87686ea81d99dd240b2","We present an automatic vehicle detection system for aerial surveillance in this paper. In this system, we escape from the stereotype and existing frameworks of vehicle detection in aerial surveillance, which are either region based or sliding window based. We design a pixelwise classification method for vehicle detection. The novelty lies in the fact that, in spite of performing pixelwise classification, relations among neighboring pixels in a region are preserved in the feature extraction process. We consider features including vehicle colors and local features. For vehicle color extraction, we utilize a color transform to separate vehicle colors and nonvehicle colors effectively. For edge detection, we apply moment preserving to adjust the thresholds of the Canny edge detector automatically, which increases the adaptability and the accuracy for detection in various aerial images. Afterward, a dynamic Bayesian network (DBN) is constructed for the classification purpose. We convert regional local features into quantitative observations that can be referenced when applying pixelwise classification via DBN. Experiments were conducted on a wide variety of aerial videos. The results demonstrate flexibility and good generalization abilities of the proposed method on a challenging data set with aerial surveillance images taken at different heights and under different camera angles. © 2011 IEEE.","Aerial surveillance; dynamic Bayesian networks (DBNs); vehicle detection","Aerial images; Aerial surveillance; Aerial video; Canny edge detectors; Color extraction; Color transform; Data sets; Different heights; Dynamic Bayesian network; dynamic Bayesian networks (DBNs); Generalization ability; Local feature; Moment preserving; Pixelwise classification; Region-based; Sliding window-based; vehicle detection; Vehicle detection systems; Bayesian networks; Color; Edge detection; Feature extraction; Monitoring; Security systems; aircraft; algorithm; article; artificial intelligence; automated pattern recognition; Bayes theorem; computer assisted diagnosis; image enhancement; methodology; motor vehicle; photography; reproducibility; sensitivity and specificity; Aircraft; Algorithms; Artificial Intelligence; Bayes Theorem; Image Enhancement; Image Interpretation, Computer-Assisted; Motor Vehicles; Pattern Recognition, Automated; Photography; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84859072436
"Fang C., Marle F.","A simulation-based risk network model for decision support in project risk management",2012,"Decision Support Systems",73,10.1016/j.dss.2011.10.021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855984356&doi=10.1016%2fj.dss.2011.10.021&partnerID=40&md5=a866ccbf57e12fb4426ad296416152a0","This paper presents a decision support system (DSS) for the modeling and management of project risks and risk interactions. This is a crucial activity in project management, as projects are facing a growing complexity with higher uncertainties and tighter constraints. Existing classical methods have limitations for modeling the complexity of project risks. For example, some phenomena like chain reactions and loops are not properly taken into account. This will influence the effectiveness of decisions for risk response planning and will lead to unexpected and undesired behavior in the project. Based on the concepts of DSS and the classical steps of project risk management, we develop an integrated DSS framework including the identification, assessment and analysis of the risk network. In the network, the nodes are the risks and the edges represent the cause and effect potential interactions between risks. The proposed simulation-based model makes it possible to re-evaluate risks and their priorities, to suggest and test mitigation actions, and then to support project manager in making decisions regarding risk response actions. An example of application is provided to illustrate the utility of the model. © 2011 Elsevier B.V. All rights reserved.","Complexity; Decision support system; Project risk management; Risk network; Simulation","Chain reaction; Classical methods; Complexity; Decision supports; Making decision; Network models; Project managers; Project risk; Project risk management; Risk response; Risk response planning; Simulation; Simulation-based; Undesired behavior; Artificial intelligence; Decision making; Decision support systems; Network management; Project management; Risk management; Computer simulation",Article,Scopus,2-s2.0-84855984356
"Gray K., Wegner D.M.","Feeling robots and human zombies: Mind perception and the uncanny valley",2012,"Cognition",72,10.1016/j.cognition.2012.06.007,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865292141&doi=10.1016%2fj.cognition.2012.06.007&partnerID=40&md5=3d377ead3d47c6a10a7c077a33927daf","The uncanny valley-the unnerving nature of humanlike robots-is an intriguing idea, but both its existence and its underlying cause are debated. We propose that humanlike robots are not only unnerving, but are so because their appearance prompts attributions of mind. In particular, we suggest that machines become unnerving when people ascribe to them experience (the capacity to feel and sense), rather than agency (the capacity to act and do). Experiment 1 examined whether a machine's humanlike appearance prompts both ascriptions of experience and feelings of unease. Experiment 2 tested whether a machine capable of experience remains unnerving, even without a humanlike appearance. Experiment 3 investigated whether the perceived lack of experience can also help explain the creepiness of unfeeling humans and philosophical zombies. These experiments demonstrate that feelings of uncanniness are tied to perceptions of experience, and also suggest that experience-but not agency-is seen as fundamental to humans, and fundamentally lacking in machines. © 2012 Elsevier B.V.","Artificial intelligence; Human-computer interaction; Humanness; Robots; Turing test; Uncanny valley","adult; article; emotion; emotionality; experience; female; human; human characteristic; human computer interaction; human experiment; male; mechanics; mental capacity; perception; personal appearance; priority journal; robotics; sensation; Emotions; Female; Humans; Intellectual Disability; Male; Robotics; Theory of Mind",Article,Scopus,2-s2.0-84865292141
"Das D.C., Roy A.K., Sinha N.","GA based frequency controller for solar thermal-diesel-wind hybrid energy generation/energy storage system",2012,"International Journal of Electrical Power and Energy Systems",71,10.1016/j.ijepes.2012.05.025,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862667693&doi=10.1016%2fj.ijepes.2012.05.025&partnerID=40&md5=b8c145548ac0fe32e5c0ae3052ece7cb","Wind, Solar photovoltaic and solar thermal power systems are emerging renewable energy technologies and can be developed as viable options for electricity generation in future. In this paper, autonomous hybrid generation systems consisting of wind turbine generators (WTGs), solar thermal power system (STPS), solar photovoltaic (PV), diesel engine generators (DEGs), fuel cells (FCs), battery energy storage system (BESS), flywheel (FW), ultra capacitors (UCs) and aqua electrolyzer (AE) have been considered for simulation studies. The power system frequency deviates for sudden changes in load or generation or the both. The comparative performance of the controllers installed to alleviate this frequency deviation for different hybrid systems, is carried out using time domain simulation. In practice, controllers (PI or PID) are tuned manually which is difficult and time consuming. The computational intelligence has opened paths to a new generation of advanced process control. Here, GA is used for optimization of controllers' gains of the proposed hybrid systems. The simulation results demonstrate the effectiveness of the GA based controllers in terms of reduced settling time, overshoot and oscillations. The results are compared with conventional controllers. © 2012 Elsevier Ltd. All rights reserved.","Aqua electrolyzer; Battery energy storage system; Diesel engine generator; Fuel cell; Genetic algorithm; Wind turbine generator","Advanced Process Control; Aqua electrolyzer; Battery energy storage systems; Conventional controllers; Diesel engine generators; Electricity generation; Frequency controllers; Frequency deviation; Hybrid energy; Hybrid generation system; Power system frequencies; Renewable energy technologies; Settling time; Simulation studies; Solar photovoltaics; Solar thermal power; Storage systems; Sudden change; Time-domain simulations; Ultracapacitors; Artificial intelligence; Diesel engines; Electric generators; Electrolytic cells; Energy storage; Fuel cells; Genetic algorithms; Hybrid systems; Intelligent control; Turbogenerators; Wind turbines; Solar concentrators",Article,Scopus,2-s2.0-84862667693
"Yan X., Zhu Y., Zou W., Wang L.","A new approach for data clustering using hybrid artificial bee colony algorithm",2012,"Neurocomputing",71,10.1016/j.neucom.2012.04.025,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865320741&doi=10.1016%2fj.neucom.2012.04.025&partnerID=40&md5=fcb41cbedadc180ed9f40a03089e35c4","Data clustering is a popular data analysis technique needed in many fields. Recent years, some swarm intelligence-based approaches for clustering were proposed and achieved encouraging results. This paper presents a Hybrid Artificial Bee Colony (HABC) algorithm for data clustering. The incentive mechanism of HABC is enhancing the information exchange (social learning) between bees by introducing the crossover operator of Genetic Algorithm (GA) to ABC. With a test on ten benchmark functions, the proposed HABC algorithm is proved to have significant improvement over canonical ABC and several other comparison algorithms. The HABC algorithm is then employed for data clustering. Six real datasets selected from the UCI machine learning repository are used. The results show that the HABC algorithm achieved better results than other algorithms and is a competitive approach for data clustering. © 2012 Elsevier B.V.","Artificial bee colony; Crossover operator; Data clustering; Hybrid artificial bee colony","Artificial bee colonies; Artificial bee colony algorithms; Benchmark functions; Crossover operator; Data analysis techniques; Data clustering; Incentive mechanism; Information exchanges; Real data sets; Social learning; UCI machine learning repository; Artificial intelligence; Cluster analysis; Clustering algorithms; Genetic algorithms; algorithm; analytic method; article; bee; cluster analysis; data analysis; genetic algorithm; machine learning; mathematical analysis; mathematical computing; organism colony; priority journal; simulation; social learning",Article,Scopus,2-s2.0-84865320741
"Gong Y.-J., Shen M., Zhang J., Kaynak O., Chen W.-N., Zhan Z.-H.","Optimizing RFID network planning by using a particle swarm optimization algorithm with redundant reader elimination",2012,"IEEE Transactions on Industrial Informatics",71,10.1109/TII.2012.2205390,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867955054&doi=10.1109%2fTII.2012.2205390&partnerID=40&md5=1dc75fd0213bd2fa4eaf85138e677bdb","The rapid development of radio frequency identification (RFID) technology creates the challenge of optimal deployment of an RFID network. The RFID network planning (RNP) problem involves many constraints and objectives and has been proven to be NP-hard. The use of evolutionary computation (EC) and swarm intelligence (SI) for solving RNP has gained significant attention in the literature, but the algorithms proposed have seen difficulties in adjusting the number of readers deployed in the network. However, the number of deployed readers has an enormous impact on the network complexity and cost. In this paper, we develop a novel particle swarm optimization (PSO) algorithm with a tentative reader elimination (TRE) operator to deal with RNP. The TRE operator tentatively deletes readers during the search process of PSO and is able to recover the deleted readers after a few generations if the deletion lowers tag coverage. By using TRE, the proposed algorithm is capable of adaptively adjusting the number of readers used in order to improve the overall performance of RFID network. Moreover, a mutation operator is embedded into the algorithm to improve the success rate of TRE. In the experiment, six RNP benchmarks and a real-world RFID working scenario are tested and four algorithms are implemented and compared. Experimental results show that the proposed algorithm is capable of achieving higher coverage and using fewer readers than the other algorithms. © 2012 IEEE.","Particle swarm optimization (PSO); radio frequency identification (RFID); redundant reader elimination; RFID network planning (RNP)","Mutation operators; Network complexity; NP-hard; Optimal deployment; Particle swarm optimization algorithm; Radio frequency identification technology; redundant reader elimination; RFID networks; Search process; Swarm Intelligence; Artificial intelligence; Particle swarm optimization (PSO); Radio frequency identification (RFID); Algorithms",Article,Scopus,2-s2.0-84867955054
"Bullinaria J.A., Levy J.P.","Extracting semantic representations from word co-occurrence statistics: Stop-lists, stemming, and SVD",2012,"Behavior Research Methods",71,10.3758/s13428-011-0183-8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865654632&doi=10.3758%2fs13428-011-0183-8&partnerID=40&md5=7509eece37498a2fd18cd10603a32132","In a previous article, we presented a systematic computational study of the extraction of semantic representations from the word-word co-occurrence statistics of large text corpora. The conclusion was that semantic vectors of pointwise mutual information values from very small co-occurrence windows, together with a cosine distance measure, consistently resulted in the best representations across a range of psychologically relevant semantic tasks. This article extends that study by investigating the use of three further factors-namely, the application of stop-lists, word stemming, and dimensionality reduction using singular value decomposition (SVD)-that have been used to provide improved performance elsewhere. It also introduces an additional semantic task and explores the advantages of using a much larger corpus. This leads to the discovery and analysis of improved SVD-based methods for generating semantic representations (that provide new state-of-the-art performance on a standard TOEFL task) and the identification and discussion of problems and misleading results that can arise without a full systematic study. © 2012 Psychonomic Society, Inc.","Corpus statistics; Semantic representation; SVD","article; artificial intelligence; computer program; decision making; human; linguistics; semantics; statistical analysis; Artificial Intelligence; Choice Behavior; Data Interpretation, Statistical; Humans; Judgment; Psycholinguistics; Semantics; Software",Article,Scopus,2-s2.0-84865654632
"Solway A., Botvinick M.M.","Goal-directed decision making as probabilistic inference: A computational framework and potential neural correlates",2012,"Psychological Review",70,10.1037/a0026435,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859737036&doi=10.1037%2fa0026435&partnerID=40&md5=9fbef7faa771e719c33a2aa992e294d7","Recent work has given rise to the view that reward-based decision making is governed by two key controllers: a habit system, which stores stimulus-response associations shaped by past reward, and a goal-oriented system that selects actions based on their anticipated outcomes. The current literature provides a rich body of computational theory addressing habit formation, centering on temporal-difference learning mechanisms. Less progress has been made toward formalizing the processes involved in goal-directed decision making. We draw on recent work in cognitive neuroscience, animal conditioning, cognitive and developmental psychology, and machine learning to outline a new theory of goal-directed decision making. Our basic proposal is that the brain, within an identifiable network of cortical and subcortical structures, implements a probabilistic generative model of reward, and that goal-directed decision making is effected through Bayesian inversion of this model. We present a set of simulations implementing the account, which address benchmark behavioral and neuroscientific findings, and give rise to a set of testable predictions. We also discuss the relationship between the proposed framework and other models of decision making, including recent models of perceptual choice, to which our theory bears a direct connection. © 2012 American Psychological Association.","Decision making; Neuroeconomics; Planning; Probabilistic inference; Reward","algorithm; animal; animal behavior; article; artificial intelligence; Bayes theorem; biological model; brain; computer simulation; conditioning; decision making; habit; human; learning; maze test; motivation; nerve cell network; neuroscience; physiology; problem solving; psychological model; rat; reward; Algorithms; Animals; Artificial Intelligence; Bayes Theorem; Behavior, Animal; Brain; Computer Simulation; Conditioning (Psychology); Decision Making; Goals; Habits; Humans; Learning; Maze Learning; Models, Neurological; Models, Psychological; Nerve Net; Neurosciences; Probability Learning; Problem Solving; Rats; Reward",Article,Scopus,2-s2.0-84859737036
"Chen T., Zhang W., Lu Q., Chen K., Zheng Z., Yu Y.","SVDFeature: A toolkit for feature-based collaborative filtering",2012,"Journal of Machine Learning Research",70,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873429648&partnerID=40&md5=9fa1d20933128456c9f689c38a1a900d","In this paper we introduce SVDFeature, a machine learning toolkit for feature-based collaborative filtering. SVDFeature is designed to efficiently solve the feature-based matrix factorization. The feature-based setting allows us to build factorization models incorporating side information such as temporal dynamics, neighborhood relationship, and hierarchical information. The toolkit is capable of both rate prediction and collaborative ranking, and is carefully designed for efficient training on large-scale data set. Using this toolkit, we built solutions to win KDD Cup for two consecutive years. © 2012 Tianci Chen, Weinan Zhang, Qiuxia Lu, Kailong Chen, Zhao Zheng and Yong Yu.","Context-aware recommendation; Large-scale collaborative filtering; Ranking","Collaborative filtering; Context-Aware; Data sets; Factorization model; Feature-based; Hierarchical information; Matrix factorizations; Ranking; Rate predictions; Side information; Temporal dynamics; Artificial intelligence; Software engineering",Article,Scopus,2-s2.0-84873429648
"Kumar A., Daumé III H.","Learning task grouping and overlap in multi-task learning",2012,"Proceedings of the 29th International Conference on Machine Learning, ICML 2012",70,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867114266&partnerID=40&md5=757d35662cd86068356e6b369f029c4a","In the paradigm of multi-task learning, multiple related prediction tasks are learned jointly, sharing information across the tasks. We propose a framework for multi-task learning that enables one to selectively share the information across the tasks. We assume that each task parameter vector is a linear combination of a finite number of underlying basis tasks. The coefficients of the linear combination are sparse in nature and the overlap in the sparsity patterns of two tasks controls the amount of sharing across these. Our model is based on the assumption that task parameters within a group lie in a low dimensional subspace but allows the tasks in different groups to overlap with each other in one or more bases. Experimental results on four datasets show that our approach outperforms competing methods. Copyright 2012 by the author(s)/owner(s).",,"Data sets; Finite number; Learning tasks; Linear combinations; Low-dimensional subspace; Multitask learning; Parameter vectors; Prediction tasks; Sharing information; Sparsity patterns; Artificial intelligence; Software engineering; Learning systems",Conference Paper,Scopus,2-s2.0-84867114266
"Castrodad A., Sapiro G.","Sparse modeling of human actions from motion imagery",2012,"International Journal of Computer Vision",70,10.1007/s11263-012-0534-7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861728486&doi=10.1007%2fs11263-012-0534-7&partnerID=40&md5=6ba85c7c14416e8b3af073b3ffdc7c10","An efficient sparse modeling pipeline for the classification of human actions from video is here developed. Spatio-temporal features that characterize local changes in the image are first extracted. This is followed by the learning of a class-structured dictionary encoding the individual actions of interest. Classification is then based on reconstruction, where the label assigned to each video comes from the optimal sparse linear combination of the learned basis vectors (action primitives) representing the actions. A low computational cost deep-layer model learning the inter-class correlations of the data is added for increasing discriminative power. In spite of its simplicity and low computational cost, the method outperforms previously reported results for virtually all standard datasets. © 2012 Springer Science+Business Media, LLC (outside the USA).","Action classification; Dictionary learning; Sparse modeling; Supervised learning","Basis vector; Computational costs; Data sets; Dictionary learning; Human actions; Linear combinations; Model learning; Motion imagery; Spatio-temporal; Artificial intelligence; Supervised learning; Software engineering",Article,Scopus,2-s2.0-84861728486
"Kang F., Li J.-J., Xu Q.","Damage detection based on improved particle swarm optimization using vibration data",2012,"Applied Soft Computing Journal",70,10.1016/j.asoc.2012.03.050,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861875265&doi=10.1016%2fj.asoc.2012.03.050&partnerID=40&md5=8bc405431bf13dcee2b79a94d8043c34","An immunity enhanced particle swarm optimization (IEPSO) algorithm, which combines particle swarm optimization (PSO) with the artificial immune system, is proposed for damage detection of structures. Some immune mechanisms, selection, receptor editing and vaccination are introduced into the basic PSO to improve its performance. The objective function for damage detection is based on vibration data, such as natural frequencies and mode shapes. The feasibility and efficiency of IEPSO are compared with the basic PSO, a differential evolution algorithm and a real-coded genetic algorithm on two examples. Results show that the proposed strategy is efficient on determining the sites and the extents of structure damages. © 2012 Elsevier B.V.","Artificial immune system; Damage identification; Modal parameter; Particle swarm optimization; Swarm intelligence","Artificial Immune System; Damage Identification; Differential evolution algorithms; Enhanced particle swarm optimization; Immune mechanism; Improved particle swarm optimization; Modal parameters; Mode shapes; Objective functions; Real-coded genetic algorithm; Receptor editing; Structure damage; Swarm Intelligence; Vibration data; Artificial intelligence; Damage detection; Immunology; Modal analysis; Particle swarm optimization (PSO)",Article,Scopus,2-s2.0-84861875265
"Akbari H., Halig L.V., Schuster D.M., Osunkoya A., Master V., Nieh P.T., Chen G.Z., Fei B.","Hyperspectral imaging and quantitative analysis for prostate cancer detection",2012,"Journal of Biomedical Optics",70,10.1117/1.JBO.17.7.076005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867246184&doi=10.1117%2f1.JBO.17.7.076005&partnerID=40&md5=85169b44270a6364dde64dc1a91190ff","Hyperspectral imaging (HSI) is an emerging modality for various medical applications. Its spectroscopic data might be able to be used to noninvasively detect cancer. Quantitative analysis is often necessary in order to differentiate healthy from diseased tissue. We propose the use of an advanced image processing and classification method in order to analyze hyperspectral image data for prostate cancer detection. The spectral signatures were extracted and evaluated in both cancerous and normal tissue. Least squares support vector machines were developed and evaluated for classifying hyperspectral data in order to enhance the detection of cancer tissue. This method was used to detect prostate cancer in tumor-bearing mice and on pathology slides. Spatially resolved images were created to highlight the differences of the reflectance properties of cancer versus those of normal tissue. Preliminary results with 11 mice showed that the sensitivity and specificity of the hyperspectral image classification method are 92.8% to 2.0% and 96.9% to 1.3%, respectively. Therefore, this imaging method may be able to help physicians to dissect malignant regions with a safe margin and to evaluate the tumor bed after resection. This pilot study may lead to advances in the optical diagnosis of prostate cancer using HSI technology. © 2012 Society of Photo-Optical Instrumentation Engineers (SPIE).","Hyperspectral imaging; Image classification; Least squares support vector machine; Optical diagnosis; Prostate cancer","algorithm; animal; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; fluorescence imaging; image enhancement; male; methodology; mouse; nude mouse; pathology; prostate tumor; reproducibility; sensitivity and specificity; spectroscopy; tumor cell line; Algorithms; Animals; Artificial Intelligence; Cell Line, Tumor; Image Enhancement; Image Interpretation, Computer-Assisted; Male; Mice; Mice, Nude; Optical Imaging; Pattern Recognition, Automated; Prostatic Neoplasms; Reproducibility of Results; Sensitivity and Specificity; Spectrum Analysis",Article,Scopus,2-s2.0-84867246184
"Li N., Zeng L., He Q., Shi Z.","Parallel implementation of apriori algorithm based on MapReduce",2012,"Proceedings - 13th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing, SNPD 2012",69,10.1109/SNPD.2012.31,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868518487&doi=10.1109%2fSNPD.2012.31&partnerID=40&md5=a2d687e0e63b69a261bed703b573a344","Searching frequent patterns in transactional databases is considered as one of the most important data mining problems and Apriori is one of the typical algorithms for this task. Developing fast and efficient algorithms that can handle large volumes of data becomes a challenging task due to the large databases. In this paper, we implement a parallel Apriori algorithm based on MapReduce, which is a framework for processing huge datasets on certain kinds of distributable problems using a large number of computers (nodes). The experimental results demonstrate that the proposed algorithm can scale well and efficiently process large datasets on commodity hardware. © 2012 IEEE.","Apriori algorithm; Frequent itemsets; Large database; MapReduce; Parallel implementation","Apriori algorithms; Item sets; Large database; Map-reduce; Parallel implementations; Artificial intelligence; Database systems; Learning algorithms; Software engineering",Conference Paper,Scopus,2-s2.0-84868518487
"Patron-Perez A., Marszalek M., Reid I., Zisserman A.","Structured learning of human interactions in TV shows",2012,"IEEE Transactions on Pattern Analysis and Machine Intelligence",69,10.1109/TPAMI.2012.24,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867753906&doi=10.1109%2fTPAMI.2012.24&partnerID=40&md5=7d1878d2a0f44628132af65f408fd095","The objective of this work is recognition and spatiotemporal localization of two-person interactions in video. Our approach is person-centric. As a first stage we track all upper bodies and heads in a video using a tracking-by-detection approach that combines detections with KLT tracking and clique partitioning, together with occlusion detection, to yield robust person tracks. We develop local descriptors of activity based on the head orientation (estimated using a set of pose-specific classifiers) and the local spatiotemporal region around them, together with global descriptors that encode the relative positions of people as a function of interaction type. Learning and inference on the model uses a structured output SVM which combines the local and global descriptors in a principled manner. Inference using the model yields information about which pairs of people are interacting, their interaction class, and their head orientation (which is also treated as a variable, enabling mistakes in the classifier to be corrected using global context). We show that inference can be carried out with polynomial complexity in the number of people, and describe an efficient algorithm for this. The method is evaluated on a new dataset comprising 300 video clips acquired from 23 different TV shows and on the benchmark UT-Interaction dataset. © 2012 IEEE.","Human interaction recognition; Structured SVM; Video retrieval","Activity-based; Clique partitioning; Data sets; Descriptors; Global context; Human interactions; Local and global descriptors; Local descriptors; Model yields; Number of peoples; Occlusion detection; Polynomial complexity; Relative positions; Spatiotemporal regions; Structured learning; Structured SVM; Video clips; Video retrieval; Artificial intelligence; Computer vision; Algorithms; article; body posture; head; human; human relation; image processing; methodology; physiology; support vector machine; television; videorecording; Head; Humans; Image Processing, Computer-Assisted; Interpersonal Relations; Posture; Support Vector Machines; Television; Video Recording",Article,Scopus,2-s2.0-84867753906
"Cho Y., Seong J.-K., Jeong Y., Shin S.Y.","Individual subject classification for Alzheimer's disease based on incremental learning using a spatial frequency representation of cortical thickness data",2012,"NeuroImage",69,10.1016/j.neuroimage.2011.09.085,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855418467&doi=10.1016%2fj.neuroimage.2011.09.085&partnerID=40&md5=0cb9d8665e39dee807d2bb95006722db","Patterns of brain atrophy measured by magnetic resonance structural imaging have been utilized as significant biomarkers for diagnosis of Alzheimer's disease (AD). However, brain atrophy is variable across patients and is non-specific for AD in general. Thus, automatic methods for AD classification require a large number of structural data due to complex and variable patterns of brain atrophy. In this paper, we propose an incremental method for AD classification using cortical thickness data. We represent the cortical thickness data of a subject in terms of their spatial frequency components, employing the manifold harmonic transform. The basis functions for this transform are obtained from the eigenfunctions of the Laplace-Beltrami operator, which are dependent only on the geometry of a cortical surface but not on the cortical thickness defined on it. This facilitates individual subject classification based on incremental learning. In general, methods based on region-wise features poorly reflect the detailed spatial variation of cortical thickness, and those based on vertex-wise features are sensitive to noise. Adopting a vertex-wise cortical thickness representation, our method can still achieve robustness to noise by filtering out high frequency components of the cortical thickness data while reflecting their spatial variation. This compromise leads to high accuracy in AD classification. We utilized MR volumes provided by Alzheimer's Disease Neuroimaging Initiative (ADNI) to validate the performance of the method. Our method discriminated AD patients from Healthy Control (HC) subjects with 82% sensitivity and 93% specificity. It also discriminated Mild Cognitive Impairment (MCI) patients, who converted to AD within 18. months, from non-converted MCI subjects with 63% sensitivity and 76% specificity. Moreover, it showed that the entorhinal cortex was the most discriminative region for classification, which is consistent with previous pathological findings. In comparison with other classification methods, our method demonstrated high classification performance in both categories, which supports the discriminative power of our method in both AD diagnosis and AD prediction. © 2011 Elsevier Inc.","Alzheimer's disease; Cortical thickness; Frequency representation; Incremental learning; Individual subject classification","adult; aged; Alzheimer disease; analytical parameters; article; brain cortex; controlled study; cortical thickness; diagnostic accuracy; diagnostic test accuracy study; disease classification; entorhinal cortex; female; geometry; human; incremental learning; intermethod comparison; learning; major clinical study; male; mild cognitive impairment; noise; nuclear magnetic resonance imaging; patient coding; priority journal; sensitivity analysis; sensitivity and specificity; spatial frequency discrimination; surface property; validation process; Aged; Aged, 80 and over; Algorithms; Alzheimer Disease; Artificial Intelligence; Atrophy; Brain; Cerebral Cortex; Databases, Factual; Disease Progression; Entorhinal Cortex; False Negative Reactions; False Positive Reactions; Female; Humans; Image Processing, Computer-Assisted; Longitudinal Studies; Magnetic Resonance Imaging; Male; Memory; Middle Aged; Mild Cognitive Impairment; Neuropsychological Tests; Positron-Emission Tomography; Reproducibility of Results",Article,Scopus,2-s2.0-84855418467
"Wachinger C., Navab N.","Entropy and Laplacian images: Structural representations for multi-modal registration",2012,"Medical Image Analysis",69,10.1016/j.media.2011.03.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-82355161020&doi=10.1016%2fj.media.2011.03.001&partnerID=40&md5=980533a39f60559c01847e3785efbf05","The standard approach to multi-modal registration is to apply sophisticated similarity metrics such as mutual information. The disadvantage of these metrics, in comparison to measuring the intensity difference with, e.g. L1 or L2 distance, is the increase in computational complexity and consequently the increase in runtime of the registration. An alternative approach, which has not yet gained much attention in the literature, is to find image representations, so called structural representations, that allow for the application of the L1 and L2 distance for multi-modal images. This has not only the advantage of a faster similarity calculation but enables also the application of more sophisticated optimization strategies. In this article, we theoretically analyze the requirements for structural representations. Further, we introduce two approaches to create such representations, which are based on the calculation of patch entropy and manifold learning, respectively. While the application of entropy has practical advantages in terms of computational complexity, the usage of manifold learning has theoretical advantages, by presenting an optimal approximation to one of the theoretical requirements. We perform experiments on multiple datasets for rigid, deformable, and groupwise registration with good results with respect to both, runtime and quality of alignment. © 2011 Elsevier B.V..","Entropy; Laplacian eigenmaps; Multi-modal registration; Structural representation","Alternative approach; Data sets; Image representations; Intensity difference; L2 distances; Laplacian eigenmaps; Laplacians; Manifold learning; Multi-modal image; Multimodal registration; Mutual informations; Optimal approximation; Optimization strategy; Runtimes; Similarity calculation; Similarity metrics; Structural representation; Computational complexity; Entropy; Laplace transforms; Optimization; Modal analysis; article; calculation; conceptual framework; entropy; image analysis; image processing; image quality; information technology; machine learning; morphometrics; multi modal registration; priority journal; Algorithms; Artificial Intelligence; Brain; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique; Tomography, X-Ray Computed",Article,Scopus,2-s2.0-82355161020
"Eiter T., Ortiz M., Šimkus M., Tran T.-K., Xiao G.","Query rewriting for Horn-SHIQ plus rules",2012,"Proceedings of the National Conference on Artificial Intelligence",68,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868294276&partnerID=40&md5=7b72eb1784d3bb18f22955e41cd5f03b","Query answering over Description Logic (DL) ontologies has become a vibrant field of research. Efficient realizations often exploit database technology and rewrite a given query to an equivalent SQL or Datalog query over a database associated with the ontology. This approach has been intensively studied for conjunctive query answering in the DL-Lite and EL families, but is much less explored for more expressive DLs and queries. We present a rewriting-based algorithm for conjunctive query answering over Horn-SHIQ ontologies, possibly extended with recursive rules under limited recursion as in DL+log. This setting not only subsumes both DL-Lite and EL, but also yields an algorithm for answering (limited) recursive queries over Horn-SHIQ ontologies (an undecidable problem for full recursive queries). A prototype implementation shows its potential for applications, as experiments exhibit efficient query answering over full Horn-SHIQ ontologies and benign downscaling to DL-Lite, where it is competitive with comparable state of the art systems. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Conjunctive queries; Database technology; Datalog; Description logic; Down-scaling; Prototype implementations; Query answering; Query rewritings; Recursions; Recursive rules; State-of-the-art system; Algorithms; Artificial intelligence; Data description; Query processing",Conference Paper,Scopus,2-s2.0-84868294276
"Linker R., Cohen O., Naor A.","Determination of the number of green apples in RGB images recorded in orchards",2012,"Computers and Electronics in Agriculture",68,10.1016/j.compag.2011.11.007,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-83455229423&doi=10.1016%2fj.compag.2011.11.007&partnerID=40&md5=de4099acda0fafc2bf700a6f1de3fa3e","This work details the development and validation of an algorithm for estimating the number of apples in color images acquired in orchards under natural illumination. Ultimately, this algorithm is intended to enable estimation of the orchard yield and be part of a management decision support system. The algorithm includes four main steps: detection of pixels that have a high probability of belonging to apples, using color and smoothness; formation and extension of ""seed areas"", which are connected sets of pixels that have a high probability of belonging to apples; segmentation of the contours of these seed areas into arcs and amorphous segments; and combination of these arcs and comparison of the resulting circle with a simple model of an apple. The performance of the algorithm is investigated using two datasets. The first dataset consists of images recorded in full automatic mode of the camera and under various lighting conditions. Although the algorithm detects correctly more than 85% of the apples visible in the images, direct illumination and color saturation cause a large number of false positive detections. The second dataset consists of images that were manually underexposed and recorded under mostly diffusive light (close to sunset). For such images the correct detection rate is close to 95% while the false positive detection rate is less than 5%. © 2011 Elsevier B.V.","Computer vision; Fruit recognition; Image processing","Automatic mode; Color images; Color saturation; Connected sets; Data sets; Detection rates; Direct illumination; False positive detection; Fruit recognition; High probability; Lighting conditions; Management decisions; Natural illumination; RGB images; Algorithms; Artificial intelligence; Color; Computer vision; Decision support systems; Image processing; Orchards; Pixels; Fruits; algorithm; color; computer simulation; data set; deciduous tree; estimation method; image processing; pixel; saturation; segmentation; Malus x domestica",Article,Scopus,2-s2.0-83455229423
"Bahrololoum A., Nezamabadi-Pour H., Bahrololoum H., Saeed M.","A prototype classifier based on gravitational search algorithm",2012,"Applied Soft Computing Journal",68,10.1016/j.asoc.2011.10.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84655169660&doi=10.1016%2fj.asoc.2011.10.008&partnerID=40&md5=530a2dae8c15e0290fd20106dba6e408","In recent years, heuristic algorithms have been successfully applied to solve clustering and classification problems. In this paper, gravitational search algorithm (GSA) which is one of the newest swarm based heuristic algorithms is used to provide a prototype classifier to face the classification of instances in multi-class data sets. The proposed method employs GSA as a global searcher to find the best positions of the representatives (prototypes). The proposed GSA-based classifier is used for data classification of some of the well-known benchmark sets. Its performance is compared with the artificial bee colony (ABC), the particle swarm optimization (PSO), and nine other classifiers from the literature. The experimental results of twelve data sets from UCI machine learning repository confirm that the GSA can successfully be applied as a classifier to classification problems. © 2011 Elsevier B.V. All rights reserved.","Classification; Gravitational search algorithm; Prototype classifier; Swarm intelligence; UCI machine learning repository","Artificial bee colonies; Best position; Data classification; Data sets; Gravitational search algorithm; Multi-class; Prototype classifier; Prototype classifiers; Search Algorithms; Swarm Intelligence; UCI machine learning repository; Artificial intelligence; Cellular automata; Classification (of information); Heuristic algorithms; Learning algorithms; Learning systems; Particle swarm optimization (PSO); Clustering algorithms",Article,Scopus,2-s2.0-84655169660
"Moreno-Torres J.G., Saez J.A., Herrera F.","Study on the impact of partition-induced dataset shift on k-fold cross-validation",2012,"IEEE Transactions on Neural Networks and Learning Systems",67,10.1109/TNNLS.2012.2199516,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876917722&doi=10.1109%2fTNNLS.2012.2199516&partnerID=40&md5=df4ee3cc13601e01a9ea6746152c082d","Cross-validation is a very commonly employed technique used to evaluate classifier performance. However, it can potentially introduce dataset shift, a harmful factor that is often not taken into account and can result in inaccurate performance estimation. This paper analyzes the prevalence and impact of partition-induced covariate shift on different k-fold cross-validation schemes. From the experimental results obtained, we conclude that the degree of partition-induced covariate shift depends on the cross-validation scheme considered. In this way, worse schemes may harm the correctness of a single-classifier performance estimation and also increase the needed number of repetitions of cross-validation to reach a stable performance estimation. © 2012 IEEE.","Covariate shift; cross-validation; dataset shift; partitioning","Classifier performance; Covariate shifts; Cross validation; Dataset shifts; K fold cross validations; partitioning; Performance estimation; Stable performance; Artificial intelligence; Computer networks; Estimation",Article,Scopus,2-s2.0-84876917722
"Fan J., Shen X., Wu Y.","Scribble tracker: A matting-based approach for robust tracking",2012,"IEEE Transactions on Pattern Analysis and Machine Intelligence",67,10.1109/TPAMI.2011.257,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862644870&doi=10.1109%2fTPAMI.2011.257&partnerID=40&md5=c33ef1d34faa7d196665aec2f87f2ffe","Model updating is a critical problem in tracking. Inaccurate extraction of the foreground and background information in model adaptation would cause the model to drift and degrade the tracking performance. The most direct yet difficult solution to the drift problem is to obtain accurate boundaries of the target. We approach such a solution by proposing a novel model adaptation framework based on the combination of matting and tracking. In our framework, coarse tracking results automatically provide sufficient and accurate scribbles for matting, which makes matting applicable in a tracking system. Meanwhile, accurate boundaries of the target can be obtained from matting results even when the target has large deformation. An effective model combining short-term features and long-term appearances is further constructed and successfully updated based on such accurate boundaries. The model can successfully handle occlusion by explicit inference. Extensive experiments show that our adaptation scheme largely avoids model drift and significantly outperforms other discriminative tracking models. © 2012 IEEE.","matting; model updating; video analysis; Visual tracking","Adaptation scheme; Background information; Critical problems; Drift problem; Large deformations; matting; Model Adaptation; Model drift; Model updating; Robust tracking; Tracking models; Tracking performance; Tracking system; Video analysis; Visual Tracking; Artificial intelligence; Computer vision",Article,Scopus,2-s2.0-84862644870
"Lin Y., Zhang J., Chung H.S.-H., Ip W.H., Li Y., Shi Y.-H.","An ant colony optimization approach for maximizing the lifetime of heterogeneous wireless sensor networks",2012,"IEEE Transactions on Systems, Man and Cybernetics Part C: Applications and Reviews",67,10.1109/TSMCC.2011.2129570,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860214713&doi=10.1109%2fTSMCC.2011.2129570&partnerID=40&md5=1d502c4a579e466f684da837f51611a2","Maximizing the lifetime of wireless sensor networks (WSNs) is a challenging problem. Although some methods exist to address the problem in homogeneous WSNs, research on this problem in heterogeneous WSNs have progressed at a slow pace. Inspired by the promising performance of ant colony optimization (ACO) to solve combinatorial problems, this paper proposes an ACO-based approach that can maximize the lifetime of heterogeneous WSNs. The methodology is based on finding the maximum number of disjoint connected covers that satisfy both sensing coverage and network connectivity. A construction graph is designed with each vertex denoting the assignment of a device in a subset. Based on pheromone and heuristic information, the ants seek an optimal path on the construction graph to maximize the number of connected covers. The pheromone serves as a metaphor for the search experiences in building connected covers. The heuristic information is used to reflect the desirability of device assignments. A local search procedure is designed to further improve the search efficiency. The proposed approach has been applied to a variety of heterogeneous WSNs. The results show that the approach is effective and efficient in finding high-quality solutions for maximizing the lifetime of heterogeneous WSNs. © 2012 IEEE.","Ant colony optimization (ACO); connectivity; coverage; network lifetime; wireless sensor networks (WSNs)","Ant Colony Optimization (ACO); connectivity; coverage; Network lifetime; Wireless sensor network (WSNs); Algorithms; Artificial intelligence; Graph theory; Wireless sensor networks",Article,Scopus,2-s2.0-84860214713
"Qin J., Zheng W.X., Gao H.","Coordination of multiple agents with double-integrator dynamics under generalized interaction topologies",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",67,10.1109/TSMCB.2011.2164523,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856277424&doi=10.1109%2fTSMCB.2011.2164523&partnerID=40&md5=386f6878a47a56c75554aff8a0e23556","The problem of the convergence of the consensus strategies for multiple agents with double-integrator dynamics is studied in this paper. The investigation covers two kinds of different settings. In the setting with the interaction topologies for the position and velocity information flows being modeled by different graphs, some sufficient conditions on the fixed interaction topologies are derived for the agents to reach consensus. In the setting with the interaction topologies for the position and velocity information flows being modeled by the same graph, we systematically investigate the consensus algorithm for the agents under both fixed and dynamically changing directed interaction topologies. Specifically, for the fixed case, a necessary and sufficient condition on the interaction topology is established for the agents to reach (average) consensus under certain assumptions. For the dynamically changing case, some sufficient conditions are obtained for the agents to reach consensus, where the condition imposed on the dynamical topologies is shown to be more relaxed than that required in the existing literature. Finally, we demonstrate the usefulness of the theoretical findings through some numerical examples. © 2011 IEEE.","Consensus strategy; double-integrator dynamics; dynamic topology; graph theory; multiple agents","Consensus algorithms; Consensus strategy; dynamic topology; Generalized interactions; Multiple agents; Numerical example; Sufficient conditions; Velocity information; Graph theory; Dynamics; algorithm; article; artificial intelligence; automated pattern recognition; computer simulation; decision support system; methodology; theoretical model; Algorithms; Artificial Intelligence; Computer Simulation; Decision Support Techniques; Models, Theoretical; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84856277424
"Su W., Chow M.-Y.","Computational intelligence-based energy management for a large-scale PHEV/PEV enabled municipal parking deck",2012,"Applied Energy",67,10.1016/j.apenergy.2011.11.088,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861662767&doi=10.1016%2fj.apenergy.2011.11.088&partnerID=40&md5=6129e62122b797daf60e78f9235436f1","There is a growing need to address the potential problems caused by the emergence of Plug-in Hybrid Electric Vehicles (PHEVs) and Plug-in Electric Vehicles (PEVs) within the next 10. years. In the near future, a large number of PHEVs/PEVs in our society will add a large-scale energy load to our power grids, as well as add substantial energy resources that can be utilized. The large penetration of these vehicles into the marketplace poses a potential threat to the existing power grid. The existing parking infrastructure is not ready for the large penetration of plug-in vehicles and the high demand of electricity. Nowadays, the advanced computational intelligence methods can be applied to solve large-scale optimization problems in a Smart Grid environment. In this paper, authors propose and implement a suite of computational intelligence-based algorithms (e.g., Estimation of Distribution Algorithm, Particle Swarm Optimization) for optimally managing a large number of PHEVs/PEVs charging at a municipal parking station. Authors characterize the performance of the proposed methods using a Matlab simulation, and compare it with other optimization techniques. © 2011 Elsevier Ltd.","Electric Vehicle (EV); Estimation of Distribution Algorithm (EDA); Particle Swarm Optimization (PSO); Plug-in Electric Vehicle (PEV); Plug-in Hybrid Electric Vehicle (PHEV); Smart Grid","Artificial intelligence; Electric power distribution; Electric power transmission networks; Electric vehicles; Energy management; Energy resources; MATLAB; Parks; Computational intelligence methods; Estimation of distribution algorithms; High demand; Large penetrations; Large-scale optimization; Matlab simulations; Optimization techniques; Parking deck; Plug-in hybrid electric vehicles; Plug-ins; Plugin hybrid electric vehicles (PHEV); Potential problems; Potential threats; Power grids; Smart grid; Substantial energy; Particle swarm optimization (PSO); algorithm; electric vehicle; intelligent transportation system; optimization; performance assessment",Article,Scopus,2-s2.0-84861662767
"Epitropakis M.G., Plagianakos V.P., Vrahatis M.N.","Evolving cognitive and social experience in Particle Swarm Optimization through Differential Evolution: A hybrid approach",2012,"Information Sciences",66,10.1016/j.ins.2012.05.017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864659396&doi=10.1016%2fj.ins.2012.05.017&partnerID=40&md5=77900f80972bc4ba87becde4c5d4ec41","In recent years, the Particle Swarm Optimization has rapidly gained increasing popularity and many variants and hybrid approaches have been proposed to improve it. In this paper, motivated by the behavior and the spatial characteristics of the social and cognitive experience of each particle in the swarm, we develop a hybrid framework that combines the Particle Swarm Optimization and the Differential Evolution algorithm. Particle Swarm Optimization has the tendency to distribute the best personal positions of the swarm particles near to the vicinity of problem's optima. In an attempt to efficiently guide the evolution and enhance the convergence, we evolve the personal experience or memory of the particles with the Differential Evolution algorithm, without destroying the search capabilities of the algorithm. The proposed framework can be applied to any Particle Swarm Optimization algorithm with minimal effort. To evaluate the performance and highlight the different aspects of the proposed framework, we initially incorporate six classic Differential Evolution mutation strategies in the canonical Particle Swarm Optimization, while afterwards we employ five state-of-the-art Particle Swarm Optimization variants and four popular Differential Evolution algorithms. Extensive experimental results on 25 high dimensional multimodal benchmark functions along with the corresponding statistical analysis, suggest that the hybrid variants are very promising and significantly improve the original algorithms in the majority of the studied cases. © 2012 Elsevier Inc. All rights reserved.","Differential Evolution; Global Optimization; Hybrid approach; Particle Swarm Optimization; Social and cognitive experience; Swarm intelligence","Differential Evolution; Differential evolution algorithms; High-dimensional; Hybrid approach; Hybrid framework; Multimodal benchmark; Mutation strategy; Original algorithms; Particle swarm optimization algorithm; Personal experience; Search capabilities; Social and cognitive experience; Spatial characteristics; Swarm Intelligence; Algorithms; Artificial intelligence; Global optimization; Particle swarm optimization (PSO)",Article,Scopus,2-s2.0-84864659396
"Song J., Tan H., Perry A.J., Akutsu T., Webb G.I., Whisstock J.C., Pike R.N.","PROSPER: An Integrated Feature-Based Tool for Predicting Protease Substrate Cleavage Sites",2012,"PLoS ONE",66,10.1371/journal.pone.0050300,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870484133&doi=10.1371%2fjournal.pone.0050300&partnerID=40&md5=47d1d39e2c260001184039cf2095865e","The ability to catalytically cleave protein substrates after synthesis is fundamental for all forms of life. Accordingly, site-specific proteolysis is one of the most important post-translational modifications. The key to understanding the physiological role of a protease is to identify its natural substrate(s). Knowledge of the substrate specificity of a protease can dramatically improve our ability to predict its target protein substrates, but this information must be utilized in an effective manner in order to efficiently identify protein substrates by in silico approaches. To address this problem, we present PROSPER, an integrated feature-based server for in silico identification of protease substrates and their cleavage sites for twenty-four different proteases. PROSPER utilizes established specificity information for these proteases (derived from the MEROPS database) with a machine learning approach to predict protease cleavage sites by using different, but complementary sequence and structure characteristics. Features used by PROSPER include local amino acid sequence profile, predicted secondary structure, solvent accessibility and predicted native disorder. Thus, for proteases with known amino acid specificity, PROSPER provides a convenient, pre-prepared tool for use in identifying protein substrates for the enzymes. Systematic prediction analysis for the twenty-four proteases thus far included in the database revealed that the features we have included in the tool strongly improve performance in terms of cleavage site prediction, as evidenced by their contribution to performance improvement in terms of identifying known cleavage sites in substrates for these enzymes. In comparison with two state-of-the-art prediction tools, PoPS and SitePrediction, PROSPER achieves greater accuracy and coverage. To our knowledge, PROSPER is the first comprehensive server capable of predicting cleavage sites of multiple proteases within a single substrate sequence using machine learning techniques. It is freely available at http://lightning.med.monash.edu.au/PROSPER. © 2012 Song et al.",,"proteinase; article; comparative study; computer model; enzyme assay; enzyme structure; enzyme substrate; prediction; protein cleavage; protein database; protein secondary structure; sequence analysis; structure analysis; support vector machine; web browser; Algorithms; Animals; Artificial Intelligence; Catalysis; Cattle; Computational Biology; Granzymes; Humans; Hydrolysis; Mice; Models, Statistical; Peptide Hydrolases; Peptides; Protein Binding; Protein Conformation; Protein Processing, Post-Translational; Proteins; ROC Curve; Software; Solvents; Substrate Specificity",Article,Scopus,2-s2.0-84870484133
"Barinova O., Lempitsky V., Kholi P.","On detection of multiple object instances using hough transforms",2012,"IEEE Transactions on Pattern Analysis and Machine Intelligence",66,10.1109/TPAMI.2012.79,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865611811&doi=10.1109%2fTPAMI.2012.79&partnerID=40&md5=acaef9712ec6ec69aa56c859eb25c7ca","Hough transform-based methods for detecting multiple objects use nonmaxima suppression or mode seeking to locate and distinguish peaks in Hough images. Such postprocessing requires the tuning of many parameters and is often fragile, especially when objects are located spatially close to each other. In this paper, we develop a new probabilistic framework for object detection which is related to the Hough transform. It shares the simplicity and wide applicability of the Hough transform but, at the same time, bypasses the problem of multiple peak identification in Hough images and permits detection of multiple objects without invoking nonmaximum suppression heuristics. Our experiments demonstrate that this method results in a significant improvement in detection accuracy both for the classical task of straight line detection and for a more modern category-level (pedestrian) detection problem. © 2012 IEEE.","Hough transforms; line detection; object detection in images; scene understanding.","Detection accuracy; Detection problems; Line detection; Mode seeking; Multiple objects; Multiple-peak; Non-maximum suppression; Nonmaxima suppression; Object Detection; Probabilistic framework; Scene understanding; Straight line detection; Transform-based methods; Feature extraction; Object recognition; Hough transforms; algorithm; article; artificial intelligence; automated pattern recognition; human; image processing; methodology; walking; Algorithms; Artificial Intelligence; Humans; Image Processing, Computer-Assisted; Pattern Recognition, Automated; Walking",Article,Scopus,2-s2.0-84865611811
"Sideratos G., Hatziargyriou N.D.","Probabilistic wind power forecasting using radial basis function neural networks",2012,"IEEE Transactions on Power Systems",66,10.1109/TPWRS.2012.2187803,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867988966&doi=10.1109%2fTPWRS.2012.2187803&partnerID=40&md5=6203bdb3646aee794bdb2166414db56e","A novel methodology for probabilistic wind power forecasting is described. The method is based on artificial intelligence and concentrates on the uncertainty information about the future wind power production predicting a set of quantiles with predefined nominal probabilities. The proposed model uses the point predictions of an existing state-of-the-art wind power forecasting model and forecasts the prediction uncertainties due to the inaccuracies of the numerical weather predictions (NWP), the weather stability and the deterministic forecasting model. The performance of the proposed model is evaluated on two wind farms that are located in areas with different weather conditions. © 1969-2012 IEEE.","Probabilistic wind power forecasting; radial basis function neural network; self-organized map","Deterministic forecasting; Novel methodology; Numerical weather prediction; Prediction uncertainty; Radial basis function neural networks; Self-organized map; Uncertainty informations; Weather conditions; Weather stability; Wind farm; Wind power forecasting; Wind power production; Artificial intelligence; Electric utilities; Wind power; Weather forecasting",Article,Scopus,2-s2.0-84867988966
"Glocker B., Feulner J., Criminisi A., Haynor D.R., Konukoglu E.","Automatic localization and identification of vertebrae in arbitrary field-of-view CT scans",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",66,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872956864&partnerID=40&md5=df927f13b0bd2c787e96daebf34ca3db","This paper presents a new method for automatic localization and identification of vertebrae in arbitrary field-of-view CT scans. No assumptions are made about which section of the spine is visible or to which extent. Thus, our approach is more general than previous work while being computationally efficient. Our algorithm is based on regression forests and probabilistic graphical models. The discriminative, regression part aims at roughly detecting the visible part of the spine. Accurate localization and identification of individual vertebrae is achieved through a generative model capturing spinal shape and appearance. The system is evaluated quantitatively on 200 CT scans, the largest dataset reported for this purpose. We obtain an overall median localization error of less than 6mm, with an identification rate of 81%. © Springer-Verlag Berlin Heidelberg 2012.",,"Medical computing; Medical imaging; Automatic localization; Computationally efficient; Generative model; Identification rates; Localization and identification; Localization errors; Probabilistic graphical models; Regression forests; Computerized tomography; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; computer assisted tomography; human; image quality; methodology; radiography; reproducibility; sensitivity and specificity; spine; Algorithms; Artificial Intelligence; Humans; Pattern Recognition, Automated; Radiographic Image Enhancement; Radiographic Image Interpretation, Computer-Assisted; Reproducibility of Results; Sensitivity and Specificity; Spine; Tomography, X-Ray Computed",Conference Paper,Scopus,2-s2.0-84872956864
"Zhang T., Szlam A., Wang Y., Lerman G.","Hybrid linear modeling via local best-fit flats",2012,"International Journal of Computer Vision",65,10.1007/s11263-012-0535-6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867053797&doi=10.1007%2fs11263-012-0535-6&partnerID=40&md5=acbb1bc33c05eb980421fc34ca357cec","We present a simple and fast geometric method for modeling data by a union of affine subspaces. The method begins by forming a collection of local best-fit affine subspaces, i.e., subspaces approximating the data in local neighborhoods. The correct sizes of the local neighborhoods are determined automatically by the Jones' β 2 numbers (we prove under certain geometric conditions that our method finds the optimal local neighborhoods). The collection of subspaces is further processed by a greedy selection procedure or a spectral method to generate the final model. We discuss applications to tracking-based motion segmentation and clustering of faces under different illuminating conditions. We give extensive experimental evidence demonstrating the state of the art accuracy and speed of the suggested algorithms on these problems and also on synthetic hybrid linear data as well as the MNIST handwritten digits data; and we demonstrate how to use our algorithms for fast determination of the number of affine subspaces. © 2012 The Author(s).","Face clustering; High-dimensional data; Hybrid linear modeling; Local PCA; Motion segmentation; Spectral clustering; Subspace clustering","Face clustering; High dimensional data; Linear modeling; Local PCA; Motion segmentation; Spectral clustering; Subspace clustering; Artificial intelligence; Software engineering; Clustering algorithms",Article,Scopus,2-s2.0-84867053797
"Qi Z., Tian Y., Shi Y.","Twin support vector machine with Universum data",2012,"Neural Networks",65,10.1016/j.neunet.2012.09.004,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867356258&doi=10.1016%2fj.neunet.2012.09.004&partnerID=40&md5=5f836211d6b3f4405b5281e66ca6137b","The Universum, which is defined as the sample not belonging to either class of the classification problem of interest, has been proved to be helpful in supervised learning. In this work, we designed a new Twin Support Vector Machine with Universum (called U-TSVM), which can utilize Universum data to improve the classification performance of TSVM. Unlike U-SVM, in U-TSVM, Universum data are located in a nonparallel insensitive loss tube by using two Hinge Loss functions, which can exploit these prior knowledge embedded in Universum data more flexible. Empirical experiments demonstrate that U-TSVM can directly improve the classification accuracy of standard TSVM that use the labeled data alone and is superior to U-SVM in most cases. © 2012 Elsevier Ltd.","Classification; Twin support vector machine; Universum","Classification accuracy; Classification performance; Empirical experiments; Labeled data; Loss functions; Prior knowledge; Twin support vector machines; Universum; Artificial intelligence; Classification (of information); Cognitive systems; Support vector machines; accuracy; article; classification algorithm; data analysis; machine learning; nonlinear system; priority journal; support vector machine; twin support vector machine; universum data; Classification; Computer Systems; Databases as Topic; Image Processing, Computer-Assisted; Motion; Neural Networks (Computer); Support Vector Machines",Article,Scopus,2-s2.0-84867356258
"Vihinen M.","How to evaluate performance of prediction methods? Measures and their interpretation in variation effect analysis.",2012,"BMC genomics",65,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868026028&partnerID=40&md5=8953c48b6cf2606f600f84c732b045a7","Prediction methods are increasingly used in biosciences to forecast diverse features and characteristics. Binary two-state classifiers are the most common applications. They are usually based on machine learning approaches. For the end user it is often problematic to evaluate the true performance and applicability of computational tools as some knowledge about computer science and statistics would be needed. Instructions are given on how to interpret and compare method evaluation results. For systematic method performance analysis is needed established benchmark datasets which contain cases with known outcome, and suitable evaluation measures. The criteria for benchmark datasets are discussed along with their implementation in VariBench, benchmark database for variations. There is no single measure that alone could describe all the aspects of method performance. Predictions of genetic variation effects on DNA, RNA and protein level are important as information about variants can be produced much faster than their disease relevance can be experimentally verified. Therefore numerous prediction tools have been developed, however, systematic analyses of their performance and comparison have just started to emerge. The end users of prediction tools should be able to understand how evaluation is done and how to interpret the results. Six main performance evaluation measures are introduced. These include sensitivity, specificity, positive predictive value, negative predictive value, accuracy and Matthews correlation coefficient. Together with receiver operating characteristics (ROC) analysis they provide a good picture about the performance of methods and allow their objective and quantitative comparison. A checklist of items to look at is provided. Comparisons of methods for missense variant tolerance, protein stability changes due to amino acid substitutions, and effects of variations on mRNA splicing are presented.",,"messenger RNA; algorithm; article; artificial intelligence; biology; genetics; methodology; predictive value; protein stability; Algorithms; Artificial Intelligence; Computational Biology; Predictive Value of Tests; Protein Stability; RNA, Messenger",Article,Scopus,2-s2.0-84868026028
"Li Q., Clifford G.D.","Dynamic time warping and machine learning for signal quality assessment of pulsatile signals",2012,"Physiological Measurement",65,10.1088/0967-3334/33/9/1491,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866315935&doi=10.1088%2f0967-3334%2f33%2f9%2f1491&partnerID=40&md5=5640d35a3b827bc4ba9766f9533c42c5","In this work, we describe a beat-by-beat method for assessing the clinical utility of pulsatile waveforms, primarily recorded from cardiovascular blood volume or pressure changes, concentrating on the photoplethysmogram (PPG). Physiological blood flow is nonstationary, with pulses changing in height, width and morphology due to changes in heart rate, cardiac output, sensor type and hardware or software pre-processing requirements. Moreover, considerable inter-individual and sensor-location variability exists. Simple template matching methods are therefore inappropriate, and a patient-specific adaptive initialization is therefore required. We introduce dynamic time warping to stretch each beat to match a running template and combine it with several other features related to signal quality, including correlation and the percentage of the beat that appeared to be clipped. The features were then presented to a multi-layer perceptron neural network to learn the relationships between the parameters in the presence of good- and bad-quality pulses. An expert-labeled database of 1055 segments of PPG, each 6s long, recorded from 104 separate critical care admissions during both normal and verified arrhythmic events, was used to train and test our algorithms. An accuracy of 97.5% on the training set and 95.2% on test set was found. The algorithm could be deployed as a stand-alone signal quality assessment algorithm for vetting the clinical utility of PPG traces or any similar quasi-periodic signal. © 2012 Institute of Physics and Engineering in Medicine.","artificial neural network; dynamic time warping; machine learning; multi-layer perceptron; photoplethysmograph; pulsatile signal; signal quality assessment","algorithm; article; artificial intelligence; human; photoelectric plethysmography; quality control; standard; time; Algorithms; Artificial Intelligence; Humans; Photoplethysmography; Quality Control; Time Factors",Article,Scopus,2-s2.0-84866315935
"Asami T., Bouix S., Whitford T.J., Shenton M.E., Salisbury D.F., McCarley R.W.","Longitudinal loss of gray matter volume in patients with first-episode schizophrenia: DARTEL automated analysis and ROI validation",2012,"NeuroImage",65,10.1016/j.neuroimage.2011.08.066,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-83055194331&doi=10.1016%2fj.neuroimage.2011.08.066&partnerID=40&md5=289d12ce11aab2b714ee931ee055cabc","Region of Interest (ROI) longitudinal studies have detected progressive gray matter (GM) volume reductions in patients with first-episode schizophrenia (FESZ). However, there are only a few longitudinal voxel-based morphometry (VBM) studies, and these have been limited in ability to detect relationships between volume loss and symptoms, perhaps because of methodologic issues. Nor have previous studies compared and validated VBM results with manual Region of Interest (ROI) analysis. In the present VBM study, high-dimensional warping and individualized baseline-rescan templates were used to evaluate longitudinal volume changes within subjects and compared with longitudinal manual ROI analysis on the same subjects. VBM evaluated thirty-three FESZ and thirty-six matched healthy control subjects (HC) at baseline (cross-sectionally) and longitudinally evaluated 21 FESZ and 23 HC after an average of 1.5. years from baseline scans. Correlation analyses detected the relationship between changes in regional GM volumes in FESZ and clinical symptoms derived from the Brief Psychiatric Rating Scale, as well as cognitive function as assessed by the Mini-Mental State Examination. At baseline, patients with FESZ had significantly smaller GM volume compared to HC in some regions including the left superior temporal gyrus (STG). On rescan after 1.5. years, patients showed significant GM volume reductions compared with HC in the left STG including Heschl's gyrus, and in widespread brain neocortical regions of frontal, parietal, and limbic regions including the cingulate gyrus. FESZ showed an association of positive symptoms and volume loss in temporal (especially STG) and frontal regions, and negative symptoms and volume loss in STG and frontal regions. Worse cognitive function was linked to widespread volume reduction, in frontal, temporal and parietal regions. The validation VBM analyses showed results similar to our previous ROI findings for STG and cingulate gyrus. We conclude FESZ show widespread, progressive GM volume reductions in many brain regions. Importantly, these reductions are directly associated with a worse clinical course. Congruence with ROI analyses suggests the promise of this longitudinal VBM methodology. © 2011.","Cognitive function; First episode; Longitudinal study; Negative symptom; Positive symptom; VBM","adult; article; autoanalysis; brain region; brain size; Brief Psychiatric Rating Scale; cingulate gyrus; clinical article; clinical feature; cognition; controlled study; cross-sectional study; disease association; disease duration; female; first episode schizophrenia; frontal cortex; gray matter; human; image analysis; limbic system; male; mini mental state examination; negative syndrome; neuropathology; onset age; parietal cortex; pathological anatomy; positive syndrome; priority journal; radiological parameters; region of interest; schizophrenia; superior temporal gyrus; validation study; voxel based morphometry; Adolescent; Adult; Algorithms; Artificial Intelligence; Female; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Longitudinal Studies; Magnetic Resonance Imaging; Male; Middle Aged; Neurons; Pattern Recognition, Automated; Reproducibility of Results; ROC Curve; Schizophrenia; Sensitivity and Specificity; Young Adult",Article,Scopus,2-s2.0-83055194331
"Bustince H., Pagola M., Mesiar R., Hüllermeier E., Herrera F.","Grouping, overlap, and generalized bientropic functions for fuzzy modeling of pairwise comparisons",2012,"IEEE Transactions on Fuzzy Systems",64,10.1109/TFUZZ.2011.2173581,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881359430&doi=10.1109%2fTFUZZ.2011.2173581&partnerID=40&md5=1fb75e8827c2d33830d35665bd8ca73e","In this paper, we propose new aggregation functions for the pairwise comparison of alternatives in fuzzy preference modeling. More specifically,we introduce the concept of a grouping function, i.e., a specific type of aggregation function that combines two degrees of support (weak preference) into a degree of information or, say, a degree of comparability between two alternatives, andwe relate this new concept to that of incomparability. Grouping functions of this type complement the existing concept of overlap functions in a natural way, since the latter can be used to turn two degrees of weak preference into a degree of indifference. We also define the so-called generalized bientropic functions that allow for a unified representation of overlap and grouping functions. Apart from analyzingmathematical properties of these types of functions and exploring relationships between them, we elaborate on their use in fuzzy preference modeling and decision making.We present an algorithm to elaborate on an alternative preference ranking that penalizes those alternatives for which the expert is not sure of his/her preference. © 2012 IEEE.","Decision making; Generalized bientropic function; Grouping function; Incomparability; Overlap function; Pairwise comparison; Preference relations","Aggregation functions; Fuzzy modeling; Fuzzy preference modeling; Incomparability; Overlap functions; Pair-wise comparison; Preference ranking; Preference relation; Artificial intelligence; Decision making; Fuzzy sets",Article,Scopus,2-s2.0-84881359430
"Scholler S., Bosse S., Treder M.S., Blankertz B., Curio G., Müller K.-R., Wiegand T.","Toward a direct measure of video quality perception using EEG",2012,"IEEE Transactions on Image Processing",64,10.1109/TIP.2012.2187672,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860112691&doi=10.1109%2fTIP.2012.2187672&partnerID=40&md5=03fb8d3c6fd723a0bdd3fca1edcda04c","An approach to the direct measurement of perception of video quality change using electroencephalography (EEG) is presented. Subjects viewed 8-s video clips while their brain activity was registered using EEG. The video signal was either uncompressed at full length or changed from uncompressed to a lower quality level at a random time point. The distortions were introduced by a hybrid video codec. Subjects had to indicate whether they had perceived a quality change. In response to a quality change, a positive voltage change in EEG (the so-called P3 component) was observed at latency of about 400-600 ms for all subjects. The voltage change positively correlated with the magnitude of the video quality change, substantiating the P3 component as a graded neural index of the perception of video quality change within the presented paradigm. By applying machine learning techniques, we could classify on a single-trial basis whether a subject perceived a quality change. Interestingly, some video clips wherein changes were missed (i.e., not reported) by the subject were classified as quality changes, suggesting that the brain detected a change, although the subject did not press a button. In conclusion, abrupt changes of video quality give rise to specific components in the EEG that can be detected on a single-trial basis. Potentially, a neurotechnological approach to video assessment could lead to a more objective quantification of quality change detection, overcoming the limitations of subjective approaches (such as subjective bias and the requirement of an overt response). Furthermore, it allows for real-time applications wherein the brain response to a video clip is monitored while it is being viewed. © 1992-2012 IEEE.","Electroencephalography (EEG); perception; video coding; video quality","Brain activity; Brain response; Direct measurement; Direct measures; Electroencephalographies (EEG); Machine learning techniques; Positive voltage; Quality change; Quality levels; Real-time application; Specific component; Time points; Video assessment; Video clips; Video codecs; Video quality; Video signal; Voltage change; Electrophysiology; Image coding; Sensory perception; Video cameras; Video signal processing; Electroencephalography; adult; article; artificial intelligence; brain; computer assisted diagnosis; electroencephalography; evoked visual response; female; human; male; methodology; physiology; reproducibility; sensitivity and specificity; videorecording; vision; vision test; Adult; Artificial Intelligence; Brain; Electroencephalography; Evoked Potentials, Visual; Female; Humans; Image Interpretation, Computer-Assisted; Male; Reproducibility of Results; Sensitivity and Specificity; Video Recording; Vision Tests; Visual Perception",Article,Scopus,2-s2.0-84860112691
"Barros D.J.F., Wilson S.K., Kahn J.M.","Comparison of orthogonal frequency-division multiplexing and pulse-amplitude modulation in indoor optical wireless links",2012,"IEEE Transactions on Communications",64,10.1109/TCOMM.2011.112311.100538,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857358808&doi=10.1109%2fTCOMM.2011.112311.100538&partnerID=40&md5=22e2102439a93894b5df2beda64299ee","We evaluate the performance of three directdetection orthogonal frequency-division multiplexing (OFDM) schemes in combating multipath distortion in indoor optical wireless links, comparing them to unipolar M-ary pulseamplitude modulation (M-PAM) with minimum mean-square error decision-feedback equalization (MMSE-DFE). The three OFDM techniques are DC-clipped OFDM and asymmetrically clipped optical OFDM (ACO-OFDM) and PAM-modulated discrete multitone (PAM-DMT). We describe an iterative procedure to achieve optimal power allocation for DC-OFDM. For each modulation method, we quantify the received electrical SNR required at a given bit rate on a given channel, considering an ensemble of 170 indoor wireless channels. When using the same symbol rate for all modulation methods, M-PAM with MMSE-DFE has better performance than any OFDM format over a range of spectral efficiencies, with the advantage of (M-PAM) increasing at high spectral efficiency. ACO-OFDM and PAM-DMT have practically identical performance at any spectral efficiency. They are the best OFDM formats at low spectral efficiency, whereas DC-OFDM is best at high spectral efficiency. When ACO-OFDM or PAM-DMT are allowed to use twice the symbol rate of M-PAM, these OFDM formats have better performance than M-PAM. When channel state information is unavailable at the transmitter, however, M-PAM significantly outperforms all OFDM formats. When using the same symbol rate for all modulation methods, M-PAM requires approximately three times more computational complexity per processor than all OFDM formats and 63% faster analog-to-digital converters, assuming oversampling ratios of 1.23 and 2 for ACO-OFDM and M-PAM, respectively. When OFDM uses twice the symbol rate of M-PAM, OFDM requires 23% faster analog-to-digital converters than M-PAM but OFDM requires approximately 40% less computational complexity than M-PAM per processor. © 2012 IEEE.","ACO-OFDM; Communications system performance; Infrared wireless; Intensity modulation with direct detection; Multicarrier optical systems; OOK; Optical wireless; Orthogonal frequency-division multiplexing; PAM-DMT; Power allocation; Visible light communications","ACO-OFDM; Communications system performance; Infrared wireless; Intensity modulations; Multi-carrier optical systems; OOK; Optical wireless; PAM-DMT; Power allocations; Visible light communications; Artificial intelligence; Computational complexity; Dense wavelength division multiplexing; Modulation; Optical communication; Spectrum analyzers; Orthogonal frequency division multiplexing",Article,Scopus,2-s2.0-84857358808
"Abrahams A.S., Jiao J., Wang G.A., Fan W.","Vehicle defect discovery from social media",2012,"Decision Support Systems",63,10.1016/j.dss.2012.04.005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868668514&doi=10.1016%2fj.dss.2012.04.005&partnerID=40&md5=2725aba7824e4cf5cfdbae4971665b15","A pressing need of vehicle quality management professionals is decision support for the vehicle defect discovery and classification process. In this paper, we employ text mining on a popular social medium used by vehicle enthusiasts: online discussion forums. We find that sentiment analysis, a conventional technique for consumer complaint detection, is insufficient for finding, categorizing, and prioritizing vehicle defects discussed in online forums, and we describe and evaluate a new process and decision support system for automotive defect identification and prioritization. Our findings provide managerial insights into how social media analytics can improve automotive quality management. © 2012 Elsevier B.V. All rights reserved.","Business intelligence; Quality management; Social media analytics; Text mining","Classification process; Consumer complaints; Conventional techniques; Decision supports; Defect identification; Management professionals; Online discussions; Online forums; Prioritization; Sentiment analysis; Social media; Text mining; Vehicle defects; Vehicle quality; Artificial intelligence; Competitive intelligence; Data mining; Decision support systems; Defects; Quality management; Vehicles",Article,Scopus,2-s2.0-84868668514
"Cantley K.D., Subramaniam A., Stiegler H.J., Chapman R.A., Vogel E.M.","Neural learning circuits utilizing nano-crystalline silicon transistors and memristors",2012,"IEEE Transactions on Neural Networks and Learning Systems",63,10.1109/TNNLS.2012.2184801,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874044335&doi=10.1109%2fTNNLS.2012.2184801&partnerID=40&md5=5f50b3733a71ccf4ad71d477cc147fda","Properties of neural circuits are demonstrated via SPICE simulations and their applications are discussed. The neuron and synapse subcircuits include ambipolar nano-crystalline silicon transistor and memristor device models based on measured data. Neuron circuit characteristics and the Hebbian synaptic learning rule are shown to be similar to biology. Changes in the average firing rate learning rule depending on various circuit parameters are also presented. The subcircuits are then connected into larger neural networks that demonstrate fundamental properties including associative learning and pulse coincidence detection. Learned extraction of a fundamental frequency component from noisy inputs is demonstrated. It is then shown that if the fundamental sinusoid of one neuron input is out of phase with the rest, its synaptic connection changes differently than the others. Such behavior indicates that the system can learn to detect which signals are important in the general population, and that there is a spike-timing-dependent component of the learning mechanism. Finally, future circuit design and considerations are discussed, including requirements for the memristive device. © 2012 IEEE.","Hebbian learning; memristor; nano-crystalline silicon; neuromorphic; SPICE; thin-film transistor","Associative learning; Coincidence detection; Fundamental frequencies; Fundamental properties; Hebbian learning; Memristor; Neuromorphic; Synaptic connections; Coincidence circuits; Integrated circuit manufacture; Memristors; Neurons; Passive filters; Resistors; Thin film transistors; SPICE; animal; artificial intelligence; artificial neural network; biological model; biomimetics; computer aided design; computer simulation; data storage device; device failure analysis; devices; equipment design; human; impedance; nanotechnology; nerve cell network; physiology; semiconductor; Animals; Artificial Intelligence; Biomimetics; Computer Simulation; Computer Storage Devices; Computer-Aided Design; Electric Impedance; Equipment Design; Equipment Failure Analysis; Humans; Models, Neurological; Nanotechnology; Nerve Net; Neural Networks (Computer); Transistors, Electronic",Article,Scopus,2-s2.0-84874044335
"Li R., Chu T.","Complete synchronization of boolean networks",2012,"IEEE Transactions on Neural Networks and Learning Systems",63,10.1109/TNNLS.2012.2190094,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867336371&doi=10.1109%2fTNNLS.2012.2190094&partnerID=40&md5=9263bf8eaa3237550ecef20003b33b47","We examine complete synchronization of two deterministic Boolean networks (BNs) coupled unidirectionally in the drive-response configuration. A necessary and sufficient criterion is presented in terms of algebraic representations of BNs. As a consequence, we show that complete synchronization can occur only between two conditionally identical BNs when the transition matrix of the drive network is nonsingular. Two examples are worked out to illustrate the obtained results. © 2012 IEEE.","Boolean networks; complete synchronization; nonsingularity","Algebraic representations; Boolean Networks; Complete synchronization; Deterministic boolean networks; Nonsingular; Nonsingularity; Sufficient criterion; Transition matrices; Artificial intelligence; Computer networks; Algebra",Article,Scopus,2-s2.0-84867336371
"Iosifidis A., Tefas A., Pitas I.","View-invariant action recognition based on artificial neural networks",2012,"IEEE Transactions on Neural Networks and Learning Systems",63,10.1109/TNNLS.2011.2181865,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869778294&doi=10.1109%2fTNNLS.2011.2181865&partnerID=40&md5=3e1962ff2a8ca2cb37193194d5e90d63","In this paper, a novel view invariant action recognition method based on neural network representation and recognition is proposed. The novel representation of action videos is based on learning spatially related human body posture prototypes using self organizing maps. Fuzzy distances from human body posture prototypes are used to produce a time invariant action representation. Multilayer perceptrons are used for action classification. The algorithm is trained using data from a multi-camera setup. An arbitrary number of cameras can be used in order to recognize actions using a Bayesian framework. The proposed method can also be applied to videos depicting interactions between humans, without any modification. The use of information captured from different viewing angles leads to high classification performance. The proposed method is the first one that has been tested in challenging experimental setups, a fact that denotes its effectiveness to deal with most of the open issues in action recognition. © 2012 IEEE.","Bayesian frameworks; fuzzy vector quantization; human action recognition; multilayer perceptrons","Action classifications; Action representations; Bayesian frameworks; Classification performance; Fuzzy vector quantization; Human body postures; Human-action recognition; View-invariant action recognition; Classification (of information); Gesture recognition; Multilayer neural networks; Self organizing maps; Vector quantization; Motion estimation; artificial intelligence; artificial neural network; automated pattern recognition; Bayes theorem; human; procedures; trends; Artificial Intelligence; Bayes Theorem; Humans; Neural Networks (Computer); Pattern Recognition, Automated",Article,Scopus,2-s2.0-84869778294
"Lam H.K.","Stabilization of nonlinear systems using sampled-data output-feedback fuzzy controller based on polynomial-fuzzy-model-based control approach",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",62,10.1109/TSMCB.2011.2163796,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856237294&doi=10.1109%2fTSMCB.2011.2163796&partnerID=40&md5=c929a1df1bd4acffb856ba5909b5963d","This paper investigates the stability of sampled-data output-feedback (SDOF) polynomial-fuzzy-model-based control systems. Representing the nonlinear plant using a polynomial fuzzy model, an SDOF fuzzy controller is proposed to perform the control process using the system output information. As only the system output is available for feedback compensation, it is more challenging for the controller design and system analysis compared to the full-state-feedback case. Furthermore, because of the sampling activity, the control signal is kept constant by the zero-order hold during the sampling period, which complicates the system dynamics and makes the stability analysis more difficult. In this paper, two cases of SDOF fuzzy controllers, which either share the same number of fuzzy rules or not, are considered. The system stability is investigated based on the Lyapunov stability theory using the sum-of-squares (SOS) approach. SOS-based stability conditions are obtained to guarantee the system stability and synthesize the SDOF fuzzy controller. Simulation examples are given to demonstrate the merits of the proposed SDOF fuzzy control approach. © 2011 IEEE.","Nonlinear systems; output feedback; polynomial fuzzy system; polynomial-fuzzy- model-based (PFMB) control; sampled-data control; stability analysis; sum of squares (SOS)","Control approach; Control process; Control signal; Controller designs; Feedback compensation; Fuzzy controllers; Fuzzy models; Lyapunov stability theory; Nonlinear plant; Output feedback; Sampled-data; Sampled-data control; Sampling period; Simulation example; stability analysis; Stability condition; Sum of squares; System Dynamics; System output; Zero-order holds; Control system stability; Fuzzy control; Mathematical models; Nonlinear analysis; Nonlinear feedback; Nonlinear systems; Polynomials; Sampled data control systems; Stabilization; State feedback; Systems analysis; Systems engineering; Controllers; algorithm; article; artificial intelligence; computer simulation; feedback system; fuzzy logic; nonlinear system; signal processing; Algorithms; Artificial Intelligence; Computer Simulation; Feedback; Fuzzy Logic; Nonlinear Dynamics; Signal Processing, Computer-Assisted",Article,Scopus,2-s2.0-84856237294
"Zhang Y., Duc P.M., Corcho O., Calbimonte J.-P.","SRBench: A streaming RDF/SPARQL benchmark",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",61,10.1007/978-3-642-35176-1-40,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868571894&doi=10.1007%2f978-3-642-35176-1-40&partnerID=40&md5=ddbd9bd1efce1d2118a124bc7b3fe38d","We introduce SRBench, a general-purpose benchmark primarily designed for streaming RDF/SPARQL engines, completely based on real-world data sets from the Linked Open Data cloud. With the increasing problem of too much streaming data but not enough tools to gain knowledge from them, researchers have set out for solutions in which Semantic Web technologies are adapted and extended for publishing, sharing, analysing and understanding streaming data. To help researchers and users comparing streaming RDF/SPARQL (strRS) engines in a standardised application scenario, we have designed SRBench, with which one can assess the abilities of a strRS engine to cope with a broad range of use cases typically encountered in real-world scenarios. The data sets used in the benchmark have been carefully chosen, such that they represent a realistic and relevant usage of streaming data. The benchmark defines a concise, yet comprehensive set of queries that cover the major aspects of strRS processing. Finally, our work is complemented with a functional evaluation on three representative strRS engines: SPARQL Stream, C-SPARQL and CQELS. The presented results are meant to give a first baseline and illustrate the state-of-the-art. © 2012 Springer-Verlag Berlin Heidelberg.",,"Application scenario; Data clouds; Data sets; Functional evaluation; General-purpose benchmarks; Real world data; Real-world scenario; Semantic Web technology; Streaming data; Artificial intelligence; Virtual reality",Conference Paper,Scopus,2-s2.0-84868571894
"Kaufmann E., Korda N., Munos R.","Thompson sampling: An asymptotically optimal finite-time analysis",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",61,10.1007/978-3-642-34106-9_18,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867888479&doi=10.1007%2f978-3-642-34106-9_18&partnerID=40&md5=6bae20ca9704f6d2a75dd1e47ac0e967","The question of the optimality of Thompson Sampling for solving the stochastic multi-armed bandit problem had been open since 1933. In this paper we answer it positively for the case of Bernoulli rewards by providing the first finite-time analysis that matches the asymptotic rate given in the Lai and Robbins lower bound for the cumulative regret. The proof is accompanied by a numerical comparison with other optimal policies, experiments that have been lacking in the literature until now for the Bernoulli case. © 2012 Springer-Verlag.",,"Asymptotic rate; Asymptotically optimal; Bernoulli; Finite-time analysis; Lower bounds; Multi-armed bandit problem; Numerical comparison; Optimal policies; Optimality; Thompson; Artificial intelligence; Optimization",Conference Paper,Scopus,2-s2.0-84867888479
"Yuan L., Wang Y., Thompson P.M., Narayan V.A., Ye J.","Multi-source feature learning for joint analysis of incomplete multiple heterogeneous neuroimaging data",2012,"NeuroImage",61,10.1016/j.neuroimage.2012.03.059,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861187815&doi=10.1016%2fj.neuroimage.2012.03.059&partnerID=40&md5=6a192e837362ab584a4fff6b25ccf6ae","Analysis of incomplete data is a big challenge when integrating large-scale brain imaging datasets from different imaging modalities. In the Alzheimer's Disease Neuroimaging Initiative (ADNI), for example, over half of the subjects lack cerebrospinal fluid (CSF) measurements; an independent half of the subjects do not have fluorodeoxyglucose positron emission tomography (FDG-PET) scans; many lack proteomics measurements. Traditionally, subjects with missing measures are discarded, resulting in a severe loss of available information. In this paper, we address this problem by proposing an incomplete Multi-Source Feature (iMSF) learning method where all the samples (with at least one available data source) can be used. To illustrate the proposed approach, we classify patients from the ADNI study into groups with Alzheimer's disease (AD), mild cognitive impairment (MCI) and normal controls, based on the multi-modality data. At baseline, ADNI's 780 participants (172. AD, 397 MCI, 211 NC), have at least one of four data types: magnetic resonance imaging (MRI), FDG-PET, CSF and proteomics. These data are used to test our algorithm. Depending on the problem being solved, we divide our samples according to the availability of data sources, and we learn shared sets of features with state-of-the-art sparse learning methods. To build a practical and robust system, we construct a classifier ensemble by combining our method with four other methods for missing value estimation. Comprehensive experiments with various parameters show that our proposed iMSF method and the ensemble model yield stable and promising results. © 2012 Elsevier Inc.","Ensemble; Incomplete data; Multi-source feature learning; Multi-task learning","fluorodeoxyglucose; adult; aged; Alzheimer disease; analytical parameters; article; cerebrospinal fluid; controlled study; data analysis; human; learning; learning algorithm; major clinical study; mild cognitive impairment; multi-source feature learning; neuroimaging; nuclear magnetic resonance imaging; patient coding; positron emission tomography; priority journal; proteomics; standard; Aged; Algorithms; Alzheimer Disease; Artificial Intelligence; Databases, Factual; Female; Fluorodeoxyglucose F18; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Male; Middle Aged; Mild Cognitive Impairment; Neuroimaging; Positron-Emission Tomography; Proteomics; Radiopharmaceuticals",Article,Scopus,2-s2.0-84861187815
"Yannakakis G.N.","Game AI revisited",2012,"CF '12 - Proceedings of the ACM Computing Frontiers Conference",61,10.1145/2212908.2212954,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862675263&doi=10.1145%2f2212908.2212954&partnerID=40&md5=5d4e62b76062a1b5cf1457a2b975338e","More than a decade after the early research efforts on the use of artificial intelligence (AI) in computer games and the establishment of a new AI domain the term ''game AI'' needs to be redefined. Traditionally, the tasks associated with game AI revolved around non player character (NPC) behavior at different levels of control, varying from navigation and pathfinding to decision making. Commercial-standard games developed over the last 15 years and current game productions, however, suggest that the traditional challenges of game AI have been well addressed via the use of sophisticated AI approaches, not necessarily following or inspired by advances in academic practices. The marginal penetration of traditional academic game AI methods in industrial productions has been mainly due to the lack of constructive communication between academia and industry in the early days of academic game AI, and the inability of academic game AI to propose methods that would significantly advance existing development processes or provide scalable solutions to real world problems. Recently, however, there has been a shift of research focus as the current plethora of AI uses in games is breaking the non-player character AI tradition. A number of those alternative AI uses have already shown a significant potential for the design of better games. This paper presents four key game AI research areas that are currently reshaping the research roadmap in the game AI field and evidently put the game AI term under a new perspective. These game AI flagship research areas include the computational modeling of player experience, the procedural generation of content, the mining of player data on massive-scale and the alternative AI research foci for enhancing NPC capabilities. © 2012 ACM.","game AI flagships; game artificial intelligence; game data mining; player experience modeling; procedural content generation","Computational modeling; Computer game; Development process; game AI flagships; Game artificial intelligence; Industrial production; Non-player character; Pathfinding; Player experience; procedural content generation; Real-world problem; Research efforts; Research roadmap; Scalable solution; Artificial intelligence; Education; Research",Conference Paper,Scopus,2-s2.0-84862675263
"Ji J., Pang W., Zhou C., Han X., Wang Z.","A fuzzy k-prototype clustering algorithm for mixed numeric and categorical data",2012,"Knowledge-Based Systems",61,10.1016/j.knosys.2012.01.006,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862800691&doi=10.1016%2fj.knosys.2012.01.006&partnerID=40&md5=e83cbacb1f85707ae8c991fa758b0702","In many applications, data objects are described by both numeric and categorical features. The k-prototype algorithm is one of the most important algorithms for clustering this type of data. However, this method performs hard partition, which may lead to misclassification for the data objects in the boundaries of regions, and the dissimilarity measure only uses the user-given parameter for adjusting the significance of attribute. In this paper, first, we combine mean and fuzzy centroid to represent the prototype of a cluster, and employ a new measure based on co-occurrence of values to evaluate the dissimilarity between data objects and prototypes of clusters. This measure also takes into account the significance of different attributes towards the clustering process. Then we present our algorithm for clustering mixed data. Finally, the performance of the proposed method is demonstrated by a series of experiments on four real world datasets in comparison with that of traditional clustering algorithms. © 2012 Elsevier B.V. All rights reserved.","Attribute significance; Data mining; Dissimilarity measure; Fuzzy clustering; Mixed data","Attribute significance; Categorical data; Categorical features; Clustering process; Co-occurrence; Data objects; Dissimilarity measures; Fuzzy centroid; K-prototype; Misclassifications; Mixed data; Real-world datasets; Traditional clustering; Artificial intelligence; Data mining; Fuzzy clustering; Software engineering; Clustering algorithms",Article,Scopus,2-s2.0-84862800691
"Xie B., Minn H.","Real-time sleep apnea detection by classifier combination",2012,"IEEE Transactions on Information Technology in Biomedicine",61,10.1109/TITB.2012.2188299,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860666399&doi=10.1109%2fTITB.2012.2188299&partnerID=40&md5=3c1c5c627f80aedb47dee251d0d9f1c2","To find an efficient and valid alternative of polysomnography (PSG), this paper investigates real-time sleep apnea and hypopnea syndrome (SAHS) detection based on electrocardiograph (ECG) and saturation of peripheral oxygen (SpO 2) signals, individually and in combination. We include ten machine-learning algorithms in our classification experiment. It is shown that our proposed SpO 2 features outperform the ECG features in terms of diagnostic ability. More importantly, we propose classifier combination to further enhance the classification performance by harnessing the complementary information provided by individual classifiers. With our selected SpO 2 and ECG features, the classifier combination using AdaBoost with Decision Stump, Bagging with REPTree, and either kNN or Decision Table achieves sensitivity, specificity, and accuracy all around 82 for a minute-based real-time SAHS detection over 25 sleep-disordered-breathing suspects full overnight recordings. © 2012 IEEE.","Classifier combination; electrocardiograph (ECG); feature selection; hypopnea; machine learning; saturation of peripheral oxygen (SpO 2); sleep apnea","Classification performance; Classifier combination; Decision stumps; Diagnostic ability; electrocardiograph (ECG); hypopnea; Individual classifiers; Polysomnography; Sleep apnea; Sleep apnea detection; Adaptive boosting; Decision tables; Feature extraction; Learning systems; Oxygen; Electrocardiography; adult; aged; article; artificial intelligence; computer system; electrocardiography; female; human; male; methodology; middle aged; monitoring; oximetry; pathophysiology; signal processing; sleep apnea syndrome; Adult; Aged; Artificial Intelligence; Computer Systems; Electrocardiography; Female; Humans; Male; Middle Aged; Monitoring, Physiologic; Oximetry; Signal Processing, Computer-Assisted; Sleep Apnea Syndromes",Article,Scopus,2-s2.0-84860666399
"Wang H., Tang Q., Zheng W.","L1-norm-based common spatial patterns",2012,"IEEE Transactions on Biomedical Engineering",61,10.1109/TBME.2011.2177523,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863182964&doi=10.1109%2fTBME.2011.2177523&partnerID=40&md5=ac57b208999ad723c030ae8543de1522","Common spatial patterns (CSP) is a commonly used method of spatial filtering for multichannel electroencephalogram (EEG) signals. The formulation of the CSP criterion is based on variance using L2-norm, which implies that CSP is sensitive to outliers. In this paper, we propose a robust version of CSP, called CSP-L1, by maximizing the ratio of filtered dispersion of one class to the other class, both of which are formulated by using L1-norm rather than L2-norm. The spatial filters of CSP-L1 are obtained by introducing an iterative algorithm, which is easy to implement and is theoretically justified. CSP-L1 is robust to outliers. Experiment results on a toy example and datasets of BCI competitions demonstrate the efficacy of the proposed method. © 2011 IEEE.","Brain-computer interfaces; common spatial patterns (CSP); CSP-L1; L1-norm; robust modeling","Common spatial patterns; CSP-L1; Data sets; Iterative algorithm; L1 norm; L2-norm; Multichannel electroencephalograms; Robust modeling; Spatial filterings; Spatial filters; Algorithms; Statistics; Brain computer interface; algorithm; article; common spatial pattern; electroencephalogram; filtration; mathematical analysis; normal value; spatial summation; Algorithms; Artificial Intelligence; Brain Mapping; Computer Simulation; Electroencephalography; Humans; Models, Theoretical; Pattern Recognition, Automated; Reproducibility of Results; Signal Processing, Computer-Assisted",Article,Scopus,2-s2.0-84863182964
"Yildiz I.B., Jaeger H., Kiebel S.J.","Re-visiting the echo state property",2012,"Neural Networks",60,10.1016/j.neunet.2012.07.005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865541331&doi=10.1016%2fj.neunet.2012.07.005&partnerID=40&md5=d62acdae945d64b5ab543c60c0efd0be","An echo state network (ESN) consists of a large, randomly connected neural network, the reservoir, which is driven by an input signal and projects to output units. During training, only the connections from the reservoir to these output units are learned. A key requisite for output-only training is the echo state property (ESP), which means that the effect of initial conditions should vanish as time passes. In this paper, we use analytical examples to show that a widely used criterion for the ESP, the spectral radius of the weight matrix being smaller than unity, is not sufficient to satisfy the echo state property. We obtain these examples by investigating local bifurcation properties of the standard ESNs. Moreover, we provide new sufficient conditions for the echo state property of standard sigmoid and leaky integrator ESNs. We furthermore suggest an improved technical definition of the echo state property, and discuss what practicians should (and should not) observe when they optimize their reservoirs for specific tasks. © 2012 Elsevier Ltd.","Bifurcation; Diagonally Schur stable; Echo state network; Lyapunov; Spectral radius","Diagonally Schur stable; Echo state networks; Initial conditions; Input signal; Local bifurcations; Lyapunov; Specific tasks; Spectral radii; Sufficient conditions; Weight matrices; Artificial intelligence; Bifurcation (mathematics); Cognitive systems; Matrix algebra; algorithm; analytical parameters; article; artificial neural network; brain function; echo state property; electroencephalogram; human; position weight matrix; priority journal; statistical analysis; task performance; Algorithms; Computer Simulation; Learning; Models, Neurological; Neural Networks (Computer); Neurons; Nonlinear Dynamics; Time Factors",Article,Scopus,2-s2.0-84865541331
"He K., Sun J.","Statistics of patch offsets for image completion",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",60,10.1007/978-3-642-33709-3_2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867859482&doi=10.1007%2f978-3-642-33709-3_2&partnerID=40&md5=cd841028e8de434f0d96db3d7ac27290","Image completion involves filling missing parts in images. In this paper we address this problem through the statistics of patch offsets. We observe that if we match similar patches in the image and obtain their offsets (relative positions), the statistics of these offsets are sparsely distributed. We further observe that a few dominant offsets provide reliable information for completing the image. With these offsets we fill the missing region by combining a stack of shifted images via optimization. A variety of experiments show that our method yields generally better results and is faster than existing state-of-the-art methods. © 2012 Springer-Verlag.",,"Image completion; Relative positions; State-of-the-art methods; Image completion; Relative positions; State-of-the-art methods; Artificial intelligence; Artificial intelligence; Computers; Computer vision; Computer vision",Conference Paper,Scopus,2-s2.0-84867859482
"Gates K.M., Molenaar P.C.M.","Group search algorithm recovers effective connectivity maps for individuals in homogeneous and heterogeneous samples",2012,"NeuroImage",60,10.1016/j.neuroimage.2012.06.026,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864402054&doi=10.1016%2fj.neuroimage.2012.06.026&partnerID=40&md5=44553181032f2ba2cd631b5cc4ee88b2","At its best, connectivity mapping can offer researchers great insight into how spatially disparate regions of the human brain coordinate activity during brain processing. A recent investigation conducted by Smith and colleagues (2011) on methods for estimating connectivity maps suggested that those which attempt to ascertain the direction of influence among ROIs rarely provide reliable results. Another problem gaining increasing attention is heterogeneity in connectivity maps. Most group-level methods require that the data come from homogeneous samples, and misleading findings may arise from current methods if the connectivity maps for individuals vary across the sample (which is likely the case). The utility of maps resulting from effective connectivity on the individual or group levels is thus diminished because they do not accurately inform researchers. The present paper introduces a novel estimation technique for fMRI researchers, Group Iterative Multiple Model Estimation (GIMME), which demonstrates that using information across individuals assists in the recovery of the existence of connections among ROIs used by Smith and colleagues (2011) and the direction of the influence. Using heterogeneous in-house data, we demonstrate that GIMME offers a unique improvement over current approaches by arriving at reliable group and individual structures even when the data are highly heterogeneous across individuals comprising the group. An added benefit of GIMME is that it obtains reliable connectivity map estimates equally well using the data from resting state, block, or event-related designs. GIMME provides researchers with a powerful, flexible tool for identifying directed connectivity maps at the group and individual levels. © 2012 Elsevier Inc.","Automatic search; Effective connectivity mapping; Heterogeneity; Networks; SEM","algorithm; article; brain mapping; computer model; computer program; computer simulation; data analysis; functional magnetic resonance imaging; mathematical computing; mathematical model; nerve cell network; neuroimaging; performance measurement system; priority journal; sensitivity analysis; Algorithms; Artificial Intelligence; Brain; Brain Mapping; Connectome; Data Interpretation, Statistical; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Nerve Net; Pattern Recognition, Automated; Reproducibility of Results; Sample Size; Sensitivity and Specificity",Article,Scopus,2-s2.0-84864402054
"Ye J., Farnum M., Yang E., Verbeeck R., Lobanov V., Raghavan N., Novak G., DiBernardo A., Narayan V.A.","Sparse learning and stability selection for predicting MCI to AD conversion using baseline ADNI data",2012,"BMC Neurology",60,10.1186/1471-2377-12-46,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862598129&doi=10.1186%2f1471-2377-12-46&partnerID=40&md5=8f9e6108c9c65a61e8f036e16ebad88e","Background: Patients with Mild Cognitive Impairment (MCI) are at high risk of progression to Alzheimer's dementia. Identifying MCI individuals with high likelihood of conversion to dementia and the associated biosignatures has recently received increasing attention in AD research. Different biosignatures for AD (neuroimaging, demographic, genetic and cognitive measures) may contain complementary information for diagnosis and prognosis of AD.Methods: We have conducted a comprehensive study using a large number of samples from the Alzheimer's Disease Neuroimaging Initiative (ADNI) to test the power of integrating various baseline data for predicting the conversion from MCI to probable AD and identifying a small subset of biosignatures for the prediction and assess the relative importance of different modalities in predicting MCI to AD conversion. We have employed sparse logistic regression with stability selection for the integration and selection of potential predictors. Our study differs from many of the other ones in three important respects: (1) we use a large cohort of MCI samples that are unbiased with respect to age or education status between case and controls (2) we integrate and test various types of baseline data available in ADNI including MRI, demographic, genetic and cognitive measures and (3) we apply sparse logistic regression with stability selection to ADNI data for robust feature selection.Results: We have used 319 MCI subjects from ADNI that had MRI measurements at the baseline and passed quality control, including 177 MCI Non-converters and 142 MCI Converters. Conversion was considered over the course of a 4-year follow-up period. A combination of 15 features (predictors) including those from MRI scans, APOE genotyping, and cognitive measures achieves the best prediction with an AUC score of 0.8587.Conclusions: Our results demonstrate the power of integrating various baseline data for prediction of the conversion from MCI to probable AD. Our results also demonstrate the effectiveness of stability selection for feature selection in the context of sparse logistic regression. © 2012 Ye et al.; licensee BioMed Central Ltd.",,"amyloid beta protein[1-42]; apoenzyme; tau protein; aged; Alzheimer disease; anterior cingulate; article; disease course; educational status; entorhinal cortex; follow up; genotype; hippocampus; human; learning; major clinical study; memory; mild cognitive impairment; nuclear magnetic resonance imaging; prognosis; Aged; Algorithms; Alzheimer Disease; Artificial Intelligence; Decision Support Systems, Clinical; Diagnosis, Computer-Assisted; Female; Humans; Male; Mild Cognitive Impairment; Proportional Hazards Models; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84862598129
"Quellec G., Lamard M., Cazuguel G., Cochener B., Roux C.","Fast wavelet-based image characterization for highly adaptive image retrieval",2012,"IEEE Transactions on Image Processing",60,10.1109/TIP.2011.2180915,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859023851&doi=10.1109%2fTIP.2011.2180915&partnerID=40&md5=bcc2366aafd766752ad1c9236a7957bb","Adaptive wavelet-based image characterizations have been proposed in previous works for content-based image retrieval (CBIR) applications. In these applications, the same wavelet basis was used to characterize each query image: This wavelet basis was tuned to maximize the retrieval performance in a training data set. We take it one step further in this paper: A different wavelet basis is used to characterize each query image. A regression function, which is tuned to maximize the retrieval performance in the training data set, is used to estimate the best wavelet filter, i.e., in terms of expected retrieval performance, for each query image. A simple image characterization, which is based on the standardized moments of the wavelet coefficient distributions, is presented. An algorithm is proposed to compute this image characterization almost instantly for every possible separable or nonseparable wavelet filter. Therefore, using a different wavelet basis for each query image does not considerably increase computation times. On the other hand, significant retrieval performance increases were obtained in a medical image data set, a texture data set, a face recognition data set, and an object picture data set. This additional flexibility in wavelet adaptation paves the way to relevance feedback on image characterization itself and not simply on the way image characterizations are combined. © 2011 IEEE.","Content-based image retrieval (CBIR); relevance feedback; wavelet adaptation; wavelet transform","Additional flexibilities; Computation time; Content-Based Image Retrieval; Data sets; Medical images; Nonseparable wavelet; One step; Query images; Regression function; relevance feedback; Retrieval performance; Texture data set; Training data sets; wavelet adaptation; Wavelet basis; Wavelet coefficients; Wavelet filters; Wavelet-based images; Content based retrieval; Face recognition; Information retrieval; Wavelet transforms; Characterization; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; hospital information system; image enhancement; image subtraction; information retrieval; methodology; wavelet analysis; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Information Storage and Retrieval; Pattern Recognition, Automated; Radiology Information Systems; Subtraction Technique; Wavelet Analysis",Article,Scopus,2-s2.0-84859023851
"Järvisalo M., Heule M.J.H., Biere A.","Inprocessing rules",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",59,10.1007/978-3-642-31365-3_28,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863620820&doi=10.1007%2f978-3-642-31365-3_28&partnerID=40&md5=a47a6b7a39c463709f567a3376454733","Decision procedures for Boolean satisfiability (SAT), especially modern conflict-driven clause learning (CDCL) solvers, act routinely as core solving engines in various real-world applications. Preprocessing, i.e., applying formula rewriting/simplification rules to the input formula before the actual search for satisfiability, has become an essential part of the SAT solving tool chain. Further, some of the strongest SAT solvers today add more reasoning to search by interleaving formula simplification and CDCL search. Such inprocessing SAT solvers witness the fact that implementing additional deduction rules in CDCL solvers leverages the efficiency of state-of-the-art SAT solving further. In this paper we establish formal underpinnings of inprocessing SAT solving via an abstract inprocessing framework that covers a wide range of modern SAT solving techniques. © 2012 Springer-Verlag.",,"Boolean satisfiability; Clause learning; Decision procedure; Deduction rule; Real-world application; SAT solvers; SAT-solving; Satisfiability; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84863620820
"Bordes A., Glorot X., Weston J., Bengio Y.","Joint learning of words and meaning representations for open-text semantic parsing",2012,"Journal of Machine Learning Research",59,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954213973&partnerID=40&md5=e583e6ce8f15bd910dbe0578457e8a92","Open-text semantic parsers are designed to interpret any statement in natural language by inferring a corresponding meaning representation (MR-a formal representation of its sense). Unfortunately, large scale systems cannot be easily machine-learned due to a lack of directly supervised data. We propose a method that learns to assign MRs to a wide range of text (using a dictionary of more than 70, 000 words mapped to more than 40, 000 entities) thanks to a training scheme that combines learning from knowledge bases (e.g. WordNet) with learning from raw text. The model jointly learns representations of words, entities and MRs via a multi-task training process operating on these diverse sources of data. Hence, the system ends up providing methods for knowledge acquisition and wordsense disambiguation within the context of semantic parsing in a single elegant framework. Experiments on these various tasks indicate the promise of the approach.",,"Artificial intelligence; Large scale systems; Formal representations; Joint learning; Knowledge basis; Natural languages; Open text; Semantic parsing; Training schemes; Word Sense Disambiguation; Semantics",Conference Paper,Scopus,2-s2.0-84954213973
"Schifano E.D., Epstein M.P., Bielak L.F., Jhun M.A., Kardia S.L.R., Peyser P.A., Lin X.","SNP Set Association Analysis for Familial Data",2012,"Genetic Epidemiology",58,10.1002/gepi.21676,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869123399&doi=10.1002%2fgepi.21676&partnerID=40&md5=1bfe025c364a03ac31fb85874f265624","Genome-wide association studies (GWAS) are a popular approach for identifying common genetic variants and epistatic effects associated with a disease phenotype. The traditional statistical analysis of such GWAS attempts to assess the association between each individual single-nucleotide polymorphism (SNP) and the observed phenotype. Recently, kernel machine-based tests for association between a SNP set (e.g., SNPs in a gene) and the disease phenotype have been proposed as a useful alternative to the traditional individual-SNP approach, and allow for flexible modeling of the potentially complicated joint SNP effects in a SNP set while adjusting for covariates. We extend the kernel machine framework to accommodate related subjects from multiple independent families, and provide a score-based variance component test for assessing the association of a given SNP set with a continuous phenotype, while adjusting for additional covariates and accounting for within-family correlation. We illustrate the proposed method using simulation studies and an application to genetic data from the Genetic Epidemiology Network of Arteriopathy (GENOA) study. © 2012 Wiley Periodicals, Inc.","Family association studies; Kernel machine; Linear mixed model; Multilocus test; Score statistics; Variance component test; Within-family correlation","artery disease; article; clinical assessment; controlled study; covariance; family; gene mapping; genetic analysis; genetic association; genetic epidemiology; genotype; human; kernel method; phenotype; scoring system; simulation; single nucleotide polymorphism; variance; algorithm; artificial intelligence; biological model; chromosome 10; gene; genetic association; genetics; single nucleotide polymorphism; acid ceramidase; ASAH1 protein, human; Acid Ceramidase; Algorithms; Artificial Intelligence; Chromosomes, Human, Pair 10; Family; Genes; Genetic Association Studies; Genome-Wide Association Study; Humans; Models, Genetic; Phenotype; Polymorphism, Single Nucleotide",Article,Scopus,2-s2.0-84869123399
"Manoj V.J., Elias E.","Artificial bee colony algorithm for the design of multiplier-less nonuniform filter bank transmultiplexer",2012,"Information Sciences",58,10.1016/j.ins.2011.02.023,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857845986&doi=10.1016%2fj.ins.2011.02.023&partnerID=40&md5=a682881cad962dd08b3e91b4c88db004","Nonuniform filter bank transmultiplexer (NUFB TMUX) can be used to implement multicarrier communication system where applications with different data rates are to be multiplexed. It is possible to reduce the hardware complexity of the NUFB TMUX by representing the filter coefficients in canonic signed digit (CSD) format. In this paper the design of a multiplier-less NUFB TMUX is presented. NUFB TMUX with continuous filter coefficients is designed and the filter coefficients are synthesized in CSD format. Filter coefficient synthesis in CSD format is formulated as an optimization problem and an artificial bee colony (ABC) algorithm is used for the optimization. To preserve the canonic nature of filter coefficients in the ABC algorithm the filter coefficients are encoded using a look-up table. The look-up table also provides the number of signed power-of-two (SPT) terms in the CSD numbers. Simulation results show that the performance of the multiplier-less NUFB TMUX designed using the proposed ABC algorithm is much better than that of the multiplier-less NUFB TMUX obtained by rounding the continuous coefficients of filters to the nearest CSD number. Multiplier-less NUFB TMUX designed by the proposed ABC algorithm also outperforms that designed using genetic algorithm (GA) and particle swarm optimization (PSO). © 2010 Elsevier Inc. All rights reserved.","Canonic signed digit; Honey bee swarm; Rayleigh-Ritz ratio; Signal to interference ratio; Swarm intelligence","Canonic signed digit; Honey bee; Rayleigh-Ritz ratio; Signal to interference ratio; Swarm Intelligence; Artificial intelligence; Communication systems; Genetic algorithms; Particle swarm optimization (PSO); Table lookup; Filter banks",Article,Scopus,2-s2.0-84857845986
"Dinakar K., Jones B., Havasi C., Lieberman H., Picard R.","Common sense reasoning for detection, prevention, and mitigation of cyberbullying",2012,"ACM Transactions on Interactive Intelligent Systems",58,10.1145/2362394.2362400,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983569734&doi=10.1145%2f2362394.2362400&partnerID=40&md5=5e67e02831f67af86bd83222ce502501","Cyberbullying (harassment on social networks) is widely recognized as a serious social problem, especially for adolescents. It is as much a threat to the viability of online social networks for youth today as spam once was to email in the early days of the Internet. Current work to tackle this problem has involved social and psychological studies on its prevalence as well as its negative effects on adolescents. While true solutions rest on teaching youth to have healthy personal relationships, few have considered innovative design of social network software as a tool for mitigating this problem. Mitigating cyberbullying involves two key components: robust techniques for effective detection and reflective user interfaces that encourage users to reflect upon their behavior and their choices. Spam filters have been successful by applying statistical approaches like Bayesian networks and hidden Markov models. They can, like Googles GMail, aggregate human spam judgments because spam is sent nearly identically to many people. Bullying is more personalized, varied, and contextual. In this work, we present an approach for bullying detection based on state-of-the-art natural language processing and a common sense knowledge base, which permits recognition over a broad spectrum of topics in everyday life. We analyze a more narrow range of particular subject matter associated with bullying (e.g. appearance, intelligence, racial and ethnic slurs, social acceptance, and rejection), and construct BullySpace, a common sense knowledge base that encodes particular knowledge about bullying situations. We then perform joint reasoning with common sense knowledge about a wide range of everyday life topics. We analyze messages using our novel AnalogySpace common sense reasoning technique. We also take into account social network analysis and other factors. We evaluate the model on real-world instances that have been reported by users on Formspring, a social networking website that is popular with teenagers. On the intervention side, we explore a set of reflective user-interaction paradigms with the goal of promoting empathy among social network participants. We propose an ""air traffic control""-like dashboard, which alerts moderators to large-scale outbreaks that appear to be escalating or spreading and helps them prioritize the current deluge of user complaints. For potential victims, we provide educational material that informs them about how to cope with the situation, and connects them with emotional support from others. A user evaluation shows that in-context, targeted, and dynamic help during cyberbullying situations fosters end-user reflection that promotes better coping strategies. © 2012 ACM.","affective computing; artificial intelligence; Common sense reasoning","Artificial intelligence; Bayesian networks; Computer crime; Design; Hidden Markov models; Human computer interaction; Internet; Knowledge based systems; Natural language processing systems; Social sciences computing; Affective Computing; Commonsense knowledge; Commonsense reasoning; Educational materials; NAtural language processing; On-line social networks; Personal relationships; Social Network Analysis; Social networking (online)",Article,Scopus,2-s2.0-84983569734
"Rogister P., Benosman R., Ieng S.-H., Lichtsteiner P., Delbruck T.","Asynchronous event-based binocular stereo matching",2012,"IEEE Transactions on Neural Networks and Learning Systems",57,10.1109/TNNLS.2011.2180025,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867723133&doi=10.1109%2fTNNLS.2011.2180025&partnerID=40&md5=c2d4ae96765f2e2ed244aa2c5bedf905","We present a novel event-based stereo matching algorithm that exploits the asynchronous visual events from a pair of silicon retinas. Unlike conventional frame-based cameras, recent artificial retinas transmit their outputs as a continuous stream of asynchronous temporal events, in a manner similar to the output cells of the biological retina. Our algorithm uses the timing information carried by this representation in addressing the stereo-matching problem on moving objects. Using the high temporal resolution of the acquired data stream for the dynamic vision sensor, we show that matching on the timing of the visual events provides a new solution to the real-time computation of 3-D objects when combined with geometric constraints using the distance to the epipolar lines. The proposed algorithm is able to filter out incorrect matches and to accurately reconstruct the depth of moving objects despite the low spatial resolution of the sensor. This brief sets up the principles for further event-based vision processing and demonstrates the importance of dynamic information and spike timing in processing asynchronous streams of visual events. © 2012 IEEE.","Asynchronous acquisition; event-based vision; frameless vision; retinas; stereo vision; time impulse encoding","Asynchronous acquisition; Dynamic vision sensors; Event-based; Geometric constraint; High temporal resolution; Real-time computations; retinas; Stereo matching algorithm; Algorithms; Sensors; Stereo vision; animal; artificial intelligence; binocular vision; biomimetics; computer assisted diagnosis; human; photography; physiology; procedures; retina; three dimensional imaging; Animals; Artificial Intelligence; Biomimetics; Humans; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Photogrammetry; Retina; Vision, Binocular",Article,Scopus,2-s2.0-84867723133
"Haykin S., Xue Y., Setoodeh P.","Cognitive radar: Step toward bridging the gap between neuroscience and engineering",2012,"Proceedings of the IEEE",57,10.1109/JPROC.2012.2203089,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867842347&doi=10.1109%2fJPROC.2012.2203089&partnerID=40&md5=2260dc3ebda341a4b8caa25f5b1285bc","In this paper, we describe a cognitive radar (CR) that mimics the visual brain. Although the visual brain and radar are different in that the visual brain does not transmit a probing signal to the environment while the active radar greatly relies on the probing signal it transmits to the environment, both of them are observers of the surrounding environment. As such, there is much that we can learn from the visual brain in building a new generation of CRs that outperform traditional radars. In this paper, we confine the discussion, in both analytic and experimental terms, to CR aimed at target tracking. From a theoretical perspective, using the posterior Cramér-Rao lower bound (PCRLB), it is shown that a cognitive tracking radar has the potential to improve tracking performance significantly. In particular, computer experiments are presented, which demonstrate that CR can indeed go beyond the theoretical limits of traditional active radars (TARs) as well as fore-active radars (FARs); the latter are radars equipped with feedback from the receiver to the transmitter. Moreover, computer experiments are presented to demonstrate another practical benefit resulting from the combined use of memory and executive attention in CR for a target-tracking application. Specifically, it is shown that with the provision of these two cognitive processes, the transition in switching from one transmit waveform to another goes forward in a smooth manner. Such a capability is beyond that of TAR or FAR. © 1963-2012 IEEE.","Artificial intelligence; attention; cognitive radar (CR); fore-active radar (FAR); memory; perception-action cycle; posterior Cramér-Rao lower bound (PCRLB); tracking; traditional active radar (TAR)","attention; cognitive radar (CR); fore-active radar (FAR); Lower bounds; Perception-action cycle; traditional active radar (TAR); Artificial intelligence; Data storage equipment; Surface discharges; Tar; Target tracking; Tracking radar; Radar tracking",Article,Scopus,2-s2.0-84867842347
"Musiolik J., Markard J., Hekkert M.","Networks and network resources in technological innovation systems: Towards a conceptual framework for system building",2012,"Technological Forecasting and Social Change",57,10.1016/j.techfore.2012.01.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861758969&doi=10.1016%2fj.techfore.2012.01.003&partnerID=40&md5=c541262ea3b1ee5f8bce2d5e50175b0d","Previous research has shown that formal networks can play a crucial role in the formation of technological innovation systems (TIS). Firms and other actors collaborate in formal networks not only to generate new knowledge but also to strategically create and shape supportive system resources such as technology specific R&D programs. This paper takes a closer look at the resources, which are developed and deployed by networks to facilitate the building up of a TIS. Networks rely not only on the organizational resources of their members but also on new resources developed at the network level including network governance structures, trust among network members, a common understanding of the strategic goals or a good reputation of the network. Our analysis shows that the capacity of networks to fulfill different tasks of system building especially depends on the network resources they are able to establish. With the differentiation of organizational, network and system resources we introduce a conceptual framework, which makes three important contributions. It highlights the strategic nature of (innovation) system building; it allows us comparing the contribution of different actors and formal networks in this regard; and it improves our understanding of how firm and system level processes are intertwined. © 2012 Elsevier Inc..","Clean technology; Innovation networks; Network resources; System building; Technological innovation system","Clean technologies; Innovation network; Network resource; System building; Technological innovation; Artificial intelligence; Industry; Innovation; Technology; Buildings; conceptual framework; innovation; knowledge; technological development",Article,Scopus,2-s2.0-84861758969
"Wang Z.-J., Li K.W.","Goal programming approaches to deriving interval weights based on interval fuzzy preference relations",2012,"Information Sciences",57,10.1016/j.ins.2012.01.019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862823224&doi=10.1016%2fj.ins.2012.01.019&partnerID=40&md5=ebf76ce2a96ce936b9af28b19ed0ebdf","This article investigates the consistency of interval fuzzy preference relations based on interval arithmetic, and new definitions are introduced for additive consistent, multiplicative consistent and weakly transitive interval fuzzy preference relations. Transformation functions are put forward to convert normalized interval weights into consistent interval fuzzy preference relations. By analyzing the relationship between interval weights and consistent interval fuzzy preference relations, goal-programming-based models are developed for deriving interval weights from interval fuzzy preference relations for both individual and group decision-making situations. The proposed models are illustrated by a numerical example and an international exchange doctoral student selection problem. © 2011 Elsevier Inc. All rights reserved.","Additive transitivity; Goal programming; Interval fuzzy preference relation; Interval weight; Multiplicative transitivity","Fuzzy preference relations; Goal programming; Group decision-making; International exchange; Interval arithmetic; Interval weights; Multiplicative transitivity; Numerical example; Selection problems; Transformation functions; Artificial intelligence; Software engineering",Article,Scopus,2-s2.0-84862823224
"Bunte K., Schneider P., Hammer B., Schleif F.-M., Villmann T., Biehl M.","Limited Rank Matrix Learning, discriminative dimension reduction and visualization",2012,"Neural Networks",57,10.1016/j.neunet.2011.10.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855962168&doi=10.1016%2fj.neunet.2011.10.001&partnerID=40&md5=88c69fa4236e42d843c4a9da079bac65","We present an extension of the recently introduced Generalized Matrix Learning Vector Quantization algorithm. In the original scheme, adaptive square matrices of relevance factors parameterize a discriminative distance measure. We extend the scheme to matrices of limited rank corresponding to low-dimensional representations of the data. This allows to incorporate prior knowledge of the intrinsic dimension and to reduce the number of adaptive parameters efficiently. In particular, for very large dimensional data, the limitation of the rank can reduce computation time and memory requirements significantly. Furthermore, two-or three-dimensional representations constitute an efficient visualization method for labeled data sets. The identification of a suitable projection is not treated as a pre-processing step but as an integral part of the supervised training. Several real world data sets serve as an illustration and demonstrate the usefulness of the suggested method. © 2011 Elsevier Ltd.","Adaptive metrics; Classification; Dimension reduction; Learning Vector Quantization; Visualization","Adaptive metrics; Adaptive parameters; Computation time; Dimension reduction; Distance measure; Generalized matrix; Integral part; Intrinsic dimensions; Labeled data; Learning Vector Quantization; Learning vector quantization algorithms; Low-dimensional representation; matrix; Memory requirements; Pre-processing step; Prior knowledge; Real world data; Square matrices; Visualization method; Classification (of information); Clustering algorithms; Data visualization; Flow visualization; Vector quantization; Visualization; Matrix algebra; article; classification algorithm; discriminant analysis; Generalized Matrix Learning Vector Quantization algorithm; learning algorithm; mathematical analysis; mathematical model; priority journal; process optimization; remote sensing; statistics; Algorithms; Artificial Intelligence; Discriminant Analysis; Humans; Learning; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84855962168
"Huang H., Qin H., Hao Z., Lim A.","Example-based learning particle swarm optimization for continuous optimization",2012,"Information Sciences",57,10.1016/j.ins.2010.10.018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80055064646&doi=10.1016%2fj.ins.2010.10.018&partnerID=40&md5=a616eb893aa372060f225703cb8a24bd","Particle swarm optimization (PSO) is a heuristic optimization technique based on swarm intelligence that is inspired by the behavior of bird flocking. The canonical PSO has the disadvantage of premature convergence. Several improved PSO versions do well in keeping the diversity of the particles during the searching process, but at the expense of rapid convergence. This paper proposes an example-based learning PSO (ELPSO) to overcome these shortcomings by keeping a balance between swarm diversity and convergence speed. Inspired by a social phenomenon that multiple good examples can guide a crowd towards making progress, ELPSO uses an example set of multiple global best particles to update the positions of the particles. In this study, the particles of the example set were selected from the best particles and updated by the better particles in the first-in-first-out order in each iteration. The particles in the example set are different, and are usually of high quality in terms of the target optimization function. ELPSO has better diversity and convergence speed than single-gbest and non-gbest PSO algorithms, which is proved by mathematical and numerical results. Finally, computational experiments on benchmark problems show that ELPSO outperforms all of the tested PSO algorithms in terms of both solution quality and convergence time. © 2011 Elsevier Inc. All rights reserved.","Continuous optimization; Example-based learning; Particle swarm optimization; Swarm intelligence","Bench-mark problems; Canonical PSO; Computational experiment; Continuous optimization; Convergence speed; Convergence time; Example-based learning; First-in-first-out; Heuristic optimization technique; High quality; Improved PSO; Numerical results; Premature convergence; PSO algorithms; Rapid convergence; Solution quality; Swarm Intelligence; Target optimization; Algorithms; Artificial intelligence; Benchmarking; Cellular automata; Convergence of numerical methods; Particle swarm optimization (PSO)",Article,Scopus,2-s2.0-80055064646
"Harvey J.E., Schrder S., Choi N., Duparr A.","Total integrated scatter from surfaces with arbitrary roughness, correlation widths, and incident angles",2012,"Optical Engineering",57,10.1117/1.OE.51.1.013402,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856869815&doi=10.1117%2f1.OE.51.1.013402&partnerID=40&md5=32085fee4770e1b9f150eb57cbf9e733","Surface scatter effects from residual optical fabrication errors can severely degrade optical performance. The total integrated scatter (TIS) from a given mirror surface is determined by the ratio of the spatial frequency band-limited relevant root-mean-square surface roughness to the wavelength of light. For short-wavelength (extreme-ultraviolet/x-ray) applications, even state-of-the-art optical surfaces can scatter a significant fraction of the total reflected light. In this paper we first discuss how to calculate the band-limited relevant roughness from surface metrology data, then present parametric plots of the TIS for optical surfaces with arbitrary roughness, surface correlation widths, and incident angles. Surfaces with both Gaussian and A BC or K-correlation power spectral density functions have been modeled. These parametric TIS predictions provide insight that is useful in determining realistic optical fabrication tolerances necessary to satisfy specific optical performance requirements. © 2012 Society of Photo-Optical Instrumentation Engineers.","bidirectional reflectance distribution function; BRDF; PSD; relevant band-limited roughness; scattering from rough surfaces; surface power spectral density function; surface scatter; TIS; total integrated scatter","Bidirectional reflectance distribution functions; BRDF; PSD; scattering from rough surfaces; surface power spectral density function; surface scatter; TIS; Total integrated scatters; Scattering from rough surfaces; Surface power; Artificial intelligence; Distribution functions; Fluidized beds; Frequency bands; Integration; Optical correlation; Surface measurement; Surface scattering; Surface roughness",Article,Scopus,2-s2.0-84856869815
"Li J., Szekely L., Eriksson L., Heddson B., Sundbom A., Czene K., Hall P., Humphreys K.","High-throughput mammographic-density measurement: a tool for risk prediction of breast cancer",2012,"Breast Cancer Research",56,10.1186/bcr3238,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864356747&doi=10.1186%2fbcr3238&partnerID=40&md5=ca044a2b734878c512f841af81d3e9f5","Introduction: Mammographic density (MD) is a strong, independent risk factor for breast cancer, but measuring MD is time consuming and reader dependent. Objective MD measurement in a high-throughput fashion would enable its wider use as a biomarker for breast cancer. We use a public domain image-processing software for the fully automated analysis of MD and penalized regression to construct a measure that mimics a well-established semiautomated measure (Cumulus). We also describe measures that incorporate additional features of mammographic images for improving the risk associations of MD and breast cancer risk.Methods: We randomly partitioned our dataset into a training set for model building (733 cases, 748 controls) and a test set for model assessment (765 cases, 747 controls). The Pearson product-moment correlation coefficient (r) was used to compare the MD measurements by Cumulus and our automated measure, which mimics Cumulus. The likelihood ratio test was used to validate the performance of logistic regression models for breast cancer risk, which included our measure capturing additional information in mammographic images.Results: We observed a high correlation between the Cumulus measure and our measure mimicking Cumulus (r = 0.884; 95% CI, 0.872 to 0.894) in an external test set. Adding a variable, which includes extra information to percentage density, significantly improved the fit of the logistic regression model of breast cancer risk (P = 0.0002).Conclusions: Our results demonstrate the potential to facilitate the integration of mammographic density measurements into large-scale research studies and subsequently into clinical practice. © 2012 Li et al.; licensee BioMed Central Ltd.",,"adult; aged; article; automation; breast cancer; cancer risk; computer program; controlled study; female; gold standard; high throughput screening; human; logistic regression analysis; major clinical study; mammography; measurement; risk assessment; statistical model; area under the curve; artificial intelligence; breast tumor; case control study; congenital malformation; early diagnosis; image processing; mammary gland; middle aged; procedures; radiography; risk factor; standards; Aged; Area Under Curve; Artificial Intelligence; Breast Neoplasms; Case-Control Studies; Early Detection of Cancer; Female; Humans; Image Processing, Computer-Assisted; Mammary Glands, Human; Mammography; Middle Aged; Risk Assessment; Risk Factors",Article,Scopus,2-s2.0-84864356747
"Nguyen T.N.A., Cai J., Zhang J., Zheng J.","Robust interactive image segmentation using convex active contours",2012,"IEEE Transactions on Image Processing",56,10.1109/TIP.2012.2191566,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864143254&doi=10.1109%2fTIP.2012.2191566&partnerID=40&md5=c34a85366452758e74942720e3a5c907","The state-of-the-art interactive image segmentation algorithms are sensitive to the user inputs and often unable to produce an accurate boundary with a small amount of user interaction. They frequently rely on laborious user editing to refine the segmentation boundary. In this paper, we propose a robust and accurate interactive method based on the recently developed continuous-domain convex active contour model. The proposed method exhibits many desirable properties of an effective interactive image segmentation algorithm, including robustness to user inputs and different initializations, the ability to produce a smooth and accurate boundary contour, and the ability to handle topology changes. Experimental results on a benchmark data set show that the proposed tool is highly effective and outperforms the state-of-the-art interactive image segmentation algorithms. © 1992-2012 IEEE.","Convex active contour; digital image editing; interactive image segmentation","Active contour model; Active contours; Benchmark data; Boundary contours; Continuous-domain; Digital image; Interactive image segmentation; Interactive methods; Segmentation boundaries; User input; User interaction; Image processing; Mathematical models; Image segmentation; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; image enhancement; methodology; reproducibility; sensitivity and specificity; automated pattern recognition; computer assisted diagnosis; image enhancement; procedures; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84864143254
"Blackwell T.","A study of collapse in bare bones particle swarm optimization",2012,"IEEE Transactions on Evolutionary Computation",56,10.1109/TEVC.2011.2136347,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861829465&doi=10.1109%2fTEVC.2011.2136347&partnerID=40&md5=8da636d502ebaba18cd98289c24fbfb3","The dynamic update rule of particle swarm optimization is formulated as a second-order stochastic difference equation and general relations are derived for search focus, search spread, and swarm stability at stagnation. The relations are applied to three particular particle swarm optimization (PSO) implementations, the standard PSO of Clerc and Kennedy, a PSO with discrete recombination, and the Bare Bones swarm. The simplicity of the Bare Bones swarm facilitates theoretical analysis and a further no-collapse condition is derived. A series of experimental trials confirms that Bare Bones situated at the edge of collapse is comparable to other PSOs, and that performance can be still further improved with the use of an adaptive distribution. It is conjectured that, subject to spread, stability and no-collapse, there is a single encompassing particle swarm paradigm, and that an important aspect of parameter tuning within any particular manifestation is to remove any deleterious behavior that ensues from the dynamics. © 2012 IEEE.","Computational and artificial intelligence; particle swarm optimization","Dynamic update; Parameter-tuning; Particle swarm; Second orders; Standard PSO; Stochastic difference equations; Artificial intelligence; Bone; Difference equations; Particle swarm optimization (PSO)",Article,Scopus,2-s2.0-84861829465
"Son S., Sim K.M.","A price-and-time-slot-negotiation mechanism for cloud service reservations",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",56,10.1109/TSMCB.2011.2174355,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861189234&doi=10.1109%2fTSMCB.2011.2174355&partnerID=40&md5=aebfdde6ae70173233c345440a052559","When making reservations for Cloud services, consumers and providers need to establish service-level agreements through negotiation. Whereas it is essential for both a consumer and a provider to reach an agreement on the price of a service and when to use the service, to date, there is little or no negotiation support for both price and time-slot negotiations (PTNs) for Cloud service reservations. This paper presents a multi-issue negotiation mechanism to facilitate the following: 1) PTNs between Cloud agents and 2) tradeoff between price and time-slot utilities. Unlike many existing negotiation mechanisms in which a negotiation agent can only make one proposal at a time, agents in this work are designed to concurrently make multiple proposals in a negotiation round that generate the same aggregated utility, differing only in terms of individual price and time-slot utilities. Another novelty of this work is formulating a novel time-slot utility function that characterizes preferences for different time slots. These ideas are implemented in an agent-based Cloud testbed. Using the testbed, experiments were carried out to compare this work with related approaches. Empirical results show that PTN agents reach faster agreements and achieve higher utilities than other related approaches. A case study was carried out to demonstrate the application of the PTN mechanism for pricing Cloud resources. © 2012 IEEE.","Automated negotiation; Cloud negotiation; Cloud resource allocation; multi-issue negotiation; negotiation agent; resource management","Agent based; Automated negotiations; Cloud services; Multi-issue negotiation; Negotiation agents; Negotiation mechanism; Negotiation support; Resource management; Service Level Agreements; Time slots; Utility functions; Distributed database systems; Economics; Resource allocation; Testbeds; Costs; algorithm; article; artificial intelligence; computer simulation; decision support system; economics; Internet; marketing; theoretical model; Algorithms; Artificial Intelligence; Computer Simulation; Decision Support Techniques; Internet; Marketing; Models, Theoretical",Article,Scopus,2-s2.0-84861189234
"Xia M., Xu Z., Zhu B.","Generalized intuitionistic fuzzy Bonferroni means",2012,"International Journal of Intelligent Systems",56,10.1002/int.20515,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-82455164083&doi=10.1002%2fint.20515&partnerID=40&md5=80a73149f413d96c233229b94e536dc0","Intuitionistic fuzzy set is a widely used tool to express the membership, nonmembership, and hesitancy information of an element to a set. To aggregate the intuitionistic fuzzy information, a lot of aggregation techniques have been developed, especially, the ones which reflect the correlations of the aggregated arguments are the hot research topics, among which Bonferroni mean (BM) is an important aggregation technique. However, the classical BM ignores some aggregation information and the weight vector of the aggregated arguments. In this paper, we introduce the generalized weighted BM and the generalized intuitionistic fuzzy weighted BM, both of which focus on the group opinion. Paying more attention to the individual opinions, we further define the generalized weighted Bonferroni geometric mean and the generalized intuitionistic fuzzy weighted Bonferroni geometric mean. Various families of the existing operators can be obtained when the parameters of the developed aggregation techniques are assigned different values. Finally, we propose an approach to multicriteria decision making on the basis of the proposed aggregation techniques and an example is also given to illustrate our results. © 2011 Wiley Periodicals, Inc.",,"Geometric mean; Hot research topics; Intuitionistic fuzzy; Intuitionistic fuzzy sets; Multi criteria decision making; Weight vector; Artificial intelligence; Software engineering; Fuzzy sets",Article,Scopus,2-s2.0-82455164083
"Rintanen J.","Planning as satisfiability: Heuristics",2012,"Artificial Intelligence",55,10.1016/j.artint.2012.08.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866502355&doi=10.1016%2fj.artint.2012.08.001&partnerID=40&md5=895bd10a21b27e8700cdff7f01d1a1c8","Reduction to SAT is a very successful approach to solving hard combinatorial problems in Artificial Intelligence and computer science in general. Most commonly, problem instances reduced to SAT are solved with a general-purpose SAT solver. Although there is the obvious possibility of improving the SAT solving process with application-specific heuristics, this has rarely been done successfully. In this work we propose a planning-specific variable selection strategy for SAT solving. The strategy is based on generic principles about properties of plans, and its performance with standard planning benchmarks often substantially improves on generic variable selection heuristics, such as VSIDS, and often lifts it to the same level with other search methods such as explicit state-space search with heuristic search algorithms. © 2012 Elsevier B.V.","Heuristics; Planning; SAT","Combinatorial problem; Heuristic search algorithms; Heuristics; Problem instances; SAT; SAT solvers; SAT-solving; Satisfiability; Search method; State-space; Variable selection; Artificial intelligence; Benchmarking; Heuristic algorithms; Planning; Formal logic",Article,Scopus,2-s2.0-84866502355
"Merz B., Vorogushyn S., Uhlemann S., Delgado J., Hundecha Y.","HESS Opinions: ""More efforts and scientific rigour are needed to attribute trends in flood time series""",2012,"Hydrology and Earth System Sciences",55,10.5194/hess-16-1379-2012,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863847971&doi=10.5194%2fhess-16-1379-2012&partnerID=40&md5=9fe7c187bef84e38ef670adf13d14d83","The question whether the magnitude and frequency of floods have changed due to climate change or other drivers of change is of high interest. The number of flood trend studies is rapidly rising. When changes are detected, many studies link the identified change to the underlying causes, i.e. they attribute the changes in flood behaviour to certain drivers of change. We propose a hypothesis testing framework for trend attribution which consists of essential ingredients for a sound attribution: evidence of consistency, evidence of inconsistency, and provision of confidence statement. Further, we evaluate the current state-of-the-art of flood trend attribution.We assess how selected recent studies approach the attribution problem, and to which extent their attribution statements seem defendable. In our opinion, the current state of flood trend attribution is poor. Attribution statements are mostly based on qualitative reasoning or even speculation. Typically, the focus of flood trend studies is the detection of change, i.e. the statistical analysis of time series, and attribution is regarded as an appendix: (1) flood time series are analysed by means of trend tests, (2) if a significant change is detected, a hypothesis on the cause of change is given, and (3) explanations or published studies are sought which support the hypothesis. We believe that we need a change in perspective and more scientific rigour: detection should be seen as an integral part of the more challenging attribution problem, and detection and attribution should be placed in a sound hypothesis testing framework. © 2012 Author(s).",,"Hypothesis testing; Integral part; Qualitative reasoning; Trend tests; Underlying cause; Artificial intelligence; Climate change; Time series; climate change; flood frequency; hypothesis testing; qualitative analysis; time series; trend analysis",Article,Scopus,2-s2.0-84863847971
"Duraipandian S., Bergholt M.S., Zheng W., Ho K.Y., Teh M., Yeoh K.G., So J.B.Y., Shabbir A., Huang Z.","Real-time Raman spectroscopy for in vivo, online gastric cancer diagnosis during clinical endoscopic examination",2012,"Journal of Biomedical Optics",55,10.1117/1.JBO.17.8.081418,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870605559&doi=10.1117%2f1.JBO.17.8.081418&partnerID=40&md5=837c2b19053746f70cc2d4bb2743bf49","Optical spectroscopic techniques including reflectance, fluorescence and Raman spectroscopy have shown promising potential for in vivo precancer and cancer diagnostics in a variety of organs. However, dataanalysis has mostly been limited to post-processing and off-line algorithm development. In this work, we develop a fully automated on-line Raman spectral diagnostics framework integrated with a multimodal image-guided Raman technique for real-time in vivo cancer detection at endoscopy. A total of 2748 in vivo gastric tissue spectra (2465 normal and 283 cancer) were acquired from 305 patients recruited to construct a spectral database for diagnostic algorithms development. The novel diagnostic scheme developed implements on-line preprocessing, outlier detection based on principal component analysis statistics (i.e., Hotelling's T2 and Q-residuals) for tissue Raman spectra verification as well as for organ specific probabilistic diagnostics using different diagnostic algorithms. Freerunning optical diagnosis and processing time of < 0.5 s can be achieved, which is critical to realizing real-time in vivo tissue diagnostics during clinical endoscopic examination. The optimized partial least squares-discriminant analysis (PLS-DA) models based on the randomly resampled training database (80% for learning and 20% for testing) provide the diagnostic accuracy of 85.6% [95% confidence interval (CI): 82.9% to 88.2%] [sensitivity of 80.5% (95% CI: 71.4% to 89.6%) and specificity of 86.2% (95% CI: 83.6% to 88.7%)] for the detection of gastric cancer. The PLS-DA algorithms are further applied prospectively on 10 gastric patients at gastroscopy, achieving the predictive accuracy of 80.0% (60/75) [sensitivity of 90.0% (27/30) and specificity of 73.3% (33/45)] for in vivo diagnosis of gastric cancer. The receiver operating characteristics curves further confirmed the efficacy of Raman endoscopy together with PLS-DA algorithms for in vivo prospective diagnosis of gastric cancer. This work successfully moves biomedical Raman spectroscopic technique into real-time, on-line clinical cancer diagnosis, especially in routine endoscopic diagnostic applications. © 2012 Society of Photo-Optical Instrumentation Engineers (SPIE).","Cancer diagnostics; In vivo optical diagnosis; Multivariate analysis; Raman spectroscopy","tumor marker; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; computer system; gastrointestinal endoscopy; human; instrumentation; metabolism; methodology; online system; Raman spectrometry; reproducibility; sensitivity and specificity; stomach tumor; Artificial Intelligence; Computer Systems; Diagnosis, Computer-Assisted; Endoscopy, Gastrointestinal; Humans; Online Systems; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Spectrum Analysis, Raman; Stomach Neoplasms; Tumor Markers, Biological",Article,Scopus,2-s2.0-84870605559
"Jung C., Kim C.","A unified spectral-domain approach for saliency detection and its application to automatic object segmentation",2012,"IEEE Transactions on Image Processing",55,10.1109/TIP.2011.2164420,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863121397&doi=10.1109%2fTIP.2011.2164420&partnerID=40&md5=ca521dfcd4e0101f096f6bede3782f5e","In this paper, a visual attention model is incorporated for efficient saliency detection, and the salient regions are employed as object seeds for our automatic object segmentation system. In contrast with existing interactive segmentation approaches that require considerable user interaction, the proposed method does not require it, i.e., the segmentation task is fulfilled in a fully automatic manner. First, we introduce a novel unified spectral-domain approach for saliency detection. Our visual attention model originates from a well-known property of the human visual system that the human visual perception is highly adaptive and sensitive to structural information in images rather than nonstructural information. Then, based on the saliency map, we propose an iterative self-adaptive segmentation framework for more accurate object segmentation. Extensive tests on a variety of cluttered natural images show that the proposed algorithm is an efficient indicator for characterizing the human perception and it can provide satisfying segmentation performance. © 2011 IEEE.","Automatic object segmentation; graph cut; human visual system (HVS); saliency detection; spectral-domain analysis","Graph cut; Human visual systems; Object segmentation; Saliency detection; Spectral domain analysis; Algorithms; Image segmentation; algorithm; article; artificial intelligence; attention; automated pattern recognition; human; methodology; vision; Algorithms; Artificial Intelligence; Attention; Humans; Pattern Recognition, Automated; Visual Perception",Article,Scopus,2-s2.0-84863121397
"Tang L., Gao H., Zhu X., Wang X., Zhou M., Jiang R.","Construction of ""small-intelligent"" focused mutagenesis libraries using well-designed combinatorial degenerate primers",2012,"BioTechniques",55,10.2144/000113820,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863280525&doi=10.2144%2f000113820&partnerID=40&md5=5d51ea2da45a600e12783b9605ad0aee","Site-saturation mutagenesis is a powerful tool for protein optimization due to its efficiency and simplicity. A degenerate codon NNN or NNS (K) is often used to encode the 20 standard amino acids, but this will produce redundant codons and cause uneven distribution of amino acids in the constructed library. Here we present a novel ""small-intelligent"" strategy to construct mutagenesis libraries that have a minimal gene library size without inherent amino acid biases, stop codons, or rare codons of Escherichia coli by coupling well-designed combinatorial degenerate primers with suitable PCR-based mutagenesis methods. The designed primer mixture contains exactly one codon per amino acid and thus allows the construction of small-intelligent mutagenesis libraries with one gene per protein. In addition, the software tool DC-Analyzer was developed to assist in primer design according to the user-defined randomization scheme for library construction. This small-intelligent strategy was successfully applied to the randomization of halohydrin dehalogenases with one or two randomized sites. With the help of DC-Analyzer, the strategy was proven to be as simple as NNS randomization and could serve as a general tool to efficiently randomize target genes at positions of interest.","Amino acid bias; Codon redundancy; Degenerate codon design; Library construction; Randomization","amino acid; article; Escherichia coli; gene library; nucleotide sequence; polymerase chain reaction; site directed mutagenesis; stop codon; Artificial Intelligence; Codon; Computational Biology; DNA Primers; Escherichia coli; Gene Library; Genes, Bacterial; Hydrolases; Mutagenesis, Site-Directed; Software; Escherichia coli",Article,Scopus,2-s2.0-84863280525
"Escolano C., Antelis J.M., Minguez J.","A telepresence mobile robot controlled with a noninvasive brain-computer interface",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",55,10.1109/TSMCB.2011.2177968,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861189819&doi=10.1109%2fTSMCB.2011.2177968&partnerID=40&md5=aff9506f2c9f9d545c5976bf1d85a322","This paper reports an electroencephalogram-based brain-actuated telepresence system to provide a user with presence in remote environments through a mobile robot, with access to the Internet. This system relies on a P300-based brain-computer interface (BCI) and a mobile robot with autonomous navigation and camera orientation capabilities. The shared-control strategy is built by the BCI decoding of task-related orders (selection of visible target destinations or exploration areas), which can be autonomously executed by the robot. The system was evaluated using five healthy participants in two consecutive steps: 1) screening and training of participants and 2) preestablished navigation and visual exploration telepresence tasks. On the basis of the results, the following evaluation studies are reported: 1) technical evaluation of the device and its main functionalities and 2) the users' behavior study. The overall result was that all participants were able to complete the designed tasks, reporting no failures, which shows the robustness of the system and its feasibility to solve tasks in real settings where joint navigation and visual exploration were needed. Furthermore, the participants showed great adaptation to the telepresence system. © 2012 IEEE.","Brain computer interfaces; rehabilitation robotics; telerobotics","Autonomous navigation; Brain-computer interfaces (BCI); Camera orientation; Evaluation study; Rehabilitation robotics; Remote environment; Technical evaluation; Tele-robotics; Telepresence; Telepresence systems; Visual exploration; Electronic document exchange; Mobile robots; Navigation; Robotics; Visual communication; Brain computer interface; algorithm; article; artificial intelligence; computer interface; computer simulation; decision support system; event related potential; human; man machine interaction; methodology; psychophysiology; robotics; theoretical model; Algorithms; Artificial Intelligence; Biofeedback, Psychology; Computer Simulation; Decision Support Techniques; Event-Related Potentials, P300; Humans; Man-Machine Systems; Models, Theoretical; Robotics; User-Computer Interface",Article,Scopus,2-s2.0-84861189819
"Liu K.-L., Li W.-J., Guo M.","Emoticon smoothed language models for twitter sentiment analysis",2012,"Proceedings of the National Conference on Artificial Intelligence",54,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868274072&partnerID=40&md5=a9f1e565ab4693b0db6781bc0ec2c0e4","Twitter sentiment analysis (TSA) has become a hot research topic in recent years. The goal of this task is to discover the attitude or opinion of the tweets, which is typically formulated as a machine learning based text classification problem. Some methods use manually labeled data to train fully supervised models, while others use some noisy labels, such as emoticons and hashtags, for model training. In general, we can only get a limited number of training data for the fully supervised models because it is very labor-intensive and time-consuming to manually label the tweets. As for the models with noisy labels, it is hard for them to achieve satisfactory performance due to the noise in the labels although it is easy to get a large amount of data for training. Hence, the best strategy is to utilize both manually labeled data and noisy labeled data for training. However, how to seamlessly integrate these two different kinds of data into the same learning framework is still a challenge. In this paper, we present a novel model, called emoticon smoothed language model (ESLAM), to handle this challenge. The basic idea is to train a language model based on the manually labeled data, and then use the noisy emoticon data for smoothing. Experiments on real data sets demonstrate that ESLAM can effectively integrate both kinds of data to outperform those methods using only one of them. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Hot research topics; Labeled data; Language model; Learning frameworks; Model training; Noisy labels; Real data sets; Sentiment analysis; Text classification; Training data; Artificial intelligence; Classification (of information); Data mining; Social networking (online); Computational linguistics",Conference Paper,Scopus,2-s2.0-84868274072
"Tagliazucchi E., von Wegner F., Morzelewski A., Borisov S., Jahnke K., Laufs H.","Automatic sleep staging using fMRI functional connectivity data",2012,"NeuroImage",54,10.1016/j.neuroimage.2012.06.036,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864413510&doi=10.1016%2fj.neuroimage.2012.06.036&partnerID=40&md5=606dc0274072aa8488f797862260ebe5","Recent EEG-fMRI studies have shown that different stages of sleep are associated with changes in both brain activity and functional connectivity. These results raise the concern that lack of vigilance measures in resting state experiments may introduce confounds and contamination due to subjects falling asleep inside the scanner. In this study we present a method to perform automatic sleep staging using only fMRI functional connectivity data, thus providing vigilance information while circumventing the technical demands of simultaneous recording of EEG, the gold standard for sleep scoring.The features to classify are the linear correlation values between 20 cortical regions identified using independent component analysis and two regions in the bilateral thalamus. The method is based on the construction of binary support vector machine classifiers discriminating between all pairs of sleep stages and the subsequent combination of them into multiclass classifiers. Different multiclass schemes and kernels are explored.After parameter optimization through 5-fold cross validation we achieve accuracies over 0.8 in the binary problem with functional connectivities obtained for epochs as short as 60. s. The multiclass classifier generalizes well to two independent datasets (accuracies over 0.8 in both sets) and can be efficiently applied to any dataset using a sliding window procedure. Modeling vigilance states in resting state analysis will avoid confounded inferences and facilitate the study of vigilance states themselves. We thus consider the method introduced in this study a novel and practical contribution for monitoring vigilance levels inside an MRI scanner without the need of extra recordings other than fMRI BOLD signals. © 2012 Elsevier Inc.","FMRI; Functional connectivity; Resting state; Sleep staging; Support vector machines","accuracy; article; BOLD signal; brain radiography; brain region; controlled study; electroencephalogram; executive function; functional magnetic resonance imaging; gold standard; human; human experiment; neuroimaging; nonREM sleep; prediction; priority journal; process optimization; scoring system; sensitivity and specificity; sleep stage; sleep waking cycle; steady state; support vector machine; thalamus; validation process; Adult; Algorithms; Artificial Intelligence; Brain; Connectome; Electroencephalography; Female; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Information Storage and Retrieval; Magnetic Resonance Imaging; Male; Nerve Net; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Sleep Stages; Young Adult",Article,Scopus,2-s2.0-84864413510
"Joshi A.J., Porikli F., Papanikolopoulos N.P.","Scalable active learning for multiclass image classification",2012,"IEEE Transactions on Pattern Analysis and Machine Intelligence",54,10.1109/TPAMI.2012.21,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866702739&doi=10.1109%2fTPAMI.2012.21&partnerID=40&md5=925ff2440b8d50e26dc1423b19df8b45","Machine learning techniques for computer vision applications like object recognition, scene classification, etc., require a large number of training samples for satisfactory performance. Especially when classification is to be performed over many categories, providing enough training samples for each category is infeasible. This paper describes new ideas in multiclass active learning to deal with the training bottleneck, making it easier to train large multiclass image classification systems. First, we propose a new interaction modality for training which requires only yes-no type binary feedback instead of a precise category label. The modality is especially powerful in the presence of hundreds of categories. For the proposed modality, we develop a Value-of-Information (VOI) algorithm that chooses informative queries while also considering user annotation cost. Second, we propose an active selection measure that works with many categories and is extremely fast to compute. This measure is employed to perform a fast seed search before computing VOI, resulting in an algorithm that scales linearly with dataset size. Third, we use locality sensitive hashing to provide a very fast approximation to active learning, which gives sublinear time scaling, allowing application to very large datasets. The approximation provides up to two orders of magnitude speedups with little loss in accuracy. Thorough empirical evaluation of classification accuracy, noise sensitivity, imbalanced data, and computational performance on a diverse set of image datasets demonstrates the strengths of the proposed algorithms. © 2012 IEEE.","Active learning; multiclass classification; object recognition; scalable machine learning","Active Learning; Binary feedback; Classification accuracy; Computational performance; Computer vision applications; Data set size; Empirical evaluations; Fast approximation; Image classification systems; Image datasets; Imbalanced data; Large datasets; Locality sensitive hashing; Machine learning techniques; Multi-class; Multi-class classification; Noise sensitivity; Orders of magnitude; Scene classification; Sublinear time; Training sample; User annotations; Algorithms; Computer vision; Image classification; Learning systems; Object recognition; Sampling; Classification (of information); algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; image enhancement; methodology; reproducibility; sensitivity and specificity; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84866702739
"Hernández-Orallo J., Flach P., Ferri C.","A unified view of performance metrics: Translating threshold choice into expected classification loss",2012,"Journal of Machine Learning Research",54,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869160181&partnerID=40&md5=f1ba062b55fc28fb73a053ba2e27d8a0","Many performance metrics have been introduced in the literature for the evaluation of classification performance, each of them with different origins and areas of application. These metrics include accuracy, unweighted accuracy, the area under the ROC curve or the ROC convex hull, the mean absolute error and the Brier score or mean squared error (with its decomposition into refinement and calibration). One way of understanding the relations among these metrics is by means of variable operating conditions (in the form of misclassification costs and/or class distributions). Thus, a metric may correspond to some expected loss over different operating conditions. One dimension for the analysis has been the distribution for this range of operating conditions, leading to some important connections in the area of proper scoring rules. We demonstrate in this paper that there is an equally important dimension which has so far received much less attention in the analysis of performance metrics. This dimension is given by the decision rule, which is typically implemented as a threshold choice method when using scoring models. In this paper, we explore many old and new threshold choice methods: fixed, score-uniform, score-driven, rate-driven and optimal, among others. By calculating the expected loss obtained with these threshold choice methods for a uniform range of operating conditions we give clear interpretations of the 0-1 loss, the absolute error, the Brier score, the AUC and the refinement loss respectively. Our analysis provides a comprehensive view of performance metrics as well as a systematic approach to loss minimisation which can be summarised as follows: given a model, apply the threshold choice methods that correspond with the available information about the operating condition, and compare their expected losses. In order to assist in this procedure we also derive several connections between the aforementioned performance metrics, and we highlight the role of calibration in choosing the threshold choice method. © 2012 Jose Hernandez-Orallo, Peter Flach and Cesar Ferri.","Area under the ROC curve (AUC); Brier score; Calibration loss; Classification performance metrics; Cost-sensitive evaluation; Operating condition; Refinement loss","Area under the ROC curve; Brier score; Classification performance; Cost-sensitive evaluation; Operating condition; Artificial intelligence; Software engineering; Calibration",Article,Scopus,2-s2.0-84869160181
"Banharnsakun A., Sirinaovakul B., Achalakul T.","Job shop scheduling with the Best-so-far ABC",2012,"Engineering Applications of Artificial Intelligence",54,10.1016/j.engappai.2011.08.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857458697&doi=10.1016%2fj.engappai.2011.08.003&partnerID=40&md5=8ffb6195a5857e6179220d5edd860046","The Job Shop Scheduling Problem (JSSP) is known as one of the most difficult scheduling problems. It is an important practical problem in the fields of production management and combinatorial optimization. Since JSSP is NP-complete, meaning that the selection of the best scheduling solution is not polynomially bounded, heuristic approaches are often considered. Inspired by the decision making capability of bee swarms in the nature, this paper proposes an effective scheduling method based on Best-so-far Artificial Bee Colony (Best-so-far ABC) for solving the JSSP. In this method, we bias the solution direction toward the Best-so-far solution rather a neighboring solution as proposed in the original ABC method. We also use the set theory to describe the mapping of our proposed method to the problem in the combinatorial optimization domain. The performance of the proposed method is then empirically assessed using 62 benchmark problems taken from the Operations Research Library (OR-Library). The solution quality is measured based on Best, Average, Standard Deviation (S.D.), and Relative Percent Error (RPE) of the objective value. The results demonstrate that the proposed method is able to produce higher quality solutions than the current state-of-the-art heuristic-based algorithms. © 2011 Published by Elsevier Ltd. All rights reserved.","Best-so-far Artificial Bee Colony (Best-so-far ABC); Job Shop Scheduling Problem (JSSP); Swarm intelligence; Variable Neighboring Search (VNS)","ABC method; Artificial bee colonies; Bench-mark problems; Heuristic approach; Job shop scheduling problems; Job-Shop scheduling; Neighboring search; NP Complete; Practical problems; Production management; Scheduling methods; Scheduling problem; Selection of the best; Solution quality; Standard deviation; Swarm intelligence; Artificial intelligence; Benchmarking; Cellular automata; Combinatorial optimization; Heuristic algorithms; Industrial management; Scheduling; Heuristic methods",Article,Scopus,2-s2.0-84857458697
"Menhas M.I., Wang L., Fei M., Pan H.","Comparative performance analysis of various binary coded PSO algorithms in multivariable PID controller design",2012,"Expert Systems with Applications",54,10.1016/j.eswa.2011.09.152,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-82255183109&doi=10.1016%2fj.eswa.2011.09.152&partnerID=40&md5=d705711abf703878849442fe52671741","In this paper, comparative performance analysis of various binary coded PSO algorithms on optimal PI and PID controller design for multiple inputs multiple outputs (MIMO) process is stated. Four algorithms such as modified particle swarm optimization (MPSO), discrete binary PSO (DBPSO), modified discrete binary PSO (MBPSO) and probability based binary PSO (PBPSO) are independently realized using MATLAB. The MIMO process of binary distillation column plant, described by Wood and Berry, with and without a decoupler having two inputs and two outputs is considered. Simulations are carried out to minimize two objective functions, that is, time integral of absolute error (ITAE) and integral of absolute error (IAE) with single stopping criterion for each algorithm called maximum number of fitness evaluations. The simulation experiments are repeated 20 times with each algorithm in each case. The performance measures for comparison of various algorithms such as mean fitness, variance of fitness, and best fitness are computed. The transient performance indicators and computation time are also recorded. The inferences are made based on analysis of statistical data obtained from 20 trials of each algorithm and after having comparison with some recently reported results about same MIMO controller design employing real coded genetic algorithm (RGA) with SBX and multi-crossover approaches, covariance matrix adaptation evolution strategy (CMAES), differential evolution (DE), modified continuous PSO (MPSO) and biggest log modulus tuning (BLT). On the basis of simulation results PBPSO is identified as a comparatively better method in terms of its simplicity, consistency, search and computational efficiency. © 2011 Elsevier Ltd. All rights reserved.","Binary PSO; Particle swarm optimization; PID control; PID tuning; Swarm intelligence","Absolute error; Binary distillation columns; Binary PSO; Comparative performance analysis; Computation time; Controller designs; Covariance matrix adaptation evolution strategies; Decouplers; Differential evolution; Discrete binary; Fitness evaluations; Modified particle swarm optimization; Multiple inputs; Multiple outputs; Multivariable PID controller; Objective functions; Particle swarm; Performance measure; PID controller design; PID tuning; PSO algorithms; Real-coded genetic algorithm; Simulation experiments; Statistical datas; Stopping criteria; Swarm Intelligence; Time integrals; Transient performance; Algorithms; Artificial intelligence; Benchmarking; Cellular automata; Computational efficiency; Controllers; Covariance matrix; Design; Distillation; Distillation columns; Electric control equipment; Health; MATLAB; Proportional control systems; Three term control systems; Particle swarm optimization (PSO)",Article,Scopus,2-s2.0-82255183109
"Damljanovic D., Agatonovic M., Cunningham H.","FREyA: An interactive way of querying linked data using natural language",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",54,10.1007/978-3-642-25953-1_11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857073165&doi=10.1007%2f978-3-642-25953-1_11&partnerID=40&md5=02a0e54c5d557a4470000a8af0217b50","Natural Language Interfaces are increasingly relevant for information systems fronting rich structured data stores such as RDF and OWL repositories, mainly because of the conception of them being intuitive for human. In the previous work, we developed FREyA, an interactive Natural Language Interface for querying ontologies. It uses syntactic parsing in combination with the ontology-based lookup in order to interpret the question, and involves the user if necessary. The user's choices are used for training the system in order to improve its performance over time. In this paper, we discuss the suitability of FREyA to query the Linked Open Data. We report its performance in terms of precision and recall using the MusicBrainz and DBpedia datasets. © 2012 Springer-Verlag.","clarification dialogs; learning; Natural language interfaces; ontologies; question-answering","Data sets; Interactive way; learning; Linked datum; Lookups; Natural language interfaces; Natural languages; Ontology-based; Precision and recall; Querying ontologies; Question Answering; Structured data; Syntactic parsing; Ontology; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84857073165
"Shirehjini A.A.N., Yassine A., Shirmohammadi S.","An RFID-based position and orientation measurement system for mobile objects in intelligent environments",2012,"IEEE Transactions on Instrumentation and Measurement",54,10.1109/TIM.2011.2181912,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861198646&doi=10.1109%2fTIM.2011.2181912&partnerID=40&md5=ecf1bfa1d1cdfffb1e80bb25f4a42be7","Ambient intelligence (AmI) considers responsive environments in which applications and services adapt their behavior according to the user's needs and changing context. One of the most challenging aspects for many applications in AmI environments is location and orientation of the surrounding objects. This is especially important for effective cooperation among mobile physical objects in such smart environments. In this paper, we propose a robust indoor positioning system that provides 2-D positioning and orientation information for mobile objects. The system utilizes low-range passive radio frequency identification (RFID) technology. The proposed system, which consists of RFID carpets and several peripherals for sensor data interpretation, is implemented and tested through extensive experiments. Our results show that the proposed system outperforms similar existing systems in minimizing the average positioning error. © 2012 IEEE.","Ambient intelligence (AmI); location and orientation measurement; radio frequency identification (RFID); smart environment","Ambient intelligence; Average positioning error; Existing systems; Intelligent environment; Mobile objects; Orientation information; Orientation measurements; Physical objects; Radio frequency identification technology; Responsive environments; Robust indoor positioning; Sensor data; Smart environment; Artificial intelligence; Data processing; Radio frequency identification (RFID)",Article,Scopus,2-s2.0-84861198646
"Calderoni S., Retico A., Biagi L., Tancredi R., Muratori F., Tosetti M.","Female children with autism spectrum disorder: An insight from mass-univariate and pattern classification analyses",2012,"NeuroImage",54,10.1016/j.neuroimage.2011.08.070,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-83055181241&doi=10.1016%2fj.neuroimage.2011.08.070&partnerID=40&md5=687c2adb2a9fa5255af7d4ad0537cdf7","Several studies on structural MRI in children with autism spectrum disorders (ASD) have mainly focused on samples prevailingly consisting of males. Sex differences in brain structure are observable since infancy and therefore caution is required in transferring to females the results obtained for males. The neuroanatomical phenotype of female children with ASD (ASDf) represents indeed a neglected area of research. In this study, we investigated for the first time the anatomic brain structures of a sample entirely composed of ASDf (n = 38; 2-7. years of age; mean = 53. months; SD = 18) with respect to 38 female age and non verbal IQ matched controls, using both mass-univariate and pattern classification approaches. The whole brain volumes of each group were compared using voxel-based morphometry (VBM) with diffeomorphic anatomical registration through exponentiated lie algebra (DARTEL) procedure, allowing us to build a study-specific template. Significantly more gray matter (GM) was found in the left superior frontal gyrus (SFG) in ASDf subjects compared to controls. The GM segments obtained in the VBM-DARTEL preprocessing are also classified with a support vector machine (SVM), using the leave-pair-out cross-validation protocol. Then, the recursive feature elimination (SVM-RFE) approach allows for the identification of the most discriminating voxels in the GM segments and these prove extremely consistent with the SFG region identified by the VBM analysis. Furthermore, the SVM-RFE map obtained with the most discriminating set of voxels corresponding to the maximum Area Under the Receiver Operating Characteristic Curve (AUC max=0.80) highlighted a more complex circuitry of increased cortical volume in ASDf, involving bilaterally the SFG and the right temporo-parietal junction (TPJ). The SFG and TPJ abnormalities may be relevant to the pathophysiology of ASDf, since these structures participate in some core atypical features of autism. © 2011 Elsevier Inc.","Autism spectrum disorders; Children; Female; Magnetic resonance imaging; Support vector machine; Voxel-based morphometry","area under the curve; article; autism; brain region; brain size; child; clinical article; controlled study; female; gray matter; human; nuclear magnetic resonance imaging; pathogenesis; pathological anatomy; priority journal; school child; scoring system; superior frontal gyrus; support vector machine; voxel based morphometry; Algorithms; Artificial Intelligence; Brain; Child; Child Development Disorders, Pervasive; Child, Preschool; Female; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Multivariate Analysis; Neurons; Organ Size; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-83055181241
"Zhang S., Huang J., Li H., Metaxas D.N.","Automatic image annotation and retrieval using group sparsity",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",54,10.1109/TSMCB.2011.2179533,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862808427&doi=10.1109%2fTSMCB.2011.2179533&partnerID=40&md5=adc12e99e0031033629572d748ff114f","Automatically assigning relevant text keywords to images is an important problem. Many algorithms have been proposed in the past decade and achieved good performance. Efforts have focused upon model representations of keywords, whereas properties of features have not been well investigated. In most cases, a group of features is preselected, yet important feature properties are not well used to select features. In this paper, we introduce a regularization-based feature selection algorithm to leverage both the sparsity and clustering properties of features, and incorporate it into the image annotation task. Using this group-sparsity-based method, the whole group of features [e.g., red green blue (RGB) or hue, saturation, and value (HSV)] is either selected or removed. Thus, we do not need to extract this group of features when new data comes. A novel approach is also proposed to iteratively obtain similar and dissimilar pairs from both the keyword similarity and the relevance feedback. Thus, keyword similarity is modeled in the annotation framework. We also show that our framework can be employed in image retrieval tasks by selecting different image pairs. Extensive experiments are designed to compare the performance between features, feature combinations, and regularization-based feature selection methods applied on the image annotation task, which gives insight into the properties of features in the image annotation task. The experimental results demonstrate that the group-sparsity-based method is more accurate and stable than others. © 2012 IEEE.","Corel5K; feature selection; group sparsity; IAPR TC12; image annotation; image retrieval; regularization","Corel5K; group sparsity; IAPR TC12; Image annotation; regularization; Algorithms; Feature extraction; Image analysis; Image retrieval; algorithm; article; artificial intelligence; automated pattern recognition; decision support system; documentation; hospital information system; information retrieval; methodology; natural language processing; Algorithms; Artificial Intelligence; Decision Support Techniques; Documentation; Information Storage and Retrieval; Natural Language Processing; Pattern Recognition, Automated; Radiology Information Systems",Article,Scopus,2-s2.0-84862808427
"De Lannoy G., François D., Delbeke J., Verleysen M.","Weighted conditional random fields for supervised interpatient heartbeat classification",2012,"IEEE Transactions on Biomedical Engineering",54,10.1109/TBME.2011.2171037,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84555190723&doi=10.1109%2fTBME.2011.2171037&partnerID=40&md5=dac501685203332a4b09f18031385230","This paper proposes a method for the automatic classification of heartbeats in an ECG signal. Since this task has specific characteristics such as time dependences between observations and a strong class unbalance, a specific classifier is proposed and evaluated on real ECG signals from the MIT arrhythmia database. This classifier is a weighted variant of the conditional random fields classifier. Experiments show that the proposed method outperforms previously reported heartbeat classification methods, especially for the pathological heartbeats. © 2011 IEEE.","Classification; conditional random fields (CRFs); electrocardiogram (ECG); physiobank; unbalance","Automatic classification; Classification methods; Conditional random field; ECG signals; electrocardiogram (ECG); physiobank; Time dependence; unbalance; Classification (of information); Content based retrieval; Electrocardiography; Random processes; article; classification; clinical practice; data base; electrocardiogram; heart arrhythmia; heart beat; human; medical instrumentation; medical society; practice guideline; standard; United States; Algorithms; Arrhythmias, Cardiac; Artificial Intelligence; Diagnosis, Computer-Assisted; Electrocardiography; Heart Rate; Humans; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84555190723
"Eickholt J., Cheng J.","Predicting protein residue-residue contacts using deep networks and boosting",2012,"Bioinformatics",53,10.1093/bioinformatics/bts598,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870415234&doi=10.1093%2fbioinformatics%2fbts598&partnerID=40&md5=8a18947d93e78c125701b60fce7bdaf9","Motivation: Protein residue-residue contacts continue to play a larger and larger role in protein tertiary structure modeling and evaluation. Yet, while the importance of contact information increases, the performance of sequence-based contact predictors has improved slowly. New approaches and methods are needed to spur further development and progress in the field.Results: Here we present DNCON, a new sequence-based residue-residue contact predictor using deep networks and boosting techniques. Making use of graphical processing units and CUDA parallel computing technology, we are able to train large boosted ensembles of residue-residue contact predictors achieving state-of-the-art performance. © The Author 2012. Published by Oxford University Press. All rights reserved.",,"protein; article; artificial intelligence; biology; chemistry; Internet; methodology; protein tertiary structure; statistical model; Artificial Intelligence; Computational Biology; Internet; Models, Statistical; Protein Structure, Tertiary; Proteins",Article,Scopus,2-s2.0-84870415234
"Koutsouleris N., Borgwardt S., Meisenzahl E.M., Bottlender R., Möller H.-J., Riecher-Rössler A.","Disease prediction in the at-risk mental state for psychosis using neuroanatomical biomarkers: Results from the fepsy study",2012,"Schizophrenia Bulletin",53,10.1093/schbul/sbr145,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869050026&doi=10.1093%2fschbul%2fsbr145&partnerID=40&md5=8e4f9c80d047ab0ce653052a94b0c9e5","Background: Reliable prognostic biomarkers are needed for the early recognition of psychosis. Recently, multivariate machine learning methods have demonstrated the feasibility to predict illness onset in clinically defined at-risk individuals using structural magnetic resonance imaging (MRI) data. However, it remains unclear whether these findings could be replicated in independent populations. Methods: We evaluated the performance of an MRI-based classification system in predicting disease conversion in at-risk individuals recruited within the prospective FePsy (Früherkennung von Psychosen) study at the University of Basel, Switzerland. Pairwise and multigroup biomarkers were constructed using the MRI data of 22 healthy volunteers, 16/21 at-risk subjects with/without a subsequent disease conversion. Diagnostic performance was measured in unseen test cases using repeated nested cross-validation. Results: The classification accuracies in the ""healthy controls (HCs) vs converters,"" ""HCs vs nonconverters,"" and ""converters vs nonconverters"" analyses were 92.3%, 66.9%, and 84.2%, respectively. A positive likelihood ratio of 6.5 in the converters vs nonconverters analysis indicated a 40% increase in diagnostic certainty by applying the biomarker to an at-risk population with a transition rate of 43%. The neuroanatomical decision functions underlying these results particularly involved the prefrontal perisylvian and subcortical brain structures. Conclusions: Our findings suggest that the early prediction of psychosis may be reliably enhanced using neuroanatomical pattern recognition operating at the single-subject level. These MRI-based biomarkers may have the potential to identify individuals at the highest risk of developing psychosis, and thus may promote informed clinical strategies aiming at preventing the full manifestation of the disease. © 2011 The Author.","biomarkers; early recognition; machine learning; magnetic resonance imaging; psychosis","biological marker; adult; article; clinical article; controlled study; diagnostic accuracy; disease classification; early diagnosis; female; human; male; mental health; neuroanatomy; nuclear magnetic resonance imaging; performance; prediction; priority journal; psychosis; subcortex; validation study; Adult; Artificial Intelligence; Brain; Case-Control Studies; Female; Humans; Image Processing, Computer-Assisted; Longitudinal Studies; Magnetic Resonance Imaging; Male; Prodromal Symptoms; Prognosis; Prospective Studies; Psychotic Disorders; Sensitivity and Specificity",Article,Scopus,2-s2.0-84869050026
"Barringer H., Falcone Y., Havelund K., Reger G., Rydeheard D.","Quantified event automata: Towards expressive and efficient runtime monitors",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",53,10.1007/978-3-642-32759-9_9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865957601&doi=10.1007%2f978-3-642-32759-9_9&partnerID=40&md5=acb937d39eb32f72a96d87e16ead5248","Runtime verification is the process of checking a property on a trace of events produced by the execution of a computational system. Runtime verification techniques have recently focused on parametric specifications where events take data values as parameters. These techniques exist on a spectrum inhabited by both efficient and expressive techniques. These characteristics are usually shown to be conflicting - in state-of-the-art solutions, efficiency is obtained at the cost of loss of expressiveness and vice-versa. To seek a solution to this conflict we explore a new point on the spectrum by defining an alternative runtime verification approach. We introduce a new formalism for concisely capturing expressive specifications with parameters. Our technique is more expressive than the currently most efficient techniques while at the same time allowing for optimizations. © 2012 Springer-Verlag.",,"Computational system; Data values; Run-time verification; Runtime monitors; Artificial intelligence; Specifications",Conference Paper,Scopus,2-s2.0-84865957601
"Nishant K., Sharma P., Krishna V., Gupta C., Singh K.P., Nitin, Rastogi R.","Load balancing of nodes in cloud using ant colony optimization",2012,"Proceedings - 2012 14th International Conference on Modelling and Simulation, UKSim 2012",53,10.1109/UKSim.2012.11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863104051&doi=10.1109%2fUKSim.2012.11&partnerID=40&md5=17391f517d05549c8b5e8c27d5c6ed09","In this paper, we proposed an algorithm for load distribution of workloads among nodes of a cloud by the use of Ant Colony Optimization (ACO). This is a modified approach of ant colony optimization that has been applied from the perspective of cloud or grid network systems with the main aim of load balancing of nodes. This modified algorithm has an edge over the original approach in which each ant build their own individual result set and it is later on built into a complete solution. However, in our approach the ants continuously update a single result set rather than updating their own result set. Further, as we know that a cloud is the collection of many nodes, which can support various types of application that is used by the clients on a basis of pay per use. Therefore, the system, which is incurring a cost for the user should function smoothly and should have algorithms that can continue the proper system functioning even at peak usage hours. © 2012 IEEE.","Ant colony optimization; Cloud computing; Grid networks; Load balancing","Ant Colony Optimization (ACO); Complete solutions; Grid network; Load distributions; Modified algorithms; Pay-per-use; Artificial intelligence; Cloud computing; Parallel architectures; Resource allocation; Algorithms",Conference Paper,Scopus,2-s2.0-84863104051
"Ricca F., Grasso G., Alviano M., Manna M., Lio V., Iiritano S., Leone N.","Team-building with answer set programming in the Gioia-Tauro seaport",2012,"Theory and Practice of Logic Programming",53,10.1017/S147106841100007X,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861367475&doi=10.1017%2fS147106841100007X&partnerID=40&md5=5435a586eb9f01b7610e244c428b6c6e","The seaport of Gioia Tauro is the largest transshipment terminal of the Mediterranean coast. A crucial management task for the companies operating in the seaport is team-building: the problem of properly allocating the available personnel for serving the incoming ships. Teams have to be carefully arranged in order to meet several constraints, such as allocation of employees with appropriate skills, fair distribution of the working load, and turnover of the heavy/dangerous roles. This makes team-building a hard and expensive task requiring several hours of manual preparation per day. In this paper we present a system based on Answer Set Programming for the automatic generation of the teams of employees in the seaport of Gioia Tauro. The system is currently exploited in the Gioia Tauro seaport by ICO BLG, a company specialized in automobile logistics. © Cambridge University Press 2011.","answer set programming; artificial intelligence; declarative problem solving; knowledge management; workforce management","Answer set programming; Automatic Generation; Fair distribution; Management tasks; Team building; Workforce management; Working loads; Artificial intelligence; Industry; Knowledge management; Logic programming; Management science; Human resource management",Article,Scopus,2-s2.0-84861367475
"Li X., Vucic J., Jungnickel V., Armstrong J.","On the capacity of intensity-modulated direct-detection systems and the information rate of ACO-OFDM for indoor optical wireless applications",2012,"IEEE Transactions on Communications",53,10.1109/TCOMM.2012.020612.090300,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862777629&doi=10.1109%2fTCOMM.2012.020612.090300&partnerID=40&md5=180b4ce901a468257dfde91b4ff4962d","In this paper we derive information theoretic results for asymmetrically clipped optical orthogonal frequency division multiplexing (ACO-OFDM) in an intensity modulated direct detection (IM/DD) optical communication system subject to a range of constraints. ACO-OFDM is a form of OFDM designed for IM/DD systems. It is an effective solution to intersymbol interference (ISI) caused by a dispersive channel and also requires less optical power than conventional optical modulation formats. Although the classical Shannon capacity formula cannot be applied directly to an IM/DD system, we show that when ACO-OFDM is used in an IM/DD system, it can be adapted to calculate the information rate of the data-carrying odd frequency subcarriers. As a result conventional water filling techniques can be used for a frequency selective channel. These results are applied to indoor wireless systems using realistic parameters for the transmitter, receiver and channel. The maximum rate at which data can be transmitted depends on the channel, the electrical bandwidth and the transmitted optical power. Even when there is no line of sight (LOS) path, when the electrical bandwidth is limited to 50 MHz and the average optical power is limited to 0.4 W, data rates of approximately 80 Mbit/s can theoretically be achieved. © 2012 IEEE.","ACO-OFDM; Channel capacity; information rate; intensity modulated direct detection; OFDM; optical wireless","ACO-OFDM; Data rates; Direct detection; Dispersive channels; Effective solution; Electrical bandwidth; Frequency selective channel; Indoor wireless systems; Information rates; Line-of-sight paths; Optical modulation format; Optical orthogonal frequency division multiplexing; Optical power; Optical wireless; Optical wireless applications; Shannon capacity; Sub-carriers; Water filling; Artificial intelligence; Bandwidth; Channel capacity; Intersymbol interference; Optical communication; Orthogonal frequency division multiplexing",Article,Scopus,2-s2.0-84862777629
"Kisi O., Shiri J., Nikoofar B.","Forecasting daily lake levels using artificial intelligence approaches",2012,"Computers and Geosciences",52,10.1016/j.cageo.2011.08.027,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857912130&doi=10.1016%2fj.cageo.2011.08.027&partnerID=40&md5=ea9e5caa339c2921b5ebb49808b9a3e1","Accurate prediction of lake-level variations is important for planning, design, construction, and operation of lakeshore structures and also in the management of freshwater lakes for water supply purposes. In the present paper, three artificial intelligence approaches, namely artificial neural networks (ANNs), adaptive-neuro-fuzzy inference system (ANFIS), and gene expression programming (GEP), were applied to forecast daily lake-level variations up to 3-day ahead time intervals. The measurements at the Lake Iznik in Western Turkey, for the period of January 1961-December 1982, were used for training, testing, and validating the employed models. The results obtained by the GEP approach indicated that it performs better than ANFIS and ANNs in predicting lake-level variations. A comparison was also made between these artificial intelligence approaches and convenient autoregressive moving average (ARMA) models, which demonstrated the superiority of GEP, ANFIS, and ANN models over ARMA models. © 2011 Elsevier Ltd.","Forecast; Genetic programming; Lake level; Neural networks; Neuro-fuzzy","Accurate prediction; Adaptive neurofuzzy inference system; Ahead-time; ARMA model; Autoregressive moving average model; Forecast; Fresh water lakes; Gene expression programming; Lake level; Lake levels; Lakeshore structure; Neuro-Fuzzy; Forecasting; Genetic programming; Neural networks; Water supply; Lakes; accuracy assessment; artificial intelligence; artificial neural network; fuzzy mathematics; lake level; model validation; performance assessment; prediction; testing method; water management; water planning; Bursa [Turkey]; Iznik Lake; Turkey",Article,Scopus,2-s2.0-84857912130
"Gottschlich C.","Curved-region-based ridge frequency estimation and curved gabor filters for fingerprint image enhancement",2012,"IEEE Transactions on Image Processing",52,10.1109/TIP.2011.2170696,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859072809&doi=10.1109%2fTIP.2011.2170696&partnerID=40&md5=a2ca30f55147fd796d3cc087f65f9f1a","Gabor filters (GFs) play an important role in many application areas for the enhancement of various types of images and the extraction of Gabor features. For the purpose of enhancing curved structures in noisy images, we introduce curved GFs that locally adapt their shape to the direction of flow. These curved GFs enable the choice of filter parameters that increase the smoothing power without creating artifacts in the enhanced image. In this paper, curved GFs are applied to the curved ridge and valley structures of low-quality fingerprint images. First, we combine two orientation-field estimation methods in order to obtain a more robust estimation for very noisy images. Next, curved regions are constructed by following the respective local orientation. Subsequently, these curved regions are used for estimating the local ridge frequency. Finally, curved GFs are defined based on curved regions, and they apply the previously estimated orientations and ridge frequencies for the enhancement of low-quality fingerprint images. Experimental results on the FVC2004 databases show improvements of this approach in comparison with state-of-the-art enhancement methods. © 2011 IEEE.","Biometrics; curvature; curved Gabor filters (GFs); curved regions; fingerprint recognition; FVC2004; image enhancement; orientation-field (OF)estimation; ridge frequency (RF) estimation; verification tests","curvature; curved regions; Fingerprint Recognition; FVC2004; Ridge frequency; verification tests; Biometrics; Estimation; Gabor filters; Image enhancement; Frequency estimation; algorithm; article; artificial intelligence; automated pattern recognition; biometry; computer assisted diagnosis; computer simulation; dermatoglyphics; human; image enhancement; image subtraction; methodology; reproducibility; sensitivity and specificity; statistical model; Algorithms; Artificial Intelligence; Biometry; Computer Simulation; Dermatoglyphics; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Models, Statistical; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-84859072809
"Ding J., Bashashati A., Roth A., Oloumi A., Tse K., Zeng T., Haffari G., Hirst M., Marra M.A., Condon A., Aparicio S., Shah S.P.","Feature-based classifiers for somatic mutation detection in tumour-normal paired sequencing data",2012,"Bioinformatics",52,10.1093/bioinformatics/btr629,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856080112&doi=10.1093%2fbioinformatics%2fbtr629&partnerID=40&md5=efd12643184a577ca3ae4c422fe43bbc","Motivation: The study of cancer genomes now routinely involves using next-generation sequencing technology (NGS) to profile tumours for single nucleotide variant (SNV) somatic mutations. However, surprisingly few published bioinformatics methods exist for the specific purpose of identifying somatic mutations from NGS data and existing tools are often inaccurate, yielding intolerably high false prediction rates. As such, the computational problem of accurately inferring somatic mutations from paired tumour/normal NGS data remains an unsolved challenge. Results: We present the comparison of four standard supervised machine learning algorithms for the purpose of somatic SNV prediction in tumour/normal NGS experiments. To evaluate these approaches (random forest, Bayesian additive regression tree, support vector machine and logistic regression), we constructed 106 features representing 3369 candidate somatic SNVs from 48 breast cancer genomes, originally predicted with naive methods and subsequently revalidated to establish ground truth labels. We trained the classifiers on this data (consisting of 1015 true somatic mutations and 2354 non-somatic mutation positions) and conducted a rigorous evaluation of these methods using a cross-validation framework and hold-out test NGS data from both exome capture and whole genome shotgun platforms. All learning algorithms employing predictive discriminative approaches with feature selection improved the predictive accuracy over standard approaches by statistically significant margins. In addition, using unsupervised clustering of the ground truth 'false positive' predictions, we noted several distinct classes and present evidence suggesting non-overlapping sources of technical artefacts illuminating important directions for future study. © The Author(s) 2011. Published by Oxford University Press.",,"algorithm; article; artificial intelligence; Bayes theorem; biological model; breast tumor; cluster analysis; computer program; exome; female; genetics; genome; human; mutation; neoplasm; single nucleotide polymorphism; support vector machine; Algorithms; Artificial Intelligence; Bayes Theorem; Breast Neoplasms; Cluster Analysis; Exome; Female; Genome; Humans; Models, Genetic; Mutation; Neoplasms; Polymorphism, Single Nucleotide; Software; Support Vector Machines",Article,Scopus,2-s2.0-84856080112
"Carlet C., Goubin L., Prouff E., Quisquater M., Rivain M.","Higher-order masking schemes for S-boxes",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",51,10.1007/978-3-642-34047-5_21,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866726757&doi=10.1007%2f978-3-642-34047-5_21&partnerID=40&md5=48112e3e3655962e1a1b200937b6659d","Masking is a common countermeasure against side-channel attacks. The principle is to randomly split every sensitive intermediate variable occurring in the computation into d+1 shares, where d is called the masking order and plays the role of a security parameter. The main issue while applying masking to protect a block cipher implementation is to design an efficient scheme for the s-box computations. Actually, masking schemes with arbitrary order only exist for Boolean circuits and for the AES s-box. Although any s-box can be represented as a Boolean circuit, applying such a strategy leads to inefficient implementation in software. The design of an efficient and generic higher-order masking scheme was hence until now an open problem. In this paper, we introduce the first masking schemes which can be applied in software to efficiently protect any s-box at any order. We first describe a general masking method and we introduce a new criterion for an s-box that relates to the best efficiency achievable with this method. Then we propose concrete schemes that aim to approach the criterion. Specifically, we give optimal methods for the set of power functions, and we give efficient heuristics for the general case. As an illustration we apply the new schemes to the DES and PRESENT s-boxes and we provide implementation results. © 2012 Springer-Verlag.",,"Arbitrary order; Block ciphers; Boolean circuit; Masking schemes; Optimal methods; Power functions; S-boxes; Security parameters; Side channel attack; Artificial intelligence; Cryptography",Conference Paper,Scopus,2-s2.0-84866726757
"Zuo C., Chen Q., Gu G., Feng S., Feng F.","High-speed three-dimensional profilometry for multiple objects with complex shapes",2012,"Optics Express",51,10.1364/OE.20.019493,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865598498&doi=10.1364%2fOE.20.019493&partnerID=40&md5=3719e5298539e7649e53d4b4a6d56974","This paper describes an easy-to-implement three-dimensional (3- D) real-time shape measurement technique using our newly developed high-speed 3-D vision system. It employs only four projection fringes to realize full-field phase unwrapping in the presence of discontinuous or isolated objects. With our self-designed pattern generation hardware and a modified low-cost DLP projectorthe four designed patterns can be generated and projected at a switching speed of 360 Hz. Using a properly synchronized high-speed camerathe high-speed fringe patterns distorted by measured objects can be acquired and processed in real-time. The resulting system can capture and display high-quality textured 3-D data at a speed of 120 frames per secondwith the resolution of 640 × 480 points. The speed can be trebled if a camera with a higher frame rate is employed. We detail our shape measurement techniqueincluding the four-pattern decoding algorithm as well as the hardware design. Some evaluation experiments have been carried out to demonstrate the validity and practicability of the proposed technique. © 2012 Optical Society of America.",,"Hardware; 3-D vision; Complex shapes; Decoding algorithm; Evaluation experiments; Frame rate; Fringe pattern; Full-field; Hardware design; High quality; High-speed; Multiple objects; Pattern Generation; Phase unwrapping; Shape measurements; Switching speed; Three dimensional; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; image enhancement; methodology; reproducibility; sensitivity and specificity; three dimensional imaging; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84865598498
"Zhang L., Wang L., Lin W.","Semisupervised biased maximum margin analysis for interactive image retrieval",2012,"IEEE Transactions on Image Processing",51,10.1109/TIP.2011.2177846,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859068530&doi=10.1109%2fTIP.2011.2177846&partnerID=40&md5=f5af43f94626a0c64b8d21c08f625fa0","With many potential practical applications, content-based image retrieval (CBIR) has attracted substantial attention during the past few years. A variety of relevance feedback (RF) schemes have been developed as a powerful tool to bridge the semantic gap between low-level visual features and high-level semantic concepts, and thus to improve the performance of CBIR systems. Among various RF approaches, support-vector-machine (SVM)-based RF is one of the most popular techniques in CBIR. Despite the success, directly using SVM as an RF scheme has two main drawbacks. First, it treats the positive and negative feedbacks equally, which is not appropriate since the two groups of training feedbacks have distinct properties. Second, most of the SVM-based RF techniques do not take into account the unlabeled samples, although they are very helpful in constructing a good classifier. To explore solutions to overcome these two drawbacks, in this paper, we propose a biased maximum margin analysis (BMMA) and a semisupervised BMMA (SemiBMMA) for integrating the distinct properties of feedbacks and utilizing the information of unlabeled samples for SVM-based RF schemes. The BMMA differentiates positive feedbacks from negative ones based on local analysis, whereas the SemiBMMA can effectively integrate information of unlabeled samples by introducing a Laplacian regularizer to the BMMA. We formally formulate this problem into a general subspace learning task and then propose an automatic approach of determining the dimensionality of the embedded subspace for RF. Extensive experiments on a large real-world image database demonstrate that the proposed scheme combined with the SVM RF can significantly improve the performance of CBIR systems. © 2011 IEEE.","Content-based image retrieval (CBIR); graph embedding; relevance feedback (RF); support vector machine (SVM)","A-Laplacian; CBIR system; Content-Based Image Retrieval; Graph embeddings; High level semantics; Interactive image retrieval; Local analysis; Maximum margin; Real-world image; Regularizer; Relevance feedback; Semantic gap; Semi-supervised; Subspace learning; support vector machine (SVM); Unlabeled samples; Visual feature; Content based retrieval; Feedback; Semantics; Vectors; Support vector machines; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; hospital information system; image enhancement; image subtraction; information retrieval; methodology; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Information Storage and Retrieval; Pattern Recognition, Automated; Radiology Information Systems; Subtraction Technique",Article,Scopus,2-s2.0-84859068530
"Huang S.-J., Zhou Z.-H.","Multi-label learning by exploiting label correlations locally",2012,"Proceedings of the National Conference on Artificial Intelligence",50,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868290385&partnerID=40&md5=a4e8edce30cf16c043a5de725fe323eb","It is well known that exploiting label correlations is important for multi-label learning. Existing approaches typically exploit label correlations globally, by assuming that the label correlations are shared by all the instances. In real-world tasks, however, different instances may share different label correlations, and few correlations are globally applicable. In this paper, we propose the ML-LOC approach which allows label correlations to be exploited locally. To encode the local influence of label correlations, we derive a LOC code to enhance the feature representation of each instance. The global discrimination fitting and local correlation sensitivity are incorporated into a unified framework, and an alternating solution is developed for the optimization. Experimental results on a number of image, text and gene data sets validate the effectiveness of our approach. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Data sets; Feature representation; Local correlations; Local influence; Multi-label; Real-world task; Unified framework; Learning systems; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868290385
"Zhang Y., Liu X., Chang M.-C., Ge W., Chen T.","Spatio-temporal phrases for activity recognition",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",50,10.1007/978-3-642-33712-3_51,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867850268&doi=10.1007%2f978-3-642-33712-3_51&partnerID=40&md5=ddbba1c38f9426db48d46f92f09bf910","The local feature based approaches have become popular for activity recognition. A local feature captures the local movement and appearance of a local region in a video, and thus can be ambiguous; e.g., it cannot tell whether a movement is from a person's hand or foot, when the camera is far away from the person. To better distinguish different types of activities, people have proposed using the combination of local features to encode the relationships of local movements. Due to the computation limit, previous work only creates a combination from neighboring features in space and/or time. In this paper, we propose an approach that efficiently identifies both local and long-range motion interactions; taking the ""push"" activity as an example, our approach can capture the combination of the hand movement of one person and the foot response of another person, the local features of which are both spatially and temporally far away from each other. Our computational complexity is in linear time to the number of local features in a video. The extensive experiments show that our approach is generically effective for recognizing a wide variety of activities and activities spanning a long term, compared to a number of state-of-the-art methods. © 2012 Springer-Verlag.","Activity Recognition; Spatio-Temporal Phrases","Activity recognition; Hand movement; Linear time; Local feature; Local region; Spatio-temporal; State-of-the-art methods; Artificial intelligence; Computer vision",Conference Paper,Scopus,2-s2.0-84867850268
"Schauerte B., Stiefelhagen R.","Quaternion-based spectral saliency detection for eye fixation prediction",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",50,10.1007/978-3-642-33709-3_9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867891493&doi=10.1007%2f978-3-642-33709-3_9&partnerID=40&md5=e2a2e01d85f6981db5ef2b6158a14d64","In recent years, several authors have reported that spectral saliency detection methods provide state-of-the-art performance in predicting human gaze in images (see, e.g., [1-3]). We systematically integrate and evaluate quaternion DCT- and FFT-based spectral saliency detection [3,4], weighted quaternion color space components [5], and the use of multiple resolutions [1]. Furthermore, we propose the use of the eigenaxes and eigenangles for spectral saliency models that are based on the quaternion Fourier transform. We demonstrate the outstanding performance on the Bruce-Tsotsos (Toronto), Judd (MIT), and Kootstra- Schomacker eye-tracking data sets. © 2012 Springer-Verlag.",,"Color space; Data sets; Eye fixations; Eye-tracking; Multiple resolutions; Saliency detection; State-of-the-art performance; Toronto; Color space; Eye fixations; Eye-tracking; Multiple resolutions; Quaternion Fourier transforms; Saliency detection; State-of-the-art performance; Toronto; Artificial intelligence; Artificial intelligence; Computers; Computer vision; Computer vision",Conference Paper,Scopus,2-s2.0-84867891493
"Williamson J.R., Bliss D.W., Browne D.W., Narayanan J.T.","Seizure prediction using EEG spatiotemporal correlation structure",2012,"Epilepsy and Behavior",50,10.1016/j.yebeh.2012.07.007,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867919804&doi=10.1016%2fj.yebeh.2012.07.007&partnerID=40&md5=332c831861548d1d0518f4c3de8d3642","A seizure prediction algorithm is proposed that combines novel multivariate EEG features with patient-specific machine learning. The algorithm computes the eigenspectra of space-delay correlation and covariance matrices from 15?s blocks of EEG data at multiple delay scales. The principal components of these features are used to classify the patient's preictal or interictal state. This is done using a support vector machine (SVM), whose outputs are averaged using a running 15-minute window to obtain a final prediction score. The algorithm was tested on 19 of 21 patients in the Freiburg EEG data set who had three or more seizures, predicting 71 of 83 seizures, with 15 false predictions and 13.8. h in seizure warning during 448.3. h of interictal data. The proposed algorithm scales with the number of available EEG signals by discovering the variations in correlation structure among any given set of signals that correlate with seizure risk. © 2012 Elsevier Inc.","Correlation structure; Eigenvalues; Electroencephalogram; Epilepsy; Machine learning; Multivariate features; Principal components; Seizure prediction; Support vector machines","adolescent; adult; algorithm; article; clinical article; correlation function; electroencephalogram; electroencephalograph; female; human; machine learning; male; prediction; scoring system; seizure; spatiotemporal correlation structure; support vector machine; Adolescent; Adult; Algorithms; Artificial Intelligence; Brain; Child; Electroencephalography; Female; Humans; Male; Middle Aged; Predictive Value of Tests; Seizures",Article,Scopus,2-s2.0-84867919804
"Bichler O., Querlioz D., Thorpe S.J., Bourgoin J.-P., Gamrat C.","Extraction of temporally correlated features from dynamic vision sensors with spike-timing-dependent plasticity",2012,"Neural Networks",50,10.1016/j.neunet.2012.02.022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861765357&doi=10.1016%2fj.neunet.2012.02.022&partnerID=40&md5=9e365599839eeddb8678d21d63c0402e","A biologically inspired approach to learning temporally correlated patterns from a spiking silicon retina is presented. Spikes are generated from the retina in response to relative changes in illumination at the pixel level and transmitted to a feed-forward spiking neural network. Neurons become sensitive to patterns of pixels with correlated activation times, in a fully unsupervised scheme. This is achieved using a special form of Spike-Timing-Dependent Plasticity which depresses synapses that did not recently contribute to the post-synaptic spike activation, regardless of their activation time. Competitive learning is implemented with lateral inhibition. When tested with real-life data, the system is able to extract complex and overlapping temporally correlated features such as car trajectories on a freeway, after only 10min of traffic learning. Complete trajectories can be learned with a 98% detection rate using a second layer, still with unsupervised learning, and the system may be used as a car counter. The proposed neural network is extremely robust to noise and it can tolerate a high degree of synaptic and neuronal variability with little impact on performance. Such results show that a simple biologically inspired unsupervised learning scheme is capable of generating selectivity to complex meaningful events on the basis of relatively little sensory experience. © 2012 Elsevier Ltd.","Dynamic vision sensor; Features extraction; Silicon retina; Spike-timing-dependent plasticity; Spiking neural network; Unsupervised learning","Dynamic vision; Features extraction; Silicon retina; Spike-timing-dependent plasticity; Spiking neural networks; Neural networks; Unsupervised learning; Pixels; article; artificial neural network; evolutionary algorithm; genetic algorithm; image processing; learning algorithm; nerve cell plasticity; postsynaptic inhibition; priority journal; signal noise ratio; signal processing; spike timing dependent plasticity; spike wave; synaptic potential; system analysis; Algorithms; Artificial Intelligence; Computer Simulation; Long-Term Potentiation; Models, Genetic; Motor Vehicles; Neural Networks (Computer); Neuronal Plasticity; Neurons; Reproducibility of Results; Retina; Signal-To-Noise Ratio; Silicon; Synapses; Synaptic Transmission",Article,Scopus,2-s2.0-84861765357
"Kisi O., Dailr A.H., Cimen M., Shiri J.","Suspended sediment modeling using genetic programming and soft computing techniques",2012,"Journal of Hydrology",50,10.1016/j.jhydrol.2012.05.031,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862679428&doi=10.1016%2fj.jhydrol.2012.05.031&partnerID=40&md5=be62e2b7dc580507e5c136a62fbb70a8","Modeling suspended sediment load is an important factor in water resources engineering as it crucially affects the design and management of water resources structures. In this study the genetic programming (GP) technique was applied for estimating the daily suspended sediment load in two stations in Cumberland River in U.S. Daily flow and sediment data from 1972 to 1989 were used to train and test the applied genetic programming models. The effect of various GP operators on sediment load estimation was investigated. The optimal fitness function, operator functions, linking function and learning algorithm were obtained for modeling daily suspended sediment. The GP estimates were compared with those of the Adaptive Neuro-Fuzzy Inference System (ANFIS), Artificial Neural Networks (ANNs) and Support Vector Machine (SVM) results, in term of coefficient of determination, mean absolute error, coefficient of residual mass and variance accounted for. The comparison results indicated that the GP is superior to the ANFIS, ANN and SVM models in estimating daily suspended sediment load. © 2012 Elsevier B.V.","Artificial intelligence; Estimating; Sediment load; Sensitivity analysis","Adaptive neuro-fuzzy inference system; Coefficient of determination; Comparison result; Estimating; Mean absolute error; Operator function; Optimal fitness; Residual mass; Sediment loads; Softcomputing techniques; Suspended sediment loads; SVM model; Artificial intelligence; Estimation; Genetic programming; Learning algorithms; Neural networks; Sedimentology; Sensitivity analysis; Soft computing; Support vector machines; Water management; Suspended sediments; algorithm; artificial intelligence; artificial neural network; computer simulation; engineering; estimation method; hydrological modeling; numerical model; sensitivity analysis; suspended sediment; water management; water resource; Cumberland River; United States",Article,Scopus,2-s2.0-84862679428
"Ma W., Sun B.","Probabilistic rough set over two universes and rough entropy",2012,"International Journal of Approximate Reasoning",50,10.1016/j.ijar.2011.12.010,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858798041&doi=10.1016%2fj.ijar.2011.12.010&partnerID=40&md5=2a2b9ca3035257733679601ac3d1b516","In this paper, we discuss the properties of the probabilistic rough set over two universes in detail. We present the parameter dependence or the continuous of the lower and upper approximations on parameters for probabilistic rough set over two universes. We also investigate some properties of the uncertainty measure, i.e., the rough degree and the precision, for probabilistic rough set over two universes. Meanwhile, we point out the limitation of the uncertainty measure for the traditional method and then define the general Shannon entropy of covering-based on universe. Then we discuss the uncertainty measure of the knowledge granularity and rough entropy for probabilistic rough set over two universes by the proposed concept. Finally, the validity of the methods and conclusions is tested by a numerical example. © 2011 Elsevier Inc. All rights reserved.","General Shannon entropy; Probabilistic approximation space over two universes; Rough set","Knowledge granularity; Lower and upper approximations; Numerical example; Parameter dependence; Probabilistic approximation space over two universes; Rough entropy; Rough set; Shannon entropy; Uncertainty measures; Artificial intelligence; Software engineering; Rough set theory",Article,Scopus,2-s2.0-84858798041
"Mavandadi S., Dimitrov S., Feng S., Yu F., Sikora U., Yaglidere O., Padmanabhan S., Nielsen K., Ozcan A.","Distributed medical image analysis and diagnosis through crowd-sourced games: A malaria case study",2012,"PLoS ONE",50,10.1371/journal.pone.0037245,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861006684&doi=10.1371%2fjournal.pone.0037245&partnerID=40&md5=447c2f9452e88829ac846679523d3605","In this work we investigate whether the innate visual recognition and learning capabilities of untrained humans can be used in conducting reliable microscopic analysis of biomedical samples toward diagnosis. For this purpose, we designed entertaining digital games that are interfaced with artificial learning and processing back-ends to demonstrate that in the case of binary medical diagnostics decisions (e.g., infected vs. uninfected), with the use of crowd-sourced games it is possible to approach the accuracy of medical experts in making such diagnoses. Specifically, using non-expert gamers we report diagnosis of malaria infected red blood cells with an accuracy that is within 1.25% of the diagnostics decisions made by a trained medical professional. © 2012 Mavandadi et al.",,"article; artificial intelligence; computer interface; cytodiagnosis; diagnostic accuracy; digital imaging; erythrocyte; game; gamers; human; image analysis; information processing; learning; malaria; medical decision making; medical expert; methodology; microscope image; nonmedical occupations; patient coding; visual memory; algorithm; automated pattern recognition; blood; blood cell; computer assisted diagnosis; parasitology; pattern recognition; problem solving; recreation; Algorithms; Artificial Intelligence; Blood Cells; Diagnosis, Computer-Assisted; Games, Experimental; Humans; Image Interpretation, Computer-Assisted; Malaria; Pattern Recognition, Automated; Pattern Recognition, Visual; Problem Solving; Video Games",Article,Scopus,2-s2.0-84861006684
"Gao L., Hailu A.","Ranking management strategies with complex outcomes: An AHP-fuzzy evaluation of recreational fishing using an integrated agent-based model of a coral reef ecosystem",2012,"Environmental Modelling and Software",50,10.1016/j.envsoft.2011.12.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856380754&doi=10.1016%2fj.envsoft.2011.12.002&partnerID=40&md5=5f24634ac2c6756c9d9db0da297b5ec3","The management of recreational fishing requires resolving conflicting interests and is thus among the most controversial natural resource related issues. Decision making is difficult because of two main factors: first, there is lack of prediction tools that help managers and other stakeholders assess the potential impacts of management changes; second, decisions or management strategies affect multiple social and ecological outcomes and picking the best among sets of multiple outcomes is a complex task. Resource management and stakeholder dialogue can be greatly improved by addressing these problems. In this paper, we propose a decision support system (DSS) for assessing management strategies. The DSS incorporates an integrated agent-based simulation model for tackling the first obstacle and an analytical hierarchy process (AHP)-fuzzy comprehensive evaluation approach to facilitate multi-criteria decision making.The agent-based simulation model incorporates recreational fishing behaviour within a reef ecosystem. Angler behaviour is driven by empirically estimated site choice models which link recreational choices to site attributes and angler characteristics. Coral reef ecosystem dynamics is modelled using a trophic-dynamic model describing the relationship among fish populations, fishing activities as well as algal and coral growth. The second component of the DSS, the AHP-fuzzy comprehensive evaluation part, allows one to combine resource managers' preferences with simulated economic and ecosystem outcomes in the assessment of alternative strategies. A fuzzy multi-criteria, multi-layer evaluation method is used to obtain final ranking.As a case study for this paper, we focus on the management of recreational fishing sites from the Ningaloo Marine Park, an iconic coral reef system in Western Australia. A set of management strategies, including a "" business-as-usual"" strategy and alternative site closure strategies are assessed using the proposed DSS. The site closure strategies evaluated vary in length and timing. Further, these evaluations are undertaken for two fishing pressure scenarios (high and low). We illustrate the usefulness of the DSS by evaluating these strategies. We also present some results from a sensitivity analysis focussing on changes in preferences. © 2011 Elsevier Ltd.","Agent-based model; AHP; Area closure; Decision support system; Fuzzy evaluation; Multiple criteria decision making; Non-market valuation; Random utility model; Recreational fishing","Agent-based model; AHP; Area closure; Decision supports; Fuzzy evaluation; Multiple criteria decision making; Non-market valuation; Random utility model; Artificial intelligence; Computational methods; Computer simulation; Decision support systems; Ecosystems; Fisheries; Fuzzy set theory; Hierarchical systems; Managers; Population statistics; Reefs; Sensitivity analysis; Decision making; angling; coral reef; decision making; decision support system; fuzzy mathematics; hierarchical system; management practice; marine park; numerical model; resource management; sensitivity analysis; stakeholder; valuation; Australia; Ningaloo Marine Park; Western Australia; algae; Anthozoa",Article,Scopus,2-s2.0-84856380754
"Schwartz W.R., Guo H., Choi J., Davis L.S.","Face identification using large feature sets",2012,"IEEE Transactions on Image Processing",50,10.1109/TIP.2011.2176951,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859067740&doi=10.1109%2fTIP.2011.2176951&partnerID=40&md5=2b4aa88c3ddd50782b685e15af39b632","With the goal of matching unknown faces against a gallery of known people, the face identification task has been studied for several decades. There are very accurate techniques to perform face identification in controlled environments, particularly when large numbers of samples are available for each face. However, face identification under uncontrolled environments or with a lack of training data is still an unsolved problem. We employ a large and rich set of feature descriptors (with more than 70000 descriptors) for face identification using partial least squares to perform multichannel feature weighting. Then, we extend the method to a tree-based discriminative structure to reduce the time required to evaluate probe samples. The method is evaluated on Facial Recognition Technology (FERET) and Face Recognition Grand Challenge (FRGC) data sets. Experiments show that our identification method outperforms current state-of-the-art results, particularly for identifying faces acquired across varying conditions. © 2011 IEEE.","Face identification; feature combination; feature selection; partial least squares (PLS)","Controlled environment; Data sets; Descriptors; Face identification; Face recognition grand challenges; Facial recognition; feature combination; Feature descriptors; Feature sets; Feature weighting; Identification method; Multi-channel; Partial least square (PLS); Training data; Tree-based; Unsolved problems; Feature extraction; Image processing; Mathematical models; Face recognition; algorithm; article; artificial intelligence; automated pattern recognition; biometry; computer assisted diagnosis; face; histology; human; image enhancement; image subtraction; information retrieval; methodology; reproducibility; sensitivity and specificity; Algorithms; Artificial Intelligence; Biometry; Face; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Information Storage and Retrieval; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-84859067740
"Hossain M., Muromachi Y.","A Bayesian network based framework for real-time crash prediction on the basic freeway segments of urban expressways",2012,"Accident Analysis and Prevention",50,10.1016/j.aap.2011.08.004,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856100242&doi=10.1016%2fj.aap.2011.08.004&partnerID=40&md5=ae5946ff2c7efef73d817d55156be0eb","The concept of measuring the crash risk for a very short time window in near future is gaining more practicality due to the recent advancements in the fields of information systems and traffic sensor technology. Although some real-time crash prediction models have already been proposed, they are still primitive in nature and require substantial improvements to be implemented in real-life. This manuscript investigates the major shortcomings of the existing models and offers solutions to overcome them with an improved framework and modeling method. It employs random multinomial logit model to identify the most important predictors as well as the most suitable detector locations to acquire data to build such a model. Afterwards, it applies Bayesian belief net (BBN) to build the real-time crash prediction model. The model has been constructed using high resolution detector data collected from Shibuya 3 and Shinjuku 4 expressways under the jurisdiction of Tokyo Metropolitan Expressway Company Limited, Japan. It has been specifically built for the basic freeway segments and it predicts the chance of formation of a hazardous traffic condition within the next 4-9 min for a particular 250 meter long road section. The performance evaluation results reflect that at an average threshold value the model is able to successful classify 66% of the future crashes with a false alarm rate less than 20%. © 2011 Elsevier Ltd. All rights reserved.","Basic freeway segments; Bayesian belief net; Random multinomial logit; Real-time crash prediction model","Basic freeway segments; Bayesian belief net; Crash prediction; Crash risk; False alarm rate; Hazardous traffic conditions; High resolution detector; Modeling method; Multinomial Logit; Multinomial logit model; Network-based framework; Performance evaluation; Road section; Time windows; Traffic sensors; Urban expressway; Bayesian networks; Forecasting; Highway accidents; Mathematical models; Real time systems; article; artificial intelligence; Bayes theorem; computer program; computer simulation; environmental planning; human; risk assessment; statistical model; statistics; traffic accident; urban population; Accidents, Traffic; Artificial Intelligence; Bayes Theorem; Computer Simulation; Environment Design; Humans; Logistic Models; Risk Assessment; Software; Urban Population",Article,Scopus,2-s2.0-84856100242
"Hu Q., Pan W., Zhang L., Zhang D., Song Y., Guo M., Yu D.","Feature selection for monotonic classification",2012,"IEEE Transactions on Fuzzy Systems",50,10.1109/TFUZZ.2011.2167235,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863116782&doi=10.1109%2fTFUZZ.2011.2167235&partnerID=40&md5=a80ee4e0525c5ca1ce2c0d4b3dad5684","Monotonic classification is a kind of special task in machine learning and pattern recognition. Monotonicity constraints between features and decision should be taken into account in these tasks. However, most existing techniques are not able to discover and represent the ordinal structures in monotonic datasets. Thus, they are inapplicable to monotonic classification. Feature selection has been proven effective in improving classification performance and avoiding overfitting. To the best of our knowledge, no technique has been specially designed to select features in monotonic classification until now. In this paper, we introduce a function, which is called rank mutual information, to evaluate monotonic consistency between features and decision in monotonic tasks. This function combines the advantages of dominance rough sets in reflecting ordinal structures and mutual information in terms of robustness. Then, rank mutual information is integrated with the search strategy of min-redundancy and max-relevance to compute optimal subsets of features. A collection of numerical experiments are given to show the effectiveness of the proposed technique. © 2012 IEEE.","Feature selection; fuzzy ordinal set; monotonic classification; rank mutual information (RMI)","Classification performance; Data sets; fuzzy ordinal set; Monotonicity constraint; Mutual informations; Numerical experiments; Optimal subsets; Ordinal structure; Overfitting; Rough set; Search strategies; Artificial intelligence; Fuzzy sets; Feature extraction",Article,Scopus,2-s2.0-84863116782
"Ballabio C., Sterlacchini S.","Support Vector Machines for Landslide Susceptibility Mapping: The Staffora River Basin Case Study, Italy",2012,"Mathematical Geosciences",50,10.1007/s11004-011-9379-9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855652855&doi=10.1007%2fs11004-011-9379-9&partnerID=40&md5=64e9cfe5d46fae5d8580bcf016a64b82","The aim of this study is the application of support vector machines (SVM) to landslide susceptibility mapping. SVM are a set of machine learning methods in which model capacity matches data complexity. The research is based on a conceptual framework targeted to apply and test all the procedural steps for landslide susceptibility modeling from model selection, to investigation of predictive variables, from empirical cross-validation of results, to analysis of predicted patterns. SVM were successfully applied and the final susceptibility map was interpreted via success and prediction rate curves and receiver operating characteristic (ROC) curves, to support the modeling results and assess the robustness of the model. SVM appeared to be very specific learners, able to discriminate between the informative input and random noise. About 78% of occurrences was identified within the 20% of the most susceptible study area for the cross-validation set. Then the final susceptibility map was compared with other maps, addressed by different statistical approaches, commonly used in susceptibility mapping, such as logistic regression, linear discriminant analysis, and naive Bayes classifier. The SVM procedure was found feasible and able to outperform other techniques in terms of accuracy and generalization capacity. The over-performance of SVM against the other techniques was around 18% for the cross-validation set, considering the 20% of the most susceptible area. Moreover, by analyzing receiver operating characteristic (ROC) curves, SVM appeared to be less prone to false positives than the other models. The study was applied in the Staffora river basin (Lombardy, Northern Italy), an area of about 275 km 2 characterized by a very high density of landslides, mainly superficial slope failures triggered by intense rainfall events. © 2011 International Association for Mathematical Geosciences.","Cross-validation; Landslide susceptibility mapping; Spatial prediction; Support Vector Machines","Conceptual frameworks; Cross validation; Data complexity; False positive; Generalization capacity; High density; Landslide susceptibility; Landslide susceptibility mapping; Linear discriminant analysis; Logistic regressions; Machine learning methods; Model Selection; Modeling results; Naive Bayes classifiers; Northern Italy; Prediction rate; Predictive variables; Procedural steps; Rainfall event; Random noise; Receiver operating characteristic curves; River basins; Slope failure; Spatial prediction; Statistical approach; Study areas; Susceptibility mapping; Classifiers; Discriminant analysis; Landslides; Learning algorithms; Mapping; Watersheds; Support vector machines; artificial intelligence; discriminant analysis; empirical analysis; geological mapping; landslide; prediction; regression analysis; river basin; slope failure; Italy; Lombardy; Pavia; Staffora Basin",Article,Scopus,2-s2.0-84855652855
"Pirovano M., Mainetti R., Baud-Bovy G., Lanzi P.L., Borghese N.A.","Self-adaptive games for rehabilitation at home",2012,"2012 IEEE Conference on Computational Intelligence and Games, CIG 2012",49,10.1109/CIG.2012.6374154,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871996436&doi=10.1109%2fCIG.2012.6374154&partnerID=40&md5=4cdc2012abb7a8a31aa385f33d952ce0","Computer games are a promising tool to support rehabilitation at home. It is widely recognized that rehabilitation games should (i) be nicely integrated in general-purpose rehabilitation stations, (ii) adhere to the constraints posed by the clinical protocols, (iii) involve movements that are functional to reach the rehabilitation goal, and (iv) adapt to the patients' current status and progress. However, the vast majority of existing rehabilitation games are stand-alone applications (not integrated in a patient station), that rarely adapt to the patients' condition. In this paper, we present the first prototype of the patient rehabilitation station we developed that integrates video games for rehabilitation with methods of computational intelligence both for on-line monitoring the movements' execution during the games and for adapting the gameplay to the patients' status. The station employs a fuzzy system to monitor the exercises execution, on-line, according to the clinical constraints defined by the therapist at configuration time, and to provide direct feedback to the patients. At the same time, it applies real-time adaptation (using the Quest Bayesian adaptive approach) to modify the gameplay according both (i) to the patient current performance and progress and (ii) to the exercise plan specified by the therapist. Finally, we present one of the games available in our patient stations (designed in tight cooperation with therapists) that integrates monitoring functionalities with in-game self-adaptation to provide the best support possible to patients during their routine. © 2012 IEEE.",,"Adaptive approach; Computer game; Current performance; Current status; Gameplay; Online monitoring; Real-time adaptation; Self adaptation; Self-adaptive; Standalone applications; Video game; Artificial intelligence; Human computer interaction; Interactive computer graphics; Patient rehabilitation",Conference Paper,Scopus,2-s2.0-84871996436
"Zambelli P., Lora C., Spinelli R., Tattoni C., Vitti A., Zatelli P., Ciolli M.","A GIS decision support system for regional forest management to assess biomass availability for renewable energy production",2012,"Environmental Modelling and Software",49,10.1016/j.envsoft.2012.05.016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864069697&doi=10.1016%2fj.envsoft.2012.05.016&partnerID=40&md5=12937838d726f1aa4c0bbdc09aed0439","Currently, the use of a mix of renewable and traditional energy sources is deemed to help in solving increasing energy demands and environmental issues, thus making it particularly important to assess the availability of renewable energy sources.In a heavily forested region, such as the Italian Alps, one of the main renewable energy sources is woody biomass.A reliable evaluation of biomass availability must take into account the local management of forest resources and the ability to reach forest areas, which is related to existing road networks, and the characteristics and morphology of the terrain.We have developed a new methodology to estimate forest biomass availability for energy production in the Alpine area and to support management decisions, combining the morphological features of the mountain landscape with the current capabilities of forest technology. The approach has been implemented in a tool for forest biomass evaluation based on the Free and Open Source Software for Geospatial (FOSS4G) framework and to refine the current estimates made by the local government.The methodology was tested on the forests of Trentino province (Italy), providing an accurate evaluation of biomass availability, which can be effectively used to identify possible locations for biomass power plants and to suggest new forest management guidelines.The methodology, combining GRASS, PostgreSQL and PostGIS, can be applied to a wide area and can also be executed as a new GRASS module. Being open source it is already available for testing and development. © 2012 Elsevier Ltd.","Bioenergy; Decision support system; Environmental sustainability; Forest residues; GIS; Harvesting techniques modelling; Renewable energy","Alpine areas; Bio-energy; Biomass availability; Biomass power plants; Current capability; Current estimates; Energy demands; Energy productions; Energy source; Environmental issues; Environmental sustainability; Forest area; Forest biomass; Forest residue; Forest resources; Forest technology; Free and open source softwares; Geo-spatial; Local management; Management decisions; Morphological features; Open sources; PostgreSQL; Renewable energies; Renewable energy source; Road network; Wide area; Artificial intelligence; Biomass; Decision support systems; Geographic information systems; Natural resources; Renewable energy resources; Forestry; bioenergy; decision support system; environmental issue; forest management; GIS; harvesting; local government; phytomass; renewable resource; sustainable forestry; Biomass; Energy; Forestry; GIS; Natural Resources; Renewable Resources; Alps; Italy",Article,Scopus,2-s2.0-84864069697
"Deng W., Chen R., He B., Liu Y., Yin L., Guo J.","A novel two-stage hybrid swarm intelligence optimization algorithm and application",2012,"Soft Computing",49,10.1007/s00500-012-0855-z,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866119861&doi=10.1007%2fs00500-012-0855-z&partnerID=40&md5=fe77d29fc06a0fb799b2d971da3cd0d1","This paper presents a novel two-stage hybrid swarm intelligence optimization algorithm called GA-PSO-ACO algorithm that combines the evolution ideas of the genetic algorithms, particle swarm optimization and ant colony optimization based on the compensation for solving the traveling salesman problem. In the proposed hybrid algorithm, the whole process is divided into two stages. In the first stage, we make use of the randomicity, rapidity and wholeness of the genetic algorithms and particle swarm optimization to obtain a series of sub-optimal solutions (rough searching) to adjust the initial allocation of pheromone in the ACO. In the second stage, we make use of these advantages of the parallel, positive feedback and high accuracy of solution to implement solving of whole problem (detailed searching). To verify the effectiveness and efficiency of the proposed hybrid algorithm, various scale benchmark problems from TSPLIB are tested to demonstrate the potential of the proposed two-stage hybrid swarm intelligence optimization algorithm. The simulation examples demonstrate that the GA-PSO-ACO algorithm can greatly improve the computing efficiency for solving the TSP and outperforms the Tabu Search, genetic algorithms, particle swarm optimization, ant colony optimization, PS-ACO and other methods in solution quality. And the experimental results demonstrate that convergence is faster and better when the scale of TSP increases. © 2012 Springer-Verlag.","Ant colony optimization; Genetic algorithms; Particle swarm optimization; Swarm intelligence; Traveling salesman problem; Two-stage hybrid algorithm","Ant Colony Optimization (ACO); Bench-mark problems; Computing efficiency; Hybrid algorithms; Randomicity; Simulation example; Solution quality; Suboptimal solution; Swarm Intelligence; Swarm intelligence optimization algorithm; Two-stage hybrid algorithms; Whole process; Artificial intelligence; Genetic algorithms; Tabu search; Traveling salesman problem; Particle swarm optimization (PSO)",Article,Scopus,2-s2.0-84866119861
"Wee H.","Public key encryption against related key attacks",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",49,10.1007/978-3-642-30057-8_16,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861714606&doi=10.1007%2f978-3-642-30057-8_16&partnerID=40&md5=5ba80521dfa5fc7ddcb4e39491c22e5f","In this work, we present efficient public-key encryption schemes resilient against linear related key attacks (RKA) under standard assumptions and in the standard model. Specifically, we obtain encryption schemes based on hardness of factoring, BDDH and LWE that remain secure even against an adversary that may query the decryption oracle on linear shifts of the actual secret key. Moreover, the ciphertext overhead is only an additive constant number of group elements. © 2012 International Association for Cryptologic Research.",,"Ciphertexts; Encryption schemes; Linear-shift; Public-key encryption; Public-key encryption scheme; Related key attacks; Secret key; Standard assumptions; The standard model; Ciphertexts; Encryption schemes; Linear-shift; Public-key encryption; Public-key encryption scheme; Related key attacks; Standard assumptions; The standard model; Artificial intelligence; Cryptography; Public key cryptography; Public key cryptography",Conference Paper,Scopus,2-s2.0-84861714606
"Huang D.-S., Jiang W.","A general CPL-AdS methodology for fixing dynamic parameters in dual environments",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",49,10.1109/TSMCB.2012.2192475,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866490627&doi=10.1109%2fTSMCB.2012.2192475&partnerID=40&md5=0eee132bd2fbb4e12e0fd751a7f83559","The algorithm of Continuous Point Location with Adaptive d-ary Search (CPL-AdS) strategy exhibits its efficiency in solving stochastic point location (SPL) problems. However, there is one bottleneck for this CPL-AdS strategy which is that, when the dimension of the feature, or the number of divided subintervals for each iteration, d is large, the decision table for elimination process is almost unavailable. On the other hand, the larger dimension of the features d can generally make this CPL-AdS strategy avoid oscillation and converge faster. This paper presents a generalized universal decision formula to solve this bottleneck problem. As a matter of fact, this decision formula has a wider usage beyond handling out this SPL problems, such as dealing with deterministic point location problems and searching data in Single Instruction Stream-Multiple Data Stream based on Concurrent Read and Exclusive Write parallel computer model. Meanwhile, we generalized the CPL-AdS strategy with an extending formula, which is capable of tracking an unknown dynamic parameter λ * in both informative and deceptive environments. Furthermore, we employed different learning automata in the generalized CPL-AdS method to find out if faster learning algorithm will lead to better realization of the generalized CPL-AdS method. All of these aforementioned contributions are vitally important whether in theory or in practical applications. Finally, extensive experiments show that our proposed approaches are efficient and feasible. © 1996-2012 IEEE.","Continuous Point Location with Adaptive d-ary Search (CPL-AdS); CPL-AdS with DP RI (CPL-AdS-RI); CPL-AdS with DP RP (CPL-AdS-RP); decision formula; extending formula","CPL-AdS with DP <sub>RI</sub> (CPL-AdS-RI); CPL-AdS with DP <sub>RP</sub> (CPL-AdS-RP); decision formula; extending formula; Point location; Decision tables; Learning algorithms; Problem solving; algorithm; article; artificial intelligence; automated pattern recognition; mathematical computing; methodology; Algorithms; Artificial Intelligence; Numerical Analysis, Computer-Assisted; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84866490627
"Mendas A., Delali A.","Integration of MultiCriteria Decision Analysis in GIS to develop land suitability for agriculture: Application to durum wheat cultivation in the region of Mleta in Algeria",2012,"Computers and Electronics in Agriculture",49,10.1016/j.compag.2012.02.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857952521&doi=10.1016%2fj.compag.2012.02.003&partnerID=40&md5=dc3247e468624abb847bfff946858112","Due to constant decrease in farmlands, it is important to identify the best lands useful for sustainable agriculture (productive and profitable agriculture that protects the environment and that is socially equitable). This requirement has resulted in the development of land suitability maps for agriculture by combining several factors of various natures and of differing importance. The integration of MultiCriteria Decision Analysis approaches (MCDA) in a Geographical Information System (GIS) provides a powerful spatial decision support system which offers the opportunity to efficiently produce these land suitability maps. Indeed, GIS is a powerful tool for analyzing spatial data and establishing a process for decision support. Because of their spatial aggregation functions, MCDA methods can facilitate decision making in situations where several solutions are available, various criteria have to be taken into account and decision-makers are in conflict (Dias et al., 2002). The parameters and the classification system used in this work are inspired from the FAO (Food and Agriculture Organization) approach dedicated to a sustainable agriculture. A spatial decision support system has been developed for establishing the land suitability map for agriculture. It incorporates the multicriteria analysis method ELECTRE Tri (ELimitation Et Choix Traduisant la REalité) in a GIS (ArcGIS) within the GIS program package environment. This approach has been tested on the area of Mleta in Algeria. A land suitability map for durum wheat has been produced. Through the obtained results, it appears that ELECTRE Tri method, integrated into ArcGIS 9.2 of ESRI, is better suited to the problem of land suitability for agriculture. The time saving during the development of the land suitability map for the agriculture of the durum wheat was considerable. The coherence of the obtained maps confirms the system effectiveness. © 2012 Elsevier B.V.","Algeria; Decision support system; Geographical information system; Land suitability for agriculture; MultiCriteria decision analysis; Sustainable agriculture","Algeria; Decision supports; Geographical Information System; Land suitability; Multi-criteria decision analysis; Sustainable agriculture; Artificial intelligence; Decision making; Decision support systems; Decision theory; Geographic information systems; Information systems; Integration; Profitability; Software packages; Cultivation; aggregation; agricultural land; alternative agriculture; cultivation; decision support system; GIS; multicriteria analysis; wheat; Algeria; Triticum turgidum subsp. durum",Article,Scopus,2-s2.0-84857952521
"Heverin T., Zach L.","Use of microblogging for collective sense-making during violent crises: A study of three campus shootings",2012,"Journal of the American Society for Information Science and Technology",49,10.1002/asi.21685,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-83655163747&doi=10.1002%2fasi.21685&partnerID=40&md5=3593150b351fcb0fc5767995edd2d039","The purpose of this study is to understand how microblogging communications change and contribute to collective sense-making over time during a crisis. Using B. Dervin's (1983) theory of sense-making applied to crises and communications during crises, we examined 7,184 microblogging communications sent in response to three violent crises that occurred on U.S. college campuses. The analysis of patterns of microblogging communications found that information-sharing behaviors dominated the early response phase of violent crises, and opinion sharing increased over time, peaking in the recovery phase of the crises. The analysis of individual microblogging communications identified various themes in the conversation threads that not only helped individual contributors make sense of the situation but also helped others who followed the conversation. The results of this study show that microblogging can play a vital role in collective sense-making during crises. © 2011 ASIS&T.",,"College campus; Microblogging; Recovery phase; Sensemaking; Artificial intelligence; Software engineering",Article,Scopus,2-s2.0-83655163747
"Hadjidimitriou S.K., Hadjileontiadis L.J.","Toward an EEG-based recognition of music liking using time-frequency analysis",2012,"IEEE Transactions on Biomedical Engineering",48,10.1109/TBME.2012.2217495,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870498019&doi=10.1109%2fTBME.2012.2217495&partnerID=40&md5=6434052c8b794a70af51ad1af5198c20","Affective phenomena, as reflected through brain activity, could constitute an effective index for the detection of music preference. In this vein, this paper focuses on the discrimination between subjects electroencephalogram (EEG) responses to self-assessed liked or disliked music, acquired during an experimental procedure, by evaluating different feature extraction approaches and classifiers to this end. Feature extraction is based on time-frequency (TF) analysis by implementing three TF techniques, i.e., spectrogram, Zhao-Atlas-Marks distribution and Hilbert-Huang spectrum (HHS). Feature estimation also accounts for physiological parameters that relate to EEG frequency bands, reference states, time intervals, and hemispheric asymmetries. Classification is performed by employing four classifiers, i.e., support vector machines, k-nearest neighbors (k-NN), quadratic and Mahalanobis distance-based discriminant analyses. According to the experimental results across nine subjects, best classification accuracy {86.52 (±0.76)} was achieved using k-NN and HHS-based feature vectors (FVs) representing a bilateral average activity, referred to a resting period, in β (13-30Hz) and γ (30-49Hz) bands. Activity in these bands may point to a connection between music preference and emotional arousal phenomena. Furthermore, HHS-based FVs were found to be robust against noise corruption. The outcomes of this study provide early evidence and pave the way for the development of a generalized brain computer interface for music preference recognition. © 2012 IEEE.","Electroencephalogram (EEG); machine learning; music liking/disliking; time-frequency (TF) analysis","Brain activity; Classification accuracy; Distance-based; Effective index; Experimental procedure; Feature estimation; Feature vectors; K-nearest neighbors; Mahalanobis; music liking/disliking; Noise corruption; Physiological parameters; Reference state; Resting period; Spectrograms; Time frequency analysis; Time interval; Discriminant analysis; Feature extraction; Frequency bands; Learning systems; Physiological models; Electroencephalography; accuracy; adult; article; brain computer interface; electroencephalogram; emotion; experimental study; female; frequency analysis; hemisphere; human; human experiment; male; music; nerve cell membrane potential; neurophysiology; normal human; Artificial Intelligence; Brain; Brain Mapping; Electroencephalography; Female; Humans; Male; Music; Pleasure; Signal Processing, Computer-Assisted; Young Adult",Article,Scopus,2-s2.0-84870498019
"Ho C.-J., Vaughan J.W.","Online task assignment in crowdsourcing markets",2012,"Proceedings of the National Conference on Artificial Intelligence",48,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868273774&partnerID=40&md5=c57d864a5bd9b054eeff4ecc58510199","We explore the problem of assigning heterogeneous tasks to workers with different, unknown skill sets in crowdsourcing markets such as Amazon Mechanical Turk. We first formalize the online task assignment problem, in which a requester has a fixed set of tasks and a budget that specifies how many times he would like each task completed. Workers arrive one at a time (with the same worker potentially arriving multiple times), and must be assigned to a task upon arrival. The goal is to allocate workers to tasks in a way that maximizes the total benefit that the requester obtains from the completed work. Inspired by recent research on the online adwords problem, we present a two-phase exploration-exploitation assignment algorithm and prove that it is competitive with respect to the optimal offline algorithm which has access to the unknown skill levels of each worker. We empirically evaluate this algorithm using data collected on Mechanical Turk and show that it performs better than random assignment or greedy algorithms. To our knowledge, this is the first work to extend the online primal-dual technique used in the online adwords problem to a scenario with unknown parameters, and the first to offer an empirical validation of an online primal-dual algorithm. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Assignment algorithms; Crowdsourcing; Empirical validation; Greedy algorithms; Mechanical turks; Off-line algorithm; Primal dual algorithms; Primal-dual; Random assignment; Skill levels; Skill sets; Task assignment; Total benefits; Algorithms; Commerce; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868273774
"Richard E., Savalle P.-A., Vayatis N.","Estimation of simultaneously sparse and low rank matrices",2012,"Proceedings of the 29th International Conference on Machine Learning, ICML 2012",48,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867132315&partnerID=40&md5=79a9664b492b0f4af32076108177d782","The paper introduces a penalized matrix estimation procedure aiming at solutions which are sparse and low-rank at the same time. Such structures arise in the context of social networks or protein interactions where underlying graphs have adjacency matrices which are block-diagonal in the appropriate basis. We introduce a convex mixed penalty which involves ℓ 1-norm and trace norm simultaneously. We obtain an oracle inequality which indicates how the two effects interact according to the nature of the target matrix. We bound generalization error in the link prediction problem. We also develop proximal descent strategies to solve the optimization problem efficiently and evaluate performance on synthetic and real data sets. Copyright 2012 by the author(s)/owner(s).",,"Adjacency matrices; Generalization Error; Link prediction; Low-rank matrices; Matrix estimation; Optimization problems; Protein interaction; Social Networks; Synthetic and real data; Target matrices; Trace-norms; Underlying graphs; Artificial intelligence; Software engineering; Learning systems",Conference Paper,Scopus,2-s2.0-84867132315
"Chen J.Y.C., Barnes M.J.","Supervisory control of multiple robots: Effects of imperfect automation and individual differences",2012,"Human Factors",48,10.1177/0018720811435843,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859330597&doi=10.1177%2f0018720811435843&partnerID=40&md5=3454be04eae46ba5b1a2b7e4ccf08403","Objective: A military multitasking environment was simulated to examine the effects of an intelligent agent, RoboLeader, on the performance of robotics operators.Background: The participants' task was to manage a team of ground robots with the assistance of RoboLeader, an intelligent agent capable of coordinating the robots and changing their routes on the basis of battlefield developments.Method: In the first experiment, RoboLeader was perfectly reliable; in the second experiment, RoboLeader's recommendations were manipulated to be either false-alarm prone or miss prone, with a reliability level of either 60% or 90%. The visual density of the targeting environment was manipulated by the presence or absence of friendly soldiers.Results: RoboLeader, when perfectly reliable, was helpful in reducing the overall mission times. The type of RoboLeader imperfection (false-alarm vs. miss prone) affected operators' performance of tasks involving visual scanning (target detection, route editing, and situation awareness). There was a consistent effect of visual density (clutter of the visual scene) for multiple performance measures. Participants' attentional control and video gaming experience affected their overall multitasking performance. In both experiments, participants with greater spatial ability consistently outperformed their low-spatial-ability counterparts in tasks that required effective visual scanning.Conclusion: Intelligent agents, such as RoboLeader, can benefit the overall human-robot teaming performance. However, the effects of type of agent unreliability, tasking requirements, and individual differences have complex effects on human-agent interaction. Application: The current results will facilitate the implementation of robots in military settings and will provide useful data to designs of systems for multirobot control. © 2012 Human Factors and Ergonomics Society.","attentional control; gaming experience; human-robot interaction; imperfect automation; individual differences; intelligent agent; military; spatial ability; supervisory control","Gaming experiences; Individual Differences; military; Spatial abilities; Supervisory control; Artificial intelligence; Experiments; Industrial robots; Machine design; Multitasking; Robotics; Intelligent agents; adult; article; clinical trial; controlled clinical trial; controlled study; female; human; individuality; male; man machine interaction; randomized controlled trial; robotics; soldier; task performance; workload; Adult; Female; Humans; Individuality; Male; Man-Machine Systems; Military Personnel; Robotics; Task Performance and Analysis; Workload; Young Adult",Article,Scopus,2-s2.0-84859330597
"Sussillo D., Nuyujukian P., Fan J.M., Kao J.C., Stavisky S.D., Ryu S., Shenoy K.","A recurrent neural network for closed-loop intracortical brain-machine interface decoders",2012,"Journal of Neural Engineering",48,10.1088/1741-2560/9/2/026027,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859127815&doi=10.1088%2f1741-2560%2f9%2f2%2f026027&partnerID=40&md5=a97dcce9401cf4e0a3ffd544a882f901","Recurrent neural networks (RNNs) are useful tools for learning nonlinear relationships in time series data with complex temporal dependences. In this paper, we explore the ability of a simplified type of RNN, one with limited modifications to the internal weights called an echostate network (ESN), to effectively and continuously decode monkey reaches during a standard center-out reach task using a cortical brain-machine interface (BMI) in a closed loop. We demonstrate that the RNN, an ESN implementation termed a FORCE decoder (from first order reduced and controlled error learning), learns the task quickly and significantly outperforms the current state-of-the-art method, the velocity Kalman filter (VKF), using the measure of target acquire time. We also demonstrate that the FORCE decoder generalizes to a more difficult task by successfully operating the BMI in a randomized point-to-point task. The FORCE decoder is also robust as measured by the success rate over extended sessions. Finally, we show that decoded cursor dynamics are more like naturalistic hand movements than those of the VKF. Taken together, these results suggest that RNNs in general, and the FORCE decoder in particular, are powerful tools for BMI decoder applications. © 2012 IOP Publishing Ltd.",,"Brain machine interface; Closed loops; Closed-loop; Error learning; First order; Hand movement; Internal weights; Non-linear relationships; State-of-the-art methods; Temporal dependence; Time-series data; Recurrent neural networks; Brain; animal experiment; article; artificial neural network; brain computer interface; controlled study; dynamics; echostate network; filter; FORCE decoder; hand movement; machine learning; male; nonhuman; priority journal; recurrent neural network; task performance; velocity; algorithm; animal; arm; artificial intelligence; biomechanics; brain cortex; computer interface; computer program; computer system; hand; Macaca mulatta; neuroprosthesis; normal distribution; physiology; prosthesis; psychomotor performance; statistical model; Algorithms; Animals; Arm; Artificial Intelligence; Biomechanics; Cerebral Cortex; Computer Systems; Hand; Linear Models; Macaca mulatta; Male; Neural Networks (Computer); Neural Prostheses; Normal Distribution; Prosthesis Design; Psychomotor Performance; Software; User-Computer Interface",Article,Scopus,2-s2.0-84859127815
"Alvarez-Alvarez A., Trivino G., Cordón O.","Human gait modeling using a genetic fuzzy finite state machine",2012,"IEEE Transactions on Fuzzy Systems",48,10.1109/TFUZZ.2011.2171973,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859705290&doi=10.1109%2fTFUZZ.2011.2171973&partnerID=40&md5=8854cac1881098343010850d709a4762","Human gait modeling consists of studying the biomechanics of this human movement. Its importance lies in the fact that its analysis can help in the diagnosis of walking and movement disorders or rehabilitation programs, among other medical situations. Fuzzy finite state machines can be used to model the temporal evolution of this type of phenomenon. Nevertheless, the definition of details of the model in each particular case is a complex task for experts. In this paper, we present an automatic method to learn the model parameters that are based on the hybridization of fuzzy finite state machines and genetic algorithms leading to genetic fuzzy finite state machines. This new genetic fuzzy system automatically learns the fuzzy rules and membership functions of the fuzzy finite state machine, while an expert defines the possible states and allowed transitions. Our final goal is to obtain a specific model for each persons gait in such a way that it can generalize well with different gaits of the same person. The obtained model must become an accurate and human friendly linguistic description of this phenomenon, with the capability to identify the relevant phases of the process. A complete experimentation is developed to test the performance of the new proposal when dealing with datasets of 20 different people, comprising a detailed analysis of results, which shows the advantages of our proposal in comparison with some other classical and computational intelligence techniques. © 2012 IEEE.","Fuzzy finite state machines; fuzzy systems; genetic algorithms (GAs); genetic fuzzy systems; human gait modeling","Automatic method; Complex task; Computational intelligence techniques; Data sets; Genetic fuzzy systems; Genetic-fuzzy; Human gait modeling; Human movements; Human-friendly; Linguistic descriptions; Model parameters; Movement disorders; Rehabilitation programs; Temporal evolution; Artificial intelligence; Biomechanics; Diagnosis; Finite automata; Fuzzy systems; Program diagnostics; Genetic algorithms",Article,Scopus,2-s2.0-84859705290
"Vescoukis V., Doulamis N., Karagiorgou S.","A service oriented architecture for decision support systems in environmental crisis management",2012,"Future Generation Computer Systems",48,10.1016/j.future.2011.03.010,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80655145724&doi=10.1016%2fj.future.2011.03.010&partnerID=40&md5=202b3ebbd17e266714c6b1739850d3bc","Efficient management of natural disasters impose great research challenges to the current environmental crisis management systems in terms of both architecture and services. This is mainly due to the fact that a large amount of geospatial content is usually distributed, non-compliant to standards, and needs to be transmitted under a QoS guaranteed framework to support effective decision making either in case of an emergency or in advance planning. Incorporating real time capabilities in Web services, both in terms of dynamic configuration and service selection, is an open research agenda. The things get worst in geospatial context due to the huge amount of data transmitted from distributed sensors under heterogeneous platforms, making the need of synchronization an important issue. In this paper, we propose a flexible service oriented architecture for planning and decision support in environmental crisis management. The suggested architecture uses real time geospatial data sets and 3D presentation tools, integrated with added-value services, such as simulation models for assisting decision making in case of emergency. The proposed architectural framework goes beyond integration and presentation of static spatial data, to include real time middleware that is responsible for selecting the most appropriate method of the available geospatial content and service in order to satisfy the QoS requirements of users and/or application. A case study of a complete, real world implementation of the suggested framework dealing with forest fire crisis management system is also presented. © 2011 Elsevier B.V. All rights reserved.","Computer applications; Crisis management; Decision support; Environmental modeling; Service oriented architectures","Added-value services; Architectural frameworks; Crisis management; Crisis management systems; Decision supports; Distributed sensor; Dynamic configuration; Environmental crisis; Environmental modeling; Forest fires; Geo-spatial; Geospatial data set; Heterogeneous platforms; Natural disasters; Presentation tools; QoS requirements; Real time; Real time capability; Real-world implementation; Research agenda; Research challenges; Service Oriented; Service selection; Simulation model; Spatial data; Artificial intelligence; Computer applications; Computer architecture; Decision making; Decision support systems; Deforestation; Disasters; Environmental management; Information services; Middleware; Planning; Quality of service; Research; Three dimensional; User interfaces; Web services; Service oriented architecture (SOA); Artificial Intelligence; Computers; Decision Making; Disasters; Information Retrieval; Planning; Standards",Article,Scopus,2-s2.0-80655145724
"Autey J., Sayed T., Zaki M.H.","Safety evaluation of right-turn smart channels using automated traffic conflict analysis",2012,"Accident Analysis and Prevention",48,10.1016/j.aap.2011.11.015,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855181535&doi=10.1016%2fj.aap.2011.11.015&partnerID=40&md5=a1966c86917d4625a3b6b332709ea750","This paper presents the results of a before-after (BA) safety evaluation of a newly proposed design for channelized right-turn lanes. The new design, termed ""Smart Channels"", decreases the angle of the channelized right turn to approximately 70°. The implementation of these modified right-turn channels is usually advocated to allow for safer pedestrian crossing. However, the benefits also extend to vehicle-vehicle interactions since the new approach angle affords drivers a better view of the traffic stream they are to merge with. The evaluation is conducted using a video-based automated traffic conflict analysis. There are several advantages that support the adoption of traffic conflict techniques in BA safety studies. Traffic conflicts are more frequent than road collisions and are of marginal social cost, they provide insight into the failure mechanism that leads to road collisions, and BA studies based on traffic conflicts can be conducted over shorter periods. As well, the use of automated conflict analysis overcomes the reliability and repeatability problems usually associated with manual conflict observations. Data for three treatment intersections and one control intersection in Penticton, British Columbia, are used in this study. The results of the evaluation show that the implementation of the right-turn treatment has resulted in a considerable reduction in the severity and frequency of merging, rear-end, and total conflicts. The total average hourly conflict was reduced by about 51% while the average conflict severity was reduced by 41%. © 2011 Elsevier Ltd. All rights reserved.","Before-and-after safety studies; Channelized right-turn; Computer vision; Safety evaluation; Smart Channels; Surrogate measures; Traffic conflicts","Channelized right-turn; Safety evaluation; Safety studies; Smart channels; Surrogate measures; Traffic conflicts; Automation; Computer vision; Reliability analysis; Roads and streets; Traffic signals; article; artificial intelligence; Canada; conflict; environmental planning; human; injury; risk reduction; safety; traffic accident; videorecording; walking; Accidents, Traffic; Artificial Intelligence; British Columbia; Conflict (Psychology); Environment Design; Humans; Risk Reduction Behavior; Safety; Video Recording; Walking",Article,Scopus,2-s2.0-84855181535
"Ciornei I., Kyriakides E.","Hybrid ant colony-genetic algorithm (GAAPI) for global continuous optimization",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",48,10.1109/TSMCB.2011.2164245,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856300028&doi=10.1109%2fTSMCB.2011.2164245&partnerID=40&md5=46fd5ec1519a2c6c0395b76c259456f6","Many real-life optimization problems often face an increased rank of nonsmoothness (many local minima) which could prevent a search algorithm from moving toward the global solution. Evolution-based algorithms try to deal with this issue. The algorithm proposed in this paper is called GAAPI and is a hybridization between two optimization techniques: a special class of ant colony optimization for continuous domains entitled API and a genetic algorithm (GA). The algorithm adopts the downhill behavior of API (a key characteristic of optimization algorithms) and the good spreading in the solution space of the GA. A probabilistic approach and an empirical comparison study are presented to prove the convergence of the proposed method in solving different classes of complex global continuous optimization problems. Numerical results are reported and compared to the existing results in the literature to validate the feasibility and the effectiveness of the proposed method. The proposed algorithm is shown to be effective and efficient for most of the test functions. © 2011 IEEE.","Ant colony optimization (ACO); genetic algorithm (GA); global continuous optimization","Ant-colony optimization; Continuous domain; Continuous optimization; Empirical comparison; global continuous optimization; Global continuous optimization problem; Global solutions; Key characteristics; Local minimums; Non-smoothness; Numerical results; Optimization algorithms; Optimization problems; Optimization techniques; Probabilistic approaches; Search Algorithms; Solution space; Special class; Test functions; Artificial intelligence; Constrained optimization; Numerical methods; Genetic algorithms; algorithm; animal; animal behavior; ant; article; artificial intelligence; automated pattern recognition; biomimetics; decision support system; methodology; physiology; Algorithms; Animals; Ants; Artificial Intelligence; Behavior, Animal; Biomimetics; Decision Support Techniques; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84856300028
"Mandal P., Madhira S.T.S., Ul haque A., Meng J., Pineda R.L.","Forecasting power output of solar photovoltaic system using wavelet transform and artificial intelligence techniques",2012,"Procedia Computer Science",48,10.1016/j.procs.2012.09.080,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880412487&doi=10.1016%2fj.procs.2012.09.080&partnerID=40&md5=276245c5c54b7fb721902a9a2a1a6c3c","With increased penetration of solar as a variable energy resource (VER), solar photovoltaic (PV) power production is rapidly increasing into large-scale power industries. Since power output of PV systems depends critically on the weather, unexpected variations of their power output may increase the operating costs of the power system. Moreover, a major barrier in integrating this VER into the grid is its unpredictability, since steady output cannot be guaranteed at any particular time. This biases power utilities against using PV power since the planning and overall balancing of the grid becomes very challenging. Developing a reliable algorithm that can minimize the errors associated with forecasting the near future PV power generation is extremely beneficial for efficiently integrating VER into the grid. PV power forecasting can play a key role in tackling these challenges. This paper presents one-hour-ahead power output forecasting of a PV system using a combination of wavelet transform (WT) and artificial intelligence (AI) techniques by incorporating the interactions of PV system with solar radiation and temperature data. In the proposed method, the WT is applied to have a significant impact on ill-behaved PV power time-series data, and AI techniques capture the nonlinear PV fluctuation in a better way. © 2012 Published by Elsevier B.V.","Artificial intelligence; Renewable energy; Solar photovoltaic power forecasting; Wavelet transform",,Conference Paper,Scopus,2-s2.0-84880412487
"Polikar R.","Ensemble learning",2012,"Ensemble Machine Learning: Methods and Applications",48,10.1007/9781441993267_1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884161251&doi=10.1007%2f9781441993267_1&partnerID=40&md5=a40cf9d81ab3f80426cc2bc0cca7cec6","Over the last couple of decades, multiple classifier systems, also called ensemble systems have enjoyed growing attention within the computational intelligence and machine learning community. This attention has been well deserved, as ensemble systems have proven themselves to be very effective and extremely versatile in a broad spectrum of problem domains and real-world applications. Originally developed to reduce the variance—thereby improving the accuracy—of an automated decision-making system, ensemble systems have since been successfully used to address a variety of machine learning problems, such as feature selection, confidence estimation, missing feature, incremental learning, error correction, classimbalanced data, learning concept drift from nonstationary distributions, among others. This chapter provides an overview of ensemble systems, their properties, and how they can be applied to such a wide spectrum of applications. © Springer Science+Business Media, LLC 2012. All rights reserved.",,"Artificial intelligence; Error correction; Learning systems; Automated decision making systems; Confidence estimation; Ensemble learning; Incremental learning; Machine learning communities; Machine learning problem; Missing features; Multiple classifier systems; Education",Book Chapter,Scopus,2-s2.0-84884161251
"Hofleitner A., Herring R., Bayen A.","Arterial travel time forecast with streaming data: A hybrid approach of flow modeling and machine learning",2012,"Transportation Research Part B: Methodological",48,10.1016/j.trb.2012.03.006,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865748237&doi=10.1016%2fj.trb.2012.03.006&partnerID=40&md5=654c28ef6b4ca8cf5138af3d1d853899","This article presents a hybrid modeling framework for estimating and predicting arterial traffic conditions using streaming GPS probe data. The model is based on a well-established theory of traffic flow through signalized intersections and is combined with a machine learning framework to both learn static parameters of the roadways (such as free flow velocity or traffic signal parameters) as well as to estimate and predict travel times through the arterial network. The machine learning component of the approach uses the significant amount of historical data collected by the Mobile Millennium system since March 2009 with over 500 probe vehicles reporting their position once per minute in San Francisco, CA.The hybrid model provides a distinct advantage over pure statistical or pure traffic theory models in that it is robust to noisy data (due to the large volumes of historical data) and it produces forecasts using traffic flow theory principles consistent with the physics of traffic. Validation of the model is performed in two different ways. First, a large scale test of the model is performed by splitting the data source into two sets, using the first to produce the estimates and the second to validate them. Second, an alternate validation approach is presented. It consists of a 3-day experiment in which GPS data was collected once per second from 20 drivers on four routes through San Francisco, allowing for precise calculation of actual travel times. The model is run by down-sampling the data and validated using the travel times from these 20 drivers. The results indicate that this approach is a significant step forward in estimating traffic states throughout the arterial network using a relatively small amount of real-time data. The estimates from our model are compared to those given by a data-driven baseline algorithm, for which we achieve a 16% improvement in terms of the root mean squared error of travel time estimates. The primary reason for success is the reliance on a flow model of traffic, which ensures that estimates are consistent with the physics of traffic. © 2012 Elsevier Ltd.","Arterial traffic; Estimation; Forecast; GPS probe data; Machine learning; Streaming data","Artificial intelligence; Estimation; Flow velocity; Forecasting; Global positioning system; Learning algorithms; Learning systems; Mean square error; Probes; Street traffic control; Traffic control; Traffic signals; Arterial traffics; Gps probe datum; Root mean squared errors; Signal parameters; Signalized intersection; Streaming data; Traffic flow theory; Validation approach; Travel time; algorithm; estimation method; forecasting method; GPS; modeling; traffic management; travel time; California; San Francisco [California]; United States",Article,Scopus,2-s2.0-84865748237
"Han H.-G., Qiao J.-F.","Adaptive computation algorithm for RBF neural network",2012,"IEEE Transactions on Neural Networks and Learning Systems",47,10.1109/TNNLS.2011.2178559,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876921616&doi=10.1109%2fTNNLS.2011.2178559&partnerID=40&md5=205fa85db84b91096c85958c504fd2b2","A novel learning algorithm is proposed for nonlinear modelling and identification using radial basis function neural networks. The proposed method simplifies neural network training through the use of an adaptive computation algorithm (ACA). In addition, the convergence of the ACA is analyzed by the Lyapunov criterion. The proposed algorithm offers two important advantages. First, the model performance can be significantly improved through ACA, and the modelling error is uniformly ultimately bounded. Secondly, the proposed ACA can reduce computational cost and accelerate the training speed. The proposed method is then employed to model classical nonlinear system with limit cycle and to identify nonlinear dynamic system, exhibiting the effectiveness of the proposed algorithm. Computational complexity analysis and simulation results demonstrate its effectiveness. © 2012 IEEE.","Adaptive computation algorithm; modelling; nonlinear systems; radial basis function neural networks","Adaptive computations; Computational complexity analysis; Computational costs; Lyapunov criterion; Neural network training; Non-linear modelling; Radial basis function neural networks; Uniformly ultimately bounded; Learning algorithms; Models; Neural networks; Nonlinear dynamical systems; Nonlinear systems; Computational mechanics; algorithm; artificial intelligence; artificial neural network; automated pattern recognition; computer simulation; feedback system; nonlinear system; procedures; statistical model; Algorithms; Artificial Intelligence; Computer Simulation; Feedback; Models, Statistical; Neural Networks (Computer); Nonlinear Dynamics; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84876921616
"Dezert J., Wang P., Tchamova A.","On the validity of Dempster-Shafer theory",2012,"15th International Conference on Information Fusion, FUSION 2012",47,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867635120&partnerID=40&md5=50a9aafa92dee8d0b7e164965470a6ea","We challenge the validity of Dempster-Shafer Theory by using an emblematic example to show that DS rule produces counter-intuitive result. Further analysis reveals that the result comes from a understanding of evidence pooling which goes against the common expectation of this process. Although DS theory has attracted some interest of the scientific community working in information fusion and artificial intelligence, its validity to solve practical problems is problematic, because it is not applicable to evidences combination in general, but only to a certain type situations which still need to be clearly identified. © 2012 ISIF (Intl Society of Information Fusi).","belief functions; Dempster-Shafer Theory; DST; Mathematical Theory of Evidence","Belief function; Dempster-Shafer theory; DS theory; DST; Mathematical theory; Practical problems; Scientific community; Artificial intelligence; Uncertainty analysis; Information fusion",Conference Paper,Scopus,2-s2.0-84867635120
"Saleem M., Ullah I., Farooq M.","BeeSensor: An energy-efficient and scalable routing protocol for wireless sensor networks",2012,"Information Sciences",47,10.1016/j.ins.2012.02.024,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860310986&doi=10.1016%2fj.ins.2012.02.024&partnerID=40&md5=e58140c36afb871db1f59f258ea66b67","Design and development of power-aware, scalable and performance-efficient routing protocols for wireless sensor networks (WSNs) is an active area of research. In this paper, we show that insect-colonies-based-intelligence - commonly referred to as Swarm Intelligence (SI) - serves as an ideal model for developing routing protocols for WSNs because they consist of minimalist, autonomous individuals that through local interactions self-organize to produce system-level behaviors that show life-long adaptivity to changes and perturbations in an external environment. In this paper, we propose bee-inspired BeeSensor protocol that is energy-aware, scalable and efficient. The important contribution of this work is a three phase protocol design strategy: (1) we first take inspiration from biological systems to develop a distributed, decentralized and simple routing protocol, (2) we formally model important performance metrics of our protocol to get an analytic insight into its behavior, and (3) we improve our protocol on the basis of our analysis in phase 2. We then evaluate its performance in a sensor network simulator. The results of our experiments demonstrate the utility of this three phase protocol engineering, which helped BeeSensor in achieving the best performance with the least communication and processing costs - two main sources of energy consumption in sensor networks - as compared to other SI based WSN routing protocols. © 2012 Elsevier Inc. All rights reserved.","Energy-efficient; Modeling and simulation; Routing protocol; Swarm Intelligence; Wireless sensor networks","Active area; Adaptivity; Design and Development; Energy aware; Energy efficient; External environments; Ideal model; In-phase; Local interactions; Modeling and simulation; Performance metrics; Power-aware; Processing costs; Protocol design; Protocol engineering; Scalable routing; Self-organize; Sensor network simulator; Si-based; Swarm Intelligence; System levels; Three phase; Wireless sensor network (WSNs); Artificial intelligence; Biological systems; Computer simulation; Cost engineering; Energy efficiency; Energy utilization; Routing protocols; Wireless sensor networks",Article,Scopus,2-s2.0-84860310986
"Savitha R., Suresh S., Sundararajan N.","A meta-cognitive learning algorithm for a Fully Complex-valued Relaxation Network",2012,"Neural Networks",47,10.1016/j.neunet.2012.02.015,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861781199&doi=10.1016%2fj.neunet.2012.02.015&partnerID=40&md5=5996fbb560ea6aeee52d7664fb43cf5d","This paper presents a meta-cognitive learning algorithm for a single hidden layer complex-valued neural network called ""Meta-cognitive Fully Complex-valued Relaxation Network (McFCRN)"". McFCRN has two components: a cognitive component and a meta-cognitive component. A Fully Complex-valued Relaxation Network (FCRN) with a fully complex-valued Gaussian like activation function (. sech) in the hidden layer and an exponential activation function in the output layer forms the cognitive component. The meta-cognitive component contains a self-regulatory learning mechanism which controls the learning ability of FCRN by deciding . what-to-learn, . when-to-learn and . how-to-learn from a sequence of training data. The input parameters of cognitive components are chosen randomly and the output parameters are estimated by minimizing a logarithmic error function. The problem of explicit minimization of magnitude and phase errors in the logarithmic error function is converted to system of linear equations and output parameters of FCRN are computed analytically. McFCRN starts with zero hidden neuron and builds the number of neurons required to approximate the target function. The meta-cognitive component selects the best learning strategy for FCRN to acquire the knowledge from training data and also adapts the learning strategies to implement best human learning components. Performance studies on a function approximation and real-valued classification problems show that proposed McFCRN performs better than the existing results reported in the literature. © 2012 Elsevier Ltd.","Complex-valued neural networks; Fully complex-valued relaxation network; Meta-cognition; Multi-category classification; Self-regulated learning mechanism","Activation functions; Cognitive components; Complex-valued neural networks; Error function; Function approximation; Gaussians; Hidden layers; Hidden neurons; Human learning; Input parameter; Learning abilities; Learning mechanism; Learning strategy; Meta-cognition; Metacognitives; Multi-category classification; Output layer; Output parameters; Performance study; Phase error; Self-regulated learning; System of linear equations; Target functions; Training data; Two-component; Learning algorithms; Cognitive systems; accuracy; analytical error; article; artificial neural network; classification algorithm; cognition; controlled study; intermethod comparison; kernel method; learning algorithm; learning style; linear system; mathematical computing; mathematical model; meta cognitive fully complex valued relaxation network; priority journal; quality control; radial based function; Algorithms; Artificial Intelligence; Benchmarking; Classification; Cognition; Female; Humans; Image Processing, Computer-Assisted; Linear Models; Mammography; Neural Networks (Computer); Normal Distribution; Software; Support Vector Machines",Article,Scopus,2-s2.0-84861781199
"Wachinger C., Yigitsoy M., Rijkhorst E.-J., Navab N.","Manifold learning for image-based breathing gating in ultrasound and MRI",2012,"Medical Image Analysis",47,10.1016/j.media.2011.11.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859434164&doi=10.1016%2fj.media.2011.11.008&partnerID=40&md5=5113efca920135c7c2d2f9a4beafbdbe","Respiratory motion is a challenging factor for image acquisition and image-guided procedures in the abdominal and thoracic region. In order to address the issues arising from respiratory motion, it is often necessary to detect the respiratory signal. In this article, we propose a novel, purely image-based retrospective respiratory gating method for ultrasound and MRI. Further, we apply this technique to acquire breathing-affected 4D ultrasound with a wobbler probe and, similarly, to create 4D MR with a slice stacking approach. We achieve the gating with Laplacian eigenmaps, a manifold learning technique, to determine the low-dimensional manifold embedded in the high-dimensional image space. Since Laplacian eigenmaps assign to each image frame a coordinate in low-dimensional space by respecting the neighborhood relationship, they are well suited for analyzing the breathing cycle. We perform the image-based gating on several 2D and 3D ultrasound datasets over time, and quantify its very good performance by comparing it to measurements from an external gating system. For MRI, we perform the manifold learning on several datasets for various orientations and positions. We achieve very high correlations by a comparison to an alternative gating with diaphragm tracking. © 2011 Elsevier B.V.","4D; Image-based breathing gating; Manifold learning; MRI; Ultrasound","3-D ultrasound; 4D; Breathing cycle; Data sets; Diaphragm tracking; Gating system; High-dimensional images; Image frames; Image-based; Image-guided procedures; Laplacian eigenmaps; Low-dimensional manifolds; Low-dimensional spaces; Manifold learning; Respiratory gatings; Respiratory motions; Respiratory signals; Gating and feeding; Magnetic resonance imaging; Respiratory mechanics; Ultrasonic applications; Ultrasonics; Three dimensional; article; breathing; image analysis; learning; nuclear magnetic resonance imaging; priority journal; quantitative analysis; three dimensional imaging; ultrasound; Algorithms; Artifacts; Artificial Intelligence; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Pattern Recognition, Automated; Reproducibility of Results; Respiratory Mechanics; Respiratory-Gated Imaging Techniques; Sensitivity and Specificity; Ultrasonography",Article,Scopus,2-s2.0-84859434164
"Xu R., Chen H., Li X.","Makespan minimization on single batch-processing machine via ant colony optimization",2012,"Computers and Operations Research",47,10.1016/j.cor.2011.05.011,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960194636&doi=10.1016%2fj.cor.2011.05.011&partnerID=40&md5=f53683569e223e4e63177aa92b89e935","This paper investigates the problem of minimizing makespan on a single batch-processing machine, and the machine can process multiple jobs simultaneously. Each job is characterized by release time, processing time, and job size. We established a mixed integer programming model and proposed a valid lower bound for this problem. By introducing a definition of waste and idle space (WIS), this problem is proven to be equivalent to minimizing the WIS for the schedule. Since the problem is NP-hard, we proposed a heuristic and an ant colony optimization (ACO) algorithm based on the theorems presented. A candidate list strategy and a new method to construct heuristic information were introduced for the ACO approach to achieve a satisfactory solution in a reasonable computational time. Through extensive computational experiments, appropriate ACO parameter values were chosen and the effectiveness of the proposed algorithms was evaluated by solution quality and run time. The results showed that the ACO algorithm combined with the candidate list was more robust and consistently outperformed genetic algorithm (GA), CPLEX, and the other two heuristics, especially for large job instances. © 2011 Elsevier Ltd. All rights reserved.","Ant colony optimization; Batch-processing machine; Makespan (Cmax); Scheduling","ACO algorithms; Ant Colony Optimization algorithms; Ant-colony optimization; Batch processing machine; Candidate list; Computational experiment; Computational time; Heuristic information; Job size; Lower bounds; Makespan; Makespan minimization; Minimizing makespan; Mixed integer programming model; NP-hard; Parameter values; Processing Time; Release time; Runtimes; Solution quality; C (programming language); Computer programming; Genetic algorithms; Heuristic methods; Integer programming; Optimization; Scheduling algorithms; Artificial intelligence",Article,Scopus,2-s2.0-79960194636
"Benosman R., Ieng S.-H., Clercq C., Bartolozzi C., Srinivasan M.","Asynchronous frameless event-based optical flow",2012,"Neural Networks",47,10.1016/j.neunet.2011.11.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856438852&doi=10.1016%2fj.neunet.2011.11.001&partnerID=40&md5=d6593315c3dd32e4e8f3edf8a497252c","This paper introduces a process to compute optical flow using an asynchronous event-based retina at high speed and low computational load. A new generation of artificial vision sensors has now started to rely on biologically inspired designs for light acquisition. Biological retinas, and their artificial counterparts, are totally asynchronous and data driven and rely on a paradigm of light acquisition radically different from most of the currently used frame-grabber technologies. This paper introduces a framework for processing visual data using asynchronous event-based acquisition, providing a method for the evaluation of optical flow. The paper shows that current limitations of optical flow computation can be overcome by using event-based visual acquisition, where high data sparseness and high temporal resolution permit the computation of optical flow with micro-second accuracy and at very low computational cost. © 2011 Elsevier Ltd.","Asynchronous acquisition; Event-based vision; Frameless vision; Optical flow; Spikes; Temporal dynamics","Asynchronous acquisition; Biological retina; Biologically inspired design; Computational costs; Current limitation; Data driven; Data sparseness; Event-based; High temporal resolution; Low computational loads; Optical flow computation; Spikes; Temporal dynamics; Visual data; Data handling; Optical flows; accuracy; article; asynchronous frameless event based optical flow; biosensor; controlled study; cost; image analysis; light; mathematical analysis; optic flow; optical resolution; priority journal; retina; visual information; Artificial Intelligence; Computer Simulation; Humans; Models, Neurological; Optic Flow; Retina; Vision, Ocular",Article,Scopus,2-s2.0-84856438852
"Yang G., Xi X., Yin Y.","Finger vein recognition based on a personalized best bit map",2012,"Sensors",47,10.3390/s120201738,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857602660&doi=10.3390%2fs120201738&partnerID=40&md5=03ad473c942ce916a53d7151462f1285","Finger vein patterns have recently been recognized as an effective biometric identifier. In this paper, we propose a finger vein recognition method based on a personalized best bit map (PBBM). Our method is rooted in a local binary pattern based method and then inclined to use the best bits only for matching. We first present the concept of PBBM and the generating algorithm. Then we propose the finger vein recognition framework, which consists of preprocessing, feature extraction, and matching. Finally, we design extensive experiments to evaluate the effectiveness of our proposal. Experimental results show that PBBM achieves not only better performance, but also high robustness and reliability. In addition, PBBM can be used as a general framework for binary pattern based recognition. © 2012 by the authors; licensee MDPI, Basel, Switzerland.","Finger vein recognition; General framework; Hamming distance; Local binary pattern; Personalized best bit map","algorithm; article; artificial intelligence; automated pattern recognition; biometry; dermatoglyphics; finger; histology; human; methodology; signal processing; vascularization; vein; Algorithms; Artificial Intelligence; Biometry; Dermatoglyphics; Fingers; Humans; Pattern Recognition, Automated; Signal Processing, Computer-Assisted; Veins",Article,Scopus,2-s2.0-84857602660
"Gharibi H., Mahvi A.H., Nabizadeh R., Arabalibeik H., Yunesian M., Sowlat M.H.","A novel approach in water quality assessment based on fuzzy logic",2012,"Journal of Environmental Management",46,10.1016/j.jenvman.2012.07.007,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864826303&doi=10.1016%2fj.jenvman.2012.07.007&partnerID=40&md5=7cc912287f450d1e5816bc344fd8a153","The present work aimed at developing a novel water quality index based on fuzzy logic, that is, a comprehensive artificial intelligence (AI) approach to the development of environmental indices for routine assessment of surface water quality, particularly for human drinking purposes. Twenty parameters were included based on their critical importance for the overall water quality and their potential impact on human health. To assess the performance of the proposed index under actual conditions, a case study was conducted at Mamloo dam, Iran, employing water quality data of four sampling stations in the water basin of the dam from 2006 to 2009. Results of this study indicated that the general quality of water in all the sampling stations over all the years of the study period is fairly low (yearly averages are usually in the range of 45-55). According to the results of ANOVA test, water quality did not significantly change over time in any of the sampling stations (P > 0.05). In addition, comparison of the outputs of the fuzzy-based proposed index proposed with those of the NSF water quality index (the WQI) and Canadian Water Quality Index (CWQI) showed similar results and were sensitive to changes in the level of water quality parameters. However, the index proposed by the present study produced a more stringent outputs compared to the WQI and CWQI. Results of the sensitivity analysis suggested that the index is robust against the changes in the rules. In conclusion, the proposed index seems to produce accurate and reliable results and can therefore be used as a comprehensive tool for water quality assessment, especially for the analysis of human drinking water. © 2012 Elsevier Ltd.","Fuzzy logic; Mamloo dam; Water quality assessment; Water quality index","surface water; artificial intelligence; assessment method; drinking water; environmental factor; fuzzy mathematics; health impact; public health; sampling; surface water; water quality; analysis of variance; article; artificial intelligence; fuzzy logic; Iran; nonhuman; physical chemistry; quality control; sensitivity analysis; water quality; water sampling; Environmental Monitoring; Fuzzy Logic; Models, Theoretical; Rivers; Water Quality; Iran; Mamloo Dam; Tehran [Iran]",Article,Scopus,2-s2.0-84864826303
"Otero F.E.B., Freitas A.A., Johnson C.G.","Inducing decision trees with an ant colony optimization algorithm",2012,"Applied Soft Computing Journal",46,10.1016/j.asoc.2012.05.028,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865861470&doi=10.1016%2fj.asoc.2012.05.028&partnerID=40&md5=75f7ef23f21928201dde9db359991bf9","Decision trees have been widely used in data mining and machine learning as a comprehensible knowledge representation. While ant colony optimization (ACO) algorithms have been successfully applied to extract classification rules, decision tree induction with ACO algorithms remains an almost unexplored research area. In this paper we propose a novel ACO algorithm to induce decision trees, combining commonly used strategies from both traditional decision tree induction algorithms and ACO. The proposed algorithm is compared against three decision tree induction algorithms, namely C4.5, CART and cACDT, in 22 publicly available data sets. The results show that the predictive accuracy of the proposed algorithm is statistically significantly higher than the accuracy of both C4.5 and CART, which are well-known conventional algorithms for decision tree induction, and the accuracy of the ACO-based cACDT decision tree algorithm. © 2012 Elsevier B.V. All rights reserved.","Ant colony optimization; Classification; Data mining; Decision tree","ACO algorithms; Ant Colony Optimization (ACO); Ant Colony Optimization algorithms; Classification rules; Conventional algorithms; Data sets; Decision tree induction; Decision-tree algorithm; Predictive accuracy; Artificial intelligence; Classification (of information); Data mining; Decision trees; Knowledge representation; Algorithms",Article,Scopus,2-s2.0-84865861470
"Gueguim Kana E.B., Oloke J.K., Lateef A., Adesiyan M.O.","Modeling and optimization of biogas production on saw dust and other co-substrates using Artificial Neural network and Genetic Algorithm",2012,"Renewable Energy",46,10.1016/j.renene.2012.03.027,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860886604&doi=10.1016%2fj.renene.2012.03.027&partnerID=40&md5=faaf1e468b178a2a2330b13d93f86050","The joint challenge of global pollution and depletion of fossil fuels is driving intense search into alternative renewable sources. This paper reports the modeling and optimization of biogas production on mixed substrates of saw dust, cow dung, banana stem, rice bran and paper waste using Artificial Neural Network (ANN) coupling Genetic Algorithm (GA).Data from twenty five mini-pilot biogas fermentations were used to train and validate a structured ANN with a topology of 5-2-1. The model served as fitness function for GA optimization process. An optimized substrate profile emerged with a predicted biogas performance of 10.144L. Evaluation of the optimal profile gave a biogas production of 10.280L, thus an increase of 8.64%, and an early biogas production initiated on the 3rd day of fermentation against the 8th day in non-optimized system. ANN coupling GA efficiently modeled the non-linear behavior of the process. A recipe for an optimum biogas production using the above co-substrates has been elucidated. © 2012 Elsevier Ltd.","Artificial intelligence; Artificial Neural Network; Biogas production; Bioprocess optimization; Genetic Algorithm; Mixed substrates","Banana stem; Biogas fermentation; Biogas production; Bioprocess optimization; Cosubstrates; Cow dung; Fitness functions; GA optimization; Global pollution; Mixed substrates; Modeling and optimization; Nonlinear behavior; Paper wastes; Renewable sources; Rice brans; Substrate profile; Artificial intelligence; Biogas; Dust; Fermentation; Fossil fuels; Genetic algorithms; Neural networks; Optimization; Substrates; alternative energy; artificial neural network; biogas; gas production; genetic algorithm; numerical model; optimization; performance assessment; renewable resource; substrate; wood",Article,Scopus,2-s2.0-84860886604
"Fodor J., De Baets B.","A single-point characterization of representable uninorms",2012,"Fuzzy Sets and Systems",46,10.1016/j.fss.2011.12.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862228326&doi=10.1016%2fj.fss.2011.12.001&partnerID=40&md5=97f44674d8b8feccbaf0a420378ed5d8","We study uninorms of which both the underlying t-norm and underlying t-conorm are strict. Such uninorms are the only candidates for being representable by an additive generator. We prove that the representability of such a uninorm depends solely on its value at a single arbitrary point in the 'remaining' open part of the unit square. More explicitly, such a uninorm turns out to be representable if and only if this single value is located strictly between the minimum and the maximum of the corresponding arguments. If this single value coincides with one of these bounds, then the value of the uninorm at any point in the 'remaining' open part is determined by the same bound. © 2011 Published by Elsevier B.V.","Additive generator; Representability; Strict t-conorm; Strict t-norm; Uninorm","Additive generators; Representability; Strict t-norm; T-conorms; Uninorms; Artificial intelligence; Fuzzy sets; Mathematical operators",Article,Scopus,2-s2.0-84862228326
"Heinonen M., Shen H., Zamboni N., Rousu J.","Metabolite identification and molecular fingerprint prediction through machine learning",2012,"Bioinformatics",46,10.1093/bioinformatics/bts437,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866437252&doi=10.1093%2fbioinformatics%2fbts437&partnerID=40&md5=20bc8b3e2e225e3656308321ffc66c8d","Motivation: Metabolite identification from tandem mass spectra is an important problem in metabolomics, underpinning subsequent metabolic modelling and network analysis. Yet, currently this task requires matching the observed spectrum against a database of reference spectra originating from similar equipment and closely matching operating parameters, a condition that is rarely satisfied in public repositories. Furthermore, the computational support for identification of molecules not present in reference databases is lacking. Recent efforts in assembling large public mass spectral databases such as MassBank have opened the door for the development of a new genre of metabolite identification methods. Results: We introduce a novel framework for prediction of molecular characteristics and identification of metabolites from tandem mass spectra using machine learning with the support vector machine. Our approach is to first predict a large set of molecular properties of the unknown metabolite from salient tandem mass spectral signals, and in the second step to use the predicted properties for matching against large molecule databases, such as PubChem. We demonstrate that several molecular properties can be predicted to high accuracy and that they are useful in de novo metabolite identification, where the reference database does not contain any spectra of the same molecule. © The Author 2012. Published by Oxford University Press. All rights reserved.",,"article; artificial intelligence; chemical database; metabolomics; methodology; tandem mass spectrometry; Artificial Intelligence; Databases, Chemical; Metabolomics; Tandem Mass Spectrometry",Article,Scopus,2-s2.0-84866437252
"Samwald M., Fehre K., de Bruin J., Adlassnig K.-P.","The Arden Syntax standard for clinical decision support: Experiences and directions",2012,"Journal of Biomedical Informatics",46,10.1016/j.jbi.2012.02.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865104851&doi=10.1016%2fj.jbi.2012.02.001&partnerID=40&md5=a315c8a3d21ef005776c46b8581f9305","Arden Syntax is a widely recognized standard for representing clinical and scientific knowledge in an executable format. It has a history that reaches back until 1989 and is currently maintained by the Health Level 7 (HL7) organization. We created a production-ready development environment, compiler, rule engine and application server for Arden Syntax. Over the course of several years, we have applied this Arden - Syntax - based CDS system in a wide variety of clinical problem domains, such as hepatitis serology interpretation, monitoring of nosocomial infections or the prediction of metastatic events in melanoma patients. We found the Arden Syntax standard to be very suitable for the practical implementation of CDS systems. Among the advantages of Arden Syntax are its status as an actively developed HL7 standard, the readability of the syntax, and various syntactic features such as flexible list handling. A major challenge we encountered was the technical integration of our CDS systems in existing, heterogeneous health information systems. To address this issue, we are currently working on incorporating the HL7 standard GELLO, which provides a standardized interface and query language for accessing data in health information systems. We hope that these planned extensions of the Arden Syntax might eventually help in realizing the vision of a global, interoperable and shared library of clinical decision support knowledge. © 2012 Elsevier Inc.","Arden Syntax; Clinical decision support; HL7; Standards","Accessing data; Application Servers; Arden Syntax; CdS; Clinical decision support; Clinical problems; Development environment; Health information systems; Health level 7; HL7; Nosocomial infection; Practical implementation; Rule engine; Scientific knowledge; Shared libraries; Standardized interfaces; Syntactic features; Technical integration; Decision support systems; Query languages; Standards; Syntactics; Arden Syntax standard; article; decision support system; hepatitis; hospital infection; human; melanoma; metastasis; monitoring; prediction; priority journal; serology; standard; Algorithms; Artificial Intelligence; Clinical Coding; Decision Support Systems, Clinical; Humans; Linguistics; Male",Article,Scopus,2-s2.0-84865104851
"Mugan J., Kuipers B.","Autonomous learning of high-level states and actions in continuous environments",2012,"IEEE Transactions on Autonomous Mental Development",46,10.1109/TAMD.2011.2160943,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858634841&doi=10.1109%2fTAMD.2011.2160943&partnerID=40&md5=b6fa85e77551e1060e8436fccab4d10b","How can an agent bootstrap up from a low-level representation to autonomously learn high-level states and actions using only domain-general knowledge? In this paper, we assume that the learning agent has a set of continuous variables describing the environment. There exist methods for learning models of the environment, and there also exist methods for planning. However, for autonomous learning, these methods have been used almost exclusively in discrete environments. We propose attacking the problem of learning high-level states and actions in continuous environments by using a qualitative representation to bridge the gap between continuous and discrete variable representations. In this approach, the agent begins with a broad discretization and initially can only tell if the value of each variable is increasing, decreasing, or remaining steady. The agent then simultaneously learns a qualitative representation (discretization) and a set of predictive models of the environment. These models are converted into plans to perform actions. The agent then uses those learned actions to explore the environment. The method is evaluated using a simulated robot with realistic physics. The robot is sitting at a table that contains a block and other distractor objects that are out of reach. The agent autonomously explores the environment without being given a task. After learning, the agent is given various tasks to determine if it learned the necessary states and actions to complete them. The results show that the agent was able to use this method to autonomously learn to perform the tasks. © 2009 IEEE.","Active learning; intrinsic motivation; qualitative reasoning; reinforcement learning; unsupervised learning","Active Learning; Autonomous learning; Continuous variables; Discrete variable representation; Discretizations; Intrinsic motivation; Learning agents; Learning models; Predictive models; Qualitative reasoning; Qualitative representation; Realistic physics; Simulated robot; Artificial intelligence; Reinforcement learning; Unsupervised learning; Autonomous agents",Article,Scopus,2-s2.0-84858634841
"Benbouzid D., Busa-Fekete R., Casagrande N., Collin F.-D., Kégl B.","MULTIBOOST: A multi-purpose boosting package",2012,"Journal of Machine Learning Research",46,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859449983&partnerID=40&md5=30bd3ff6db4270b9b276a20318d509cb","The MULTIBOOST package provides a fast C++ implementation of multi-class/multi-label/multitask boosting algorithms. It is based on ADABOOST.MH but it also implements popular cascade classifiers and FILTERBOOST. The package contains common multi-class base learners (stumps, trees, products, Haar filters). Further base learners and strong learners following the boosting paradigm can be easily implemented in a flexible framework. © 2012 Djalel Benbouzid, Róbert Busa-Fekete, Norman Casagrande, François-David Collin and Balázs Kégl.","ADABOOST.MH; Boosting; Cascade classifier; FILTERBOOST","ADABOOST.MH; Base learners; Boosting; Boosting algorithm; Cascade classifier; Cascade classifiers; FILTERBOOST; Flexible framework; Haar filter; Multi-class; Multi-purpose; Artificial intelligence; Software engineering",Article,Scopus,2-s2.0-84859449983
"Shan J., Cheng H.D., Wang Y.","Completely automated segmentation approach for breast ultrasound images using multiple-domain features",2012,"Ultrasound in Medicine and Biology",46,10.1016/j.ultrasmedbio.2011.10.022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862926199&doi=10.1016%2fj.ultrasmedbio.2011.10.022&partnerID=40&md5=8c690e0d0b5f4efd329e7669754f0328","Lesion segmentation is a challenging task for computer aided diagnosis systems. In this article, we propose a novel and fully automated segmentation approach for breast ultrasound (BUS) images. The major contributions of this work are: an efficient region-of-interest (ROI) generation method is developed and new features to characterize lesion boundaries are proposed. After a ROI is located automatically, two newly proposed lesion features (phase in max-energy orientation and radial distance), combined with a traditional intensity-and-texture feature, are utilized to detect the lesion by a trained artificial neural network. The proposed features are tested on a database of 120 images and the experimental results prove their strong distinguishing ability. Compared with other breast ultrasound segmentation methods, the proposed method improves the TP rate from 84.9% to 92.8%, similarity rate from 79.0% to 83.1% and reduces the FP rate from 14.1% to 12.0%, using the same database. In addition, sensitivity analysis demonstrates the robustness of the proposed method. © 2012 World Federation for Ultrasound in Medicine & Biology.","Breast ultrasound segmentation; CAD (computer-aided diagnosis); Distance feature; Phase feature; Region-of-interest (ROI)","Artificial Neural Network; Automated segmentation; Breast ultrasound; Computer aided diagnosis systems; Distance feature; Generation method; Phase features; Radial distance; Region of interest; Segmentation methods; Image segmentation; Medical imaging; Neural networks; Sensitivity analysis; Ultrasonic applications; Ultrasonics; Computer aided diagnosis; accuracy; article; artificial neural network; automated segmentation approach; breast cancer; breast tumor; computer assisted diagnosis; controlled study; echomammography; human; image analysis; image processing; major clinical study; priority journal; radiological procedures; scoring system; sensitivity analysis; ultrasound scanner; Algorithms; Artificial Intelligence; Breast Neoplasms; Female; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Ultrasonography, Mammary",Article,Scopus,2-s2.0-84862926199
"Ḧullermeier E., Rifqi M., Henzgen S., Senge R.","Comparing fuzzy partitions: A generalization of the rand index and related measures",2012,"IEEE Transactions on Fuzzy Systems",45,10.1109/TFUZZ.2011.2179303,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867616600&doi=10.1109%2fTFUZZ.2011.2179303&partnerID=40&md5=0751fa1135243bde155e1df0476b257f","In this paper, we introduce a fuzzy extension of a class of measures to compare clustering structures, namely, measures that are based on the number of concordant and the number of discordant pairs of data points. This class includes the well-known Rand index but also commonly used alternatives, such as the Jaccard measure. In contrast with previous proposals, our extension exhibits desirable metrical properties. Apart from elaborating on formal properties of this kind, we present an experimental study in which we compare different fuzzy extensions of the Rand index and the Jaccard measure. © 2013 IEEE.","Clustering; Distance; Fuzzy partition; Jaccard index; Rand index; Similarity","Clustering; Distance; Fuzzy partition; Jaccard index; Rand index; Similarity; Artificial intelligence; Fuzzy sets",Article,Scopus,2-s2.0-84867616600
"Hu R., Jia W., Ling H., Huang D.","Multiscale distance matrix for fast plant leaf recognition",2012,"IEEE Transactions on Image Processing",45,10.1109/TIP.2012.2207391,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867853365&doi=10.1109%2fTIP.2012.2207391&partnerID=40&md5=a5a5fa55fc41e7315e4db0ee35b9df98","In this brief, we propose a novel contour-based shape descriptor, called the multiscale distance matrix, to capture the shape geometry while being invariant to translation, rotation, scaling, and bilateral symmetry. The descriptor is further combined with a dimensionality reduction to improve its discriminative power. The proposed method avoids the time-consuming pointwise matching encountered in most of the previously used shape recognition algorithms. It is therefore fast and suitable for real-time applications. We applied the proposed method to the task of plan leaf recognition with experiments on two data sets, the Swedish Leaf data set and the ICL Leaf data set. The experimental results clearly demonstrate the effectiveness and efficiency of the proposed descriptor. © 2012 IEEE.","Cost matrix; inner distance; multiscale distance matrix (MDM); plant leaf; shape recognition","Cost matrices; Distance matrices; inner distance; Plant leaf; Shape recognition; Image processing; Mathematical models; Geometry; algorithm; artificial intelligence; automated pattern recognition; classification; factual database; histology; image processing; letter; methodology; plant leaf; reproducibility; Algorithms; Artificial Intelligence; Databases, Factual; Image Processing, Computer-Assisted; Pattern Recognition, Automated; Plant Leaves; Reproducibility of Results",Article,Scopus,2-s2.0-84867853365
"Landeras G., López J.J., Kisi O., Shiri J.","Comparison of Gene Expression Programming with neuro-fuzzy and neural network computing techniques in estimating daily incoming solar radiation in the Basque Country (Northern Spain)",2012,"Energy Conversion and Management",45,10.1016/j.enconman.2012.03.025,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862537228&doi=10.1016%2fj.enconman.2012.03.025&partnerID=40&md5=697efc0bd34a844ff29bb11e2580aec9","Surface incoming solar radiation is a key variable for many agricultural, meteorological and solar energy conversion related applications. In absence of the required meteorological sensors for the detection of global solar radiation it is necessary to estimate this variable. Temperature based modeling procedures are reported in this study for estimating daily incoming solar radiation by using Gene Expression Programming (GEP) for the first time, and other artificial intelligence models such as Artificial Neural Networks (ANNs), and Adaptive Neuro-Fuzzy Inference System (ANFIS). A comparison was also made among these techniques and traditional temperature based global solar radiation estimation equations. Root mean square error (RMSE), mean absolute error (MAE) RMSE-based skill score (SS RMSE), MAE-based skill score (SS MAE) and r 2 criterion of Nash and Sutcliffe criteria were used to assess the models' performances. An ANN (a four-input multilayer perceptron with 10 neurons in the hidden layer) presented the best performance among the studied models (2.93 MJ m -2 d -1 of RMSE). The ability of GEP approach to model global solar radiation based on daily atmospheric variables was found to be satisfactory. © 2012 Elsevier Ltd. All rights reserved.","Artificial intelligence; Gene Expression Programming; Global solar radiation; Temperature","Adaptive neuro-fuzzy inference system; Atmospheric variables; Basque Country; Gene expression programming; Global solar radiation; Hidden layers; Key variables; Mean absolute error; Meteorological sensors; Modeling procedure; Multi layer perceptron; Network computing; Neuro-Fuzzy; Root mean square errors; Skill Score; Artificial intelligence; Energy conversion; Mean square error; Neural networks; Solar radiation; Sun; Temperature; Estimation",Article,Scopus,2-s2.0-84862537228
"Bao Q., Ruan D., Shen Y., Hermans E., Janssens D.","Improved hierarchical fuzzy TOPSIS for road safety performance evaluation",2012,"Knowledge-Based Systems",45,10.1016/j.knosys.2011.08.014,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861591272&doi=10.1016%2fj.knosys.2011.08.014&partnerID=40&md5=d17ac7aa4ff62e0d7e36510ad42aaac0","With the ever increasing public awareness of complicated road safety phenomenon, much more detailed aspects of crash and injury causation rather than only crash data are extensively investigated in the current road safety research. Safety performance indicators (SPIs), which are causally related to the number of crashes or to the injury consequences of a crash, are rapidly developed and increasingly used. To measure the multi-dimensional concept of road safety which cannot be captured by a single indicator, the exploration of a composite road safety performance index is vital for rational decision-making about road safety. In doing so, a proper decision support system is required. In this study, we propose an improved hierarchical fuzzy TOPSIS model to combine the multilayer SPIs into one overall index by incorporating experts' knowledge. Using the number of road fatalities per million inhabitants as a relevant reference, the proposed model provides with a promising intelligent decision support system to evaluate the road safety performance for a case study of a given set of European countries. It effectively handles experts' linguistic expressions and takes the layered hierarchy of the indicators into account. The comparison results with those from the original hierarchical fuzzy TOPSIS model further verify the robustness of the proposed model, and imply the feasibility of applying this model to a great number of performance evaluation and decision making activities in other wide ranging fields as well. © 2011 Elsevier B.V. All rights reserved.","Composite index; Decision support system; Fuzzy set theory; Hierarchical structure; Multi-criteria decision making; Road safety performance indicators; TOPSIS","Composite index; Hierarchical structures; Multi-criteria decision making; Road safety; TOPSIS; Artificial intelligence; Benchmarking; Decision making; Decision support systems; Fuzzy set theory; Accident prevention",Article,Scopus,2-s2.0-84861591272
"Bohy A., Bruyère V., Filiot E., Jin N., Raskin J.-F.","Acacia+, a tool for LTL synthesis",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",45,10.1007/978-3-642-31424-7_45,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864039419&doi=10.1007%2f978-3-642-31424-7_45&partnerID=40&md5=95c3093a459f2b950be1337505839b49","We present Acacia+, a tool for solving the LTL realizability and synthesis problems. We use recent approaches that reduce these problems to safety games, and can be solved efficiently by symbolic incremental algorithms based on antichains. The reduction to safety games offers very interesting properties in practice: the construction of compact solutions (when they exist) and a compositional approach for large conjunctions of LTL formulas. © 2012 Springer-Verlag.","antichains; Church problem; LTL synthesis; Moore machines; safety games","antichains; Compact solutions; Incremental algorithm; Moore machines; Realizability; Synthesis problems; Artificial intelligence; Computer aided analysis",Conference Paper,Scopus,2-s2.0-84864039419
"Wang H., Nie F., Huang H., Risacher S.L., Saykin A.J., Shen L.","Identifying disease sensitive and quantitative trait-relevant biomarkers from multidimensional heterogeneous imaging genetics data via sparse multimodal multitask learning",2012,"Bioinformatics",45,10.1093/bioinformatics/bts228,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863509119&doi=10.1093%2fbioinformatics%2fbts228&partnerID=40&md5=0abb5ad11746f452788047f65ce6034f","Motivation: Recent advances in brain imaging and high-throughput genotyping techniques enable new approaches to study the influence of genetic and anatomical variations on brain functions and disorders. Traditional association studies typically perform independent and pairwise analysis among neuroimaging measures, cognitive scores and disease status, and ignore the important underlying interacting relationships between these units. Results: To overcome this limitation, in this article, we propose a new sparse multimodal multitask learning method to reveal complex relationships from gene to brain to symptom. Our main contributions are three-fold: (i) introducing combined structured sparsity regularizations into multimodal multitask learning to integrate multidimensional heterogeneous imaging genetics data and identify multimodal biomarkers; (ii) utilizing a joint classification and regression learning model to identify disease-sensitive and cognition-relevant biomarkers; (iii) deriving a new efficient optimization algorithm to solve our non-smooth objective function and providing rigorous theoretical analysis on the global optimum convergency. Using the imaging genetics data from the Alzheimer's Disease Neuroimaging Initiative database, the effectiveness of the proposed method is demonstrated by clearly improved performance on predicting both cognitive scores and disease status. The identified multimodal biomarkers could predict not only disease status but also cognitive function to help elucidate the biological pathway from gene to brain structure and function, and to cognition and disease. © The Author(s) 2012. Published by Oxford University Press.",,"biological marker; algorithm; Alzheimer disease; article; artificial intelligence; brain; cognitive defect; computer program; genetics; genotype; human; metabolism; neuroimaging; phenotype; quantitative trait locus; support vector machine; Algorithms; Alzheimer Disease; Artificial Intelligence; Biological Markers; Brain; Cognition Disorders; Genotype; Humans; Neuroimaging; Phenotype; Quantitative Trait Loci; Software; Support Vector Machines",Article,Scopus,2-s2.0-84863509119
"Islam T., Rico-Ramirez M.A., Han D., Srivastava P.K.","Artificial intelligence techniques for clutter identification with polarimetric radar signatures",2012,"Atmospheric Research",45,10.1016/j.atmosres.2012.02.007,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859437946&doi=10.1016%2fj.atmosres.2012.02.007&partnerID=40&md5=25300aca253f167711479502dda54e74","The use of different artificial intelligence (AI) techniques for clutter signals identification in the context of radar based precipitation estimation is presented. The clutter signals considered are because of ground clutter, sea clutter and anomalous propagation whereas the explored AI techniques include the support vector machine (SVM), the artificial neural network (ANN), the decision tree (DT), and the nearest neighbour (NN) systems. Eight different radar measurement combinations comprising of various polarimetric spectral signatures - the reflectivity (Z H), differential reflectivity (Z DR), differential propagation phase (Φ DP), cross-correlation coefficient (ρ HV), velocity (V) and spectral width (W) from a C-band polarimetric radar are taken into account as input vectors to the AI systems. The results reveal that all four AI classifiers can identify the clutter echoes with around 98-99% accuracy when all radar input signatures are used. As standalone input vectors, the polarimetric textures of the Φ DP and the Z DR have also demonstrated excellent skills distinguishing clutter echoes with an accuracy of 97-98% approximately. If no polarimetric signature is available, a combination of the texture of Z H, V and W representing typical measurements from a single-polarization Doppler radar may be used for clutter identification, but with a lower accuracy when compared to the use of polarimetric radar measurements. In contrast, the use of Z H or W alone is found less reliable for clutter classification. Among the AI techniques, the SVM has a slightly better score in terms of various clutter identification indicators as compared to the others. Conversely, the NN algorithm has shown a lower performance in identifying the clutter echoes correctly considering the standalone radar signatures as inputs. Despite this, the performance among the different AI techniques is comparable indicating the suitability of the developed systems, and this is further supported when results are compared with the fuzzy logic and Bayes classifiers. © 2012 Elsevier B.V..","Anomalous propagation; Artificial neural network (ANN); Decision tree (DT); Dual polarization radar; Nearest neighbour algorithm (NN); Non-meteorological echo; Non-precipitation echo; Pattern recognition; Rainfall estimation uncertainty; Support vector machine (SVM); Target classification","Anomalous propagation; Artificial neural network (ANN); Dual polarization radars; Nearest neighbour; Non-meteorological echo; Non-precipitation echoes; Rainfall estimation uncertainty; Support vector machine (SVM); Target classification; Decision trees; Doppler radar; Fuzzy logic; Meteorological radar; Neural networks; Pattern recognition; Polarimeters; Radar measurement; Radar target recognition; Reflection; Support vector machines; Textures; Clutter (information theory); artificial intelligence; nearest neighbor analysis; pattern recognition; precipitation assessment; radar imagery; rainfall; remote sensing",Article,Scopus,2-s2.0-84859437946
"Kakade S.M., Shalev-Shwartz S., Tewari A.","Regularization techniques for learning with matrices",2012,"Journal of Machine Learning Research",45,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859411801&partnerID=40&md5=0c37ea1e1290f9413b417e54612ea310","There is growing body of learning problems for which it is natural to organize the parameters into a matrix. As a result, it becomes easy to impose sophisticated prior knowledge by appropriately regularizing the parameters under some matrix norm. This work describes and analyzes a systematic method for constructing such matrix-based regularization techniques. In particular, we focus on how the underlying statistical properties of a given problem can help us decide which regularization function is appropriate. Our methodology is based on a known duality phenomenon: a function is strongly convex with respect to some norm if and only if its conjugate function is strongly smooth with respect to the dual norm. This result has already been found to be a key component in deriving and analyzing several learning algorithms. We demonstrate the potential of this framework by deriving novel generalization and regret bounds for multi-task learning, multi-class learning, and multiple kernel learning. © 2012 Sham M. Kakade, Shai Shalev-Shwartz and Ambuj Tewari.","Generalization bounds; Multi-class learning; Multi-task learning; Multiple kernel learning; Regret bounds; Regularization; Strong convexity","Generalization bound; Multi-class; Multiple Kernel Learning; Multitask learning; Regret bounds; Regularization; Strong convexity; Artificial intelligence; Software engineering; Learning algorithms",Article,Scopus,2-s2.0-84859411801
"Dong G., Guo W.W., Tickle K.","Solving the traveling salesman problem using cooperative genetic ant systems",2012,"Expert Systems with Applications",45,10.1016/j.eswa.2011.10.012,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855901720&doi=10.1016%2fj.eswa.2011.10.012&partnerID=40&md5=d09daa146f567856aea70c1a0f9da02b","The travelling salesman problem (TSP) is a classic problem of combinatorial optimization and has applications in planning, scheduling, and searching in many scientific and engineering fields. Ant colony optimization (ACO) has been successfully used to solve TSPs and many associated applications in the last two decades. However, ACO has problem in regularly reaching the global optimal solutions for TSPs due to enormity of the search space and numerous local optima within the space. In this paper, we propose a new hybrid algorithm, cooperative genetic ant system (CGAS) to deal with this problem. Unlike other previous studies that regarded GA as a sequential part of the whole searching process and only used the result from GA as the input to subsequent ACO iterations, this new approach combines both GA and ACO together in a cooperative manner to improve the performance of ACO for solving TSPs. The mutual information exchange between ACO and GA in the end of the current iteration ensures the selection of the best solutions for next iteration. This cooperative approach creates a better chance in reaching the global optimal solution because independent running of GA maintains a high level of diversity in next generation of solutions. Compared with results from other GA/ACO algorithms, our simulation shows that CGAS has superior performance over other GA and ACO algorithms for solving TSPs in terms of capability and consistency of achieving the global optimal solution, and quality of average optimal solutions, particularly for small TSPs. © 2011 Elsevier Ltd. All rights reserved.","Ant colony optimization; Ant system; Genetic algorithm; Traveling salesman problem","ACO algorithms; Ant system; Ant systems; Ant-colony optimization; Engineering fields; Global optimal solutions; Hybrid algorithms; Local optima; Mutual informations; Optimal solutions; Search spaces; Selection of the best; Traveling salesman; Travelling salesman problem; Artificial intelligence; Combinatorial optimization; Genetic algorithms; Optimal systems; Traveling salesman problem",Article,Scopus,2-s2.0-84855901720
"Xu R., Xu J., Wunsch D.C.","A comparison study of validity indices on swarm-intelligence-based clustering",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",45,10.1109/TSMCB.2012.2188509,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864125280&doi=10.1109%2fTSMCB.2012.2188509&partnerID=40&md5=cd2249a6613999a11026370ffc790c18","Swarm intelligence has emerged as a worthwhile class of clustering methods due to its convenient implementation, parallel capability, ability to avoid local minima, and other advantages. In such applications, clustering validity indices usually operate as fitness functions to evaluate the qualities of the obtained clusters. However, as the validity indices are usually data dependent and are designed to address certain types of data, the selection of different indices as the fitness functions may critically affect cluster quality. Here, we compare the performances of eight well-known and widely used clustering validity indices, namely, the Caliski-Harabasz index, the CS index, the Davies-Bouldin index, the Dunn index with two of its generalized versions, the I index, and the silhouette statistic index, on both synthetic and real data sets in the framework of differential-evolution-particle-swarm-optimization (DEPSO)-based clustering. DEPSO is a hybrid evolutionary algorithm of the stochastic optimization approach (differential evolution) and the swarm intelligence method (particle swarm optimization) that further increases the search capability and achieves higher flexibility in exploring the problem space. According to the experimental results, we find that the silhouette statistic index stands out in most of the data sets that we examined. Meanwhile, we suggest that users reach their conclusions not just based on only one index, but after considering the results of several indices to achieve reliable clustering structures. © 2012 IEEE.","Clustering; differential evolution (DE); particle swarm optimization (PSO); swarm intelligence; validity index","Clustering; Clustering methods; Clustering validity index; Comparison study; Data dependent; Data sets; Davies-Bouldin index; Differential Evolution; Fitness functions; Hybrid evolutionary algorithm; Local minimums; Problem space; Search capabilities; Stochastic optimization approach; Swarm Intelligence; Synthetic and real data; Validity index; Particle swarm optimization (PSO); Artificial intelligence",Article,Scopus,2-s2.0-84864125280
"Ugulino W., Cardador D., Vega K., Velloso E., Milidiú R., Fuks H.","Wearable computing: Accelerometers’ data classification of body postures and movements",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",45,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952063069&partnerID=40&md5=42a80b8b295bca67d343fec86e0c4153","During the last 5 years, research on Human Activity Recognition (HAR) has reported on systems showing good overall recognition performance. As a consequence, HAR has been considered as a potential technology for e- health systems. Here, we propose a machine learning based HAR classifier. We also provide a full experimental description that contains the HAR wearable de- vices setup and a public domain dataset comprising 165,633 samples. We consider 5 activity classes, gathered from 4 subjects wearing accelerometers mounted on their waist, left thigh, right arm, and right ankle. As basic input features to our classifier we use 12 attributes derived from a time window of 150ms. Finally, the classifier uses a committee AdaBoost that combines ten Decision Trees. The observed classifier accuracy is 99.4%. © Springer-Verlag Berlin Heidelberg 2012.","Accelerometer; Human Activity Recognition; Machine Learning; Wearable Computing","Accelerometers; Adaptive boosting; Artificial intelligence; Decision trees; Learning systems; Pattern recognition; Wearable computers; Wearable sensors; Wearable technology; Body postures; Data classification; e-Health systems; Human activity recognition; Input features; Potential technologies; Public domains; Wearable computing; Classification (of information)",Conference Paper,Scopus,2-s2.0-84952063069
"Barbu A., Bridge A., Burchill Z., Coroian D., Dickinson S., Fidler S., Michaux A., Mussman S., Narayanaswamy S., Salvi D., Schmidt L., Shangguan J., Siskind J.M., Waggoner J., Wang S., Wei J., Yin Y., Zhang Z.","Video in sentences out",2012,"Uncertainty in Artificial Intelligence - Proceedings of the 28th Conference, UAI 2012",44,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885996388&partnerID=40&md5=1d0b74634e7c9b2a406ed8924bf2b14e","We present a system that produces sentential descriptions of video: who did what to whom, and where and how they did it. Action class is rendered as a verb, participant objects as noun phrases, properties of those objects as adjectival modifiers in those noun phrases, spatial relations between those participants as prepositional phrases, and characteristics of the event as prepositional-phrase adjuncts and adverbial modifiers. Extracting the information needed to render these linguistic entities requires an approach to event recognition that recovers object tracks, the trackto- role assignments, and changing body posture.",,"Adjectival modifier; Body postures; Event recognition; Noun phrase; Object track; Prepositional phrase; Role assignment; Spatial relations; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84885996388
"Taha Z., Rostam S.","A hybrid fuzzy AHP-PROMETHEE decision support system for machine tool selection in flexible manufacturing cell",2012,"Journal of Intelligent Manufacturing",44,10.1007/s10845-011-0560-2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870934893&doi=10.1007%2fs10845-011-0560-2&partnerID=40&md5=1cf977e6b9064d616680deda6743c473","The selection process of a suitable machine tool among the increased number of alternatives has been an important issue for manufacturing companies for years. This is because the improper selection of a machine tool may cause many problems that will affect the overall performance. In this paper, a decision support system (DSS) is presented to select the best alternative machine using a hybrid approach of fuzzy analytic hierarchy process (fuzzy AHP) and preference ranking organization method for enrichment evaluation (PROMETHEE). A MATLAB- based fuzzy AHP is used to determine the weights of the criteria and it is called program for PriorityWeights of the Evaluation Criteria (PWEC), and the PROMETHEE method is applied for the final ranking. The proposed model is structured to select the most suitable computer numerical controlled (CNC) turning centre machine for a flexible manufacturing cell (FMC) among the alternatives which are assigned from a database (DB) created for this purpose. A numerical example is presented to show the applicability of the model. It is concluded that the proposed model has the capability of dealing with a wide range of desired criteria and to select any type of machine tool required for building an FMC. © Springer Science+Business Media, LLC 2011.","CNC machines; Decision-making; FMC; Fuzzy AHP; Machine selection; PROMETHEE","CNC machine; FMC; Fuzzy AHP; Machine selection; PROMETHEE; Artificial intelligence; Computer control systems; Decision support systems; Hierarchical systems; Machine tools; MATLAB; Flexible manufacturing systems",Article,Scopus,2-s2.0-84870934893
"Pandey G., McBride J.R., Savarese S., Eustice R.M.","Automatic targetless extrinsic calibration of a 3D lidar and camera by maximizing mutual information",2012,"Proceedings of the National Conference on Artificial Intelligence",44,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868297958&partnerID=40&md5=b5deff9a81eac5ec5287e8c118d8610e","This paper reports on a mutual information (MI) based algorithm for automatic extrinsic calibration of a 3D laser scanner and optical camera system. By using MI as the registration criterion, our method is able to work in situ without the need for any specific calibration targets, which makes it practical for in-field calibration. The calibration parameters are estimated by maximizing the mutual information obtained between the sensor-measured surface intensities. We calculate the Cramer-Rao-Lower-Bound (CRLB) and show that the sample variance of the estimated parameters empirically approaches the CRLB for a sufficient number of views. Furthermore, we compare the calibration results to independent ground-truth and observe that the mean error also empirically approaches to zero as the number of views are increased. This indicates that the proposed algorithm, in the limiting case, calculates a minimum variance unbiased (MVUB) estimate of the calibration parameters. Experimental results are presented for data collected by a vehicle mounted with a 3D laser scanner and an omnidirectional camera system. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"3D laser scanners; Calibration parameters; Calibration targets; Estimated parameter; Extrinsic calibration; In-field; Limiting case; Mean errors; Minimum variance; Mutual informations; Omnidirectional camera system; Optical camera; Sample variance; Algorithms; Artificial intelligence; Laser applications; Lasers; Optical radar; Parameter estimation; Scanning; Three dimensional computer graphics; Calibration",Conference Paper,Scopus,2-s2.0-84868297958
"Bland W., Bouteiller A., Herault T., Hursey J., Bosilca G., Dongarra J.J.","An evaluation of user-level failure mitigation support in MPI",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",44,10.1007/978-3-642-33518-1_24,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867646266&doi=10.1007%2f978-3-642-33518-1_24&partnerID=40&md5=b0b61f7cbb06b2e5e392db0a1bdfac6d","As the scale of computing platforms becomes increasingly extreme, the requirements for application fault tolerance are increasing as well. Techniques to address this problem by improving the resilience of algorithms have been developed, but they currently receive no support from the programming model, and without such support, they are bound to fail. This paper discusses the failure-free overhead and recovery impact aspects of the User-Level Failure Mitigation proposal presented in the MPI Forum. Experiments demonstrate that fault-aware MPI has little or no impact on performance for a range of applications, and produces satisfactory recovery times when there are failures. © 2012 Springer-Verlag.",,"Computing platform; Failure mitigation; Programming models; Recovery time; Artificial intelligence; Fault tolerance",Conference Paper,Scopus,2-s2.0-84867646266
"Yin Y.H., Xie J.Y., Xu L.D., Chen H.","Imaginal thinking-based human-machine design methodology for the configuration of reconfigurable machine tools",2012,"IEEE Transactions on Industrial Informatics",44,10.1109/TII.2012.2188900,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864558877&doi=10.1109%2fTII.2012.2188900&partnerID=40&md5=33c23172748fe8270c458ce1a190ec13","Reconfigurable manufacturing systems (RMSs) has emerged as an advanced system for applying Enterprise Systems (ES) in manufacturing enterprises to dynamically adapt to the changing market. Although it has been recognized that human-machine interaction plays a significant role in configuration design, especially for reconfigurable machines tools (RMTs), for the core of RMS, there are no platforms available for the harmonious interaction between human and computer, mainly because of the limitation of traditional artificial intelligence frame. In this paper, we present a human-machine design methodology for the configuration of RMTs based on imaginal thinking, a thinking style described as forming and comparison of images, which is radically different from traditional logical and intuitive thinking. The knowledge and experience that are highly relevant to mechanical design and the related information in logical deduction are organized as high dimensional images. By imitating human imaginal thinking, computer employs Lie subgroup or submanifold and Quotient kinematics to generate all possible configurations in the form of images, while humans make judgments and complete the optimization process through the imaginal human-machine interface. Both human's decision-making ability based on knowledge and experience and computer's high-speed logical computation are exploited to a full extent. This novel design methodology has illustrated its effectiveness by an example concerning the design of a pipe-cutting RMT. © 2005-2012 IEEE.","Configuration design; enterprise systems; human-machine interface; imaginal thinking; industrial informatics","Configuration designs; Enterprise system; Human Machine Interface; Imaginal thinkings; Industrial informatics; Artificial intelligence; Machine design; Man machine systems; Human computer interaction",Article,Scopus,2-s2.0-84864558877
"Jurman G., Riccadonna S., Furlanello C.","A comparison of MCC and CEN error measures in multi-class prediction",2012,"PLoS ONE",44,10.1371/journal.pone.0041882,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864668842&doi=10.1371%2fjournal.pone.0041882&partnerID=40&md5=4a2273016b113fd3dd4ff419823063b2","We show that the Confusion Entropy, a measure of performance in multiclass problems has a strong (monotone) relation with the multiclass generalization of a classical metric, the Matthews Correlation Coefficient. Analytical results are provided for the limit cases of general no-information (n-face dice rolling) of the binary classification. Computational evidence supports the claim in the general case. © 2012 Jurman et al.",,"article; comparative study; confusion entropy; correlation coefficient; entropy; information processing; Matthews correlation coefficient; prediction; statistical analysis; Algorithms; Area Under Curve; Artificial Intelligence; Computational Biology; Computers; Entropy; Genotype; Models, Statistical; Oligonucleotide Array Sequence Analysis; Probability; Reproducibility of Results; ROC Curve; Software; Statistics as Topic",Article,Scopus,2-s2.0-84864668842
"Rocha A., Carvalho T., Jelinek H.F., Goldenstein S., Wainer J.","Points of interest and visual dictionaries for automatic retinal lesion detection",2012,"IEEE Transactions on Biomedical Engineering",44,10.1109/TBME.2012.2201717,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864225627&doi=10.1109%2fTBME.2012.2201717&partnerID=40&md5=9f3c5752c5cf387c93151c62ee583318","In this paper, we present an algorithm to detect the presence of diabetic retinopathy (DR)-related lesions from fundus images based on a common analytical approach that is capable of identifying both red and bright lesions without requiring specific pre- or postprocessing. Our solution constructs a visual word dictionary representing points of interest (PoIs) located within regions marked by specialists that contain lesions associated with DR and classifies the fundus images based on the presence or absence of these PoIs as normal or DR-related pathology. The novelty of our approach is in locating DR lesions in the optic fundus images using visual words that combines feature information contained within the images in a framework easily extendible to different types of retinal lesions or pathologies and builds a specific projection space for each class of interest (e.g., white lesions such as exudates or normal regions) instead of a common dictionary for all classes. The visual words dictionary was applied to classifying bright and red lesions with classical cross validation and cross dataset validation to indicate the robustness of this approach. We obtained an area under the curve (AUC) of 95.3 for white lesion detection and an AUC of 93.3 for red lesion detection using fivefold cross validation and our own data consisting of 687 images of normal retinae, 245 images with bright lesions, 191 with red lesions, and 109 with signs of both bright and red lesions. For cross dataset analysis, the visual dictionary also achieves compelling results using our images as the training set and the RetiDB and Messidor images as test sets. In this case, the image classification resulted in an AUC of 88.1 when classifying the RetiDB dataset and in an AUC of 89.3 when classifying the Messidor dataset, both cases for bright lesion detection. The results indicate the potential for training with different acquisition images under different setup conditions with a high accuracy of referral based on the presence of either red or bright lesions or both. The robustness of the visual dictionary against image quality (blurring), resolution, and retinal background, makes it a strong candidate for DR screening of large, diverse communities with varying cameras and settings and levels of expertise for image capture. © 2012 IEEE.","diabetes automated screening; Diabetic retinopathy (DR); hard exudate detection; hemorrhage detection; microaneurysm detection; red and bright lesion classification; visual dictionaries","Automated screening; Diabetic retinopathy; Hard exudates; Hemorrhage detection; Lesion classification; Microaneurysms; Automation; Diseases; Eye protection; Ophthalmology; Pathology; Statistical tests; Aldehydes; accuracy; article; camera; classification algorithm; diabetic retinopathy; diagnostic imaging; eye fundus; image processing; image quality; screening; validation process; Algorithms; Artificial Intelligence; Databases, Factual; Diabetic Retinopathy; Diagnostic Techniques, Ophthalmological; Humans; Image Interpretation, Computer-Assisted; Reproducibility of Results; Retina; Retinal Vessels",Article,Scopus,2-s2.0-84864225627
"Swartjes F.A., Rutgers M., Lijzen J.P.A., Janssen P.J.C.M., Otte P.F., Wintersen A., Brand E., Posthuma L.","State of the art of contaminated site management in The Netherlands: Policy framework and risk assessment tools",2012,"Science of the Total Environment",44,10.1016/j.scitotenv.2012.02.078,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861224372&doi=10.1016%2fj.scitotenv.2012.02.078&partnerID=40&md5=a238207f29c4318c1ccf6ef017d7dcdd","This paper presents the policy framework of contaminated site management in The Netherlands and the corresponding risk assessment tools, including innovations that have taken place since an overview was published in 1999.According to the Dutch Soil Protection Act assessment framework, soils are subdivided into three quality classes: clean, slightly contaminated and seriously contaminated. Historic cases of slightly contaminated soils are managed in a sustainable way by re-use of soil material within a region on the basis of risk-based and land use specific Maximal Values and Background Values. In case of serious soil contamination remediation is in principle necessary and the urgency of remediation has to be determined based on site-specific risks for human health, the ecosystem and groundwater.The major risk assessment tools in The Netherlands are the CSOIL exposure model (human health risks and food safety), Species Sensitivity Distributions and the Soil Quality Triad (ecological risks), along with a procedure to assess the risks due to contaminant spreading to and in the groundwater.Following the principle 'simple if possible, complex when necessary', tiered approaches are used. Contaminated site practices are supported with web-based decision support systems. © 2012 Elsevier B.V.","Ecological risks; Human health risks; Intervention values; Maximal values; Soil policy; Urgency of remediation","Background value; Contaminated site management; Contaminated sites; Contaminated soils; Ecological risks; Human health; Human health risks; Intervention values; Maximal values; Netherlands; Policy framework; Risk assessment tool; Risk-based; Site-specific; Soil contamination; Soil materials; Soil Protection; Soil quality; Species sensitivity distributions; State of the art; Web-based decision support systems; Artificial intelligence; Contamination; Decision support systems; Ecology; Geologic models; Remediation; Risk assessment; Soil conservation; Soil pollution; Soils; Websites; Health risks; decision support system; environmental management; environmental risk; health risk; risk assessment; soil pollution; soil quality; soil remediation; article; bioremediation; environmental exposure; food safety; hazard assessment; health hazard; Netherlands; priority journal; risk assessment; soil management; soil pollution; soil quality; species distribution; Ecosystem; Environmental Policy; Environmental Pollution; Food Safety; Government Regulation; Humans; Models, Biological; Netherlands; Risk Assessment; Soil Pollutants; Waste Management; Water Pollutants, Chemical; Water Pollution; Netherlands",Article,Scopus,2-s2.0-84861224372
"Janzing D., Mooij J., Zhang K., Lemeire J., Zscheischler J., Daniušis P., Steudel B., Schölkopf B.","Information-geometric approach to inferring causal directions",2012,"Artificial Intelligence",44,10.1016/j.artint.2012.01.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857129458&doi=10.1016%2fj.artint.2012.01.002&partnerID=40&md5=93d71b32c781ba76ae2e69e0d9568c0d","While conventional approaches to causal inference are mainly based on conditional (in)dependences, recent methods also account for the shape of (conditional) distributions. The idea is that the causal hypothesis ""X causes Y"" imposes that the marginal distribution PX and the conditional distribution PY| X represent independent mechanisms of nature. Recently it has been postulated that the shortest description of the joint distribution PX, Y should therefore be given by separate descriptions of PX and PY| X. Since description length in the sense of Kolmogorov complexity is uncomputable, practical implementations rely on other notions of independence. Here we define independence via orthogonality in information space. This way, we can explicitly describe the kind of dependence that occurs between PY and PX| Y making the causal hypothesis ""Y causes X"" implausible. Remarkably, this asymmetry between cause and effect becomes particularly simple if X and Y are deterministically related. We present an inference method that works in this case. We also discuss some theoretical results for the non-deterministic case although it is not clear how to employ them for a more general inference method. © 2012 Elsevier B.V. All rights reserved.","Cause-effect pairs; Deterministic causal relations; Pythagorean triple","Causal relations; Cause-effect; Conditional distribution; Conventional approach; Inference methods; Information spaces; Joint distributions; Kolmogorov complexity; Marginal distribution; Orthogonality; Practical implementation; Pythagorean triple; Theoretical result; Artificial intelligence",Article,Scopus,2-s2.0-84857129458
"Greenfield S., Chiclana F., John R., Coupland S.","The sampling method of defuzzification for type-2 fuzzy sets: Experimental evaluation",2012,"Information Sciences",44,10.1016/j.ins.2011.11.042,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855876107&doi=10.1016%2fj.ins.2011.11.042&partnerID=40&md5=eefbdec960e0081d32801ff6a3f98d14","For generalised type-2 fuzzy sets the defuzzification process has historically been slow and inefficient. This has hampered the development of type-2 Fuzzy Inferencing Systems for real applications and therefore no advantage has been taken of the ability of type-2 fuzzy sets to model higher levels of uncertainty. The research reported here provides a novel approach for improving the speed of defuzzification for discretised generalised type-2 fuzzy sets. The traditional type-reduction method requires every embedded type-2 fuzzy set to be processed. The high level of redundancy in the huge number of embedded sets inspired the development of our sampling method which randomly samples the embedded sets and processes only the sample. The paper presents detailed experimental results for defuzzification of constructed sets of known defuzzified value. The sampling defuzzifier is compared on aggregated type-2 fuzzy sets resulting from the inferencing stage of a FIS, in terms of accuracy and speed, with other methods including the exhaustive and techniques based on the α-planes representation. The results indicate that by taking only a sample of the embedded sets we are able to dramatically reduce the time taken to process a type-2 fuzzy set with very little loss in accuracy. © 2011 Elsevier Inc. All rights reserved.","Defuzzification; Sampling method; Type-2 fuzzy set; Type-reduced set; Type-reduction","Defuzzifications; Sampling method; Type-2 fuzzy set; Type-reduced set; Type-reduction; Artificial intelligence; Software engineering; Fuzzy sets",Article,Scopus,2-s2.0-84855876107
"Broccatelli F., Cruciani G., Benet L.Z., Oprea T.I.","Bddcs class prediction for new molecular entities",2012,"Molecular Pharmaceutics",44,10.1021/mp2004302,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858039726&doi=10.1021%2fmp2004302&partnerID=40&md5=138e33c9aba7060258541c80e72e2f3b","The Biopharmaceutics Drug Disposition Classification System (BDDCS) was successfully employed for predicting drug - drug interactions (DDIs) with respect to drug metabolizing enzymes (DMEs), drug transporters and their interplay. The major assumption of BDDCS is that the extent of metabolism (EoM) predicts high versus low intestinal permeability rate, and vice versa, at least when uptake transporters or paracellular transport is not involved. We recently published a collection of over 900 marketed drugs classified for BDDCS. We suggest that a reliable model for predicting BDDCS class, integrated with in vitro assays, could anticipate disposition and potential DDIs of new molecular entities (NMEs). Here we describe a computational procedure for predicting BDDCS class from molecular structures. The model was trained on a set of 300 oral drugs, and validated on an external set of 379 oral drugs, using 17 descriptors calculated or derived from the VolSurf+ software. For each molecule, a probability of BDDCS class membership was given, based on predicted EoM, FDA solubility (FDAS) and their confidence scores. The accuracy in predicting FDAS was 78% in training and 77% in validation, while for EoM prediction the accuracy was 82% in training and 79% in external validation. The actual BDDCS class corresponded to the highest ranked calculated class for 55% of the validation molecules, and it was within the top two ranked more than 92% of the time. The unbalanced stratification of the data set did not affect the prediction, which showed highest accuracy in predicting classes 2 and 3 with respect to the most populated class 1. For class 4 drugs a general lack of predictability was observed. A linear discriminant analysis (LDA) confirming the degree of accuracy for the prediction of the different BDDCS classes is tied to the structure of the data set. This model could routinely be used in early drug discovery to prioritize in vitro tests for NMEs (e.g., affinity to transporters, intestinal metabolism, intestinal absorption and plasma protein binding). We further applied the BDDCS prediction model on a large set of medicinal chemistry compounds (over 30,000 chemicals). Based on this application, we suggest that solubility, and not permeability, is the major difference between NMEs and drugs. We anticipate that the forecast of BDDCS categories in early drug discovery may lead to a significant R&D cost reduction. © 2012 American Chemical Society.","ADMET; BDDCS; drug disposition; drug-drug interactions; FDA solubility; GRID; machine learning; MIF; VolSurf+","carrier protein; drug; accuracy; article; computer program; drug classification; drug metabolism; drug penetration; drug solubility; drug structure; drug transport; drug uptake; food and drug administration; human; intestine mucosa permeability; prediction; priority journal; validation study; Artificial Intelligence; Biopharmaceutics; Drug Interactions; Models, Theoretical; Pharmaceutical Preparations; Solubility",Article,Scopus,2-s2.0-84858039726
"Kang Q., Lan T., Yan Y., Wang L., Wu Q.","Group search optimizer based optimal location and capacity of distributed generations",2012,"Neurocomputing",44,10.1016/j.neucom.2011.05.030,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-82655162101&doi=10.1016%2fj.neucom.2011.05.030&partnerID=40&md5=780c96ab7b198ac20a23d16adbbfa920","This paper presents a novel efficient population-based heuristic approach for optimal location and capacity of distributed generations (DGs) in distribution networks, with the objectives of minimization of fuel cost, power loss reduction, and voltage profile improvement. The approach employs an improved group search optimizer (iGSO) proposed in this paper by incorporating particle swarm optimization (PSO) into group search optimizer (GSO) for optimal setting of DGs. The proposed approach is executed on a networked distribution system-the IEEE 14-bus test system for different objectives. The results are also compared to those that executed by basic GSO algorithm and PSO algorithm on the same test system. The results show the effectiveness and promising applications of the proposed approach in optimal location and capacity of DGs. © 2011 Elsevier B.V.","Distributed generations; Improved group search optimizer; Optimal location and capacity; Swarm intelligence","Fuel cost; Heuristic approach; Optimal locations; Optimal setting; Power loss reduction; PSO algorithms; Search optimizer; Swarm Intelligence; Test systems; Voltage profile improvement; Algorithms; Artificial intelligence; Cellular automata; Distributed power generation; Heuristic methods; Particle swarm optimization (PSO); Location; algorithm; article; distributed generation; electric potential; electricity; group search optimizer; heuristic algorithm; intelligence; network learning; power supply; priority journal; process optimization; simulation",Article,Scopus,2-s2.0-82655162101
"Porayska-Pomsta K., Frauenberger C., Pain H., Rajendran G., Smith T., Menzies R., Foster M.E., Alcorn A., Wass S., Bernadini S., Avramides K., Keay-Bright W., Chen J., Waller A., Guldberg K., Good J., Lemon O.","Developing technology for autism: An interdisciplinary approach",2012,"Personal and Ubiquitous Computing",44,10.1007/s00779-011-0384-2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857639401&doi=10.1007%2fs00779-011-0384-2&partnerID=40&md5=3ad91b4665c1b5fcc03d2522b8a58f9f","We present an interdisciplinary methodology for designing interactive multi-modal technology for young children with autism spectrum disorders (ASDs). In line with many other researchers in the field, we believe that the key to developing technology in this context is to embrace perspectives from diverse disciplines to arrive at a methodology that delivers satisfactory outcomes for all stakeholders. The ECHOES project provided us with the opportunity to develop a technology-enhanced learning (TEL) environment that facilitates acquisition and exploration of social skills by typically developing (TD) children and children with autism spectrum disorders (ASDs). ECHOES' methodology and the learning environment rely crucially on multi-disciplinary expertise including developmental psychology, visual arts, human-computer interaction, artificial intelligence, education, and several other cognate disciplines. In this article, we reflect on the methods needed to develop a TEL environment for young users with ASDs by identifying key features, benefits, and challenges of this approach. © 2011 Springer-Verlag London Limited.","Autism; Autonomous agents; Interdisciplinary research; Social interactions; Social signal processing; Technology-enhanced intervention","Autism; Interdisciplinary research; Social interactions; Social signals; Technology-enhanced intervention; Artificial intelligence; Autonomous agents; Education; Signal processing; Technology; Diseases",Article,Scopus,2-s2.0-84857639401
"Deng L.","The MNIST database of handwritten digit images for machine learning research",2012,"IEEE Signal Processing Magazine",44,10.1109/MSP.2012.2211477,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032752689&doi=10.1109%2fMSP.2012.2211477&partnerID=40&md5=efb588a7093fa4ddb9e65d13b64dcc89","In this issue, Best of the Web presents the modified National Institute of Standards and Technology (MNIST) resources, consisting of a collection of handwritten digit images used extensively in optical character recognition and machine learning research. © 2012 IEEE.",,"Artificial intelligence; Character recognition; Optical character recognition; Handwritten digit; Machine learning research; Mnist database; National Institute of Standards and Technology; Learning systems",Article,Scopus,2-s2.0-85032752689
"Starbird K., Muzny G., Palen L.","Learning from the crowd: Collaborative filtering techniques for identifying on-the-ground Twitterers during mass disruptions",2012,"ISCRAM 2012 Conference Proceedings - 9th International Conference on Information Systems for Crisis Response and Management",44,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905592855&partnerID=40&md5=6075962cf68663a5b30fa597a11ac1c5","Social media tools, including the microblogging platform Twitter, have been appropriated during mass disruption events by those affected as well as the digitally-convergent crowd. Though tweets sent by those local to an event could be a resource both for responders and those affected, most Twitter activity during mass disruption events is generated by the remote crowd. Tweets from the remote crowd can be seen as noise that must be filtered, but another perspective considers crowd activity as a filtering and recommendation mechanism. This paper tests the hypothesis that crowd behavior can serve as a collaborative filter for identifying people tweeting from the ground during a mass disruption event. We test two models for classifying on-the-ground Twitterers, finding that machine learning techniques using a Support Vector Machine with asymmetric soft margins can be effective in identifying those likely to be on the ground during a mass disruption event. © 2012 ISCRAM.","Crisis informatics; Human computation; Machine learning; Mass disruption; Microblogging; Political protest; Support vector machine","Artificial intelligence; Information systems; Learning systems; Social networking (online); Support vector machines; Crisis informatics; Human computation; Mass disruption; Microblogging; Political protest; Behavioral research",Conference Paper,Scopus,2-s2.0-84905592855
"Yao K., Li X.","Uncertain alternating renewal process and its application",2012,"IEEE Transactions on Fuzzy Systems",43,10.1109/TFUZZ.2012.2194152,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870513575&doi=10.1109%2fTFUZZ.2012.2194152&partnerID=40&md5=42ceb666100b7f5671ad1fc2fb8ed259","Uncertain process is a sequence of uncertain variables indexed by time and space. First, this paper presents a kind of uncertain process, known as the uncertain alternating renewal process, whose alternating interarrival times are uncertain variables. Then, it proves an uncertain alternating renewal theorem on the limit value of average working rate. Finally, an application of the alternating renewal theorem is discussed. © 2012 IEEE.","Alternating renewal process; renewal process; uncertain process; uncertainty theory","Alternating renewal process; Inter-arrival time; Limit values; Renewal process; Renewal theorem; Uncertain process; Uncertain variables; Uncertainty theory; Artificial intelligence; Fuzzy sets",Article,Scopus,2-s2.0-84870513575
"Ballan L., Taneja A., Gall J., Van Gool L., Pollefeys M.","Motion capture of hands in action using discriminative salient points",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",43,10.1007/978-3-642-33783-3_46,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867852231&doi=10.1007%2f978-3-642-33783-3_46&partnerID=40&md5=9bff49c2e2bebc0280c641d279bc8550","Capturing the motion of two hands interacting with an object is a very challenging task due to the large number of degrees of freedom, self-occlusions, and similarity between the fingers, even in the case of multiple cameras observing the scene. In this paper we propose to use discriminatively learned salient points on the fingers and to estimate the finger-salient point associations simultaneously with the estimation of the hand pose. We introduce a differentiable objective function that also takes edges, optical flow and collisions into account. Our qualitative and quantitative evaluations show that the proposed approach achieves very accurate results for several challenging sequences containing hands and objects in action. © 2012 Springer-Verlag.",,"Hand pose; Motion capture; Multiple cameras; Number of degrees of freedom; Objective functions; Quantitative evaluation; Salient points; Self-occlusions; Two hand; Artificial intelligence; Computer vision",Conference Paper,Scopus,2-s2.0-84867852231
"Wagstaff K.L.","Machine learning that matters",2012,"Proceedings of the 29th International Conference on Machine Learning, ICML 2012",43,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867137280&partnerID=40&md5=86716dd9fbca4e70270e7aefa982dfca","Much of current machine learning (ML) research has lost its connection to problems of import to the larger world of science and society. From this perspective, there exist glaring limitations in the data sets we investigate, the metrics we employ for evaluation, and the degree to which results are communicated back to their originating domains. What changes are needed to how we conduct research to increase the impact that ML has? We present six Impact Challenges to explicitly focus the field's energy and attention, and we discuss existing obstacles that must be addressed. We aim to inspire ongoing discussion and focus on ML that matters. Copyright 2012 California Institute of Technology.",,"Data sets; Artificial intelligence; Software engineering; Learning systems",Conference Paper,Scopus,2-s2.0-84867137280
"Mariani V.C., Duck A.R.K., Guerra F.A., Coelho L.D.S., Rao R.V.","A chaotic quantum-behaved particle swarm approach applied to optimization of heat exchangers",2012,"Applied Thermal Engineering",43,10.1016/j.applthermaleng.2012.03.022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861098570&doi=10.1016%2fj.applthermaleng.2012.03.022&partnerID=40&md5=a399f9d7e4133ed881123ebee4045f38","Particle swarm optimization (PSO) method is a population-based optimization technique of swarm intelligence field in which each solution called ""particle"" flies around in a multidimensional problem search space. During the flight, every particle adjusts its position according to its own experience, as well as the experience of neighboring particles, using the best position encountered by itself and its neighbors. In this paper, a new quantum particle swarm optimization (QPSO) approach combined with Zaslavskii chaotic map sequences (QPSOZ) to shell and tube heat exchanger optimization is presented based on the minimization from economic view point. The results obtained in this paper for two case studies using the proposed QPSOZ approach, are compared with those obtained by using genetic algorithm, PSO and classical QPSO showing the best performance of QPSOZ. In order to verify the capability of the proposed method, two case studies are also presented showing that significant cost reductions are feasible with respect to traditionally designed exchangers. Referring to the literature test cases, reduction of capital investment up to 20% and 6% for the first and second cases, respectively, were obtained. Therefore, the annual pumping cost decreased markedly 72% and 75%, with an overall decrease of total cost up to 30% and 27%, respectively, for the cases 1 and 2, respectively, showing the improvement potential of the proposed method, QPSOZ. © 2011 Elsevier Ltd. All rights reserved.","Chaos theory; Economic optimization; Heat exchanger design; Particle swarm optimization; Quantum particle swarm optimization; Shell and tube heat exchanger (STHE)","Best position; Capital investment; Chaotic map; Economic optimization; Economic views; Heat exchanger design; Multidimensional problems; Particle swarm; Population-based optimization; Pumping cost; Quantum particle swarm optimization; Search spaces; Shell and tube heat exchangers; Swarm Intelligence; Test case; Total costs; Artificial intelligence; Chaos theory; Chaotic systems; Heat exchangers; Investments; Tubes (components); Particle swarm optimization (PSO)",Article,Scopus,2-s2.0-84861098570
"Kim K.-J., Ahn H.","A corporate credit rating model using multi-class support vector machines with an ordinal pairwise partitioning approach",2012,"Computers and Operations Research",43,10.1016/j.cor.2011.06.023,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855202615&doi=10.1016%2fj.cor.2011.06.023&partnerID=40&md5=0e7f5c43f3ca5a0c4f59024d45465626","Predicting corporate credit-rating using statistical and artificial intelligence (AI) techniques has received considerable research attention in the literature. In recent years, multi-class support vector machines (MSVMs) have become a very appealing machine-learning approach due to their good performance. Until now, researchers have proposed a variety of techniques for adapting support vector machines (SVMs) to multi-class classification, since SVMs were originally devised for binary classification. However, most of them have only focused on classifying samples into nominal categories; thus, the unique characteristic of credit-rating ordinality seldom has been considered in the proposed approaches. This study proposes a new type of MSVM classifier (named OMSVM) that is designed to extend the binary SVMs by applying an ordinal pairwise partitioning (OPP) strategy. Our model can efficiently and effectively handle multiple ordinal classes. To validate OMSVM, we applied it to a real-world case of bond rating. We compared the results of our model with those of conventional MSVM approaches and other AI techniques including MDA, MLOGIT, CBR, and ANNs. The results showed that our proposed model improves the performance of classification in comparison to other typical multi-class classification techniques and uses fewer computational resources. © 2011 Elsevier Ltd. All rights reserved.","Corporate credit rating; Multi-class classification; Ordinal pairwise partitioning; Support vector machines","AI techniques; Binary classification; Bond ratings; CBr; Computational resources; Credit ratings; Machine-learning; Multi-class classification; Multi-class support vector machines; Ordinal pairwise partitioning; Support vector; Artificial intelligence; Classifiers; Support vector machines; Vectors; Rating",Article,Scopus,2-s2.0-84855202615
"De Paola A., Gaglio S., Lo Re G., Ortolani M.","Sensor 9k: A testbed for designing and experimenting with WSN-based ambient intelligence applications",2012,"Pervasive and Mobile Computing",43,10.1016/j.pmcj.2011.02.006,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861186505&doi=10.1016%2fj.pmcj.2011.02.006&partnerID=40&md5=cfae8bc0e45f4f3433d54d0705ccda55","Ambient Intelligence systems are typically characterized by the use of pervasive equipment for monitoring and modifying the environment according to users' needs, and to globally defined constraints. Our work describes the implementation of a testbed providing the hardware and software tools for the development and management of AmI applications based on wireless sensor and actuator networks, whose main goal is energy saving for global sustainability. A sample application is presented that addresses temperature control in a work environment, through a multi-objective fuzzy controller taking into account users' preferences and energy consumption. © 2012 Elsevier B.V. All rights reserved.","Ambient intelligence; Energy efficiency; Probabilistic reasoning; Wireless sensor and actuator networks","Ambient intelligence; Ambient intelligence systems; Fuzzy controllers; Global sustainability; Hardware and software; Multi objective; Probabilistic reasoning; Wireless sensor and actuator networks; Work environments; Actuators; Artificial intelligence; Energy efficiency; Energy utilization; Sensors; Testbeds; Wireless sensor networks",Article,Scopus,2-s2.0-84861186505
"Wang S., Watada J.","A hybrid modified PSO approach to VaR-based facility location problems with variable capacity in fuzzy random uncertainty",2012,"Information Sciences",43,10.1016/j.ins.2010.02.014,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857870178&doi=10.1016%2fj.ins.2010.02.014&partnerID=40&md5=7df3690e271c0871a2bca1055eb8b3e6","This paper studies a facility location model with fuzzy random parameters and its swarm intelligence approach. A Value-at-Risk (VaR) based fuzzy random facility location model (VaR-FRFLM) is built in which both the costs and demands are assumed to be fuzzy random variables, and the capacity of each facility is unfixed but a decision variable assuming continuous values. Under this setting, the VaR-FRFLM is inherently a two-stage mixed 0-1 integer fuzzy random programming problem, to which analytical nonlinear programming methods are not applicable. A hybrid modified particle swarm optimization (MPSO) approach is proposed to solve the VaR-FRFLM. In this hybrid mechanism, an approximation algorithm is utilized to compute the fuzzy random VaR objective, a continuous Nbest-Gbest-based PSO and a genotype-phenotype-based binary PSO vehicles are designed to deal with the continuous capacity decisions and the binary location decisions, respectively, and two mutation operators are incorporated into the PSO to further decrease the possibility of becoming trapped in the local optima. A numerical experiment illustrates the application of the proposed hybrid MPSO algorithm and lays out its robustness to the system parameter settings. The comparison shows that the hybrid MPSO exhibits better performance than that when hybrid regular continuous-binary PSO and genetic algorithm (GA) are used to solve the VaR-FRFLM. © 2010 Elsevier Inc. All rights reserved.","Facility location; Fuzzy random variable; Mixed 0-1 integer fuzzy random programming; Particle swarm optimization; Value-at-Risk; Variable capacity","Facility locations; Fuzzy random programming; Fuzzy random variable; Particle swarm; Value at Risk; Variable capacity; Approximation algorithms; Artificial intelligence; Facilities; Genetic algorithms; Integer programming; Particle swarm optimization (PSO); Random variables; Value engineering",Article,Scopus,2-s2.0-84857870178
"Yang X., Zheng X.-Q., Lv L.-N.","A spatiotemporal model of land use change based on ant colony optimization, Markov chain and cellular automata",2012,"Ecological Modelling",43,10.1016/j.ecolmodel.2012.03.011,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860226539&doi=10.1016%2fj.ecolmodel.2012.03.011&partnerID=40&md5=2545f0509f104a5ce85bc917d77b0414","This paper proposes a spatiotemporal model of land use change based on ant colony optimization (ACO), Markov chain and cellular automata (CA). These three methodologies have previously been used separately or in pairs to simulate land use change. In this paper, we apply them in combination, using ant colony optimization and cellular automata to manage the spatial distribution of land use, and applying Markov chain and cellular automata to manage the total amount of land use coverage. We first describe the principle and implementation of the model. Then a land use map of an experimental area (Changping, a district of Beijing) based on land use maps from 1988 and 1998 is simulated for 2008 using the model. By analyzing with real situation, accuracy of the simulation result manifests that the model is useful for land use change simulation. And compared with the other two models (CA-Markov model and ACO-CA model), the model is more appropriate in predicting both the quantity and spatial distribution of land use change in the study area. Therefore the model proposed by this paper is capable of simulating land use change. © 2012 Elsevier B.V..","Ant colony optimization; Cellular automata; Land use change; Markov","Ant Colony Optimization (ACO); Land use maps; Land-use change; Markov; Real situation; Spatio-temporal models; Study areas; Algorithms; Artificial intelligence; Cellular automata; Land use; Markov processes; Spatial distribution; Computer simulation; accuracy assessment; algorithm; cellular automaton; land use change; Markov chain; optimization; spatial distribution; spatiotemporal analysis; Beijing [China]; Changping; China",Article,Scopus,2-s2.0-84860226539
"Liakata M., Saha S., Dobnik S., Batchelor C., Rebholz-schuhmann D.","Automatic recognition of conceptualization zones in scientific articles and two life science applications",2012,"Bioinformatics",43,10.1093/bioinformatics/bts071,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859229402&doi=10.1093%2fbioinformatics%2fbts071&partnerID=40&md5=951ab06bf66a402fe89f358b84e120e6","Motivation: Scholarly biomedical publications report on the findings of a research investigation. Scientists use a well-established discourse structure to relate their work to the state of the art, express their own motivation and hypotheses and report on their methods, results and conclusions. In previous work, we have proposed ways to explicitly annotate the structure of scientific investigations in scholarly publications. Here we present the means to facilitate automatic access to the scientific discourse of articles by automating the recognition of 11 categories at the sentence level, which we call Core Scientific Concepts (CoreSCs). These include: Hypothesis, Motivation, Goal, Object, Background, Method, Experiment, Model, Observation, Result and Conclusion. CoreSCs provide the structure and context to all statements and relations within an article and their automatic recognition can greatly facilitate biomedical information extraction by characterizing the different types of facts, hypotheses and evidence available in a scientific publication.Results: We have trained and compared machine learning classifiers (support vector machines and conditional random fields) on a corpus of 265 full articles in biochemistry and chemistry to automatically recognize CoreSCs. We have evaluated our automatic classifications against a manually annotated gold standard, and have achieved promising accuracies with 'Experiment', 'Background' and 'Model' being the categories with the highest F1-scores (76%, 62% and 53%, respectively). We have analysed the task of CoreSC annotation both from a sentence classification as well as sequence labelling perspective and we present a detailed feature evaluation. The most discriminative features are local sentence features such as unigrams, bigrams and grammatical dependencies while features encoding the document structure, such as section headings, also play an important role for some of the categories. We discuss the usefulness of automatically generated CoreSCs in two biomedical applications as well as work in progress. © The Author(s) 2012. Published by Oxford University Press.",,"algorithm; article; artificial intelligence; automated pattern recognition; classification; computer program; Internet; methodology; publication; support vector machine; Algorithms; Artificial Intelligence; Internet; Pattern Recognition, Automated; Periodicals as Topic; Software; Support Vector Machines",Article,Scopus,2-s2.0-84859229402
"Xia Y.-M., Cheng B., Chen J.-L., Meng X.-W., Liu D.","Optimizing services composition based on improved ant colony algorithm",2012,"Jisuanji Xuebao/Chinese Journal of Computers",43,10.3724/SP.J.1016.2012.00270,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863242413&doi=10.3724%2fSP.J.1016.2012.00270&partnerID=40&md5=4dd97f0df2363ce4e8e525fda7170ddd","In order to optimize services composition, adapt the dynamic and instable characteristics of Web services and the limitation of multi-QoS attributes in the process of services composition, this paper puts forward an algorithm named Multi-pheromone and Dynamically Updating Ant Colony Optimization Algorithm (MPDACO), which includes one global optimizing algorithm and another local optimizing algorithm. The algorithm, which is based on the ACO and composition model that has been built, can fit for such conditions as service invalidation, QoS changing, etc. In addition, the algorithm has improved the ACO strategy on the basis of experiment to make itself be able to converge to optimal solution. In order to verify the feasibility of the above algorithms, this paper makes a simulation experiment on a prototype in tourism, and the results show that the two algorithms are more effective than ACO and the Genetic Algorithm applied to service selection.","Ant colony algorithm; Optimization; Semantic; Service selection; Services composition","Ant colony algorithms; Ant Colony Optimization algorithms; Composition model; Improved ant colony algorithm; Optimal solutions; Optimizing algorithm; Service selection; Services composition; Simulation experiments; Artificial intelligence; Experiments; Optimization; Semantics; Web services; Algorithms",Article,Scopus,2-s2.0-84863242413
"Colton S., Goodwin J., Veale T.","Full-FACE poetry generation",2012,"Proceedings of the 3rd International Conference on Computational Creativity, ICCC 2012",43,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984853573&partnerID=40&md5=a4bc6ee9c415e67199104d036cc1216a","We describe a corpus-based poetry generation system which uses templates to construct poems according to given constraints on rhyme, meter, stress, sentiment, word frequency and word similarity. Moreover, the software constructs a mood for the day by analysing newspaper articles; uses this to determine both an article to base a poem on and a template for the poem; creates an aesthetic based on relevance to the article, lyricism, sentiment and flamboyancy; searches for an instantiation of the template which maximises the aesthetic; and provides a commentary for the whole process to add value to the creative act. We describe the processes behind this approach, present some experimental results which helped in fine tuning, and provide some illustrative poems and commentaries. We argue that this is the first poetry system which generates examples, forms concepts, invents aesthetics and frames its work, and so can be assessed favourably with respect to the FACE model for comparing creative systems.",,"Corpus-based; Creative systems; Face modeling; Generation systems; Software constructs; Whole process; Word frequencies; Word similarity; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84984853573
"Naderloo L., Alimardani R., Omid M., Sarmadian F., Javadikia P., Torabi M.Y., Alimardani F.","Application of ANFIS to predict crop yield based on different energy inputs",2012,"Measurement: Journal of the International Measurement Confederation",43,10.1016/j.measurement.2012.03.025,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975887098&doi=10.1016%2fj.measurement.2012.03.025&partnerID=40&md5=343b20153de2bd8f79d862e292c5f7e6","In this paper, adaptive neuro-fuzzy inference system (ANFIS) was used to predict the grain yield of irrigated wheat in Abyek town of Ghazvin province, Iran. Due to large number of inputs (eight inputs) for ANFIS, the input vector was clustered into two groups and two networks were trained. Inputs for ANFIS 1 were diesel fuel, fertilizer and electricity energies and for ANFIS 2 were human labor, machinery, chemicals, water for irrigation and seed energies. The RMSE and R2 values were found 0.013 and 0.996 for ANFIS 1 and 0.018 and 0.992 for ANFIS 2, respectively. These results showed that ANFIS 1 and ANFIS 2 could well predict the yield. Finally, the predicted values of the two networks were used as inputs to the third ANFIS. The results indicated that the energy inputs in ANFIS 1 have a greater impact on the final yield production than other energy inputs. Also, the RMSE and R2 values for ANFIS 3 were 0.013 and 0.996, respectively. These results showed that ANFIS 1 and the combined network (ANFIS 3) could both predict the grain yield with good accuracy. © 2012 Elsevier Ltd. All rights reserved.","ANFIS; Artificial intelligence; Energy consumption; Wheat; Yield","Agriculture; Artificial intelligence; Diesel fuels; Energy utilization; Grain (agricultural product); Machinery; Adaptive neuro-fuzzy inference system; ANFIS; Crop yield; Energy inputs; Grain yield; Human labor; Input vector; Seed energy; Wheat; Yield; Yield production; Forecasting",Article,Scopus,2-s2.0-84975887098
"Vellido A., Martín-Guerrero J.D., Lisboa P.J.","Making machine learning models interpretable",2012,"ESANN 2012 proceedings, 20th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning",43,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947754719&partnerID=40&md5=71beb9ee6e20b8c2c8797eeeeb401d17","Data of different levels of complexity and of ever growing diversity of characteristics are the raw materials that machine learning practitioners try to model using their wide palette of methods and tools. The obtained models are meant to be a synthetic representation of the available, observed data that captures some of their intrinsic regularities or patterns. Therefore, the use of machine learning techniques for data analysis can be understood as a problem of pattern recognition or, more informally, of knowledge discovery and data mining. There exists a gap, though, between data modeling and knowledge extraction. Models, depending on the machine learning techniques employed, can be described in diverse ways but, in order to consider that some knowledge has been achieved from their description, we must take into account the human cognitive factor that any knowledge extraction process entails. These models as such can be rendered powerless unless they can be interpreted, and the process of human interpretation follows rules that go well beyond technical prowess. For this reason, interpretability is a paramount quality that machine learning methods should aim to achieve if they are to be applied in practice. This paper is a brief introduction to the special session on interpretable models in machine learning, organized as part of the 20th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning. It includes a discussion on the several works accepted for the session, with an overview of the context of wider research on interpretability of machine learning models. © 2012, i6doc.com publication. All rights reserved.",,"Artificial intelligence; Complex networks; Data mining; Extraction; Information analysis; Learning algorithms; Neural networks; Pattern recognition; Cognitive factors; Interpretability; Knowledge discovery and data minings; Knowledge extraction; Machine learning methods; Machine learning models; Machine learning techniques; Synthetic representations; Learning systems",Conference Paper,Scopus,2-s2.0-84947754719
"Buckland M.","What kind of science can information science be?",2012,"Journal of the American Society for Information Science and Technology",43,10.1002/asi.21656,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-83655201309&doi=10.1002%2fasi.21656&partnerID=40&md5=51b26a9d31fcdc9c9c6ea524a6ed875c","During the 20th century there was a strong desire to develop an information science from librarianship, bibliography, and documentation and in 1968 the American Documentation Institute changed its name to the American Society for Information Science. By the beginning of the 21st century, however, departments of (library and) information science had turned instead towards the social sciences. These programs address a variety of important topics, but they have been less successful in providing a coherent explanation of the nature and scope of the field. Progress can be made towards a coherent, unified view of the roles of archives, libraries, museums, online information services, and related organizations if they are treated as information-providing services. However, such an approach seems significantly incomplete on ordinary understandings of the providing of information. Instead of asking what information science is or what we might wish it to become, we ask instead what kind of field it can be given our assumptions about it. We approach the question by examining some keywords: science, information, knowledge, and interdisciplinary. We conclude that if information science is concerned with what people know, then it is a form of cultural engagement, and at most, a science of the artificial. © 2011 ASIS&T.",,"20th century; Online information services; Artificial intelligence; Software engineering; Information services",Article,Scopus,2-s2.0-83655201309
"Ding Q., Hu X., Sun L., Wang Y.","An improved ant colony optimization and its application to vehicle routing problem with time windows",2012,"Neurocomputing",42,10.1016/j.neucom.2011.09.040,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867196374&doi=10.1016%2fj.neucom.2011.09.040&partnerID=40&md5=8284b3041c11ed18a68f7237188a6118","The ant colony optimization (ACO), inspired from the foraging behavior of ant species, is a swarm intelligence algorithm for solving hard combinatorial optimization problems. The algorithm, however, has the weaknesses of premature convergence and low search speed, which greatly hinder its application. To improve the performance of the algorithm, a hybrid ant colony optimization (HACO) is presented in this paper. By adjusting pheromone approach and introducing a disaster operator, the HACO prevents the search process from getting trapped in the local optimal solution. Then, by taking the candidate list into consideration and combining the ACO with the saving algorithm and λ-interchange mechanism, the HACO improves the convergence speed. Furthermore, this paper gives a guideline on how to adjust the parameters to achieve the good performance of the algorithm. Finally, the HACO is applied to solve the vehicle routing problem with time windows. The effectiveness of the HACO on solving combinatorial optimization problems is validated by comparing the computational results with those previously presented in the literature. © 2012 Elsevier B.V.","Ant colony optimization; Combinatorial optimization problems; Vehicle routing problem with time windows","Ant Colony Optimization (ACO); Ant species; Candidate list; Combinatorial optimization problems; Computational results; Convergence speed; Foraging behaviors; Hybrid ant colony optimization; Improved ant colony optimization; Interchange mechanism; Local optimal solution; Premature convergence; Saving algorithm; Search process; Search speed; Swarm Intelligence; Vehicle routing problem with time windows; Artificial intelligence; Combinatorial optimization; Algorithms; pheromone; algorithm; analytical parameters; article; comparative study; computer analysis; controlled study; foraging behavior; hybrid ant colony optimization; mathematical model; nonhuman; performance; priority journal; traffic; validation study",Article,Scopus,2-s2.0-84867196374
"Wang L., Chen T.","Multistability of neural networks with mexican-hat-type activation functions",2012,"IEEE Transactions on Neural Networks and Learning Systems",42,10.1109/TNNLS.2012.2210732,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876126652&doi=10.1109%2fTNNLS.2012.2210732&partnerID=40&md5=fcbd2d4c0504daf533711ca3527860cd","In this paper, we are concerned with a class of neural networks with Mexican-hat-type activation functions. Due to the different structure from neural networks with saturated activation functions, a set of new sufficient conditions are presented to study the multistability, including the total number of equilibrium points, their locations, and stability. Furthermore, the attraction basins of stable equilibrium points are investigated for two-neuron neural networks. The investigation shows that the stable manifolds of unstable equilibrium points constitute the boundaries of attraction basins of stable equilibrium points. Several illustrative examples are given to verify the effectiveness of our results. © 2012 IEEE.","Attraction basin; Mexican-hat activation; multistability; neural networks; stability analysis","Activation functions; Attraction basin; Different structure; Multi stabilities; Stability analysis; Stable equilibrium points; Sufficient conditions; Unstable equilibrium points; Artificial intelligence; Computer networks; Neural networks; algorithm; artificial intelligence; artificial neural network; computer simulation; human; nonlinear system; time; Algorithms; Artificial Intelligence; Computer Simulation; Humans; Neural Networks (Computer); Nonlinear Dynamics; Time Factors",Article,Scopus,2-s2.0-84876126652
"Dambre J., Verstraeten D., Schrauwen B., Massar S.","Information processing capacity of dynamical systems",2012,"Scientific Reports",42,10.1038/srep00514,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864147615&doi=10.1038%2fsrep00514&partnerID=40&md5=000cd20e6cbff7a59cecd2761045e8fb","Many dynamical systems, both natural and artificial, are stimulated by time dependent external signals, somehow processing the information contained therein. We demonstrate how to quantify the different modes in which information can be processed by such systems and combine them to define the computational capacity of a dynamical system. This is bounded by the number of linearly independent state variables of the dynamical system, equaling it if the system obeys the fading memory condition. It can be interpreted as the total number of linearly independent functions of its stimuli the system can compute. Our theory combines concepts from machine learning (reservoir computing), system modeling, stochastic processes, and functional analysis. We illustrate our theory by numerical simulations for the logistic map, a recurrent neural network, and a two-dimensional reaction diffusion system, uncovering universal trade-offs between the non-linearity of the computation and the system's short-term memory.",,"article; artificial intelligence; computer simulation; information processing; theoretical model; Artificial Intelligence; Automatic Data Processing; Computer Simulation; Models, Theoretical",Article,Scopus,2-s2.0-84864147615
"Guo G., Zhang J., Thalmann D.","A simple but effective method to incorporate trusted neighbors in recommender systems",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",42,10.1007/978-3-642-31454-4_10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863609971&doi=10.1007%2f978-3-642-31454-4_10&partnerID=40&md5=50ad83dd35337779cef0dab09956617d","Providing high quality recommendations is important for online systems to assist users who face a vast number of choices in making effective selection decisions. Collaborative filtering is a widely accepted technique to provide recommendations based on ratings of similar users. But it suffers from several issues like data sparsity and cold start. To address these issues, in this paper, we propose a simple but effective method, namely ""Merge"", to incorporate social trust information (i.e. trusted neighbors explicitly specified by users) in providing recommendations. More specifically, ratings of a user's trusted neighbors are merged to represent the preference of the user and to find similar other users for generating recommendations. Experimental results based on three real data sets demonstrate that our method is more effective than other approaches, both in accuracy and coverage of recommendations. © 2012 Springer-Verlag.",,"Cold start; Collaborative filtering; Data sparsity; High quality; Real data sets; Selection decisions; Artificial intelligence; Mathematical models",Conference Paper,Scopus,2-s2.0-84863609971
"Kong G., Xu D.-L., Body R., Yang J.-B., MacKway-Jones K., Carley S.","A belief rule-based decision support system for clinical risk assessment of cardiac chest pain",2012,"European Journal of Operational Research",42,10.1016/j.ejor.2011.10.044,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857922232&doi=10.1016%2fj.ejor.2011.10.044&partnerID=40&md5=5bc4633d3ef68d74b8ad2d702525cc0f","This paper describes a prototype clinical decision support system (CDSS) for risk stratification of patients with cardiac chest pain. A newly developed belief rule-based inference methodology-RIMER was employed for developing the prototype. Based on the belief rule-based inference methodology, the prototype CDSS can deal with uncertainties in both clinical domain knowledge and clinical data. Moreover, the prototype can automatically update its knowledge base via a belief rule base (BRB) learning module which can adjust BRB through accumulated historical clinical cases. The domain specific knowledge used to construct the knowledge base of the prototype was learned from real patient data. We simulated a set of 1000 patients in cardiac chest pain to validate the prototype. The belief rule-based prototype CDSS has been found to perform extremely well. Firstly, the system can provide more reliable and informative diagnosis recommendations than manual diagnosis using traditional rules when there are clinical uncertainties. Secondly, the diagnostic performance of the system can be significantly improved after training the BRB through accumulated clinical cases. © 2011 Elsevier B.V. All rights reserved.","Belief rule base; Clinical risk assessment; Decision support systems; Evidential reasoning approach; OR in medicine; Uncertainty modeling","Belief rule base; Chest pain; Clinical data; Clinical decision support systems; Decision supports; Diagnostic performance; Domain knowledge; Domain-specific knowledge; Evidential reasoning approaches; Knowledge base; Learning modules; Patient data; Risk stratification; Rule-based decision support system; Rule-based inference; Rule-based prototype; Uncertainty modeling; Artificial intelligence; Decision support systems; Diagnosis; Hospital data processing; Knowledge based systems; Risk assessment; Health",Article,Scopus,2-s2.0-84857922232
"Andre B., Vercauteren T., Buchner A.M., Wallace M.B., Ayache N.","Learning semantic and visual similarity for endomicroscopy video retrieval",2012,"IEEE Transactions on Medical Imaging",42,10.1109/TMI.2012.2188301,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861873452&doi=10.1109%2fTMI.2012.2188301&partnerID=40&md5=add65083369713096a8484a6ab765589","Content-based image retrieval (CBIR) is a valuable computer vision technique which is increasingly being applied in the medical community for diagnosis support. However, traditional CBIR systems only deliver visual outputs, i.e., images having a similar appearance to the query, which is not directly interpretable by the physicians. Our objective is to provide a system for endomicroscopy video retrieval which delivers both visual and semantic outputs that are consistent with each other. In a previous study, we developed an adapted bag-of-visual-words method for endomicroscopy retrieval, called Dense-Sift, that computes a visual signature for each video. In this paper, we present a novel approach to complement visual similarity learning with semantic knowledge extraction, in the field of in vivo endomicroscopy. We first leverage a semantic ground truth based on eight binary concepts, in order to transform these visual signatures into semantic signatures that reflect how much the presence of each semantic concept is expressed by the visual words describing the videos. Using cross-validation, we demonstrate that, in terms of semantic detection, our intuitive Fisher-based method transforming visual-word histograms into semantic estimations outperforms support vector machine (SVM) methods with statistical significance. In a second step, we propose to improve retrieval relevance by learning an adjusted similarity distance from a perceived similarity ground truth. As a result, our distance learning method allows to statistically improve the correlation with the perceived similarity. We also demonstrate that, in terms of perceived similarity, the recall performance of the semantic signatures is close to that of visual signatures and significantly better than those of several state-of-the-art CBIR methods. The semantic signatures are thus able to communicate high-level medical knowledge while being consistent with the low-level visual signatures and much shorter than them. In our resulting retrieval system, we decide to use visual signatures for perceived similarity learning and retrieval, and semantic signatures for the output of an additional information, expressed in the endoscopist own language, which provides a relevant semantic translation of the visual retrieval outputs. © 2012 IEEE.","Bag-of-visual-words (BoW); content-based image retrieval (CBIR); endomicroscopy; semantic and visual similarity; semantic gap; similarity learning","Bag-of-visual-words; Content-Based Image Retrieval; endomicroscopy; Semantic gap; similarity learning; Visual similarity; Computer vision; Content based retrieval; Diagnosis; Distance education; Semantics; Support vector machines; Visual languages; algorithm; article; artificial intelligence; automated pattern recognition; capsule endoscopy; colon tumor; computer assisted diagnosis; hospital information system; human; image enhancement; image subtraction; information retrieval; methodology; pathology; reproducibility; semantics; sensitivity and specificity; video microscopy; Algorithms; Artificial Intelligence; Capsule Endoscopy; Colonic Neoplasms; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Information Storage and Retrieval; Microscopy, Video; Pattern Recognition, Automated; Radiology Information Systems; Reproducibility of Results; Semantics; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-84861873452
"Przybył A., Cpałka K.","A new method to construct of interpretable models of dynamic systems",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",42,10.1007/978-3-642-29350-4-82,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861044866&doi=10.1007%2f978-3-642-29350-4-82&partnerID=40&md5=7afbb831d005dc726523af256e9e4fb1","The paper presents a new method to create model of nonlinear dynamic systems which gives a real opportunity for the interpretation of accumulated knowledge. By combining methods of control theory with fuzzy logic rules a good accuracy of the model can be achieved with use of a small number of fuzzy rules. © 2012 Springer-Verlag Berlin Heidelberg.",,"Combining method; Fuzzy logic rules; Artificial intelligence; Fuzzy logic; Nonlinear dynamical systems; Soft computing; Dynamical systems",Conference Paper,Scopus,2-s2.0-84861044866
"Zalasiński M., Cpałka K.","Novel algorithm for the on-line signature verification",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",42,10.1007/978-3-642-29350-4-44,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861079907&doi=10.1007%2f978-3-642-29350-4-44&partnerID=40&md5=980fef98bf00da49b592ec0d548baaf6","On-line signature is a biometric attribute used in a identity verification process. One of the most effective methods of signature verification is the method based on partitioning of signature trajectories. In this paper a concept of new approach to identity verification based on partitioning of trajectories is presented. In this approach signature is partitioned into subspaces which are weighted by weights of importance. The weights are used in classification process. Partitions associated with high values of weight have greater importance in classification process than partitions associated with low weight values. The algorithm was tested with use of public on-line signature database SVC 2004. © 2012 Springer-Verlag Berlin Heidelberg.",,"Classification process; Identity verification; Novel algorithm; On-line signature verification; Online signature; Signature verification; Weight values; Algorithms; Artificial intelligence; Biometrics; Soft computing; Cryptography",Conference Paper,Scopus,2-s2.0-84861079907
"Mandal S.K., Chan F.T.S., Tiwari M.K.","Leak detection of pipeline: An integrated approach of rough set theory and artificial bee colony trained SVM",2012,"Expert Systems with Applications",42,10.1016/j.eswa.2011.08.170,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80255124873&doi=10.1016%2fj.eswa.2011.08.170&partnerID=40&md5=e07473d3259d970c7334bec6283ca32c","The generation of leak along the pipeline carrying crude oils and liquid fuels results enormous financial loss to the industry and also affects the public health. Hence, the leak detection and localization problem has always been a major concern for the companies. In spite of the various techniques developed, accuracy and time involved in the prediction is still a matter of concern. In this paper, a novel leak detection scheme based on rough set theory and support vector machine (SVM) is proposed to overcome the problem of false leak detection. In this approach, 'rough set theory' is explored to reduce the length of experimental data as well as generate rules. It is embedded to enhance the decision making process. Further, SVM classifier is employed to inspect the cases that could not be detected by applied rules. For the computational training of SVM, this paper uses swarm intelligence technique: artificial bee colony (ABC) algorithm, which imitates intelligent food searching behavior of honey bees. The results of proposed leak detection scheme with ABC are compared with those obtained by using particle swarm optimization (PSO) and one of its variants, so-called enhanced particle swarm optimization (EPSO). The experimental results advocate the use of propounded method for detecting leaks with maximum accuracy. © 2011 Elsevier Ltd. All rights reserved.","Artificial bee colony (ABC) algorithm; Leak detection; Rough set theory; Rule generation; Support vector machine (SVM)","Artificial bee colonies; Artificial bee colony (ABC) algorithm; Decision making process; Detection and localization; Detection scheme; Enhanced particle swarm optimization; Experimental data; Financial loss; Food searching; Honey bee; Integrated approach; Public health; Rule generation; Support vector; SVM classifiers; Swarm intelligence techniques; Algorithms; Artificial intelligence; Cellular automata; Decision making; Leak detection; Losses; Particle swarm optimization (PSO); Support vector machines; Rough set theory",Article,Scopus,2-s2.0-80255124873
"Zhang L., Wang L., Lin W.","Generalized biased discriminant analysis for content-based image retrieval",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",42,10.1109/TSMCB.2011.2165335,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856275694&doi=10.1109%2fTSMCB.2011.2165335&partnerID=40&md5=85bc32c1a57c4e7ff470c20ccc4f9bb8","Biased discriminant analysis (BDA) is one of the most promising relevance feedback (RF) approaches to deal with the feedback sample imbalance problem for content-based image retrieval (CBIR). However, the singular problem of the positive within-class scatter and the Gaussian distribution assumption for positive samples are two main obstacles impeding the performance of BDA RF for CBIR. To avoid both of these intrinsic problems in BDA, in this paper, we propose a novel algorithm called generalized BDA (GBDA) for CBIR. The GBDA algorithm avoids the singular problem by adopting the differential scatter discriminant criterion (DSDC) and handles the Gaussian distribution assumption by redesigning the between-class scatter with a nearest neighbor approach. To alleviate the overfitting problem, GBDA integrates the locality preserving principle; therefore, a smooth and locally consistent transform can also be learned. Extensive experiments show that GBDA can substantially outperform the original BDA, its variations, and related support-vector-machine-based RF algorithms. © 2011 IEEE.","Biased discriminant analysis (BDA); content-based image retrieval (CBIR); differential scatter discriminant criterion (DSDC); relevance feedback (RF)","Biased discriminant analysis; Content-Based Image Retrieval; differential scatter discriminant criterion (DSDC); Discriminant criteria; Imbalance problem; Nearest-neighbor approaches; Novel algorithm; Over fitting problem; Relevance feedback; relevance feedback (RF); Singular problem; Algorithms; Content based retrieval; Discriminant analysis; Gaussian distribution; Feedback; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; data mining; discriminant analysis; hospital information system; image subtraction; methodology; Algorithms; Artificial Intelligence; Data Mining; Discriminant Analysis; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Radiology Information Systems; Subtraction Technique",Article,Scopus,2-s2.0-84856275694
"Jiménez-Ruiz E., Grau B.C., Zhou Y., Horrocks I.","Large-scale interactive ontology matching: Algorithms and implementation",2012,"Frontiers in Artificial Intelligence and Applications",42,10.3233/978-1-61499-098-7-444,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878794297&doi=10.3233%2f978-1-61499-098-7-444&partnerID=40&md5=ed8093072fbe548783403ad603f367b6","In this paper we present the ontology matching system LogMap 2, a much improved version of its predecessor LogMap. LogMap 2 supports user interaction during the matching process, which is essential for use cases requiring very accurate mappings. Interactivity, however, imposes very strict scalability requirements; we are able to satisfy these requirements by providing real-time user response even for large-scale ontologies. Finally, LogMap 2 implements scalable reasoning and diagnosis algorithms, which minimise any logical inconsistencies introduced by the matching process. © 2012 The Author(s).",,"Algorithms; Artificial intelligence; Accurate mapping; Diagnosis algorithms; Interactivity; Large-scale ontologies; Matching process; Ontology matching; Scalable reasonings; User interaction; Ontology",Conference Paper,Scopus,2-s2.0-84878794297
"Chen S.-M., Sarosh A., Dong Y.-F.","Simulated annealing based artificial bee colony algorithm for global numerical optimization",2012,"Applied Mathematics and Computation",41,10.1016/j.amc.2012.09.052,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870056206&doi=10.1016%2fj.amc.2012.09.052&partnerID=40&md5=9dc23c785de6def0b587c701d326be24","Artificial bee colony (ABC) algorithm is a global optimization algorithm, which has been shown to be competitive with some conventional swarm algorithm, such as genetic algorithm (GA) and particle swarm optimization (PSO). However, there is still an insufficiency in ABC algorithm, in that it has poor convergence rate in some situations. Inspired by simulated annealing algorithm, a simulated annealing based ABC algorithm (SAABC) is proposed. Simulated annealing algorithm is introduced into employed bees search process to improve the exploitation of the algorithm. The experimental results are tested on a set of numerical benchmark functions with different dimensions. That show that SAABC algorithm can outperform ABC and global best guided ABC algorithms in most of the experiments. © 2012 Elsevier Inc. All rights reserved.","Artificial bee colony; Global best guided ABC; Global numerical optimization; Optimization; Simulated annealing algorithm; Swarm intelligence","Artificial bee colonies; Global best guided ABC; Numerical optimizations; Simulated annealing algorithms; Swarm Intelligence; Artificial intelligence; Genetic algorithms; Global optimization; Optimization; Particle swarm optimization (PSO); Simulated annealing",Article,Scopus,2-s2.0-84870056206
"Churchill D., Saffidine A., Buro M.","Fast heuristic search for RTS game combat scenarios",2012,"Proceedings of the 8th AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, AIIDE 2012",41,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883115335&partnerID=40&md5=e2ecde651dc7e1d02196acda275c8216","Heuristic search has been very successful in abstract game domains such as Chess and Go. In video games, however, adoption has been slow due to the fact that state and move spaces are much larger, real-time constraints are harsher, and constraints on computational resources are tighter. In this paper we present a fast search method-Alpha-Beta search for durative moves - that can defeat commonly used AI scripts in RTS game combat scenarios of up to 8 vs. 8 units running on a single core in under 5ms per search episode. This performance is achieved by using standard search enhancements such as transposition tables and iterative deepening, and novel usage of combat AI scripts for sorting moves and state evaluation via playouts. We also present evidence that commonly used combat scripts are highly exploitable - opening the door for a promising line of research on opponent combat modelling. Copyright © 2012, Association for the Advancement of Artificial Intelligence.",,"Abstract games; Computational resources; Fast search; Heuristic search; Iterative deepening; Real time constraints; State evaluation; Transposition table; Artificial intelligence; Computer software; Heuristic algorithms; Iterative methods; Modular robots; Human computer interaction",Conference Paper,Scopus,2-s2.0-84883115335
"Nie F., Huang H., Ding C.","Low-rank matrix recovery via efficient schatten p-norm minimization",2012,"Proceedings of the National Conference on Artificial Intelligence",41,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868279038&partnerID=40&md5=7db5c53710419b39c15a2e4f53d0ef89","As an emerging machine learning and information retrieval technique, the matrix completion has been successfully applied to solve many scientific applications, such as collaborative prediction in information retrieval, video completion in computer vision, etc. The matrix completion is to recover a low-rank matrix with a fraction of its entries arbitrarily corrupted. Instead of solving the popularly used trace norm or nuclear norm based objective, we directly minimize the original formulations of trace norm and rank norm. We propose a novel Schatten p-Norm optimization framework that unifies different norm formulations. An efficient algorithm is derived to solve the new objective and followed by the rigorous theoretical proof on the convergence. The previous main solution strategy for this problem requires computing singular value decompositions - a task that requires increasingly cost as matrix sizes and rank increase. Our algorithm has closed form solution in each iteration, hence it converges fast. As a consequence, our algorithm has the capacity of solving large-scale matrix completion problems. Empirical studies on the recommendation system data sets demonstrate the promising performance of our new optimization framework and efficient algorithm. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Closed form solutions; Data sets; Empirical studies; Low-rank matrices; Matrix completion; Matrix completion problems; Matrix size; Optimization framework; P-norm minimization; Scientific applications; Solution strategy; Trace-norms; Video completion; Algorithms; Artificial intelligence; Computer vision; Information retrieval; Optimization; Iterative methods",Conference Paper,Scopus,2-s2.0-84868279038
"Kuhlmann M., Gogolla M.","From UML and OCL to relational logic and back",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",41,10.1007/978-3-642-33666-9_27,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867650633&doi=10.1007%2f978-3-642-33666-9_27&partnerID=40&md5=58c4bc2f4b8724d51fb08d74d06ef36f","Languages like UML and OCL are used to precisely model systems. Complex UML and OCL models therefore represent a crucial part of model-driven development, as they formally specify the main system properties. Consequently, creating complete and correct models is a critical concern. For this purpose, we provide a lightweight model validation method based on efficient SAT solving techniques. In this paper, we present a transformation from UML class diagram and OCL concepts into relational logic. Relational logic in turn represents the source for advanced SAT-based model instance finders like Kodkod. This paper focuses on a natural transformation approach which aims to exploit the features of relational logic as directly as possible through straitening the handling of main UML and OCL features. This approach allows us to explicitly benefit from the efficient handling of relational logic in Kodkod and to interpret found results backwards in terms of UML and OCL. © 2012 Springer-Verlag.",,"Model driven development; Model system; Model validation; Natural transformations; Relational logic; SAT-solving; System property; UML class diagrams; Artificial intelligence; Models",Conference Paper,Scopus,2-s2.0-84867650633
"Nguyen L.T.T., Vo B., Hong T.-P., Thanh H.C.","Classification based on association rules: A lattice-based approach",2012,"Expert Systems with Applications",41,10.1016/j.eswa.2012.03.036,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861339184&doi=10.1016%2fj.eswa.2012.03.036&partnerID=40&md5=ab248310d6a6c808c7578d3efec18197","Classification plays an important role in decision support systems. A lot of methods for mining classification rules have been developed in recent years, such as C4.5 and ILA. These methods are, however, based on heuristics and greedy approaches to generate rule sets that are either too general or too overfitting for a given dataset. They thus often yield high error ratios. Recently, a new method for classification from data mining, called the Classification Based on Associations (CBA), has been proposed for mining class-association rules (CARs). This method has more advantages than the heuristic and greedy methods in that the former could easily remove noise, and the accuracy is thus higher. It can additionally generate a rule set that is more complete than C4.5 and ILA. One of the weaknesses of mining CARs is that it consumes more time than C4.5 and ILA because it has to check its generated rule with the set of the other rules. We thus propose an efficient pruning approach to build a classifier quickly. Firstly, we design a lattice structure and propose an algorithm for fast mining CARs using this lattice. Secondly, we develop some theorems and propose an algorithm for pruning redundant rules quickly based on these theorems. Experimental results also show that the proposed approach is more efficient than those used previously. © 2012 Elsevier Ltd. All rights reserved.","Class association rules; Classifier; Data mining; Lattice; Rule pruning","Classification based on association rules; Classification based on associations; Data sets; Error ratio; Greedy method; Lattice; Lattice structures; Mining classification rules; Overfitting; Redundant rules; REmove noise; Rule pruning; Rule set; Algorithms; Artificial intelligence; Association rules; Classifiers; Data mining; Decision support systems; Heuristic methods",Article,Scopus,2-s2.0-84861339184
"Mezura-Montes E., Cetina-Domínguez O.","Empirical analysis of a modified Artificial Bee Colony for constrained numerical optimization",2012,"Applied Mathematics and Computation",41,10.1016/j.amc.2012.04.057,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862850206&doi=10.1016%2fj.amc.2012.04.057&partnerID=40&md5=27056162c410e1745ed27c90145f755b","A modified Artificial Bee Colony algorithm to solve constrained numerical optimization problems is presented in this paper. Four modifications related with the selection mechanism, the scout bee operator, and the equality and boundary constraints are made to the algorithm with the aim to modify its behavior in a constrained search space. Six performance measures found in the specialized literature are employed to analyze different capabilities in the proposed algorithm such as the ability and cost to generate feasible solutions, the capacity and cost to locate the feasible global optimum solution and the competency to improve feasible solutions. Three experiments, including a comparison with state-of-the-art algorithms, are considered in the test design where twenty four well-known benchmark problems with different features are utilized. The overall results show that the proposed algorithm differs in its behavior with respect to the original Artificial Bee Colony algorithm but its performance is improved, mostly in problems with small feasible regions due to the presence of equality constraints. Crown Copyright © 2012 Published by Elsevier Inc. All rights reserved.","Constraint-handling; Evolutionary algorithms; Global optimization; Swarm Intelligence","Artificial bee colonies; Artificial bee colony algorithms; Bench-mark problems; Boundary constraints; Constraint handling; Empirical analysis; Equality constraints; Feasible regions; Feasible solution; Global optimum solutions; Numerical optimizations; Performance measure; Search spaces; Selection mechanism; State-of-the-art algorithms; Swarm Intelligence; Test designs; Artificial intelligence; Constrained optimization; Global optimization; Evolutionary algorithms",Article,Scopus,2-s2.0-84862850206
"Song M., Tao D., Huang X., Chen C., Bu J.","Three-dimensional face reconstruction from a single image by a coupled RBF network",2012,"IEEE Transactions on Image Processing",41,10.1109/TIP.2012.2183882,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860110906&doi=10.1109%2fTIP.2012.2183882&partnerID=40&md5=a5594e1af61b1744009264b615a49089","Reconstruction of a 3-D face model from a single 2-D face image is fundamentally important for face recognition and animation because the 3-D face model is invariant to changes of viewpoint, illumination, background clutter, and occlusions. Given a coupled training set that contains pairs of 2-D faces and the corresponding 3-D faces, we train a novel coupled radial basis function network (C-RBF) to recover the 3-D face model from a single 2-D face image. The C-RBF network explores: 1) the intrinsic representations of 3-D face models and those of 2-D face images; 2) mappings between a 3-D face model and its intrinsic representation; and 3) mappings between a 2-D face image and its intrinsic representation. Since a particular face can be reconstructed by its nearest neighbors, we can assume that the linear combination coefficients for a particular 2-D face image reconstruction are identical to those for the corresponding 3-D face model reconstruction. Therefore, we can reconstruct a 3-D face model by using a single 2-D face image based on the C-RBF network. Extensive experimental results on the BU3D database indicate the effectiveness of the proposed C-RBF network for recovering the 3-D face model from a single 2-D face image. © 1992-2012 IEEE.","3-D face reconstruction; Coupled RBF network; single image","3D faces; Background clutter; Face image reconstruction; Face images; Face reconstruction; Linear combination coefficients; Nearest neighbors; Single images; Training sets; Animation; C (programming language); Face recognition; Image reconstruction; Radial basis function networks; Three dimensional; algorithm; article; artificial intelligence; automated pattern recognition; biometry; computer assisted diagnosis; face; histology; human; image enhancement; methodology; reproducibility; sensitivity and specificity; Algorithms; Artificial Intelligence; Biometry; Face; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84860110906
"Tahmasebi P., Hezarkhani A.","A hybrid neural networks-fuzzy logic-genetic algorithm for grade estimation",2012,"Computers and Geosciences",41,10.1016/j.cageo.2012.02.004,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857929384&doi=10.1016%2fj.cageo.2012.02.004&partnerID=40&md5=6af883c1dac59c37d4c9ec9e19ce7a52","The grade estimation is a quite important and money/time-consuming stage in a mine project, which is considered as a challenge for the geologists and mining engineers due to the structural complexities in mineral ore deposits. To overcome this problem, several artificial intelligence techniques such as Artificial Neural Networks (ANN) and Fuzzy Logic (FL) have recently been employed with various architectures and properties. However, due to the constraints of both methods, they yield the desired results only under the specific circumstances. As an example, one major problem in FL is the difficulty of constructing the membership functions (MFs).Other problems such as architecture and local minima could also be located in ANN designing. Therefore, a new methodology is presented in this paper for grade estimation. This method which is based on ANN and FL is called ""Coactive Neuro-Fuzzy Inference System"" (CANFIS) which combines two approaches, ANN and FL. The combination of these two artificial intelligence approaches is achieved via the verbal and numerical power of intelligent systems. To improve the performance of this system, a Genetic Algorithm (GA) - as a well-known technique to solve the complex optimization problems - is also employed to optimize the network parameters including learning rate, momentum of the network and the number of MFs for each input. A comparison of these techniques (ANN, Adaptive Neuro-Fuzzy Inference System or ANFIS) with this new method (CANFIS-GA) is also carried out through a case study in Sungun copper deposit, located in East-Azerbaijan, Iran. The results show that CANFIS-GA could be a faster and more accurate alternative to the existing time-consuming methodologies for ore grade estimation and that is, therefore, suggested to be applied for grade estimation in similar problems. © 2012.","Artificial neural networks; Coactive neuro-fuzzy inference system (CANFIS).; Genetic algorithm; Grade estimation; Parallel optimization","Adaptive neuro-fuzzy inference system; Artificial intelligence techniques; Complex optimization; Learning rates; Local minimums; Mining engineers; Network parameters; Neuro-fuzzy inference systems; Ore grades; Parallel optimization; Structural complexity; Copper deposits; Estimation; Fuzzy logic; Fuzzy systems; Intelligent systems; Network architecture; Neural networks; Optimization; Genetic algorithms; artificial intelligence; artificial neural network; copper; estimation method; fuzzy mathematics; genetic algorithm; mineral deposit; mining; optimization; ore deposit; ore grade; East Azerbaijan; Iran",Article,Scopus,2-s2.0-84857929384
"Kratz L., Nishino K.","Tracking pedestrians using local spatio-temporal motion patterns in extremely crowded scenes",2012,"IEEE Transactions on Pattern Analysis and Machine Intelligence",41,10.1109/TPAMI.2011.173,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859198469&doi=10.1109%2fTPAMI.2011.173&partnerID=40&md5=08e970bd1317c1013dfad11b4d1e82b9","Tracking pedestrians is a vital component of many computer vision applications, including surveillance, scene understanding, and behavior analysis. Videos of crowded scenes present significant challenges to tracking due to the large number of pedestrians and the frequent partial occlusions that they produce. The movement of each pedestrian, however, contributes to the overall crowd motion (i.e., the collective motions of the scene's constituents over the entire video) that exhibits an underlying spatially and temporally varying structured pattern. In this paper, we present a novel Bayesian framework for tracking pedestrians in videos of crowded scenes using a space-time model of the crowd motion. We represent the crowd motion with a collection of hidden Markov models trained on local spatio-temporal motion patterns, i.e., the motion patterns exhibited by pedestrians as they move through local space-time regions of the video. Using this unique representation, we predict the next local spatio-temporal motion pattern a tracked pedestrian will exhibit based on the observed frames of the video. We then use this prediction as a prior for tracking the movement of an individual in videos of extremely crowded scenes. We show that our approach of leveraging the crowd motion enables tracking in videos of complex scenes that present unique difficulty to other approaches. © 2012 IEEE.","crowded scenes; hidden Markov models; spatio-temporal motion patterns; Tracking; video analysis","Bayesian frameworks; Behavior analysis; Collective motions; Complex scenes; Computer vision applications; crowded scenes; Hidden markov models (HMMs); Motion pattern; Partial occlusions; Scene understanding; Space-time model; Space-time region; Spatio-temporal; Structured patterns; Video analysis; Computer vision; Hidden Markov models; Surface discharges; Time and motion study; algorithm; article; artificial intelligence; automated pattern recognition; Bayes theorem; crowding; human; image processing; methodology; probability; videorecording; walking; Algorithms; Artificial Intelligence; Bayes Theorem; Crowding; Humans; Image Processing, Computer-Assisted; Markov Chains; Pattern Recognition, Automated; Video Recording; Walking",Article,Scopus,2-s2.0-84859198469
"Jeng D.J.-F., Tzeng G.-H.","Social influence on the use of Clinical Decision Support Systems: Revisiting the Unified Theory of Acceptance and Use of Technology by the fuzzy DEMATEL technique",2012,"Computers and Industrial Engineering",41,10.1016/j.cie.2011.12.016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858073630&doi=10.1016%2fj.cie.2011.12.016&partnerID=40&md5=abf55ec09afc0801b02bf130fdd7ffed","The aim of study is to examine whether social influence affects medical professionals' behavioral intention to use while introducing a new Clinical Decision Support System (CDSS). The series of Technology Acceptance Models (TAMs) have been widely applied to examine new technology acceptance by scholars; nevertheless, these models omit system diversity and the user's profession. On the other hand, causal analysis greatly affects the efficiency of decision-making, and it is usually analyzed by Structural Equation Modeling (SEM); however, the method is often misapplied. This research applies the Decision-Making Trial and Evaluation Laboratory (DEMATEL) technique to explore the causal relationship between the significant Unified Theory of Acceptance and Use of Technology (UTAUT) variables. Fuzzy concept is applied to illustrate human vague judgment. It is significant that, in contrary with UTAUT, this study found that social influence does not matter in the behavioral intention to use the CDSS for medical professionals. © 2011 Elsevier Ltd. All rights reserved.","Clinical Decision Support System (CDSS); Fuzzy Decision-Making Trial and Evaluation Laboratory (fuzzy DEMATEL); Social influence; Technology Acceptance Model (TAM); Unified Theory of Acceptance and Use of Technology (UTAUT)","Clinical decision support systems; DEMATEL; Social influence; Technology Acceptance Model (TAM); Unified theory of acceptance and use of technology; Artificial intelligence; Decision making; Decision support systems; Technology; Economic and social effects",Article,Scopus,2-s2.0-84858073630
"Gagie T., Gawrychowski P., Kärkkäinen J., Nekrich Y., Puglisi S.J.","A faster grammar-based self-index",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",41,10.1007/978-3-642-28332-1_21,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857847846&doi=10.1007%2f978-3-642-28332-1_21&partnerID=40&md5=45b06cd9b9f7910822cdfe18d1360e78","To store and search genomic databases efficiently, researchers have recently started building compressed self-indexes based on straight-line programs and LZ77. In this paper we show how, given a balanced straight-line program for a string S[1..n] whose LZ77 parse consists of z phrases, we can add O(z log log z) words and obtain a compressed self-index for S such that, given a pattern P [1..m], we can list the occ occurrences of P in S in O(m 2 + (m + occ) log log n) time. All previous self-indexes are either larger or slower in the worst case. © 2012 Springer-Verlag.",,"Genomic database; Self-index; Straight line program; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84857847846
"Järvisalo M., Berre D.L., Roussel O., Simon L.","The international SAT solver competitions",2012,"AI Magazine",41,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861400735&partnerID=40&md5=a5182f3e7422c69446dad2a765ff69b8","The International SAT Solver Competition is today an established series of competitive events aiming at objectively evaluating the progress in state-of-the-art procedures for solving Boolean satisfiability (SAT) instances. Over the years, the competitions have significantly contributed to the fast progress in SAT solver technology that has made SAT a practical success story of computer science. This short article provides an overview of the SAT solver competitions. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Boolean satisfiability; SAT solvers; State-of-the-art procedures; Artificial intelligence; Formal logic",Article,Scopus,2-s2.0-84861400735
"Mourão-Miranda J., Oliveira L., Ladouceur C.D., Marquand A., Brammer M., Birmaher B., Axelson D., Phillips M.L.","Pattern recognition and functional neuroimaging help to discriminate healthy adolescents at risk for mood disorders from low risk adolescents",2012,"PLoS ONE",41,10.1371/journal.pone.0029482,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857077961&doi=10.1371%2fjournal.pone.0029482&partnerID=40&md5=ee8008a7d23d8d4a2f6725e34a522cdd","Introduction: There are no known biological measures that accurately predict future development of psychiatric disorders in individual at-risk adolescents. We investigated whether machine learning and fMRI could help to: 1. differentiate healthy adolescents genetically at-risk for bipolar disorder and other Axis I psychiatric disorders from healthy adolescents at low risk of developing these disorders; 2. identify those healthy genetically at-risk adolescents who were most likely to develop future Axis I disorders. Methods: 16 healthy offspring genetically at risk for bipolar disorder and other Axis I disorders by virtue of having a parent with bipolar disorder and 16 healthy, age- and gender-matched low-risk offspring of healthy parents with no history of psychiatric disorders (12-17 year-olds) performed two emotional face gender-labeling tasks (happy/neutral; fearful/neutral) during fMRI. We used Gaussian Process Classifiers (GPC), a machine learning approach that assigns a predictive probability of group membership to an individual person, to differentiate groups and to identify those at-risk adolescents most likely to develop future Axis I disorders. Results: Using GPC, activity to neutral faces presented during the happy experiment accurately and significantly differentiated groups, achieving 75% accuracy (sensitivity = 75%, specificity = 75%). Furthermore, predictive probabilities were significantly higher for those at-risk adolescents who subsequently developed an Axis I disorder than for those at-risk adolescents remaining healthy at follow-up. Conclusions: We show that a combination of two promising techniques, machine learning and neuroimaging, not only discriminates healthy low-risk from healthy adolescents genetically at-risk for Axis I disorders, but may ultimately help to predict which at-risk adolescents subsequently develop these disorders. © 2012 Mourão-Miranda et al.",,"adolescent; article; bipolar disorder; clinical article; controlled study; diagnostic and statistical manual of mental disorders; female; functional magnetic resonance imaging; genetic risk; human; machine learning; male; mood disorder; neuroimaging; normal distribution; pattern recognition; prediction; probability; sensitivity and specificity; artificial intelligence; bipolar disorder; case control study; child; comparative study; follow up; functional neuroimaging; longitudinal study; mental disease; mood disorder; nuclear magnetic resonance imaging; prognosis; psychological aspect; receiver operating characteristic; risk factor; Adolescent; Artificial Intelligence; Bipolar Disorder; Case-Control Studies; Child; Child of Impaired Parents; Female; Follow-Up Studies; Functional Neuroimaging; Humans; Longitudinal Studies; Magnetic Resonance Imaging; Male; Mental Disorders; Mood Disorders; Pattern Recognition, Physiological; Prognosis; Risk Factors; ROC Curve",Article,Scopus,2-s2.0-84857077961
"Bulut E., Duru O., Keçeci T., Yoshida S.","Use of consistency index, expert prioritization and direct numerical inputs for generic fuzzy-AHP modeling: A process model for shipping asset management",2012,"Expert Systems with Applications",41,10.1016/j.eswa.2011.08.056,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054947843&doi=10.1016%2fj.eswa.2011.08.056&partnerID=40&md5=20e9d070b1f3ad88347cece5f7ee63dd","The aim of this paper is to develop a generic version of the conventional fuzzy-analytic hierarchy process (FAHP) method and investigate the shipping asset management (SAM) problem in the dry bulk shipping market. The recent literature has various applications of the FAHP, but these studies lack consistency control, use identical decision support rather than weighted expert choices, and lack measurable criteria. The proposed model, generic fuzzy-AHP (here after GF-AHP), provides a standard control of consistency on the decision matrix for the expert group. GF-AHP also improves the capabilities of the FAHP by executing direct numerical inputs without expert consultation. In practical business, some of the criteria can be easily calculated and expert consultation is a redundant process. Therefore, GF-AHP presents how to transform such numerical inputs to a priority scale. Finally, expertise differences on the decision group are reflected in the GF-AHP process by an expert weighting algorithm. © 2011 Elsevier Ltd. All rights reserved.","Consistency index; Decision support systems; Generic fuzzy-AHP; Ship investment","Consistency control; Consistency index; Decision matrices; Decision supports; Dry bulk shipping market; Expert consultation; Fuzzy AHP; Hierarchy process; Prioritization; Process model; Artificial intelligence; Asset management; Decision support systems; Freight transportation; Investments; Numerical methods; Hierarchical systems",Conference Paper,Scopus,2-s2.0-80054947843
"Wang Y., Cai Z.","A dynamic hybrid framework for constrained evolutionary optimization",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",41,10.1109/TSMCB.2011.2161467,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856273612&doi=10.1109%2fTSMCB.2011.2161467&partnerID=40&md5=2e5b3016a5a427ad2bfece9f1a3bfe3a","Based on our previous work, this paper presents a dynamic hybrid framework, called DyHF, for solving constrained optimization problems. This framework consists of two major steps: global search model and local search model. In the global and local search models, differential evolution serves as the search engine, and Pareto dominance used in multiobjective optimization is employed to compare the individuals in the population. Unlike other existing methods, the above two steps are executed dynamically according to the feasibility proportion of the current population in this paper, with the purpose of reasonably distributing the computational resource for the global and local search during the evolution. The performance of DyHF is tested on 22 benchmark test functions. The experimental results clearly show that the overall performance of DyHF is highly competitive with that of a number of state-of-the-art approaches from the literature. © 2011 IEEE.","Constrained evolutionary optimization; constraint-handling technique; dynamic hybrid framework (DyHF); multiobjective optimization","Benchmark tests; Computational resources; Constrained evolutionary optimization; Constrained optimization problems; Constraint-handling techniques; Differential Evolution; Global search; Hybrid framework; Local search; Pareto dominance; State-of-the-art approach; Benchmarking; Constrained optimization; Evolutionary algorithms; Search engines; Multiobjective optimization; algorithm; article; artificial intelligence; automated pattern recognition; computer simulation; decision support system; methodology; theoretical model; Algorithms; Artificial Intelligence; Computer Simulation; Decision Support Techniques; Models, Theoretical; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84856273612
"Kiran M.S., Özceylan E., Gündüz M., Paksoy T.","Swarm intelligence approaches to estimate electricity energy demand in Turkey",2012,"Knowledge-Based Systems",40,10.1016/j.knosys.2012.06.009,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867888973&doi=10.1016%2fj.knosys.2012.06.009&partnerID=40&md5=f6697ae639f57046adbf9e78b0a7314a","This paper proposes two new models based on artificial bee colony (ABC) and particle swarm optimization (PSO) techniques to estimate electricity energy demand in Turkey. ABC and PSO electricity energy estimation models (ABCEE and PSOEE) are developed by incorporating gross domestic product (GDP), population, import and export figures of Turkey as inputs. All models are proposed in two forms, linear and quadratic. Also different neighbor selection mechanisms are attempted for ABCEE model to increase convergence to minimum of the algorithm. In order to indicate the applicability and accuracy of the proposed models, a comparison is made with ant colony optimization (ACO) which is available for the same problem in the literature. According to obtained results, relative estimation errors of the proposed models are lower than ACO and quadratic form provides better-fit solutions than linear form due to fluctuations of the socio-economic indicators. Finally, Turkey's electricity energy demand is projected until 2025 according to three different scenarios. © 2012 Elsevier B.V. All rights reserved.","Ant colony optimization; Artificial bee colony; Electricity energy estimation; Particle swarm optimization; Swarm intelligence","Ant Colony Optimization (ACO); Artificial bee colonies; Energy demands; Energy estimation; Gross domestic products; Neighbor selection; Particle swarm optimization technique; Quadratic form; Relative estimation; Socio-economics; Swarm Intelligence; Algorithms; Economics; Electricity; Energy management; Estimation; Number theory; Particle swarm optimization (PSO); Artificial intelligence",Article,Scopus,2-s2.0-84867888973
"Chen X., Kar S., Ralescu D.A.","Cross-entropy measure of uncertain variables",2012,"Information Sciences",40,10.1016/j.ins.2012.02.049,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860311821&doi=10.1016%2fj.ins.2012.02.049&partnerID=40&md5=82d8c0b67d090754d975d1894b4317b5","ross-entropy is a measure of the difference between two distribution functions. In order to deal with the divergence of uncertain variables via uncertainty distributions, this paper aims at introducing the concept of cross-entropy for uncertain variables based on uncertain theory, as well as investigating some mathematical properties of this concept. Several practical examples are also provided to calculate uncertain cross-entropy. Furthermore, the minimum cross-entropy principle is proposed in this paper. Finally, a study of generalized cross-entropy for uncertain variables is carried out. © 2012 Elsevier Inc. All rights reserved.","Cross-entropy; Minimum cross-entropy principle; Uncertain variable","Cross entropy; Mathematical properties; Minimum cross-entropy principle; Uncertain variables; Uncertainty distributions; Artificial intelligence; Software engineering",Article,Scopus,2-s2.0-84860311821
"Fang P., Zeng L.-L., Shen H., Wang L., Li B., Liu L., Hu D.","Increased Cortical-Limbic Anatomical Network Connectivity in Major Depression Revealed by Diffusion Tensor Imaging",2012,"PLoS ONE",40,10.1371/journal.pone.0045972,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866878342&doi=10.1371%2fjournal.pone.0045972&partnerID=40&md5=90e9c67692d15a71f654cf3bfe45847b","Magnetic resonance imaging studies have reported significant functional and structural differences between depressed patients and controls. Little attention has been given, however, to the abnormalities in anatomical connectivity in depressed patients. In the present study, we aim to investigate the alterations in connectivity of whole-brain anatomical networks in those suffering from major depression by using machine learning approaches. Brain anatomical networks were extracted from diffusion magnetic resonance images obtained from both 22 first-episode, treatment-naive adults with major depressive disorder and 26 matched healthy controls. Using machine learning approaches, we differentiated depressed patients from healthy controls based on their whole-brain anatomical connectivity patterns and identified the most discriminating features that represent between-group differences. Classification results showed that 91.7% (patients = 86.4%, controls = 96.2%; permutation test, p<0.0001) of subjects were correctly classified via leave-one-out cross-validation. Moreover, the strengths of all the most discriminating connections were increased in depressed patients relative to the controls, and these connections were primarily located within the cortical-limbic network, especially the frontal-limbic network. These results not only provide initial steps toward the development of neurobiological diagnostic markers for major depressive disorder, but also suggest that abnormal cortical-limbic anatomical networks may contribute to the anatomical basis of emotional dysregulation and cognitive impairments associated with this disease. © 2012 Fang et al.",,"adult; article; brain cortex; classification; clinical article; controlled study; diffusion tensor imaging; disease marker; female; frontal cortex; human; limbic system; machine learning; major depression; neuroanatomy; neurobiology; probability; structure analysis; validation process; Adult; Algorithms; Artificial Intelligence; Brain; Brain Mapping; Case-Control Studies; Cognition Disorders; Depressive Disorder, Major; Diffusion Tensor Imaging; Female; Humans; Limbic System; Magnetic Resonance Imaging; Male; Middle Aged; Models, Statistical; ROC Curve; Sensitivity and Specificity; Temporal Lobe",Article,Scopus,2-s2.0-84866878342
"Guo N.L., Wan Y.-W., Denvir J., Porter D.W., Pacurari M., Wolfarth M.G., Castranova V., Qian Y.","Multiwalled carbon nanotube-induced gene signatures in the mouse lung: Potential predictive value for human lung cancer risk and prognosis",2012,"Journal of Toxicology and Environmental Health - Part A: Current Issues",40,10.1080/15287394.2012.699852,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865211150&doi=10.1080%2f15287394.2012.699852&partnerID=40&md5=eec61e34ab794e0fd507ca04fe44da1c","Concerns over the potential for multiwalled carbon nanotubes (MWCNT) to induce lung carcinogenesis have emerged. This study sought to (1) identify gene expression signatures in the mouse lungs following pharyngeal aspiration of well-dispersed MWCNT and (2) determine if these genes were associated with human lung cancer risk and progression. Genome-wide mRNA expression profiles were analyzed in mouse lungs (n=160) exposed to 0, 10, 20, 40, or 80μg of MWCNT by pharyngeal aspiration at 1, 7, 28, and 56 d postexposure. By using pairwise statistical analysis of microarray (SAM) and linear modeling, 24 genes were selected, which have significant changes in at least two time points, have a more than 1.5-fold change at all doses, and are significant in the linear model for the dose or the interaction of time and dose. Additionally, a 38-gene set was identified as related to cancer from 330 genes differentially expressed at d 56 postexposure in functional pathway analysis. Using the expression profiles of the cancer-related gene set in 8 mice at d 56 postexposure to 10 g of MWCNT, a nearest centroid classification accurately predicts human lung cancer survival with a significant hazard ratio in training set (n=256) and test set (n=186). Furthermore, both gene signatures were associated with human lung cancer risk (n=164) with significant odds ratios. These results may lead to development of a surveillance approach for early detection of lung cancer and prognosis associated with MWCNT in the workplace. © 2012 Copyright Taylor and Francis Group, LLC.",,"messenger RNA; multi walled nanotube; animal experiment; animal model; animal tissue; article; cancer growth; cancer prognosis; cancer risk; cancer survival; controlled study; gene expression; genetic analysis; hazard ratio; lung cancer; male; microarray analysis; mouse; nasopharyngeal aspiration; nonhuman; nucleotide sequence; priority journal; risk; statistical analysis; statistical model; Adult; Aged; Animals; Artificial Intelligence; Cohort Studies; Computational Biology; Female; Gene Expression Profiling; Genome-Wide Association Study; Humans; Inhalation Exposure; Lung; Lung Neoplasms; Male; Mice; Mice, Inbred C57BL; Middle Aged; Nanotubes, Carbon; Neoplasm Staging; Predictive Value of Tests; Prognosis; Retrospective Studies; Risk Assessment; Specific Pathogen-Free Organisms; Tumor Markers, Biological",Article,Scopus,2-s2.0-84865211150
"Chang F.-C., Huang H.-C.","A refactoring method for cache-efficient swarm intelligence algorithms",2012,"Information Sciences",40,10.1016/j.ins.2010.02.025,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863230634&doi=10.1016%2fj.ins.2010.02.025&partnerID=40&md5=2354c2bccd0a798cbaded7e346b86e92","With advances in hardware technology, conventional approaches to software development are not effective for developing efficient algorithms for run-time environments. The problem comes from the overly simplified hardware abstraction model in the software development procedure. The mismatch between the hypothetical hardware model and real hardware design should be compensated for in designing an efficient algorithm. In this paper, we focus on two schemes: one is the memory hierarchy, and the other is the algorithm design. Both the cache properties and the cache-aware development are investigated. We then propose a few simple guidelines for revising a developed algorithm in order to increase the utilization of the cache. To verify the effectiveness of the guidelines proposed, optimization techniques, including particle swarm optimization (PSO) and the genetic algorithm (GA), are employed. Simulation results demonstrate that the guidelines are potentially helpful for revising various algorithms. © 2010 Elsevier Inc. All rights reserved.","Cache; Genetic algorithm; Memory hierarchy; Miss rate; Particle swarm optimization; Swarm intelligence","Cache; Memory hierarchy; Miss-rate; Particle swarm; Swarm Intelligence; Artificial intelligence; Computer architecture; Particle swarm optimization (PSO); Software design; Genetic algorithms",Article,Scopus,2-s2.0-84863230634
"Li H., Oren N., Norman T.J.","Probabilistic argumentation frameworks",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",40,10.1007/978-3-642-29184-5_1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859617767&doi=10.1007%2f978-3-642-29184-5_1&partnerID=40&md5=e519116f850a52fa1021c4d422ff8e13","In this paper, we extend Dung's seminal argument framework to form a probabilistic argument framework by associating probabilities with arguments and defeats. We then compute the likelihood of some set of arguments appearing within an arbitrary argument framework induced from this probabilistic framework. We show that the complexity of computing this likelihood precisely is exponential in the number of arguments and defeats, and thus describe an approximate approach to computing these likelihoods based on Monte-Carlo simulation. Evaluating the latter approach against the exact approach shows significant computational savings. Our probabilistic argument framework is applicable to a number of real world problems; we show its utility by applying it to the problem of coalition formation. © 2012 Springer-Verlag.",,"Coalition formations; Computational savings; Exact approach; Monte Carlo Simulation; Probabilistic argumentation; Probabilistic arguments; Probabilistic framework; Real-world problem; Artificial intelligence; Intelligent systems",Conference Paper,Scopus,2-s2.0-84859617767
"Huang T., Zhang J., Xu Z.-P., Hu L.-L., Chen L., Shao J.-L., Zhang L., Kong X.-Y., Cai Y.-D., Chou K.-C.","Deciphering the effects of gene deletion on yeast longevity using network and machine learning approaches",2012,"Biochimie",40,10.1016/j.biochi.2011.12.024,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862792208&doi=10.1016%2fj.biochi.2011.12.024&partnerID=40&md5=2e816d7e44f95204af1a3d199af6370e","Longevity is one of the most basic and one of the most essential properties of all living organisms. Identification of genes that regulate longevity would increase understanding of the mechanisms of aging, so as to help facilitate anti-aging intervention and extend the life span. In this study, based on the network features and the biochemical/physicochemical features of the deletion network and deletion genes, as well as their functional features, a two-layer model was developed for predicting the deletion effects on yeast longevity. The first stage of our prediction approach was to identify whether the deletion of one gene would change the life span of yeast; if it did, the second stage of our procedure would automatically proceed to predict whether the deletion of one gene would increase or decrease the life span. It was observed by analyzing the predicted results that the functional features (such as mitochondrial function and chromatin silencing), the network features (such as the edge density and edge weight density of the deletion network), and the local centrality of deletion gene, would have important impact for predicting the deletion effects on longevity. It is anticipated that our model may become a useful tool for studying longevity from the angle of genes and networks. Moreover, it has not escaped our notice that, after some modification, the current model can also be used to study many other phenotype prediction problems from the angle of systems biology. © 2011 Published by Elsevier Masson SAS.","Feature selection; Gene deletion; Longevity; Machine learning; Protein network","article; cell function; chromatin; controlled study; DNA modification; gene deletion; gene identification; gene silencing; lifespan; longevity; machine learning; mitochondrion; network learning; nonhuman; phenotype; physical chemistry; prediction; systems biology; yeast; Aging; Algorithms; Animals; Artificial Intelligence; Caenorhabditis elegans; Computer Simulation; Gene Deletion; Genes, Fungal; Longevity; Microbial Viability; Models, Genetic; Saccharomyces cerevisiae",Article,Scopus,2-s2.0-84862792208
"Hanaoka G., Kawai Y., Kunihiro N., Matsuda T., Weng J., Zhang R., Zhao Y.","Generic construction of chosen ciphertext secure proxy re-encryption",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",40,10.1007/978-3-642-27954-6_22,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863280137&doi=10.1007%2f978-3-642-27954-6_22&partnerID=40&md5=15bf3cc70051e14169c63d9f28147587","In this paper, we present the first generic construction of a chosen-ciphertext (CCA) secure uni-directional proxy re-encryption (PRE) scheme. In particular, full CCA security (i.e., not relaxed CCA security such as replayable CCA security) of our proposed scheme is proven even against powerful adversaries that are given a more advantageous attack environment than in all previous works, and furthermore, random oracles are not required. To achieve such strong security, we establish a totally novel methodology for designing PRE based on a specific class of threshold encryption. Via our generic construction, we present the first construction that is CCA secure in the standard model. © 2012 Springer-Verlag.",,"CCA securities; Ciphertexts; First constructions; Full CCA; Generic construction; Novel methodology; Proxy re encryptions; Random Oracle; The standard model; Threshold encryption; Artificial intelligence; Cryptography",Conference Paper,Scopus,2-s2.0-84863280137
"Carneiro G., Nascimento J.C., Freitas A.","The segmentation of the left ventricle of the heart from ultrasound data using deep learning architectures and derivative-based search methods",2012,"IEEE Transactions on Image Processing",40,10.1109/TIP.2011.2169273,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857295176&doi=10.1109%2fTIP.2011.2169273&partnerID=40&md5=2e038592c6709e0b643f6fe986230d60","We present a new supervised learning model designed for the automatic segmentation of the left ventricle (LV) of the heart in ultrasound images. We address the following problems inherent to supervised learning models: 1) the need of a large set of training images; 2) robustness to imaging conditions not present in the training data; and 3) complex search process. The innovations of our approach reside in a formulation that decouples the rigid and nonrigid detections, deep learning methods that model the appearance of the LV, and efficient derivative-based search algorithms. The functionality of our approach is evaluated using a data set of diseased cases containing 400 annotated images (from 12 sequences) and another data set of normal cases comprising 80 annotated images (from two sequences), where both sets present long axis views of the LV. Using several error measures to compute the degree of similarity between the manual and automatic segmentations, we show that our method not only has high sensitivity and specificity but also presents variations with respect to a gold standard (computed from the manual annotations of two experts) within interuser variability on a subset of the diseased cases. We also compare the segmentations produced by our approach and by two state-of-the-art LV segmentation models on the data set of normal cases, and the results show that our approach produces segmentations that are comparable to these two approaches using only 20 training images and increasing the training set to 400 images causes our approach to be generally more accurate. Finally, we show that efficient search methods reduce up to tenfold the complexity of the method while still producing competitive segmentations. In the future, we plan to include a dynamical model to improve the performance of the algorithm, to use semisupervised learning methods to reduce even more the dependence on rich and large training sets, and to design a shape model less dependent on the training set. © 2011 IEEE.",,"Automatic segmentations; Complex searches; Data sets; Deep learning; Degree of similarity; Dynamical model; Error measures; Following problem; Gold standards; High sensitivity; Imaging conditions; Left ventricles; Long axis; Manual annotation; Search Algorithms; Search method; Semi-supervised learning methods; Shape model; Training data; Training image; Training sets; Ultrasound data; Ultrasound images; Algorithms; Sensitivity analysis; Supervised learning; Ultrasonics; Image segmentation; algorithm; article; artificial intelligence; computer assisted diagnosis; echography; heart left ventricle function; heart left ventricle hypertrophy; heart ventricle; human; methodology; receiver operating characteristic; Algorithms; Artificial Intelligence; Heart Ventricles; Humans; Hypertrophy, Left Ventricular; Image Interpretation, Computer-Assisted; ROC Curve; Ventricular Dysfunction, Left",Article,Scopus,2-s2.0-84857295176
"Im J., Lu Z., Rhee J., Quackenbush L.J.","Impervious surface quantification using a synthesis of artificial immune networks and decision/regression trees from multi-sensor data",2012,"Remote Sensing of Environment",40,10.1016/j.rse.2011.06.024,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855469971&doi=10.1016%2fj.rse.2011.06.024&partnerID=40&md5=5de16e69b36ad898412a3efcd81d4fde","Impervious surface quantification is important for many planning and management applications because of the impacts that impervious surfaces have on a range of environmental resources such as groundwater. This research proposes an integrated method to quantify impervious surfaces at multiple spatial scales via a synthesis of several machine learning approaches. In this study, we 1) proposed a hierarchical classification method to detect impervious surfaces through a fusion of optimized artificial immune networks (OPTINC) and decision trees at high spatial resolution, 2) evaluated the method using multi-sensor data (i.e., high spatial resolution WorldView-2 and LiDAR data) to map impervious surfaces, 3) tested the applicability of the binary impervious surface maps to quantify sub-pixel imperviousness from Landsat TM data of a larger region using regression trees at moderate spatial resolution, and 4) examined the model sensitivity of regression trees to training sample size for impervious surface quantification. OPTINC and decision trees successfully identified impervious surfaces at high resolution (overall accuracy&gt;90%). The regression tree predicted imperviousness from the TM data with a moderate success (R 2=0.64 and MAE=14.2%). Although the regression tree appeared robust when training sample size was sufficiently large, it was not stable with small sample size (e.g., &lt;100). © 2011 Elsevier Inc.","Artificial immune networks; Decision/regression trees; Impervious surfaces; Lidar; WorldView-2","Artificial immune networks; Decision/regression trees; Environmental resources; Hierarchical classification; High resolution; High spatial resolution; Impervious surface; Integrated method; Landsat TM data; LIDAR data; Machine-learning; Management applications; Model sensitivity; Multi-sensor data; Regression trees; Small Sample Size; Spatial resolution; Spatial scale; Sub pixels; Training sample; WorldView-2; Binary trees; Decision trees; Forestry; Groundwater; Groundwater resources; Image resolution; Information management; Optical radar; Regression analysis; Sampling; Sensor data fusion; Sensors; Trees (mathematics); decision making; groundwater resource; Landsat thematic mapper; lidar; optimization; pixel; regression analysis; satellite data; spatial analysis; spatial resolution; water management; Artificial Intelligence; Decision Making; Forestry; Ground Water; Neural Networks; Regression Analysis; Satellites; Sensors; Surface Treatment; Synthesis",Article,Scopus,2-s2.0-84855469971
"Dao T.H., Jeong S.R., Ahn H.","A novel recommendation model of location-based advertising: Context-Aware Collaborative Filtering using GA approach",2012,"Expert Systems with Applications",40,10.1016/j.eswa.2011.09.070,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80255126180&doi=10.1016%2fj.eswa.2011.09.070&partnerID=40&md5=292e41829e80a9c1ee9535eae9e3fccd","Recommender systems are the efficient and most used tools that prevail over the information overload problem, provide users with the most appropriate content by considering their personal preferences (mostly, ratings). In addition to these preferences, taking into account the interaction context of users will improve the relevancy of the recommendation process. However, only a few prior studies have tried to adopt context-awareness to the recommendation model. Although a number of studies have developed recommendation models using collaborative filtering (CF), few of them have tried to adopt both CF and other artificial intelligence techniques, such as genetic algorithm (GA), as a tool to improve recommendation results. In this paper, we propose a new recommendation model, which we termed Context-Aware Collaborative Filtering using genetic algorithm (CACF-GA), for location-based advertising (LBA) based on both user's preferences and interaction's context. We first defined discrete contexts, and then applied the concept of ""context similarity"" to conventional CF to create the context-aware recommendation model. The context similarity between two contexts is designed to be optimized using GA. We collect real-world data from mobile users, build a LBA recommendation model using CACF-GA, and then perform an empirical test to validate the usefulness of CACF-GA. Experiments show our proposed model provides the most accurate prediction results compared to comparative ones. © 2011 Elsevier Ltd. All rights reserved.","Collaborative filtering; Context-awareness; Genetic algorithm; Location-based advertising; Recommendation model","Accurate prediction; Artificial intelligence techniques; Collaborative filtering; Context-Aware; Context-awareness; Empirical test; Information overloads; Interaction context; Location-based advertising; Mobile users; Personal preferences; Real world data; Recommendation model; Artificial intelligence; Global system for mobile communications; Marketing; Genetic algorithms",Article,Scopus,2-s2.0-80255126180
"Wang J.-W., Wu H.-N., Li H.-X.","Distributed proportional-spatial derivative control of nonlinear parabolic systems via fuzzy PDE modeling approach",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",40,10.1109/TSMCB.2012.2185046,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862828417&doi=10.1109%2fTSMCB.2012.2185046&partnerID=40&md5=91e5d19580484be3a38b2af8791212df","In this paper, a distributed fuzzy control design based on Proportional-spatial Derivative (P-sD) is proposed for the exponential stabilization of a class of nonlinear spatially distributed systems described by parabolic partial differential equations (PDEs). Initially, a Takagi-Sugeno (T-S) fuzzy parabolic PDE model is proposed to accurately represent the nonlinear parabolic PDE system. Then, based on the T-S fuzzy PDE model, a novel distributed fuzzy P-sD state feedback controller is developed by combining the PDE theory and the Lyapunov technique, such that the closed-loop PDE system is exponentially stable with a given decay rate. The sufficient condition on the existence of an exponentially stabilizing fuzzy controller is given in terms of a set of spatial differential linear matrix inequalities (SDLMIs). A recursive algorithm based on the finite-difference approximation and the linear matrix inequality (LMI) techniques is also provided to solve these SDLMIs. Finally, the developed design methodology is successfully applied to the feedback control of the Fitz-Hugh-Nagumo equation. © 2012 IEEE.","Exponential stability; fuzzy control; linear matrix inequalities (LMIs); spatially distributed systems (SDSs); Takagi-Sugeno (T-S) fuzzy model","Closed-loop; Decay rate; Design Methodology; Distributed fuzzy control; Exponential stabilization; Exponentially stable; Finite difference approximations; Fuzzy controllers; Linear matrix inequality (LMIs); Linear matrix inequality techniques; Lyapunov techniques; Modeling approach; Nonlinear parabolic systems; Parabolic partial differential equations; Parabolic PDEs; PDE model; Recursive algorithms; Spatially-distributed system; State feedback controller; Sufficient conditions; T-S fuzzy; Takagi Sugeno fuzzy models; Takagi-sugeno; Approximation algorithms; Asymptotic stability; Decay (organic); Feedback control; Fuzzy control; Nonlinear control systems; Partial differential equations; Spatial distribution; State feedback; Linear matrix inequalities; algorithm; article; artificial intelligence; automated pattern recognition; computer simulation; decision support system; feedback system; fuzzy logic; methodology; nonlinear system; theoretical model; Algorithms; Artificial Intelligence; Computer Simulation; Decision Support Techniques; Feedback; Fuzzy Logic; Models, Theoretical; Nonlinear Dynamics; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84862828417
"Schetke S., Haase D., Kötter T.","Towards sustainable settlement growth: A new multi-criteria assessment for implementing environmental targets into strategic urban planning",2012,"Environmental Impact Assessment Review",40,10.1016/j.eiar.2011.08.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054890297&doi=10.1016%2fj.eiar.2011.08.008&partnerID=40&md5=78a86e1331cce55cd66ea6e798c87e89","For nearly one decade, the German political and research-agenda has been to a large extent determined by the ongoing question of how to limit the expansion of settlement areas around cities in order to preserve natural resources, make settlement growth more sustainable and to strengthen the re-use of existing inner-urban areas (see a.o. Kötter et al. 2009a, 2010; Schetke et al. 2009, 2010b). What is already under discussion within the international literature are the recommendations of the German Council for Sustainability to quantitatively reduce the daily greenfield consumption from the current rate of over 100. ha per day to a rate of 30. ha per day in 2020 and to bring urban infill development up to a ratio of 3:1 with greenfield development (German Council for Sustainability, 2004).). This paper addresses the added value beyond those abstract political targets and presents an innovative, multi-criteria assessment (MCA) of greenfield and infill sites to evaluate their sustainability and resource efficiency. MCA development and its incorporation into a Decision Support System (DSS) were accomplished by utilising a stakeholder-driven approach. The resulting tool can be applied in preparing and revising land-use plans. The paper presents the concept and the development process of the MCA-DSS. Test runs with planners prove that the evaluation of potential housing sites using individually weighted environmental indicators helps to identify those strategies of housing development that accord most closely with sustainability goals. The tests further show that the development of greenfield sites generally exhibits less sustainability than that of infill sites. © 2011 Elsevier Inc.","Decision support system; Ecological targets; Land consumption; Multi-criteria assessment; Urban land use","Added values; Current rate; Development process; Ecological targets; Environmental indicators; Environmental targets; Greenfield; Housing development; Infill development; Land consumption; Land-use plan; Multi-criteria assessment; Resource efficiencies; Strategic urban planning; Sustainable settlements; Test runs; Urban land use; Artificial intelligence; Decision making; Decision support systems; Housing; Land use; Rating; Urban planning; Sustainable development; decision support system; human settlement; infill; inner city area; land use planning; multicriteria analysis; strategic approach; strategic environmental assessment; sustainability; urban planning; Germany",Article,Scopus,2-s2.0-80054890297
"Xu Y., Xu D., Lin S., Han T.X., Cao X., Li X.","Detection of sudden pedestrian crossings for driving assistance systems",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",40,10.1109/TSMCB.2011.2175726,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861183293&doi=10.1109%2fTSMCB.2011.2175726&partnerID=40&md5=5b46e2f8de92bd9d5267cd001a00685c","In this paper, we study the problem of detecting sudden pedestrian crossings to assist drivers in avoiding accidents. This application has two major requirements: to detect crossing pedestrians as early as possible just as they enter the view of the car-mounted camera and to maintain a false alarm rate as low as possible for practical purposes. Although many current sliding-window-based approaches using various features and classification algorithms have been proposed for image-/video-based pedestrian detection, their performance in terms of accuracy and processing speed falls far short of practical application requirements. To address this problem, we propose a three-level coarse-to-fine video-based framework that detects partially visible pedestrians just as they enter the camera view, with low false alarm rate and high speed. The framework is tested on a new collection of high-resolution videos captured from a moving vehicle and yields a performance better than that of state-of-the-art pedestrian detection while running at a frame rate of 55 fps. © 2012 IEEE.","Coarse to fine; pedestrian detection; performance evaluation; spatiotemporal refinement; sudden pedestrian crossing","A-frames; Application requirements; Camera view; Classification algorithm; Coarse to fine; Driving assistance systems; False alarm rate; High resolution; Moving vehicles; Pedestrian detection; Performance evaluation; Processing speed; spatiotemporal refinement; Three-level; Cameras; Errors; Footbridges; Intelligent vehicle highway systems; Video signal processing; Crosswalks; algorithm; article; artificial intelligence; automated pattern recognition; car driving; computer simulation; decision support system; methodology; theoretical model; traffic accident; Accidents, Traffic; Algorithms; Artificial Intelligence; Automobile Driving; Computer Simulation; Decision Support Techniques; Models, Theoretical; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84861183293
"Saif H., He Y., Alani H.","Alleviating data sparsity for twitter sentiment analysis",2012,"CEUR Workshop Proceedings",39,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889855944&partnerID=40&md5=4c6b034e083e88c3f3a8ffd43cd35301","Twitter has brought much attention recently as a hot research topic in the domain of sentiment analysis. Training sentiment classifiers from tweets data often faces the data sparsity problem partly due to the large variety of short and irregular forms introduced to tweets because of the 140-character limit. In this work we propose using two different sets of features to alleviate the data sparseness problem. One is the semantic feature set where we extract semantically hidden concepts from tweets and then incorporate them into classifier training through interpolation. Another is the sentiment-topic feature set where we extract latent topics and the associated topic sentiment from tweets, then augment the original feature space with these sentiment-topics. Experimental results on the Stanford Twitter Sentiment Dataset show that both feature sets outperform the baseline model using unigrams only. Moreover, using semantic features rivals the previously reported best result. Using sentimenttopic features achieves 86.3% sentiment classification accuracy, which outperforms existing approaches. Categories and Subject Descriptors I.2.7 [Artificial Intelligence]: Natural Language Processing-Text Analysis General Terms Algorithms, Experimentation.","Data sparsity; Microblogs; Opinion mining; Semantic smoothing; Sentiment analysis; Twitter","Data sparsity; Microblogs; Opinion mining; Sentiment analysis; Twitter; Artificial intelligence; Classification (of information); Data mining; Image segmentation; Semantics; Social networking (online); Natural language processing systems",Conference Paper,Scopus,2-s2.0-84889855944
"Subramanian K., Suresh S.","Human action recognition using meta-cognitive neuro-fuzzy inference system",2012,"International Journal of Neural Systems",39,10.1142/S0129065712500281,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870480229&doi=10.1142%2fS0129065712500281&partnerID=40&md5=88b7a076d7aece2c76df0ce9e990f15d","We propose a sequential Meta-Cognitive learning algorithm for Neuro-Fuzzy Inference System (McFIS) to efficiently recognize human actions from video sequence. Optical flow information between two consecutive image planes can represent actions hierarchically from local pixel level to global object level, and hence are used to describe the human action in McFIS classifier. McFIS classifier and its sequential learning algorithm is developed based on the principles of self-regulation observed in human meta-cognition. McFIS decides on what-to-learn, when-to-learn and how-to-learn based on the knowledge stored in the classifier and the information contained in the new training samples. The sequential learning algorithm of McFIS is controlled and monitored by the meta-cognitive components which uses class-specific, knowledge based criteria along with self-regulatory thresholds to decide on one of the following strategies: (i) Sample deletion (ii) Sample learning and (iii) Sample reserve. Performance of proposed McFIS based human action recognition system is evaluated using benchmark Weizmann and KTH video sequences. The simulation results are compared with well known SVM classifier and also with state-of-the-art action recognition results reported in the literature. The results clearly indicates McFIS action recognition system achieves better performances with minimal computational effort. © 2012 World Scientific Publishing Company.","classification; human action recognition; Meta-cognition; neuro-fuzzy inference system; self-regulatory learning","Action recognition; Action recognition systems; Computational effort; Consecutive images; Flow informations; Human actions; Human-action recognition; Meta-cognition; Metacognitives; Neuro-fuzzy inference systems; Pixel level; Sample learning; self-regulatory learning; Sequential learning algorithm; SVM classifiers; Training sample; Video sequences; Benchmarking; Classification (of information); Cognitive systems; Fuzzy systems; Gesture recognition; Image recognition; Knowledge based systems; Learning algorithms; Video recording; Motion estimation; algorithm; article; artificial intelligence; automated pattern recognition; evaluation; fuzzy logic; human; methodology; movement (physiology); statistics; Algorithms; Artificial Intelligence; Fuzzy Logic; Humans; Movement; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84870480229
"Li Q., Clifford G.D.","Signal quality and data fusion for false alarm reduction in the intensive care unit",2012,"Journal of Electrocardiology",39,10.1016/j.jelectrocard.2012.07.015,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867842919&doi=10.1016%2fj.jelectrocard.2012.07.015&partnerID=40&md5=ba13b06df6b27ae533fb36f809af7e3c","Due to a lack of integration between different sensors, false alarms (FA) in the intensive care unit (ICU) are frequent and can lead to reduced standard of care. We present a novel framework for FA reduction using a machine learning approach to combine up to 114 signal quality and physiological features extracted from the electrocardiogram, photoplethysmograph, and optionally the arterial blood pressure waveform. A machine learning algorithm was trained and evaluated on a database of 4107 expert-labeled life-threatening arrhythmias, from 182 separate ICU visits. On the independent test data, FA suppression results with no true alarm (TA) suppression were 86.4% for asystole, 100% for extreme bradycardia and 27.8% for extreme tachycardia. For the ventricular tachycardia alarms, the best FA suppression performance was 30.5% with a TA suppression rate below 1%. To reduce the TA suppression rate to zero, a reduction in FA suppression performance to 19.7% was required. © 2012 Elsevier Inc.","False alarm reduction; Genetic algorithm; Intensive care unit; Relevance vector machine; Signal quality assessment","alarm monitoring; algorithm; article; blood pressure; data processing; electrocardiography; false alarm; heart arrest; heart ventricle tachycardia; human; intensive care unit; machine learning; photoelectric plethysmography; priority journal; Arrhythmias, Cardiac; Artificial Intelligence; Clinical Alarms; Diagnosis, Computer-Assisted; Diagnostic Errors; False Positive Reactions; Humans; Intensive Care; Monitoring, Physiologic; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84867842919
"Kleiner A., Talwalkar A., Sarkar P., Jordan M.I.","The big data bootstrap",2012,"Proceedings of the 29th International Conference on Machine Learning, ICML 2012",39,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867129586&partnerID=40&md5=4b2bdea30080daf0e5c8f5f585ecc62a","The bootstrap provides a simple and powerful means of assessing the quality of estimators. However, in settings involving large datasets, the computation of bootstrap-based quantities can be prohibitively demanding. As an alternative, we present the Bag of Little Bootstraps (BLB), a new procedure which incorporates features of both the bootstrap and subsampling to obtain a robust, computationally efficient means of assessing estimator quality. BLB is well suited to modern parallel and distributed computing architectures and retains the generic applicability, statistical efficiency, and favorable theoretical properties of the bootstrap. We provide the results of an extensive empirical and theoretical investigation of BLB's behavior, including a study of its statistical correctness, its largescale implementation and performance, selection of hyperparameters, and performance on real data. Copyright 2012 by the author(s)/owner(s).",,"Big datum; Computationally efficient; Hyperparameters; Large datasets; Parallel and distributed computing; Statistical efficiency; Theoretical investigations; Artificial intelligence; Software engineering; Learning systems",Conference Paper,Scopus,2-s2.0-84867129586
"Gurulingappa H., Rajput A.M., Roberts A., Fluck J., Hofmann-Apitius M., Toldo L.","Development of a benchmark corpus to support the automatic extraction of drug-related adverse effects from medical case reports",2012,"Journal of Biomedical Informatics",39,10.1016/j.jbi.2012.04.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865989881&doi=10.1016%2fj.jbi.2012.04.008&partnerID=40&md5=883936adfb41d0afcd2d9d922a06c986","A significant amount of information about drug-related safety issues such as adverse effects are published in medical case reports that can only be explored by human readers due to their unstructured nature. The work presented here aims at generating a systematically annotated corpus that can support the development and validation of methods for the automatic extraction of drug-related adverse effects from medical case reports. The documents are systematically double annotated in various rounds to ensure consistent annotations. The annotated documents are finally harmonized to generate representative consensus annotations. In order to demonstrate an example use case scenario, the corpus was employed to train and validate models for the classification of informative against the non-informative sentences. A Maximum Entropy classifier trained with simple features and evaluated by 10-fold cross-validation resulted in the F1 score of 0.70 indicating a potential useful application of the corpus. © 2012 Elsevier Inc..","Adverse drug effect; Annotation; Benchmark corpus; Harmonization; Sentence classification","10-fold cross-validation; Adverse effect; Amount of information; Annotation; Automatic extraction; Benchmark corpus; Drug effects; Harmonization; Human readers; Maximum entropy; Medical case; Safety issues; Use case scenario; Bioinformatics; Computer applications; article; automation; drug industry; drug safety; drug surveillance program; gold standard; human; Medline; priority journal; quality control; validation process; Artificial Intelligence; Data Mining; Databases, Factual; Documentation; Drug Toxicity; Humans; PubMed; Reproducibility of Results; Semantics",Article,Scopus,2-s2.0-84865989881
"Korošec P., Šilc J., Filipič B.","The differential ant-stigmergy algorithm",2012,"Information Sciences",39,10.1016/j.ins.2010.05.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857832526&doi=10.1016%2fj.ins.2010.05.002&partnerID=40&md5=ca76f80a2ff017a10a14cb64af716a39","Ant-Colony Optimization (ACO) is a popular swarm intelligence scheme known for its efficiency in solving combinatorial optimization problems. However, despite some extensions of this approach to continuous optimization, high-dimensional problems remain a challenge for ACO. This paper presents an ACO-based algorithm for numerical optimization capable of solving high-dimensional real-parameter optimization problems. The algorithm, called the Differential Ant-Stigmergy Algorithm (DASA), transforms a real-parameter optimization problem into a graph-search problem. The parameters' differences assigned to the graph vertices are used to navigate through the search space. We compare the algorithm results with the results of previous studies on recent benchmark functions and show that the DASA is a competitive continuous optimization algorithm that solves high-dimensional problems effectively and efficiently. © 2010 Elsevier Inc. All rights reserved.","Ant-Colony Optimization; Global optimization; High-dimensional problems; Stigmergy; Swarm intelligence","Ant-colony optimization; Benchmark functions; Combinatorial optimization problems; Continuous optimization; Graph vertex; High-dimensional; High-dimensional problems; Its efficiencies; Numerical optimizations; Real-parameter optimization; Search spaces; Stigmergy; Swarm Intelligence; Artificial intelligence; Combinatorial optimization; Global optimization; Optimization; Algorithms",Article,Scopus,2-s2.0-84857832526
"Yuwono M., Moulton B.D., Su S.W., Celler B.G., Nguyen H.T.","Unsupervised machine-learning method for improving the performance of ambulatory fall-detection systems",2012,"BioMedical Engineering Online",39,10.1186/1475-925X-11-9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856852493&doi=10.1186%2f1475-925X-11-9&partnerID=40&md5=2321f30c444231dfff041bd9ee8d652b","Background: Falls can cause trauma, disability and death among older people. Ambulatory accelerometer devices are currently capable of detecting falls in a controlled environment. However, research suggests that most current approaches can tend to have insufficient sensitivity and specificity in non-laboratory environments, in part because impacts can be experienced as part of ordinary daily living activities.Method: We used a waist-worn wireless tri-axial accelerometer combined with digital signal processing, clustering and neural network classifiers. The method includes the application of Discrete Wavelet Transform, Regrouping Particle Swarm Optimization, Gaussian Distribution of Clustered Knowledge and an ensemble of classifiers including a multilayer perceptron and Augmented Radial Basis Function (ARBF) neural networks.Results: Preliminary testing with 8 healthy individuals in a home environment yields 98.6% sensitivity to falls and 99.6% specificity for routine Activities of Daily Living (ADL) data. Single ARB and MLP classifiers were compared with a combined classifier. The combined classifier offers the greatest sensitivity, with a slight reduction in specificity for routine ADL and an increased specificity for exercise activities. In preliminary tests, the approach achieves 100% sensitivity on in-group falls, 97.65% on out-group falls, 99.33% specificity on routine ADL, and 96.59% specificity on exercise ADL.Conclusion: The pre-processing and feature-extraction steps appear to simplify the signal while successfully extracting the essential features that are required to characterize a fall. The results suggest this combination of classifiers can perform better than MLP alone. Preliminary testing suggests these methods may be useful for researchers who are attempting to improve the performance of ambulatory fall-detection systems. © 2012 Yuwono et al; licensee BioMed Central Ltd.",,"Activities of Daily Living; Combination of classifiers; Controlled environment; Daily living; Ensemble of classifiers; Healthy individuals; Home environment; Machine-learning; MLP classifiers; Multi layer perceptron; Neural network classifier; Older People; Pre-processing; Radial basis functions; Sensitivity and specificity; Triaxial accelerometer; Accelerometers; Discrete wavelet transforms; Partial discharges; Radial basis function networks; Signal processing; Learning systems; acceleration; adult; algorithm; ambulatory monitoring; article; artificial intelligence; artificial neural network; daily life activity; falling; female; human; instrumentation; male; methodology; movement (physiology); physiology; sensitivity and specificity; signal processing; Acceleration; Accidental Falls; Activities of Daily Living; Adult; Algorithms; Artificial Intelligence; Female; Humans; Male; Monitoring, Ambulatory; Movement; Neural Networks (Computer); Sensitivity and Specificity; Signal Processing, Computer-Assisted",Article,Scopus,2-s2.0-84856852493
"Aguado A., Caño A.D., De La Cruz M.P., Gómez D., Josa A.","Sustainability assessment of concrete structures within the Spanish structural concrete code",2012,"Journal of Construction Engineering and Management",39,10.1061/(ASCE)CO.1943-7862.0000419,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858389062&doi=10.1061%2f%28ASCE%29CO.1943-7862.0000419&partnerID=40&md5=ba5062d06c1caa31b0557ca35e374df1","Since 2008, the Spanish Structural Concrete Code (EHE in Spanish) has generally focused on respecting the environment; as a result, it has become a support tool for designing sustainable concrete structures. The Ministry of Public Works took charge of the initiative and a wide range of professionals and researchers, encompassing various points of view in this sector, carried it through. It is a pioneering initiative, opening up an avenue for the future of sustainable structural design. The purpose of this paper is to summarize the procedure used in drafting the sustainability appendix for the EHE and to present the EHE model for assessing the sustainability of concrete structures. Although the project has had its problems, the model that it produced is very thorough. It includes practically all the main criteria, taking into account the current state of the art and the fact that this was the first time a sustainability assessment technique has been included in a structural code. © 2012 American Society of Civil Engineers.","Concrete structures; Decision support systems; Quantitative analysis; Standards and codes; Sustainable development","Decision supports; Standards and codes; State of the art; Structural codes; Structural concretes; Support tool; Sustainability assessment; Sustainable concretes; Artificial intelligence; Chemical analysis; Concrete construction; Decision support systems; Public works; Structural design; Sustainable development",Article,Scopus,2-s2.0-84858389062
"Li S., Fang L., Yin H.","An efficient dictionary learning algorithm and its application to 3-d medical image denoising",2012,"IEEE Transactions on Biomedical Engineering",39,10.1109/TBME.2011.2173935,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862960680&doi=10.1109%2fTBME.2011.2173935&partnerID=40&md5=d0dccf5d03c829a1d0bc5edb9880d1dd","In this paper, we propose an efficient dictionary learning algorithm for sparse representation of given data and suggest a way to apply this algorithm to 3-D medical image denoising. Our learning approach is composed of two main parts: sparse coding and dictionary updating. On the sparse coding stage, an efficient algorithm named multiple clusters pursuit (MCP) is proposed. The MCP first applies a dictionary structuring strategy to cluster the atoms with high coherence together, and then employs a multiple-selection strategy to select several competitive atoms at each iteration. These two strategies can greatly reduce the computation complexity of the MCP and assist it to obtain better sparse solution. On the dictionary updating stage, the alternating optimization that efficiently approximates the singular value decomposition is introduced. Furthermore, in the 3-D medical image denoising application, a joint 3-D operation is proposed for taking the learning capabilities of the presented algorithm to simultaneously capture the correlations within each slice and correlations across the nearby slices, thereby obtaining better denoising results. The experiments on both synthetically generated data and real 3-D medical images demonstrate that the proposed approach has superior performance compared to some well-known methods. © 2006 IEEE.","3-D medical image denoising; Dictionary learning; k-means clustering; multiple-selection strategy; sparse representation","Dictionary learning; K-means clustering; Medical images; multiple-selection strategy; Sparse representation; Clustering algorithms; Codes (symbols); Image processing; Learning algorithms; Medical imaging; Noise pollution control; Singular value decomposition; Three dimensional; article; atom; book; coding; computer assisted tomography; echography; image analysis; learning algorithm; mathematical analysis; three dimensional imaging; Algorithms; Artificial Intelligence; Cluster Analysis; Female; Head; Humans; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Liver; Male; Models, Theoretical; Pelvis; Tomography, X-Ray Computed; Ultrasonography",Article,Scopus,2-s2.0-84862960680
"Zhu J.","Max-margin nonparametric latent feature models for link prediction",2012,"Proceedings of the 29th International Conference on Machine Learning, ICML 2012",38,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867115033&partnerID=40&md5=f4a734f0ae45d5c4f414be581bab93a2","We present a max-margin nonparametric latent feature relational model, which unites the ideas of max-margin learning and Bayesian nonparametrics to discover discriminative latent features for link prediction and automatically infer the unknown latent social dimension. By minimizing a hinge-loss using the linear expectation operator, we can perform posterior inference efficiently without dealing with a highly nonlinear link likelihood function; by using a fully-Bayesian formulation, we can avoid tuning regularization constants. Experimental results on real datasets appear to demonstrate the benefits inherited from max-margin learning and fully-Bayesian nonparametric inference. Copyright 2012 by the author(s)/owner(s).",,"Bayesian nonparametrics; Expectation operator; Feature models; Highly nonlinear; Likelihood functions; Link prediction; Non-parametric; Nonparametric inference; Real data sets; Relational Model; Social dimensions; Artificial intelligence; Software engineering; Learning systems",Conference Paper,Scopus,2-s2.0-84867115033
"Verberne F.M.F., Ham J., Midden C.J.H.","Trust in smart systems: Sharing driving goals and giving information to increase trustworthiness and acceptability of smart systems in cars",2012,"Human Factors",38,10.1177/0018720812443825,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867360941&doi=10.1177%2f0018720812443825&partnerID=40&md5=a776f3d1c00f75aca4d45127454230c6","Objective: We examine whether trust in smart systems is generated analogously to trust in humans and whether the automation level of smart systems affects trustworthiness and acceptability of those systems. Background: Trust is an important factor when considering acceptability of automation technology. As shared goals lead to social trust, and intelligent machines tend to be treated like humans, the authors expected that shared driving goals would also lead to increased trustworthiness and acceptability of adaptive cruise control (ACC) systems. Method: In an experiment, participants (N = 57) were presented with descriptions of three ACCs with different automation levels that were described as systems that either shared their driving goals or did not. Trustworthiness and acceptability of all the ACCs were measured. Results: ACCs sharing the driving goals of the user were more trustworthy and acceptable than were ACCs not sharing the driving goals of the user. Furthermore, ACCs that took over driving tasks while providing information were more trustworthy and acceptable than were ACCs that took over driving tasks without providing information. Trustworthiness mediated the effects of both driving goals and automation level on acceptability of ACCs. Conclusion: As when trusting other humans, trusting smart systems depends on those systems sharing the user's goals. Furthermore, based on their description, smart systems that take over tasks are judged more trustworthy and acceptable when they also provide information. Application: For optimal acceptability of smart systems, goals of the user should be shared by the smart systems, and smart systems should provide information to their user. Copyright © 2012, Human Factors and Ergonomics Society.","acceptance; adaptive cruise control systems; automation level; shared value similarity; social trust; system trust","acceptance; Automation levels; Shared values; social trust; system trust; Adaptive cruise control; Computer control; Automation; analysis of variance; article; artificial intelligence; automation; car; car driving; female; human; male; man machine interaction; Netherlands; protective equipment; psychological aspect; traffic accident; trust; Accidents, Traffic; Analysis of Variance; Artificial Intelligence; Automation; Automobile Driving; Automobiles; Female; Humans; Male; Man-Machine Systems; Netherlands; Protective Devices; Trust",Article,Scopus,2-s2.0-84867360941
"Wu B., Qian C., Ni W., Fan S.","Hybrid harmony search and artificial bee colony algorithm for global optimization problems",2012,"Computers and Mathematics with Applications",38,10.1016/j.camwa.2012.06.026,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866732700&doi=10.1016%2fj.camwa.2012.06.026&partnerID=40&md5=2c9a8150c16db1a46694f38182cfd131","Harmony search (HS) is one of the newest and the easiest to code music inspired heuristics for optimization problems. In order to enhance the accuracy and convergence rate of harmony search, a hybrid harmony search is proposed by incorporating the artificial bee colony algorithm (ABC). The artificial bee colony algorithm is a new swarm intelligence technique inspired by intelligent foraging behavior of honey bees. The ABC and its variants are used to improve harmony memory (HM). To compare and analyze the performance of our proposed hybrid algorithms, a number of experiments are carried out on a set of well-known benchmark global optimization problems. The effects of the parameters about the hybrid algorithms are discussed by a uniform design experiment. Numerical results show that the proposed algorithms can find better solutions when compared to HS and other heuristic algorithms and are powerful search algorithms for various global optimization problems. © 2012 Elsevier Ltd. All rights reserved.","Artificial bee colony algorithm; Chaos; Global optimization; Harmony search algorithm; Particle swarm optimization; Uniform design","Artificial bee colony algorithms; Compare and analyze; Convergence rates; Foraging behaviors; Global optimization problems; Harmony search; Harmony search algorithms; Honey bee; Hybrid algorithms; Numerical results; Optimization problems; Search Algorithms; Swarm intelligence techniques; Uniform design; Artificial intelligence; Benchmarking; Chaos theory; Experiments; Heuristic algorithms; Learning algorithms; Particle swarm optimization (PSO); Global optimization",Article,Scopus,2-s2.0-84866732700
"Liu X., Gao C., Li P.","A comparative analysis of support vector machines and extreme learning machines",2012,"Neural Networks",38,10.1016/j.neunet.2012.04.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863876822&doi=10.1016%2fj.neunet.2012.04.002&partnerID=40&md5=2a2eaf22ace6f05f0366ac8fb9982628","The theory of extreme learning machines (ELMs) has recently become increasingly popular. As a new learning algorithm for single-hidden-layer feed-forward neural networks, an ELM offers the advantages of low computational cost, good generalization ability, and ease of implementation. Hence the comparison and model selection between ELMs and other kinds of state-of-the-art machine learning approaches has become significant and has attracted many research efforts. This paper performs a comparative analysis of the basic ELMs and support vector machines (SVMs) from two viewpoints that are different from previous works: one is the Vapnik-Chervonenkis (VC) dimension, and the other is their performance under different training sample sizes. It is shown that the VC dimension of an ELM is equal to the number of hidden nodes of the ELM with probability one. Additionally, their generalization ability and computational complexity are exhibited with changing training sample size. ELMs have weaker generalization ability than SVMs for small sample but can generalize as well as SVMs for large sample. Remarkably, great superiority in computational speed especially for large-scale sample problems is found in ELMs. The results obtained can provide insight into the essential relationship between them, and can also serve as complementary knowledge for their past experimental and theoretical comparisons. © 2012 Elsevier Ltd.","Computational complexity; ELM; Generalization ability; SVM; VC dimension","Comparative analysis; Computational costs; Computational speed; ELM; Extreme learning machine; Generalization ability; Hidden nodes; Learning approach; Model Selection; Research efforts; Small samples; SVM; Training sample; Vapnik-Chervonenkis dimensions; VC dimension; Computational complexity; Learning algorithms; Sampling; Support vector machines; article; controlled study; extreme learning machine; machine learning; priority journal; probability; sample size; support vector machine; theoretical study; Artificial Intelligence; Databases, Factual; Humans; Support Vector Machines",Article,Scopus,2-s2.0-84863876822
"Ho J.-H., Shih H.-C., Liao B.-Y., Chu S.-C.","A ladder diffusion algorithm using ant colony optimization for wireless sensor networks",2012,"Information Sciences",38,10.1016/j.ins.2011.03.013,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857860482&doi=10.1016%2fj.ins.2011.03.013&partnerID=40&md5=7f7351e4b86a0798cce51497de70de26","In this paper, an algorithm based on ladder diffusion and ACO [5,6] is proposed to solve the power consumption and transmission routing problems in wireless sensor networks. The proposed ladder diffusion algorithm is employed to route paths for data relay and transmission in wireless sensor networks, reducing both power consumption and processing time to build the routing table and simultaneously avoiding the generation of circle routes. Moreover, to ensure the safety and reliability of data transmission, our algorithm provides backup routes to avoid wasted power and processing time when rebuilding the routing table in case part of sensor nodes are missing. According to the experimental results, the proposed algorithm not only reduces power consumption by 52.36% but also increases data forwarding efficiency by 61.11% as compared to the directed diffusion algorithm. This decrease is because the algorithm properly assigns the transmission routes to balance the load on every sensor node. © 2010 Elsevier Inc. All rights reserved.",,"Ant Colony Optimization (ACO); Backup routes; Data relays; Data-forwarding; Diffusion algorithm; Directed diffusion; Processing time; Routing problems; Routing table; Transmission route; Artificial intelligence; Data processing; Diffusion; Ladders; Sensor nodes; Algorithms",Article,Scopus,2-s2.0-84857860482
"Keskinturk T., Yildirim M.B., Barut M.","An ant colony optimization algorithm for load balancing in parallel machines with sequence-dependent setup times",2012,"Computers and Operations Research",38,10.1016/j.cor.2010.12.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053372003&doi=10.1016%2fj.cor.2010.12.003&partnerID=40&md5=58c682a890546d4efb2276511b599cd6","This study introduces the problem of minimizing average relative percentage of imbalance (ARPI) with sequence-dependent setup times in a parallel-machine environment. A mathematical model that minimizes ARPI is proposed. Some heuristics, and two metaheuristics, an ant colony optimization algorithm and a genetic algorithm are developed and tested on various random data. The proposed ant colony optimization method outperforms heuristics and genetic algorithm. On the other hand, heuristics using the cumulative processing time obtain better results than heuristics using setup avoidance and a hybrid rule in assignment. © 2010 Elsevier Ltd. All rights reserved.","Ant colony optimization; Genetic algorithm; Heuristics; Load balancing; Parallel-machine scheduling; Sequence-dependent setups","Ant Colony Optimization algorithms; Ant colony optimization methods; Ant-colony optimization; Heuristics; Load-Balancing; Meta heuristics; Parallel machine; Processing Time; Random data; Sequence dependent setups; Sequence-dependent setup time; Genetic algorithms; Mathematical models; Optimization; Parallel architectures; Artificial intelligence",Conference Paper,Scopus,2-s2.0-80053372003
"Ni J., Jin X.","Decision support systems for effective maintenance operations",2012,"CIRP Annals - Manufacturing Technology",38,10.1016/j.cirp.2012.03.065,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861592241&doi=10.1016%2fj.cirp.2012.03.065&partnerID=40&md5=159e6a6709deb0ab53731044d5c38d75","To compete successfully in the market place, leading manufacturing companies are pursuing effective maintenance operations. Existing computerized maintenance management systems (CMMS) can no longer meet the needs of dynamic maintenance operations. This paper describes newly developed decision support tools for effective maintenance operations: (1) data-driven short-term throughput bottleneck identification, (2) estimation of maintenance windows of opportunity, (3) prioritization of maintenance tasks, (4) joint production and maintenance scheduling systems, and (5) maintenance staff management. Mathematical algorithms and simulation tools are utilized to illustrate the concepts of these decision support systems. Results from real implementations in automotive manufacturing are presented to demonstrate the effectiveness of these tools. © 2012 CIRP.","Decision making; Maintenance; Manufacturing systems","Automotive manufacturing; Computerized maintenance management system; Decision support tools; Dynamic maintenances; Joint production; Maintenance operations; Maintenance scheduling; Maintenance staff; Maintenance tasks; Manufacturing companies; Market place; Mathematical algorithms; Prioritization; Artificial intelligence; Decision making; Decision support systems; Maintenance; Manufacture; Scheduling; Throughput; Maintainability",Article,Scopus,2-s2.0-84861592241
"Shen K.-K., Fripp J., Mériaudeau F., Chételat G., Salvado O., Bourgeat P.","Detecting global and local hippocampal shape changes in Alzheimer's disease using statistical shape models",2012,"NeuroImage",38,10.1016/j.neuroimage.2011.10.014,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855497815&doi=10.1016%2fj.neuroimage.2011.10.014&partnerID=40&md5=6fbfbac0e34bc6c9610268eddae57e33","The hippocampus is affected at an early stage in the development of Alzheimer's disease (AD). With the use of structural magnetic resonance (MR) imaging, we can investigate the effect of AD on the morphology of the hippocampus. The hippocampal shape variations among a population can be usually described using statistical shape models (SSMs). Conventional SSMs model the modes of variations among the population via principal component analysis (PCA). Although these modes are representative of variations within the training data, they are not necessarily discriminative on labeled data or relevant to the differences between the subpopulations. We use the shape descriptors from SSM as features to classify AD from normal control (NC) cases. In this study, a Hotelling's T 2 test is performed to select a subset of landmarks which are used in PCA. The resulting variation modes are used as predictors of AD from NC. The discrimination ability of these predictors is evaluated in terms of their classification performances with bagged support vector machines (SVMs). Restricting the model to landmarks with better separation between AD and NC increases the discrimination power of SSM. The predictors extracted on the subregions also showed stronger correlation with the memory-related measurements such as Logical Memory, Auditory Verbal Learning Test (AVLT) and the memory subscores of Alzheimer Disease Assessment Scale (ADAS). © 2011.",,"Alzheimer disease; Alzheimer Disease Assessment Scale; article; Auditory Verbal Learning Test; brain region; clinical assessment; controlled study; correlation analysis; discrimination learning; disease classification; hippocampus; human; Logical Memory; principal component analysis; priority journal; statistical model; statistical shape model; Student t test; support vector machine; Aged; Alzheimer Disease; Artificial Intelligence; Atrophy; Brain Mapping; Data Interpretation, Statistical; Databases, Factual; Educational Status; Female; Functional Laterality; Hippocampus; Humans; Magnetic Resonance Imaging; Male; Memory Disorders; Mental Recall; Models, Anatomic; Models, Statistical; Neuropsychological Tests; Principal Component Analysis; Support Vector Machines; Verbal Learning",Article,Scopus,2-s2.0-84855497815
"Malley J.D., Kruppa J., Dasgupta A., Malley K.G., Ziegler A.","Probability Machines: Consistent probability estimation using nonparametric learning machines",2012,"Methods of Information in Medicine",38,10.3414/ME00-01-0052,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855764322&doi=10.3414%2fME00-01-0052&partnerID=40&md5=892af46d89077fad162dea50fa266516","Background: Most machine learning approaches only provide a classification for binary responses. However, probabilities are required for risk estimation using individual patient characteristics. It has been shown recently that every statistical learning machine known to be consistent for a nonparametric regression problem is a probability machine that is provably consistent for this estimation problem. Objectives: The aim of this paper is to show how random forests and nearest neighbors can be used for consistent estimation of individual probabilities. Methods: Two random forest algorithms and two nearest neighbor algorithms are described in detail for estimation of individual probabilities. We discuss the consistency of random forests, nearest neighbors and other learning machines in detail. We conduct a simulation study to illustrate the validity of the methods. We exemplify the algorithms by analyzing two well-known data sets on the diagnosis of appendicitis and the diagnosis of diabetes in Pima Indians. Results: Simulations demonstrate the validity of the method. With the real data application, we show the accuracy and practicality of this approach. We provide sample code from R packages in which the probability estimation is already available. This means that all calculations can be performed using existing software. Conclusions: Random forest algorithms as well as nearest neighbor approaches are valid machine learning methods for estimating individual probabilities for binary responses. Freely available implementations are available in R and may be used for applications. © Schattauer 2012.","Brier score; Consistency; K nearest neighbor; Logistic regression; Probability estimation; Random forest","article; artificial intelligence; computer simulation; human; learning; methodology; nonparametric test; probability; statistical model; statistics; Artificial Intelligence; Computer Simulation; Humans; Learning; Logistic Models; Models, Statistical; Probability; Statistics as Topic; Statistics, Nonparametric",Article,Scopus,2-s2.0-84855764322
"Thimm M.","A probabilistic semantics for abstract argumentation",2012,"Frontiers in Artificial Intelligence and Applications",38,10.3233/978-1-61499-098-7-750,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878777210&doi=10.3233%2f978-1-61499-098-7-750&partnerID=40&md5=570ac44ccca08e45f5c798d50bfa428b","Classical semantics for abstract argumentation frameworks are usually defined in terms of extensions or, more recently, labelings. That is, an argument is either regarded as accepted with respect to a labeling or not. In order to reason with a specific semantics one takes either a credulous or skeptical approach, i. e. an argument is ultimately accepted, if it is accepted in one or all labelings, respectively. In this paper, we propose a more general approach for a semantics that allows for a more fine-grained differentiation between those two extreme views on reasoning. In particular, we propose a probabilistic semantics for abstract argumentation that assigns probabilities or degrees of belief to individual arguments. We show that our semantics generalizes the classical notions of semantics and we point out interesting relationships between concepts from argumentation and probabilistic reasoning. We illustrate the usefulness of our semantics on an example from the medical domain. © 2012 The Author(s).",,"Artificial intelligence; Problem solving; Abstract argumentation; Classical semantics; Fine grained; Labelings; Medical domains; Probabilistic reasoning; Probabilistic semantics; Specific semantics; Semantics",Conference Paper,Scopus,2-s2.0-84878777210
"Baumann R.","What does it take to enforce an argument? Minimal change in abstract argumentation",2012,"Frontiers in Artificial Intelligence and Applications",38,10.3233/978-1-61499-098-7-127,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878786504&doi=10.3233%2f978-1-61499-098-7-127&partnerID=40&md5=672e2d2b2c7c9d6a05ef0fdb0856c918","Argumentation is a dynamic process. The enforcing problem in argumentation, i.e. the question whether it is possible to modify a given argumentation framework (AF) in such a way that a desired set of arguments becomes an extension or a subset of an extension, was first studied in and positively answered under certain conditions. In this paper, we take up this research and study the more general problem of minimal change. That is, in brief, i) is it possible to enforce a desired set of arguments, and if so, ii) what is the minimal number of modifications (additions or removals of attacks) to reach such an enforcement, the so-called characteristic. We show for several Dung semantics that this problem can be decided by local criteria encoded by the so-called value functions. Furthermore, we introduce the corresponding equivalence notions between two AFs which guarantee equal minimal efforts needed to enforce certain subsets, namely minimal-E-equivalence and the more general minimal change equivalence. We present characterization theorems for several Dung semantics and finally, we show the relations to standard and the recently proposed strong equivalence for a whole range of semantics. © 2012 The Author(s).",,"Artificial intelligence; Semantics; Abstract argumentation; Argumentation frameworks; Characterization theorems; Dynamic process; Equivalence notions; Value functions; Problem solving",Conference Paper,Scopus,2-s2.0-84878786504
"Jin X., Sandhu R., Krishnan R.","RABAC: Role-centric attribute-based access control",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",37,10.1007/978-3-642-33704-8-8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881151887&doi=10.1007%2f978-3-642-33704-8-8&partnerID=40&md5=363a77fa4a3065586708c73b0d3d5ada","Role-based access control (RBAC) is a commercially dominant model, standardized by the National Institute of Standards and Technology (NIST). Although RBAC provides compelling benefits for security management it has several known deficiencies such as role explosion, wherein multiple closely related roles are required (e.g., attending-doctor role is separately defined for each patient). Numerous extensions to RBAC have been proposed to overcome these shortcomings. Recently NIST announced an initiative to unify and standardize these extensions by integrating roles with attributes, and identified three approaches: use attributes to dynamically assign users to roles, treat roles as just another attribute, and constrain the permissions of a role via attributes. The first two approaches have been previously studied. This paper presents a formal model for the third approach for the first time in the literature. We propose the novel role-centric attribute-based access control (RABAC) model which extends the NIST RBAC model with permission filtering policies. Unlike prior proposals addressing the role-explosion problem, RABAC does not fundamentally modify the role concept and integrates seamlessly with the NIST RBAC model. We also define an XACML profile for RABAC based on the existing XACML profile for RBAC. © 2012 Springer-Verlag Berlin Heidelberg.","access control; attribute; NIST-RBAC; XACML","attribute; Attribute based access control; Formal model; National Institute of Standards and Technology; NIST-RBAC; Role-based Access Control; Security management; XACML; Artificial intelligence; Computer science; Access control",Conference Paper,Scopus,2-s2.0-84881151887
"Zou A.-M., Kumar K.D.","Neural network-based distributed attitude coordination control for spacecraft formation flying with input saturation",2012,"IEEE Transactions on Neural Networks and Learning Systems",37,10.1109/TNNLS.2012.2196710,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875746716&doi=10.1109%2fTNNLS.2012.2196710&partnerID=40&md5=ca68f048a5d665adfe04e0bfdca80098","This brief considers the attitude coordination control problem for spacecraft formation flying when only a subset of the group members has access to the common reference attitude. A quaternion-based distributed attitude coordination control scheme is proposed with consideration of the input saturation and with the aid of the sliding-mode observer, separation principle theorem, Chebyshev neural networks, smooth projection algorithm, and robust control technique. Using graph theory and a Lyapunov-based approach, it is shown that the distributed controller can guarantee the attitude of all spacecraft to converge to a common time-varying reference attitude when the reference attitude is available only to a portion of the group of spacecraft. Numerical simulations are presented to demonstrate the performance of the proposed distributed controller. © 2012 IEEE.","Attitude coordination control; Chebyshev neural networks; control input saturation; quaternion; spacecraft formation flying","Chebyshev neural networks; Control input saturation; Coordination control; quaternion; Spacecraft formation flying; Graph theory; Neural networks; Robust control; Spacecraft; Spacecraft propulsion; Controllers; algorithm; artificial intelligence; artificial neural network; attitude; computer simulation; signal processing; space flight; standards; Algorithms; Artificial Intelligence; Attitude; Computer Simulation; Neural Networks (Computer); Signal Processing, Computer-Assisted; Spacecraft",Article,Scopus,2-s2.0-84875746716
"Ling X., Weld D.S.","Fine-grained entity recognition",2012,"Proceedings of the National Conference on Artificial Intelligence",37,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868276493&partnerID=40&md5=f65b2512aac0946b23d0ace9ce4aa909","Entity Recognition (ER) is a key component of relation extraction systems and many other natural-language processing applications. Unfortunately, most ER systems are restricted to produce labels from to a small set of entity classes, e.g., person, organization, location or miscellaneous. In order to intelligently understand text and extract a wide range of information, it is useful to more precisely determine the semantic classes of entities mentioned in unstructured text. This paper defines a fine-grained set of 112 tags, formulates the tagging problem as multi-class, multi-label classification, describes an unsupervised method for collecting training data, and presents the FIGER implementation. Experiments show that the system accurately predicts the tags for entities. Moreover, it provides useful information for a relation extraction system, increasing the F1 score by 93%. We make FIGER and its data available as a resource for future work. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Entity recognition; ITS data; Multi-class; Multi-label; Processing applications; Relation extraction; Semantic class; Tagging problem; Training data; Unsupervised method; Artificial intelligence; Classification (of information); Semantics; Natural language processing systems",Conference Paper,Scopus,2-s2.0-84868276493
"Trzcinski T., Lepetit V.","Efficient discriminative projections for compact binary descriptors",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",37,10.1007/978-3-642-33718-5_17,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867893100&doi=10.1007%2f978-3-642-33718-5_17&partnerID=40&md5=a788e49637829e3c83f528038c4a3ec4","Binary descriptors of image patches are increasingly popular given that they require less storage and enable faster processing. This, however, comes at a price of lower recognition performances. To boost these performances, we project the image patches to a more discriminative subspace, and threshold their coordinates to build our binary descriptor. However, applying complex projections to the patches is slow, which negates some of the advantages of binary descriptors. Hence, our key idea is to learn the discriminative projections so that they can be decomposed into a small number of simple filters for which the responses can be computed fast. We show that with as few as 32 bits per descriptor we outperform the state-of-the-art binary descriptors in terms of both accuracy and efficiency. © 2012 Springer-Verlag.",,"Descriptors; Image patches; Recognition performance; Artificial intelligence; Computer vision",Conference Paper,Scopus,2-s2.0-84867893100
"Liu F., Sun J., Si J., Guo W., Mei S.","A boundedness result for the direct heuristic dynamic programming",2012,"Neural Networks",37,10.1016/j.neunet.2012.02.005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862811991&doi=10.1016%2fj.neunet.2012.02.005&partnerID=40&md5=d5f89336dd761edf022a794c54dea219","Approximate/adaptive dynamic programming (ADP) has been studied extensively in recent years for its potential scalability to solve large state and control space problems, including those involving continuous states and continuous controls. The applicability of ADP algorithms, especially the adaptive critic designs has been demonstrated in several case studies. Direct heuristic dynamic programming (direct HDP) is one of the ADP algorithms inspired by the adaptive critic designs. It has been shown applicable to industrial scale, realistic and complex control problems. In this paper, we provide a uniformly ultimately boundedness (UUB) result for the direct HDP learning controller under mild and intuitive conditions. By using a Lyapunov approach we show that the estimation errors of the learning parameters or the weights in the action and critic networks remain UUB. This result provides a useful controller convergence guarantee for the first time for the direct HDP design. © 2012 Elsevier Ltd.","Approximate dynamic programming (ADP); Direct heuristic dynamic programming (direct HDP); Lyapunov stability; Uniformly ultimately boundedness (UUB)","Adaptive critic designs; Approximate dynamic programming; Boundedness; Complex control problems; Continuous control; Continuous state; Control spaces; Critic network; Direct heuristic dynamic programming; Direct heuristic dynamic programming (direct HDP); Estimation errors; Industrial scale; Learning controllers; Learning parameters; Lyapunov approach; Lyapunov stability; Control theory; Controllers; Dynamic programming; Heuristic algorithms; accuracy; analytical error; article; artificial neural network; controlled study; direct heuristic dynamic programming; intermethod comparison; learning algorithm; linear system; mathematical computing; mathematical model; nonlinear system; online system; priority journal; process development; Algorithms; Artificial Intelligence; Neural Networks (Computer); Neurons; Online Systems; Programming, Linear",Article,Scopus,2-s2.0-84862811991
"Basilico N., Gatti N., Amigoni F.","Patrolling security games: Definition and algorithms for solving large instances with single patroller and single intruder",2012,"Artificial Intelligence",37,10.1016/j.artint.2012.03.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859701666&doi=10.1016%2fj.artint.2012.03.003&partnerID=40&md5=e08963ec57735041748e0718573a2ad2","Security games are gaining significant interest in artificial intelligence. They are characterized by two players (a defender and an attacker) and by a set of targets the defender tries to protect from the attacker's intrusions by committing to a strategy. To reach their goals, players use resources such as patrollers and intruders. Security games are Stackelberg games where the appropriate solution concept is the leader-follower equilibrium. Current algorithms for solving these games are applicable when the underlying game is in normal form (i.e., each player has a single decision node). In this paper, we define and study security games with an extensive-form infinite-horizon underlying game, where decision nodes are potentially infinite. We introduce a novel scenario where the attacker can undertake actions during the execution of the defender's strategy. We call this new game class patrolling security games (PSGs), since its most prominent application is patrolling environments against intruders. We show that PSGs cannot be reduced to security games studied so far and we highlight their generality in tackling adversarial patrolling on arbitrary graphs. We then design algorithms to solve large instances with single patroller and single intruder. © 2012 Elsevier B.V. All rights reserved.","Adversarial patrolling; Algorithmic game theory; Security games","Adversarial patrolling; Algorithmic Game Theory; Arbitrary graphs; Infinite horizons; Leader-follower; Normal form; Security games; Single decision; Solution concepts; Stackelberg Games; Artificial intelligence; Game theory; Algorithms",Article,Scopus,2-s2.0-84859701666
"Lang S., Block M., Rojas R.","Sign language recognition using kinect",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",37,10.1007/978-3-642-29347-4_46,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861029915&doi=10.1007%2f978-3-642-29347-4_46&partnerID=40&md5=d7cbb1ebe565385e87ec68766e304436","An open source framework for general gesture recognition is presented and tested with isolated signs of sign language. Other than common systems for sign language recognition, this framework makes use of Kinect, a depth camera which makes real-time 3D-reconstruction easily applicable. Recognition is done using hidden Markov models with a continuous observation density. The framework also offers an easy way of initializing and training new gestures or signs by performing them several times in front of the camera. First results with a recognition rate of 97% show that depth cameras are well-suited for sign language recognition. © 2012 Springer-Verlag Berlin Heidelberg.",,"3D reconstruction; Continuous observation; Depth camera; Open source frameworks; Recognition rates; Sign language; Sign Language recognition; Artificial intelligence; Cameras; Gesture recognition; Hidden Markov models; Soft computing; Three dimensional",Conference Paper,Scopus,2-s2.0-84861029915
"Soeken M., Frehse S., Wille R., Drechsler R.","RevKit: An open source toolkit for the design of reversible circuits",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",37,10.1007/978-3-642-29517-1_6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860807692&doi=10.1007%2f978-3-642-29517-1_6&partnerID=40&md5=8de2bbc57863fed08777a7d23bd77de4","In recent years, research in the domain of reversible circuit design has attracted significant attention leading to many different approaches e.g. for synthesis, optimization, simulation, verification, and test. The open source toolkit RevKit is an attempt to make these developments publicly available to other researchers. For this purpose, a modular and extendable framework has been provided which easily enables the addition of new methods and tools. In this paper, we introduce the functionality as well as the internals of RevKit. We provide examples and use cases showing how to apply RevKit and its components in order to create and execute customized design flows. Furthermore, we demonstrate how the architecture and the design concepts of RevKit can be exploited to easily develop new or improved methods for reversible circuit design. © 2012 Springer-Verlag Berlin Heidelberg.",,"Design concept; Design flows; Open sources; Reversible circuits; Artificial intelligence; Design",Conference Paper,Scopus,2-s2.0-84860807692
"Trung T.Q., Tien N.T., Seol Y.G., Lee N.-E.","Transparent and flexible organic field-effect transistor for multi-modal sensing",2012,"Organic Electronics: physics, materials, applications",37,10.1016/j.orgel.2011.12.015,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856504168&doi=10.1016%2fj.orgel.2011.12.015&partnerID=40&md5=a366b614b3f5abed15a8e86962924267","Multi-functional devices having responsiveness to multiple physical stimuli, mechanical flexibility and optical transparency are of great interest in applications for human-machine interfaces. Here we demonstrate transparent and flexible organic field effect transistor (tf-OFET) devices with multi-modal sensing capability of detecting infrared (IR) light, pressure, and strain simultaneously. The multi-modal sensing layer with piezoelectricity and pyroelectricity was directly integrated into OFETs as the gate dielectric so that a new type of multi-modal sensing device with simple structure having possibility of increasing the sensor cell density can be easily fabricated. For decoupling of pyro- and piezoelectric responses in a single device under simultaneous stimulations of IR exposure and strain, an approach of determining two input stimuli by separating the polarization changes inside the gate dielectric (V o) and the modulation in the product of effective field-effect channel mobility and gate capacitance (μC). The high sensitivity of the devices to IR from human body may also enable the devices to be applied for the realization of artificial intelligence that contacts directly with human body such as artificial e-skin, biomedical monitoring, and tactile sensing. © 2012 Elsevier B.V. All rights reserved.","Flexible; Multi-modal; OFET; Sensor; Transparent","Biomedical monitoring; Channel mobility; Field-effect; Flexible; Gate capacitance; High sensitivity; Human bodies; Human Machine Interface; Mechanical flexibility; Multi-modal; Multifunctional devices; Optical transparency; Piezoelectric response; Sensing devices; Sensing layers; Sensor cells; Simple structures; Tactile sensing; Transparent; Artificial intelligence; Crystallography; Field effect transistors; Gate dielectrics; Organic field effect transistors; Piezoelectricity; Sensors; Equipment",Article,Scopus,2-s2.0-84856504168
"Cappelli R., Ferrara M., Maio D.","A fast and accurate palmprint recognition system based on minutiae",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",37,10.1109/TSMCB.2012.2183635,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861189481&doi=10.1109%2fTSMCB.2012.2183635&partnerID=40&md5=7919808ee7f8f9d49a97e1228c85a0b5","Palmprint recognition is a challenging problem, mainly due to low quality of the pattern, large nonlinear distortion between different impressions of the same palm and large image size, which makes feature extraction and matching computationally demanding. This paper introduces a high-resolution palmprint recognition system based on minutiae. The proposed system follows the typical sequence of steps used in fingerprint recognition, but each step has been specifically designed and optimized to process large palmprint images with a good tradeoff between accuracy and speed. A sequence of robust feature extraction steps allows to reliably detect minutiae; moreover, the matching algorithm is very efficient and robust to skin distortion, being based on a local matching strategy and an efficient and compact representation of the minutiae. Experimental results show that the proposed system compares very favorably with the state of the art. © 2012 IEEE.","Local matching; Minutia Cylinder-Code (MCC); minutiae; palmprint; relaxation","Local matching; Minutia; minutiae; Palmprints; relaxation; Anthropometry; Feature extraction; Image matching; artificial intelligence; automated pattern recognition; biometry; computer assisted diagnosis; hand; histology; human; image subtraction; letter; methodology; Artificial Intelligence; Biometry; Hand; Humans; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Subtraction Technique",Article,Scopus,2-s2.0-84861189481
"Swatantran A., Dubayah R., Goetz S., Hofton M., Betts M.G., Sun M., Simard M., Holmes R.","Mapping migratory bird prevalence using remote sensing data fusion",2012,"PLoS ONE",37,10.1371/journal.pone.0028922,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855315676&doi=10.1371%2fjournal.pone.0028922&partnerID=40&md5=c0204496053dd79ca965394c74739e9c","Background: Improved maps of species distributions are important for effective management of wildlife under increasing anthropogenic pressures. Recent advances in lidar and radar remote sensing have shown considerable potential for mapping forest structure and habitat characteristics across landscapes. However, their relative efficacies and integrated use in habitat mapping remain largely unexplored. We evaluated the use of lidar, radar and multispectral remote sensing data in predicting multi-year bird detections or prevalence for 8 migratory songbird species in the unfragmented temperate deciduous forests of New Hampshire, USA. Methodology and Principal Findings: A set of 104 predictor variables describing vegetation vertical structure and variability from lidar, phenology from multispectral data and backscatter properties from radar data were derived. We tested the accuracies of these variables in predicting prevalence using Random Forests regression models. All data sets showed more than 30% predictive power with radar models having the lowest and multi-sensor synergy (""fusion"") models having highest accuracies. Fusion explained between 54% and 75% variance in prevalence for all the birds considered. Stem density from discrete return lidar and phenology from multispectral data were among the best predictors. Further analysis revealed different relationships between the remote sensing metrics and bird prevalence. Spatial maps of prevalence were consistent with known habitat preferences for the bird species. Conclusion and Significance: Our results highlight the potential of integrating multiple remote sensing data sets using machine-learning methods to improve habitat mapping. Multi-dimensional habitat structure maps such as those generated from this study can significantly advance forest management and ecological research by facilitating fine-scale studies at both stand and landscape level. © 2012 Swatantran et al.",,"accuracy; article; controlled study; habitat structure; migratory species; nonhuman; phenology; population density; predictor variable; prevalence; random forest; remote sensing; songbird; species distribution; species habitat; temperate deciduous forest; United States; vegetation; animal; artificial intelligence; ecosystem; methodology; population migration; statistics; telecommunication; time; tree; Aves; Passeri; Animal Migration; Animals; Artificial Intelligence; Ecosystem; Radar; Songbirds; Statistics as Topic; Time Factors; Trees",Article,Scopus,2-s2.0-84855315676
"Colton S., Wiggins G.A.","Computational creativity: The final frontier?",2012,"Frontiers in Artificial Intelligence and Applications",37,10.3233/978-1-61499-098-7-21,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878772256&doi=10.3233%2f978-1-61499-098-7-21&partnerID=40&md5=36de34947f389336f752892c73d19335","Notions relating to computational systems exhibiting creative behaviours have been explored since the very early days of computer science, and the field of Computational Creativity research has formed in the last dozen years to scientifically explore the potential of such systems. We describe this field via a working definition; a brief history of seminal work; an exploration of the main issues, technologies and ideas; and a look towards future directions. As a society, we are jealous of our creativity: creative people and their contributions to cultural progression are highly valued. Moreover, creative behaviour in people draws on a full set of intelligent abilities, so simulating such behaviour represents a serious technical challenge for Artificial Intelligence research. As such, we believe it is fair to characterise Computational Creativity as a frontier for AI research beyond all others-maybe, even, the final frontier. © 2012 The Author(s).",,"Artificial intelligence; Artificial intelligence research; Computational creativities; Computational system; Technical challenges; Behavioral research",Conference Paper,Scopus,2-s2.0-84878772256
"Mukhopadhyay S., Banerjee S.","Global optimization of an optical chaotic system by Chaotic Multi Swarm Particle Swarm Optimization",2012,"Expert Systems with Applications",37,10.1016/j.eswa.2011.07.089,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-81855164795&doi=10.1016%2fj.eswa.2011.07.089&partnerID=40&md5=ccfe7b3eb684480916b5d2c949925abd","The control and estimation of unknown parameters of chaotic systems are a daunting task till date from the perspective of nonlinear science. Inspired from ecological co-habitation, we propose a variant of Particle Swarm Optimization (PSO), known as Chaotic Multi Swarm Particle Swarm Optimization (CMS-PSO), by modifying the generic PSO with the help of the chaotic sequence for multi-dimension unknown parameter estimation and optimization by forming multiple cooperating swarms. This achieves load balancing by delegating the global optimizing task to concurrently operating swarms. We apply it successfully in estimating the unknown parameters of an autonomous chaotic laser system derived from Maxwell-Bloch equations. Numerical results and comparison demonstrate that for the given system parameters, CMS-PSO can identify the optimized parameters effectively evolving at each iteration to attain the pareto optimal solution with small population size and enhanced convergence speedup. © 2011 Elsevier Ltd. All rights reserved.","Chaotic sequences; Computational intelligence; Global optimization; Laser; Multi dimension; Parameter estimation; Particle swarm optimization","Chaotic lasers; Chaotic sequence; Convergence speedup; Estimation and optimization; Maxwell-Bloch equation; Multi dimension; Multi-swarms; Nonlinear science; Numerical results; Optimized parameter; Pareto optimal solutions; Particle swarm; Small population; Unknown parameters; Artificial intelligence; Chaotic systems; Convergence of numerical methods; Estimation; Global optimization; Lasers; Maxwell equations; Observability; Parameter estimation; Population statistics; Particle swarm optimization (PSO)",Article,Scopus,2-s2.0-81855164795
"Yu B., Yang Z.-Z., Jin P.-H., Wu S.-H., Yao B.-Z.","Transit route network design-maximizing direct and transfer demand density",2012,"Transportation Research Part C: Emerging Technologies",37,10.1016/j.trc.2011.12.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862776824&doi=10.1016%2fj.trc.2011.12.003&partnerID=40&md5=c708eef0400283e6c938f71a12435d91","Transit network design is an important part of urban transportation planning. The purpose of this paper is to build on direct traveler density model and extend it to design transit network considering demand density relating to direct demands and transfers, and lengths of routes. The proposed method aiming to maximize demand density of route under some resource constraints divides transit network design problem into three stages, i.e., skeleton route design, main route design and branch route design, based on the objective functions with different transfer coefficients. An ant colony optimization (ACO) is used to solve the model. The model and algorithm are illustrated with data from Dalian city, China and results show that the approach can improve the solution quality if the transfer coefficient is reasonably set. © 2011 Elsevier Ltd.","ACO; Demand density; Transfer coefficient; Transit network design","Ant colony optimization; Artificial intelligence; Transportation; Transportation routes; Urban transportation; Ant Colony Optimization (ACO); Model and algorithms; Objective functions; Resource Constraint; Transfer coefficient; Transit network design; Transit route networks; Transportation planning; Mass transportation; algorithm; demand analysis; network design; numerical model; transportation planning; urban planning; urban transport; China; Dalian; Liaoning",Article,Scopus,2-s2.0-84862776824
"Semenkin E., Semenkina M.","Self-configuring genetic algorithm with modified uniform crossover operator",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",36,10.1007/978-3-642-30976-2_50,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875119901&doi=10.1007%2f978-3-642-30976-2_50&partnerID=40&md5=3d8e1cd4a4830ca4724f686fbc1237da","For genetic algorithms, new variants of the uniform crossover operator that introduce selective pressure on the recombination stage are proposed. Operator probabilistic rates based approach to genetic algorithms self-configuration is suggested. The usefulness of the proposed modifications is demonstrated on benchmark tests and real world problems. © 2012 Springer-Verlag.","genetic algorithms; performance comparison; selective pressure recombination; self-configuration; uniform crossover","Benchmark tests; Performance comparison; Real-world problem; Selective pressure; Self configuration; Self-configuring genetic algorithms; Uniform crossover operator; Uniform crossovers; Artificial intelligence; Benchmarking; Genetic algorithms; Probabilistic logics",Conference Paper,Scopus,2-s2.0-84875119901
"Niepert M.","Markov chains on orbits of permutation groups",2012,"Uncertainty in Artificial Intelligence - Proceedings of the 28th Conference, UAI 2012",36,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877769953&partnerID=40&md5=8d85961f8b8062b4ce73f872422c1a04","We present a novel approach to detecting and utilizing symmetries in probabilistic graphical models with two main contributions. First, we present a scalable approach to computing generating sets of permutation groups representing the symmetries of graphical models. Second, we introduce orbital Markov chains, a novel family of Markov chains leveraging model symmetries to reduce mixing times. We establish an insightful connection between model symmetries and rapid mixing of orbital Markov chains. Thus, we present the first lifted MCMC algorithm for probabilistic graphical models. Both analytical and empirical results demonstrate the effectiveness and efficiency of the approach.",,"Effectiveness and efficiencies; GraphicaL model; Leveraging model; MCMC algorithms; Model symmetries; Permutation group; Probabilistic graphical models; Scalable approach; Artificial intelligence; Mixing; Markov processes",Conference Paper,Scopus,2-s2.0-84877769953
"Chen W., Lu W., Zhang N.","Time-critical influence maximization in social networks with time-delayed diffusion process",2012,"Proceedings of the National Conference on Artificial Intelligence",36,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868268648&partnerID=40&md5=379c239a519294793d7763c3d50dacb1","Influence maximization is a problem of finding a small set of highly influential users in a social network such that the spread of influence under certain propagation models is maximized. In this paper, we consider time-critical influence maximization, in which one wants to maximize influence spread within a given deadline. Since timing is considered in the optimization, we also extend the Independent Cascade (IC) model to incorporate the time delay aspect of influence diffusion in social networks. We show that time-critical influence maximization under the time-delayed IC model maintains desired properties such as submodularity, which allows a greedy algorithm to achieve an approximation ratio of 1 - 1/e, to circumvent the NP-hardness of the problem. To overcome the inefficiency of the approximation algorithm, we design two heuristic algorithms: the first one is based on a dynamic programming procedure that computes exact influence in tree structures, while the second one converts the problem to one in the original IC model and then applies existing fast heuristics to it. Our simulation results demonstrate that our heuristics achieve the same level of influence spread as the greedy algorithm while running a few orders of magnitude faster, and they also outperform existing algorithms that disregard the deadline constraint and delays in diffusion. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Approximation ratios; Deadline constraint; Diffusion process; Greedy algorithms; IC-Models; Influence maximizations; NP-hardness; Orders of magnitude; Propagation models; Social Networks; Submodularity; Tree structures; Artificial intelligence; Diffusion; Heuristic algorithms; Social networking (online); Time delay; Trees (mathematics); Approximation algorithms",Conference Paper,Scopus,2-s2.0-84868268648
"Sadilek A., Kautz H., Silenzio V.","Predicting disease transmission from geo-tagged micro-blog data",2012,"Proceedings of the National Conference on Artificial Intelligence",36,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868268429&partnerID=40&md5=adeafe49ba93bc3c38d55525b18c1243","Researchers have begun to mine social network data in order to predict a variety of social, economic, and health related phenomena. While previous work has focused on predicting aggregate properties, such as the prevalence of seasonal influenza in a given country, we consider the task of fine-grained prediction of the health of specific people from noisy and incomplete data. We construct a probabilistic model that can predict if and when an individual will fall ill with high precision and good recall on the basis of his social ties and co-locations with other people, as revealed by their Twitter posts. Our model is highly scalable and can be used to predict general dynamic properties of individuals in large real-world social networks. These results provide a foundation for research on fundamental questions of public health, including the identification of non-cooperative disease carriers (""Typhoid Marys""), adaptive vaccination policies, and our understanding of the emergence of global epidemics from day-to-day interpersonal interactions. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Aggregate properties; Disease transmission; General dynamics; Global epidemic; High precision; Incomplete data; Micro-blog; Non-cooperative; Probabilistic models; Social Networks; Social ties; Artificial intelligence; Social networking (online); Forecasting",Conference Paper,Scopus,2-s2.0-84868268429
"Hadžiosmanović D., Simionato L., Bolzoni D., Zambon E., Etalle S.","N-gram against the machine: On the feasibility of the N-gram network analysis for binary protocols",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",36,10.1007/978-3-642-33338-5_18,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867893847&doi=10.1007%2f978-3-642-33338-5_18&partnerID=40&md5=b811eda6150e680cda2df10bf90214bd","In recent years we have witnessed several complex and high-impact attacks specifically targeting ""binary"" protocols (RPC, Samba and, more recently, RDP). These attacks could not be detected by current - signature-based - detection solutions, while - at least in theory - they could be detected by state-of-the-art anomaly-based systems. This raises once again the still unanswered question of how effective anomaly-based systems are in practice. To contribute to answering this question, in this paper we investigate the effectiveness of a widely studied category of network intrusion detection systems: anomaly-based algorithms using n-gram analysis for payload inspection. Specifically, we present a thorough analysis and evaluation of several detection algorithms using variants of n-gram analysis on real-life environments. Our tests show that the analyzed systems, in presence of data with high variability, cannot deliver high detection and low false positive rates at the same time. © 2012 Springer-Verlag.",,"Analysis and evaluation; Binary protocols; Detection algorithm; False positive rates; High variability; Network intrusion detection systems; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84867893847
"Yang P., Li X.-L., Mei J.-P., Kwoh C.-K., Ng S.-K.","Positive-unlabeled learning for disease gene identification",2012,"Bioinformatics",36,10.1093/bioinformatics/bts504,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870411465&doi=10.1093%2fbioinformatics%2fbts504&partnerID=40&md5=7156de8cefc11c1b56cf1d4a8aa6c1ee","Background: Identifying disease genes from human genome is an important but challenging task in biomedical research. Machine learning methods can be applied to discover new disease genes based on the known ones. Existing machine learning methods typically use the known disease genes as the positive training set P and the unknown genes as the negative training set N (non-disease gene set does not exist) to build classifiers to identify new disease genes from the unknown genes. However, such kind of classifiers is actually built from a noisy negative set N as there can be unknown disease genes in N itself. As a result, the classifiers do not perform as well as they could be. Result: Instead of treating the unknown genes as negative examples in N, we treat them as an unlabeled set U. We design a novel positive-unlabeled (PU) learning algorithm PUDI (PU learning for disease gene identification) to build a classifier using P and U. We first partition U into four sets, namely, reliable negative set RN, likely positive set LP, likely negative set LN and weak negative set WN. The weighted support vector machines are then used to build a multi-level classifier based on the four training sets and positive training set P to identify disease genes. Our experimental results demonstrate that our proposed PUDI algorithm outperformed the existing methods significantly. Conclusion: The proposed PUDI algorithm is able to identify disease genes more accurately by treating the unknown data more appropriately as unlabeled set U instead of negative set N. Given that many machine learning problems in biomedical research do involve positive and unlabeled data instead of negative data, it is possible that the machine learning methods for these problems can be further improved by adopting PU learning methods, as we have done here for disease gene identification. © The Author 2012. Published by Oxford University Press. All rights reserved.",,"algorithm; article; artificial intelligence; diseases; evaluation; gene; genetics; human; support vector machine; Algorithms; Artificial Intelligence; Disease; Genes; Humans; Support Vector Machines",Article,Scopus,2-s2.0-84870411465
"Sharma A., Imoto S., Miyano S., Sharma V.","Null space based feature selection method for gene expression data",2012,"International Journal of Machine Learning and Cybernetics",36,10.1007/s13042-011-0061-9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868119274&doi=10.1007%2fs13042-011-0061-9&partnerID=40&md5=5d26e1407c81664048b8b1654f23ee3e","Feature selection is quite an important process in gene expression data analysis. Feature selection methods discard unimportant genes from several thousands of genes for finding important genes or pathways for the target biological phenomenon like cancer. The obtained gene subset is used for statistical analysis for prediction such as survival as well as functional analysis for understanding biological characteristics. In this paper we propose a null space based feature selection method for gene expression data in terms of supervised classification. The proposed method discards the redundant genes by applying the information of null space of scatter matrices. We derive the method theoretically and demonstrate its effectiveness on several DNA gene expression datasets. The method is easy to implement and computationally efficient. © 2011 Springer-Verlag.","Biological significance; Classification accuracy; DNA microarray gene expression data; Feature selection; Null space","Biological characteristic; Biological phenomena; Biological significance; Classification accuracy; Computationally efficient; DNA micro-array; Feature selection methods; Gene Expression Data; Gene expression data analysis; Gene expression datasets; Null space; Scatter matrix; Supervised classification; Artificial intelligence; Feature extraction; Software engineering; Gene expression",Article,Scopus,2-s2.0-84868119274
"Claes J., Vanderfeesten I., Reijers H.A., Pinggera J., Weidlich M., Zugal S., Fahland D., Weber B., Mendling J., Poels G.","Tying process model quality to the modeling process: The impact of structuring, movement, and speed",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",36,10.1007/978-3-642-32885-5_3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866375928&doi=10.1007%2f978-3-642-32885-5_3&partnerID=40&md5=c0b54fd1812bb52a25183045e550475e","In an investigation into the process of process modeling, we examined how modeling behavior relates to the quality of the process model that emerges from that. Specifically, we considered whether (i) a modeler's structured modeling style, (ii) the frequency of moving existing objects over the modeling canvas, and (iii) the overall modeling speed is in any way connected to the ease with which the resulting process model can be understood. In this paper, we describe the exploratory study to build these three conjectures, clarify the experimental set-up and infrastructure that was used to collect data, and explain the used metrics for the various concepts to test the conjectures empirically. We discuss various implications for research and practice from the conjectures, all of which were confirmed by the experiment. © 2012 Springer-Verlag.","business process modeling; empirical research; modeling process; process model quality","Business process modeling; Empirical research; Experimental setup; Exploratory studies; Modeling behavior; Modeling process; Process model; Process Modeling; Structured modeling; Artificial intelligence; Enterprise resource management",Conference Paper,Scopus,2-s2.0-84866375928
"Kurtek S., Klassen E., Gore J.C., Ding Z., Srivastava A.","Elastic geodesic paths in shape space of parameterized surfaces",2012,"IEEE Transactions on Pattern Analysis and Machine Intelligence",36,10.1109/TPAMI.2011.233,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862565528&doi=10.1109%2fTPAMI.2011.233&partnerID=40&md5=815ae31bbea1ebb77cd7908719a7b2e5","This paper presents a novel Riemannian framework for shape analysis of parameterized surfaces. In particular, it provides efficient algorithms for computing geodesic paths which, in turn, are important for comparing, matching, and deforming surfaces. The novelty of this framework is that geodesics are invariant to the parameterizations of surfaces and other shape-preserving transformations of surfaces. The basic idea is to formulate a space of embedded surfaces (surfaces seen as embeddings of a unit sphere in R 3 and impose a Riemannian metric on it in such a way that the reparameterization group acts on this space by isometries. Under this framework, we solve two optimization problems. One, given any two surfaces at arbitrary rotations and parameterizations, we use a path-straightening approach to find a geodesic path between them under the chosen metric. Second, by modifying a technique presented in [CHECK END OF SENTENCE], we solve for the optimal rotation and parameterization (registration) between surfaces. Their combined solution provides an efficient mechanism for computing geodesic paths in shape spaces of parameterized surfaces. We illustrate these ideas using examples from shape analysis of anatomical structures and other general surfaces. © 2012 IEEE.","geodesics; parameterization invariance; path-straightening; Riemannian distance; Shape analysis","Anatomical structures; Arbitrary rotation; Combined solution; Embeddings; Geodesic paths; geodesics; Optimization problems; Parameterizations; Parameterized; path-straightening; Reparameterization; Riemannian distance; Riemannian framework; Riemannian metrics; Shape analysis; Shape space; Shape-preserving; Algorithms; Geodesy; Geometry; Optimization; Parameterization; algorithm; animal; article; artificial intelligence; automated pattern recognition; brain; histology; human; image processing; methodology; nuclear magnetic resonance imaging; Algorithms; Animals; Artificial Intelligence; Brain; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84862565528
"Lien L.-C., Cheng M.-Y.","A hybrid swarm intelligence based particle-bee algorithm for construction site layout optimization",2012,"Expert Systems with Applications",36,10.1016/j.eswa.2012.02.134,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859214537&doi=10.1016%2fj.eswa.2012.02.134&partnerID=40&md5=6391c69f12138e5f36d8e12859bce4d0","The construction site layout (CSL) design presents a particularly interesting area of study because of its relatively high level of attention to usability qualities, in addition to common engineering objectives such as cost and performance. However, it is difficult combinatorial optimization problem for engineers. Swarm intelligence (SI) was very popular and widely used in many complex optimization problems which was collective behavior of social systems such as honey bees (bee algorithm, BA) and birds (particle swarm optimization, PSO). This study proposed an optimization hybrid swarm algorithm namely particle-bee algorithm (PBA) based on a particular intelligent behavior of honey bee and bird swarms by integrates theirs advantages. This study compares the performance of PBA with that of BA and PSO for hypothetical construction engineering of CSL problems. The results show that the performance of PBA is comparable to those of the mentioned algorithms and can be efficiently employed to solve those hypothetical CSL problems with high dimensionality. © 2012 Published by Elsevier Ltd. All rights reserved.","Bee algorithm; Construction site layout; Particle swarm optimization; Particle-bee algorithm; Swarm intelligence","Bee Algorithm; Collective behavior; Combinatorial optimization problems; Complex optimization; Construction engineering; Construction site layout; Construction sites; High dimensionality; Honey bee; Intelligent behavior; Particle swarm; Social systems; Swarm algorithms; Swarm Intelligence; Algorithms; Artificial intelligence; Civil engineering; Combinatorial optimization; Cost engineering; Food products; Packet networks; Particle swarm optimization (PSO)",Article,Scopus,2-s2.0-84859214537
"Gao Y., Kikinis R., Bouix S., Shenton M., Tannenbaum A.","A 3D interactive multi-object segmentation tool using local robust statistics driven active contours",2012,"Medical Image Analysis",36,10.1016/j.media.2012.06.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866122464&doi=10.1016%2fj.media.2012.06.002&partnerID=40&md5=2c402991e34df3fd9d953dc9c675f481","Extracting anatomical and functional significant structures renders one of the important tasks for both the theoretical study of the medical image analysis, and the clinical and practical community. In the past, much work has been dedicated only to the algorithmic development. Nevertheless, for clinical end users, a well designed algorithm with an interactive software is necessary for an algorithm to be utilized in their daily work. Furthermore, the software would better be open sourced in order to be used and validated by not only the authors but also the entire community. Therefore, the contribution of the present work is twofolds: first, we propose a new robust statistics based conformal metric and the conformal area driven multiple active contour framework, to simultaneously extract multiple targets from MR and CT medical imagery in 3. D. Second, an open source graphically interactive 3. D segmentation tool based on the aforementioned contour evolution is implemented and is publicly available for end users on multiple platforms. In using this software for the segmentation task, the process is initiated by the user drawn strokes (seeds) in the target region in the image. Then, the local robust statistics are used to describe the object features, and such features are learned adaptively from the seeds under a non-parametric estimation scheme. Subsequently, several active contours evolve simultaneously with their interactions being motivated by the principles of action and reaction-this not only guarantees mutual exclusiveness among the contours, but also no longer relies upon the assumption that the multiple objects fill the entire image domain, which was tacitly or explicitly assumed in many previous works. In doing so, the contours interact and converge to equilibrium at the desired positions of the desired multiple objects. Furthermore, with the aim of not only validating the algorithm and the software, but also demonstrating how the tool is to be used, we provide the reader reproducible experiments that demonstrate the capability of the proposed segmentation tool on several public available data sets. © 2012 Elsevier B.V.","Active contours; Interactive segmentation; Multiple object segmentation; Open science; Robust statistics","Active contours; Interactive segmentation; Multiple objects; Open science; Robust statistics; Algorithms; Image segmentation; algorithm; article; caudate nucleus; clinical feature; computer assisted tomography; computer program; heart left ventricle; human; image analysis; image processing; image reconstruction; learning algorithm; meningioma; nuclear magnetic resonance imaging; priority journal; quantitative analysis; statistical analysis; three dimensional imaging; tissue structure; validation process; white matter; Algorithms; Artificial Intelligence; Computer Simulation; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Models, Biological; Models, Statistical; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; User-Computer Interface",Article,Scopus,2-s2.0-84866122464
"Meadmore K.L., Hughes A.-M., Freeman C.T., Cai Z., Tong D., Burridge J.H., Rogers E.","Functional electrical stimulation mediated by iterative learning control and 3D robotics reduces motor impairment in chronic stroke",2012,"Journal of NeuroEngineering and Rehabilitation",36,10.1186/1743-0003-9-32,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861859970&doi=10.1186%2f1743-0003-9-32&partnerID=40&md5=7df3e36d28fa1e4b5cd83fe5c6a782e2","Background: Novel stroke rehabilitation techniques that employ electrical stimulation (ES) and robotic technologies are effective in reducing upper limb impairments. ES is most effective when it is applied to support the patients voluntary effort; however, current systems fail to fully exploit this connection. This study builds on previous work using advanced ES controllers, and aims to investigate the feasibility of Stimulation Assistance through Iterative Learning (SAIL), a novel upper limb stroke rehabilitation system which utilises robotic support, ES, and voluntary effort. Methods. Five hemiparetic, chronic stroke participants with impaired upper limb function attended 18, 1 hour intervention sessions. Participants completed virtual reality tracking tasks whereby they moved their impaired arm to follow a slowly moving sphere along a specified trajectory. To do this, the participants arm was supported by a robot. ES, mediated by advanced iterative learning control (ILC) algorithms, was applied to the triceps and anterior deltoid muscles. Each movement was repeated 6 times and ILC adjusted the amount of stimulation applied on each trial to improve accuracy and maximise voluntary effort. Participants completed clinical assessments (Fugl-Meyer, Action Research Arm Test) at baseline and post-intervention, as well as unassisted tracking tasks at the beginning and end of each intervention session. Data were analysed using t-tests and linear regression. Results: From baseline to post-intervention, Fugl-Meyer scores improved, assisted and unassisted tracking performance improved, and the amount of ES required to assist tracking reduced. Conclusions: The concept of minimising support from ES using ILC algorithms was demonstrated. The positive results are promising with respect to reducing upper limb impairments following stroke, however, a larger study is required to confirm this. © 2012 Meadmore et al.","Functional electrical stimulation; Iterative learning control; Robotic support; Stroke rehabilitation; Upper limb; Virtual reality","adult; aged; algorithm; arm; article; artificial intelligence; cerebrovascular accident; computer interface; convalescence; electrostimulation; equipment design; feasibility study; female; human; male; methodology; middle aged; motor dysfunction; movement (physiology); paresis; physiology; psychomotor performance; robotics; self help; skeletal muscle; treatment outcome; Adult; Aged; Algorithms; Arm; Artificial Intelligence; Electric Stimulation; Equipment Design; Feasibility Studies; Female; Humans; Male; Middle Aged; Movement; Movement Disorders; Muscle, Skeletal; Paresis; Psychomotor Performance; Recovery of Function; Robotics; Self-Help Devices; Stroke; Treatment Outcome; Upper Extremity; User-Computer Interface",Article,Scopus,2-s2.0-84861859970
"He J., Gu H., Liu W.","Imbalanced multi-modal multi-label learning for subcellular localization prediction of human proteins with both single and multiple sites",2012,"PLoS ONE",36,10.1371/journal.pone.0037155,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862027781&doi=10.1371%2fjournal.pone.0037155&partnerID=40&md5=790d668c9de55c819520d91a95f46461","It is well known that an important step toward understanding the functions of a protein is to determine its subcellular location. Although numerous prediction algorithms have been developed, most of them typically focused on the proteins with only one location. In recent years, researchers have begun to pay attention to the subcellular localization prediction of the proteins with multiple sites. However, almost all the existing approaches have failed to take into account the correlations among the locations caused by the proteins with multiple sites, which may be the important information for improving the prediction accuracy of the proteins with multiple sites. In this paper, a new algorithm which can effectively exploit the correlations among the locations is proposed by using Gaussian process model. Besides, the algorithm also can realize optimal linear combination of various feature extraction technologies and could be robust to the imbalanced data set. Experimental results on a human protein data set show that the proposed algorithm is valid and can achieve better performance than the existing approaches. © 2012 He et al.",,"accuracy; algorithm; article; cellular distribution; classifier; computer prediction; Gaussian process; information service; intermethod comparison; Internet; mathematical computing; nucleotide sequence; protein localization; algorithm; artificial intelligence; biological model; human; metabolism; physiology; protein transport; protein; Algorithms; Artificial Intelligence; Humans; Models, Biological; Protein Transport; Proteins",Article,Scopus,2-s2.0-84862027781
"Bergeron C., Moore G., Zaretzki J., Breneman C.M., Bennett K.P.","Fast bundle algorithm for multiple-instance learning",2012,"IEEE Transactions on Pattern Analysis and Machine Intelligence",36,10.1109/TPAMI.2011.194,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860251069&doi=10.1109%2fTPAMI.2011.194&partnerID=40&md5=ffcbcc78f82c802a08592fe709e82c59","We present a bundle algorithm for multiple-instance classification and ranking. These frameworks yield improved models on many problems possessing special structure. Multiple-instance loss functions are typically nonsmooth and nonconvex, and current algorithms convert these to smooth nonconvex optimization problems that are solved iteratively. Inspired by the latest linear-time subgradient-based methods for support vector machines, we optimize the objective directly using a nonconvex bundle method. Computational results show this method is linearly scalable, while not sacrificing generalization accuracy, permitting modeling on new and larger data sets in computational chemistry and other applications. This new implementation facilitates modeling with kernels. © 2012 IEEE.","Artificial intelligence; Bundle methods; Machine learning; Medicine and science; Multiple-instance learning; Nonsmooth optimization; Ranking","Bundle methods; Computational results; Data sets; Generalization accuracy; Improved models; Loss functions; Multiple-instance learning; Non-smooth; Nonconvex; Nonconvex optimization problem; Nonsmooth optimization; Other applications; Ranking; Special structure; Artificial intelligence; Learning systems; Optimization; Algorithms; algorithm; article; artificial intelligence; artificial neural network; automated pattern recognition; human; methodology; support vector machine; Algorithms; Artificial Intelligence; Humans; Neural Networks (Computer); Pattern Recognition, Automated; Support Vector Machines",Article,Scopus,2-s2.0-84860251069
"Kindermans P.-J., Verstraeten D., Schrauwen B.","A Bayesian model for exploiting application constraints to enable unsupervised training of a p300-based BCI",2012,"PLoS ONE",36,10.1371/journal.pone.0033758,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859305478&doi=10.1371%2fjournal.pone.0033758&partnerID=40&md5=03a3faf45c52fbc33a07f43c4186820e","This work introduces a novel classifier for a P300-based speller, which, contrary to common methods, can be trained entirely unsupervisedly using an Expectation Maximization approach, eliminating the need for costly dataset collection or tedious calibration sessions. We use publicly available datasets for validation of our method and show that our unsupervised classifier performs competitively with supervised state-of-the-art spellers. Finally, we demonstrate the added value of our method in different experimental settings which reflect realistic usage situations of increasing difficulty and which would be difficult or impossible to tackle with existing supervised or adaptive methods. © 2012 Kindermans et al.",,"accuracy; article; Bayes theorem; brain computer interface; calibration; classifier; event related potential; learning algorithm; statistical model; validation process; algorithm; artificial intelligence; brain; computer interface; event related potential; human; language; physiology; signal processing; theoretical model; Algorithms; Artificial Intelligence; Bayes Theorem; Brain; Event-Related Potentials, P300; Humans; Language; Models, Theoretical; Signal Processing, Computer-Assisted; User-Computer Interface",Article,Scopus,2-s2.0-84859305478
"Croitoru A.-E., Holobaca I.-H., Lazar C., Moldovan F., Imbroane A.","Air temperature trend and the impact on winter wheat phenology in Romania",2012,"Climatic Change",36,10.1007/s10584-011-0133-6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856724773&doi=10.1007%2fs10584-011-0133-6&partnerID=40&md5=cfbf61d502939453debcddf9024a807a","Air temperature variability and trends in Romania were analysed using monthly, seasonal, and annual datasets. Temperature data of winter wheat season were also analysed. The Mann-Kendall test, Sen's slope estimate, the sequential version of the Mann-Kendall test, the Pettitt test and spatial and temporal hierarchical cluster analyses were used. First, the datasets were checked for changing points. The 106-year period was divided into two long periods of 100 years each to verify the importance of a very short interval in changing of general trends; after that it was divided into three shorter periods of 35-36 years each. The main conclusions are as follows: the 6 years making up the difference between the two long periods are very important in the context of the recent global warming; the three shorter periods analysis indicate some fluctuations rather than continuous warming. The latest short period is the most relevant for global warming. Spatial hierarchical cluster analysis indicated the existence of two distinctive groups. One of them, which includes stations in the south-east part of the country, seems to be influenced by the Black Sea surface temperature. Temporal hierarchical cluster analysis reveals that annual data series have the closest relation with the summer data series. Further, the impact of temperature changes on winter wheat phenology was determined using a phenology simulation performed with the model from the Decision Support System for Agrotechnology Transfer v. 4. 0. 2. 0 platform. Earlier occurrences of anthesis and maturity were noticed for several regions in the country. © 2011 Springer Science+Business Media B.V.",,"Agrotechnology transfer; Air temperature; Black sea; Data series; Data sets; General trends; Hierarchical cluster analysis; Mann-Kendall test; Pettitt test; Romania; Short periods; Short-interval; Temperature changes; Temperature data; Winter wheat; Artificial intelligence; Biology; Cluster analysis; Computer simulation; Crops; Decision support systems; Global warming; Hierarchical systems; Phenols; Stream flow; Atmospheric temperature; air temperature; cluster analysis; data set; decision support system; global warming; maturation; phenology; sea surface temperature; temperature effect; temperature profile; wheat; Romania; Triticum aestivum",Article,Scopus,2-s2.0-84856724773
"Nguyen T.M., Wu Q.M.J.","Gaussian-mixture-model-based spatial neighborhood relationships for pixel labeling problem",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",36,10.1109/TSMCB.2011.2161284,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856246435&doi=10.1109%2fTSMCB.2011.2161284&partnerID=40&md5=c88e4394132facb4eebde9612439ce96","In this paper, we present a new algorithm for pixel labeling and image segmentation based on the standard Gaussian mixture model (GMM). Unlike the standard GMM where pixels themselves are considered independent of each other and the spatial relationship between neighboring pixels is not taken into account, the proposed method incorporates this spatial relationship into the standard GMM. Moreover, the proposed model requires fewer parameters compared with the models based on Markov random fields. In order to estimate model parameters from observations, instead of utilizing an expectation-maximization algorithm, we employ gradient method to minimize a higher bound on the data negative log-likelihood. The performance of the proposed model is compared with methods based on both standard GMM and Markov random fields, demonstrating the robustness, accuracy, and effectiveness of our method. © 2011 IEEE.","Gaussian mixture models (GMMs); image segmentation; pixel labeling; spatial neighborhood relationships","Estimate model; Expectation-maximization algorithms; Gaussian Mixture Model; Gaussian mixture models; Log likelihood; Markov Random Fields; spatial neighborhood relationships; Spatial neighborhoods; Spatial relationships; Algorithms; Communication channels (information theory); Gaussian distribution; Gradient methods; Image segmentation; Object recognition; Standards; Pixels; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; computer simulation; methodology; normal distribution; statistical model; Algorithms; Artificial Intelligence; Computer Simulation; Image Interpretation, Computer-Assisted; Models, Statistical; Normal Distribution; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84856246435
"Batmanghelich N.K., Taskar B., Davatzikos C.","Generative-discriminative basis learning for medical imaging",2012,"IEEE Transactions on Medical Imaging",36,10.1109/TMI.2011.2162961,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855382390&doi=10.1109%2fTMI.2011.2162961&partnerID=40&md5=62db8a23560ce9cdda4d56f66e111b2e","This paper presents a novel dimensionality reduction method for classification in medical imaging. The goal is to transform very high-dimensional input (typically, millions of voxels) to a low-dimensional representation (small number of constructed features) that preserves discriminative signal and is clinically interpretable. We formulate the task as a constrained optimization problem that combines generative and discriminative objectives and show how to extend it to the semi-supervised learning (SSL) setting. We propose a novel large-scale algorithm to solve the resulting optimization problem. In the fully supervised case, we demonstrate accuracy rates that are better than or comparable to state-of-the-art algorithms on several datasets while producing a representation of the group difference that is consistent with prior clinical reports. Effectiveness of the proposed algorithm for SSL is evaluated with both benchmark and medical imaging datasets. In the benchmark datasets, the results are better than or comparable to the state-of-the-art methods for SSL. For evaluation of the SSL setting in medical datasets, we use images of subjects with mild cognitive impairment (MCI), which is believed to be a precursor to Alzheimer's disease (AD), as unlabeled data. AD subjects and normal control (NC) subjects are used as labeled data, and we try to predict conversion from MCI to AD on follow-up. The semi-supervised extension of this method not only improves the generalization accuracy for the labeled data (AD/NC) slightly but is also able to predict subjects which are likely to converge to AD. © 2011 IEEE.","Basis learning; classification; feature construction; generative-discriminative learning; machine learning; matrix factorization; morphological pattern analysis; optimization; semi-supervised learning; sparsity","Basis learning; feature construction; generative-discriminative learning; Machine-learning; Matrix factorizations; Morphological patterns; Semi-supervised learning; sparsity; Classification (of information); Constrained optimization; Learning algorithms; Optimization; Supervised learning; Medical imaging; algorithm; Alzheimer disease; article; artificial intelligence; automated pattern recognition; diagnostic imaging; discriminant analysis; factual database; human; methodology; neuroimaging; nuclear magnetic resonance imaging; pathology; Algorithms; Alzheimer Disease; Artificial Intelligence; Databases, Factual; Diagnostic Imaging; Discriminant Analysis; Humans; Magnetic Resonance Imaging; Neuroimaging; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84855382390
"Waltman L., Van Eck N.J., Van Raan A.F.J.","Universality of citation distributions revisited",2012,"Journal of the American Society for Information Science and Technology",36,10.1002/asi.21671,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-83655201282&doi=10.1002%2fasi.21671&partnerID=40&md5=460467917ca29f43b5b6c097c585373c","Radicchi, Fortunato, and Castellano (2008) claim that, apart from a scaling factor, all fields of science are characterized by the same citation distribution. We present a large-scale validation study of this universality-of-citation-distributions claim. Our analysis shows that claiming citation distributions to be universal for all fields of science is not warranted. Although many fields indeed seem to have fairly similar citation distributions, there are exceptions as well. We also briefly discuss the consequences of our findings for the measurement of scientific impact using citation-based bibliometric indicators. © 2011 ASIS&T.",,"Citation distribution; Scaling factors; Validation study; Artificial intelligence; Software engineering",Article,Scopus,2-s2.0-83655201282
"Feller E., Morin C., Esnault A.","A case for fully decentralized dynamic VM consolidation in clouds",2012,"CloudCom 2012 - Proceedings: 2012 4th IEEE International Conference on Cloud Computing Technology and Science",35,10.1109/CloudCom.2012.6427585,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874262534&doi=10.1109%2fCloudCom.2012.6427585&partnerID=40&md5=17df6d3b07ab37ae0b3fbe647f7e7f06","One way to conserve energy in cloud data centers is to transition idle servers into a power saving state during periods of low utilization. Dynamic virtual machine (VM) consolidation (VMC) algorithms are proposed to create idle times by periodically repacking VMs on the least number of physical machines (PMs). Existing works mostly apply VMC on top of centralized, hierarchical, or ring-based system topologies which result in poor scalability and/or packing efficiency with increasing number of PMs and VMs. In this paper, we propose a novel fully decentralized dynamic VMC schema based on an unstructured peer-to-peer (P2P) network of PMs. The proposed schema is validated using three well known VMC algorithms: First-Fit Decreasing (FFD), Sercon, V-MAN, and a novel migration-cost aware ACO-based algorithm. Extensive experiments performed on the Grid'5000 testbed show that once integrated in our fully decentralized VMC schema, traditional VMC algorithms achieve a global packing efficiency very close to a centralized system. Moreover, the system remains scalable with increasing number of PMs and VMs. Finally, the migration-cost aware ACO-based algorithm outperforms FFD and Sercon in the number of released PMs and requires less migrations than FFD and V-MAN. © 2012 IEEE.","Ant Colony Optimization; Cloud Computing; Dynamic VM Consolidation; Unstructured P2P Network; Virtualization","Aco-based algorithms; Centralized systems; Cloud data centers; Conserve energy; First fit; Idle time; Packing efficiency; Power saving state; System topology; Unstructured P2P network; Unstructured peer-to-peer; Virtual machines; Virtualizations; Ant colony optimization; Artificial intelligence; Cloud computing; Peer to peer networks; Algorithms",Conference Paper,Scopus,2-s2.0-84874262534
"Nie M., Tan W.W.","Analytical structure and characteristics of symmetric karnik-mendel type-reduced interval type-2 fuzzy PI and PD controllers",2012,"IEEE Transactions on Fuzzy Systems",35,10.1109/TFUZZ.2011.2174061,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878720392&doi=10.1109%2fTFUZZ.2011.2174061&partnerID=40&md5=36eabb648e2f35ad5f8aefac0b51fe3e","This paper presents the analytical structure of a class of interval type-2 (IT2) fuzzy proportional derivative (PD) and proportional integral (PI) controllers that have symmetrical rule base and symmetrical consequent sets. Two assumptions are made: 1) The Zadeh AND operator is employed as the t-norm operator; 2) type-reduction is performed by the Karnik-Mendel (KM) typereduction method. The main contributions are the methodology that identifies the input conditions, where the KM algorithm uses a new switch point to compute the bounds of the type-reduced set, the closed-form expressions that relate the inputs and output of an IT2 fuzzy controller, and insights into the potential performance improvement because of the inclusion of the footprint of uncertainty (FOU). Compared with its T1 counterpart, two additional FOU parameters generate 31 extra local regions, each providing a unique relationship between the inputs and output signals. The generation of a relatively large number of local regions at the cost of two extra design parameters indicates that an IT2 fuzzy controller may be able to provide better performance. Furthermore, by comparing the analytical structure with the corresponding T1 counterpart, the potential advantages to use the IT2 over the T1 fuzzy controller are studied. Four interesting characteristics are identified, and they provide insights into why the IT2 fuzzy controller may better balance the conflicting aims of fast rise time and small overshoot. © 2012 IEEE.","Analytical structure; Fuzzy proportional derivative (PD) and proportional integral (PI) controllers; Interval type-2 (IT2) fuzzy logic system","Analytical structure; Better performance; Closed-form expression; Footprint of uncertainties; Fuzzy logic system; Interval type-2 fuzzy; Proportional derivatives; Proportional integral controllers; Artificial intelligence; Fuzzy sets; Two term control systems",Article,Scopus,2-s2.0-84878720392
"Bagwari A., Singh B.","Comparative performance evaluation of spectrum sensing techniques for cognitive radio networks",2012,"Proceedings - 4th International Conference on Computational Intelligence and Communication Networks, CICN 2012",35,10.1109/CICN.2012.66,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872017089&doi=10.1109%2fCICN.2012.66&partnerID=40&md5=f4928b1e39523f2d771e4ea530d672da","Cognitive radio is the key technology for future wireless communication. Spectrum sensing is one of the most important functions in cognitive radio (CR) applications. It involves the detection of primary user (PU) transmissions on a preassigned frequency band. PU licensed band can be sensed via appropriate spectrum sensing techniques. In this paper, we consider three basic spectrum sensing techniques of transmitter detection: Matched filter detection, Energy detection, and Cyclostationary feature detection. Using simulations, a comparative analysis of the three techniques has been carried out in terms of probability of false alarm Pf, probability of detection alarm Pd, and probability of miss detection Pm. Finally, Numerical result shows that at low signal to noise ratio (SNR), cyclostationary feature detection outperforms other two techniques, thus have some difficulties like implementation is complex, long observation time, etc. For simulation we used MATLAB software. © 2012 IEEE.","Cognitive Radio System; CR; PU; SNR; Spectrum Sensing","Cognitive radio network; Comparative analysis; Cyclostationary feature detection; Energy detection; Key technologies; Low signal-to-noise ratio; Matlab- software; Miss detection; Numerical results; Performance evaluation; Probability of detection; Probability of false alarm; SNR; Spectrum sensing; Wireless communications; Artificial intelligence; Chromium; Cognitive radio; Computer software; Frequency bands; MATLAB; Plutonium; Radio systems; Timing jitter; Wireless telecommunication systems; Sensors",Conference Paper,Scopus,2-s2.0-84872017089
"Gorkin D.U., Lee D., Reed X., Fletez-Brant C., Bessling S.L., Loftus S.K., Beer M.A., Pavan W.J., McCallion A.S.","Integration of ChIP-seq and machine learning reveals enhancers and a predictive regulatory sequence vocabulary in melanocytes",2012,"Genome Research",35,10.1101/gr.139360.112,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868318661&doi=10.1101%2fgr.139360.112&partnerID=40&md5=585b1e768ccc275ed5d4d93d4c118d19","We take a comprehensive approach to the study of regulatory control of gene expression in melanocytes that proceeds from large-scale enhancer discovery facilitated by ChIP-seq; to rigorous validation in silico, in vitro, and in vivo; and finally to the use of machine learning to elucidate a regulatory vocabulary with genome-wide predictive power. We identify 2489 putative melanocyte enhancer loci in the mouse genome by ChIP-seq for EP300 and H3K4me1. We demonstrate that these putative enhancers are evolutionarily constrained, enriched for sequence motifs predicted to bind key melanocyte transcription factors, located near genes relevant to melanocyte biology, and capable of driving reporter gene expression in melanocytes in culture (86%; 43/50) and in transgenic zebrafish (70%; 7/10). Next, using the sequences of these putative enhancers as a training set for a supervised machine learning algorithm, we develop a vocabulary of 6-mers predictive of melanocyte enhancer function. Lastly, we demonstrate that this vocabulary has genome-wide predictive power in both the mouse and human genomes. This study provides deep insight into the regulation of gene expression in melanocytes and demonstrates a powerful approach to the investigation of regulatory sequences that can be applied to other cell types. © 2012, Published by Cold Spring Harbor Laboratory Press.",,"microphthalmia associated transcription factor; animal cell; article; chromatin immunoprecipitation; controlled study; cytology; DNA flanking region; enhancer region; gene expression; gene locus; human; in vivo study; machine learning; melanocyte; mouse; nonhuman; priority journal; protein binding; regulatory sequence; zebra fish; Algorithms; Animals; Artificial Intelligence; Chromatin Immunoprecipitation; E1A-Associated p300 Protein; Enhancer Elements, Genetic; Evolution, Molecular; Gene Expression Regulation; Genes, Reporter; Genome, Human; Histones; Humans; Melanocytes; Mice; Sequence Analysis, DNA; Transcription Factors; Zebrafish; Danio rerio",Article,Scopus,2-s2.0-84868318661
"Desai C., Ramanan D.","Detecting actions, poses, and objects with relational phraselets",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",35,10.1007/978-3-642-33765-9_12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867846313&doi=10.1007%2f978-3-642-33765-9_12&partnerID=40&md5=bd7096116acfffc1a09f2e037e2287cc","We present a novel approach to modeling human pose, together with interacting objects, based on compositional models of local visual interactions and their relations. Skeleton models, while flexible enough to capture large articulations, fail to accurately model self-occlusions and interactions. Poselets and Visual Phrases address this limitation, but do so at the expense of requiring a large set of templates. We combine all three approaches with a compositional model that is flexible enough to model detailed articulations but still captures occlusions and object interactions. Unlike much previous work on action classification, we do not assume test images are labeled with a person, and instead present results for ""action detection"" in an unlabeled image. Notably, for each detection, our model reports back a detailed description including an action label, articulated human pose, object poses, and occlusion flags. We demonstrate that modeling occlusion is crucial for recognizing human-object interactions. We present results on the PASCAL Action Classification challenge that shows our unified model advances the state-of-the-art for detection, action classification, and articulated pose estimation. © 2012 Springer-Verlag.",,"Compositional models; Human pose; Human-object interaction; Object interactions; Object pose; Pose estimation; Self-occlusions; Test images; Unified model; Visual interaction; Visual phrase; Artificial intelligence; Computer vision",Conference Paper,Scopus,2-s2.0-84867846313
"Cambria E., Benson T., Eckl C., Hussain A.","Sentic PROMs: Application of sentic computing to the development of a novel unified framework for measuring health-care quality",2012,"Expert Systems with Applications",35,10.1016/j.eswa.2012.02.120,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861185572&doi=10.1016%2fj.eswa.2012.02.120&partnerID=40&md5=6941075fce5f99742c064da155bb25c0","Barriers to use health related quality of life measuring systems include the time needed to complete the forms and the need for staff to be trained to understand the results. An ideal system of health assessment needs to be clinically useful, timely, sensitive to change, culturally sensitive, low burden, low cost, involving for the patient and built into standard procedures. A new generation of short and easy-to-use tools to monitor patient outcomes on a regular basis has been recently proposed. These tools are quick, effective and easy to understand, as they are very structured and rigid. Such structuredness, however, leaves no space to those patients who would like to say something more. Patients, in fact, are usually willing to express their opinions and feelings in free text, rather than simply filling in a questionnaire, for either speaking out their satisfaction or for cathartic complaining. Sentic PROMs allow patients to evaluate their health status and experience in a semi-structured way and accordingly aggregate input data by means of sentic computing, while tracking patients' physio-emotional sensitivity. © 2012 Elsevier Ltd. All rights reserved.","AI; E-Health; Natural language processing; Opinion mining; Sentiment analysis","Ehealth; Filling in; Free texts; Health assessments; Health status; Health-related quality of lives; Ideal systems; Input datas; Low costs; Measuring systems; NAtural language processing; Opinion mining; Semi-structured; Sentiment analysis; Standard procedures; Unified framework; Artificial intelligence; Computational linguistics; Data mining; Natural language processing systems; Patient monitoring; Health",Article,Scopus,2-s2.0-84861185572
"Si D., Ji S., Nasr K.A., He J.","A machine learning approach for the identification of protein secondary structure elements from electron cryo-microscopy density maps",2012,"Biopolymers",35,10.1002/bip.22063,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862874104&doi=10.1002%2fbip.22063&partnerID=40&md5=7bf6dec53512093d59b43505037fbffe","The accuracy of the secondary structure element (SSE) identification from volumetric protein density maps is critical for de-novo backbone structure derivation in electron cryo-microscopy (cryoEM). It is still challenging to detect the SSE automatically and accurately from the density maps at medium resolutions (∼5-10 Å). We present a machine learning approach, SSELearner, to automatically identify helices and β-sheets by using the knowledge from existing volumetric maps in the Electron Microscopy Data Bank. We tested our approach using 10 simulated density maps. The averaged specificity and sensitivity for the helix detection are 94.9% and 95.8%, respectively, and those for the β-sheet detection are 86.7% and 96.4%, respectively. We have developed a secondary structure annotator, SSID, to predict the helices and β-strands from the backbone Cα trace. With the help of SSID, we tested our SSELearner using 13 experimentally derived cryo-EM density maps. The machine learning approach shows the specificity and sensitivity of 91.8% and 74.5%, respectively, for the helix detection and 85.2% and 86.5% respectively for the β-sheet detection in cryoEM maps of Electron Microscopy Data Bank. The reduced detection accuracy reveals the challenges in SSE detection when the cryoEM maps are used instead of the simulated maps. Our results suggest that it is effective to use one cryoEM map for learning to detect the SSE in another cryoEM map of similar quality. © 2012 Wiley Periodicals, Inc.","beta sheet; density map; electron cryo-microscopy; helix; local structure tensor; machine learning; protein; secondary structure","Backbone structures; Beta-sheet; Data bank; Density maps; Detection accuracy; helix; Identification of proteins; Learning approach; Local structure tensors; Protein density; Secondary structure elements; Secondary structures; Database systems; Electron microscopes; Electron microscopy; Medical imaging; Proteins; Learning systems; alpha helix; beta sheet; conference paper; cryoelectron microscopy; density; electron microscopy; machine learning; protein secondary structure; sensitivity and specificity; Artificial Intelligence; Cryoelectron Microscopy; Protein Structure, Secondary",Conference Paper,Scopus,2-s2.0-84862874104
"Molinos-Senante M., Garrido-Baserba M., Reif R., Hernández-Sancho F., Poch M.","Assessment of wastewater treatment plant design for small communities: Environmental and economic aspects",2012,"Science of the Total Environment",35,10.1016/j.scitotenv.2012.04.023,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861196087&doi=10.1016%2fj.scitotenv.2012.04.023&partnerID=40&md5=07eb519bc28bb2f2ccfced2e4086dde1","The preliminary design and economic assessment of small wastewater treatment plants (less than 2000 population equivalent) are issues of particular interest since wastewaters from most of these agglomerations are not covered yet. This work aims to assess nine different technologies set-up for the secondary treatment in such type of facilities embracing both economic and environmental parameters. The main novelty of this work is the combination of an innovative environmental decision support system (EDSS) with a pioneer approach based on the inclusion of the environmental benefits derived from wastewater treatment. The integration of methodologies based on cost-benefit analysis tools with the vast amount of knowledge from treatment technologies contained in the EDSS was applied in nine scenarios comprising different wastewater characteristics and reuse options. Hence, a useful economic feasibility indicator is obtained for each technology including internal and external costs and, for the first time, benefits associated with the environmental damage avoided. This new methodology proved to be crucial for supporting the decision process, contributing to improve the sustainability of new treatment facilities and allows the selection of the most feasible technologies of a wide set of possibilities. © 2012 Elsevier B.V.","Cost-benefit analysis; Environmental benefits; Environmental decision support system (EDSS); Shadow prices; Wastewater treatment","Analysis tools; Decision process; Economic aspects; Economic assessments; Economic feasibilities; Environmental benefits; Environmental damage; Environmental decision support systems; Environmental parameter; External costs; Preliminary design; Secondary treatment; Shadow price; Small community; Treatment technologies; Wastewater characteristics; Wastewater treatment plants; Artificial intelligence; Cost effectiveness; Decision support systems; Sewage pumping plants; Technology; Toxicity; Wastewater reclamation; Wastewater treatment; Water treatment plants; Environmental technology; cost-benefit analysis; decision support system; environmental assessment; environmental economics; shadow pricing; sustainability; wastewater; water treatment; article; community; cost benefit analysis; decision support system; economic aspect; environmental factor; methodology; priority journal; technology; waste water; waste water management; waste water recycling; waste water treatment plant; Bioreactors; Cost-Benefit Analysis; Decision Support Techniques; Filtration; Ponds; Silicon Dioxide; Waste Disposal, Fluid; Water Purification",Article,Scopus,2-s2.0-84861196087
"Belohlavek R.","Sup-t-norm and inf-residuum are one type of relational product: Unifying framework and consequences",2012,"Fuzzy Sets and Systems",35,10.1016/j.fss.2011.07.015,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856317854&doi=10.1016%2fj.fss.2011.07.015&partnerID=40&md5=2c64b7c82f1cec5753f518132ae16705","We present a simple framework which enables us to consider the well-known sup-t-norm and inf-residuum products of relations as two particular cases of a single, more general type of product. We present basic properties of the framework and consequences for the theory of fuzzy relations. Informally, the paper implies that in many cases of fuzzy relational modeling, such as in solving fuzzy relational equations, there is no need to develop the methods for sup-t-norm and inf-residuum products separately, because these methods are just two particular instances of a single method. © 2011 Elsevier B.V.","Duality; Fuzzy logic; Fuzzy relations; Relational product; Residuated lattice","Basic properties; Duality; Fuzzy relational equations; Fuzzy relations; Relational modeling; Relational product; Residuated lattices; Artificial intelligence; Fuzzy sets; Fuzzy logic",Conference Paper,Scopus,2-s2.0-84856317854
"Cern A., Golbraikh A., Sedykh A., Tropsha A., Barenholz Y., Goldblum A.","Quantitative structure - Property relationship modeling of remote liposome loading of drugs",2012,"Journal of Controlled Release",35,10.1016/j.jconrel.2011.11.029,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861710385&doi=10.1016%2fj.jconrel.2011.11.029&partnerID=40&md5=18fd320cdb66e9894fe4dc1d5e80f9c9","Remote loading of liposomes by trans-membrane gradients is used to achieve therapeutically efficacious intra-liposome concentrations of drugs. We have developed Quantitative Structure Property Relationship (QSPR) models of remote liposome loading for a data set including 60 drugs studied in 366 loading experiments internally or elsewhere. Both experimental conditions and computed chemical descriptors were employed as independent variables to predict the initial drug/lipid ratio (D/L) required to achieve high loading efficiency. Both binary (to distinguish high vs. low initial D/L) and continuous (to predict real D/L values) models were generated using advanced machine learning approaches and 5-fold external validation. The external prediction accuracy for binary models was as high as 91-96%; for continuous models the mean coefficient R 2 for regression between predicted versus observed values was 0.76-0.79. We conclude that QSPR models can be used to identify candidate drugs expected to have high remote loading capacity while simultaneously optimizing the design of formulation experiments. © 2011 Elsevier B.V.","Chemical descriptors; Liposome; Loading conditions; Loading efficiency; QSPR; Remote loading","Binary models; Candidate drugs; Chemical descriptors; Continuous models; Data sets; Experimental conditions; High loadings; Independent variables; Learning approach; Loading capacities; Loading condition; Loading efficiency; Loading experiment; Prediction accuracy; QSPR; QSPR model; Quantitative structure-property relationship models; Quantitative structures; Forecasting; Liposomes; Loading; 6 [2 (dimethylamino)ethylamino] 3 hydroxy 7h indeno[2,1 c]quinolin 7 one; acridine orange; brucine; bupivacaine; carbendazim; chloroquine; chlorpromazine; citicoline; diclofenac; epirubicin; ethambutol; fasudil; ferulic acid; ganciclovir; gemcitabine; idarubicin; imatinib; irinotecan; levofloxacin; liposome; mepacrine; oxymatrine; pilocarpine; primaquine; propranolol; quinidine; quinine; salicylic acid; sildenafil; unindexed drug; article; drug delivery system; drug design; drug formulation; drug structure; machine learning; predictor variable; priority journal; quantitative structure property relation; Artificial Intelligence; Chemistry, Pharmaceutical; Computer Simulation; Decision Trees; Drug Carriers; Drug Compounding; Hydrophobic and Hydrophilic Interactions; Membranes, Artificial; Models, Chemical; Molecular Structure; Pharmaceutical Preparations; Predictive Value of Tests; Quantitative Structure-Activity Relationship; Reproducibility of Results; Software",Article,Scopus,2-s2.0-84861710385
"Liu H., Abraham A., Snášel V., McLoone S.","Swarm scheduling approaches for work-flow applications with security constraints in distributed data-intensive computing environments",2012,"Information Sciences",35,10.1016/j.ins.2011.12.032,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862777930&doi=10.1016%2fj.ins.2011.12.032&partnerID=40&md5=a9901f54d4673a150cdaa1bbe908e545","The scheduling problem in distributed data-intensive computing environments has become an active research topic due to the tremendous growth in grid and cloud computing environments. As an innovative distributed intelligent paradigm, swarm intelligence provides a novel approach to solving these potentially intractable problems. In this paper, we formulate the scheduling problem for work-flow applications with security constraints in distributed data-intensive computing environments and present a novel security constraint model. Several meta-heuristic adaptations to the particle swarm optimization algorithm are introduced to deal with the formulation of efficient schedules. A variable neighborhood particle swarm optimization algorithm is compared with a multi-start particle swarm optimization and multi-start genetic algorithm. Experimental results illustrate that population based meta-heuristics approaches usually provide a good balance between global exploration and local exploitation and their feasibility and effectiveness for scheduling work-flow applications. © 2010 Elsevier Inc. All rights reserved.","Distributed data-intensive computing environments; Particle swarm; Scheduling problem; Security constraints; Swarm intelligence; Work-flow","Data-intensive computing; Particle swarm; Scheduling problem; Security constraint; Swarm Intelligence; Work-flows; Algorithms; Artificial intelligence; Particle swarm optimization (PSO); Scheduling; Computer systems",Article,Scopus,2-s2.0-84862777930
"Pureza V., Morabito R., Reimann M.","Vehicle routing with multiple deliverymen: Modeling and heuristic approaches for the VRPTW",2012,"European Journal of Operational Research",35,10.1016/j.ejor.2011.12.005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855792252&doi=10.1016%2fj.ejor.2011.12.005&partnerID=40&md5=58550d0bce748a186d6e9b4c32b3b277","In real life distribution of goods, relatively long service times may make it difficult to serve all requests during regular working hours. These difficulties are even greater if the beginning of the service in each demand site must occur within a time window and violations of routing time restrictions are particularly undesirable. We address this situation by considering a variant of the vehicle routing problem with time windows for which, besides routing and scheduling decisions, a number of extra deliverymen can be assigned to each route in order to reduce service times. This problem appears, for example, in the distribution of beverage and tobacco in highly dense Brazilian urban areas. We present a mathematical programming formulation for the problem, as well as a tabu search and an ant colony optimization heuristics for obtaining minimum cost routes. The performance of the model and the heuristic approaches are evaluated using instances generated from a set of classic examples from the literature. © 2011 Elsevier B.V. All rights reserved.","Ant colony optimization; Beverage distribution; Mixed integer programming; Tabu search; Vehicle routing with multiple deliverymen; Vehicle routing with time windows","Ant-colony optimization; Heuristic approach; Highly dense; Minimum cost; Mixed integer programming; Routing and scheduling; Service time; Time restriction; Time windows; Urban areas; Vehicle routing problem with time windows; Vehicle routing with time windows; Working hours; Algorithms; Artificial intelligence; Beverages; Heuristic methods; Integer programming; Tabu search; Vehicle routing; Vehicles; Distribution of goods",Article,Scopus,2-s2.0-84855792252
"Farquad M.A.H., Bose I.","Preprocessing unbalanced data using support vector machine",2012,"Decision Support Systems",35,10.1016/j.dss.2012.01.016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859213527&doi=10.1016%2fj.dss.2012.01.016&partnerID=40&md5=93ddef1789c521f3a27db540aba7e302","This paper deals with the application of support vector machine (SVM) to deal with the class imbalance problem. The objective of this paper is to examine the feasibility and efficiency of SVM as a preprocessor. Our study analyzes different classification algorithms that are employed to predict the customers with caravan car policy based on his/her sociodemographic data and history of product ownership. A series of experiments was conducted to test various computational intelligence techniques viz., Multilayer Perceptron (MLP), Logistic Regression (LR), and Random Forest (RF). Various standard balancing techniques such as under-sampling, over-sampling and Synthetic Minority Over-sampling TEchnique (SMOTE) are also employed. Subsequently, a strategy of data balancing for handling imbalanced distribution in data is proposed. The proposed approach first employs SVM as a preprocessor and the actual target values of training data are then replaced by the predictions of trained SVM. Later, this modified training data is used to train techniques such as MLP, LR, and RF. Based on the measure of sensitivity, it is observed that the proposed approach not only balances the data effectively but also provides more number of instances for minority class, which in turn enhances the performance of the intelligence techniques. © 2012 Elsevier B.V. All rights reserved.","COIL data; Hybrid method; Preprocessor; SVM; Unbalanced data","Balancing techniques; Class imbalance problems; Classification algorithm; COIL data; Computational intelligence techniques; Hybrid method; Logistic regressions; Multi layer perceptron; Over sampling; Random forests; Sociodemographic data; Support vector machine (SVM); SVM; Synthetic minority over-sampling techniques; Target values; Training data; Unbalanced data; Under-sampling; Artificial intelligence; Decision trees; Logistics; Program processors; Support vector machines",Article,Scopus,2-s2.0-84859213527
"Cheng D., Feng J.-E., Lv H.","Solving fuzzy relational equations via semitensor product",2012,"IEEE Transactions on Fuzzy Systems",35,10.1109/TFUZZ.2011.2174243,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859706343&doi=10.1109%2fTFUZZ.2011.2174243&partnerID=40&md5=e8c813ce6fd1d3f0b7074c3249669364","The problem of solving max-min fuzzy relational equations is investigated. First, we show that if there is a solution, then there is a corresponding solution within the set of parameters [briefly, the parameter set solution (PSS)]. Then, the semitensor product of matrices is used to convert the logical equations into algebraic equations via the vector expression of logical variables. Under this form, every PSS can be obtained. It is proved that all the solutions can be revealed from their corresponding PSS. Some examples are presented to demonstrate the algorithm to solve fuzzy relational equations. © 2012 IEEE.","Fuzzy relational equation (FRE); multivalued logic; parameter set solution (PSS); semitensor product","Algebraic equations; Corresponding solutions; Fuzzy relational equations; Logical equations; Logical variables; Max-min; Multivalued logic; Parameter set; Semi-tensor product; Semi-tensor product of matrices; Artificial intelligence; Fuzzy sets; Transistor transistor logic circuits",Article,Scopus,2-s2.0-84859706343
"Cheng F., Ikenaga Y., Zhou Y., Yu Y., Li W., Shen J., Du Z., Chen L., Xu C., Liu G., Lee P.W., Tang Y.","In silico assessment of chemical biodegradability",2012,"Journal of Chemical Information and Modeling",35,10.1021/ci200622d,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859189951&doi=10.1021%2fci200622d&partnerID=40&md5=d8a60b1fe9fe8f9d631e85d42147f9f4","Biodegradation is the principal environmental dissipation process. Due to a lack of comprehensive experimental data, high study cost and time-consuming, in silico approaches for assessing the biodegradable profiles of chemicals are encouraged and is an active current research topic. Here we developed in silico methods to estimate chemical biodegradability in the environment. At first 1440 diverse compounds tested under the Japanese Ministry of International Trade and Industry (MITI) protocol were used. Four different methods, namely support vector machine, k-nearest neighbor, naïve Bayes, and C4.5 decision tree, were used to build the combinatorial classification probability models of ready versus not ready biodegradability using physicochemical descriptors and fingerprints separately. The overall predictive accuracies of the best models were more than 80% for the external test set of 164 diverse compounds. Some privileged substructures were further identified for ready or not ready biodegradable chemicals by combining information gain and substructure fragment analysis. Moreover, 27 new predicted chemicals were selected for experimental assay through the Japanese MITI test protocols, which validated that all 27 compounds were predicted correctly. The predictive accuracies of our models outperform the commonly used software of the EPI Suite. Our study provided critical tools for early assessment of biodegradability of new organic chemicals in environmental hazard assessment. © 2012 American Chemical Society.",,"Active current; Best model; Environmental hazards; Experimental data; In-silico; Information gain; K-nearest neighbors; Physico-chemical descriptors; Predictive accuracy; Probability models; Research topics; Test protocols; Test sets; Decision trees; International trade; Organic chemicals; Biodegradation; article; artificial intelligence; Bayes theorem; biology; biotransformation; computer program; decision tree; half life time; methodology; physical chemistry; reproducibility; Artificial Intelligence; Bayes Theorem; Biotransformation; Computational Biology; Decision Trees; Half-Life; Physicochemical Processes; Reproducibility of Results; Software",Article,Scopus,2-s2.0-84859189951
"Bitansky N., Canetti R., Halevi S.","Leakage-tolerant interactive protocols",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",35,10.1007/978-3-642-28914-9_15,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858329133&doi=10.1007%2f978-3-642-28914-9_15&partnerID=40&md5=412ee887d6edcc01d63dd56c5a537b2c","We put forth a framework for expressing security requirements from interactive protocols in the presence of arbitrary leakage. The framework allows capturing different levels of leakage-tolerance of protocols, namely the preservation (or degradation) of security, under coordinated attacks that include various forms of leakage from the secret states of participating components. The framework extends the universally composable (UC) security framework. We also prove a variant of the UC theorem that enables modular design and analysis of protocols even in face of general, non-modular leakage. We then construct leakage-tolerant protocols for basic tasks, such as secure message transmission, message authentication, commitment, oblivious transfer and zero-knowledge. A central component in several of our constructions is the observation that resilience to adaptive party corruptions (in some strong sense) implies leakage-tolerance in an essentially optimal way. © 2012 Springer-Verlag.",,"Central component; Coordinated attack; Interactive protocols; Leakage-tolerant; Message authentication; Modular designs; Oblivious transfer; Secure message transmission; Security requirements; Universally Composable Security; Zero knowledge; Coordinated attack; Interactive protocols; Leakage tolerance; Message authentication; Oblivious transfer; Secure message transmission; Security requirements; Universally Composable Security; Artificial intelligence; Artificial intelligence; Computers; Cryptography; Cryptography",Conference Paper,Scopus,2-s2.0-84858329133
"Liu H.-W.","Semi-uninorms and implications on a complete lattice",2012,"Fuzzy Sets and Systems",35,10.1016/j.fss.2011.08.010,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855544416&doi=10.1016%2fj.fss.2011.08.010&partnerID=40&md5=87b70168a24c22b26e13d1d91e97ba5f","An extension of a uninorm called a semi-uninorm is introduced and discussed in this paper. First, we introduce the concept of semi-uninorms on a complete lattice. Then, we discuss two kinds of residual operators of semi-uninorms and give conditions such that the operators are implications. We also give equivalent conditions for infinitely ∨-distributive left- and right-conjunctive semi-uninorms. Furthermore, we define two classes of induced operators by implications on a complete lattice and give conditions such that they are semi-uninorms. We also provide the equivalent conditions for the infinitely ∧-distributive implications in their second variables. © 2011 Elsevier B.V. All rights reserved.","Fuzzy connectives; Implications; Infinitely ∧ -distributive; Infinitely ∨ -distributive; Semi-uninorms","Complete lattices; Equivalent condition; Fuzzy connectives; Implications; Induced operators; Semi-uninorms; Uninorms; Artificial intelligence; Fuzzy sets; Mathematical operators",Article,Scopus,2-s2.0-84855544416
"Chicharro D., Ledberg A.","When two become one: The limits of causality analysis of brain dynamics",2012,"PLoS ONE",35,10.1371/journal.pone.0032466,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858425279&doi=10.1371%2fjournal.pone.0032466&partnerID=40&md5=46f8ae3fe270b6fae5811881447af205","Biological systems often consist of multiple interacting subsystems, the brain being a prominent example. To understand the functions of such systems it is important to analyze if and how the subsystems interact and to describe the effect of these interactions. In this work we investigate the extent to which the cause-and-effect framework is applicable to such interacting subsystems. We base our work on a standard notion of causal effects and define a new concept called natural causal effect. This new concept takes into account that when studying interactions in biological systems, one is often not interested in the effect of perturbations that alter the dynamics. The interest is instead in how the causal connections participate in the generation of the observed natural dynamics. We identify the constraints on the structure of the causal connections that determine the existence of natural causal effects. In particular, we show that the influence of the causal connections on the natural dynamics of the system often cannot be analyzed in terms of the causal effect of one subsystem on another. Only when the causing subsystem is autonomous with respect to the rest can this interpretation be made. We note that subsystems in the brain are often bidirectionally connected, which means that interactions rarely should be quantified in terms of cause-and-effect. We furthermore introduce a framework for how natural causal effects can be characterized when they exist. Our work also has important consequences for the interpretation of other approaches commonly applied to study causality in the brain. Specifically, we discuss how the notion of natural causal effects can be combined with Granger causality and Dynamic Causal Modeling (DCM). Our results are generic and the concept of natural causal effects is relevant in all areas where the effects of interactions between subsystems are of interest. © 2012 Chicharro, Ledberg.",,"article; artificial intelligence; biology; BOLD signal; brain function; causal modeling; cause and effect framework; conceptual framework; electric potential; electroencephalogram; mathematical computing; molecular dynamics; neurophysiology; signal transduction; standard; statistical model; animal; biological model; brain; epidemiology; human; nerve cell network; physiology; systems biology; Animals; Brain; Causality; Humans; Models, Neurological; Nerve Net; Systems Biology",Article,Scopus,2-s2.0-84858425279
"Xu H., Mannor S.","Robustness and generalization",2012,"Machine Learning",35,10.1007/s10994-011-5268-1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867738590&doi=10.1007%2fs10994-011-5268-1&partnerID=40&md5=a7f9581feb090603237dca0bcdade4f3","We derive generalization bounds for learning algorithms based on their robustness: the property that if a testing sample is ""similar"" to a training sample, then the testing error is close to the training error. This provides a novel approach, different from complexity or stability arguments, to study generalization of learning algorithms. One advantage of the robustness approach, compared to previous methods, is the geometric intuition it conveys. Consequently, robustness-based analysis is easy to extend to learning in non-standard setups such as Markovian samples or quantile loss. We further show that a weak notion of robustness is both sufficient and necessary for generalizability, which implies that robustness is a fundamental property that is required for learning algorithms to work. © The Author(s) 2011.","Generalization; Non-IID sample; Quantile loss; Robust","Fundamental properties; Generalization; Generalization bound; Markovian; Non-IID sample; Robust; Testing errors; Testing samples; Training errors; Training sample; Artificial intelligence; Software engineering; Learning algorithms",Article,Scopus,2-s2.0-84867738590
"Vieira S.M., Sousa J.M.C., Kaymak U.","Fuzzy criteria for feature selection",2012,"Fuzzy Sets and Systems",35,10.1016/j.fss.2011.09.009,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-83655201315&doi=10.1016%2fj.fss.2011.09.009&partnerID=40&md5=c45c398b4bb625c24cd3c18483ce9435","The presence of less relevant or highly correlated features often decrease classification accuracy. Feature selection in which most informative variables are selected for model generation is an important step in data-driven modeling. In feature selection, one often tries to satisfy multiple criteria such as feature discriminating power, model performance or subset cardinality. Therefore, a multi-objective formulation of the feature selection problem is more appropriate. In this paper, we propose to use fuzzy criteria in feature selection by using a fuzzy decision making framework. This formulation allows for a more flexible definition of the goals in feature selection, and avoids the problem of weighting different goals is classical multi-objective optimization. The optimization problem is solved using an ant colony optimization algorithm proposed in our previous work. We illustrate the added value of the approach by applying our proposed fuzzy feature selection algorithm to eight benchmark problems. © 2011 Elsevier B.V. All rights reserved.","Ant colony optimization; Feature selection; Fuzzy criteria; Fuzzy models","Added values; Ant Colony Optimization algorithms; Ant-colony optimization; Bench-mark problems; Cardinalities; Classification accuracy; Data-driven modeling; Discriminating power; Feature selection problem; Flexible definition; Fuzzy criteria; Fuzzy Decision making; Fuzzy features; Fuzzy models; Highly-correlated; Model generation; Model performance; Multi objective; Multi-objective formulation; Multiple criteria; Optimization problems; Algorithms; Artificial intelligence; Decision making; Multiobjective optimization; Feature extraction",Article,Scopus,2-s2.0-83655201315
"García-Pedrajas N., Pérez-Rodríguez J., García-Pedrajas M., Ortiz-Boyer D., Fyfe C.","Class imbalance methods for translation initiation site recognition in DNA sequences",2012,"Knowledge-Based Systems",35,10.1016/j.knosys.2011.05.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052415991&doi=10.1016%2fj.knosys.2011.05.002&partnerID=40&md5=d3a74343c88f0019efad7d66fd7e2deb","Translation initiation site (TIS) recognition is one of the first steps in gene structure prediction, and one of the common components in any gene recognition system. Many methods have been described in the literature to identify TIS in transcribed sequences such as mRNA, EST and cDNA sequences. However, the recognition of TIS in DNA sequences is a far more challenging task, and the methods described so far for transcripts achieve poor results in DNA sequences. Most methods approach this problem taking into account its biological characteristics. In this work we try a different view, considering this classification problem from a purely machine learning perspective. From the point of view of machine learning, TIS recognition is a class imbalance problem. Thus, in this paper we approach TIS recognition from this angle, and apply the different methods that have been developed to deal with imbalanced datasets. The proposed approach has two advantages. Firstly, it improves the results using standard classification methods. Secondly, it broadens the set of classification algorithms that can be used, as some of the class-imbalance methods, such as undersampling, are also useful as methods for scaling up data mining algorithms as they reduce the size of the dataset. In this way, classifiers that cannot be applied to the whole dataset, due to long training time or large memory requirements, can be used when undersampling method is applied. Results show an advantage of class imbalance methods with respect to the same methods applied without considering the class imbalance nature of the problem. The applied methods are also able to improve the results obtained with the best method in the literature, which is based on looking for the next in-frame stop codon from the putative TIS that must be predicted. © 2011 Elsevier B.V. All rights reserved.","Bioinformatics; Classification; Gene recognition; Imbalance datasets; Translation initiation site recognition","Biological characteristic; cDNA sequence; Class imbalance; Class imbalance problems; Classification algorithm; Classification methods; Data mining algorithm; Data sets; Gene recognition; Gene structure prediction; Imbalanced Data-sets; Memory requirements; Scaling-up; Training time; Translation initiation site; Translation initiation site recognition; Under-sampling; Algorithms; Artificial intelligence; Bioinformatics; Classification (of information); Data mining; DNA; Genes; Learning systems; DNA sequences",Article,Scopus,2-s2.0-80052415991
"Mooney C., Pollastri G., Shields D.C., Haslam N.J.","Prediction of short linear protein binding regions",2012,"Journal of Molecular Biology",35,10.1016/j.jmb.2011.10.025,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855278383&doi=10.1016%2fj.jmb.2011.10.025&partnerID=40&md5=d0712c836f2eafb019bcb1294d3cdfe0","Short linear motifs in proteins (typically 3-12 residues in length) play key roles in protein-protein interactions by frequently binding specifically to peptide binding domains within interacting proteins. Their tendency to be found in disordered segments of proteins has meant that they have often been overlooked. Here we present SLiMPred (short linear motif predictor), the first general de novo method designed to computationally predict such regions in protein primary sequences independent of experimentally defined homologs and interactors. The method applies machine learning techniques to predict new motifs based on annotated instances from the Eukaryotic Linear Motif database, as well as structural, biophysical, and biochemical features derived from the protein primary sequence. We have integrated these data sources and benchmarked the predictive accuracy of the method, and found that it performs equivalently to a predictor of protein binding regions in disordered regions, in addition to having predictive power for other classes of motif sites such as polyproline II helix motifs and short linear motifs lying in ordered regions. It will be useful in predicting peptides involved in potential protein associations and will aid in the functional characterization of proteins, especially of proteins lacking experimental information on structures and interactions. We conclude that, despite the diversity of motif sequences and structures, SLiMPred is a valuable tool for prioritizing potential interaction motifs in proteins. © 2011 Elsevier Ltd. All rights reserved.","intrinsically unstructured proteins; linear motif; molecular recognition; peptide binding; protein-protein interface","accuracy; amino acid sequence; article; binding site; biochemistry; biophysics; machine learning; predictive validity; priority journal; protein analysis; protein binding; protein function; protein motif; protein protein interaction; protein structure; short linear motif predictor; short linear protein binding region; Amino Acid Motifs; Amino Acid Sequence; Artificial Intelligence; Binding Sites; Databases, Protein; Humans; Protein Binding; Protein Interaction Domains and Motifs; Protein Structure, Secondary; Protein Structure, Tertiary; Proteins; Proteome; Sequence Alignment; Sequence Analysis, Protein; Eukaryota",Article,Scopus,2-s2.0-84855278383
"Shanahan M.","The brain's connective core and its role in animal cognition",2012,"Philosophical Transactions of the Royal Society B: Biological Sciences",35,10.1098/rstb.2012.0128,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865403478&doi=10.1098%2frstb.2012.0128&partnerID=40&md5=c9a3bbe4d77e0c3b2bb6fde735b60550","This paper addresses the question of how the brain of an animal achieves cognitive integration-that is to say how it manages to bring its fullest resources to bear on an ongoing situation. To fully exploit its cognitive resources, whether inherited or acquired through experience, it must be possible for unanticipated coalitions of brain processes to form. This facilitates the novel recombination of the elements of an existing behavioural repertoire, and thereby enables innovation. But in a system comprising massively many anatomically distributed assemblies of neurons, it is far from clear how such openended coalition formation is possible. The present paper draws on contemporary findings in brain connectivity and neurodynamics, as well as the literature of artificial intelligence, to outline a possible answer in terms of the brain'smost richly connected and topologically central structures, its so-called connective core. © 2012 The Royal Society.","Animal cognition; Brain networks; Computational neuroscience","anatomy; animal; artificial intelligence; brain; cognition; learning; neurology; resource use; topology",Article,Scopus,2-s2.0-84865403478
"Lin H., Bilmes J.","Learning mixtures of submodular shells with application to document summarization",2012,"Uncertainty in Artificial Intelligence - Proceedings of the 28th Conference, UAI 2012",34,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886074716&partnerID=40&md5=a2fe63e7b3f983daf9ee6d6d1ee63c14","We introduce a method to learn a mixture of submodular shells in a large-margin setting. A submodular shell is an abstract submodular function that can be instantiated with a ground set and a set of parameters to produce a submodular function. A mixture of such shells can then also be so instantiated to produce a more complex submodular function. What our algorithm learns are the mixture weights over such shells. We provide a risk bound guarantee when learning in a large-margin structured-prediction setting using a projected subgradient method when only approximate submodular optimization is possible (such as with submodular function maximization). We apply this method to the problem of multi-document summarization and produce the best results reported so far on the widely used NIST DUC-05 through DUC-07 document summarization corpora.",,"Document summarization; Multi-document summarization; Projected subgradient methods; Risk bounds; Submodular; Submodular functions; Submodular optimizations; Artificial intelligence; Mixtures; Shells (structures)",Conference Paper,Scopus,2-s2.0-84886074716
"Wang C., Wang M., Liu T., Hill D.J.","Learning from ISS-modular adaptive NN control of nonlinear strict-feedback systems",2012,"IEEE Transactions on Neural Networks and Learning Systems",34,10.1109/TNNLS.2012.2205702,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876914295&doi=10.1109%2fTNNLS.2012.2205702&partnerID=40&md5=ae89ae7d88a3dcb1e785252dcbe59765","This paper studies learning from adaptive neural control (ANC) for a class of nonlinear strict-feedback systems with unknown affine terms. To achieve the purpose of learning, a simple input-to-state stability (ISS) modular ANC method is first presented to ensure the boundedness of all the signals in the closed-loop system and the convergence of tracking errors in finite time. Subsequently, it is proven that learning with the proposed stable ISS-modular ANC can be achieved. The cascade structure and unknown affine terms of the considered systems make it very difficult to achieve learning using existing methods. To overcome these difficulties, the stable closed-loop system in the control process is decomposed into a series of linear time-varying (LTV) perturbed subsystems with the appropriate state transformation. Using a recursive design, the partial persistent excitation condition for the radial basis function neural network (NN) is established, which guarantees exponential stability of LTV perturbed subsystems. Consequently, accurate approximation of the closed-loop system dynamics is achieved in a local region along recurrent orbits of closed-loop signals, and learning is implemented during a closed-loop feedback control process. The learned knowledge is reused to achieve stability and an improved performance, thereby avoiding the tremendous repeated training process of NNs. Simulation studies are given to demonstrate the effectiveness of the proposed method. © 2012 IEEE.","Adaptive neural control; deterministic learning; exponential stability; persistent excitation; strict-feedback systems","Adaptive neural control; Closed-loop feedback control; Deterministic learning; Input-to-state stability; Partial persistent excitation; Persistent excitation; Radial basis function neural networks; Strict feedback systems; Asymptotic stability; Closed loop systems; Feedback control; Linear transformations; Mathematical transformations; Process control; algorithm; artificial intelligence; artificial neural network; automated pattern recognition; computer simulation; feedback system; nonlinear system; procedures; statistical model; Algorithms; Artificial Intelligence; Computer Simulation; Feedback; Linear Models; Neural Networks (Computer); Nonlinear Dynamics; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84876914295
"Zhang R., Xu Z.-B., Huang G.-B., Wang D.","Global convergence of online BP training with dynamic learning rate",2012,"IEEE Transactions on Neural Networks and Learning Systems",34,10.1109/TNNLS.2011.2178315,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868489803&doi=10.1109%2fTNNLS.2011.2178315&partnerID=40&md5=3aa3a419c2d344168d27ee6aeb5da6c3","The online backpropagation (BP) training procedure has been extensively explored in scientific research and engineering applications. One of the main factors affecting the performance of the online BP training is the learning rate. This paper proposes a new dynamic learning rate which is based on the estimate of the minimum error. The global convergence theory of the online BP training procedure with the proposed learning rate is further studied. It is proved that: 1) the error sequence converges to the global minimum error; and 2) the weight sequence converges to a fixed point at which the error function attains its global minimum. The obtained global convergence theory underlies the successful applications of the online BP training procedure. Illustrative examples are provided to support the theoretical analysis. © 2012 IEEE.","Backpropagation (BP) neural networks; dynamic learning rate; global convergence analysis; online BP training procedure","Back-propagation neural networks; Dynamic learning rates; Engineering applications; Error function; Global conver-gence; Learning rates; Scientific researches; Training procedures; Backpropagation; Learning algorithms; E-learning; algorithm; artificial intelligence; artificial neural network; automated pattern recognition; computer simulation; feedback system; online system; procedures; statistical model; Algorithms; Artificial Intelligence; Computer Simulation; Feedback; Models, Statistical; Neural Networks (Computer); Online Systems; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84868489803
"Lhuillier M.","Incremental fusion of structure-from-motion and GPS using constrained bundle adjustments",2012,"IEEE Transactions on Pattern Analysis and Machine Intelligence",34,10.1109/TPAMI.2012.157,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867801427&doi=10.1109%2fTPAMI.2012.157&partnerID=40&md5=5439ceae2914593f20844538cccd5ffa","Two problems occur when bundle adjustment (BA) is applied on long image sequences: large calculation time and drift (or error accumulation). In recent work, the calculation time is reduced by local BAs applied in an incremental scheme. The drift may be reduced by fusion of GPS and Structure-from-Motion. An existing fusion method is BA minimizing a weighted sum of image and GPS errors. This paper introduces two constrained BAs for fusion which enforce an upper bound for the reprojection error. These BAs are alternatives to the existing fusion BA which does not guarantee a small reprojection error and requires a weight as input. Then, the three fusion BAs are integrated in an incremental Structure-from-Motion method based on local BA. Last, we will compare the fusion results on long monocular image sequences and low cost GPS. © 2012 IEEE.","Fusion; Local bundle adjustment; Low cost GPS; Structure-from-motion","Bundle adjustments; Calculation time; Error accumulation; Fusion methods; Image sequence; Incremental scheme; Low costs; Monocular image sequence; Reprojection error; Structure from motion; Upper Bound; Weighted Sum; Artificial intelligence; Fusion reactions; Computer vision",Article,Scopus,2-s2.0-84867801427
"Chikh M.A., Saidi M., Settouti N.","Diagnosis of Diabetes Diseases Using An Artificial Immune Recognition System2 (AIRS2) with Fuzzy K-nearest neighbor",2012,"Journal of Medical Systems",34,10.1007/s10916-011-9748-4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867298235&doi=10.1007%2fs10916-011-9748-4&partnerID=40&md5=70ec047e59af131e157416e5fc3d48bc","The use of expert systems and artificial intelligence techniques in disease diagnosis has been increasing gradually. Artificial Immune Recognition System (AIRS) is one of the methods used in medical classification problems. AIRS2 is a more efficient version of the AIRS algorithm. In this paper, we used a modified AIRS2 called MAIRS2 where we replace the K- nearest neighbors algorithm with the fuzzy K-nearest neighbors to improve the diagnostic accuracy of diabetes diseases. The diabetes disease dataset used in our work is retrieved from UCI machine learning repository. The performances of the AIRS2 and MAIRS2 are evaluated regarding classification accuracy, sensitivity and specificity values. The highest classification accuracy obtained when applying the AIRS2 and MAIRS2 using 10-fold cross-validation was, respectively 82.69% and 89.10%. © 2011 Springer Science+Business Media, LLC.","AIRS2; Diagnosis; Fuzzy k- nearest neighbors; Pima Indians diabetes data set","drug; glucose; insulin; algorithm; antigen recognition; article; Artificial Immune Recognition System 2; artificial intelligence; body mass; decision support system; diabetes mellitus; diagnostic accuracy; diastolic blood pressure; disease classification; fuzzy logic; glucose blood level; human; Indian; insulin blood level; k nearest neighbor; major clinical study; memory cell; non insulin dependent diabetes mellitus; oral glucose tolerance test; sensitivity and specificity; skinfold thickness; age; American Indian; artificial neural network; automated pattern recognition; blood pressure; body size; cell function; classification; fuzzy logic; methodology; non insulin dependent diabetes mellitus; United States; Age Factors; Arizona; Blood Glucose; Blood Pressure; Body Size; Cell Physiological Phenomena; Diabetes Mellitus, Type 2; Fuzzy Logic; Humans; Indians, North American; Neural Networks (Computer); Pattern Recognition, Automated",Article,Scopus,2-s2.0-84867298235
"Pandey A., Davis N.A., White B.C., Pajewski N.M., Savitz J., Drevets W.C., Mckinney B.A.","Epistasis network centrality analysis yields pathway replication across two GWAS cohorts for bipolar disorder",2012,"Translational Psychiatry",34,10.1038/tp.2012.80,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866081004&doi=10.1038%2ftp.2012.80&partnerID=40&md5=da62c2f0fb1827da7bb6c9d43abde630","Most pathway and gene-set enrichment methods prioritize genes by their main effect and do not account for variation due to interactions in the pathway. A portion of the presumed missing heritability in genome-wide association studies (GWAS) may be accounted for through gene-gene interactions and additive genetic variability. In this study, we prioritize genes for pathway enrichment in GWAS of bipolar disorder (BD) by aggregating gene-gene interaction information with main effect associations through a machine learning (evaporative cooling) feature selection and epistasis network centrality analysis. We validate this approach in a two-stage (discovery/replication) pathway analysis of GWAS of BD. The discovery cohort comes from the Wellcome Trust Case Control Consortium (WTCCC) GWAS of BD, and the replication cohort comes from the National Institute of Mental Health (NIMH) GWAS of BD in European Ancestry individuals. Epistasis network centrality yields replicated enrichment of Cadherin signaling pathway, whose genes have been hypothesized to have an important role in BD pathophysiology but have not demonstrated enrichment in previous analysis. Other enriched pathways include Wnt signaling, circadian rhythm pathway, axon guidance and neuroactive ligand-receptor interaction. In addition to pathway enrichment, the collective network approach elevates the importance of ANK3, DGKH and ODZ4 for BD susceptibility in the WTCCC GWAS, despite their weak single-locus effect in the data. These results provide evidence that numerous small interactions among common alleles may contribute to the diathesis for BD and demonstrate the importance of including information from the network of gene-gene interactions as well as main effects when prioritizing genes for pathway analysis. © 2012 Macmillan Publishers Limited.","eigenvector centrality; epistasis network; evaporative cooling machine learning feature selection; pathway enrichment analysis; regression-based genetic association interaction network (reGAIN); SNPrank","cadherin; ligand; receptor; Wnt protein; cadherin; allele; ANK3 gene; article; bipolar disorder; circadian rhythm; cohort analysis; DGKH gene; gene; gene interaction; gene locus; gene regulatory network; genetic association; genetic epistasis; genetic susceptibility; human; ligand binding; machine learning; nerve fiber growth; ODZ4 gene; pathophysiology; signal transduction; single nucleotide polymorphism; validation process; algorithm; artificial intelligence; bipolar disorder; genetic predisposition; genetic variability; genetics; methodology; statistical model; validation study; Algorithms; Artificial Intelligence; Bipolar Disorder; Cadherins; Cohort Studies; Epistasis, Genetic; Gene Regulatory Networks; Genetic Predisposition to Disease; Genetic Variation; Genome-Wide Association Study; Humans; Linear Models; Polymorphism, Single Nucleotide",Article,Scopus,2-s2.0-84866081004
"Artikis A., Etzion O., Feldman Z., Fournier F.","Tutorial: Event processing under uncertainty",2012,"Proceedings of the 6th ACM International Conference on Distributed Event-Based Systems, DEBS'12",34,10.1145/2335484.2335488,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864976793&doi=10.1145%2f2335484.2335488&partnerID=40&md5=16a8c43b136345e9698fd143d5d81a73","Big data is recognized as one of the three technology trends at the leading edge a CEO cannot afford to overlook in 2012. Big data is characterized by volume, velocity, variety and veracity (""data in doubt""). As big data applications, many of the emerging event processing applications must process events that arrive from sources such as sensors and social media, which have inherent uncertainties associated with them. Consider, for example, the possibility of incomplete data streams and streams including inaccurate data. In this tutorial we classify the different types of uncertainty found in event processing applications and discuss the implications on event representation and reasoning. An area of research in which uncertainty has been studied is Artificial Intelligence. We discuss, therefore, the main Artificial Intelligence-based event processing systems that support probabilistic reasoning. The presented approaches are illustrated using an example concerning crime detection. Copyright © 2012 ACM.","Artificial intelligence; Event processing; Event recognition; Pattern matching; Uncertainty","Big datum; Crime detection; Event recognition; Incomplete data; Leading edge; Probabilistic reasoning; Processing applications; Processing systems; Social media; Technology trends; Uncertainty; Pattern matching; Software architecture; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84864976793
"Gamberini L., Spagnolli A., Corradi N., Jacucci G., Tusa G., Mikkola T., Zamboni L., Hoggan E.","Tailoring feedback to users' actions in a persuasive game for household electricity conservation",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",34,10.1007/978-3-642-31037-9_9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864708059&doi=10.1007%2f978-3-642-31037-9_9&partnerID=40&md5=af890b79c9ca24a7194fed0c884172b8","Recent work has begun to focus on the use of games as a platform for energy awareness and eco-feedback research. While technical advancements (wireless sensors, fingerprinting) make timely and tailored feedback an objective within easy reach, we argue that taking into account the users' own personal consumption behavior and tailoring feedback accordingly is a key requirement and a harder challenge. We present a first attempt in this direction, EnergyLife, which is designed to support the users' actions and embeds contextualized feedback triggered by specific actions of the user, called 'smart advice'. We conclude by showing the results of a four-month trial with four households that returned promising results on the effectiveness and acceptance of this feature. © 2012 Springer-Verlag.","adaptive; context aware; design; energy awareness; feedback; persuasive technology; smart advice; Sustainability","adaptive; Context-Aware; Energy awareness; Persuasive technology; smart advice; Artificial intelligence; Design; Feedback; Sustainable development; Technology",Conference Paper,Scopus,2-s2.0-84864708059
"Plewczynski D., Basu S., Saha I.","AMS 4.0: Consensus prediction of post-translational modifications in protein sequences",2012,"Amino Acids",34,10.1007/s00726-012-1290-2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864743678&doi=10.1007%2fs00726-012-1290-2&partnerID=40&md5=bbce9e86d38d9dd1be1bb97b051c5856","We present here the 2011 update of the Auto-Motif Service (AMS 4.0) that predicts the wide selection of 88 different types of the single amino acid post-translational modifications (PTM) in protein sequences. The selection of experimentally confirmed modifications is acquired from the latest UniProt and Phospho.ELM databases for training. The sequence vicinity of each modified residue is represented using amino acids physico-chemical features encoded using high quality indices (HQI) obtaining by automatic clustering of known indices extracted from AAindex database. For each type of the numerical representation, the method builds the ensemble of Multi-Layer Perceptron (MLP) pattern classifiers, each optimising different objectives during the training (for example the recall, precision or area under the ROC curve (AUC)). The consensus is built using brainstorming technology, which combines multi-objective instances of machine learning algorithm, and the data fusion of different training objects representations, in order to boost the overall prediction accuracy of conserved short sequence motifs. The performance of AMS 4.0 is compared with the accuracy of previous versions, which were constructed using single machine learning methods (artificial neural networks, support vector machine). Our software improves the average AUC score of the earlier version by close to 7 % as calculated on the test datasets of all 88 PTM types. Moreover, for the selected most-difficult sequence motifs types it is able to improve the prediction performance by almost 32 %, when compared with previously used single machine learning methods. Summarising, the brainstorming consensus meta-learning methodology on the average boosts the AUC score up to around 89 %, averaged over all 88 PTM types. Detailed results for single machine learning methods and the consensus methodology are also provided, together with the comparison to previously published methods and state-of-the-art software tools. The source code and precompiled binaries of brainstorming tool are available at http://code.google.com/p/automotifserver/under Apache 2.0 licensing. © Springer-Verlag 2011.","AMS-4; Consensus; High quality indices; MLP; Post-translational modifications","amino acid; AAindex database; accuracy; algorithm; amino acid sequence; area under the curve; article; artificial neural network; automation; biotechnology; computer program; controlled study; intermethod comparison; Internet; machine learning; performance; Phospho.ELM; physical chemistry; prediction; priority journal; protein database; protein motif; protein processing; scoring system; SWISS-PROT; UniProt; Algorithms; Amino Acid Sequence; Area Under Curve; Artificial Intelligence; Consensus Sequence; Protein Processing, Post-Translational; Proteins; Sequence Analysis, Protein; Software",Article,Scopus,2-s2.0-84864743678
"Basin D., Klaedtke F., Zǎlinescu E.","Algorithms for monitoring real-time properties",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",34,10.1007/978-3-642-29860-8_20,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861214465&doi=10.1007%2f978-3-642-29860-8_20&partnerID=40&md5=644e28a12357c9d0708209f1601b8474","We present and analyze monitoring algorithms for a safety fragment of metric temporal logics, which differ in their underlying time model. The time models considered have either dense or discrete time domains and are point-based or interval-based. Our analysis reveals differences and similarities between the time models for monitoring and highlights key concepts underlying our and prior monitoring algorithms. © 2012 Springer-Verlag.",,"Discrete time-domain; Metric temporal logic; Monitoring algorithms; Point-based; Real-time properties; Time model; Artificial intelligence; Algorithms",Conference Paper,Scopus,2-s2.0-84861214465
"Wang J., Gao X., Wang Q., Li Y.","ProDis-ContSHC: Learning protein dissimilarity measures and hierarchical context coherently for protein-protein comparison in protein database retrieval",2012,"BMC Bioinformatics",34,10.1186/1471-2105-13-S7-S2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872784072&doi=10.1186%2f1471-2105-13-S7-S2&partnerID=40&md5=6be64ce788990cc6f17a6df1e9d20cdd","Background: The need to retrieve or classify protein molecules using structure or sequence-based similarity measures underlies a wide range of biomedical applications. Traditional protein search methods rely on a pairwise dissimilarity/similarity measure for comparing a pair of proteins. This kind of pairwise measures suffer from the limitation of neglecting the distribution of other proteins and thus cannot satisfy the need for high accuracy of the retrieval systems. Recent work in the machine learning community has shown that exploiting the global structure of the database and learning the contextual dissimilarity/similarity measures can improve the retrieval performance significantly. However, most existing contextual dissimilarity/similarity learning algorithms work in an unsupervised manner, which does not utilize the information of the known class labels of proteins in the database.Results: In this paper, we propose a novel protein-protein dissimilarity learning algorithm, ProDis-ContSHC. ProDis-ContSHC regularizes an existing dissimilarity measure dij by considering the contextual information of the proteins. The context of a protein is defined by its neighboring proteins. The basic idea is, for a pair of proteins (i, j), if their context N (i) and N (j) is similar to each other, the two proteins should also have a high similarity. We implement this idea by regularizing dij by a factor learned from the context N (i) and N (j). Moreover, we divide the context to hierarchial sub-context and get the contextual dissimilarity vector for each protein pair. Using the class label information of the proteins, we select the relevant (a pair of proteins that has the same class labels) and irrelevant (with different labels) protein pairs, and train an SVM model to distinguish between their contextual dissimilarity vectors. The SVM model is further used to learn a supervised regularizing factor. Finally, with the new Supervised learned Dissimilarity measure, we update the Protein Hierarchial Context Coherently in an iterative algorithm--ProDis-ContSHC.We test the performance of ProDis-ContSHC on two benchmark sets, i.e., the ASTRAL 1.73 database and the FSSP/DALI database. Experimental results demonstrate that plugging our supervised contextual dissimilarity measures into the retrieval systems significantly outperforms the context-free dissimilarity/similarity measures and other unsupervised contextual dissimilarity measures that do not use the class label information.Conclusions: Using the contextual proteins with their class labels in the database, we can improve the accuracy of the pairwise dissimilarity/similarity measures dramatically for the protein retrieval tasks. In this work, for the first time, we propose the idea of supervised contextual dissimilarity learning, resulting in the ProDis-ContSHC algorithm. Among different contextual dissimilarity learning approaches that can be used to compare a pair of proteins, ProDis-ContSHC provides the highest accuracy. Finally, ProDis-ContSHC compares favorably with other methods reported in the recent literature. © 2012 Wang et al.; licensee BioMed Central Ltd.",,"Biomedical applications; Class label informations; Contextual information; Dissimilarity measures; Dissimilarity vectors; Machine learning communities; Retrieval performance; Similarity measure; Benchmarking; Database systems; Information retrieval; Learning algorithms; Medical applications; Proteins; protein; algorithm; article; artificial intelligence; chemical model; chemistry; classification; isolation and purification; protein database; Algorithms; Artificial Intelligence; Databases, Protein; Models, Chemical; Proteins",Article,Scopus,2-s2.0-84872784072
"Chang Y.-H., Chang C.-W., Tao C.-W., Lin H.-W., Taur J.-S.","Fuzzy sliding-mode control for ball and beam system with fuzzy ant colony optimization",2012,"Expert Systems with Applications",34,10.1016/j.eswa.2011.09.052,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80255141867&doi=10.1016%2fj.eswa.2011.09.052&partnerID=40&md5=cb88ed113884f7b18c821e0a698541eb","This paper mainly addresses the balance control of a ball and beam system, where a pair of decoupled fuzzy sliding-mode controllers (DFSMCs) are proposed. The DFSMC has the advantage of reducing the design complexity, in which the coupling dynamics of the state-error dynamics are considered as disturbance terms. Stability analysis of the ball and beam system with DFSMCs is also discussed in detail. To further improve the control performance, an improved ant colony optimization (ACO) is proposed to optimize the controller parameters. The proposed ACO algorithm has the enhanced capability of fuzzy pheromone updating and adaptive parameter tuning. The proposed ACO-optimized scheme is utilized to tune the parameters of the fuzzy sliding-mode controllers for a real ball-and-beam system. Compared to some conventional ACO algorithms, simulation and experimental results all indicate that the proposed scheme can provide better performance in the aspect of convergence rate and accuracy. © 2011 Elsevier Ltd. All rights reserved.","Ant colony optimization; Ball and beam system; Fuzzy sliding-mode control","ACO algorithms; Adaptive parameters; Ant-colony optimization; Balance control; Ball and beam systems; Control performance; Controller parameter; Convergence rates; Coupling dynamics; Design complexity; Fuzzy sliding-mode control; Improved ant colony optimization; Sliding mode controller; Stability analysis; Artificial intelligence; Controllers; Convergence of numerical methods; Optimization; Position control; Sliding mode control; Spheres; Ultrasonic devices; Algorithms",Article,Scopus,2-s2.0-80255141867
"Tang X., Liu Y., Lv C., Sun D.","Hand motion classification using a multi-channel surface electromyography sensor",2012,"Sensors",34,10.3390/s120201130,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863264787&doi=10.3390%2fs120201130&partnerID=40&md5=4446eee96027e84f2e749a326a985c36","The human hand has multiple degrees of freedom (DOF) for achieving high-dexterity motions. Identifying and replicating human hand motions are necessary to perform precise and delicate operations in many applications, such as haptic applications. Surface electromyography (sEMG) sensors are a low-cost method for identifying hand motions, in addition to the conventional methods that use data gloves and vision detection. The identification of multiple hand motions is challenging because the error rate typically increases significantly with the addition of more hand motions. Thus, the current study proposes two new methods for feature extraction to solve the problem above. The first method is the extraction of the energy ratio features in the time-domain, which are robust and invariant to motion forces and speeds for the same gesture. The second method is the extraction of the concordance correlation features that describe the relationship between every two channels of the multi-channel sEMG sensor system. The concordance correlation features of a multi-channel sEMG sensor system were shown to provide a vast amount of useful information for identification. Furthermore, a new cascaded-structure classifier is also proposed, in which 11 types of hand gestures can be identified accurately using the newly defined features. Experimental results show that the success rate for the identification of the 11 gestures is significantly high. © 2012 by the authors; licensee MDPI, Basel, Switzerland.","Cascaded; Concordance correlation; Hand motion; Multi-channel; Surface electromyography","algorithm; article; artificial intelligence; automated pattern recognition; electromyography; equipment; equipment design; genetic procedures; gesture; hand; human; instrumentation; methodology; movement (physiology); physiology; Algorithms; Artificial Intelligence; Biosensing Techniques; Electromyography; Equipment Design; Equipment Failure Analysis; Gestures; Hand; Humans; Movement; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84863264787
"Wang Z., Hope R.M., Wang Z., Ji Q., Gray W.D.","Cross-subject workload classification with a hierarchical Bayes model",2012,"NeuroImage",34,10.1016/j.neuroimage.2011.07.094,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054712832&doi=10.1016%2fj.neuroimage.2011.07.094&partnerID=40&md5=d0c105cc4fcd5c92a76d89bd95e060e2","Most of the current EEG-based workload classifiers are subject-specific; that is, a new classifier is built and trained for each human subject. In this paper we introduce a cross-subject workload classifier based on a hierarchical Bayes model. The cross-subject classifier is trained and tested with data from a group of subjects. In our work, it was trained and tested on EEG data collected from 8 subjects as they performed the Multi-Attribute Task Battery across three levels of difficulty. The accuracy of this cross-subject classifier is stable across the three levels of workload and comparable to a benchmark subject-specific neural network classifier. © 2011 Elsevier Inc.","Artificial neural network; EEG; Hierarchical Bayes model; Workload classification","accuracy; adult; article; artificial neural network; Bayes theorem; classifier; electroencephalography; female; human; human experiment; male; mathematical analysis; mental load; normal human; priority journal; probability; quality control; task performance; Algorithms; Artificial Intelligence; Bayes Theorem; Electroencephalography; Female; Humans; Male; Models, Theoretical; Pattern Recognition, Automated; Task Performance and Analysis; Workload; Young Adult",Article,Scopus,2-s2.0-80054712832
"Tian S., Wang J., Li Y., Xu X., Hou T.","Drug-likeness analysis of traditional chinese medicines: Prediction of drug-likeness using machine learning approaches",2012,"Molecular Pharmaceutics",33,10.1021/mp300198d,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870177359&doi=10.1021%2fmp300198d&partnerID=40&md5=62aca3e2bed357b0b766c345cb2e36eb","Quantitative or qualitative characterization of the drug-like features of known drugs may help medicinal and computational chemists to select higher quality drug leads from a huge pool of compounds and to improve the efficiency of drug design pipelines. For this purpose, the theoretical models for drug-likeness to discriminate between drug-like and non-drug-like based on molecular physicochemical properties and structural fingerprints were developed by using the naive Bayesian classification (NBC) and recursive partitioning (RP) techniques, and then the drug-likeness of the compounds from the Traditional Chinese Medicine Compound Database (TCMCD) was evaluated. First, the impact of molecular physicochemical properties and structural fingerprints on the prediction accuracy of drug-likeness was examined. We found that, compared with simple molecular properties, structural fingerprints were more essential for the accurate prediction of drug-likeness. Then, a variety of Bayesian classifiers were constructed by changing the ratio of drug-like to non-drug-like molecules and the size of the training set. The results indicate that the prediction accuracy of the Bayesian classifiers was closely related to the size and the degree of the balance of the training set. When a balanced training set was used, the best Bayesian classifier based on 21 physicochemical properties and the LCFP-6 fingerprint set yielded an overall leave-one-out (LOO) cross-validated accuracy of 91.4% for the 140,000 molecules in the training set and 90.9% for the 40,000 molecules in the test set. In addition, the RP classifiers with different maximum depth were constructed and compared with the Bayesian classifiers, and we found that the best Bayesian classifier outperformed the best RP model with respect to overall prediction accuracy. Moreover, the Bayesian classifier employing structural fingerprints highlights the important substructures favorable or unfavorable for drug-likeness, offering extra valuable information for getting high quality lead compounds in the early stage of the drug design/discovery process. Finally, the best Bayesian classifier was used to predict the drug-likeness of 33,961 compounds in TCMCD. Our calculations show that 59.37% of the molecules in TCMCD were identified as drug-like molecules, indicating that traditional Chinese medicines (TCMs) are therefore an excellent source of drug-like molecules. Furthermore, the important structural fingerprints in TCMCD were detected and analyzed. Considering that the pharmacology of TCMCD and MDDR (MDL Drug Data Report) was linked by the important common structural features, the potential pharmacology of the compounds in TCMCD may therefore be annotated by these important structural signatures identified from Bayesian analysis, which may be valuable to promote the development of TCMs. © 2012 American Chemical Society.",,"article; Bayesian learning; Chinese medicine; drug adsorption; drug design; drug distribution; drug excretion; drug metabolism; molecular weight; partition coefficient; physical chemistry; priority journal; Artificial Intelligence; Bayes Theorem; Drug Design; Medicine, Chinese Traditional; Models, Chemical; Molecular Structure; Pharmaceutical Preparations",Article,Scopus,2-s2.0-84870177359
"Sasikumar P., Khara S.","k-means clustering in wireless sensor networks",2012,"Proceedings - 4th International Conference on Computational Intelligence and Communication Networks, CICN 2012",33,10.1109/CICN.2012.136,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872040105&doi=10.1109%2fCICN.2012.136&partnerID=40&md5=9f71e433051c15ea305d01760c94b4bd","A wireless sensor network (WSN) consists of spatially distributed autonomous sensors to monitor physical or environmental conditions and to cooperatively pass their data through the network to a Base Station. Clustering is a critical task in Wireless Sensor Networks for energy efficiency and network stability. Clustering through Central Processing Unit in wireless sensor networks is well known and in use for a long time. Presently clustering through distributed methods is being developed for dealing with the issues like network lifetime and energy. In our work, we implemented both centralized and distributed k-means clustering algorithm in network simulator. k-means is a prototype based algorithm that alternates between two major steps, assigning observations to clusters and computing cluster centers until a stopping criterion is satisfied. Simulation results are obtained and compared which show that distributed clustering is efficient than centralized clustering. © 2012 IEEE.","clustering; k-means; network stability; ns-2; wireless sensor network","Autonomous sensors; clustering; Computing clusters; Critical tasks; Distributed clustering; Distributed methods; Environmental conditions; K-means; K-means clustering; K-Means clustering algorithm; Network lifetime; Network simulators; Network stability; ns-2; Stopping criteria; Artificial intelligence; Cluster computing; Clustering algorithms; Energy efficiency; Program processors; Wireless sensor networks",Conference Paper,Scopus,2-s2.0-84872040105
"Endriss U., Grandi U., Porello D.","Complexity of judgment aggregation",2012,"Journal of Artificial Intelligence Research",33,10.1613/jair.3708,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875148108&doi=10.1613%2fjair.3708&partnerID=40&md5=965e84e64a8687994bc3659bb55aef87","We analyse the computational complexity of three problems in judgment aggregation: (1) computing a collective judgment from a profile of individual judgments (the winner determination problem); (2) deciding whether a given agent can influence the outcome of a judgment aggregation procedure in her favour by reporting insincere judgments (the strategic manipulation problem); and (3) deciding whether a given judgment aggregation scenario is guaranteed to result in a logically consistent outcome, independently from what the judgments supplied by the individuals are (the problem of the safety of the agenda). We provide results both for specific aggregation procedures (the quota rules, the premisebased procedure, and a distance-based procedure) and for classes of aggregation procedures characterised in terms of fundamental axioms. © 2012 AI Access Foundation.",,"Distance-based; Winner determination problem; Artificial intelligence",Article,Scopus,2-s2.0-84875148108
"Piao Y., Piao M., Park K., Ryu K.H.","An ensemble correlation-based gene selection algorithm for cancer classification with gene expression data",2012,"Bioinformatics",33,10.1093/bioinformatics/bts602,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870790399&doi=10.1093%2fbioinformatics%2fbts602&partnerID=40&md5=cf7f5492f75e15bfb222795b974ce761","Motivation: Gene selection for cancer classification is one of the most important topics in the biomedical field. However, microarray data pose a severe challenge for computational techniques. We need dimension reduction techniques that identify a small set of genes to achieve better learning performance. From the perspective of machine learning, the selection of genes can be considered to be a feature selection problem that aims to find a small subset of features that has the most discriminative information for the target.Results: In this article, we proposed an Ensemble Correlation-Based Gene Selection algorithm based on symmetrical uncertainty and Support Vector Machine. In our method, symmetrical uncertainty was used to analyze the relevance of the genes, the different starting points of the relevant subset were used to generate the gene subsets and the Support Vector Machine was used as an evaluation criterion of the wrapper. The efficiency and effectiveness of our method were demonstrated through comparisons with other feature selection techniques, and the results show that our method outperformed other methods published in the literature.Availability: By request from the author. © 2012 The Author.",,"algorithm; article; artificial intelligence; classification; DNA microarray; evaluation; gene expression; gene expression profiling; genetics; human; metabolism; neoplasm; support vector machine; Algorithms; Artificial Intelligence; Gene Expression; Gene Expression Profiling; Humans; Neoplasms; Oligonucleotide Array Sequence Analysis; Support Vector Machines",Article,Scopus,2-s2.0-84870790399
"Niggemann O., Stein B., Maier A., Vodenčarević A., Büning H.K.","Learning behavior models for hybrid timed systems",2012,"Proceedings of the National Conference on Artificial Intelligence",33,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868293556&partnerID=40&md5=5e592291e2f5ad7047d6a70a0d63cc7e","A tailored model of a system is the prerequisite for various analysis tasks, such as anomaly detection, fault identification, or quality assurance. This paper deals with the algorithmic learning of a system's behavior model given a sample of observations. In particular, we consider real-world production plants where the learned model must capture timing behavior, dependencies between system variables, as well as mode switches - in short: hybrid system's characteristics. Usually, such model formation tasks are solved by human engineers, entailing the well-known bunch of problems including knowledge acquisition, development cost, or lack of experience. Our contributions to the outlined field are as follows. (1) We present a taxonomy of learning problems related to model formation tasks. As a result, an important open learning problem for the domain of production system is identified: The learning of hybrid timed automata. (2) For this class of models, the learning algorithm HyBUTLA is presented. This algorithm is the first of its kind to solve the underlying model formation problem at scalable precision. (3) We present two case studies that illustrate the usability of this approach in realistic settings. (4) We give a proof for the learning and runtime properties of HyBUTLA. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.","Machine learning; Model formation; Simulation; Technical systems","Algorithmic learning; Anomaly detection; Behavior model; Development costs; Fault identifications; Learning behavior; Learning problem; Mode switches; Open learning; Production plant; Production system; Runtimes; Simulation; System variables; Technical systems; Timed Automata; Timed systems; Artificial intelligence; Automata theory; Hybrid systems; Knowledge acquisition; Learning algorithms; Learning systems; Production engineering; Quality assurance; Time sharing systems; Computer simulation",Conference Paper,Scopus,2-s2.0-84868293556
"He Z., Chen C., Bu J., Wang C., Zhang L., Cai D., He X.","Document summarization based on data reconstruction",2012,"Proceedings of the National Conference on Artificial Intelligence",33,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868295095&partnerID=40&md5=6ca42a3601813a530b8ed263a0f48920","Document summarization is of great value to many real world applications, such as snippets generation for search results and news headlines generation. Traditionally, document summarization is implemented by extracting sentences that cover the main topics of a document with a minimum redundancy. In this paper, we take a different perspective from data reconstruction and propose a novel framework named Document Summarization based on Data Reconstruction (DSDR). Specifically, our approach generates a summary which consist of those sentences that can best reconstruct the original document. To model the relationship among sentences, we introduce two objective functions: (1) linear reconstruction, which approximates the document by linear combinations of the selected sentences; (2) nonnegative linear reconstruction, which allows only additive, not subtractive, linear combinations. In this framework, the reconstruction error becomes a natural criterion for measuring the quality of the summary. For each objective function, we develop an efficient algorithm to solve the corresponding optimization problem. Extensive experiments on summarization benchmark data sets DUC 2006 and DUC 2007 demonstrate the effectiveness of our proposed approach. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Benchmark data; Data reconstruction; Document summarization; Linear combinations; Linear reconstruction; Objective functions; Optimization problems; Real-world application; Reconstruction error; Search results; Algorithms; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868295095
"Tapkan P., Ozbakir L., Baykasoglu A.","Modeling and solving constrained two-sided assembly line balancing problem via bee algorithms",2012,"Applied Soft Computing Journal",33,10.1016/j.asoc.2012.06.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865842759&doi=10.1016%2fj.asoc.2012.06.003&partnerID=40&md5=b47e4cc5e7060b669e5f237412fceca3","Designing and operating two-sided assembly lines are crucial for manufacturing companies which assemble large-sized products such as trucks, buses and industrial refrigerators. This type of assembly line structure has several advantages over one-sided assembly lines such as shortened line length and reduced throughput time. The research area has recently focused on balancing two-sided assembly lines owing to these advantages. However, due to the complex structure of this problem, some practical constraints have been disregarded or have not been fully incorporated. In order to overcome these deficiencies, a fully constrained two-sided assembly line balancing problem is addressed in this research paper. Initially, a mathematical programming model is presented in order to describe the problem formally. Due to the problem complexity, two different swarm intelligence based search algorithms are implemented to solve large-sized instances. Bees algorithm and artificial bee colony algorithm have been applied to the fully constrained two-sided assembly line balancing problem so as to minimize the number of workstations and to obtain a balanced line. An extensive computational study has also been performed and the comparative results have been evaluated. © 2012 Elsevier B.V. All rights reserved.","Artificial bee colony algorithm; Bees algorithm; Integer programming; Swarm intelligence; Two-sided assembly line balancing","Artificial bee colony algorithms; Assembly line; Balanced line; Bee Algorithm; Bees algorithms; Complex structure; Computational studies; Manufacturing companies; Mathematical programming models; Problem complexity; Research papers; Search Algorithms; Swarm Intelligence; Throughput time; Two-sided assembly line balancing; Two-sided assembly lines; Artificial intelligence; Assembly; Assembly machines; Computer programming; Integer programming; Product design; Algorithms",Article,Scopus,2-s2.0-84865842759
"Li N., Yu Y., Zhou Z.-H.","Diversity regularized ensemble pruning",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",33,10.1007/978-3-642-33460-3_27,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866869028&doi=10.1007%2f978-3-642-33460-3_27&partnerID=40&md5=5185a2a925324547509f4d2fe333b5cb","Diversity among individual classifiers is recognized to play a key role in ensemble, however, few theoretical properties are known for classification. In this paper, by focusing on the popular ensemble pruning setting (i.e., combining classifier by voting and measuring diversity in pairwise manner), we present a theoretical study on the effect of diversity on the generalization performance of voting in the PAC-learning framework. It is disclosed that the diversity is closely-related to the hypothesis space complexity, and encouraging diversity can be regarded to apply regularization on ensemble methods. Guided by this analysis, we apply explicit diversity regularization to ensemble pruning, and propose the Diversity Regularized Ensemble Pruning (DREP) method. Experimental results show the effectiveness of DREP. © 2012 Springer-Verlag.","diversity; diversity regularization; ensemble pruning","Combining classifiers; diversity; diversity regularization; Ensemble methods; Ensemble pruning; Generalization performance; Hypothesis space; Individual classifiers; PAC learning; Theoretical study; Artificial intelligence; Learning systems",Conference Paper,Scopus,2-s2.0-84866869028
"Hackmack K., Paul F., Weygandt M., Allefeld C., Haynes J.-D.","Multi-scale classification of disease using structural MRI and wavelet transform",2012,"NeuroImage",33,10.1016/j.neuroimage.2012.05.022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861633811&doi=10.1016%2fj.neuroimage.2012.05.022&partnerID=40&md5=c8652eacd3f7fb54568b72a485dc628d","Recently, multivariate analysis algorithms have become a popular tool to diagnose neurological diseases based on neuroimaging data. Most studies, however, are biased for one specific scale, namely the scale given by the spatial resolution (i.e. dimension) of the data. In the present study, we propose to use the dual-tree complex wavelet transform to extract information on different spatial scales from structural MRI data and show its relevance for disease classification. Based on the magnitude representation of the complex wavelet coefficients calculated from the MR images, we identified a new class of features taking scale, directionality and potentially local information into account simultaneously. By using a linear support vector machine, these features were shown to discriminate significantly between spatially normalized MR images of 41 patients suffering from multiple sclerosis and 26 healthy controls. Interestingly, the decoding accuracies varied strongly among the different scales and it turned out that scales containing low frequency information were partly superior to scales containing high frequency information. Usually, this type of information is neglected since most decoding studies use only the original scale of the data. In conclusion, our proposed method has not only a high potential to assist in the diagnostic process of multiple sclerosis, but can be applied to other diseases or general decoding problems in structural or functional MRI. © 2012 Elsevier Inc.","Multi-scale classification; Multiple sclerosis; Structural MRI; Support vector machine; Wavelet transform","accuracy; adult; article; clinical article; controlled study; data extraction; disease classification; dual tree complex wavelet transform; female; human; image processing; male; multiple sclerosis; nuclear magnetic resonance imaging; priority journal; support vector machine; wavelet analysis; Adult; Algorithms; Artificial Intelligence; Female; Humans; Magnetic Resonance Imaging; Male; Middle Aged; Multiple Sclerosis; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Wavelet Analysis",Article,Scopus,2-s2.0-84861633811
"Köse C., Şevik U., Ikibaş C., Erdöl H.","Simple methods for segmentation and measurement of diabetic retinopathy lesions in retinal fundus images",2012,"Computer Methods and Programs in Biomedicine",33,10.1016/j.cmpb.2011.06.007,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861966362&doi=10.1016%2fj.cmpb.2011.06.007&partnerID=40&md5=3757313f75734817b86c5410ac9a1b25","Diabetic retinopathy (DR) is one of the most important complications of diabetes mellitus, which causes serious damages in the retina, consequently visual loss and sometimes blindness if necessary medical treatment is not applied on time. One of the difficulties in this illness is that the patient with diabetes mellitus requires a continuous screening for early detection. So far, numerous methods have been proposed by researchers to automate the detection process of DR in retinal fundus images. In this paper, we developed an alternative simple approach to detect DR. This method was built on the inverse segmentation method, which we suggested before to detect Age Related Macular Degeneration (ARMDs). Background image approach along with inverse segmentation is employed to measure and follow up the degenerations in retinal fundus images. Direct segmentation techniques generate unsatisfactory results in some cases. This is because of the fact that the texture of unhealthy areas such as DR is not homogenous. The inverse method is proposed to exploit the homogeneity of healthy areas rather than dealing with varying structure of unhealthy areas for segmenting bright lesions (hard exudates and cotton wool spots). On the other hand, the background image, dividing the retinal image into high and low intensity areas, is exploited in segmentation of hard exudates and cotton wool spots, and microaneurysms (MAs) and hemorrhages (HEMs), separately. Therefore, a complete segmentation system is developed for segmenting DR, including hard exudates, cotton wool spots, MAs, and HEMs. This application is able to measure total changes across the whole retinal image. Hence, retinal images that belong to the same patients are examined in order to monitor the trend of the illness. To make a comparison with other methods, a Naïve Bayes method is applied for segmentation of DR. The performance of the system, tested on different data sets including various qualities of retinal fundus images, is over 95% in detection of the optic disc (OD), and 90% in segmentation of the DR. © 2011 Elsevier Ireland Ltd.","Automatic diagnosis of DR; Automatic segmentation; Background image extraction; DR; Medical image analysis; Optic disc; Retinal images","Automatic diagnosis; Automatic segmentations; Background image; DR; Medical image analysis; Optic disc; Retinal image; Cotton; Diagnosis; Eye protection; Inverse problems; Ophthalmology; Wool; Yarn; Image segmentation; article; Bayesian learning; diabetic retinopathy; disease course; eye fundus; human; image analysis; image quality; microaneurysm; optic disk; retina hemorrhage; retina macula age related degeneration; Algorithms; Artificial Intelligence; Diabetic Retinopathy; Fluorescein Angiography; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Retinoscopy; Sensitivity and Specificity",Article,Scopus,2-s2.0-84861966362
"Glaab E., Bacardit J., Garibaldi J.M., Krasnogor N.","Using rule-based machine learning for candidate disease gene prioritization and sample classification of cancer gene expression data",2012,"PLoS ONE",33,10.1371/journal.pone.0039932,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863856522&doi=10.1371%2fjournal.pone.0039932&partnerID=40&md5=b251fc3280402bc81ae3a0c694f26bc3","Microarray data analysis has been shown to provide an effective tool for studying cancer and genetic diseases. Although classical machine learning techniques have successfully been applied to find informative genes and to predict class labels for new samples, common restrictions of microarray analysis such as small sample sizes, a large attribute space and high noise levels still limit its scientific and clinical applications. Increasing the interpretability of prediction models while retaining a high accuracy would help to exploit the information content in microarray data more effectively. For this purpose, we evaluate our rule-based evolutionary machine learning systems, BioHEL and GAssist, on three public microarray cancer datasets, obtaining simple rule-based models for sample classification. A comparison with other benchmark microarray sample classifiers based on three diverse feature selection algorithms suggests that these evolutionary learning techniques can compete with state-of-the-art methods like support vector machines. The obtained models reach accuracies above 90% in two-level external cross-validation, with the added value of facilitating interpretation by using only combinations of simple if-then-else rules. As a further benefit, a literature mining analysis reveals that prioritizations of informative genes extracted from BioHEL's classification rule sets can outperform gene rankings obtained from a conventional ensemble feature selection in terms of the pointwise mutual information between relevant disease terms and the standardized names of top-ranked genes. © 2012 Glaab et al.",,"accuracy; algorithm; article; cancer classification; data processing; DNA microarray; gene expression; gene identification; intermethod comparison; machine learning; model; quality control; rule based machine learning; support vector machine; tumor gene; validation process; Algorithms; Artificial Intelligence; Breast Neoplasms; Databases, Genetic; Female; Gene Expression Profiling; Gene Expression Regulation, Neoplastic; Humans; Lymphoma; Male; Neoplasm Proteins; Oligonucleotide Array Sequence Analysis; Prostatic Neoplasms",Article,Scopus,2-s2.0-84863856522
"Ferreira L., Borenstein D.","A fuzzy-Bayesian model for supplier selection",2012,"Expert Systems with Applications",33,10.1016/j.eswa.2012.01.068,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858342275&doi=10.1016%2fj.eswa.2012.01.068&partnerID=40&md5=e2048959edc8115f84039df282ee832d","The selection supplier problem has received a lot of attention from academics in recent years. Several models were developed in the literature, combining consolidated operations research and artificial intelligence methods and techniques. However, the tools presented in the literature neglected learning and adaptation, since this decision making process is approached as a static one rather than a highly dynamic process. Delays, lack of capacity, quality related issues are common examples of dynamic aspects that have a direct impact on long-term relationships with suppliers. This paper presents a novel method based on the integration of influence diagram and fuzzy logic to rank and evaluate suppliers. The model was developed to support managers in exploring the strengths and weaknesses of each alternative, to assist the setting of priorities between conflicting criteria, to study the sensitivity of the behavior of alternatives to changes in underlying decision situations, and finally to identify a preferred course of action. To be effective, the computational implementation of the method was embedded into an information system that includes several functionalities such as supply chain simulation and supplier's databases. A case study in the biodiesel supply chain illustrates the effectiveness of the developed method. © 2012 Elsevier Ltd. All rights reserved.","Bayesian networks; Fuzzy; Influence diagrams; Supplier selection; Supply chain","Artificial intelligence methods; Computational implementations; Course of action; Decision making process; Decision situation; Direct impact; Dynamic aspects; Dynamic process; Fuzzy; Influence diagram; Learning and adaptation; Supplier selection; Supply chain simulation; Artificial intelligence; Expert systems; Fuzzy logic; Supply chains; Bayesian networks",Article,Scopus,2-s2.0-84858342275
"Erol R., Sahin C., Baykasoglu A., Kaplanoglu V.","A multi-agent based approach to dynamic scheduling of machines and automated guided vehicles in manufacturing systems",2012,"Applied Soft Computing Journal",33,10.1016/j.asoc.2012.02.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859594279&doi=10.1016%2fj.asoc.2012.02.001&partnerID=40&md5=df97617bb93da7ba2c5fbf44524ef737","In real manufacturing environments, the control of system elements such as automated guided vehicles has some difficulties when planning operations dynamically. Multi agent-based systems, a newly maturing area of distributed artificial intelligence, provide some effective mechanisms for the management of such dynamic operations in manufacturing environments. This paper proposes a multi-agent based scheduling approach for automated guided vehicles and machines within a manufacturing system. The proposed multi-agent based approach works under a real-time environment and generates feasible schedules using negotiation/bidding mechanisms between agents. This approach is tested on off-line scheduling problems from the literature. The results show that our approach is capable of generating good schedules in real time comparable with the optimization algorithms and the frequently used dispatching rules. © 2012 Elsevier B.V. All rights reserved.","Automated guided vehicles; Manufacturing systems; Multi-agent systems; Online scheduling","Agent-based systems; Automated guided vehicles; Dispatching rules; Distributed Artificial Intelligence; Dynamic operations; Dynamic scheduling; Effective mechanisms; Feasible schedule; Manufacturing environments; Multi-agent based approach; Off line scheduling; Online scheduling; Optimization algorithms; Real time; Real-time environment; System elements; Algorithms; Dynamics; Mobile robots; Multi agent systems; Scheduling; Manufacture",Article,Scopus,2-s2.0-84859594279
"Desaraju V.R., How J.P.","Decentralized path planning for multi-agent teams with complex constraints",2012,"Autonomous Robots",33,10.1007/s10514-012-9275-2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862129361&doi=10.1007%2fs10514-012-9275-2&partnerID=40&md5=7e2f3de86c52948deefd141583e71229","This paper presents a novel approach to address the challenge of planning paths for multi-agent systems subject to complex constraints. The technique, called the Decentralized Multi-Agent Rapidly-exploring Random Tree (DMA-RRT) algorithm, extends the Closed-loop RRT (CL-RRT) algorithm to handle multiple agents while retaining its ability to plan quickly. A core component of the DMA-RRT algorithm is a merit-based token passing coordination strategy that makes use of the tree of feasible trajectories grown in the CL-RRT algorithm to dynamically update the order in which agents replan. The reordering is based on a measure of each agent's incentive to change the plan and allows agents with a greater potential improvement to replan sooner, which is demonstrated to improve the team's overall performance compared to a traditional, scripted replan order. The main contribution of the work is a version of the algorithm, called Cooperative DMA-RRT, which introduces a cooperation strategy that allows an agent to modify its teammates' plans in order to select paths that reduce their combined cost. This modification further improves team performance and avoids certain common deadlock scenarios. The paths generated by both algorithms are proven to satisfy inter-agent constraints, such as collision avoidance, and numerous simulation and experimental results are presented to demonstrate their performance. © 2011 Springer-Verlag.","Collision avoidance; Cooperative planning; RRT","Closed-loop; Co-operation strategy; Complex constraints; Cooperative planning; Coordination strategy; Core components; Multi agent system (MAS); Multiagent teams; Multiple agents; Rapidly-exploring random trees; RRT; Team performance; Token passing; Algorithms; Collision avoidance; Forestry; Intelligent agents; Motion planning; Multi agent systems; Trees (mathematics); Algorithms; Artificial Intelligence; Scheduling",Article,Scopus,2-s2.0-84862129361
"Yousefi S., Azmi R., Zahedi M.","Brain tissue segmentation in MR images based on a hybrid of MRF and social algorithms",2012,"Medical Image Analysis",33,10.1016/j.media.2012.01.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859428914&doi=10.1016%2fj.media.2012.01.001&partnerID=40&md5=124cb46cc0336a5ea96c296348e56fed","Effective abnormality detection and diagnosis in Magnetic Resonance Images (MRIs) requires a robust segmentation strategy. Since manual segmentation is a time-consuming task which engages valuable human resources, automatic MRI segmentations received an enormous amount of attention. For this goal, various techniques have been applied. However, Markov Random Field (MRF) based algorithms have produced reasonable results in noisy images compared to other methods. MRF seeks a label field which minimizes an energy function. The traditional minimization method, simulated annealing (SA), uses Monte Carlo simulation to access the minimum solution with heavy computation burden. For this reason, MRFs are rarely used in real time processing environments. This paper proposed a novel method based on MRF and a hybrid of social algorithms that contain an ant colony optimization (ACO) and a Gossiping algorithm which can be used for segmenting single and multispectral MRIs in real time environments. Combining ACO with the Gossiping algorithm helps find the better path using neighborhood information. Therefore, this interaction causes the algorithm to converge to an optimum solution faster. Several experiments on phantom and real images were performed. Results indicate that the proposed algorithm outperforms the traditional MRF and hybrid of MRF-ACO in speed and accuracy. © 2012 Elsevier B.V.","Ant colony optimization (ACO); Brain segmentation; Gossiping algorithm; Magnetic resonance image (MRI); Markov random field (MRF)","Abnormality detection; Ant Colony Optimization (ACO); Brain segmentation; Brain tissue; Computation burden; Energy functions; Magnetic resonance image (MRI); Magnetic resonance images; Manual segmentation; Markov random field; Minimization methods; Minimum solution; Monte Carlo Simulation; MR images; MRI segmentation; Multi-spectral; Neighborhood information; Noisy image; Optimum solution; Real images; Real-time environment; Realtime processing; Robust segmentation; Time-consuming tasks; Artificial intelligence; Image segmentation; Magnetic resonance imaging; Monte Carlo methods; Simulated annealing; Algorithms; accuracy; algorithm; article; automation; brain tissue; human; image analysis; Monte Carlo method; nuclear magnetic resonance imaging; priority journal; simulation; social participation; Algorithms; Brain; Data Interpretation, Statistical; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Markov Chains; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84859428914
"Wall D.P., Kosmicki J., Deluca T.F., Harstad E., Fusaro V.A.","Use of machine learning to shorten observation-based screening and diagnosis of autism",2012,"Translational Psychiatry",33,10.1038/tp.2012.10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859589892&doi=10.1038%2ftp.2012.10&partnerID=40&md5=dc33ed4c2e9a9644845017b5d3fc0aca","The Autism Diagnostic Observation Schedule-Generic (ADOS) is one of the most widely used instruments for behavioral evaluation of autism spectrum disorders. It is composed of four modules, each tailored for a specific group of individuals based on their language and developmental level. On average, a module takes between 30 and 60 min to deliver. We used a series of machine-learning algorithms to study the complete set of scores from Module 1 of the ADOS available at the Autism Genetic Resource Exchange (AGRE) for 612 individuals with a classification of autism and 15 non-spectrum individuals from both AGRE and the Boston Autism Consortium (AC). Our analysis indicated that 8 of the 29 items contained in Module 1 of the ADOS were sufficient to classify autism with 100% accuracy. We further validated the accuracy of this eight-item classifier against complete sets of scores from two independent sources, a collection of 110 individuals with autism from AC and a collection of 336 individuals with autism from the Simons Foundation. In both cases, our classifier performed with nearly 100% sensitivity, correctly classifying all but two of the individuals from these two resources with a diagnosis of autism, and with 94% specificity on a collection of observed and simulated non-spectrum controls. The classifier contained several elements found in the ADOS algorithm, demonstrating high test validity, and also resulted in a quantitative score that measures classification confidence and extremeness of the phenotype. With incidence rates rising, the ability to classify autism effectively and quickly requires careful design of assessment and diagnostic tools. Given the brevity, accuracy and quantitative nature of the classifier, results from this study may prove valuable in the development of mobile tools for preliminary evaluation and clinical prioritization-in particular those focused on assessment of short home videos of children-that speed the pace of initial evaluation and broaden the reach to a significantly larger percentage of the population at risk. © 2012 Macmillan Publishers Limited All rights reserved.","algorithm to classify autism; autism classification; autism classifier; autism diagnostic observation schedule; autism spectrum disorders; machine learning; rapid detection of autism","article; autism; Autism Diagnostic Observation Schedule; diagnostic test; morbidity; phenotype; screening test; sensitivity and specificity; algorithm; artificial intelligence; autism; child; classification; computer assisted diagnosis; female; genetic predisposition; genetics; human; male; mass screening; observation; personality test; psychometry; reference value; reproducibility; statistics; task performance; Algorithms; Artificial Intelligence; Child; Child Development Disorders, Pervasive; Diagnosis, Computer-Assisted; Female; Genetic Predisposition to Disease; Humans; Male; Mass Screening; Observation; Personality Assessment; Psychometrics; Reference Values; Reproducibility of Results; Time and Motion Studies",Article,Scopus,2-s2.0-84859589892
"Mossakowski T., Moratz R.","Qualitative reasoning about relative direction of oriented points",2012,"Artificial Intelligence",33,10.1016/j.artint.2011.10.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857369421&doi=10.1016%2fj.artint.2011.10.003&partnerID=40&md5=35b9e9a89ee7a96f3d6fccd8ca1f9bca","An important issue in qualitative spatial reasoning is the representation of relative directions. In this paper we present simple geometric rules that enable reasoning about the relative direction between oriented points. This framework, the oriented point algebra OPRAm, has a scalable granularity m. We develop a simple algorithm for computing the OPRAm composition tables and prove its correctness. Using a composition table, algebraic closure for a set of OPRAm statements is very useful for solving spatial navigation tasks. It turns out that scalable granularity is useful in these navigation tasks. © 2012 Elsevier B.V. All rights reserved.","Constraint-based reasoning; Qualitative simulation; Qualitative spatial reasoning","Composition tables; Constraint-based reasoning; Geometric rules; Navigation tasks; Point algebras; Qualitative reasoning; Qualitative simulation; Qualitative spatial reasoning; SIMPLE algorithm; Spatial navigation; Algebra; Artificial intelligence",Article,Scopus,2-s2.0-84857369421
"Ghodrati A., Lotfi S.","A hybrid CS/PSO algorithm for global optimization",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",33,10.1007/978-3-642-28493-9_11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858727791&doi=10.1007%2f978-3-642-28493-9_11&partnerID=40&md5=150ebc5fc1c0d8e7c49f4a2838a185a0","This paper presents the hybrid approach of two nature inspired metaheuristic algorithms; Cuckoo Search (CS) and Particle Swarm Optimization (PSO) for solving optimization problems. Cuckoo birds lay their own eggs to other host birds. If the host birds discover the alien birds, they will leave the nest or throw the egg away. Cuckoo birds migrate to the environments that reduce the chance of their eggs to be discovered by the host birds. In standard CS, cuckoo birds experience new places by the Lévy Flight. In the proposed hybrid algorithm, cuckoo birds are aware of each other positions and make use of swarm intelligence in PSO in order to reach to better solutions. Experimental results are examined with some standard benchmark functions and the results show a promising performance of this algorithm. © 2012 Springer-Verlag.","Cuckoo Search; Global optimization; Metaheuristic and Hybrid evolutionary algorithm; PSO","Benchmark functions; Cuckoo searches; Hybrid algorithms; Hybrid approach; Meta heuristic algorithm; Metaheuristic; Optimization problems; PSO; Swarm Intelligence; Algorithms; Artificial intelligence; Benchmarking; Database systems; Global optimization; Search engines; Particle swarm optimization (PSO)",Conference Paper,Scopus,2-s2.0-84858727791
"Tarko A.P.","Use of crash surrogates and exceedance statistics to estimate road safety",2012,"Accident Analysis and Prevention",33,10.1016/j.aap.2011.07.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856108231&doi=10.1016%2fj.aap.2011.07.008&partnerID=40&md5=f89e770146467a9591cf8e87ed9757c4","The limited ability of existing safety models to properly reflect crash causality has its source in cross-sectional analysis applied to the estimation of the intrinsically complex safety factors with highly aggregated and frequently poor quality of data. The adequacy of the data may be improved thanks to the unprecedented progress in sensing technologies and the invention of the naturalistic driving method of data collection. Proposed in this paper is a new modeling paradigm that integrates several types of safety models. The primary improvement results from a more adequate representation of the crash occurrence process by incorporating crash precursor events into the modeling framework. A Pareto-based estimating method for the likelihood of a collision occurrence, given a precursor event, is explained and illustrated with the simple example of road departures. © 2011 Elsevier Ltd. All rights reserved.","Continuum of traffic events; Crash causality; Generalized Pareto distribution; Safety modeling; Surrogate events; Traffic conflicts","Crash causality; Generalized Pareto Distributions; Surrogate events; Traffic conflicts; Traffic event; Roads and streets; Safety factor; Estimation; article; artificial intelligence; biomechanics; car driving; computer program; epidemiology; high risk behavior; human; information processing; mathematical computing; methodology; safety; statistical model; statistics; traffic accident; videorecording; Accidents, Traffic; Artificial Intelligence; Automobile Driving; Biomechanics; Causality; Data Collection; Humans; Likelihood Functions; Mathematical Computing; Models, Statistical; Research Design; Risk-Taking; Safety; Software; Video Recording",Article,Scopus,2-s2.0-84856108231
"Grundmann J., Schütze N., Schmitz G.H., Al-Shaqsi S.","Towards an integrated arid zone water management using simulation-based optimisation",2012,"Environmental Earth Sciences",33,10.1007/s12665-011-1253-z,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856957416&doi=10.1007%2fs12665-011-1253-z&partnerID=40&md5=3676ff51919ff852d3dbdace3ec5aa9f","For ensuring both optimal sustainable water resources management and long-term planning in a changing arid environment, we propose an integrated Assessment-, Prognoses-, Planning- and Management tool (APPM). The new APPM integrates the complex interactions of the strongly nonlinear meteorological, hydrological and agricultural phenomena, considering the socio-economic aspects. It aims at achieving best possible solutions for water allocation, groundwater storage and withdrawals including saline water management together with a substantial increase of the water use efficiency employing novel optimisation strategies for irrigation control and scheduling. To obtain a robust and fast operation of the water management system, it unites process modeling with artificial intelligence tools and evolutionary optimisation techniques for managing both water quality and quantity. We demonstrate some key components of our methodology by an exemplary application to the south Al-Batinah region in the Sultanate of Oman which is affected by saltwater intrusion into a coastal aquifer due to excessive groundwater withdrawal for irrigated agriculture. We show the effectiveness and functionality of a new simulation-based water management system for the optimisation and evaluation of different irrigation practices, crop pattern and resulting abstraction scenarios. The results of several optimisation runs indicate that due to contradicting objectives, such as profit-oriented agriculture versus aquifer sustainability only a multi-objective optimisation can provide sustainable solutions for the management of the water resources in respect of the environment as well as the socio-economic development. © 2011 Springer-Verlag.","Artificial intelligence; Density dependent groundwater flow modelling; Integrated water resources management; Irrigation; Multi-criteria optimisation","Arid environments; Arid zones; Artificial intelligence tools; Coastal aquifers; Complex interaction; Density dependent; Evolutionary optimisation; Excessive groundwater withdrawal; Fast operation; Groundwater storage; Integrated assessment; Integrated water resources management; Irrigated agriculture; Irrigation practices; Key component; Long term planning; Management tool; Multi-criteria; Optimisations; Possible solutions; Process Modeling; Simulation-based; Socio-economic development; Socioeconomic aspects; Strongly nonlinear; Sultanate of Oman; Sustainable solution; Sustainable water resources; Water allocations; Water management systems; Water use efficiency; Aquifers; Arid regions; Artificial intelligence; Economics; Groundwater flow; Irrigation; Management; Multiobjective optimization; Profitability; Sustainable development; Water conservation; Water management; Water quality; Water supply; Environmental management; arid region; artificial intelligence; coastal aquifer; groundwater; hydrological cycle; hydrological modeling; integrated approach; irrigation; meteorology; numerical model; optimization; saline intrusion; salinity; sustainability; sustainable development; water management; water planning; water resource; water use efficiency; Batinah; Oman",Article,Scopus,2-s2.0-84856957416
"Cummings M.L., How J.P., Whitten A., Toupet O.","The impact of human-automation collaboration in decentralized multiple unmanned vehicle control",2012,"Proceedings of the IEEE",33,10.1109/JPROC.2011.2174104,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857360727&doi=10.1109%2fJPROC.2011.2174104&partnerID=40&md5=6b16e03b75a217eaf95d67b0999b143c","For future systems that require one or a small team of operators to supervise a network of automated agents, automated planners are critical since they are faster than humans for path planning and resource allocation in multivariate, dynamic, time-pressured environments. However, such planners can be brittle and unable to respond to emergent events. Human operators can aid such systems by bringing their knowledge-based reasoning and experience to bear. Given a decentralized task planner and a goal-based operator interface for a network of unmanned vehicles in a search, track, and neutralize mission, we demonstrate with a human-on-the-loop experiment that humans guiding these decentralized planners improved system performance by up to 50%. However, those tasks that required precise and rapid calculations were not significantly improved with human aid. Thus, there is a shared space in such complex missions for human-automation collaboration. © 2011 IEEE.","Command and control; decentralized task planning; decision support systems; human supervisory control; human-automation interaction; unmanned vehicles","Command and control; Decision supports; Human supervisory control; Human-automation interactions; Task planning; Artificial intelligence; Automation; Command and control systems; Control system synthesis; Decision support systems; Motion planning; Planning; Unmanned vehicles",Conference Paper,Scopus,2-s2.0-84857360727
"Zhang Y., Puterman M.L., Nelson M., Atkins D.","A simulation optimization approach to long-term care capacity planning",2012,"Operations Research",33,10.1287/opre.1110.1026,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861477623&doi=10.1287%2fopre.1110.1026&partnerID=40&md5=028fcf60aed6cfc4089bf064d235f06f","This paper describes a methodology for setting long-term care capacity levels over a multiyear planning horizon to achieve target wait time service levels. Our approach integrates demographic and survival analysis, discrete event simulation, and optimization. Based on this methodology, we developed a decision support system for use in practice. We illustrate this approach through two case studies: one for a regional health authority in British Columbia, Canada, and the other for a long-term care facility. We also compare our approach to the fixed ratio approach used in practice and the SIPP (stationary, independent, period by period) and MOL (modified offered load) approaches developed in the call center literature. Our results suggest that our approach is preferable. The fixed ratio approach lacks a rigorous foundation, and the SIPP and MOL approaches do not perform reliably mainly because of long service times. We conclude the paper with policy recommendations. © 2012 INFORMS.","Capacity planning; Long-term care; Optimization; Service level; Survival analysis; simulation","British Columbia , Canada; Call centers; Capacity planning; Long term care; Multi-year planning; Service levels; Service time; Simulation optimization; Survival analysis; Artificial intelligence; Bioinformatics; Decision support systems; Discrete event simulation; Optimization",Article,Scopus,2-s2.0-84861477623
"Liu F., Er M.J.","A novel efficient learning algorithm for self-generating fuzzy neural network with applications",2012,"International Journal of Neural Systems",33,10.1142/S0129065712003067,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862968287&doi=10.1142%2fS0129065712003067&partnerID=40&md5=93677531e290a75b9a6b783c2d392de1","In this paper, a novel efficient learning algorithm towards self-generating fuzzy neural network (SGFNN) is proposed based on ellipsoidal basis function (EBF) and is functionally equivalent to a Takagi-Sugeno-Kang (TSK) fuzzy system. The proposed algorithm is simple and efficient and is able to generate a fuzzy neural network with high accuracy and compact structure. The structure learning algorithm of the proposed SGFNN combines criteria of fuzzy-rule generation with a pruning technology. The Kalman filter (KF) algorithm is used to adjust the consequent parameters of the SGFNN. The SGFNN is employed in a wide range of applications ranging from function approximation and nonlinear system identification to chaotic time-series prediction problem and real-world fuel consumption prediction problem. Simulation results and comparative studies with other algorithms demonstrate that a more compact architecture with high performance can be obtained by the proposed algorithm. In particular, this paper presents an adaptive modeling and control scheme for drug delivery system based on the proposed SGFNN. Simulation study demonstrates the ability of the proposed approach for estimating the drug's effect and regulating blood pressure at a prescribed level. © 2012 World Scientific Publishing Company.","Criteria of generation and pruning; Ellipsoidal Basis Function (EBF); Kalman filter (KF) algorithm; Self-generating fuzzy neural network","Adaptive modeling; Chaotic time series prediction; Compact architecture; Compact structures; Comparative studies; Criteria of generation and pruning; Drug delivery system; Efficient learning; Ellipsoidal basis functions; Function approximation; Prediction problem; Simulation studies; Structure learning algorithm; Takagi-Sugeno-Kang fuzzy system; Adaptive control systems; Blood pressure; Chaotic systems; Drug delivery; Fuzzy filters; Fuzzy neural networks; Kalman filters; Learning algorithms; algorithm; article; artificial intelligence; artificial neural network; blood pressure; drug delivery system; drug effect; fuzzy logic; learning; Algorithms; Artificial Intelligence; Blood Pressure; Drug Delivery Systems; Fuzzy Logic; Learning; Neural Networks (Computer)",Article,Scopus,2-s2.0-84862968287
"Rousseau R.","Basic properties of both percentile rank scores and the I3 indicator",2012,"Journal of the American Society for Information Science and Technology",33,10.1002/asi.21684,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857372737&doi=10.1002%2fasi.21684&partnerID=40&md5=bb62fdc1765bf58c42a34f8b74b0f96a","We introduce the notions of congruous indicator of relative performance and congruous indicator of absolute performance. These notions are very similar to the notions of independence and consistency,yet slightly different. It is shown that percentile rank scores, as recently introduced by Leydesdorff, Bornmann, Mutz, and Opthof (2011), are strictly congruous indicators of relative performance, and similarly, that the Integrated Impact Indicator (I3), introduced by Leydesdorff and Bornmann (2011), is a strictly congruous indicator of absolute performance. Our analysis highlights the challenge of finding adequate axioms for ranking and for research evaluation. © 2011 ASIS&T.",,"Absolute performance; Basic properties; Impact indicators; Rank scores; Relative performance; Research evaluation; Artificial intelligence; Software engineering",Article,Scopus,2-s2.0-84857372737
"Raskutti G., Wainwright M.J., Yu B.","Minimax-optimal rates for sparse additive models over kernel classes via convex programming",2012,"Journal of Machine Learning Research",33,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857824105&partnerID=40&md5=6e1538e77a59a4d83134d31ec4cc73f3","Sparse additive models are families of d-variate functions with the additive decomposition f* = Σ j∈S f* j, where S is an unknown subset of cardinality s ≪ d. In this paper, we consider the case where each univariate component function f* j lies in a reproducing kernel Hilbert space (RKHS), and analyze a method for estimating the unknown function f* based on kernels combined with ℓ 1-type convex regularization. Working within a high-dimensional framework that allows both the dimension d and sparsity s to increase with n, we derive convergence rates in the L 2(ℙ) and L 2(ℙ n) norms over the class F d,s,H of sparse additive models with each univariate function f* j in the unit ball of a univariate RKHS with bounded kernel function. We complement our upper bounds by deriving minimax lower bounds on the L 2(ℙ) error, thereby showing the optimality of our method. Thus, we obtain optimal minimax rates for many interesting classes of sparse additive models, including polynomials, splines, and Sobolev classes. We also show that if, in contrast to our univariate conditions, the d-variate function class is assumed to be globally bounded, then much faster estimation rates are possible for any sparsity s = Ω(√n), showing that global boundedness is a significant restriction in the high-dimensional setting. © 2012 Garvesh Raskutti, Martin J. Wainwright and Bin Yu.","Convex; Kernel; Minimax; Non-parametric; Sparsity","Convex; Kernel; Minimax; Non-parametric; Sparsity; Artificial intelligence; Software engineering; Optimization",Article,Scopus,2-s2.0-84857824105
"Nguyen T.-P., Ho T.-B.","Detecting disease genes based on semi-supervised learning and protein-protein interaction networks",2012,"Artificial Intelligence in Medicine",33,10.1016/j.artmed.2011.09.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-83555177301&doi=10.1016%2fj.artmed.2011.09.003&partnerID=40&md5=7166cd166009e059641d8749455a67a3","Objective: Predicting or prioritizing the human genes that cause disease, or "" disease genes"" , is one of the emerging tasks in biomedicine informatics. Research on network-based approach to this problem is carried out upon the key assumption of "" the network-neighbour of a disease gene is likely to cause the same or a similar disease"" , and mostly employs data regarding well-known disease genes, using supervised learning methods. This work aims to find an effective method to exploit the disease gene neighbourhood and the integration of several useful omics data sources, which potentially enhance disease gene predictions. Methods: We have presented a novel method to effectively predict disease genes by exploiting, in the semi-supervised learning (SSL) scheme, data regarding both disease genes and disease gene neighbours via protein-protein interaction network. Multiple proteomic and genomic data were integrated from six biological databases, including Universal Protein Resource, Interologous Interaction Database, Reactome, Gene Ontology, Pfam, and InterDom, and a gene expression dataset. Results: By employing a 10 times stratified 10-fold cross validation, the SSL method performs better than the k-nearest neighbour method and the support vector machines method in terms of sensitivity of 85%, specificity of 79%, precision of 81%, accuracy of 82%, and a balanced F-function of 83%. The other comparative experimental evaluations demonstrate advantages of the proposed method given a small amount of labeled data with accuracy of 78%. We have applied the proposed method to detect 572 putative disease genes, which are biologically validated by some indirect ways. Conclusion: Semi-supervised learning improved ability to study disease genes, especially a specific disease when the known disease genes (as labeled data) are very often limited. In addition to the computational improvement, the analysis of predicted disease proteins indicates that the findings are beneficial in deciphering the pathogenic mechanisms. © 2011 Elsevier B.V.","Disease gene neighbours; Disease-causing gene prediction; Multiple data resources integration; Protein-protein interaction network; Semi-supervised learning","Disease genes; Disease-causing gene; Multiple data; Protein-protein interaction networks; Semi-supervised learning; Active networks; Forecasting; Gene expression; Integration; Ontology; Proteins; Supervised learning; Pathology; actinin; Janus kinase 1; laminin; laminin alpha4; low density lipoprotein; myosin; protein tyrosine kinase; spectrin; accuracy; article; biomedicine; cell growth; cell maturation; gene expression; genetic analysis; genomics; human; human cell; learning; priority journal; protein analysis; protein protein interaction; proteomics; reliability; semi supervised learning; sensitivity and specificity; support vector machine; Algorithms; Artificial Intelligence; Databases, Nucleic Acid; Databases, Protein; Disease; Genetic Predisposition to Disease; Genomics; Humans; Probability Learning; Protein Interaction Mapping; Protein Interaction Maps; Proteomics; Sensitivity and Specificity",Article,Scopus,2-s2.0-83555177301
"Kaufmann E., Cappé O., Garivier A.","On Bayesian upper confidence bounds for bandit problems",2012,"Journal of Machine Learning Research",33,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954519509&partnerID=40&md5=2af9a7fd67397812e250db29f2d16289","Stochastic bandit problems have been analyzed from two different perspectives: a frequentist view, where the parameter is a deterministic unknown quantity, and a Bayesian approach, where the parameter is drawn from a prior distribution. We show in this paper that methods derived from this second perspective prove optimal when evaluated using the frequentist cumulated regret as a measure of performance. We give a general formulation for a class of Bayesian index policies that rely on quantiles of the posterior distribution. For binary bandits, we prove that the corresponding algorithm, termed Bayes- UCB, satisfies finite-time regret bounds that imply its asymptotic optimality. More generally, Bayes-UCB appears as an unifying framework for several variants of the UCB algorithm addressing different bandit problems (parametric multi-armed bandits, Gaussian bandits with unknown mean and variance, linear bandits). But the generality of the Bayesian approach makes it possible to address more challenging models. In particular, we show how to handle linear bandits with sparsity constraints by resorting to Gibbs sampling.",,"Artificial intelligence; Bayesian networks; Stochastic systems; Asymptotic optimality; Bayesian approaches; Measure of performance; Multi armed bandit; Posterior distributions; Prior distribution; Sparsity constraints; Upper confidence bound; Probability",Conference Paper,Scopus,2-s2.0-84954519509
"Raiko T., Valpola H., LeCun Y.","Deep learning made easier by linear transformations in perceptrons",2012,"Journal of Machine Learning Research",33,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893409634&partnerID=40&md5=941eb35e89139c9e87df27f5296a2dbb","We transform the outputs of each hidden neuron in a multi-layer perceptron network to have zero output and zero slope on average, and use separate shortcut connections to model the linear dependencies instead. This transformation aims at separating the problems of learning the linear and nonlinear parts of the whole input-output mapping, which has many benefits. We study the theoretical properties of the transformation by noting that they make the Fisher information matrix closer to a diagonal matrix, and thus standard gradient closer to the natural gradient. We experimentally confirm the usefulness of the transformations by noting that they make basic stochastic gradient learning competitive with state-of-the-art learning algorithms in speed, and that they seem also to help find solutions that generalize better. The experiments include both classification of small images and learning a lowdimensional representation for images by using a deep unsupervised auto-encoder network. The transformations were beneficial in all cases, with and without regularization and with networks from two to five hidden layers.",,"Artificial intelligence; Fisher information matrix; Learning algorithms; Mathematical transformations; Matrix algebra; Network layers; Stochastic systems; Diagonal matrices; Input-output mapping; Linear dependency; Low-dimensional representation; Multi layer perceptron networks; Natural gradient; Short-cut connection; Stochastic gradient; Linear transformations",Conference Paper,Scopus,2-s2.0-84893409634
"Fister I., Jr., Fister I., Brest J., Yang X.-S.","Memetic firefly algorithm for combinatorial optimization",2012,"Proceedings of the 5th International Conference on Bioinspired Optimization Methods and their Applications, BIOMA 2012",33,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976359390&partnerID=40&md5=0c78036d6e86ce25baaab583debfc58d","Firefly algorithms belong to modern meta-heuristic algorithms inspired by nature that can be successfully applied to continuous optimization problems. In this paper, we have been applied the firefly algorithm, hybridized with local search heuristic, to combinatorial optimization problems, where we use graph 3-coloring problems as test benchmarks. The results of the proposed memetic firefly algorithm (MFFA) were compared with the results of the Hybrid Evolutionary Algorithm (HEA), Tabucol, and the evolutionary algorithm with SAW method (EA-SAW) by coloring the suite of medium-scaled random graphs (graphs with 500 vertices) generated using the Culberson random graph generator. The results of firefly algorithm were very promising and showed a potential that this algorithm could successfully be applied in near future to the other combinatorial optimization problems as well.","Firefly algorithm; Graph 3-coloring; Nature inspired algorithms; NPcomplete problem; Swarm intelligence","Algorithms; Artificial intelligence; Bioluminescence; Combinatorial mathematics; Combinatorial optimization; Computational complexity; Evolutionary algorithms; Graph theory; Heuristic algorithms; Combinatorial optimization problems; Continuous optimization problems; Firefly algorithms; Graph 3 colorings; Hybrid evolutionary algorithm; Meta heuristic algorithm; Nature inspired algorithms; Swarm Intelligence; Optimization",Conference Paper,Scopus,2-s2.0-84976359390
"Cordasco G., De Chiara R., Mancuso A., Mazzeo D., Scarano V., Spagnuolo C.","A framework for distributing agent-based simulations",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",32,10.1007/978-3-642-29737-3-51,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878637675&doi=10.1007%2f978-3-642-29737-3-51&partnerID=40&md5=c05c8b11431bdc8488874e3b73c7b4ad","Agent-based simulation models are an increasingly popular tool for research and management in many, different and diverse fields. In executing such simulations the ""speed"" is one of the most general and important issues. The traditional answer to this issue is to invest resources in deploying a dedicated installation of dedicated computers. In this paper we present a framework that is a parallel version of the Mason, a library for writing and running Agent-based simulations. © 2012 Springer-Verlag Berlin Heidelberg.","Agent-based simulation; Distributed Systems; Heterogeneous Computing; Load-Balancing","Agent based simulation; Agent-based simulation models; Dedicated computers; Distributed systems; Diverse fields; Heterogeneous computing; Load-Balancing; Parallel version; Artificial intelligence; Computer science; Parallel architectures",Conference Paper,Scopus,2-s2.0-84878637675
"Anderson B., Storlie C., Lane T.","Improving malware classification: Bridging the static/dynamic gap",2012,"Proceedings of the ACM Conference on Computer and Communications Security",32,10.1145/2381896.2381900,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869822279&doi=10.1145%2f2381896.2381900&partnerID=40&md5=1b9824c1610c6a6e59b6b318b24e3e55","Malware classification systems have typically used some machine learning algorithm in conjunction with either static or dynamic features collected from the binary. Recently, more advanced malware has introduced mechanisms to avoid detection in these views by using obfuscation techniques to avoid static detection and execution-stalling techniques to avoid dynamic detection. In this paper we construct a classification framework that is able to incorporate both static and dynamic views into a unified framework in the hopes that, while a malicious executable can disguise itself in some views, disguising itself in every view while maintaining malicious intent will prove to be substantially more difficult. Our method uses kernels to place a similarity metric on each distinct view and then employs multiple kernel learning to find a weighted combination of the data sources which yields the best classification accuracy in a support vector machine classifier. Our approach opens up new avenues of malware research which will allow the research community to elegantly look at multiple facets of malware simultaneously, and which can easily be extended to integrate any new data sources that may become popular in the future.","Computer Security; Machine Learning; Malware; Multiple Kernel Learning","Classification accuracy; Classification framework; Classification system; Data-sources; Dynamic detection; Dynamic features; Malwares; Multiple Kernel Learning; Research communities; Similarity metrics; Static and dynamic; Unified framework; Artificial intelligence; Computer viruses; Learning systems; Security of data; Support vector machines; Learning algorithms",Conference Paper,Scopus,2-s2.0-84869822279
"Ammirati E., Cannistraci C.V., Cristell N.A., Vecchio V., Palini A.G., Tornvall P., Paganoni A.M., Miendlarzewska E.A., Sangalli L.M., Monello A., Pernow J., Björnstedt Bennermo M., Marenzi G., Hu D., Uren N.G., Cianflone D., Ravasi T., Manfredi A.A., Maseri A.","Identification and predictive value of interleukin-6+ interleukin-10+ and interleukin-6-interleukin-10+ cytokine patterns in st-elevation acute myocardial infarction",2012,"Circulation Research",32,10.1161/CIRCRESAHA.111.262477,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868711085&doi=10.1161%2fCIRCRESAHA.111.262477&partnerID=40&md5=230109e9399675e1115b9aa0431144e6","RATIONALE: At the onset of ST-elevation acute myocardial infarction (STEMI), patients can present with very high circulating interleukin-6 (IL-6) levels or very low-IL-6 levels. OBJECTIVE: We compared these 2 groups of patients to understand whether it is possible to define specific STEMI phenotypes associated with outcome based on the cytokine response. METHODS AND RESULTS: We compared 109 patients with STEMI in the top IL-6 level (median, 15.6 pg/mL; IL-6 STEMI) with 96 in the bottom IL-6 level (median, 1.7 pg/mL; IL-6 STEMI) and 103 matched controls extracted from the multiethnic First Acute Myocardial Infarction study. We found minimal clinical differences between IL-6 STEMI and IL-6 STEMI. We assessed the inflammatory profiles of the 2 STEMI groups and the controls by measuring 18 cytokines in blood samples. We exploited clustering analysis algorithms to infer the functional modules of interacting cytokines. IL-6 STEMI patients were characterized by the activation of 2 modules of interacting signals comprising IL-10, IL-8, macrophage inflammatory protein-1α, and C-reactive protein, and monocyte chemoattractant protein-1, macrophage inflammatory protein-1β, and monokine induced by interferon-γ. IL-10 was increased both in IL-6 STEMI and IL-6 STEMI patients compared with controls. IL-6IL-10 STEMI patients had an increased risk of systolic dysfunction at discharge and an increased risk of death at 6 months in comparison with IL-6IL-10 STEMI patients. We combined IL-10 and monokine induced by interferon-γ (derived from the 2 identified cytokine modules) with IL-6 in a formula yielding a risk index that outperformed any single cytokine in the prediction of systolic dysfunction and death. CONCLUSIONS: We have identified a characteristic circulating inflammatory cytokine pattern in STEMI patients, which is not related to the extent of myocardial damage. The simultaneous elevation of IL-6 and IL-10 levels distinguishes STEMI patients with worse clinical outcomes from other STEMI patients. These observations could have potential implications for risk-oriented patient stratification and immune-modulating therapies. © 2012 American Heart Association, Inc.","acute myocardial infarction; bioengineering; computational and systems biology; cytokines; inflammation; interleukin-10; interleukin-6","acetylsalicylic acid; angiotensin receptor antagonist; beta adrenergic receptor blocking agent; C reactive protein; calcium antagonist; clopidogrel; cytokine; dipeptidyl carboxypeptidase inhibitor; diuretic agent; fibrinogen receptor antagonist; gamma interferon; heparin; hydroxymethylglutaryl coenzyme A reductase inhibitor; interleukin 10; interleukin 6; interleukin 8; low molecular weight heparin; macrophage inflammatory protein 1alpha; macrophage inflammatory protein 1beta; monocyte chemotactic protein 1; monokine; nitrate; ticlopidine; adult; aged; algorithm; article; blood sampling; clinical assessment; cluster analysis; controlled study; cytokine production; diagnostic test accuracy study; diuretic therapy; ethnic difference; female; histopathology; hospital discharge; human; hypertension; inflammation; major clinical study; male; mortality; multicenter study; outcome assessment; percutaneous coronary intervention; phenotype; predictive value; priority journal; protein blood level; protein expression; protein function; risk factor; sensitivity and specificity; signal transduction; ST segment elevation myocardial infarction; systolic dysfunction; Aged; Algorithms; Artificial Intelligence; Cluster Analysis; Electrocardiography; Female; Humans; Interleukin-10; Interleukin-6; Male; Middle Aged; Myocardial Infarction; Predictive Value of Tests; Prognosis; Risk Factors; ROC Curve; Signal Transduction; Systole",Article,Scopus,2-s2.0-84868711085
"Zhu Z., Chen X., Ji F., Zhang L., Farahmand F., Jue J.P.","Energy-efficient translucent optical transport networks with mixed regenerator placement",2012,"Journal of Lightwave Technology",32,10.1109/JLT.2012.2213296,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867292803&doi=10.1109%2fJLT.2012.2213296&partnerID=40&md5=0ace342330a552df0990693561ba9337","Translucent networks utilize sparse placements of optical-electronic- optical (O/E/O) 3R (reamplification, reshaping, and retiming) regenerators to improve the cost effectiveness and energy efficiency of wavelength-routed optical transport networks. In this paper, we show that the energy cost of a translucent network can be further reduced by leveraging the energy efficiency of all-optical 2R (reamplification and reshaping) regenerators. We propose a translucent network infrastructure that uses all-optical 2R regenerators to partially replace O/E/O 3R regenerators and implements mixed regenerator placements (MRP). © 2012 IEEE.","Ant colony optimization (ACO); energy-efficient optical networks; genetic algorithm (GA); mixed regenerator placement (MRP); network planning and provisioning; translucent optical networks","Ant Colony Optimization (ACO); Energy efficient; Network planning and provisioning; Regenerator placement; Translucent optical networks; Algorithms; Artificial intelligence; Energy efficiency; Genetic algorithms; Regenerators; Signal systems; Fiber optic networks",Article,Scopus,2-s2.0-84867292803
"Weston J., Wang C., Weiss R., Berenzeig A.","Latent collaborative retrieval",2012,"Proceedings of the 29th International Conference on Machine Learning, ICML 2012",32,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867113541&partnerID=40&md5=fd1beaf60df2bf638c87673ad77e3f38","Retrieval tasks typically require a ranking of items given a query. Collaborative filtering tasks, on the other hand, learn to model user's preferences over items. In this paper we study the joint problem of recommending items to a user with respect to a given query, which is a surprisingly common task. This setup differs from the standard collaborative filtering one in that we are given a query x user x item tensor for training instead of the more traditional user x item matrix. Compared to document retrieval we do have a query, but we may or may not have content features (we will consider both cases) and we can also take account of the user's profile. We introduce a factorized model for this new task that optimizes the top-ranked items returned for the given query and user. We report empirical results where it outperforms several baselines. Copyright 2012 by the author(s)/owner(s).",,"Collaborative filtering; Document Retrieval; Artificial intelligence; Software engineering; Learning systems",Conference Paper,Scopus,2-s2.0-84867113541
"Sato J.R., Hoexter M.Q., Castellanos X.F., Rohde L.A.","Abnormal Brain Connectivity Patterns in Adults with ADHD: A Coherence Study",2012,"PLoS ONE",32,10.1371/journal.pone.0045671,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866841510&doi=10.1371%2fjournal.pone.0045671&partnerID=40&md5=98b9ffb3fc2f738244bb59db39f60885","Studies based on functional magnetic resonance imaging (fMRI) during the resting state have shown decreased functional connectivity between the dorsal anterior cingulate cortex (dACC) and regions of the Default Mode Network (DMN) in adult patients with Attention-Deficit/Hyperactivity Disorder (ADHD) relative to subjects with typical development (TD). Most studies used Pearson correlation coefficients among the BOLD signals from different brain regions to quantify functional connectivity. Since the Pearson correlation analysis only provides a limited description of functional connectivity, we investigated functional connectivity between the dACC and the posterior cingulate cortex (PCC) in three groups (adult patients with ADHD, n = 21; TD age-matched subjects, n = 21; young TD subjects, n = 21) using a more comprehensive analytical approach - unsupervised machine learning using a one-class support vector machine (OC-SVM) that quantifies an abnormality index for each individual. The median abnormality index for patients with ADHD was greater than for TD age-matched subjects (p = 0.014); the ADHD and young TD indices did not differ significantly (p = 0.480); the median abnormality index of young TD was greater than that of TD age-matched subjects (p = 0.016). Low frequencies below 0.05 Hz and around 0.20 Hz were the most relevant for discriminating between ADHD patients and TD age-matched controls and between the older and younger TD subjects. In addition, we validated our approach using the fMRI data of children publicly released by the ADHD-200 Competition, obtaining similar results. Our findings suggest that the abnormal coherence patterns observed in patients with ADHD in this study resemble the patterns observed in young typically developing subjects, which reinforces the hypothesis that ADHD is associated with brain maturation deficits. © 2012 Sato et al.",,"adult; anterior cingulate; article; attention deficit disorder; BOLD signal; brain dysfunction; brain region; clinical article; controlled study; executive function; female; functional magnetic resonance imaging; human; image analysis; image processing; machine learning; male; posterior cingulate; reinforcement; support vector machine; Adolescent; Adult; Age Factors; Artificial Intelligence; Attention Deficit Disorder with Hyperactivity; Brain; Brain Mapping; Child; Female; Gyrus Cinguli; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Male; Middle Aged; Models, Statistical; Neural Pathways; Pattern Recognition, Automated; Reproducibility of Results; Support Vector Machines; Young Adult",Article,Scopus,2-s2.0-84866841510
"Chen H.-L., Yang B., Wang G., Wang S.-J., Liu J., Liu D.-Y.","Support vector machine based diagnostic system for breast cancer using swarm intelligence",2012,"Journal of Medical Systems",32,10.1007/s10916-011-9723-0,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866075139&doi=10.1007%2fs10916-011-9723-0&partnerID=40&md5=b91a7311fda2e80b662726a5bb401897","Breast cancer is becoming a leading cause of death among women in the whole world, meanwhile, it is confirmed that the early detection and accurate diagnosis of this disease can ensure a long survival of the patients. In this paper, a swarm intelligence technique based support vector machine classifier (PSO-SVM) is proposed for breast cancer diagnosis. In the proposed PSO-SVM, the issue of model selection and feature selection in SVM is simultaneously solved under particle swarm (PSO optimization) framework. A weighted function is adopted to design the objective function of PSO, which takes into account the average accuracy rates of SVM (ACC), the number of support vectors (SVs) and the selected features simultaneously. Furthermore, time varying acceleration coefficients (TVAC) and inertia weight (TVIW) are employed to efficiently control the local and global search in PSO algorithm. The effectiveness of PSO-SVM has been rigorously evaluated against the Wisconsin Breast Cancer Dataset (WBCD), which is commonly used among researchers who use machine learning methods for breast cancer diagnosis. The proposed system is compared with the grid search method with feature selection by F-score. The experimental results demonstrate that the proposed approach not only obtains much more appropriate model parameters and discriminative feature subset, but also needs smaller set of SVs for training, giving high predictive accuracy. In addition, Compared to the existing methods in previous studies, the proposed system can also be regarded as a promising success with the excellent classification accuracy of 99.3% via 10-fold cross validation (CV) analysis. Moreover, a combination of five informative features is identified, which might provide important insights to the nature of the breast cancer disease and give an important clue for the physicians to take a closer attention. We believe the promising result can ensure that the physicians make very accurate diagnostic decision in clinical breast cancer diagnosis. © 2011 Springer Science+Business Media, LLC.","Breast cancer diagnosis; Feature selection; Particle swarm optimization; Support vector machines; Swarm intelligence","acceleration; algorithm; article; breast cancer; cancer classification; cancer diagnosis; clinical decision making; clinical effectiveness; decision support system; diagnostic accuracy; diagnostic value; medical information system; predictive value; process optimization; support vector machine; weight; artificial intelligence; breast tumor; classification; computer assisted diagnosis; female; human; Algorithms; Artificial Intelligence; Breast Neoplasms; Diagnosis, Computer-Assisted; Female; Humans; Support Vector Machines",Article,Scopus,2-s2.0-84866075139
"Guy C., Combemale B., Derrien S., Steel J.R.H., Jézéquel J.-M.","On model subtyping",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",32,10.1007/978-3-642-31491-9_30,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864046909&doi=10.1007%2f978-3-642-31491-9_30&partnerID=40&md5=4c0faa9db1ed5b971aaccd5dfacb9da0","Various approaches have recently been proposed to ease the manipulation of models for specific purposes (e.g., automatic model adaptation or reuse of model transformations). Such approaches raise the need for a unified theory that would ease their combination, but would also outline the scope of what can be expected in terms of engineering to put model manipulation into action. In this work, we address this problem from the model substitutability point of view, through model typing. We introduce four mechanisms to achieve model substitutability, each formally defined by a subtyping relation. We then discuss how to declare and check these subtyping relations. This work provides a formal reference specification establishing a family of model-oriented type systems. These type systems enable many facilities that are well known at the programming language level. Such facilities range from abstraction, reuse and safety to impact analyses and auto-completion. © 2012 Springer-Verlag.","Model Substitutability; Model Typing; Modeling Languages; SLE","Automatic models; Impact analysis; Model transformation; Modeling languages; SLE; Subtyping relation; Subtypings; Type systems; Unified theory; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84864046909
"Kokar M.M., Endsley M.R.","Situation awareness and cognitive modeling",2012,"IEEE Intelligent Systems",32,10.1109/MIS.2012.61,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863908313&doi=10.1109%2fMIS.2012.61&partnerID=40&md5=e956ad9dbc076a2dbd33e5b609b6d15b","This paper discusses the basic concepts of computer support for situationn awareness. The main idea is to share situations by computer and human agents. Two cases of situation awareness are discussed. Some of the future research topics are listed. © 2011 IEEE.","human-computer collaboration; ontological representation of situations; situation assessment; situation awareness; situation theory ontology","Human-computer collaboration; Ontological representation; Situation assessment; Situation awareness; Situation theory; Intelligent systems; Artificial intelligence",Article,Scopus,2-s2.0-84863908313
"Pei D.","Formalization of implication based fuzzy reasoning method",2012,"International Journal of Approximate Reasoning",32,10.1016/j.ijar.2012.01.007,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860216287&doi=10.1016%2fj.ijar.2012.01.007&partnerID=40&md5=a355c089eb659efde5920427a515e615","Fuzzy reasoning includes a number of important inference methods for addressing uncertainty. This line of fuzzy reasoning forms a common logical foundation in various fields, such as fuzzy logic control and artificial intelligence. The full implication triple I method (a method only based on implication, TI method for short) for fuzzy reasoning is proposed in 1999 to improve the popular CRI method (a hybrid method based on implication and composition). The current paper delves further into the TI method, and a sound logical foundation is set for the TI method based on the monoidal t-norm based logical system MTL. © 2012 Elsevier Inc. All rights reserved.","Fuzzy reasoning; Monoidal t-norm based logic; Triple I method","CRI method; Fuzzy logic control; Fuzzy reasoning; Hybrid method; Inference methods; Logical foundations; Logical system; Monoidal t-norm based logic; Triple I method; Artificial intelligence; Fuzzy inference",Article,Scopus,2-s2.0-84860216287
"Piltan M., Shiri H., Ghaderi S.F.","Energy demand forecasting in Iranian metal industry using linear and nonlinear models based on evolutionary algorithms",2012,"Energy Conversion and Management",32,10.1016/j.enconman.2011.12.022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856285262&doi=10.1016%2fj.enconman.2011.12.022&partnerID=40&md5=2404033277cf74639dce94a0cc1f9503","Developing energy-forecasting models is known as one of the most important steps in long-term planning. In order to achieve sustainable energy supply toward economic development and social welfare, it is required to apply precise forecasting model. Applying artificial intelligent models for estimation complex economic and social functions is growing up considerably in many researches recently. In this paper, energy consumption in industrial sector as one of the critical sectors in the consumption of energy has been investigated. Two linear and three nonlinear functions have been used in order to forecast and analyze energy in the Iranian metal industry, Particle Swarm Optimization (PSO) and Genetic Algorithms (GAs) are applied to attain parameters of the models. The Real-Coded Genetic Algorithm (RCGA) has been developed based on real numbers, which is introduced as a new approach in the field of energy forecasting. In the proposed model, electricity consumption has been considered as a function of different variables such as electricity tariff, manufacturing value added, prevailing fuel prices, the number of employees, the investment in equipment and consumption in the previous years. Mean Square Error (MSE), Root Mean Square Error (RMSE), Mean Absolute Deviation (MAD) and Mean Absolute Percent Error (MAPE) are the four functions which have been used as the fitness function in the evolutionary algorithms. The results show that the logarithmic nonlinear model using PSO algorithm with 1.91 error percentage has the best answer. Furthermore, the prediction of electricity consumption in industrial sector of Turkey and also Turkish industrial sector is reinvestigated, the results indicate significant improvement. © 2012 Elsevier Ltd. All rights reserved.","Energy demand; Evolutionary algorithm; Forecasting; Industrial sector","Artificial intelligent; Consumption of energy; Economic development; Electricity tariff; Electricity-consumption; Energy demand forecasting; Energy demands; Energy forecasting; Error percentage; Fitness functions; Forecasting models; Fuel prices; Industrial sector; Long term planning; Mean absolute deviations; Metal industries; Non-linear model; Nonlinear functions; Previous year; PSO algorithms; Real number; Real-coded genetic algorithm; Root mean square errors; Social function; Social welfare; Sustainable energy supply; Turkishs; Artificial intelligence; Economics; Energy utilization; Evolutionary algorithms; Forecasting; Industry; Investments; Metallurgy; Nonlinear systems; Mean square error",Article,Scopus,2-s2.0-84856285262
"Majumdar A., Maiti D.K., Maity D.","Damage assessment of truss structures from changes in natural frequencies using ant colony optimization",2012,"Applied Mathematics and Computation",32,10.1016/j.amc.2012.03.031,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860485145&doi=10.1016%2fj.amc.2012.03.031&partnerID=40&md5=0c9ca86015b96cb8d044709b0486d22e","A method is presented to detect and assess structural damages from changes in natural frequencies using ant colony optimization (ACO) algorithm. It is possible to formulate the inverse problem in terms of optimization and then to utilize a solution technique employing ACO to assess the damages of structures using natural frequencies. The study indicates the potentiality of the developed code to solve a wide range of inverse identification problems in a systematic way. The developed code is used to assess damages of truss like structures using first few natural frequencies. The outcomes of the results show that the developed method can detect and estimate the amount of damages with satisfactory precision. © 2012 Elsevier Inc. All rights reserved.","Ant colony optimization; Damage assessment; Finite element method; Inverse problem; Natural frequency; Stiffness reduction factor","Ant Colony Optimization (ACO); Ant Colony Optimization algorithms; Damage assessments; Inverse identification; Solution techniques; Stiffness reduction; Structural damages; Truss structure; Artificial intelligence; Damage detection; Finite element method; Inverse problems; Natural frequencies; Algorithms",Article,Scopus,2-s2.0-84860485145
"Trietsch D., Baker K.R.","PERT 21: Fitting PERT/CPM for use in the 21st century",2012,"International Journal of Project Management",32,10.1016/j.ijproman.2011.09.004,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859217753&doi=10.1016%2fj.ijproman.2011.09.004&partnerID=40&md5=aaca8f90a5f21193e0fba02ee45740e8","More than half a century after the debut of CPM and PERT, we still lack a project scheduling system with calibrated and validated distributions and without requiring complex user input. Modern decision support systems (DSS) for project management are more sophisticated and comprehensive than PERT/CPM. Nonetheless, in terms of stochastic analysis, they show insufficient progress. PERT 21 offers a radically different stochastic analysis for projects, based on relevant and validated theory. Operationally, it is sophisticated yet simple to use. It is designed to enhance existing DSS, and thus it can be implemented without sacrificing the investment already made in project management systems. Finally, regarding the important sequencing and crashing models developed under CPM, PERT 21 permits their adaptation to stochastic reality. © 2011 APM and IPMA and Elsevier Ltd.","Critical Chain; PERT/CPM; Planning and control; Project management decision support systems (DSS); Stochastic project scheduling","Critical chain; PERT/CPM; Planning and control; Project management decision support systems (DSS); Project scheduling; Artificial intelligence; Critical path analysis; Decision support systems; Scheduling; Stochastic models; Stochastic systems; PERT; decision support system; project management; stochasticity; twenty first century",Article,Scopus,2-s2.0-84859217753
"Das S., Kale A., Vaswani N.","Particle filter with a mode tracker for visual tracking across illumination changes",2012,"IEEE Transactions on Image Processing",32,10.1109/TIP.2011.2174370,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859033272&doi=10.1109%2fTIP.2011.2174370&partnerID=40&md5=36b323a0cfd6900be05d4c15867c277d","In this correspondence, our goal is to develop a visual tracking algorithm that is able to track moving objects in the presence of illumination variations in the scene and that is robust to occlusions. We treat the illumination and motion (x-y translation and scale) parameters as the unknown state sequence. The observation is the entire image, and the observation model allows for occasional occlusions (modeled as outliers). The nonlinearity and multimodality of the observation model necessitate the use of a particle filter (PF). Due to the inclusion of illumination parameters, the state dimension increases, thus making regular PFs impractically expensive. We show that the recently proposed approach using a PF with a mode tracker can be used here since, even in most occlusion cases, the posterior of illumination conditioned on motion and the previous state is unimodal and quite narrow. The key idea is to importance sample on the motion states while approximating importance sampling by posterior mode tracking for estimating illumination. Experiments demonstrate the advantage of the proposed algorithm over existing PF-based approaches for various face and vehicle tracking. We are also able to detect illumination model changes, e.g., those due to transition from shadow to sunlight or vice versa by using the generalized expected log-likelihood statistics and successfully compensate for it without ever loosing track. © 2011 IEEE.","Monte Carlo methods; particle filter (PF); tracking; visual tracking","Illumination changes; Illumination models; Illumination variation; Importance sample; Log-likelihood statistics; Mode tracking; Motion state; Moving objects; Multi-modality; Non-Linearity; Observation model; Particle filter; particle filter (PF); Unimodal; Unknown state; Visual Tracking; Visual tracking algorithm; Algorithms; Monte Carlo methods; Nonlinear filtering; Surface discharges; Tracking (position); algorithm; artificial intelligence; automated pattern recognition; computer assisted diagnosis; illumination; image enhancement; letter; methodology; motion; reproducibility; sensitivity and specificity; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Lighting; Motion; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84859033272
"Huang M.-L., Hung Y.-H., Lee W.-M., Li R.K., Wang T.-H.","Usage of case-based reasoning, neural network and adaptive neuro-fuzzy inference system classification techniques in breast cancer dataset classification diagnosis",2012,"Journal of Medical Systems",32,10.1007/s10916-010-9485-0,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863192451&doi=10.1007%2fs10916-010-9485-0&partnerID=40&md5=30a5ee56558915b69e856f8ee085811d","Breast cancer is a common to females worldwide. Today, technological advancements in cancer treatment innovations have increased the survival rates. Many theoretical and experimental studies have shown that a multiple classifier system is an effective technique for reducing prediction errors. This study compared the particle swarm optimizer (PSO) based artificial neural network (ANN), the adaptive neuro-fuzzy inference system (ANFIS), and a case-based reasoning (CBR) classifier with a logistic regression model and decision tree model. It also applied three classification techniques to the Mammographic Mass Data Set, and measured its improvements in accuracy and classification errors. The experimental results showed that, the best CBR-based classification accuracy is 83.60%, and the classification accuracies of the PSO-based ANN classifier and ANFIS are 91.10% and 92.80%, respectively. © Springer Science+Business Media, LLC 2010.","ANFIS; Breast cancer; Case-based reasoning; Particle swarm optimizer","adaptive neuro fuzzy inference system; article; artificial intelligence; artificial neural network; breast cancer; cancer diagnosis; case based reasoning; classification; decision tree; diagnostic accuracy; fuzzy system; genetic algorithm; human; intermethod comparison; logistic regression analysis; machine learning; mammography; particle swarm optimizer; support vector machine; adolescent; adult; age; aged; algorithm; breast tumor; computer assisted diagnosis; female; fuzzy logic; methodology; middle aged; Adolescent; Adult; Age Factors; Aged; Aged, 80 and over; Algorithms; Breast Neoplasms; Decision Trees; Diagnosis, Computer-Assisted; Female; Fuzzy Logic; Humans; Middle Aged; Neural Networks (Computer); Young Adult",Article,Scopus,2-s2.0-84863192451
"Huang X., Boulgouris N.V.","Gait recognition with shifted energy image and structural feature extraction",2012,"IEEE Transactions on Image Processing",32,10.1109/TIP.2011.2180914,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859054605&doi=10.1109%2fTIP.2011.2180914&partnerID=40&md5=635ce976cb9dc37016a1566e23d9d4be","In this paper, we present a novel and efficient gait recognition system. The proposed system uses two novel gait representations, i.e., the shifted energy image and the gait structural profile, which have increased robustness to some classes of structural variations. Furthermore, we introduce a novel method for the simulation of walking conditions and the generation of artificial subjects that are used for the application of linear discriminant analysis. In the decision stage, the two representations are fused. Thorough experimental evaluation, conducted using one traditional and two new databases, demonstrates the advantages of the proposed system in comparison with current state-of-the-art systems. © 2011 IEEE.","Biometrics; gait recognition; surveillance","Experimental evaluation; Gait recognition; Linear discriminant analysis; State-of-the-art system; Structural feature; Structural profiles; Structural variations; System use; Biometrics; Gait analysis; Space surveillance; Feature extraction; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; gait; human; image enhancement; image subtraction; methodology; physiology; reproducibility; sensitivity and specificity; whole body imaging; Algorithms; Artificial Intelligence; Gait; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique; Whole Body Imaging",Article,Scopus,2-s2.0-84859054605
"García-Nieto J., Alba E., Carolina Olivera A.","Swarm intelligence for traffic light scheduling: Application to real urban areas",2012,"Engineering Applications of Artificial Intelligence",32,10.1016/j.engappai.2011.04.011,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855806275&doi=10.1016%2fj.engappai.2011.04.011&partnerID=40&md5=35c240f22758a50fd251f378a17fddd5","Congestion, pollution, security, parking, noise, and many other problems derived from vehicular traffic are present every day in most cities around the world. The growing number of traffic lights that control the vehicular flow requires a complex scheduling, and hence, automatic systems are indispensable nowadays for optimally tackling this task. In this work, we propose a Swarm Intelligence approach to find successful cycle programs of traffic lights. Using a microscopic traffic simulator, the solutions obtained by our algorithm are evaluated in the context of two large and heterogeneous metropolitan areas located in the cities of Málaga and Sevilla (in Spain). In comparison with cycle programs predefined by experts (close to real ones), our proposal obtains significant profits in terms of two main indicators: the number of vehicles that reach their destinations on time and the global trip time. © 2011 Elsevier Ltd. All rights reserved.","Cycle program optimization; Particle swarm optimization; Realistic traffic instances; SUMO microscopic simulator of urban mobility; Traffic light scheduling","Particle swarm; Program optimization; Realistic traffic instances; SUMO microscopic simulator of urban mobility; Traffic light scheduling; Artificial intelligence; Cellular automata; Optimization; Profitability; Vehicle actuated signals; Traffic congestion",Article,Scopus,2-s2.0-84855806275
"Columbus C.C., Chandrasekaran K., Simon S.P.","Nodal ant colony optimization for solving profit based unit commitment problem for GENCOs",2012,"Applied Soft Computing Journal",32,10.1016/j.asoc.2011.08.057,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-81155132319&doi=10.1016%2fj.asoc.2011.08.057&partnerID=40&md5=be17f5399cd19771708803e209916aa7","This paper proposes a nodal ant colony optimization (NACO) technique to solve profit based unit commitment problem (PBUCP). Generation companies (GENCOs) in a competitive restructured power market, schedule their generators with an objective to maximize their own profit without any regard for system social benefit. Power and reserve prices become important factors in decision process. Ant colony optimization that mimics the behavior of ants foraging activities is suitably implemented to search the UCP search space. Here a search space consisting of optimal combination of binary nodes for unit ON/OFF status is represented for the movement of the ants to maintain good exploration and exploitation search capabilities. The proposed model help GENCOs to make decisions on the quantity of power and reserve that must be put up for sale in the markets and also to schedule generators in order to receive the maximum profit. The effectiveness of the proposed technique for PBUCP is validated on 10 and 36 generating unit systems available in the literature. NACO yields an increase of profit, greater than 1.5%, in comparison with the basic ACO, Muller method and hybrid LR-GA. © 2011 Elsevier B.V. All rights reserved.","Ant colony optimization and nodal ant colony optimization; Deregulated market; Profit based unit commitment problem","Ant-colony optimization; Decision process; Deregulated markets; Exploration and exploitation; Generating unit; Generation companies; Muller method; Optimal combination; Profit based unit commitment; Reserve price; Restructured power markets; Search capabilities; Search spaces; Social benefits; Artificial intelligence; Commerce; Deregulation; Optimization; Profitability; Sodium compounds; Algorithms",Article,Scopus,2-s2.0-81155132319
"Häfner M., Liedlgruber M., Uhl A., Vécsei A., Wrba F.","Color treatment in endoscopic image classification using multi-scale local color vector patterns",2012,"Medical Image Analysis",32,10.1016/j.media.2011.05.006,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-82355161022&doi=10.1016%2fj.media.2011.05.006&partnerID=40&md5=2c2d2de512dd7f50f58341eee32b8232","In this work we propose a novel method to describe local texture properties within color images with the aim of automated classification of endoscopic images. In contrast to comparable Local Binary Patterns operator approaches, where the respective texture operator is almost always applied to each color channel separately, we construct a color vector field from an image. Based on this field the proposed operator computes the similarity between neighboring pixels. The resulting image descriptor is a compact 1D-histogram which we use for a classification using the k-nearest neighbors classifier.To show the usability of this operator we use it to classify magnification-endoscopic images according to the pit pattern classification scheme. Apart from that, we also show that compared to previously proposed operators we are not only able to get competitive classification results in our application scenario, but that the proposed operator is also able to outperform the other methods either in terms of speed, feature compactness, or both. © 2011 Elsevier B.V..","Classification; Colon cancer; Color; Local binary patterns; Multi-scale","Application scenario; Automated classification; Classification results; Colon cancer; Color channels; Color images; Color vector; Descriptors; Endoscopic image; K-nearest neighbors; Local binary patterns; Local color; Local Texture; Multiscales; Pit patterns; Texture operators; Classification (of information); Content based retrieval; Endoscopy; Textures; Color; article; colon polyp; colonoscopy; color vision; histogram; human; image analysis; k nearest neighbor; magnifying endoscopy; priority journal; Algorithms; Artificial Intelligence; Colonic Polyps; Color; Colorimetry; Endoscopy, Gastrointestinal; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-82355161022
"Li H., Wang M., Zhou X., Zhao J.","An interval set model for learning rules from incomplete information table",2012,"International Journal of Approximate Reasoning",32,10.1016/j.ijar.2011.09.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-81155139533&doi=10.1016%2fj.ijar.2011.09.002&partnerID=40&md5=01c7ee609303e82aeb47ed8693ce8456","A novel interval set approach is proposed in this paper to induce classification rules from incomplete information table, in which an interval-set-based model to represent the uncertain concepts is presented. The extensions of the concepts in incomplete information table are represented by interval sets, which regulate the upper and lower bounds of the uncertain concepts. Interval set operations are discussed, and the connectives of concepts are represented by the operations on interval sets. Certain inclusion, possible inclusion, and weak inclusion relations between interval sets are presented, which are introduced to induce strong rules and weak rules from incomplete information table. The related properties of the inclusion relations are proved. It is concluded that the strong rules are always true whatever the missing values may be, while the weak rules may be true when missing values are replaced by some certain known values. Moreover, a confidence function is defined to evaluate the weak rule. The proposed approach presents a new view on rule induction from incomplete data based on interval set. © 2011 Elsevier Inc. All rights reserved.","Incomplete information table; Interval extension; Interval set; Rule induction","Classification rules; Inclusion relation; Incomplete data; Incomplete information; Interval extension; Interval set; Interval sets; Learning rules; Missing values; Rule induction; Set operation; Upper and lower bounds; Weak inclusion; Artificial intelligence; Software engineering",Article,Scopus,2-s2.0-81155139533
"Greco S., Słowiński R., Szczech I.","Properties of rule interestingness measures and alternative approaches to normalization of measures",2012,"Information Sciences",31,10.1016/j.ins.2012.05.018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864723140&doi=10.1016%2fj.ins.2012.05.018&partnerID=40&md5=156995b27db6d51af1535ae566ef92fc","We are considering properties of interestingness measures of rules induced from data. These are: Bayesian confirmation property, two properties related to the case of entailment or refutation, called (Ex 1) and logicality L, and a group of symmetry properties. We propose a modification of properties (Ex 1) and L, called weak (Ex 1), and weak L, that deploy the concept of confirmation in its larger sense. We demonstrate that properties (Ex 1) and L do not fully reflect such understanding of the confirmation concept, and thus, we propose to substitute (Ex 1) by weak (Ex 1) and L by weak L. Moreover, we introduce four new approaches to normalization of confirmation measures in order to transform measures so that they would obtain desired properties. The analysis of the results of the normalizations of the confirmation measures takes into account all considered properties. We advocate for two normalized confirmation measures: measure Z considered in the literature, and newly proposed measure A. Finally, we provide some ideas for combining them in a single measure keeping all desirable properties. © 2012 Elsevier Inc. All rights reserved.","Confirmation; Normalization; Properties of measures; Rule interestingness measures","Alternative approach; Bayesian confirmation; Confirmation; Interestingness measures; Normalization; Properties of measures; Rule interestingness; Symmetry properties; Artificial intelligence; Software engineering; Association rules",Article,Scopus,2-s2.0-84864723140
"Morandat F., Hill B., Osvald L., Vitek J.","Evaluating the design of the r language objects and functions for data analysis",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",31,10.1007/978-3-642-31057-7-6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879698011&doi=10.1007%2f978-3-642-31057-7-6&partnerID=40&md5=e1a9f9b9b436af6e4c64a54b7f373fd2","R is a dynamic language for statistical computing that combines lazy functional features and object-oriented programming. This rather unlikely linguistic cocktail would probably never have been prepared by computer scientists, yet the language has become surprisingly popular. With millions of lines of R code available in repositories, we have an opportunity to evaluate the fundamental choices underlying the R language design. Using a combination of static and dynamic program analysis we assess the success of different language features. © 2012 Springer-Verlag Berlin Heidelberg.",,"Computer scientists; Dynamic languages; Dynamic program analysis; Functional features; Language features; R languages; Statistical computing; Artificial intelligence; Computer science; Object oriented programming",Conference Paper,Scopus,2-s2.0-84879698011
"Risi S., Lehman J., D'ambrosio D.B., Hall R., Stanley K.O.","Combining search-based procedural content generation and social gaming in the petalz video game",2012,"Proceedings of the 8th AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, AIIDE 2012",31,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881574567&partnerID=40&md5=8743e1b0fbd4414427371375d28b2b33","Search-based procedural content generation methods allow video games to introduce new content continually, thereby engaging the player for a longer time while reducing the burden on developers. However, games so far have not explored the potential economic value of unique evolved artifacts. Building on this insight, this paper presents for the first time a Facebook game called Petalz in which players can share flowers they breed themselves with other players through a global marketplace. In particular, the market in this social game allows players to set the price of their evolved aestheticallypleasing flowers in virtual currency. Furthermore, the transaction in which one player buys seeds from another creates a new social element that links the players in the transaction. The combination of unique user-generated content and social gaming in Petalz facilitates meaningful collaboration between users, positively influences the dynamics of the game, and opens new possibilities in digital entertainment. Copyright © 2012, Association for the Advancement of Artificial Intelligence.",,"Digital entertainment; Economic values; Global marketplaces; Procedural content generations; Social elements; Social gaming; User-generated content; Virtual currency; Artificial intelligence; Commerce; Copyrights; Economics; Interactive computer graphics; Human computer interaction",Conference Paper,Scopus,2-s2.0-84881574567
"Rojas-Martínez M., Mañanas M.A., Alonso J.F.","High-density surface EMG maps from upper-arm and forearm muscles",2012,"Journal of NeuroEngineering and Rehabilitation",31,10.1186/1743-0003-9-85,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874063291&doi=10.1186%2f1743-0003-9-85&partnerID=40&md5=5d508ea5ce87d3860578fa803a82aa48","Background: sEMG signal has been widely used in different applications in kinesiology and rehabilitation as well as in the control of human-machine interfaces. In general, the signals are recorded with bipolar electrodes located in different muscles. However, such configuration may disregard some aspects of the spatial distribution of the potentials like location of innervation zones and the manifestation of inhomogineties in the control of the muscular fibers. On the other hand, the spatial distribution of motor unit action potentials has recently been assessed with activation maps obtained from High Density EMG signals (HD-EMG), these lasts recorded with arrays of closely spaced electrodes. The main objective of this work is to analyze patterns in the activation maps, associating them with four movement directions at the elbow joint and with different strengths of those tasks. Although the activation pattern can be assessed with bipolar electrodes, HD-EMG maps could enable the extraction of features that depend on the spatial distribution of the potentials and on the load-sharing between muscles, in order to have a better differentiation between tasks and effort levels. Methods. An experimental protocol consisting of isometric contractions at three levels of effort during flexion, extension, supination and pronation at the elbow joint was designed and HD-EMG signals were recorded with 2D electrode arrays on different upper-limb muscles. Techniques for the identification and interpolation of artifacts are explained, as well as a method for the segmentation of the activation areas. In addition, variables related to the intensity and spatial distribution of the maps were obtained, as well as variables associated to signal power of traditional single bipolar recordings. Finally, statistical tests were applied in order to assess differences between information extracted from single bipolar signals or from HD-EMG maps and to analyze differences due to type of task and effort level. Results: Significant differences were observed between EMG signal power obtained from single bipolar configuration and HD-EMG and better results regarding the identification of tasks and effort levels were obtained with the latter. Additionally, average maps for a population of 12 subjects were obtained and differences in the co-activation pattern of muscles were found not only from variables related to the intensity of the maps but also to their spatial distribution. Conclusions: Intensity and spatial distribution of HD-EMG maps could be useful in applications where the identification of movement intention and its strength is needed, for example in robotic-aided therapies or for devices like powered- prostheses or orthoses. Finally, additional data transformations or other features are necessary in order to improve the performance of tasks identification. © 2012 Rojas-Martínez et al.; licensee BioMed Central Ltd.","2D electrode arrays; Artifact detection; EMG pattern recognition; High-Density surface electromyography; Prosthetics; Rehabilitation; Robotics","adult; algorithm; arm; article; artifact; artificial intelligence; elbow; electrode; electromyography; forearm; functions of the skin and its appendages; histology; human; impedance; limb prosthesis; male; movement (physiology); muscle contraction; physiology; reproducibility; robotics; skeletal muscle; statistical analysis; Adult; Algorithms; Arm; Artifacts; Artificial Intelligence; Artificial Limbs; Data Interpretation, Statistical; Elbow Joint; Electric Impedance; Electrodes; Electromyography; Forearm; Humans; Male; Movement; Muscle Contraction; Muscle, Skeletal; Reproducibility of Results; Robotics; Skin Physiological Phenomena; Young Adult",Article,Scopus,2-s2.0-84874063291
"Yin J., Wang Y., Hu J.","A new dimensionality reduction algorithm for hyperspectral image using evolutionary strategy",2012,"IEEE Transactions on Industrial Informatics",31,10.1109/TII.2012.2205397,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867975740&doi=10.1109%2fTII.2012.2205397&partnerID=40&md5=61e7e3b4f2c7f569a2e1c1a814972178","Reducing the redundancy of spectral information is an important technique in classification of hyperspectral image. The existing methods are classified into two categories: feature extraction and band selection. Compared with the feature extraction, the band selection method preserves most of the characteristics of the original data without losing valuable details. However, the choice of the effective band remains challenging, especially when considering the computational burden, which makes many enumerative methods infeasible. Recently, immune clonal strategy (ICS) has been applied to solve complex computation problems. The major advantages of algorithms based on ICS are that they are highly paralleled, distributed, adaptive, and self-organizing. Therefore, in this paper, we convert the band selection problem into an optimization issue and propose a new algorithm, ICS-based effective band selection (ICS-EBS), to select effective band combinations. Then, the selected bands are used in classification of hyperspectral image. We evaluated the proposed algorithm by using two data sets collected from the Washington DC Mall and Northwest Tippecanoe County. ICS-EBS was compared against one latest proposed band selection algorithm, interclass separability index Algorithm (ICSIA). We also compared the results with those achieved by other stochastic algorithms such as genetic algorithm (GA) and ant colony optimization (ACO). The experimental results indicate that our proposed algorithm outperforms ICSIA, GA-EBS, and ACO-EBS for hyperspectral image classification. © 2012 IEEE.","Band selection; classification; immune clonal strategy; optimization","Ant Colony Optimization (ACO); Band selection; Complex computation; Computational burden; Data sets; Dimensionality reduction algorithms; Enumerative method; Evolutionary strategies; Hyper-spectral images; Hyperspectral image classification; Immune clonal strategies; Self organizing; Separability index; Spectral information; Stochastic algorithms; Washington; Artificial intelligence; Classification (of information); Feature extraction; Optimization; Spectroscopy; Genetic algorithms",Article,Scopus,2-s2.0-84867975740
"Hadwiger M., Beyer J., Jeong W.-K., Pfister H.","Interactive volume exploration of petascale microscopy data streams using a visualization-driven virtual memory approach",2012,"IEEE Transactions on Visualization and Computer Graphics",31,10.1109/TVCG.2012.240,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867678132&doi=10.1109%2fTVCG.2012.240&partnerID=40&md5=8bbc3606dada18c4667a4183a3de4191","This paper presents the first volume visualization system that scales to petascale volumes imaged as a continuous stream of high-resolution electron microscopy images. Our architecture scales to dense, anisotropic petascale volumes because it: (1) decouples construction of the 3D multi-resolution representation required for visualization from data acquisition, and (2) decouples sample access time during ray-casting from the size of the multi-resolution hierarchy. Our system is designed around a scalable multi-resolution virtual memory architecture that handles missing data naturally, does not pre-compute any 3D multi-resolution representation such as an octree, and can accept a constant stream of 2D image tiles from the microscopes. A novelty of our system design is that it is visualization-driven: we restrict most computations to the visible volume data. Leveraging the virtual memory architecture, missing data are detected during volume ray-casting as cache misses, which are propagated backwards for on-demand out-of-core processing. 3D blocks of volume data are only constructed from 2D microscope image tiles when they have actually been accessed during ray-casting. We extensively evaluate our system design choices with respect to scalability and performance, compare to previous best-of-breed systems, and illustrate the effectiveness of our system for real microscopy data from neuroscience. © 1995-2012 IEEE.","high-resolution microscopy; high-throughput imaging; neuroscience; Petascale volume exploration","2D images; Access time; Cache Miss; Data stream; High-resolution microscopy; High-throughput; Microscope images; Missing data; Multi resolution representation; Multi-resolutions; neuroscience; Out-of-core processing; Petascale; Raycasting; Virtual memory; Volume data; Volume visualization; Data mining; Data visualization; Digital storage; Memory architecture; Systems analysis; Three dimensional; Visualization; Three dimensional computer graphics; algorithm; animal; artificial intelligence; brain cortex; electron microscopy; factual database; hippocampus; image processing; mouse; procedures; theoretical model; ultrastructure; Algorithms; Animals; Artificial Intelligence; Cerebral Cortex; Databases, Factual; Hippocampus; Image Processing, Computer-Assisted; Mice; Microscopy, Electron; Models, Theoretical",Article,Scopus,2-s2.0-84867678132
"Schölkopf B., Janzing D., Peters J., Sgouritsa E., Zhang K., Mooij J.","On causal and anticausal learning",2012,"Proceedings of the 29th International Conference on Machine Learning, ICML 2012",31,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867113617&partnerID=40&md5=47fb8ab9f5958e2c4d3cf8cec386c040","We consider the problem of function estimation in the case where an underlying causal model can be inferred. This has implications for popular scenarios such as covariate shift, concept drift, transfer learning and semi-supervised learning. We argue that causal knowledge may facilitate some approaches for a given problem, and rule out others. In particular, we formulate a hypothesis for when semi-supervised learning can help, and corroborate it with empirical results. Copyright 2012 by the author(s)/owner(s).",,"Causal model; Concept drifts; Covariate shifts; Function estimation; Semi-supervised learning; Transfer learning; Artificial intelligence; Software engineering; Supervised learning",Conference Paper,Scopus,2-s2.0-84867113617
"Cecchi G.A., Huang L., Hashmi J.A., Baliki M., Centeno M.V., Rish I., Apkarian A.V.","Predictive Dynamics of Human Pain Perception",2012,"PLoS Computational Biology",31,10.1371/journal.pcbi.1002719,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868133578&doi=10.1371%2fjournal.pcbi.1002719&partnerID=40&md5=edaa7625120fc608f9b39170d6697a58","While the static magnitude of thermal pain perception has been shown to follow a power-law function of the temperature, its dynamical features have been largely overlooked. Due to the slow temporal experience of pain, multiple studies now show that the time evolution of its magnitude can be captured with continuous online ratings. Here we use such ratings to model quantitatively the temporal dynamics of thermal pain perception. We show that a differential equation captures the details of the temporal evolution in pain ratings in individual subjects for different stimulus pattern complexities, and also demonstrates strong predictive power to infer pain ratings, including readouts based only on brain functional images. © 2012 Cecchi et al.",,"accuracy; adult; article; BOLD signal; controlled study; female; functional magnetic resonance imaging; functional neuroimaging; human; human experiment; male; mathematical model; neurophysiology; nociception; normal human; pain; pain assessment; pain threshold; prediction; psychophysics; stimulus response; temperature dependence; temperature sensitivity; thermal pain; thermal stimulation; Adult; Artificial Intelligence; Brain; Female; Hot Temperature; Humans; Magnetic Resonance Imaging; Male; Models, Neurological; Pain; Pain Perception; Psychophysics; Random Allocation; Regression Analysis",Article,Scopus,2-s2.0-84868133578
"Vahdani B., Iranmanesh S.H., Mousavi S.M., Abdollahzade M.","A locally linear neuro-fuzzy model for supplier selection in cosmetics industry",2012,"Applied Mathematical Modelling",31,10.1016/j.apm.2011.12.006,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861841510&doi=10.1016%2fj.apm.2011.12.006&partnerID=40&md5=a9c995be80091464c09fa7b92c3357dd","Supplier selection and evaluation is a complicated and disputed issue in supply chain network management, by virtue of the variety of intellectual property of the suppliers, the several variables involved in supply demand relationship, the complex interactions and the inadequate information of suppliers. The recent literature confirms that neural networks achieve better performance than conventional methods in this area. Hence, in this paper, an effective artificial intelligence (AI) approach is presented to improve the decision making for a supply chain which is successfully utilized for long-term prediction of the performance data in cosmetics industry. A computationally efficient model known as locally linear neuro-fuzzy (LLNF) is introduced to predict the performance rating of suppliers. The proposed model is trained by a locally linear model tree (LOLIMOT) learning algorithm. To demonstrate the performance of the proposed model, three intelligent techniques, multi-layer perceptron (MLP) neural network, radial basis function (RBF) neural network and least square-support vector machine (LS-SVM) are considered. Their results are compared by using an available dataset in cosmetics industry. The computational results show that the presented model performs better than three foregoing techniques. © 2011 Elsevier Inc.","Artificial intelligence (AI); Locally linear neuro-fuzzy model; Neural networks; Supplier selection problem; Supply chain management","Complex interaction; Computational results; Computationally efficient; Conventional methods; Data sets; Intelligent techniques; Locally linear model trees; Long-term prediction; Multi layer perceptron; Neuro-Fuzzy; Neuro-Fuzzy model; Performance data; Performance ratings; Radial basis function neural networks; Several variables; Supplier selection; Supply chain network; Supply-demand relationships; Vector machines; Learning algorithms; Network management; Radial basis function networks; Supply chain management; Support vector machines; Neural networks",Article,Scopus,2-s2.0-84861841510
"Perry A.M., Cardesa-Salzmann T.M., Meyer P.N., Colomo L., Smith L.M., Fu K., Greiner T.C., Delabie J., Gascoyne R.D., Rimsza L., Jaffe E.S., Ott G., Rosenwald A., Braziel R.M., Tubbs R., Cook J.R., Staudt L.M., Connors J.M., Sehn L.H., Vose J.M., Loṕez-Guillermo A., Campo E., Chan W.C., Weisenburger D.D.","A new biologic prognostic model based on immunohistochemistry predicts survival in patients with diffuse large B-cell lymphoma",2012,"Blood",31,10.1182/blood-2012-05-430389,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866318188&doi=10.1182%2fblood-2012-05-430389&partnerID=40&md5=6d947f6a8be75fe4a5dfe5ca18d72eac","Biologic factors that predict the survival of patients with a diffuse large B-cell lymphoma, such as cell of origin and stromal signatures, have been discovered by gene expression profiling. We attempted to simulate these gene expression profiling findings and create a new biologic prognostic model based on immunohistochemistry. We studied 199 patients (125 in the training set, 74 in the validation set) with de novo diffuse large B-cell lymphoma treated with rituximab and CHOP (cyclophosphamide, doxorubicin, vincristine, and prednisone) or CHOP-like therapies, and immunohistochemical stains were performed on paraffin-embedded tissue microarrays. In the model, 1 point was awarded for each adverse prognostic factor: nongerminal center B cell-like subtype, SPARC (secreted protein, acidic, and rich in cysteine) < 5%, and microvascular density quartile 4. The model using these 3 biologic markers was highly predictive of overall survival and event-free survival in multivariate analysis after adjusting for the International Prognostic Index in both the training and validation sets. This new model delineates 2 groups of patients, 1 with a low biologic score (0-1) and good survival and the other with a high score (2-3) and poor survival. This new biologic prognostic model could be used with the International Prognostic Index to stratify patients for novel or risk-adapted therapies.",,"cyclophosphamide; doxorubicin; osteonectin; prednisone; rituximab; stromal cell derived factor 1; vincristine; adult; aged; article; cancer prognosis; cancer survival; clinical feature; female; gene expression; human; human tissue; immunohistochemistry; large cell lymphoma; major clinical study; male; overall survival; priority journal; scoring system; simulation; tissue microarray; validation study; Adult; Aged; Aged, 80 and over; Antibodies, Monoclonal, Murine-Derived; Antineoplastic Combined Chemotherapy Protocols; Artificial Intelligence; Cohort Studies; Cyclophosphamide; Doxorubicin; Expert Systems; Female; Follow-Up Studies; Humans; Immunohistochemistry; Lymphoma, Large B-Cell, Diffuse; Male; Middle Aged; Models, Biological; Neoplasm Proteins; Prednisone; Prognosis; Survival Analysis; Tissue Array Analysis; Vincristine; Young Adult",Article,Scopus,2-s2.0-84866318188
"Szuvovivski I., Fernandes T.S.P., Aoki A.R.","Simultaneous allocation of capacitors and voltage regulators at distribution networks using Genetic Algorithms and Optimal Power Flow",2012,"International Journal of Electrical Power and Energy Systems",31,10.1016/j.ijepes.2012.02.006,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860780332&doi=10.1016%2fj.ijepes.2012.02.006&partnerID=40&md5=850d3b04fb6ac24b072990f6c0c4d5dd","The high reactive power level demanded by the distribution systems, the loads growth and consequent increase of system losses introduce variations at the buses voltage magnitudes, which compromise the quality of the supplied electric energy. To assure high quality, some devices such as voltage regulators - VRs and capacitors banks - CBs, are installed to allow effective control of voltage magnitude, reactive power and power factor. The present work proposes a methodology to allocate simultaneously these devices using both Genetic Algorithms - GAs and Optimal Power Flow - OPF. The strategy proposed involves the adoption of GA for the allocation of CBs with specification for the type of bank (fixed or automatic) and the reactive power (kvar), as well as the allocation of VRs with adjustment of their secondary voltage. The OPF is responsible for the solution of the power balance equations, tap adjustments of the VRs that assure the voltage level at their exits according to the voltage level specified by the GA for the diverse load curve and for the attainment of the nominal current of the VRs allocated. © 2012 Elsevier Ltd. All rights reserved.","Artificial intelligence; Capacitor; Distribution network; Genetic Algorithm; Optimal Power Flow; Voltage regulator","Distribution systems; Electric energies; High quality; Load curves; Nominal currents; Optimal power flows; Power balance equations; Power factors; Secondary voltage; System loss; Voltage levels; Voltage magnitude; Acoustic generators; Artificial intelligence; Electric circuit breakers; Electric load flow; Electric losses; Electric power distribution; Electric power factor; Genetic algorithms; Reactive power; Voltage regulators; Capacitors",Article,Scopus,2-s2.0-84860780332
"Vural R.A., Yildirim T.","Analog circuit sizing via swarm intelligence",2012,"AEU - International Journal of Electronics and Communications",31,10.1016/j.aeue.2012.01.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861911550&doi=10.1016%2fj.aeue.2012.01.003&partnerID=40&md5=7226bce6f95ff41cde836b17d76cf1db","Together with the increase in electronic circuit complexity, the design and optimization processes have to be automated with high accuracy. Predicting and improving the design quality in terms of performance, robustness and cost is the central concern of electronic design automation. Generally, optimization is a very difficult and time consuming task including many conflicting criteria and a wide range of design parameters. Particle swarm optimization (PSO) was introduced as an efficient method for exploring the search space and handling constrained optimization problems. In this work, PSO has been utilized for accommodating required functionalities and performance specifications considering optimal sizing of analog integrated circuits with high optimization ability in short computational time. PSO based design results are verified with SPICE simulations and compared to previous studies. © 2012 Elsevier GmbH. All rights reserved.","Area oriented optimization; Circuit design automation; CMOS analog integrated circuit sizing; Particle swarm optimization","Analog circuit sizing; Analog integrated circuit; Circuit designs; CMOS analog integrated circuits; Computational time; Constrained optimization problems; Design and optimization; Design parameters; Design Quality; Electronic design automation; Optimal sizing; Optimization ability; Performance specifications; Required functionalities; Search spaces; SPICE simulations; Swarm Intelligence; Time-consuming tasks; Artificial intelligence; Computer aided design; Constrained optimization; Linear integrated circuits; Particle swarm optimization (PSO)",Article,Scopus,2-s2.0-84861911550
"Khan A., Moideen F., Lopez J., Khoo W.L., Zhu Z.","KinDectect: Kinect detecting objects",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",31,10.1007/978-3-642-31534-3_86,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864818073&doi=10.1007%2f978-3-642-31534-3_86&partnerID=40&md5=3a640415f8eb272063406c1e0b343c52","Detecting humans and objects in images has been a very challenging problem due to variation in illumination, pose, clothing, background and other complexities. Depth information is an important cue when humans recognize objects and other humans. In this work we utilize the depth information that a Kinect sensor - Xtion Pro Live provides to detect humans and obstacles in real time for a blind or visually impaired user. The system runs in two modes. For the first mode, we focus on how to track and/or detect multiple humans and moving objects and transduce the information to the user. For the second mode, we present a novel approach on how to avoid obstacles for safe navigation for a blind or visually-impaired user in an indoor environment. In addition, we present a user study with some blind-folded users to measure the efficiency and robustness of our algorithms and approaches. © 2012 Springer-Verlag.",,"Avoid obstacles; Depth information; Detecting objects; Indoor environment; Moving objects; Real time; User study; Visually-impaired users; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84864818073
"Sanin C., Toro C., Haoxi Z., Sanchez E., Szczerbicki E., Carrasco E., Peng W., Mancilla-Amaya L.","Decisional DNA: A multi-technology shareable knowledge structure for decisional experience",2012,"Neurocomputing",31,10.1016/j.neucom.2011.08.029,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860233830&doi=10.1016%2fj.neucom.2011.08.029&partnerID=40&md5=ec0d050e4e3e3b0b4cb47f0e246486ce","Knowledge representation and engineering techniques are becoming useful and popular components of hybrid integrated systems used to solve complicated practical problems in different disciplines. These techniques offer features such as: learning from experience, handling noisy and incomplete data, helping with decision making, and predicting capabilities. In this paper, we present a multi-domain knowledge representation structure called Decisional DNA that can be implemented and shared for the exploitation of embedded knowledge in multiple technologies. Decisional DNA, as a knowledge representation structure, offers great possibilities on gathering explicit knowledge of formal decision events as well as a tool for decision making processes. Its applicability is shown in this paper when applied to different decisional technologies. The main advantages of using the Decisional DNA rely on: (i) versatility and dynamicity of the knowledge structure, (ii) storage of day-to-day explicit experience in a single structure, (iii) transportability and shareability of the knowledge, and (iv) predicting capabilities based on the collected experience. Thus, after analysis and results, we conclude that the Decisional DNA, as a unique multi-domain structure, can be applied and shared among multiple technologies while enhancing them with predicting capabilities and facilitating knowledge engineering processes inside decision making systems. © 2012 Elsevier B.V.","Artificial intelligence; Decision making; Decisional DNA; Knowledge engineering; Knowledge representation; Set of experience knowledge structure","Decision making process; Decision-making systems; Decisional DNA; Engineering techniques; Explicit knowledge; Incomplete data; Integrated systems; Knowledge structures; Multi domains; Multidomain structure; Multiple technology; Practical problems; Set of experience knowledge structure; Single structure; Artificial intelligence; Decision making; DNA; Forecasting; Knowledge engineering; Technology; Knowledge representation; article; artificial intelligence; computer system; decision making; decision support system; Decisional DNA; experience; information processing; information retrieval; information storage; information technology; knowledge; knowledge management; machine learning; prediction; priority journal",Article,Scopus,2-s2.0-84860233830
"Libert B., Paterson K.G., Quaglia E.A.","Anonymous broadcast encryption: Adaptive security and efficient constructions in the standard model",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",31,10.1007/978-3-642-30057-8_13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861702751&doi=10.1007%2f978-3-642-30057-8_13&partnerID=40&md5=be6572c5375fed7685619f79a0c25e68","In this paper we consider anonymity in the context of Broadcast Encryption (BE). This issue has received very little attention so far and all but one of the currently available BE schemes fail to provide anonymity. Yet, we argue that it is intrinsically desirable to provide anonymity in standard applications of BE and that it can be achieved at a moderate cost. We provide a security definition for Anonymous Broadcast Encryption (ANOBE) and show that it is achievable assuming only the existence of IND-CCA secure public key encryption (PKE). Focusing on reducing the size of ciphertexts, we then give two generic constructions for ANOBE. The first is from any anonymous (key-private) IND-CCA secure PKE scheme, and the second is from any IBE scheme that satisfies a weak security notion in the multi-TA setting. Furthermore, we show how randomness re-use techniques can be deployed in the ANOBE context to reduce computational and communication costs, and how a new cryptographic primitive - anonymous hint systems - can be used to speed up the decryption process in our ANOBE constructions. All of our results are in the standard model, achieving fully collusion-resistant ANOBE schemes secure against adaptive IND-CCA adversaries. © 2012 International Association for Cryptologic Research.","Anonymity; Broadcast Encryption","Adaptive security; Anonymity; Anonymous broadcast; Broadcast encryption; Ciphertexts; Communication cost; Cryptographic primitives; Decryption process; Efficient construction; Generic construction; IND-CCA; Public-key encryption; Security definitions; Security notion; The standard model; Anonymity; Anonymous broadcast; Broadcast encryption; Cryptographic primitives; Efficient construction; Generic construction; Public-key encryption; Security definitions; Artificial intelligence; Computation theory; Public key cryptography; Public key cryptography; Cryptography",Conference Paper,Scopus,2-s2.0-84861702751
"Firat M., Hurkens C.A.J.","An improved MIP-based approach for a multi-skill workforce scheduling problem",2012,"Journal of Scheduling",31,10.1007/s10951-011-0245-x,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861233155&doi=10.1007%2fs10951-011-0245-x&partnerID=40&md5=d570052d58296c9a315d12707e1c3542","This paper deals with scheduling complex tasks with an inhomogeneous set of resources. The problem is to assign technicians to tasks with multi-level skill requirements. Here, the requirements are merely the presence of a set of technicians that possess the necessary capabilities. An additional complication is that a set of combined technicians stays together for the duration of a work day. This typically applies to scheduling of maintenance and installation operations. We build schedules by repeated application of a flexible matching model that selects tasks to be processed and forms groups of technicians assigned to combinations of tasks. The underlying mixed integer programming (MIP) model is capable of revising technician-task allocations and performs very well, especially in the case of rare skills. © The Author(s) 2011.","Mixed integer programming; Multi-skill workforce scheduling; Project scheduling","Complex task; Flexible matching model; Mixed integer programming; Mixed integer programming model; Project scheduling; Repeated application; Skill requirements; Workforce scheduling; Artificial intelligence; Software engineering; Scheduling",Article,Scopus,2-s2.0-84861233155
"Zamuda A., Brest J.","Population reduction differential evolution with multiple mutation strategies in real world industry challenges",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",31,10.1007/978-3-642-29353-5_18,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860662277&doi=10.1007%2f978-3-642-29353-5_18&partnerID=40&md5=28d2a24345755711503689cfab2025a3","This paper presents a novel differential evolution algorithm for optimization of state-of-the-art real world industry challenges. The algorithm includes the self-adaptive jDE algorithm with one of its strongest extensions, population reduction, and is now combined with multiple mutation strategies. The two mutation strategies used are run dependent on the population size, which is reduced with growing function evaluation number. The problems optimized reflect several of the challenges in current industry problems tackled by optimization algorithms nowadays. We present results on all of the 22 problems included in the Problem Definitions for a competition on Congress on Evolutionary Computation (CEC) 2011. Performance of the proposed algorithm is compared to two algorithms from the competition, where the average final best results obtained for each test problem on three different number of total function evaluations allowed are compared. © 2012 Springer-Verlag.","multiple mutation strategies; population reduction; real world industry challenges; self-adaptive differential evolution","Differential Evolution; Differential evolution algorithms; Industry problems; Multiple mutations; Mutation strategy; Optimization algorithms; Population sizes; Problem definition; Self-adaptive; Test problem; World industry; Artificial intelligence; Calculations; Function evaluation; Industry; Optimization; Population statistics; Soft computing; Evolutionary algorithms",Conference Paper,Scopus,2-s2.0-84860662277
"Jeyarani R., Nagaveni N., Vasanth Ram R.","Design and implementation of adaptive power-aware virtual machine provisioner (APA-VMP) using swarm intelligence",2012,"Future Generation Computer Systems",31,10.1016/j.future.2011.06.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857371074&doi=10.1016%2fj.future.2011.06.002&partnerID=40&md5=c70a8fe7aa589ef133e6fd0bcf85a6ef","Cloud computing aims at providing dynamic leasing of server capabilities as scalable, virtualized services to end users. Our work focuses on the Infrastructure as a Service (IaaS) model where custom Virtual Machines (VM) are launched in appropriate servers available in a data center. The cloud data center taken into consideration is heterogeneous and large scale in nature. Such a resource pool is basically characterized by high resource dynamics caused by non-linear variation in the availability of processing elements, memory size, storage capacity, bandwidth and power drawn resulting from the sporadic nature of workload. Apart from the said resource dynamics, our proposed work also considers the processor transitions to various sleep states and their corresponding wake up latencies that are inherent in contemporary enterprise servers. The primary objective of the proposed metascheduler is to map efficiently a set of VM instances onto a set of servers from a highly dynamic resource pool by fulfilling resource requirements of maximum number of workloads. As the cloud data centers are overprovisioned to meet the unexpected workload surges, huge power consumption has become one of the major issues of concern. We have proposed a novel metascheduler called Adaptive Power-Aware Virtual Machine Provisioner (APA-VMP) that schedules the workload in such a way that the total incremental power drawn by the server pool is minimum without compromising the performance objectives. The APA-VMP makes use of swarm intelligence methodology to detect and track the changing optimal target servers for VM placement very efficiently. The scenario was experimented by novel Self-adaptive Particle Swarm Optimization (SAPSO) for VM provisioning, which makes best possible use of the power saving states of idle servers and instantaneous workload on the operational servers. It is evident from the results that there is a significant reduction in the power numbers against the existing strategies. © 2011 Elsevier B.V. All rights reserved.","Cloud computing; Dynamic adaptive PSO; Dynamic Voltage Frequency Scaling (DVFS); Particle Swarm Optimization (PSO); Power conservation; Power saving states; Resource dynamics; VM provisioning","Cloud data; Data centers; Dynamic Voltage Frequency Scaling (DVFS); End users; Enterprise servers; Memory size; Metascheduler; Non-linear variation; Optimal target; Performance objective; Power conservation; Power savings; Power-aware; Primary objective; Processing elements; Resource requirements; Self-adaptive; Sleep state; Storage capacity; Swarm Intelligence; Virtual machines; Virtualized services; VM provisioning; Wake-up latency; Artificial intelligence; Cellular automata; Cloud computing; Dynamics; Lakes; Computer simulation",Article,Scopus,2-s2.0-84857371074
"Tamez-Peña J.G., Farber J., González P.C., Schreyer E., Schneider E., Totterman S.","Unsupervised segmentation and quantification of anatomical knee features: Data from the osteoarthritis initiative",2012,"IEEE Transactions on Biomedical Engineering",31,10.1109/TBME.2012.2186612,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858983399&doi=10.1109%2fTBME.2012.2186612&partnerID=40&md5=9b199b42e0d2c963a3136fe3e5b7c41b","This paper presents a fully automated method for segmenting articular knee cartilage and bone from in vivo 3-D dual echo steady state images. The magnetic resonance imaging (MRI) datasets were obtained from the Osteoarthritis Initiative (OAI) pilot study and include longitudinal images from controls and subjects with knee osteoarthritis (OA) scanned twice at each visit (baseline, 24 month). Initially, human experts segmented six MRI series. Five of the six resultant sets served as reference atlases for a multiatlas segmentation algorithm. The methodology created precise knee segmentations that were used to extract articular cartilage volume, surface area, and thickness as well as subchondral bone plate curvature. Comparison to manual segmentation showed Dice similarity coefficient (DSC) of 0.88 and 0.84 for the femoral and tibial cartilage. In OA subjects, thickness measurements showed test-retest precision ranging from 0.014 mm (0.6%) at the femur to 0.038 mm (1.6%) at the femoral trochlea. In the same population, the curvature test-retest precision ranged from 0.0005 mm -1 (3.6%) at the femur to 0.0026 mm -1 (11.7%) at the medial tibia. Thickness longitudinal changes showed OA Pearson correlation coefficient of 0.94 for the femur. In conclusion, the fully automated segmentation methodology produces reproducible cartilage volume, thickness, and shape measurements valuable for the study of OA progression. © 2006 IEEE.","Biomedical image processing; cartilage segmentation; magnetic resonance imaging (MRI); osteoarthritis (OA); three-dimensional (3-D) image segmentation","Articular cartilages; Automated methods; Automated segmentation; Bio-medical image processing; Bone plate; cartilage segmentation; Cartilage volume; Data sets; Human expert; In-vivo; Knee osteoarthritis; Manual segmentation; osteoarthritis (OA); Pearson correlation coefficients; Pilot studies; Segmentation algorithms; Shape measurements; Similarity coefficients; Steady state; Surface area; Unsupervised segmentation; Bone; Cartilage; Correlation methods; Image segmentation; Magnetic resonance imaging; Thickness measurement; Three dimensional; accuracy; adult; algorithm; article; articular cartilage volume; automation; bone; cartilage; clinical article; comparative anatomy; controlled study; correlation coefficient; human; image analysis; image processing; image segmentation; in vivo study; knee; knee osteoarthritis; nuclear magnetic resonance imaging; quantitative analysis; radiological parameters; radiological procedures; reproducibility; subchondral bone plate curvature; surface area; thickness; three dimensional imaging; tissue structure; Adult; Aged; Algorithms; Artificial Intelligence; Echo-Planar Imaging; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Knee Joint; Male; Middle Aged; Osteoarthritis, Knee; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84858983399
"Tiwari P., Viswanath S., Kurhanewicz J., Sridhar A., Madabhushi A.","Multimodal wavelet embedding representation for data combination (MaWERiC): Integrating magnetic resonance imaging and spectroscopy for prostate cancer detection",2012,"NMR in Biomedicine",31,10.1002/nbm.1777,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858846668&doi=10.1002%2fnbm.1777&partnerID=40&md5=5c8300e5bb8b785ff1975738f861e641","Recently, both Magnetic Resonance (MR) Imaging (MRI) and Spectroscopy (MRS) have emerged as promising tools for detection of prostate cancer (CaP). However, due to the inherent dimensionality differences in MR imaging and spectral information, quantitative integration of T 2 weighted MRI (T 2w MRI) and MRS for improved CaP detection has been a major challenge. In this paper, we present a novel computerized decision support system called multimodal wavelet embedding representation for data combination (MaWERiC) that employs, (i) wavelet theory to extract 171 Haar wavelet features from MRS and 54 Gabor features from T 2w MRI, (ii) dimensionality reduction to individually project wavelet features from MRS and T 2w MRI into a common reduced Eigen vector space, and (iii), a random forest classifier for automated prostate cancer detection on a per voxel basis from combined 1.5 T in vivo MRI and MRS. A total of 36 1.5T endorectal in vivo T 2w MRI and MRS patient studies were evaluated per voxel by MaWERiC using a three-fold cross validation approach over 25 iterations. Ground truth for evaluation of results was obtained by an expert radiologist annotations of prostate cancer on a per voxel basis who compared each MRI section with corresponding ex vivo wholemount histology sections with the disease extent mapped out on histology. Results suggest that MaWERiC based MRS T 2w meta-classifier (mean AUC, μ=0.89±0.02) significantly outperformed (i) a T 2w MRI (using wavelet texture features) classifier (μ=0.55±0.02), (ii) a MRS (using metabolite ratios) classifier (μ=0.77±0.03), (iii) a decision fusion classifier obtained by combining individual T 2w MRI and MRS classifier outputs (μ=0.85±0.03), and (iv) a data combination method involving a combination of metabolic MRS and MR signal intensity features (μ=0.66±0.02). © 2011 John Wiley &amp; Sons, Ltd.","Gabor texture features; Haar wavelets; Magnetic resonance imaging; Magnetic resonance spectroscopy; Multimodal integration; PCA, random forest classifier; Prostate cancer","Gabor texture; Haar wavelets; Multimodal integration; Prostate cancers; Random forest classifier; Artificial intelligence; Data handling; Decision support systems; Decision trees; Forestry; Histology; Magnetic resonance; Magnetic resonance imaging; Magnetic resonance spectroscopy; Metabolism; Diseases; area under the curve; article; cancer diagnosis; computer analysis; computer system; evaluation; multimodal wavelet embedding; nuclear magnetic resonance imaging; nuclear magnetic resonance spectroscopy; priority journal; prostate cancer; Aged; Aged, 80 and over; Diagnosis, Computer-Assisted; Humans; Magnetic Resonance Imaging; Magnetic Resonance Spectroscopy; Male; Middle Aged; Pattern Recognition, Automated; Prostatic Neoplasms; Reproducibility of Results; Sensitivity and Specificity; Systems Integration; Tumor Markers, Biological; Wavelet Analysis; Artificial Intelligence; Cancers; Decision Theory; Histology; Magnetic Resonance; Metabolism",Article,Scopus,2-s2.0-84858846668
"Gu D.-X., Liang C.-Y., Bichindaritz I., Zuo C.-R., Wang J.","A case-based knowledge system for safety evaluation decision making of thermal power plants",2012,"Knowledge-Based Systems",31,10.1016/j.knosys.2011.08.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84155189047&doi=10.1016%2fj.knosys.2011.08.002&partnerID=40&md5=6bf1f2fc15d702fd57e1a39ffc6e8791","Safety assessment of thermal power plants (TPP) is an important means to ensure the safety of production in thermal power production enterprises. Modern information technology can play an important role in TPP safety assessment. The evaluation of power plant systems relies, to a large extent, on the knowledge and experience of the experts undertaking the task. Case-based reasoning (CBR) is introduced for the safety assessment of TPP since it models expertise through experience management. This paper provides a case-based approach for the Management System safety assessment decision making of TPP (MSSATPP). We introduce a case matching method named CBR-Grey, which integrates the Delphi approach and grey system theory. Based on this method, we implement a prototype of case-based knowledge system (CBRSYS-TPP) for the evaluation decision making of the panel of experts. Our experimental results based on a real-world TPP safety assessment data set show that CBRSYS-TPP has high accuracy and systematically good performance. © 2011 Elsevier B.V. All rights reserved.","Case-based reasoning; Grey system theory; Intelligent decision support system; Knowledge-based system; Thermal power plants safety evaluation","Case matching; Case-based approach; Case-based knowledge; CBr; Data sets; Experience management; Grey system theory; Intelligent decision support systems; Knowledge and experience; Management systems; Power plant system; Safety assessments; Safety evaluations; Thermal power; Thermal power plants; Artificial intelligence; Decision making; Decision support systems; Information technology; Knowledge based systems; Power plants; Rating; Safety engineering; System theory; Thermoelectric power plants; Case based reasoning",Article,Scopus,2-s2.0-84155189047
"Kutyniok G., Lim W.-Q.","Image separation using wavelets and shearlets",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",31,10.1007/978-3-642-27413-8_26,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855682147&doi=10.1007%2f978-3-642-27413-8_26&partnerID=40&md5=e582835b18135a45c685faa65d494749","In this paper, we present an image separation method for separating images into point- and curvelike parts by employing a combined dictionary consisting of wavelets and compactly supported shearlets utilizing the fact that they sparsely represent point and curvilinear singularities, respectively. Our methodology is based on the very recently introduced mathematical theory of geometric separation, which shows that highly precise separation of the morphologically distinct features of points and curves can be achieved by ℓ 1 minimization. Finally, we present some experimental results showing the effectiveness of our algorithm, in particular, the ability to accurately separate points from curves even if the curvature is relatively large due to the excellent localization property of compactly supported shearlets. © 2012 Springer-Verlag.","ℓ 1 minimization; Geometric separation; shearlets; sparse approximation; wavelets","Compactly supported; Curvilinear singularity; Geometric separation; Image separation; Localization properties; Mathematical theory; Shearlets; Sparse approximations; wavelets; Artificial intelligence; Separation",Conference Paper,Scopus,2-s2.0-84855682147
"Jevtić A., Gutierrez Á., Andina D., Jamshidi M.","Distributed bees algorithm for task allocation in swarm of robots",2012,"IEEE Systems Journal",31,10.1109/JSYST.2011.2167820,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861755946&doi=10.1109%2fJSYST.2011.2167820&partnerID=40&md5=cdc3d3a63e60a899bcfa1c4d3842948d","In this paper, we propose the distributed bees algorithm (DBA) for task allocation in a swarm of robots. In the proposed scenario, task allocation consists in assigning the robots to the found targets in a 2-D arena. The expected distribution is obtained from the targets' qualities that are represented as scalar values. Decision-making mechanism is distributed and robots autonomously choose their assignments taking into account targets' qualities and distances. We tested the scalability of the proposed DBA algorithm in terms of number of robots and number of targets. For that, the experiments were performed in the simulator for various sets of parameters, including number of robots, number of targets, and targets' utilities. Control parameters inherent to DBA were tuned to test how they affect the final robot distribution. The simulation results show that by increasing the robot swarm size, the distribution error decreased. © 2012 IEEE.","Multirobot systems; scalability; swarm intelligence; task allocation","Bees algorithms; Control parameters; DBA algorithms; Decision-making mechanisms; Multi-robot systems; Robot swarms; Scalar values; Swarm Intelligence; Swarm of robots; Task allocation; Algorithms; Artificial intelligence; Scalability; Robots",Article,Scopus,2-s2.0-84861755946
"Baudin P.-Y., Azzabou N., Carlier P.G., Paragios N.","Prior knowledge, random walks and human skeletal muscle segmentation",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",31,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872575766&partnerID=40&md5=88035467641931cb19e6d74410bc5c64","In this paper, we propose a novel approach for segmenting the skeletal muscles in MRI automatically. In order to deal with the absence of contrast between the different muscle classes, we proposed a principled mathematical formulation that integrates prior knowledge with a random walks graph-based formulation. Prior knowledge is represented using a statistical shape atlas that once coupled with the random walks segmentation leads to an efficient iterative linear optimization system. We reveal the potential of our approach on a challenging set of real clinical data. © Springer-Verlag Berlin Heidelberg 2012.",,"Linear programming; Medical computing; Medical imaging; Random processes; Clinical data; Graph-based; Linear optimization; Mathematical formulation; Prior knowledge; Random Walk; Skeletal muscle; Statistical shapes; Muscle; algorithm; article; artificial intelligence; automated pattern recognition; computer program; human; image processing; methodology; nuclear magnetic resonance imaging; pathology; probability; reproducibility; skeletal muscle; statistical model; theoretical model; three dimensional imaging; Algorithms; Artificial Intelligence; Humans; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Markov Chains; Models, Statistical; Models, Theoretical; Muscle, Skeletal; Pattern Recognition, Automated; Probability; Reproducibility of Results; Software",Conference Paper,Scopus,2-s2.0-84872575766
"Zhang K., Lan L., Wang Z., Moerchen F.","Scaling up kernel SVM on limited resources: A low-rank linearization approach",2012,"Journal of Machine Learning Research",31,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879867381&partnerID=40&md5=9295cc77b0d460b35c8c8851560430ba","Kernel Support Vector Machine delivers state-of-the-art results in non-linear classification, but the need to maintain a large number of support vectors poses a challenge in large scale training and testing. In contrast, linear SVM is much more scalable even on limited computing recourses (e.g. daily life PCs), but the learned model cannot capture non-linear concepts. To scale up kernel SVM on limited resources, we propose a lowrank linearization approach that transforms a non-linear SVM to a linear one via a novel, approximate empirical kernel map computed from efficient low-rank approximation of kernel matrices. We call it LLSVM (Lowrank Linearized SVM). We theoretically study the gap between the solutions of the optimal and approximate kernel map, which is used in turn to provide important guidance on the sampling based kernel approximations. Our algorithm inherits high efficiency of linear SVMs and rich repesentability of kernel classifiers. Evaluation against large-scale linear and kernel SVMs on several truly large data sets shows that the proposed method achieves a better tradeoff between scalability and model representability. © Copyright 2012 by the authors.",,"Approximation theory; Artificial intelligence; Importance sampling; Linearization; Mathematical transformations; Speech recognition; Empirical kernel map; Kernel approximation; Kernel classifiers; Low rank approximations; Nonlinear classification; Representability; State of the art; Training and testing; Support vector machines",Conference Paper,Scopus,2-s2.0-84879867381
"Chang Y.-W., Huang M.-H.","A study of the evolution of interdisciplinarity in library and information science: Using three bibliometric methods",2012,"Journal of the American Society for Information Science and Technology",31,10.1002/asi.21649,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-83655202966&doi=10.1002%2fasi.21649&partnerID=40&md5=3d03f80a880dd79ad02b4c8a320e21b4","This study uses three bibliometric methods: direct citation, bibliographic coupling, and co-authorship analysis, to investigate interdisciplinary changes in library and information science (LIS) from 1978 to 2007. The results reveal that LIS researchers most frequently cite publications in their own discipline. In addition, half of all co-authors of LIS articles are affiliated with LIS-related institutes. The results confirm that the degree of interdisciplinarity within LIS has increased, particularly co-authorship. However, the study found sources of direct citations in LIS articles are widely distributed across 30 disciplines, but co-authors of LIS articles are distributed across only 25 disciplines. The degree of interdisciplinarity was found ranging from 0.61 to 0.82 with citation to references in all articles being the highest and that of co-authorship being the lowest. Percentages of contribution attributable to LIS show a decreasing tendency based on the results of direct citation and co-authorship analysis, but an increasing tendency based on those of bibliographic coupling analysis. Such differences indicate each of the three bibliometric methods has its strength and provides insights respectively for viewing various aspects of interdisciplinarity, suggesting the use of no single bibliometric method can reveal all aspects of interdisciplinarity due to its multifaceted nature. © 2011 ASIS&T.",,"Bibliographic couplings; Coauthorship; Interdisciplinarity; Library and information science; Artificial intelligence; Software engineering; Information science",Article,Scopus,2-s2.0-83655202966
"Huang H., Xie H.-B., Guo J.-Y., Chen H.-J.","Ant colony optimization-based feature selection method for surface electromyography signals classification",2012,"Computers in Biology and Medicine",31,10.1016/j.compbiomed.2011.10.004,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-83855160795&doi=10.1016%2fj.compbiomed.2011.10.004&partnerID=40&md5=56b753036134d639d819d847a7696605","This paper presented a new ant colony optimization (ACO) feature selection method to classify hand motion surface electromyography (sEMG) signals. The multiple channels of sEMG recordings make the dimensionality of sEMG feature grow dramatically. It is known that the informative feature subset with small size is a precondition for the accurate and computationally efficient classification strategy. Therefore, this study proposed an ACO based feature selection scheme using the heuristic information measured by the minimum redundancy maximum relevance criterion (ACO-mRMR). The experiments were conducted on ten subjects with eight upper limb motions. Two feature sets, i.e., time domain features combined with autoregressive model coefficients (TDAR) and wavelet transform (WT) features, were extracted from the recorded sEMG signals. The average classification accuracies of using ACO reduced TDAR and WT features were 95.45±2.2% and 96.08±3.3%, respectively. The principal component analysis (PCA) was also conducted on the same data sets for comparison. The average classification accuracies of using PCA reduced TDAR and WT features were 91.51±4.9% and 89.87±4.4%, respectively. The results demonstrated that the proposed ACO-mRMR based feature selection method can achieve considerably high classification rates in sEMG motion classification task and be applicable to other biomedical signals pattern analysis. © 2011 Elsevier Ltd.","Ant colony optimization; Feature selection; Minimum redundancy maximum relevance; Pattern classification; Surface electromyography","Ant colonies; Ant-colony optimization; Autoregressive model coefficients; Biomedical signal; Classification accuracy; Classification rates; Computationally efficient; Data sets; Feature selection methods; Feature sets; Feature subset; Hand motion; Heuristic information; Minimum redundancy maximum relevance; Motion classification; Multiple channels; Pattern analysis; Relevance criteria; Small size; Surface electromyography; Surface electromyography signals; Time domain; Upper limb motion; Algorithms; Artificial intelligence; Bioelectric phenomena; Optimization; Pattern recognition; Principal component analysis; Redundancy; Time domain analysis; Wavelet transforms; Feature extraction; accuracy; ant colony optimization; arm; article; biology; biomedical engineering; electromyography; human; methodology; motion; principal component analysis; priority journal; redundancy analysis; signal processing; wavelet analysis; Algorithms; Computational Biology; Databases, Factual; Electromyography; Hand; Humans; Movement; Pattern Recognition, Automated; Principal Component Analysis; Reproducibility of Results; Signal Processing, Computer-Assisted",Article,Scopus,2-s2.0-83855160795
"Cornaz D., Galand L., Spanjaard O.","Bounded single-peaked width and proportional representation",2012,"Frontiers in Artificial Intelligence and Applications",31,10.3233/978-1-61499-098-7-270,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878791753&doi=10.3233%2f978-1-61499-098-7-270&partnerID=40&md5=bc95d93a08e307387ac093d08c1f8b54","This paper is devoted to the proportional representation (PR) problem when the preferences are clustered single-peaked. PR is a ""multi-winner"" election problem, that we study in Chamberlin and Courant's scheme [6]. We define clustered single-peakedness as a form of single-peakedness with respect to clusters of candidates, i.e. subsets of candidates that are consecutive (in arbitrary order) in the preferences of all voters. We show that the PR problem becomes polynomial when the size of the largest cluster of candidates (width) is bounded. Furthermore, we establish the polynomiality of determining the single-peaked width of a preference profile (minimum width for a partition of candidates into clusters compatible with clustered single-peakedness) when the preferences are narcissistic (i.e., every candidate is the most preferred one for some voter). © 2012 The Author(s).",,"Arbitrary order; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84878791753
"Yao W., Chen X., Zhao Y., Van Tooren M.","Concurrent subspace width optimization method for RBF neural network modeling",2012,"IEEE Transactions on Neural Networks and Learning Systems",30,10.1109/TNNLS.2011.2178560,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875152882&doi=10.1109%2fTNNLS.2011.2178560&partnerID=40&md5=1639ed4998442199201bd99710a1992c","Radial basis function neural networks (RBFNNs) are widely used in nonlinear function approximation. One of the challenges in RBFNN modeling is determining how to effectively optimize width parameters to improve approximation accuracy. To solve this problem, a width optimization method, concurrent subspace width optimization (CSWO), is proposed based on a decomposition and coordination strategy. This method decomposes the large-scale width optimization problem into several subspace optimization (SSO) problems, each of which has a single optimization variable and smaller training and validation data sets so as to greatly simplify optimization complexity. These SSOs can be solved concurrently, thus computational time can be effectively reduced. With top-level system coordination, the optimization of SSOs can converge to a consistent optimum, which is equivalent to the optimum of the original width optimization problem. The proposed method is tested with four mathematical examples and one practical engineering approximation problem. The results demonstrate the efficiency and robustness of CSWO in optimizing width parameters over the traditional width optimization methods. © 2012 IEEE.","Concurrent subspace width optimization; coordination; decomposition; radial basis function neural network","Approximation accuracy; coordination; Coordination strategy; Nonlinear function approximation; Optimization problems; Optimization variables; Practical engineering; Radial basis function neural networks; Decomposition; Neural networks; Optimization; Problem solving; algorithm; artificial intelligence; artificial neural network; automated pattern recognition; computer simulation; nonlinear system; procedures; statistical model; Algorithms; Artificial Intelligence; Computer Simulation; Models, Statistical; Neural Networks (Computer); Nonlinear Dynamics; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84875152882
"Yang W., Lu Z., Yu M., Huang M., Feng Q., Chen W.","Content-based retrieval of focal liver lesions using bagof-visual-words representations of single- and multiphase contrast-enhanced CT images",2012,"Journal of Digital Imaging",30,10.1007/s10278-012-9495-1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870060150&doi=10.1007%2fs10278-012-9495-1&partnerID=40&md5=015133abc96565e5d08ad033304c700c","This paper is aimed at developing and evaluating a content-based retrieval method for contrastenhanced liver computed tomographic (CT) images using bag-of-visual-words (BoW) representations of single and multiple phases. The BoW histograms are extracted using the raw intensity as local patch descriptor for each enhance phase by densely sampling the image patches within the liver lesion regions. The distance metric learning algorithms are employed to obtain the semantic similarity on the Hellinger kernel feature map of the BoW histograms. The different visual vocabularies for BoW and learned distance metrics are evaluated in a contrast-enhanced CT image dataset comprised of 189 patients with three types of focal liver lesions, including 87 hepatomas, 62 cysts, and 60 hemangiomas. For each single enhance phase, the mean of average precision (mAP) of BoW representations for retrieval can reach above 90 % which is significantly higher than that of intensity histogram and Gabor filters. Furthermore, the combined BoW representations of the three enhance phases can improve mAP to 94.5 %. These preliminary results demonstrate that the BoW representation is effective and feasible for retrieval of liver lesions in contrast-enhanced CT images. © Society for Imaging Informatics in Medicine 2012.","Bag of visual words; Content-based image retrieval; Contrast-enhanced CT; Distance metric learning; Liver lesion","Bag-of-visual-words; Content based image retrieval; Contrast-enhanced; Contrast-enhanced CT; Descriptors; Distance Metric Learning; Distance metrics; Feature map; Focal liver lesions; Image patches; Semantic similarity; Tomographic; Visual vocabularies; Content based retrieval; Face recognition; Graphic methods; Information retrieval; Learning algorithms; Statistical methods; Computerized tomography; contrast medium; diagnostic agent; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; hospital information system; human; liver disease; medical informatics; methodology; radiography; spiral computer assisted tomography; Algorithms; Artificial Intelligence; Contrast Media; Humans; Liver Diseases; Medical Informatics; Pattern Recognition, Automated; Radiographic Image Interpretation, Computer-Assisted; Radiology Information Systems; Tomography, Spiral Computed",Article,Scopus,2-s2.0-84870060150
"Abibullaev B., An J.","Classification of frontal cortex haemodynamic responses during cognitive tasks using wavelet transforms and machine learning algorithms",2012,"Medical Engineering and Physics",30,10.1016/j.medengphy.2012.01.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869502428&doi=10.1016%2fj.medengphy.2012.01.002&partnerID=40&md5=f7627332d33136b4e767429fa0cd2fc3","Recent advances in neuroimaging demonstrate the potential of functional near-infrared spectroscopy (fNIRS) for use in brain-computer interfaces (BCIs). fNIRS uses light in the near-infrared range to measure brain surface haemoglobin concentrations and thus determine human neural activity. Our primary goal in this study is to analyse brain haemodynamic responses for application in a BCI. Specifically, we develop an efficient signal processing algorithm to extract important mental-task-relevant neural features and obtain the best possible classification performance. We recorded brain haemodynamic responses due to frontal cortex brain activity from nine subjects using a 19-channel fNIRS system. Our algorithm is based on continuous wavelet transforms (CWTs) for multi-scale decomposition and a soft thresholding algorithm for de-noising. We adopted three machine learning algorithms and compared their performance. Good performance can be achieved by using the de-noised wavelet coefficients as input features for the classifier. Moreover, the classifier performance varied depending on the type of mother wavelet used for wavelet decomposition. Our quantitative results showed that CWTs can be used efficiently to extract important brain haemodynamic features at multiple frequencies if an appropriate mother wavelet function is chosen. The best classification results were obtained by a specific combination of input feature type and classifier. © 2012 IPEM.","ANN; Brain-computer interface; Functional near infrared spectroscopy; LDA; Mental task classification; SVM; Wavelet transforms","ANN; Functional near infrared spectroscopy; LDA; Mental task classification; SVM; Brain; Brain computer interface; Interfaces (computer); Learning algorithms; Learning systems; Near infrared spectroscopy; Neurons; Signal processing; Wavelet decomposition; Wavelet transforms; Functional neuroimaging; algorithm; article; brain computer interface; brain cortex; classification; classifier; cognition; continuous wavelet transform; decomposition; frontal cortex; hemodynamics; infrared radiation; machine learning; mathematical analysis; mental task; near infrared spectroscopy; nervous system function; neuroimaging; priority journal; quantitative analysis; signal processing; task performance; Adult; Algorithms; Artificial Intelligence; Cognition; Discriminant Analysis; Female; Frontal Lobe; Hemodynamics; Humans; Male; Neural Networks (Computer); Spectroscopy, Near-Infrared; Support Vector Machines; Wavelet Analysis",Article,Scopus,2-s2.0-84869502428
"Agell N., Sánchez M., Prats F., Roselló L.","Ranking multi-attribute alternatives on the basis of linguistic labels in group decisions",2012,"Information Sciences",30,10.1016/j.ins.2012.05.005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862670872&doi=10.1016%2fj.ins.2012.05.005&partnerID=40&md5=d8f3624b8a995fe5e79cc9730d2afbbc","This paper presents a new approach based upon qualitative reasoning techniques for representing and synthesising the information given by a group of evaluators. A mathematical formulation is developed that contributes to decision-making analysis in the context of multi-attribute and group decision-making. This method, is applied in choice and ranking problems and can work at different precision levels. To represent non-trivial domain knowledge, the patterns or alternatives to be ranked are characterised by a set of features, which are evaluated by each member of the group through linguistic labels corresponding to ordinal values. Different levels of precision are considered to draw the distinctions required by evaluators' reasoning processes. The method used for ranking alternatives is based on comparing distances against an optimal reference point. A theorem on the consistency of the proposed method is presented and proved. Three real-life applications are presented to outline areas of management where the proposed method has been implemented and achieved interesting results. © 2012 Elsevier Inc. All rights reserved.","Goal programming; Group decision-making; Multi-attribute decision-making; Qualitative reasoning; Reference point method; Uncertainty modelling","Goal programming; Group decision-making; Multi attribute decision making; Qualitative reasoning; Reference point method; Uncertainty modelling; Artificial intelligence; Linguistics; Theorem proving; Uncertainty analysis; Decision making",Article,Scopus,2-s2.0-84862670872
"Wang Y., Tran D., Liao Z., Forsyth D.","Discriminative hierarchical part-based models for human parsing and action recognition",2012,"Journal of Machine Learning Research",30,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867855463&partnerID=40&md5=4d48a20aabb4d46599e3b6b30021b4d6","We consider the problem of parsing human poses and recognizing their actions in static images with part-based models. Most previous work in part-based models only considers rigid parts (e.g., torso, head, half limbs) guided by human anatomy. We argue that this representation of parts is not necessarily appropriate. In this paper, we introduce hierarchical poselets-a new representation for modeling the pose configuration of human bodies. Hierarchical poselets can be rigid parts, but they can also be parts that cover large portions of human bodies (e.g., torso + left arm). In the extreme case, they can be the whole bodies. The hierarchical poselets are organized in a hierarchical way via a structured model. Human parsing can be achieved by inferring the optimal labeling of this hierarchical model. The pose information captured by this hierarchical model can also be used as a intermediate representation for other high-level tasks. We demonstrate it in action recognition from static images. © 2012 Yang Wang, Duan Tran, Zicheng Liao and David Forsyth.","Action recognition; Hierarchical poselets; Human parsing; Maxmargin structured learning; Part-based models","Action recognition; Hierarchical model; Hierarchical poselets; Human anatomy; Human bodies; Human parsing; Human pose; Intermediate representations; Optimal labeling; Pose information; Static images; Structured learning; Structured model; Whole body; Artificial intelligence; Software engineering; Hierarchical systems",Article,Scopus,2-s2.0-84867855463
"Lalla-Ruiz E., Melián-Batista B., Marcos Moreno-Vega J.","Artificial intelligence hybrid heuristic based on tabu search for the dynamic berth allocation problem",2012,"Engineering Applications of Artificial Intelligence",30,10.1016/j.engappai.2012.06.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864763674&doi=10.1016%2fj.engappai.2012.06.001&partnerID=40&md5=756678cc038083cc0d7f2d762055c605","This paper considers the Dynamic Berth Allocation Problem, in which vessels are assigned to discrete positions in berths. This problem, whose goal is to minimize the total time the vessels stay at the port, constitutes one of the most important processes at any containers terminal. We propose a hybrid metaheuristic that combines Tabu Search with Path Relinking, T2 SPR. The results reached by this hybrid algorithm are compared with the optimal values given by the best mathematical model that appears in the literature for this problem, GSPP, and with a tabu search algorithm from the literature, T 2S. For small instances, the algorithm T2SPR is able to obtain most of the optimal solutions in an amount of computational time that is lower than the time required to solve the GSPP model. For medium and large size instances, GSPP cannot be solved to optimality, whereas the proposed hybrid algorithm outperforms T 2S. Moreover, the computational experiments carried out in this paper confirm the robustness of the proposed algorithm with respect to both the parameters governing the procedure and the problem size. © 2012 Elsevier Ltd. All rights reserved.","Artificial intelligence; Berth allocation; Path relinking; Port logistics; Tabu search","Berth allocation; Computational experiment; Computational time; Dynamic berth allocation; Hybrid algorithms; Hybrid heuristics; Metaheuristic; Optimal solutions; Optimal values; Optimality; Path relinking; Port logistics; Problem size; Tabu search algorithms; Artificial intelligence; Mathematical models; Optimal systems; Ports and harbors; Tabu search; Transfer cases (vehicles); Algorithms",Article,Scopus,2-s2.0-84864763674
"Gupta O., Willwacher T., Velten A., Veeraraghavan A., Raskar R.","Reconstruction of hidden 3D shapes using diffuse reflections",2012,"Optics Express",30,10.1364/OE.20.019096,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865604655&doi=10.1364%2fOE.20.019096&partnerID=40&md5=e99d33010236033731e20b732d148adc","We analyze multi-bounce propagation of light in an unknown hidden volume and demonstrate that the reflected light contains sufficient information to recover the 3D structure of the hidden scene. We formulate the forward and inverse theory of secondary scattering using ideas from energy front propagation and tomography. We show that using Fresnel approximation greatly simplifies this problem and the inversion can be achieved via a backpropagation process. We study the invertibilityuniqueness and choices of space-time-angle dimensions using synthetic examples. We show that a 2D streak camera can be used to discover and reconstruct hidden geometry. Using a 1D high speed time of flight camerawe show that our method can be used recover 3D shapes of objects ""around the corner"". © 2012 Optical Society of America.",,"Forward scattering; 3-D shape; 3D Structure; Diffuse reflection; Fresnel approximation; Front propagation; Hidden Volumes; Inverse theory; Reflected light; Time of flight; Three dimensional; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; image enhancement; methodology; reproducibility; sensitivity and specificity; three dimensional imaging; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84865604655
"Azamathulla H.M., Zahiri A.","Flow discharge prediction in compound channels using linear genetic programming",2012,"Journal of Hydrology",30,10.1016/j.jhydrol.2012.05.065,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863867661&doi=10.1016%2fj.jhydrol.2012.05.065&partnerID=40&md5=a3a8b666ab44d1e22e99598bfa70c48e","Flow discharge determination in rivers is one of the key elements in mathematical modelling in the design of river engineering projects. Because of the inundation of floodplains and sudden changes in river geometry, flow resistance equations are not applicable for compound channels. Therefore, many approaches have been developed for modification of flow discharge computations. Most of these methods have satisfactory results only in laboratory flumes. Due to the ability to model complex phenomena, the artificial intelligence methods have recently been employed for wide applications in various fields of water engineering. Linear genetic programming (LGP), a branch of artificial intelligence methods, is able to optimise the model structure and its components and to derive an explicit equation based on the variables of the phenomena. In this paper, a precise dimensionless equation has been derived for prediction of flood discharge using LGP. The proposed model was developed using published data compiled for stage-discharge data sets for 394 laboratories, and field of 30 compound channels. The results indicate that the LGP model has a better performance than the existing models. © 2012 Elsevier B.V.","Flooded rivers; Floodplains; Linear genetic programming; Stage-discharge curve","Artificial intelligence methods; Compound channel; Data sets; Explicit equations; Flood discharge; Flood-plains; Flow discharges; Flow resistance equations; Key elements; Linear genetic programming; Model complexes; River engineering; River geometry; Stage-discharge; Sudden change; Water engineering; Artificial intelligence; Flowmeters; Rivers; Stream flow; Genetic programming; artificial intelligence; data set; floodplain; flow regulation; linear programing; numerical model; optimization; river discharge; river engineering; river flow",Article,Scopus,2-s2.0-84863867661
"Quellec G., Lamard M., Abràmoff M.D., Decencière E., Lay B., Erginay A., Cochener B., Cazuguel G.","A multiple-instance learning framework for diabetic retinopathy screening",2012,"Medical Image Analysis",30,10.1016/j.media.2012.06.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866110731&doi=10.1016%2fj.media.2012.06.003&partnerID=40&md5=20b2c47c97fcacdb9ffc6767b6b1c925","A novel multiple-instance learning framework, for automated image classification, is presented in this paper. Given reference images marked by clinicians as relevant or irrelevant, the image classifier is trained to detect patterns, of arbitrary size, that only appear in relevant images. After training, similar patterns are sought in new images in order to classify them as either relevant or irrelevant images. Therefore, no manual segmentations are required. As a consequence, large image datasets are available for training. The proposed framework was applied to diabetic retinopathy screening in 2-D retinal image datasets: Messidor (1200 images) and e-ophtha, a dataset of 25,702 examination records from the Ophdiat screening network (107,799 images). In this application, an image (or an examination record) is relevant if the patient should be referred to an ophthalmologist. Trained on one half of Messidor, the classifier achieved high performance on the other half of Messidor (Az=0.881) and on e-ophtha (Az=0.761). We observed, in a subset of 273 manually segmented images from e-ophtha, that all eight types of diabetic retinopathy lesions are detected. © 2012 Elsevier B.V.","Diabetic retinopathy; Lesion detection; Multiple-instance learning; Pathology screening","Data sets; Diabetic retinopathy; Diabetic retinopathy screening; Image Classifiers; Large images; Lesion detection; Manual segmentation; Multiple-instance learning; Reference image; Retinal image; Segmented images; Similar pattern; Computer vision; Medical imaging; Eye protection; article; clinical feature; data base; diabetic retinopathy; eye photography; human; image analysis; image processing; medical record; multiple instance learning framework; ophthalmologist; patient referral; physician; priority journal; retinography; scoring system; Algorithms; Artificial Intelligence; Diabetic Retinopathy; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Mass Screening; Pattern Recognition, Automated; Reproducibility of Results; Retinoscopy; Sensitivity and Specificity",Article,Scopus,2-s2.0-84866110731
"Hendler J., Holm J., Musialek C., Thomas G.","US government linked open data: Semantic.data.gov",2012,"IEEE Intelligent Systems",30,10.1109/MIS.2012.27,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863893576&doi=10.1109%2fMIS.2012.27&partnerID=40&md5=389acd0f651f76a3a154c450337bb519","This article discusses Data.gov, the world's largest open government, data-sharing website, and the use of linked data in some of the site's community pages. © 2011 IEEE.","data.gov; linked data; open data; open government; Semantic Web","data.gov; Linked datum; open data; open government; US government; Artificial intelligence; Intelligent systems; Semantic Web; Data handling",Article,Scopus,2-s2.0-84863893576
"Cowling P.I., Powley E.J., Whitehouse D.","Information set Monte Carlo tree search",2012,"IEEE Transactions on Computational Intelligence and AI in Games",30,10.1109/TCIAIG.2012.2200894,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862586602&doi=10.1109%2fTCIAIG.2012.2200894&partnerID=40&md5=f40393b4f420357050a46ea40e901ba9","Monte Carlo tree search (MCTS) is an AI technique that has been successfully applied to many deterministic games of perfect information. This paper investigates the application of MCTS methods to games with hidden information and uncertainty. In particular, three new information set MCTS (ISMCTS) algorithms are presented which handle different sources of hidden information and uncertainty in games. Instead of searching minimax trees of game states, the ISMCTS algorithms search trees of information sets, more directly analyzing the true structure of the game. These algorithms are tested in three domains with different characteristics, and it is demonstrated that our new algorithms outperform existing approaches to handling hidden information and uncertainty in games. © 2012 IEEE.","Artificial intelligence (AI); game tree search; hidden information; Monte Carlo methods; Monte Carlo tree search (MCTS); uncertainty","AI techniques; Game tree search; Hidden information; Information set; Minimax; MONTE CARLO; Search trees; Tree search; uncertainty; Algorithms; Artificial intelligence; Monte Carlo methods; Forestry; Algorithms; Artificial Intelligence; Information Retrieval; Optimization",Article,Scopus,2-s2.0-84862586602
"Wibowo S., Deng H.","Intelligent decision support for effectively evaluating and selecting ships under uncertainty in marine transportation",2012,"Expert Systems with Applications",30,10.1016/j.eswa.2012.01.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857651835&doi=10.1016%2fj.eswa.2012.01.003&partnerID=40&md5=9e059ab5bcb1c5334d17519ac81cb3bc","This paper presents an intelligent decision support system for evaluating and selecting specific ships under uncertainty. A task-oriented procedure is developed for determining the relative importance of the evaluation and selection criteria with respect to a specific shipping task. A fuzzy multicriteria analysis algorithm is developed for determining the overall performance of each ship across all the selection criteria and their associated sub-criteria. An intelligent decision support system capable of integrating the developments above is proposed for facilitating the ship evaluation and selection process. An example is presented to demonstrate the effectiveness of the proposed intelligent decision support system. © 2011 Elsevier Ltd. All rights reserved.","Decision making; Intelligent decision support; Multicriteria analysis; Task-oriented weighting; Uncertainty","Fuzzy Multicriteria analysis; Intelligent decision support; Intelligent decision support systems; Marine transportation; Multi Criteria Analysis; Relative importance; Selection criteria; Selection process; Task-oriented weighting; Uncertainty; Artificial intelligence; Decision making; Ships; Decision support systems",Article,Scopus,2-s2.0-84857651835
"Smialowski P., Doose G., Torkler P., Kaufmann S., Frishman D.","PROSO II - A new method for protein solubility prediction",2012,"FEBS Journal",30,10.1111/j.1742-4658.2012.08603.x,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861888345&doi=10.1111%2fj.1742-4658.2012.08603.x&partnerID=40&md5=7db0dec92a7af740c377ff777c1e3140","Many fields of science and industry depend on efficient production of active protein using heterologous expression in Escherichia coli. The solubility of proteins upon expression is dependent on their amino acid sequence. Prediction of solubility from sequence is therefore highly valuable. We present a novel machine-learning-based model called PROSO II which makes use of new classification methods and growth in experimental data to improve coverage and accuracy of solubility predictions. The classification algorithm is organized as a two-layered structure in which the output of a primary Parzen window model for sequence similarity and a logistic regression classifier of amino acid k-mer composition serve as input for a second-level logistic regression classifier. Compared with previously published research our model is trained on five times more data than used by any other method before (82 000 proteins). When tested on a separate holdout set not used at any point of method development our server attained the best results in comparison with other currently available methods: accuracy 75.4%, Matthew's correlation coefficient 0.39, sensitivity 0.731, specificity 0.759, gain (soluble) 2.263. In summary, due to utilization of cutting edge machine learning technologies combined with the largest currently available experimental data set the PROSO II server constitutes a substantial improvement in protein solubility predictions. PROSO II is available at http://mips.helmholtz-muenchen.de/prosoII. © 2012 FEBS.","classification; feature selection; machine learning; prediction; protein solubility","amino acid; Escherichia coli protein; accuracy; algorithm; amino acid sequence; amino terminal sequence; analytic method; article; carboxy terminal sequence; controlled study; Escherichia coli; hydrophilicity; in vitro study; nonhuman; prediction; priority journal; PROSO II; protein analysis; protein expression; protein structure; sensitivity and specificity; solubility; Artificial Intelligence; Proteins; Solubility; Escherichia coli",Article,Scopus,2-s2.0-84861888345
"Maggi F.M., Westergaard M., Montali M., Van Der Aalst W.M.P.","Runtime verification of LTL-based declarative process models",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",30,10.1007/978-3-642-29860-8_11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861217384&doi=10.1007%2f978-3-642-29860-8_11&partnerID=40&md5=05a74d6a2f510296485ec423eee85408","Linear Temporal Logic (LTL) on finite traces has proven to be a good basis for the analysis and enactment of flexible constraint-based business processes. The Declare language and system benefit from this basis. Moreover, LTL-based languages like Declare can also be used for runtime verification. As there are often many interacting constraints, it is important to keep track of individual constraints and combinations of potentially conflicting constraints. In this paper, we operationalize the notion of conflicting constraints and demonstrate how innovative automata-based techniques can be applied to monitor running process instances. Conflicting constraints are detected immediately and our toolset (realized using Declare and ProM) provides meaningful diagnostics. © 2012 Springer-Verlag.","Declarative Business Processes; Finite State Automata; Linear Temporal Logic; Monitoring; Operational Support; Process Mining","Business Process; Constraint-based; Finite traces; Linear temporal logic; Process mining; Process model; Run-time verification; Running process; Toolsets; Artificial intelligence; Finite automata; Monitoring; Temporal logic",Conference Paper,Scopus,2-s2.0-84861217384
"Armstrong J., Green R.J., Higgins M.D.","Comparison of three receiver designs for optical wireless communications using white LEDs",2012,"IEEE Communications Letters",30,10.1109/LCOMM.2012.031912.112206,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861202024&doi=10.1109%2fLCOMM.2012.031912.112206&partnerID=40&md5=b329fbde1901cf3254eda7afa6a5adbe","Three visible light optical wireless communication systems using asymmetrically clipped optical orthogonal frequency division multiplexing (ACO-OFDM) and white light emitting diodes (LEDs) are compared. The LEDs considered have a larger modulation bandwidth for the blue optical frequencies than for the rest of the transmitted optical spectrum. It is shown that for typical parameter values, a novel diversity combining receiver has slightly greater capacity than a system optimized for reception of blue light only, and that both have greater capacity than a system designed to receive the entire visible light spectrum. © 2012 IEEE.","ACO-OFDM; Diversity reception; OFDM; Optical wireless; Visible light communications","ACO-OFDM; Blue light; Diversity combining; Modulation bandwidth; Optical frequency; Optical orthogonal frequency division multiplexing; Optical spectra; Optical wireless; Optical wireless communication systems; Optical wireless communications; Parameter values; Receiver design; Visible light; Visible light communications; White LED; White light emitting diodes; Artificial intelligence; Diversity reception; Optical communication; Orthogonal frequency division multiplexing; Wireless telecommunication systems; Light emitting diodes",Article,Scopus,2-s2.0-84861202024
"Dodis Y., Kiltz E., Pietrzak K., Wichs D.","Message authentication, revisited",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",30,10.1007/978-3-642-29011-4_22,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859941399&doi=10.1007%2f978-3-642-29011-4_22&partnerID=40&md5=12e07794fbdc160e9472477f2f9387f7","Traditionally, symmetric-key message authentication codes (MACs) are easily built from pseudorandom functions (PRFs). In this work we propose a wide variety of other approaches to building efficient MACs, without going through a PRF first. In particular, unlike deterministic PRF-based MACs, where each message has a unique valid tag, we give a number of probabilistic MAC constructions from various other primitives/assumptions. Our main results are summarized as follows: We show several new probabilistic MAC constructions from a variety of general assumptions, including CCA-secure encryption, Hash Proof Systems and key-homomorphic weak PRFs. By instantiating these frameworks under concrete number theoretic assumptions, we get several schemes which are more efficient than just using a state-of-the-art PRF instantiation under the corresponding assumption. For probabilistic MACs, unlike deterministic ones, unforgeability against a chosen message attack (uf-cma ) alone does not imply security if the adversary can additionally make verification queries (uf-cmva ). We give an efficient generic transformation from any uf-cma secure MAC which is ""message-hiding"" into a uf-cmva secure MAC. This resolves the main open problem of Kiltz et al. from Eurocrypt'11; By using our transformation on their constructions, we get the first efficient MACs from the LPN assumption. While all our new MAC constructions immediately give efficient actively secure, two-round symmetric-key identification schemes, we also show a very simple, three-round actively secure identification protocol from any weak PRF. In particular, the resulting protocol is much more efficient than the trivial approach of building a regular PRF from a weak PRF. © 2012 International Association for Cryptologic Research.",,"Chosen message attacks; Generic transformations; Identification protocol; Identification scheme; Message authentication; Message authentication codes; Open problems; Proof system; Pseudo-random functions; Symmetric keys; Unforgeability; Chosen message attacks; Generic transformations; Hash Proof Systems; Identification protocol; Message authentication; Message hiding; Pseudo-random functions; State of the art; Artificial intelligence; Authentication; Electronic document identification systems; Cryptography; Cryptography",Conference Paper,Scopus,2-s2.0-84859941399
"Bellare M., Dowsley R., Waters B., Yilek S.","Standard security does not imply security against selective-opening",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",30,10.1007/978-3-642-29011-4_38,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859985296&doi=10.1007%2f978-3-642-29011-4_38&partnerID=40&md5=00c5f10ffd95dd3e0eb8c53b8ca0c35e","We show that no commitment scheme that is hiding and binding according to the standard definition is semantically-secure under selective opening attack (SOA), resolving a long-standing and fundamental open question about the power of SOAs. We also obtain the first examples of IND-CPA encryption schemes that are not secure under SOA, both for sender corruptions where encryption coins are revealed and receiver corruptions where decryption keys are revealed. These results assume only the existence of collision-resistant hash functions. © 2012 International Association for Cryptologic Research.",,"Collision-resistant hash functions; Commitment scheme; Decryption keys; Encryption schemes; Standard definitions; Collision-resistant hash functions; Commitment scheme; Decryption keys; Encryption schemes; Selective opening attacks; Standard definitions; Artificial intelligence; Hash functions; Hash functions; Cryptography",Conference Paper,Scopus,2-s2.0-84859985296
"Bunkhumpornpat C., Sinapiromsaran K., Lursinsap C.","DBSMOTE: Density-based synthetic minority over-sampling technique",2012,"Applied Intelligence",30,10.1007/s10489-011-0287-y,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862140885&doi=10.1007%2fs10489-011-0287-y&partnerID=40&md5=c2f49bc9b0585db9b9ad6698608d26a0","A dataset exhibits the class imbalance problem when a target class has a very small number of instances relative to other classes. A trivial classifier typically fails to detect a minority class due to its extremely low incidence rate. In this paper, a new over-sampling technique called DBSMOTE is proposed. Our technique relies on a density-based notion of clusters and is designed to over-sample an arbitrarily shaped cluster discovered by DB-SCAN. DBSMOTE generates synthetic instances along a shortest path from each positive instance to a pseudo-centroid of a minority-class cluster. Consequently, these synthetic instances are dense near this centroid and are sparse far from this centroid. Our experimental results show that DBSMOTE improves precision, F-value, and AUC more effectively than SMOTE, Borderline-SMOTE, and Safe-Level-SMOTE for imbalanced datasets. © 2011 Springer-Verlag.","Class imbalance; Classification; Density-based; Over-sampling","Class imbalance; Class imbalance problems; Data sets; Density-based; Imbalanced Data-sets; Incidence rate; Over sampling; Shortest path; Synthetic minority over-sampling techniques; Target class; Classification (of information); Artificial intelligence",Article,Scopus,2-s2.0-84862140885
"Kentzoglanakis K., Poole M.","A swarm intelligence framework for reconstructing gene networks: Searching for biologically plausible architectures",2012,"IEEE/ACM Transactions on Computational Biology and Bioinformatics",30,10.1109/TCBB.2011.87,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856509277&doi=10.1109%2fTCBB.2011.87&partnerID=40&md5=c8c6fb7344cf7a1d8aeec7a894cd719b","In this paper, we investigate the problem of reverse engineering the topology of gene regulatory networks from temporal gene expression data. We adopt a computational intelligence approach comprising swarm intelligence techniques, namely particle swarm optimization (PSO) and ant colony optimization (ACO). In addition, the recurrent neural network (RNN) formalism is employed for modeling the dynamical behavior of gene regulatory systems. More specifically, ACO is used for searching the discrete space of network architectures and PSO for searching the corresponding continuous space of RNN model parameters. We propose a novel solution construction process in the context of ACO for generating biologically plausible candidate architectures. The objective is to concentrate the search effort into areas of the structure space that contain architectures which are feasible in terms of their topological resemblance to real-world networks. The proposed framework is initially applied to the reconstruction of a small artificial network that has previously been studied in the context of gene network reverse engineering. Subsequently, we consider an artificial data set with added noise for reconstructing a subnetwork of the genetic interaction network of S. cerevisiae (yeast). Finally, the framework is applied to a real-world data set for reverse engineering the SOS response system of the bacterium Escherichia coli. Results demonstrate the relative advantage of utilizing problem-specific knowledge regarding biologically plausible structural properties of gene networks over conducting a problem-agnostic search in the vast space of network architectures. © 2006 IEEE.","ant colony optimization; degree distribution; Gene regulatory networks; network inference; particle swarm optimization; recurrent neural networks; swarm intelligence","Ant-colony optimization; Degree distributions; Gene regulatory networks; network inference; Particle swarm; Swarm Intelligence; Algorithms; Artificial intelligence; Cellular automata; Escherichia coli; Gene expression; Mathematical models; Particle swarm optimization (PSO); Recurrent neural networks; Reverse engineering; Topology; Virtual reality; Network architecture; Bacteria (microorganisms); Escherichia coli; Saccharomyces cerevisiae; algorithm; biological model; biology; Escherichia coli; gene expression; gene expression profiling; gene regulatory network; genetics; procedures; Saccharomyces cerevisiae; article; biology; gene regulatory network; methodology; Algorithms; Computational Biology; Escherichia coli; Gene Expression; Gene Expression Profiling; Gene Regulatory Networks; Models, Genetic; Saccharomyces cerevisiae; Algorithms; Computational Biology; Escherichia coli; Gene Expression; Gene Expression Profiling; Gene Regulatory Networks; Models, Genetic; Saccharomyces cerevisiae",Article,Scopus,2-s2.0-84856509277
"Arnold S.E., Xie S.X., Leung Y.-Y., Wang L.-S., Kling M.A., Han X., Kim E.J., Wolk D.A., Bennett D.A., Chen-Plotkin A., Grossman M., Hu W., Lee V.M.-Y., MacKin R.S., Trojanowski J.Q., Wilson R.S., Shaw L.M.","Plasma biomarkers of depressive symptoms in older adults",2012,"Translational Psychiatry",30,10.1038/tp.2011.63,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863018225&doi=10.1038%2ftp.2011.63&partnerID=40&md5=451d458e97806f356b8029decf78afdf","The pathophysiology of negative affect states in older adults is complex, and a host of central nervous system and peripheral systemic mechanisms may play primary or contributing roles. We conducted an unbiased analysis of 146 plasma analytes in a multiplex biochemical biomarker study in relation to number of depressive symptoms endorsed by 566 participants in the Alzheimer's Disease Neuroimaging Initiative (ADNI) at their baseline and 1-year assessments. Analytes that were most highly associated with depressive symptoms included hepatocyte growth factor, insulin polypeptides, pregnancy-associated plasma protein-A and vascular endothelial growth factor. Separate regression models assessed contributions of past history of psychiatric illness, antidepressant or other psychotropic medicine, apolipoprotein E genotype, body mass index, serum glucose and cerebrospinal fluid (CSF) τand amyloid levels, and none of these values significantly attenuated the main effects of the candidate analyte levels for depressive symptoms score. Ensemble machine learning with Random Forests found good accuracy (∼80%) in classifying groups with and without depressive symptoms. These data begin to identify biochemical biomarkers of depressive symptoms in older adults that may be useful in investigations of pathophysiological mechanisms of depression in aging and neurodegenerative dementias and as targets of novel treatment approaches. © 2012 Macmillan Publishers Limited. All rights reserved.","Alzheimer's disease neuroimaging initiative; biochemical biomarker; geriatric depression; mild cognitive impairment","biological marker; HGF protein, human; insulin; peptide fragment; pregnancy associated plasma protein A; scatter factor; vasculotropin A; VEGFA protein, human; aged; Alzheimer disease; article; artificial intelligence; blood; depression; female; follow up; human; male; metabolism; middle aged; reference value; statistics; Aged; Aged, 80 and over; Alzheimer Disease; Artificial Intelligence; Biological Markers; Depressive Disorder; Female; Follow-Up Studies; Hepatocyte Growth Factor; Humans; Insulin; Male; Middle Aged; Peptide Fragments; Pregnancy-Associated Plasma Protein-A; Reference Values; Statistics as Topic; Vascular Endothelial Growth Factor A",Article,Scopus,2-s2.0-84863018225
"Chen Y., Hsu C.-Y., Liu L., Yang S.","Constructing a nutrition diagnosis expert system",2012,"Expert Systems with Applications",30,10.1016/j.eswa.2011.07.069,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054933612&doi=10.1016%2fj.eswa.2011.07.069&partnerID=40&md5=8ebdf9516f4b2bedd6f028be3fffb970","This paper presents a research of constructing a web-based expert system for nutrition diagnosis by utilizing the expert system techniques in artificial intelligence. The research implements Nutritional Care Process and Model (NCPM) defined by American Dietetic Association (ADA) in 2008 and integrate the nutrition diagnosis knowledge from dietetics professionals to establish the basics of building the rule-based expert system with its knowledge base. The system is built using Microsoft Visual Studio 2008 on.NET Framework 3.5SP1 utilizing the built in rule engine which comes with Windows Workflow Foundation. With the help of this system, it is easier for dietetics professionals to adapt to the newly introduced concept of nutrition diagnosis. At the heart of the web based expert system is a knowledge base, it has a rule engine which contains the nutrition diagnosis rules converted from signs and symptoms for nutrition diagnosis from dietetics professionals and are expressed in XML format which are then stored in a SQL database. A knowledge engineer will be able to use a rule editor to add new rules or to update existing rules within the rule database. Dietetics professionals would be able to enter patient's basic data, anthropometric data, physical exam findings, biochemical data, and food/nutrition history into the program. After dietetics professionals complete nutrition assessment, the program will make inference to the rule base and make nutrition diagnosis. Dietetics professionals could then make the final diagnosis decision for the patient based on the diagnosis report generated by the web based nutrition diagnosis expert system. For this study, I have selected 100 chronic kidney disease patients under hemodialysis from a university hospital, recorded their albumin, cholesterol, creatinine before dialysis, height, and dry weight and then use these data to perform nutrition diagnosis with both the expert system and a practicing dietitian. After comparing the result, I found that the expert system is faster and more accurate than human dietitian. © 2011 Elsevier Ltd. All rights reserved.","ASP.NET; Expert system; Nutrition diagnosis system; Rule-based","Anthropometric data; ASP.NEt; Biochemical data; Care process; Chronic kidney disease; Diagnosis decision; Dry weight; Knowledge base; Knowledge Engineers; MicroSoft; Nutrition diagnosis; Nutrition diagnosis system; Rule base; Rule based; Rule database; Rule engine; Rule-based expert system; SQL database; Visual studios; Web based; Web based expert system; Windows Workflow Foundation; XML format; Anthropometry; Artificial intelligence; Dialysis; Expert systems; Java programming language; Nutrition; Program diagnostics; User interfaces; World Wide Web; Diagnosis",Conference Paper,Scopus,2-s2.0-80054933612
"Romero N., Xu N., Nozick L.K., Dobson I., Jones D.","Investment planning for electric power systems under terrorist threat",2012,"IEEE Transactions on Power Systems",30,10.1109/TPWRS.2011.2159138,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856266489&doi=10.1109%2fTPWRS.2011.2159138&partnerID=40&md5=b22c97633babfca92a454ab6ee669036","Access to electric power is critical to societal welfare. In this paper, we analyze the interaction between a defender and a terrorist who threatens the operation of an electric power system. The defender wants to find a strategic defense to minimize the consequences of an attack. Both parties have limited budgets and behave in their own self-interest. The problem is formulated as a multi-level mixed-integer programming problem. A Tabu Search with an embedded greedy algorithm for the attack problem is implemented to find the optimum defense strategy. We apply the algorithm to a 24-bus network for a combination of four different defense budgets, three attack budgets, and three assumptions as to how the terrorists craft their attacks. © 2006 IEEE.","Decision support system; game theory; load flow analysis; power system security; systems engineering","Defense budgets; Defense strategy; Electric power; Greedy algorithms; Investment planning; load flow analysis; Mixed-Integer Programming; Multi-level; power system security; Terrorist threats; Algorithms; Artificial intelligence; Decision support systems; Electric power systems; Electricity; Game theory; Integer programming; Investments; Power transmission; Systems engineering; Tabu search; Terrorism; Budget control",Article,Scopus,2-s2.0-84856266489
"Frank M., Streich A.P., Basin D., Buhmann J.M.","Multi-assignment clustering for Boolean data",2012,"Journal of Machine Learning Research",30,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857856090&partnerID=40&md5=9367d004f5fe5c24631f0abe2de5e9c5","We propose a probabilistic model for clustering Boolean data where an object can be simultaneously assigned to multiple clusters. By explicitly modeling the underlying generative process that combines the individual source emissions, highly structured data are expressed with substantially fewer clusters compared to single-assignment clustering. As a consequence, such a model provides robust parameter estimators even when the number of samples is low. We extend the model with different noise processes and demonstrate that maximum-likelihood estimation with multiple assignments consistently infers source parameters more accurately than single-assignment clustering. Our model is primarily motivated by the task of role mining for role-based access control, where users of a system are assigned one or more roles. In experiments with real-world access-control data, our model exhibits better generalization performance than state-of-the-art approaches. © 2012 Mario Frank, Andreas P. Streich, David Basin and Joachim M. Buhmann.","Boolean data; Clustering; Latent feature models; Multi-assignments; Overlapping clusters; Role mining","Boolean data; Clustering; Feature models; Multi-assignments; Overlapping clusters; Artificial intelligence; Software engineering; Access control",Article,Scopus,2-s2.0-84857856090
"Karasulu B., Korukoglu S.","Moving object detection and tracking by using annealed background subtraction method in videos: Performance optimization",2012,"Expert Systems with Applications",30,10.1016/j.eswa.2011.06.040,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-81855170092&doi=10.1016%2fj.eswa.2011.06.040&partnerID=40&md5=34355a34d9f2e98b69a6fbb6dc612c98","In computer vision, moving object detection and tracking methods are the most important preliminary steps for higher-level video analysis applications. In this frame, background subtraction (BS) method is a well-known method in video processing and it is based on frame differencing. The basic idea is to subtract the current frame from a background image and to classify each pixel either as foreground or background by comparing the difference with a threshold. Therefore, the moving object is detected and tracked by using frame differencing and by learning an updated background model. In addition, simulated annealing (SA) is an optimization technique for soft computing in the artificial intelligence area. The p-median problem is a basic model of discrete location theory of operational research (OR) area. It is a NP-hard combinatorial optimization problem. The main aim in the p-median problem is to find p number facility locations, minimize the total weighted distance between demand points (nodes) and the closest facilities to demand points. The SA method is used to solve the p-median problem as a probabilistic metaheuristic. In this paper, an SA-based hybrid method called entropy-based SA (EbSA) is developed for performance optimization of BS, which is used to detect and track object(s) in videos. The SA modification to the BS method (SA-BS) is proposed in this study to determine the optimal threshold for the foreground-background (i.e.; bi-level) segmentation and to learn background model for object detection. At these segmentation and learning stages, all of the optimization problems considered in this study are taken as p-median problems. Performances of SA-BS and regular BS methods are measured using four videoclips. Therefore, these results are evaluated quantitatively as the overall results of the given method. The obtained performance results and statistical analysis (i.e.; Wilcoxon median test) show that our proposed method is more preferable than regular BS method. Meanwhile, the contribution of this study is discussed. © 2011 Elsevier Ltd. All rights reserved.","Computer vision; Performance optimization; Simulated annealing; Soft computing","Background image; Background model; Background subtraction; Background subtraction method; Basic models; Combinatorial optimization problems; Current frame; Discrete location; Facility locations; Frame differencing; Hybrid method; Metaheuristic; Moving object detection and tracking; Moving objects; NP-hard; Object Detection; Operational research; Optimal threshold; Optimization problems; Optimization techniques; P-median problems; Performance optimizations; Video analysis; Video processing; Video-clips; Weighted distance; Artificial intelligence; Combinatorial optimization; Computer vision; Location; Models; Object recognition; Soft computing; Simulated annealing",Article,Scopus,2-s2.0-81855170092
"Liao X., Zhou J., Zhang R., Zhang Y.","An adaptive artificial bee colony algorithm for long-term economic dispatch in cascaded hydropower systems",2012,"International Journal of Electrical Power and Energy Systems",29,10.1016/j.ijepes.2012.04.009,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864415427&doi=10.1016%2fj.ijepes.2012.04.009&partnerID=40&md5=1542819ce0bb4190fa0d9dc2213f5e3c","In this paper, we present a novel adaptive artificial bee colony (AABC) algorithm and compare its efficiency with other existing algorithms for long-term dispatch of cascaded hydropower systems. We formulate the long-term economic dispatch of hydropower systems as a complicated nonlinear optimization problem with a group of complex constraints. We analyze the performances of three different values of the control parameter modification rate (MR) in the AABC. We modify the employed bee phase to improve the global optimal capability of the AABC algorithm, and utilize a novel probabilistic method to enhance the search ability of the onlooker bee phase. Furthermore, we change the scout bee phase to avoid local maxima. We demonstrate the performance of the AABC algorithm and compare it with other algorithms using the data from hydropower systems of Three Gorges in China. © 2012 Elsevier Ltd. All rights reserved.","Adaptive; Artificial bee colony algorithm; Cascaded hydropower system; Constrained optimization; Economic dispatch; Swarm intelligence","Adaptive; Artificial bee colony algorithms; Cascaded hydro-power system; Economic Dispatch; Swarm Intelligence; Artificial intelligence; Constrained optimization; Evolutionary algorithms; Scheduling; Hydroelectric power",Article,Scopus,2-s2.0-84864415427
"Cambria E., Olsher D., Kwok K.","Sentic activation: A two-level affective common sense reasoning framework",2012,"Proceedings of the National Conference on Artificial Intelligence",29,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868296994&partnerID=40&md5=319976c75734e8ccd48659d28f56c243","An important difference between traditional AI systems and human intelligence is our ability to harness common sense knowledge gleaned from a lifetime of learning and experiences to inform our decision making and behavior. This allows humans to adapt easily to novel situations where AI fails catastrophically for lack of situation-specific rules and generalization capabilities. Common sense knowledge also provides the background knowledge for humans to successfully operate in social situations where such knowledge is typically assumed. In order for machines to exploit common sense knowledge in reasoning as humans do, moreover, we need to endow them with human-like reasoning strategies. In this work, we propose a two-level affective reasoning framework that concurrently employs multi-dimensionality reduction and graph mining techniques to mimic the integration of conscious and unconscious reasoning, and exploit it for sentiment analysis. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"AI systems; Background knowledge; Commonsense knowledge; Commonsense reasoning; Generalization capability; Graph mining; Human intelligence; Multidimensionality; Reasoning framework; Sentiment analysis; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868296994
"Siva P., Russell C., Xiang T.","In defence of negative mining for annotating weakly labelled data",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",29,10.1007/978-3-642-33712-3_43,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867866572&doi=10.1007%2f978-3-642-33712-3_43&partnerID=40&md5=cdf098b057d8d85df8826efdef896146","We propose a novel approach to annotating weakly labelled data. In contrast to many existing approaches that perform annotation by seeking clusters of self-similar exemplars (minimising intra-class variance), we perform image annotation by selecting exemplars that have never occurred before in the much larger, and strongly annotated, negative training set (maximising inter-class variance). Compared to existing methods, our approach is fast, robust, and obtains state of the art results on two challenging data-sets - voc2007 (all poses), and the msr2 action data-set, where we obtain a 10% increase. Moreover, this use of negative mining complements existing methods, that seek to minimize the intra-class variance, and can be readily integrated with many of them. © 2012 Springer-Verlag.","Automatic Annotation; Multiple-Instance Learning; Negative Mining; Weakly Supervised Learning","Automatic annotation; Data sets; Image annotation; Multiple-instance learning; Self-similar; State of the art; Training sets; Weakly supervised learning; Artificial intelligence; Computer vision",Conference Paper,Scopus,2-s2.0-84867866572
"Ibragimov B., Likar B., Pernuš F., Vrtovec T.","A game-theoretic framework for landmark-based image segmentation",2012,"IEEE Transactions on Medical Imaging",29,10.1109/TMI.2012.2202915,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867496657&doi=10.1109%2fTMI.2012.2202915&partnerID=40&md5=fcebad995216320c7d4f58814eb8e1d8","A novel game-theoretic framework for landmark- based image segmentation is presented. Landmark detection is formulated as a game, in which landmarks are players, landmark candidate points are strategies, and likelihoods that candidate points represent landmarks are payoffs, determined according to the similarity of image intensities and spatial relationships between the candidate points in the target image and their corresponding landmarks in images from the training set. The solution of the formulated game-theoretic problem is the equilibrium of candidate points that represent landmarks in the target image and is obtained by a novel iterative scheme that solves the segmentation problem in polynomial time. The object boundaries are finally extracted by applying dynamic programming to the optimal path searching problem between the obtained adjacent landmarks. The performance of the proposed framework was evaluated for segmentation of lung fields from chest radiographs and heart ventricles from cardiac magnetic resonance cross sections. The comparison to other landmark-based segmentation techniques shows that the results obtained by the proposed game-theoretic framework are highly accurate and precise in terms of mean boundary distance and area overlap. Moreover, the framework overcomes several shortcomings of the existing techniques, such as sensitivity to initialization and convergence to local optima. © 2012 IEEE.","Game theory; Heart ventricles; Landmark-based segmentation; Lung fields; Supervised segmentation","Cardiac magnetic resonance; Chest radiographs; Game-theoretic; Game-theoretic problems; Image intensities; Iterative schemes; Landmark detection; Landmark-based segmentation; Local optima; Lung fields; Object boundaries; Optimal paths; Polynomial-time; Spatial relationships; Supervised segmentation; Target images; Training sets; Biological organs; Game theory; Iterative methods; Magnetic resonance; Polynomial approximation; Image segmentation; algorithm; article; artificial intelligence; game; heart ventricle; histology; human; image processing; lung; methodology; nuclear magnetic resonance imaging; reproducibility; thorax radiography; Algorithms; Artificial Intelligence; Game Theory; Heart Ventricles; Humans; Image Processing, Computer-Assisted; Lung; Magnetic Resonance Imaging; Radiography, Thoracic; Reproducibility of Results",Article,Scopus,2-s2.0-84867496657
"Damianou A.C., Ek C.H., Titsias M.K., Lawrence N.D.","Manifold relevance determination",2012,"Proceedings of the 29th International Conference on Machine Learning, ICML 2012",29,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867113123&partnerID=40&md5=d92fc4d5b0826c18f26b592b0e14e57c","In this paper we present a fully Bayesian latent variable model which exploits conditional non-linear (in)-dependence structures to learn an efficient latent representation. The latent space is factorized to represent shared and private information from multiple views of the data. In contrast to previous approaches, we introduce a relaxation to the discrete segmentation and allow for a ""softly"" shared latent space. Further, Bayesian techniques allow us to automatically estimate the dimensionality of the latent spaces. The model is capable of capturing structure underlying extremely high dimensional spaces. This is illustrated by modelling unprocessed images with tenths of thousands of pixels. This also allows us to directly generate novel images from the trained model by sampling from the discovered latent spaces. We also demonstrate the model by prediction of human pose in an ambiguous setting. Our Bayesian framework allows us to perform disambiguation in a principled manner by including latent space priors which incorporate the dynamic nature of the data. Copyright 2012 by the author(s)/owner(s).",,"Bayesian frameworks; Bayesian techniques; Dynamic nature; High dimensional spaces; Human pose; Latent variable models; Multiple views; Private information; Artificial intelligence; Software engineering; Learning systems",Conference Paper,Scopus,2-s2.0-84867113123
"Wang G., Hoiem D., Forsyth D.","Learning image similarity from flickr groups using fast kernel machines",2012,"IEEE Transactions on Pattern Analysis and Machine Intelligence",29,10.1109/TPAMI.2012.29,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866644789&doi=10.1109%2fTPAMI.2012.29&partnerID=40&md5=0b84f08a3aa094c00df9675da1fd38f2","Measuring image similarity is a central topic in computer vision. In this paper, we propose to measure image similarity by learning from the online Flickr image groups. We do so by: Choosing 103 Flickr groups, building a one-versus-all multiclass classifier to classify test images into a group, taking the set of responses of the classifiers as features, calculating the distance between feature vectors to measure image similarity. Experimental results on the Corel dataset and the PASCAL VOC 2007 dataset show that our approach performs better on image matching, retrieval, and classification than using conventional visual features. To build our similarity measure, we need one-versus-all classifiers that are accurate and can be trained quickly on very large quantities of data. We adopt an SVM classifier with a histogram intersection kernel. We describe a novel fast training algorithm for this classifier: the Stochastic Intersection Kernel MAchine (SIKMA) training algorithm. This method can produce a kernel classifier that is more accurate than a linear classifier on tens of thousands of examples in minutes. © 2012 IEEE.","image classification; image organization; Image similarity; kernel machines; online learning; stochastic gradient descent","Data sets; Feature vectors; Histogram intersection kernels; Image similarity; Kernel classifiers; Kernel machine; Linear classifiers; Multi-class classifier; Online learning; Similarity measure; Stochastic gradient descent; SVM classifiers; Test images; Training algorithms; Visual feature; Classification (of information); Computer vision; Image analysis; Image classification; Image matching; E-learning; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; image enhancement; image subtraction; methodology; reproducibility; sensitivity and specificity; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-84866644789
"Sahillioǧlu Y., Yemez Y.","Minimum-distortion isometric shape correspondence using em algorithm",2012,"IEEE Transactions on Pattern Analysis and Machine Intelligence",29,10.1109/TPAMI.2012.26,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866696225&doi=10.1109%2fTPAMI.2012.26&partnerID=40&md5=ab8629c14a57145a7e45adb80ae5d94a","We present a purely isometric method that establishes 3D correspondence between two (nearly) isometric shapes. Our method evenly samples high-curvature vertices from the given mesh representations, and then seeks an injective mapping from one vertex set to the other that minimizes the isometric distortion. We formulate the problem of shape correspondence as combinatorial optimization over the domain of all possible mappings, which then reduces in a probabilistic setting to a log-likelihood maximization problem that we solve via the Expectation-Maximization (EM) algorithm. The EM algorithm is initialized in the spectral domain by transforming the sampled vertices via classical Multidimensional Scaling (MDS). Minimization of the isometric distortion, and hence maximization of the log-likelihood function, is then achieved in the original 3D euclidean space, for each iteration of the EM algorithm, in two steps: by first using bipartite perfect matching, and then a greedy optimization algorithm. The optimal mapping obtained at convergence can be one-to-one or many-to-one upon choice. We demonstrate the performance of our method on various isometric (or nearly isometric) pairs of shapes for some of which the ground-truth correspondence is available. © 2012 IEEE.","3D isometric shape correspondence; bipartite perfect matching; EM algorithm; greedy optimization; isometric distortion; multidimensional scaling; spectral embedding","3D isometric shape correspondence; EM algorithms; Greedy optimization; Multi-dimensional scaling; Perfect matchings; spectral embedding; Algorithms; Combinatorial optimization; Mapping; Three dimensional; Three dimensional computer graphics; Iterative methods; algorithm; article; artifact; artificial intelligence; automated pattern recognition; computer assisted diagnosis; image enhancement; image subtraction; information retrieval; methodology; reproducibility; sensitivity and specificity; three dimensional imaging; Algorithms; Artifacts; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Information Storage and Retrieval; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-84866696225
"Son C.-S., Kim Y.-N., Kim H.-S., Park H.-S., Kim M.-S.","Decision-making model for early diagnosis of congestive heart failure using rough set and decision tree approaches",2012,"Journal of Biomedical Informatics",29,10.1016/j.jbi.2012.04.013,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866002275&doi=10.1016%2fj.jbi.2012.04.013&partnerID=40&md5=461d3f12a2ac980476805794324e8638","The accurate diagnosis of heart failure in emergency room patients is quite important, but can also be quite difficult due to our insufficient understanding of the characteristics of heart failure. The purpose of this study is to design a decision-making model that provides critical factors and knowledge associated with congestive heart failure (CHF) using an approach that makes use of rough sets (RSs) and decision trees. Among 72 laboratory findings, it was determined that two subsets (RBC, EOS, Protein, O2SAT, Pro BNP) in an RS-based model, and one subset (Gender, MCHC, Direct bilirubin, and Pro BNP) in a logistic regression (LR)-based model were indispensable factors for differentiating CHF patients from those with dyspnea, and the risk factor Pro BNP was particularly so. To demonstrate the usefulness of the proposed model, we compared the discriminatory power of decision-making models that utilize RS- and LR-based decision models by conducting 10-fold cross-validation. The experimental results showed that the RS-based decision-making model (accuracy: 97.5%, sensitivity: 97.2%, specificity: 97.7%, positive predictive value: 97.2%, negative predictive value: 97.7%, and area under ROC curve: 97.5%) consistently outperformed the LR-based decision-making model (accuracy: 88.7%, sensitivity: 90.1%, specificity: 87.5%, positive predictive value: 85.3%, negative predictive value: 91.7%, and area under ROC curve: 88.8%). In addition, a pairwise comparison of the ROC curves of the two models showed a statistically significant difference (p<. 0.01; 95% CI: 2.63-14.6). © 2012 Elsevier Inc..","Congestive heart failure; Decision tree; Decision-making model; Discernibility matrix and function; Maximum entropy principle; Rough set","Congestive heart failures; Decision making models; Discernibility matrix; Maximum entropy principle; Rough set; Cardiology; Decision trees; Logistics; Rough set theory; Table lookup; Decision making; amino terminal pro brain natriuretic peptide; bilirubin; bilirubin glucuronide; adult; aged; article; bilirubin blood level; cardiovascular risk; congestive heart failure; controlled study; decision making; decision support system; decision tree; diagnostic accuracy; diagnostic test accuracy study; dyspnea; early diagnosis; emergency ward; female; human; major clinical study; male; predictive value; priority journal; receiver operating characteristic; rough set; sensitivity and specificity; validation process; Aged; Aged, 80 and over; Algorithms; Artificial Intelligence; Databases, Factual; Decision Trees; Diagnosis, Computer-Assisted; Entropy; Female; Heart Failure; Humans; Logistic Models; Male; Middle Aged; Reproducibility of Results; ROC Curve; Sensitivity and Specificity",Article,Scopus,2-s2.0-84866002275
"Ter Beek M.H., Mazzanti F., Sulova A.","VMC: A tool for product variability analysis",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",29,10.1007/978-3-642-32759-9_36,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866000858&doi=10.1007%2f978-3-642-32759-9_36&partnerID=40&md5=b7b13c62e3dd3f9533da9fbb3771df4b","We present VMC, a tool for the modeling and analysis of variability in product lines. It accepts a product family specified as a modal transition system, possibly with additional variability constraints, after which it can automatically generate all the family's valid products, visualize the family/products as modal/labeled transition systems, and efficiently model check properties expressed in an action- and state-based branching-time temporal logic over products and families alike. © 2012 Springer-Verlag.",,"Modal Transition Systems; Model check; Modeling and analysis; Product families; Product variability; Product-lines; Transition system; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84866000858
"Eriksson A., Van Den Hengel A.","Efficient computation of robust weighted low-rank matrix approximations using the L 1 Norm",2012,"IEEE Transactions on Pattern Analysis and Machine Intelligence",29,10.1109/TPAMI.2012.116,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865593660&doi=10.1109%2fTPAMI.2012.116&partnerID=40&md5=4b8fbf44e97d7ff8954e83a33adf48ff","The calculation of a low-rank approximation to a matrix is fundamental to many algorithms in computer vision and other fields. One of the primary tools used for calculating such low-rank approximations is the Singular Value Decomposition, but this method is not applicable in the case where there are outliers or missing elements in the data. Unfortunately, this is often the case in practice. We present a method for low-rank matrix approximation which is a generalization of the Wiberg algorithm. Our method calculates the rank-constrained factorization, which minimizes the L 1 norm and does so in the presence of missing data. This is achieved by exploiting the differentiability of linear programs, and results in an algorithm can be efficiently implemented using existing optimization software. We show the results of experiments on synthetic and real data. © 2012 IEEE.","L 1-minimization; Low-rank matrix approximation","Differentiability; Efficient computation; Linear programs; Low rank approximations; Low-rank matrices; Missing data; Optimization software; Synthetic and real data; Computer vision; Matrix algebra; Approximation algorithms; algorithm; animal; article; artificial intelligence; automated pattern recognition; computer simulation; face; histology; human; image processing; methodology; reproducibility; Algorithms; Animals; Artificial Intelligence; Computer Simulation; Face; Humans; Image Processing, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results",Article,Scopus,2-s2.0-84865593660
"Abdi M.J., Hosseini S.M., Rezghi M.","A novel weighted support vector machine based on particle swarm optimization for gene selection and tumor classification",2012,"Computational and Mathematical Methods in Medicine",29,10.1155/2012/320698,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864952538&doi=10.1155%2f2012%2f320698&partnerID=40&md5=f3420eeefaf7db09406c065dade70736","We develop a detection model based on support vector machines (SVMs) and particle swarm optimization (PSO) for gene selection and tumor classification problems. The proposed model consists of two stages: first, the well-known minimum redundancy-maximum relevance (mRMR) method is applied to preselect genes that have the highest relevance with the target class and are maximally dissimilar to each other. Then, PSO is proposed to form a novel weighted SVM (WSVM) to classify samples. In this WSVM, PSO not only discards redundant genes, but also especially takes into account the degree of importance of each gene and assigns diverse weights to the different genes. We also use PSO to find appropriate kernel parameters since the choice of gene weights influences the optimal kernel parameters and vice versa. Experimental results show that the proposed mRMR-PSO-WSVM model achieves highest classification accuracy on two popular leukemia and colon gene expression datasets obtained from DNA microarrays. Therefore, we can conclude that our proposed method is very promising compared to the previously reported results. © 2012 Mohammad Javad Abdi et al.",,"accuracy; algorithm; article; computer program; DNA microarray; gene expression; genetic selection; machine learning; particle swarm optimization; support vector machine; tumor classification; artificial intelligence; biology; colon tumor; gene expression profiling; gene expression regulation; genetic database; genetics; human; leukemia; methodology; neoplasm; pathology; reproducibility; statistical model; support vector machine; Algorithms; Artificial Intelligence; Colonic Neoplasms; Computational Biology; Databases, Genetic; Gene Expression Profiling; Gene Expression Regulation, Leukemic; Gene Expression Regulation, Neoplastic; Humans; Leukemia; Models, Statistical; Neoplasms; Oligonucleotide Array Sequence Analysis; Reproducibility of Results; Support Vector Machines",Article,Scopus,2-s2.0-84864952538
"Brunner C., Fischer A., Luig K., Thies T.","Pairwise support vector machines and their application to large scale problems",2012,"Journal of Machine Learning Research",29,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869189040&partnerID=40&md5=cb1fd79fd5a727ee4ac96afd2e4a937d","Pairwise classification is the task to predict whether the examples a,b of a pair (a,b) belong to the same class or to different classes. In particular, interclass generalization problems can be treated in this way. In pairwise classification, the order of the two input examples should not affect the classification result. To achieve this, particular kernels as well as the use of symmetric training sets in the framework of support vector machines were suggested. The paper discusses both approaches in a general way and establishes a strong connection between them. In addition, an efficient implementation is discussed which allows the training of several millions of pairs. The value of these contributions is confirmed by excellent results on the labeled faces in the wild benchmark. © 2012 Carl Brunner, Andreas Fischer, Klaus Luig and Thorsten Thies.","Interclass generalization; Large scale problems; Pairwise kernels; Pairwise support vector machines","Classification results; Efficient implementation; Interclass generalization; Large-scale problem; Pairwise classification; Pairwise kernels; Training sets; Artificial intelligence; Software engineering; Support vector machines",Article,Scopus,2-s2.0-84869189040
"Mitra J., Kato Z., Martí R., Oliver A., Lladó X., Sidibé D., Ghose S., Vilanova J.C., Comet J., Meriaudeau F.","A spline-based non-linear diffeomorphism for multimodal prostate registration",2012,"Medical Image Analysis",29,10.1016/j.media.2012.04.006,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866118888&doi=10.1016%2fj.media.2012.04.006&partnerID=40&md5=315bca03aa8a30cfac94f7278e147245","This paper presents a novel method for non-rigid registration of transrectal ultrasound and magnetic resonance prostate images based on a non-linear regularized framework of point correspondences obtained from a statistical measure of shape-contexts. The segmented prostate shapes are represented by shape-contexts and the Bhattacharyya distance between the shape representations is used to find the point correspondences between the 2D fixed and moving images. The registration method involves parametric estimation of the non-linear diffeomorphism between the multimodal images and has its basis in solving a set of non-linear equations of thin-plate splines. The solution is obtained as the least-squares solution of an over-determined system of non-linear equations constructed by integrating a set of non-linear functions over the fixed and moving images. However, this may not result in clinically acceptable transformations of the anatomical targets. Therefore, the regularized bending energy of the thin-plate splines along with the localization error of established correspondences should be included in the system of equations. The registration accuracies of the proposed method are evaluated in 20 pairs of prostate mid-gland ultrasound and magnetic resonance images. The results obtained in terms of Dice similarity coefficient show an average of 0.980 ± 0.004, average 95% Hausdorff distance of 1.63 ± 0.48. mm and mean target registration and target localization errors of 1.60 ± 1.17. mm and 0.15 ± 0.12. mm respectively. © 2012 Elsevier B.V.","Multimodal images; Non-linear registration; Prostate biopsy; Regularization; Thin-plate splines","Multi-modal image; Non-linear registration; Prostate biopsy; Regularization; Thin plate spline; Functions; Image segmentation; Linear equations; Magnetic resonance; Magnetic resonance imaging; Pore pressure; Urology; article; image analysis; image reconstruction; imaging system; nuclear magnetic resonance imaging; priority journal; prostate; register; transrectal ultrasonography; Algorithms; Artificial Intelligence; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Male; Nonlinear Dynamics; Pattern Recognition, Automated; Prostatic Neoplasms; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique; Ultrasonography",Article,Scopus,2-s2.0-84866118888
"Kim Y.-M., Lee E.-J., Park H.-S., Choi J.-K., Park H.-S.","Ant colony based self-adaptive energy saving routing for energy efficient Internet",2012,"Computer Networks",29,10.1016/j.comnet.2012.03.024,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862008035&doi=10.1016%2fj.comnet.2012.03.024&partnerID=40&md5=5c5eac6ab62550beb4e55b6040894b69","According to recent research, the current Internet wastes energy due to an un-optimized network design, which does not consider the energy consumption of network elements such as routers and switches. Looking toward energy saving networks, a generalized problem called the energy consumption minimized network (EMN) had been proposed. However, due to the NP-completeness of this problem, it requires a considerable amount of time to obtain the solution, making it practically intractable for large-scale networks. In this paper, we re-formulate the NP-complete EMN problem into a simpler one using a newly defined concept called 'traffic centrality'. We then propose a new ant colony-based self-adaptive energy saving routing scheme, referred to as A-ESR, which exploits the ant colony optimization (ACO) method to make the Internet more energy efficient. The proposed A-ESR algorithm heuristically solves the re-formulated problem without any supervised control by allowing the incoming flows to be autonomously aggregated on specific heavily-loaded links and switching off the other lightly-loaded links. Additionally, the A-ESR algorithm adjusts the energy consumption by tuning the aggregation parameter β, which can dramatically reduce the energy consumption during nighttime hours (at the expense of tolerable network delay performance). Another promising capability of this algorithm is that it provides a high degree of self-organizing capabilities due to the amazing advantages of the swarm intelligence of artificial ants. The simulation results in real IP networks show that the proposed A-ESR algorithm performs better than previous algorithms in terms of its energy efficiency. The results also show that this efficiency can be adjusted by tuning β. © 2012 Elsevier B.V. All rights reserved.","Ant colony optimization; Energy saving routing; Pheromone trails; Traffic centrality","Ant colonies; Ant Colony Optimization (ACO); Ant colony optimization methods; Artificial ant; Energy efficient; Incoming flows; IP networks; Large-scale network; Looking toward; Network delays; Network design; Network element; NP Complete; Np-completeness; Pheromone trails; Routing scheme; Self-adaptive; Self-organizing capability; Swarm Intelligence; Artificial intelligence; Computational complexity; Energy efficiency; Energy utilization; Internet; Algorithms",Article,Scopus,2-s2.0-84862008035
"Zhao N., Li S., Wu Z.","Cognitive radio engine design based on ant colony optimization",2012,"Wireless Personal Communications",29,10.1007/s11277-011-0225-7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861726962&doi=10.1007%2fs11277-011-0225-7&partnerID=40&md5=8c79e447824ec61adc895458ac97853e","In this letter, a mutated ant colony optimization (MACO) cognitive radio engine is proposed, and it is the first time to apply ACO algorithm to this problem. The cognitive radio is a promising technology nowadays to alleviate the apparent scarcity of available radio spectrum, and the cognitive radio engine determines the optimal radio transmission parameters for the system. The cognitive engine problem is usually solved by genetic algorithm (GA), however, the GA converges slowly and its performance can still be improved. Hence, MACO algorithm with excellent performance is applied to the cognitive engine in this letter. Simulation results show that the fitness scores obtained by the MACO engine are much better than the ACO and GA engines in different scenarios. © 2011 Springer Science+Business Media, LLC.","Ant colony optimization; Cognitive radio; Decision engine; Genetic algorithm; Mutation mechanism","ACO algorithms; Ant Colony Optimization (ACO); Cognitive radio engine; Decision engines; Excellent performance; Mutation mechanism; Radio spectra; Transmission parameters; Artificial intelligence; Cognitive radio; Genetic algorithms; Machine design; Radio systems; Radio transmission; Engines",Article,Scopus,2-s2.0-84861726962
"Bi Y.","The impact of diversity on the accuracy of evidential classifier ensembles",2012,"International Journal of Approximate Reasoning",29,10.1016/j.ijar.2011.12.011,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858795085&doi=10.1016%2fj.ijar.2011.12.011&partnerID=40&md5=19b7f1d3f7b9267b91eb74d8fccb677f","Diversity being inherent in classifiers is widely acknowledged as an important issue in constructing successful classifier ensembles. Although many statistics have been employed in measuring diversity among classifiers to ascertain whether it correlates with ensemble performance in the literature, most of these measures are incorporated and explained in a non-evidential context. In this paper, we provide a modelling for formulating classifier outputs as triplet mass functions and a uniform notation for defining diversity measures. We then assess the relationship between diversity obtained by four pairwise and non-pairwise diversity measures and the improvement in accuracy of classifiers combined in different orders by Demspter's rule of combination, Smets' conjunctive rule, the Proportion and Yager's rules in the framework of belief functions. Our experimental results demonstrate that the accuracy of classifiers combined by Dempster's rule is not strongly correlated with the diversity obtained by the four measures, and the correlation between the diversity and the ensemble accuracy made by Proportion and Yager's rules is negative, which is not in favor of the claim that increasing diversity could lead to reduction of generalization error of classifier ensembles. © 2011 Elsevier Inc. All rights reserved.","Belief functions; Diversity; Ensemble learning; Triplet evidence structure","Belief function; Classifier ensembles; Dempster's rule; Demspter's rule; Different order; Diversity; Diversity measure; Ensemble learning; Ensemble performance; Generalization Error; Mass functions; Artificial intelligence; Software engineering; Uncertainty analysis",Article,Scopus,2-s2.0-84858795085
"Song X., Xin X., Huang W.","Exponential stability of delayed and impulsive cellular neural networks with partially Lipschitz continuous activation functions",2012,"Neural Networks",29,10.1016/j.neunet.2012.01.006,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862801064&doi=10.1016%2fj.neunet.2012.01.006&partnerID=40&md5=f59f1c5338db0a3b86ce11ec885b6980","The paper discusses exponential stability of distributed delayed and impulsive cellular neural networks with partially Lipschitz continuous activation functions. By relative nonlinear measure method, some novel criteria are obtained for the uniqueness and exponential stability of the equilibrium point. Our method abandons usual assumptions on global Lipschitz continuity, boundedness and monotonicity of activation functions. Our results are generalization and improvement of some existing ones. Finally, two examples and their simulations are presented to illustrate the correctness of our analysis. © 2012 Elsevier Ltd.","Cellular neural networks; Distributed delays; Exponential stability; Partial Lipschitz continuity; Relative nonlinear measure","Activation functions; Boundedness; Distributed delays; Equilibrium point; Impulsive cellular neural networks; Lipschitz continuity; Lipschitz continuous; Monotonicity; Partial Lipschitz; Relative nonlinear measures; Artificial intelligence; Asymptotic stability; Cognitive systems; Cellular neural networks; article; computer simulation; equilibrium constant; Lipschitz continuity; mathematical analysis; mathematical computing; mathematical model; nerve cell network; nonlinear system; priority journal; theory construction; Neural Networks (Computer); Neurons; Time Factors",Article,Scopus,2-s2.0-84862801064
"Li Y., Liu X., Gao J.X., Maropoulos P.G.","A dynamic feature information model for integrated manufacturing planning and optimization",2012,"CIRP Annals - Manufacturing Technology",29,10.1016/j.cirp.2012.03.085,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861651136&doi=10.1016%2fj.cirp.2012.03.085&partnerID=40&md5=d9790c646999a477435601508aaf44c9","This paper presents a new, dynamic feature representation method for high value parts consisting of complex and intersecting features. The method first extracts features from the CAD model of a complex part. Then the dynamic status of each feature is established between various operations to be carried out during the whole manufacturing process. Each manufacturing and verification operation can be planned and optimized using the real conditions of a feature, thus enhancing accuracy, traceability and process control. The dynamic feature representation is complementary to the design models used as underlining basis in current CAD/CAM and decision support systems. © 2012 CIRP.","Computer aided design (CAD); Feature; Integration","CAD models; Cad/cams; Complex parts; Design models; Dynamic features; Feature; Integrated manufacturing; Manufacturing process; Artificial intelligence; Decision support systems; Information theory; Integration; Manufacture; Optimization; Computer aided design",Article,Scopus,2-s2.0-84861651136
"Stiglic G., Kocbek S., Pernek I., Kokol P.","Comprehensive decision tree models in bioinformatics",2012,"PLoS ONE",29,10.1371/journal.pone.0033812,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859144390&doi=10.1371%2fjournal.pone.0033812&partnerID=40&md5=a2b563a442c23fac58d1507fe0ce1d7c","Purpose: Classification is an important and widely used machine learning technique in bioinformatics. Researchers and other end-users of machine learning software often prefer to work with comprehensible models where knowledge extraction and explanation of reasoning behind the classification model are possible. Methods: This paper presents an extension to an existing machine learning environment and a study on visual tuning of decision tree classifiers. The motivation for this research comes from the need to build effective and easily interpretable decision tree models by so called one-button data mining approach where no parameter tuning is needed. To avoid bias in classification, no classification performance measure is used during the tuning of the model that is constrained exclusively by the dimensions of the produced decision tree. Results: The proposed visual tuning of decision trees was evaluated on 40 datasets containing classical machine learning problems and 31 datasets from the field of bioinformatics. Although we did not expected significant differences in classification performance, the results demonstrate a significant increase of accuracy in less complex visually tuned decision trees. In contrast to classical machine learning benchmarking datasets, we observe higher accuracy gains in bioinformatics datasets. Additionally, a user study was carried out to confirm the assumption that the tree tuning times are significantly lower for the proposed method in comparison to manual tuning of the decision tree. Conclusions: The empirical results demonstrate that by building simple models constrained by predefined visual boundaries, one not only achieves good comprehensibility, but also very good classification performance that does not differ from usually more complex models built using default settings of the classical decision tree algorithm. In addition, our study demonstrates the suitability of visually tuned decision trees for datasets with binary class attributes and a high number of possibly redundant attributes that are very common in bioinformatics. © 2012 Stiglic et al.",,"accuracy; algorithm; article; bioinformatics; data analysis; data mining; decision tree; performance measurement system; process design; process development; process model; process technology; quality control; Artificial Intelligence; Computational Biology; Data Mining; Databases, Genetic; Decision Trees; Gene Expression Profiling; Humans; Models, Theoretical; Proteins; Reproducibility of Results; Solubility",Article,Scopus,2-s2.0-84859144390
"Qin X., Gao F., Chen G.","Wastewater quality monitoring system using sensor fusion and machine learning techniques",2012,"Water Research",29,10.1016/j.watres.2011.12.005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856112872&doi=10.1016%2fj.watres.2011.12.005&partnerID=40&md5=1bc32fb7af4dd5651747c5eee73151a3","A multi-sensor water quality monitoring system incorporating an UV/Vis spectrometer and a turbidimeter was used to monitor the Chemical Oxygen Demand (COD), Total Suspended Solids (TSS) and Oil & Grease (O&G) concentrations of the effluents from the Chinese restaurant on campus and an electrocoagulation-electroflotation (EC-EF) pilot plant. In order to handle the noise and information unbalance in the fused UV/Vis spectra and turbidity measurements during the calibration model building, an improved boosting method, Boosting-Iterative Predictor Weighting-Partial Least Squares (Boosting-IPW-PLS), was developed in the present study. The Boosting-IPW-PLS method incorporates IPW into boosting scheme to suppress the quality-irrelevant variables by assigning small weights, and builds up the models for the wastewater quality predictions based on the weighted variables. The monitoring system was tested in the field with satisfactory results, underlying the potential of this technique for the online monitoring of water quality. © 2011 Elsevier Ltd.","Boosting-IPW-PLS; Online monitoring; Turbidity; UV/Vis spectroscopy; Variable weighting; Wastewater treatment","Boosting method; Boosting-IPW-PLS; Calibration model; Least Square; Machine learning techniques; Monitoring system; Multi sensor; Online monitoring; Sensor fusion; Total suspended solids; Turbidity measurements; UV/ Vis spectroscopy; UV/Vis spectra; Variable weighting; Wastewater quality; Water quality monitoring systems; Chemical oxygen demand; Coagulation; Effluents; Least squares approximations; Partial discharges; Pilot plants; Sensors; Spectrometers; Turbidity; Ultraviolet visible spectroscopy; Wastewater; Wastewater treatment; Water quality; Monitoring; chemical oxygen demand; domestic waste; learning; qualitative analysis; sensor; spectrometer; suspended load; turbidity; ultraviolet radiation; visible spectrum; waste treatment; wastewater; water quality; article; biosensor; Boosting Iterative Predictor Weighting Partial Least Square; calibration; controlled study; machine learning; online monitoring; partial least squares regression; prediction; priority journal; signal noise ratio; turbidity; ultraviolet spectroscopy; waste water; waste water management; waste water quality monitoring system; water analysis; water pollution; water pollution control; water quality; Artificial Intelligence; Biological Oxygen Demand Analysis; Electrocoagulation; Least-Squares Analysis; Models, Chemical; Oils; Spectrophotometry, Ultraviolet; Waste Disposal, Fluid; Water Quality",Article,Scopus,2-s2.0-84856112872
"Wang L., Liu Z.-P., Zhang X.-S., Chen L.","Prediction of hot spots in protein interfaces using a random forest model with hybrid features",2012,"Protein Engineering, Design and Selection",29,10.1093/protein/gzr066,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863285682&doi=10.1093%2fprotein%2fgzr066&partnerID=40&md5=3f56cd99a943776cf7f24b9be406e989","Prediction of hot spots in protein interfaces provides crucial information for the research on proteinprotein interaction and drug design. Existing machine learning methods generally judge whether a given residue is likely to be a hot spot by extracting features only from the target residue. However, hot spots usually form a small cluster of residues which are tightly packed together at the center of protein interface. With this in mind, we present a novel method to extract hybrid features which incorporate a wide range of information of the target residue and its spatially neighboring residues, i.e. the nearest contact residue in the other face (mirror-contact residue) and the nearest contact residue in the same face (intra-contact residue). We provide a novel random forest (RF) model to effectively integrate these hybrid features for predicting hot spots in protein interfaces. Our method can achieve accuracy (ACC) of 82.4% and Matthews correlation coefficient (MCC) of 0.482 in Alanine Scanning Energetics Database, and ACC of 77.6% and MCC of 0.429 in Binding Interface Database. In a comparison study, performance of our RF model exceeds other existing methods, such as Robetta, FOLDEF, KFC, KFC2, MINERVA and HotPoint. Of our hybrid features, three physicochemical features of target residues (mass, polarizability and isoelectric point), the relative side-chain accessible surface area and the average depth index of mirror-contact residues are found to be the main discriminative features in hot spots prediction. We also confirm that hot spots tend to form large contact surface areas between two interacting proteins. Source data and code are available at: http://www.aporc.org/doc/wiki/ HotSpot. © The Author 2012. Published by Oxford University Press. All rights reserved.","hot spot; prediction; protein interface; random forest; structural bioinformatics","Accessible surface areas; Binding interface; Comparison study; Contact surface area; Correlation coefficient; Discriminative features; Drug Design; Extracting features; Hot spot; Hybrid features; Iso-electric points; Machine learning methods; Physicochemical features; Polarizabilities; Protein interfaces; Protein-protein interactions; Random forests; Side-chains; Small clusters; Source data; Structural bioinformatics; Amino acids; Bioinformatics; Decision trees; Drug interactions; Forecasting; Forestry; Learning systems; Mirrors; Proteins; hybrid protein; accuracy; article; classification algorithm; correlation coefficient; drug design; energy transfer; isoelectric point; physical chemistry; prediction; priority journal; protein protein interaction; random forest; Alanine; Algorithms; Amino Acids; Artificial Intelligence; Calmodulin; Computer Simulation; Humans; Hydrophobic and Hydrophilic Interactions; Models, Chemical; Models, Molecular; Mutagenesis, Site-Directed; Myosin-Light-Chain Kinase; Protein Binding; Protein Conformation; Protein Interaction Mapping; ROC Curve; Static Electricity; Thrombin; Thrombomodulin; Amino Acids; Decision Theory; Drugs; Forecasts; Interfaces; Proteins",Article,Scopus,2-s2.0-84863285682
"Zhang S., Zhao X., Lei B.","Facial expression recognition based on local binary patterns and local fisher discriminant analysis",2012,"WSEAS Transactions on Signal Processing",29,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861116813&partnerID=40&md5=33c8427c51a3dd85a900316809f90aa5","Automatic facial expression recognition is an interesting and challenging subject in signal processing, pattern recognition, artificial intelligence, etc. In this paper, a new method of facial expression recognition based on local binary patterns (LBP) and local Fisher discriminant analysis (LFDA) is presented. The LBP features are firstly extracted from the original facial expression images. Then LFDA is used to produce the low dimensional discriminative embedded data representations from the extracted high dimensional LBP features with striking performance improvement on facial expression recognition tasks. Finally, support vector machines (SVM) classifier is used for facial expression classification. The experimental results on the popular JAFFE facial expression database demonstrate that the presented facial expression recognition method based on LBP and LFDA obtains the best recognition accuracy of 90.7% with 11 reduced features, outperforming the other used methods such as principal component analysis (PCA), linear discriminant analysis (LDA), locality preserving projection (LPP).","Facial expression recognition; Linear discriminant analysis; Local binary patterns; Local Fisher discriminant analysis; Locality preserving projection; Principal component analysis; Support vector machines","Facial expression recognition; Fisher discriminant analysis; Linear discriminant analysis; Local binary patterns; Locality preserving projections; Artificial intelligence; Content based retrieval; Fisher information matrix; Gesture recognition; Principal component analysis; Signal processing; Support vector machines; Face recognition",Article,Scopus,2-s2.0-84861116813
"Guillaumin M., Mensink T., Verbeek J., Schmid C.","Face recognition from caption-based supervision",2012,"International Journal of Computer Vision",29,10.1007/s11263-011-0447-x,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855350007&doi=10.1007%2fs11263-011-0447-x&partnerID=40&md5=d9d43dd18e4178083d242331fed64c4c","In this paper, we present methods for face recognition using a collection of images with captions. We consider two tasks: retrieving all faces of a particular person in a data set, and establishing the correct association between the names in the captions and the faces in the images. This is challenging because of the very large appearance variation in the images, as well as the potential mismatch between images and their captions. For both tasks, we compare generative and discriminative probabilistic models, as well as methods that maximize subgraph densities in similarity graphs. We extend them by considering different metric learning techniques to obtain appropriate face representations that reduce intra person variability and increase inter person separation. For the retrieval task, we also study the benefit of query expansion. To evaluate performance, we use a new fully labeled data set of 31147 faces which extends the recent Labeled Faces in the Wild data set. We present extensive experimental results which show that metric learning significantly improves the performance of all approaches on both tasks. © 2011 Springer Science+Business Media, LLC.","Constrained clustering; Face recognition; Face retrieval; Metric learning; Weakly supervised learning","Constrained clustering; Data sets; Face representations; Face retrieval; Labeled data; Metric learning; Probabilistic models; Query expansion; Subgraphs; Weakly supervised learning; Artificial intelligence; Software engineering; Face recognition",Article,Scopus,2-s2.0-84855350007
"Ventura J., Höllerer T.","Wide-area scene mapping for mobile visual tracking",2012,"ISMAR 2012 - 11th IEEE International Symposium on Mixed and Augmented Reality 2012, Science and Technology Papers",28,10.1109/ISMAR.2012.6402531,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873529428&doi=10.1109%2fISMAR.2012.6402531&partnerID=40&md5=431d502ec64e3748888435d47aee7ed5","We propose a system for easily preparing arbitrary wide-area environments for subsequent real-time tracking with a handheld device. Our system evaluation shows that minimal user effort is required to initialize a camera tracking session in an unprepared environment. We combine panoramas captured using a handheld omnidirectional camera from several viewpoints to create a point cloud model. After the offline modeling step, live camera pose tracking is initialized by feature point matching, and continuously updated by aligning the point cloud model to the camera image. Given a reconstruction made with less than five minutes of video, we achieve below 25 cm translational error and 0.5 degrees rotational error for over 80% of images tested. In contrast to camera-based simultaneous localization and mapping (SLAM) systems, our methods are suitable for handheld use in large outdoor spaces. © 2012 IEEE.","C.5.3 [Computer System Implementation]: Microcomputers - Portable Devices (e.g., laptops, personal digital assistants); I.2.10 [Artificial Intelligence]: Vision and Scene Understanding - 3D/stereo scene analysis; I.4.8 [Image Processing and Computer Vision]: Scene Analysis - Tracking; I.5.4 [Pattern Recognition]: Applications - Computer Vision","Camera images; Camera tracking; Camera-based; Feature point matching; Hand held device; Handhelds; I.4.8 [Image Processing and Computer Vision]: Scene Analysis - Tracking; Live camera; Off-line modeling; Omnidirectional cameras; Outdoor space; Point cloud; Portable device; Pose tracking; Real time tracking; Rotational errors; Simultaneous localization and mapping; System evaluation; Unprepared environments; Vision and scene understanding; Visual Tracking; Wide area; Artificial intelligence; Augmented reality; Cameras; Image processing; Laptop computers; Mathematical techniques; Pattern recognition; Personal digital assistants; Robotics; Tracking (position)",Conference Paper,Scopus,2-s2.0-84873529428
"Liwicki S., Zafeiriou S., Tzimiropoulos G., Pantic M.","Efficient online subspace learning with an indefinite kernel for visual tracking and recognition",2012,"IEEE Transactions on Neural Networks and Learning Systems",28,10.1109/TNNLS.2012.2208654,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875886116&doi=10.1109%2fTNNLS.2012.2208654&partnerID=40&md5=f7b7e482a380dd4ebc0c94334528afc6","We propose an exact framework for online learning with a family of indefinite (not positive) kernels. As we study the case of nonpositive kernels, we first show how to extend kernel principal component analysis (KPCA) from a reproducing kernel Hilbert space to Krein space. We then formulate an incremental KPCA in Krein space that does not require the calculation of preimages and therefore is both efficient and exact. Our approach has been motivated by the application of visual tracking for which we wish to employ a robust gradient-based kernel. We use the proposed nonlinear appearance model learned online via KPCA in Krein space for visual tracking in many popular and difficult tracking scenarios. We also show applications of our kernel framework for the problem of face recognition. © 2012 IEEE.","Gradient-based kernel; online kernel learning; principal component analysis with indefinite kernels; recognition; robust tracking","Gradient based; Indefinite kernel; Online kernel learning; recognition; Robust tracking; Face recognition; Principal component analysis; Tracking (position); E-learning; algorithm; anatomy and histology; artificial intelligence; automated pattern recognition; biometry; computer assisted diagnosis; face; human; image subtraction; online system; principal component analysis; procedures; Algorithms; Artificial Intelligence; Biometry; Face; Humans; Image Interpretation, Computer-Assisted; Online Systems; Pattern Recognition, Automated; Principal Component Analysis; Subtraction Technique",Article,Scopus,2-s2.0-84875886116
"Del Caño A., Gómez D., De La Cruz M.P.","Uncertainty analysis in the sustainable design of concrete structures: A probabilistic method",2012,"Construction and Building Materials",28,10.1016/j.conbuildmat.2012.04.020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870251067&doi=10.1016%2fj.conbuildmat.2012.04.020&partnerID=40&md5=6eaeec894025e4185e92f20db31e1c71","This paper presents a sustainability assessment model based on requirement trees, value analysis, the Analytic Hierarchy Process, and the Monte Carlo simulation technique. It embraces the approach for assessing sustainability taken by the Spanish Structural Concrete Code. Nevertheless, the deterministic model of the Spanish Code can cause significant problems in terms of adequately managing a project's structural sustainability objective. Thus, a method not only has to assess the potential sustainability index at the end of the project. It also has to evaluate the degree of uncertainty that may make it difficult to achieve the sustainability objective established by the client or promoter. © 2012 Elsevier Ltd. All rights reserved.","Concrete structures; Decision support systems; Monte Carlo; Quantitative analysis; Simulation; Standards and codes; Sustainable development; Uncertainty","Degree of uncertainty; Deterministic models; MONTE CARLO; Monte carlo simulation technique; Probabilistic methods; Simulation; Standards and codes; Structural concretes; Sustainability assessment; Sustainability index; Sustainable design; Uncertainty; Artificial intelligence; Chemical analysis; Concrete buildings; Concrete construction; Decision support systems; Monte Carlo methods; Uncertainty analysis; Sustainable development",Article,Scopus,2-s2.0-84870251067
"Tran-Thanh L., Chapman A., Rogers A., Jennings N.R.","Knapsack based optimal policies for budget-limited multi-armed bandits",2012,"Proceedings of the National Conference on Artificial Intelligence",28,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868281643&partnerID=40&md5=db5861bda6fb482140271b2e709fdc21","In budget-limited multi-armed bandit (MAB) problems, the learner's actions are costly and constrained by a fixed budget. Consequently, an optimal exploitation policy may not be to pull the optimal arm repeatedly, as is the case in other variants of MAB, but rather to pull the sequence of different arms that maximises the agent's total reward within the budget. This difference from existing MABs means that new approaches to maximising the total reward are required. Given this, we develop two pulling policies, namely: (i) KUBE; and (ii) fractional KUBE. Whereas the former provides better performance up to 40% in our experimental settings, the latter is computationally less expensive. We also prove logarithmic upper bounds for the regret of both policies, and show that these bounds are asymptotically optimal (i.e. they only differ from the best possible regret by a constant factor). Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Asymptotically optimal; Constant factors; Fixed budget; Multi armed bandit; Optimal exploitation; Optimal policies; Upper Bound; Artificial intelligence; Optimization; Budget control",Conference Paper,Scopus,2-s2.0-84868281643
"Alghowinem S., Goecke R., Wagner M., Epps J., Breakspear M., Parker G.","From joyous to clinically depressed: Mood detection using spontaneous speech",2012,"Proceedings of the 25th International Florida Artificial Intelligence Research Society Conference, FLAIRS-25",28,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865055679&partnerID=40&md5=bfa0cfde6082f864ad37fbcfb94061f1","Depression and other mood disorders are common and disabling disorders. We present work towards an objective diagnostic aid supporting clinicians using affective sensing technology with a focus on acoustic and statistical features from spontaneous speech. This work investigates differences in expressing positive and negative emotions in depressed and healthy control subjects as well as whether initial gender classification increases the recognition rate. To this end, spontaneous speech from interviews of 30 subjects of each depressed and controls was analysed, with a focus on questions eliciting positive and negative emotions. Using HMMs with GMMs for classification with 30-fold cross-validation, we found that MFCC, energy and intensity features gave highest recognition rates when female and male subjects were analysed together. When the dataset was first split by gender, log energy and shimmer features, respectively, were found to give the highest recognition rates in females, while it was loudness for males. Overall, correct recognition rates from acoustic features for depressed female subjects were higher than for male subjects. Using temporal features, we found that the response time and average syllable duration were longer in depressed subjects, while the interaction involvement and articulation rate wesre higher in control subjects. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Acoustic features; Cross validation; Data sets; Gender classification; Healthy controls; In-control; Mood detection; Mood disorders; Recognition rates; Sensing technology; Spontaneous speech; Statistical features; Temporal features; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84865055679
"Stefan I., Elgala H., Haas H.","Study of dimming and LED nonlinearity for ACO-OFDM based VLC systems",2012,"IEEE Wireless Communications and Networking Conference, WCNC",28,10.1109/WCNC.2012.6214520,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864361962&doi=10.1109%2fWCNC.2012.6214520&partnerID=40&md5=bd5ec640e84c6bc3611218da7ff007aa","In this paper, the performance of an asymmetrically clipped optical orthogonal frequency division multiplexing (ACO-OFDM) indoor wireless system is investigated. In particular, the illuminance distribution inside a room (5m x 5 m x 3m) under different brightness conditions, i.e. dimming levels, and the electrical signal-to-noise ratio (SNR) at the receiver are considered. The system performance is also assessed in terms of bit-error-ratio (BER) as a function of average transmitted optical power. Brightness control is achieved using the continuous current reduction (CCR) technique to assure higher luminous efficacy compared to the pulse-width modulation (PWM) dimming technique. In addition, the impact of the induced distortions due to the nonlinear behavior of the light-emitting diode (LED) as a light source is included in the simulation model. For a circular area of about 1m radius located directly below one of the four LED illumination modules utilized in the room, having illuminance values above 300 lx (10% brightness), a 9 dB average electrical SNR is achieved at the receiver located on a desktop. © 2012 IEEE.","dimming; LED; Non-linearity; OFDM; Optical wireless communication","Bit error ratios; Brightness control; Current reduction; Illuminance distribution; Indoor wireless systems; LED illumination; Luminous efficacy; Non-Linearity; Nonlinear behavior; Optical orthogonal frequency division multiplexing; Optical power; Optical wireless communications; Signaltonoise ratio (SNR); Simulation model; Artificial intelligence; Computer simulation; Detector circuits; Dimming (lamps); Light sources; Luminance; Orthogonal frequency division multiplexing; Voltage control; Wireless telecommunication systems; Light emitting diodes",Conference Paper,Scopus,2-s2.0-84864361962
"Zhang L., Wang L., Lin W.","Conjunctive patches subspace learning with side information for collaborative image retrieval",2012,"IEEE Transactions on Image Processing",28,10.1109/TIP.2012.2195014,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864144566&doi=10.1109%2fTIP.2012.2195014&partnerID=40&md5=a2528cd5888486ed4300980c03488828","Content-based image retrieval (CBIR) has attracted substantial attention during the past few years for its potential practical applications to image management. A variety of relevance feedback schemes have been designed to bridge the semantic gap between low-level visual features and high-level semantic concepts for an image retrieval task. Various collaborative image retrieval (CIR) schemes aim to utilize the user historical feedback log data with similar and dissimilar pairwise constraints to improve the performance of a CBIR system. However, existing subspace learning approaches with explicit label information cannot be applied for a CIR task although the subspace learning techniques play a key role in various computer vision tasks, e.g., face recognition and image classification. In this paper, we propose a novel subspace learning framework, i.e., conjunctive patches subspace learning (CPSL) with side information, for learning an effective semantic subspace by exploiting the user historical feedback log data for a CIR task. CPSL can effectively integrate the discriminative information of labeled log images, the geometrical information of labeled log images, and the weakly similar information of unlabeled images together to learn a reliable subspace. We formulate this problem into a constrained optimization problem and then present a new subspace learning technique to exploit the user historical feedback log data. Extensive experiments on both synthetic datasets and a real-world image database demonstrate the effectiveness of the proposed scheme in improving the performance of a CBIR system by exploiting the user historical feedback log data. © 1992-2012 IEEE.","Collaborative image retrieval (CIR); Log data; Side information; Subspace learning","CBIR system; Collaborative image retrieval; Constrained optimization problems; Content-Based Image Retrieval; Geometrical informations; High level semantics; Image management; Label information; Log data; Pairwise constraints; Real-world image; Relevance feedback; Semantic gap; Side information; Subspace learning; Synthetic datasets; Visual feature; Bridges; Computer vision; Constrained optimization; Face recognition; Feedback; Learning algorithms; Semantics; Image retrieval; algorithm; artificial intelligence; automated pattern recognition; computer assisted diagnosis; hospital information system; image enhancement; image subtraction; information retrieval; procedures; reproducibility; sensitivity and specificity; article; automated pattern recognition; computer assisted diagnosis; image enhancement; information retrieval; methodology; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Information Storage and Retrieval; Pattern Recognition, Automated; Radiology Information Systems; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Information Storage and Retrieval; Pattern Recognition, Automated; Radiology Information Systems; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-84864144566
"Jaffar J., Murali V., Navas J.A., Santosa A.E.","TRACER: A symbolic execution tool for verification",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",28,10.1007/978-3-642-31424-7_61,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864046845&doi=10.1007%2f978-3-642-31424-7_61&partnerID=40&md5=a6ef8bb79aeee90fe5dd466ff63ae525","We present TRACER, a verifier for safety properties of sequential C programs. It is based on symbolic execution (se) and its unique features are in how it makes se finite in presence of unbounded loops and its use of interpolants from infeasible paths to tackle the path-explosion problem. © 2012 Springer-Verlag.",,"C programs; Infeasible paths; Interpolants; Safety property; Symbolic execution; Unique features; Artificial intelligence; Computer aided analysis",Conference Paper,Scopus,2-s2.0-84864046845
"Nekrich Y., Navarro G.","Sorted range reporting",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",28,10.1007/978-3-642-31155-0_24,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863084667&doi=10.1007%2f978-3-642-31155-0_24&partnerID=40&md5=524199de5feaf87e234c3b8fbb26cd28","We consider a variant of the orthogonal range reporting problem when all points should be reported in the sorted order of their x-coordinates. We show that reporting two-dimensional points with this additional condition can be organized (almost) as efficiently as the standard range reporting. Moreover, our results generalize and improve the previously known results for the orthogonal range successor problem and can be used to obtain better solutions for some stringology problems. © 2012 Springer-Verlag.",,"Stringology; Artificial intelligence; Algorithms",Conference Paper,Scopus,2-s2.0-84863084667
"Sulaiman S.I., Rahman T.K.A., Musirin I., Shaari S., Sopian K.","An intelligent method for sizing optimization in grid-connected photovoltaic system",2012,"Solar Energy",28,10.1016/j.solener.2012.04.009,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861682318&doi=10.1016%2fj.solener.2012.04.009&partnerID=40&md5=c6af029d2aa703996df25af4814be95e","This paper presents an intelligent sizing technique for sizing grid-connected photovoltaic (GCPV) system using evolutionary programming (EP). EP was used to select the optimal set of photovoltaic (PV) module and inverter for the system such that the technical or economic performance of the system could be optimized. The decision variables for the optimization process are the PV module and inverter which had been encoded as specific integers in the respective database. On the other hand, the objective function of the optimization task was set to be either to optimize the technical performance or the economic performance of the system. Before implementing the intelligent-based sizing algorithm, a conventional sizing model had been presented which later led to the development of an iterative-based sizing algorithm, known as ISA. As the ISA tested all available combinations of PV modules and inverters to be considered for the system, the overall sizing process became time consuming and tedious. Therefore, the proposed EP-based sizing algorithm, known as EPSA, was developed to accelerate the sizing process. During the development of EPSA, different EP models had been tested with a non-linear scaling factor being introduced to improve the performance of these models. Results showed that the EPSA had outperformed ISA in terms of producing lower computation time. Besides that, the incorporation of non-linear scaling factor had also improved the performance of all EP models under investigation. In addition, EPSA had also shown the best optimization performance when compared with other intelligent-based sizing algorithms using different types of Computational Intelligence. © 2012 Elsevier Ltd.","Evolutionary programming (EP); Grid-connected photovoltaic (GCPV); Optimization; Photovoltaic (PV); Sizing","Computation time; Decision variables; Economic performance; Grid-connected; Grid-connected photovoltaic system; Intelligent method; Non-linear scaling; Objective functions; Optimal sets; Optimization process; Optimization task; Photovoltaic; Photovoltaic modules; PV modules; Sizing; Sizing algorithms; Sizing optimization; Sizing process; Technical performance; Artificial intelligence; Computer programming; Evolutionary algorithms; Intelligent control; Optimization; Photovoltaic effects; Integer programming; algorithm; database; economic analysis; optimization; performance assessment; photovoltaic system; solar power",Article,Scopus,2-s2.0-84861682318
"Ou C.-M.","Host-based intrusion detection systems adapted from agent-based artificial immune systems",2012,"Neurocomputing",28,10.1016/j.neucom.2011.07.031,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860237427&doi=10.1016%2fj.neucom.2011.07.031&partnerID=40&md5=f5d0ff69656835338a8db4815079efd3","Agent-based artificial immune system (ABAIS) is adopted to intrusion detection system (IDS). An agent-based IDS (ABIDS) inspired by the danger theory of human immune system is proposed. Multiple agents are embedded to ABIDS, where agents coordinate one another to calculate mature context antigen value (MCAV) and update activation threshold for security responses. The intelligence behind ABIDS is based on the danger theory and the functionalities of dendritic cells in human immune systems, while dendritic cells agents (DC agent) are emulated for innate immune subsystem and artificial T-cell agents (TC agent) are for adaptive immune subsystem. Antigens are profiles of system calls while corresponding behaviors are regarded as signals. This ABIDS is based on the dual detections of DC agents for signals and TC agents for antigens. ABAIS is an intelligent system with learning and memory capabilities. According to MCAVs, immune response to malicious behaviors is activated by either computer host or Security Operating Center. Accordingly, computer hosts met with malicious intrusions can be effectively detected by input signals and temporary output signals such as PAMP, danger and safe signals. © 2012 Elsevier B.V.","Agent; Artificial immune system; Danger theory; Dendritic cell algorithm; Intrusion detection","Activation thresholds; Agent based; Artificial Immune System; Computer hosts; Danger theories; Dendritic cell algorithms; Dendritic cells; Host-based intrusion detection system; Human immune systems; Immune response; Input signal; Intrusion Detection Systems; Learning and memory; Malicious behavior; Multiple agents; Operating centers; Output signal; System calls; Agents; Antigens; Cells; Computer crime; Immunology; Intelligent systems; Intrusion detection; adaptive immunity; agent based artificial immune system; algorithm; article; artificial intelligence; computer memory; computer security; danger theory; dendritic cell; information system; innate immunity; intrusion detection system; machine learning; mathematical computing; mature context antigen value; priority journal; T lymphocyte; theory",Article,Scopus,2-s2.0-84860237427
"Alperin-Sheriff J., Peikert C.","Circular and KDM security for identity-based encryption",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",28,10.1007/978-3-642-30057-8_20,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861665303&doi=10.1007%2f978-3-642-30057-8_20&partnerID=40&md5=ff6d24e6060ed1825e735c90bceefe95","We initiate the study of security for key-dependent messages (KDM), sometimes also known as ""circular"" or ""clique"" security, in the setting of identity-based encryption (IBE). Circular/KDM security requires that ciphertexts preserve secrecy even when they encrypt messages that may depend on the secret keys, and arises in natural usage scenarios for IBE. We construct an IBE system that is circular secure for affine functions of users' secret keys, based on the learning with errors (LWE) problem (and hence on worst-case lattice problems). The scheme is secure in the standard model, under a natural extension of a selectiveidentity attack. Our three main technical contributions are (1) showing the circular/KDM-security of a ""dual""-style LWE public-key cryptosystem, (2) proving the hardness of a version of the ""extended LWE"" problem due to O'Neill, Peikert and Waters (CRYPTO'11), and (3) building an IBE scheme around the dual-style system using a novel lattice-based ""all-but-d"" trapdoor function. © 2012 International Association for Cryptologic Research.",,"Affine function; Ciphertexts; Identity Based Encryption; Lattice problems; Natural extension; Public key cryptosystems; Secret key; Technical contribution; The standard model; Trapdoor functions; Usage scenarios; Identity Based Encryption; Lattice problems; Learning with Errors; Natural extension; Public key cryptosystems; Technical contribution; The standard model; Trapdoor functions; Artificial intelligence; Public key cryptography; Public key cryptography; Cryptography",Conference Paper,Scopus,2-s2.0-84861665303
"Haug A., Hvam L., Mortensen N.H.","Definition and evaluation of product configurator development strategies",2012,"Computers in Industry",28,10.1016/j.compind.2012.02.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861099750&doi=10.1016%2fj.compind.2012.02.001&partnerID=40&md5=a42cd575b22d2d3c75396817e5aea558","Product configurators represent one of the most successful applications of artificial intelligence principles. Product configurators are a subtype of software-based expert systems with a focus on the creation of product specifications. The use of product configurators has resulted in many positive effects in engineering-oriented companies such as reduced lead times, fewer errors, shorter learning periods for new employees, etc. Unfortunately, many configuration projects also fail because the task of developing the configurator turns out to be much more difficult and time-consuming than anticipated. Thus, it is crucial to apply the appropriate strategy. However, the literature does not discuss different strategic alternatives in a detailed manner; it only provides generalised recommendations of single strategies. To deal with this issue, this paper defines and compares seven different strategies for the development of product configurators. The relevance of the defined strategies is supported by seven named case studies. © 2012 Elsevier B.V. All rights reserved.","Expert systems; Knowledge acquisition; Knowledge engineering; Product configuration; Product configurator","Development strategies; Engineering-oriented; Lead time; Product configuration; Product configurator; Product specifications; Software-based; Artificial intelligence; Expert systems; Knowledge engineering; Knowledge acquisition",Article,Scopus,2-s2.0-84861099750
"Jiang J.Q., McQuay L.J.","Predicting protein function by multi-label correlated semi-supervised learning",2012,"IEEE/ACM Transactions on Computational Biology and Bioinformatics",28,10.1109/TCBB.2011.156,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861492190&doi=10.1109%2fTCBB.2011.156&partnerID=40&md5=5b6addd2fe1bf86cc8ccc57998c045c6","Assigning biological functions to uncharacterized proteins is a fundamental problem in the postgenomic era. The increasing availability of large amounts of data on protein-protein interactions (PPIs) has led to the emergence of a considerable number of computational methods for determining protein function in the context of a network. These algorithms, however, treat each functional class in isolation and thereby often suffer from the difficulty of the scarcity of labeled data. In reality, different functional classes are naturally dependent on one another. We propose a new algorithm, Multi-label Correlated Semi-supervised Learning (MCSL), to incorporate the intrinsic correlations among functional classes into protein function prediction by leveraging the relationships provided by the PPI network and the functional class network. The guiding intuition is that the classification function should be sufficiently smooth on subgraphs where the respective topologies of these two networks are a good match. We encode this intuition as regularized learning with intraclass and interclass consistency, which can be understood as an extension of the graph-based learning with local and global consistency (LGC) method. Cross validation on the yeast proteome illustrates that MCSL consistently outperforms several state-of-the-art methods. Most notably, it effectively overcomes the problem associated with scarcity of label data. The supplementary files are freely available at http://sites.google.com/site/csaijiang/MCSL. © 2012 IEEE.","functional class correlation; kernel learning; multi-label learning; Protein function prediction; semi-supervised learning","Functional class; Kernel learning; Multi-label; Protein function prediction; Semi-supervised learning; Algorithms; Supervised learning; Proteins; protein; proteome; article; artificial intelligence; biology; chemistry; classification; methodology; protein analysis; protein database; reproducibility; yeast; Artificial Intelligence; Computational Biology; Databases, Protein; Protein Interaction Mapping; Proteins; Proteome; Reproducibility of Results; Yeasts",Article,Scopus,2-s2.0-84861492190
"Tian J., Chen L.","Image noise estimation using a variation-adaptive evolutionary approach",2012,"IEEE Signal Processing Letters",28,10.1109/LSP.2012.2197200,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861449140&doi=10.1109%2fLSP.2012.2197200&partnerID=40&md5=2d22c73aef7e6a508d2f3297f589c629","The estimation of noise statistics is critical to optimize many computer vision algorithms. The main issue is how to identify the homogeneous image patches for estimating the noise statistics. The smallest variance used in the conventional approaches is not always a good measure of homogeneity of image patches. In addition, the conventional approaches neglect the fact that homogeneous image patches tend to cluster together due to local spatial smoothness in images. In view of this, a new image noise estimation approach is proposed in this letter. The proposed approach has two key components. First, a graphical representation is proposed to model the relationship among image patches. Second, the ant colony optimization (ACO) technique is used to automatically select a set of patches for estimating the noise statistics. To be more specific, the proposed approach guides the spatial movement of artificial ants towards homogeneous locations in the graph, by considering both global (i.e., clustering measure) properties and local (i.e., homogeneity measure) properties of patches. Experimental results are provided to justify that the proposed approach outperforms nine conventional approaches to provide more accurate noise statistics estimation. © 2012 IEEE.","Ant colony optimization; Gaussian noise; image denoising; image restoration","Ant Colony Optimization (ACO); Artificial ant; Computer vision algorithms; Conventional approach; Evolutionary approach; Graphical representations; Image noise; Image patches; Noise statistics; Spatial smoothness; Algorithms; Artificial intelligence; Computer vision; Gaussian noise (electronic); Image denoising; Image reconstruction; Estimation",Article,Scopus,2-s2.0-84861449140
"Zhong W.-S., Liu G.-P., Thomas C.","Global bounded consensus of multiagent systems with nonidentical nodes and time delays",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",28,10.1109/TSMCB.2012.2192428,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866494409&doi=10.1109%2fTSMCB.2012.2192428&partnerID=40&md5=4eaec6d0a69a02299ff35a2e9a014e2d","This paper investigates the global bounded consensus problem of networked multiagent systems consisting of nonlinear nonidentical node dynamics with the communication time-delay topology. We derive globally bounded controlled consensus conditions for both delay-independent and delay-dependent conditions based on the Lyapunov-Krasovskii functional method. The proposed consensus criteria ensure that all agents eventually move along the desired trajectory in the sense of boundedness. Meanwhile, the bounded consensus criteria can be viewed as an extension of the case of identical agent dynamics to the case of nonidentical agent dynamics. We finally demonstrate the effectiveness of the theoretical results by means of a numerical simulation. © 1996-2012 IEEE.","Complex dynamical network; consensus; multiagent systems (MASs); networked control systems","Boundedness; Communication time delays; Complex dynamical networks; consensus; Consensus problems; Delay dependent conditions; Delay independent; Lyapunov-Krasovskii functional method; Theoretical result; Dynamics; Lyapunov functions; Networked control systems; Time delay; Multi agent systems; algorithm; article; artificial intelligence; automated pattern recognition; computer simulation; decision support system; theoretical model; Algorithms; Artificial Intelligence; Computer Simulation; Decision Support Techniques; Models, Theoretical; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84866494409
"SanMiguel J.C., Cavallaro A., Martínez J.M.","Adaptive online performance evaluation of video trackers",2012,"IEEE Transactions on Image Processing",28,10.1109/TIP.2011.2182520,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860179164&doi=10.1109%2fTIP.2011.2182520&partnerID=40&md5=71c0edde178e5faa9eb19518e2835f62","We propose an adaptive framework to estimate the quality of video tracking algorithms without ground-truth data. The framework is divided into two main stages, namely, the estimation of the tracker condition to identify temporal segments during which a target is lost and the measurement of the quality of the estimated track when the tracker is successful. A key novelty of the proposed framework is the capability of evaluating video trackers with multiple failures and recoveries over long sequences. Successful tracking is identified by analyzing the uncertainty of the tracker, whereas track recovery from errors is determined based on the time-reversibility constraint. The proposed approach is demonstrated on a particle filter tracker over a heterogeneous data set. Experimental results show the effectiveness and robustness of the proposed framework that improves state-of-the-art approaches in the presence of tracking challenges such as occlusions, illumination changes, and clutter and on sequences containing multiple tracking errors and recoveries. © 1992-2012 IEEE.","Failure detection; particle filter; time reversibility; track quality; tracking uncertainty; video tracking","Failure detection; Particle filter; Time reversibilities; Track quality; Tracking uncertainty; Video tracking; Errors; Nonlinear filtering; Recovery; Video recording; Uncertainty analysis; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; image enhancement; methodology; photography; reproducibility; sensitivity and specificity; three dimensional imaging; videorecording; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Pattern Recognition, Automated; Photography; Reproducibility of Results; Sensitivity and Specificity; Video Recording",Article,Scopus,2-s2.0-84860179164
"Salido M.A., Rodriguez-Molins M., Barber F.","A decision support system for managing combinatorial problems in container terminals",2012,"Knowledge-Based Systems",28,10.1016/j.knosys.2011.06.021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857369955&doi=10.1016%2fj.knosys.2011.06.021&partnerID=40&md5=bee880094d1bf20176d429c93c701745","A container terminal is a facility where cargo containers are transshipped between different transport vehicles. We focus our attention on the transshipment between vessels and land vehicles, in which case the terminal is described as a maritime container terminal. In these container terminals, many combinatorial related problems appear and the solution of one of the problems may affect to the solution of other related problems. For instance, the berth allocation problem can affect to the crane assignment problem and both could also affect to the Container Stacking Problem. Thus, terminal operators normally demand all containers to be loaded into an incoming vessel should be ready and easily accessible in the yard before vessel's arrival. Similarly, customers (i.e., vessel owners) expect prompt berthing of their vessels upon arrival. However the efficiency of the loading/unloading tasks of containers in a vessel depends on the number of assigned cranes and the efficiency of the container yard logistic. In this paper, we present a decision support system to guide the operators in the development of these typical tasks. Due to some of these problems are combinatorial, some analytical formulas are presented to estimate the behavior of the container terminal. © 2011 Elsevier B.V. All rights reserved.","Artificial intelligence; Berth allocation problem; Container Stacking Problem; Decision support system; Metaheuristic techniques; Quay Crane Assignment Problem","Analytical formulas; Assignment problems; Berth allocation problem; Cargo containers; Combinatorial problem; Container terminal; Container yard; Decision supports; Land vehicles; Maritime container terminal; Meta-heuristic techniques; Quay cranes; Terminal operators; Transport vehicles; Artificial intelligence; Cranes; Decision support systems; Railroad yards and terminals; Transfer cases (vehicles); Port terminals",Conference Paper,Scopus,2-s2.0-84857369955
"Daliri M.R.","A hybrid automatic system for the diagnosis of lung cancer based on genetic algorithm and fuzzy extreme learning machines",2012,"Journal of Medical Systems",28,10.1007/s10916-011-9806-y,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863210448&doi=10.1007%2fs10916-011-9806-y&partnerID=40&md5=0f1f023312cc9c9c62b4378c451b01e6","An automatic system for the diagnosis of lung cancer has been proposed in this manuscript. The proposed method is based on combination of genetic algorithm (GA) for the feature selection and newly proposed approach, namely the extreme learning machines (ELM) for the classification of lung cancer data. The dimension of the feature space is reduced by the GA in this scheme and the effective features are selected in this way. The data are then fed to a fuzzy inference system (FIS) which is trained by the fuzzy extreme learning machines approach. The results on real data indicate that the proposed system is very effective in the diagnosis of lung cancer and can be used for clinical applications. © Springer Science+Business Media, LLC 2011.","Automatic Medical System; Diagnosis; Extreme Learning Machines (ELM); Fuzzy Inference System (FIS); Genetic Algorithm (GA); Lung Cancer","article; automation; data analysis; diagnostic accuracy; diagnostic approach route; diagnostic value; fuzzy logic; genetic algorithm; hybrid; lung cancer; machine learning; algorithm; artificial intelligence; fuzzy logic; human; image processing; lung tumor; methodology; Algorithms; Artificial Intelligence; Fuzzy Logic; Humans; Image Processing, Computer-Assisted; Lung Neoplasms",Article,Scopus,2-s2.0-84863210448
"Kotsiantis S.B.","Use of machine learning techniques for educational proposes: A decision support system for forecasting students' grades",2012,"Artificial Intelligence Review",28,10.1007/s10462-011-9234-x,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871069884&doi=10.1007%2fs10462-011-9234-x&partnerID=40&md5=069fe8e535b549ae8679811edfe8d8fc","Use of machine learning techniques for educational proposes (or educational data mining) is an emerging field aimed at developing methods of exploring data from computational educational settings and discovering meaningful patterns. The stored data (virtual courses, e-learning log file, demographic and academic data of students, admissions/registration info, and so on) can be useful for machine learning algorithms. In this article, we cite the most current articles that use machine learning techniques for educational proposes and we present a case study for predicting students' marks. Students' key demographic characteristics and their marks in a small number of written assignments can constitute the training set for a regression method in order to predict the student's performance. Finally, a prototype version of software support tool for tutors has been constructed.","Decision support tools; Educational data mining; Machine learning","Decision support tools; Demographic characteristics; Educational data mining; Educational settings; Log file; Machine learning techniques; Prototype versions; Regression method; Software support; Student's performance; Training sets; Virtual course; Artificial intelligence; Data mining; Decision support systems; E-learning; Forecasting; Learning algorithms; Population statistics; Software prototyping; Students; Teaching; Learning systems",Article,Scopus,2-s2.0-84871069884
"Aydodu I., Saka M.P.","Ant colony optimization of irregular steel frames including elemental warping effect",2012,"Advances in Engineering Software",28,10.1016/j.advengsoft.2011.05.029,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80955165948&doi=10.1016%2fj.advengsoft.2011.05.029&partnerID=40&md5=663749bd0aab17ef1b07c96220199edd","The effect of warping in the design of steel space frames having members of thin walled steel sections is significant. In this paper the optimum design problem of steel space frames is formulated according to the provisions of LRFD-AISC (Load and Resistance factor design of American Institute of Steel Construction) in which the effect of warping is also taken into account. Ant colony optimization technique is used to obtain the solution of the design problem. A number of space frame examples are designed by the algorithm developed in order to demonstrate the effect of warping in the optimum design. © 2011 Civil-Comp Ltd and Elsevier Ltd. All rights reserved.","Ant colony optimization; Minimum weight; Optimum structural design; Steel space frame; Stochastic search techniques; Warping effect","Algorithms; Artificial intelligence; Structural design; Structural frames; Thin walled structures; Ant-colony optimization; Minimum weight; Optimum structural design; Steel space frame; Stochastic search techniques; Warping effects; Structural optimization",Conference Paper,Scopus,2-s2.0-80955165948
"Wu H.-J., Su Y.-L., Lee S.-J.","A fast method for computing the centroid of a type-2 fuzzy set",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",28,10.1109/TSMCB.2011.2177085,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861201398&doi=10.1109%2fTSMCB.2011.2177085&partnerID=40&md5=62c5b205c7ac4dc3f915db9aaf0c7155","Type reduction does the work of computing the centroid of a type-2 fuzzy set. The result is a type-1 fuzzy set from which a corresponding crisp number can then be obtained through defuzzification. Type reduction is one of the major operations involved in type-2 fuzzy inference. Therefore, making type reduction efficient is a significant task in the application of type-2 fuzzy systems. Liu introduced a horizontal slice representation, called the α-plane representation, and proposed a type-reduction method for a type-2 fuzzy set. By exploring some useful properties of the α-plane representation and of the type reduction for interval type-2 fuzzy sets, a fast method is developed for computing the centroid of a type-2 fuzzy set. The number of computations and comparisons involved is greatly reduced. Convergence in each iteration can then speed up, and type reduction can be done much more efficiently. The effectiveness of the proposed method is analyzed mathematically and demonstrated by experimental results. © 2012 IEEE.","α-plane representation; Enhanced Karnik-Mendel (KM) (EKM) algorithm; KM algorithm; type reduction; type-2 fuzzy system","Crisp numbers; Defuzzifications; Fast methods; Horizontal slices; Interval type-2 fuzzy sets; Type reduction; Type-2 fuzzy set; Type-2 fuzzy systems; Useful properties; Algorithms; Fuzzy systems; Fuzzy sets; algorithm; article; artificial intelligence; automated pattern recognition; computer simulation; decision support system; fuzzy logic; methodology; theoretical model; Algorithms; Artificial Intelligence; Computer Simulation; Decision Support Techniques; Fuzzy Logic; Models, Theoretical; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84861201398
"Lei J.Z., Ghorbani A.A.","Improved competitive learning neural networks for network intrusion and fraud detection",2012,"Neurocomputing",28,10.1016/j.neucom.2011.02.021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-82455199118&doi=10.1016%2fj.neucom.2011.02.021&partnerID=40&md5=4af105daa6b60abc390f42fa7ddf6284","In this research, we propose two new clustering algorithms, the improved competitive learning network (ICLN) and the supervised improved competitive learning network (SICLN), for fraud detection and network intrusion detection. The ICLN is an unsupervised clustering algorithm, which applies new rules to the standard competitive learning neural network (SCLN). The network neurons in the ICLN are trained to represent the center of the data by a new reward-punishment update rule. This new update rule overcomes the instability of the SCLN. The SICLN is a supervised version of the ICLN. In the SICLN, the new supervised update rule uses the data labels to guide the training process to achieve a better clustering result. The SICLN can be applied to both labeled and unlabeled data and is highly tolerant to missing or delay labels. Furthermore, the SICLN is capable to reconstruct itself, thus is completely independent from the initial number of clusters.To assess the proposed algorithms, we have performed experimental comparisons on both research data and real-world data in fraud detection and network intrusion detection. The results demonstrate that both the ICLN and the SICLN achieve high performance, and the SICLN outperforms traditional unsupervised clustering algorithms. © 2011 Elsevier B.V..","Competitive learning; Fraud detection; Intrusion detection; Neural network; Supervised/unsupervised clustering","Clustering results; Competitive learning; Data labels; Experimental comparison; Fraud detection; Labeled and unlabeled data; Network intrusion detection; Network intrusions; Real world data; Research data; Supervised/unsupervised clustering; Training process; Unsupervised clustering algorithm; Computer crime; Crime; Intrusion detection; Learning algorithms; Learning systems; Neural networks; Clustering algorithms; accuracy; article; artificial intelligence; artificial neural network; classification algorithm; controlled study; electronic commerce; fuzzy system; intermethod comparison; kernel method; mathematical model; online analysis; online system; prediction; priority journal; process development; process optimization; signal detection",Article,Scopus,2-s2.0-82455199118
"Zhou G., Sohn K., Lee H.","Online incremental feature learning with denoising autoencoders",2012,"Journal of Machine Learning Research",28,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954202765&partnerID=40&md5=c3fd09c817ce688fbccc1de7f31ce887","While determining model complexity is an important problem in machine learning, many feature learning algorithms rely on cross-validation to choose an optimal number of features, which is usually challenging for online learning from a massive stream of data. In this paper, we propose an incremental feature learning algorithm to determine the optimal model complexity for large-scale, online datasets based on the denoising autoencoder. This algorithm is composed of two processes: adding features and merging features. Specifically, it adds new features to minimize the objective function's residual and merges similar features to obtain a compact feature representation and prevent over-fitting. Our experiments show that the proposed model quickly converges to the optimal number of features in a large-scale online setting. In classification tasks, our model outperforms the (non-incremental) denoising autoencoder, and deep networks constructed from our algorithm perform favorably compared to deep belief networks and stacked denoising autoencoders. Further, the algorithm is effective in recognizing new patterns when the data distribution changes over time in the massive online data stream. © Copyright 2012 by the authors.",,"Algorithms; Artificial intelligence; Complex networks; Data mining; E-learning; Learning systems; Classification tasks; Compact Features; Cross validation; Data distribution; Deep belief networks; Feature learning; Model complexity; Objective functions; Learning algorithms",Conference Paper,Scopus,2-s2.0-84954202765
"Romera-Paredes B., Argyriou A., Bianchi-Berthouze N., Pontil M.","Exploiting unrelated tasks in multi-task learning",2012,"Journal of Machine Learning Research",28,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908495158&partnerID=40&md5=4484b4708a48f37f533b8541b29095da","We study the problem of learning a group of principal tasks using a group of auxiliary tasks, unrelated to the principal ones. In many applications, joint learning of unrelated tasks which use the same input data can be beneficial. The reason is that prior knowledge about which tasks are unrelated can lead to sparser and more informative representations for each task, essentially screening out idiosyncrasies of the data distribution. We propose a novel method which builds on a prior multitask methodology by favoring a shared low dimensional representation within each group of tasks. In addition, we impose a penalty on tasks from different groups which encourages the two representations to be orthogonal. We further discuss a condition which ensures convexity of the optimization problem and argue that it can be solved by alternating minimization. We present experiments on synthetic and real data, which indicate that incorporating unrelated tasks can improve significantly over standard multi-task learning methods.",,"Artificial intelligence; Optimization; Alternating minimization; Data distribution; Joint learning; Low-dimensional representation; Multitask learning; Optimization problems; Prior knowledge; Synthetic and real data; Learning systems",Conference Paper,Scopus,2-s2.0-84908495158
"Skillen K.-L., Chen L., Nugent C.D., Donnelly M.P., Burns W., Solheim I.","Ontological user profile modeling for context-aware application personalization",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",27,10.1007/978-3-642-35377-2_36,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870351888&doi=10.1007%2f978-3-642-35377-2_36&partnerID=40&md5=8dd5cebbeedcfe45ed77343087854598","Existing context-aware adaptation techniques are limited in their support for user personalization. There is relatively less developed research involving adaptive user modeling for user applications in the emerging areas of mobile and pervasive computing. This paper describes the creation of a User Profile Ontology for context-aware application personalization within mobile environments. We analyze users' behavior and characterize users' needs for context-aware applications. Special emphasis is placed in the ontological modeling of dynamic components for use in adaptable applications. We illustrate the use of the model in the context of a case study, focusing on providing personalized services to older people via smart-device technologies. © 2012 Springer-Verlag.","context-aware; Ontology; personalization; user profile","Context aware applications; Context-Aware; Context-aware adaptation; Mobile environments; Modeling of dynamics; Older People; Personalizations; Personalized service; User Modeling; User profile; Artificial intelligence; Computer applications; Ontology; Ubiquitous computing",Conference Paper,Scopus,2-s2.0-84870351888
"Uchida A., Ito Y., Nakano K.","An efficient GPU implementation of Ant Colony Optimization for the Traveling Salesman Problem",2012,"Proceedings of the 2012 3rd International Conference on Networking and Computing, ICNC 2012",27,10.1109/ICNC.2012.22,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874236718&doi=10.1109%2fICNC.2012.22&partnerID=40&md5=ac7fa6a63aa7bef093f8053bcfe0e496","Graphics Processing Units (GPUs) are specialized microprocessors that accelerate graphics operations. Recent GPUs, which have many processing units connected with an off-chip global memory, can be used for general purpose parallel computation. Ant Colony Optimization (ACO) approaches have been introduced as nature-inspired heuristics to find good solutions of the Traveling Salesman Problem (TSP). In ACO approaches, a number of ants traverse the cities of the TSP to find better solutions of the TSP. The ants randomly select next visiting cities based on the probabilities determined by total amounts of their pheromone spread on routes. The main contribution of this paper is to present sophisticated and efficient implementation of one of the ACO approaches on the GPU. In our implementation, we have considered many programming issues of the GPU architecture including coalesced access of global memory, shared memory bank conflicts, etc. In particular, we present a very efficient method for random selection of next cities by a number of ants. Our new method uses iterative random trial which can find next cities in few computational costs with high probability. The experimental results on NVIDIA GeForce GTX 580 show that our implementation for 1002 cities runs in 8.71 seconds, while a conventional CPU implementation runs in 381.95 seconds. Thus, our GPU implementation attains a speed-up factor of 43.47. © 2012 IEEE.","Ant Colony Optimization; CUDA; GPU; Parallel processing; Traveling Salesman Problem","Ant Colony Optimization (ACO); Computational costs; CUDA; Efficient implementation; General purpose; GPU; GPU implementation; Graphics processing units; High probability; Off-chip; Parallel Computation; Parallel processing; Processing units; Random selection; Shared memories; Specialized microprocessor; Speed-up factors; Algorithms; Ant colony optimization; Artificial intelligence; Computer graphics; Iterative methods; Memory architecture; Program processors; Random access storage; Traveling salesman problem; Computer graphics equipment",Conference Paper,Scopus,2-s2.0-84874236718
"Mazahery A., Shabani M.O.","Assistance of novel artificial intelligence in optimization of aluminum matrix nanocomposite by genetic algorithm",2012,"Metallurgical and Materials Transactions A: Physical Metallurgy and Materials Science",27,10.1007/s11661-012-1339-6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870423564&doi=10.1007%2fs11661-012-1339-6&partnerID=40&md5=6050be662fc667f1793150edcd11d682","In this article, a genetic algorithm (GA) is used to predict the mechanical properties and to optimize the process conditions of Al nanocomposites. An artificial intelligence method is also implemented as an assisting tool for engineering tasks of GAs. The principle of the survival of the fittest is applied to produce successively superior approximations to a solution. A population of points at each iteration is generated. The population approaches an optimal solution. The next population by computations that involve random choices is selected. The optimal volume percentage of SiC, cooling rate, and temperature gradient are computed to be 2.84 pct, 283 K/s (10 °C/s), 1273 K/m (1000 °C/m), respectively. © 2012 The Minerals, Metals & Materials Society and ASM International.",,"Aluminum matrix; Artificial intelligence methods; Assisting tools; Cooling rates; Engineering tasks; Optimal solutions; Process condition; Random choice; Volume percentage; Aluminum; Artificial intelligence; Iterative methods; Mechanical properties; Nanocomposites; Optimization; Silicon carbide; Genetic algorithms",Article,Scopus,2-s2.0-84870423564
"Silva T.C., Zhao L.","Network-based stochastic semisupervised learning",2012,"IEEE Transactions on Neural Networks and Learning Systems",27,10.1109/TNNLS.2011.2181413,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873150054&doi=10.1109%2fTNNLS.2011.2181413&partnerID=40&md5=ea36854442fae52d630a8e38e3146786","Semisupervised learning is a machine learning approach that is able to employ both labeled and unlabeled samples in the training process. In this paper, we propose a semisupervised data classification model based on a combined random-preferential walk of particles in a network (graph) constructed from the input dataset. The particles of the same class cooperate among themselves, while the particles of different classes compete with each other to propagate class labels to the whole network. A rigorous model definition is provided via a nonlinear stochastic dynamical system and a mathematical analysis of its behavior is carried out. A numerical validation presented in this paper confirms the theoretical predictions. An interesting feature brought by the competitive-cooperative mechanism is that the proposed model can achieve good classification rates while exhibiting low computational complexity order in comparison to other network-based semisupervised algorithms. Computer simulations conducted on synthetic and real-world datasets reveal the effectiveness of the model. © 2012 IEEE.","Classification; complex networks; preferential walk; random walk; semisupervised learning; stochastic competitive learning","Data classification models; Low computational complexity; Machine learning approaches; Preferential walks; Random Walk; Semi- supervised learning; Stochastic competitive learning; Stochastic dynamical system; Classification (of information); Complex networks; Dynamical systems; Learning algorithms; Stochastic models; Stochastic systems; Supervised learning; Computer simulation; artificial intelligence; artificial neural network; automated pattern recognition; factual database; procedures; statistics; statistics and numerical data; Artificial Intelligence; Databases, Factual; Neural Networks (Computer); Pattern Recognition, Automated; Stochastic Processes",Article,Scopus,2-s2.0-84873150054
"Pillac V., Guéret C., Medaglia A.L.","An event-driven optimization framework for dynamic vehicle routing",2012,"Decision Support Systems",27,10.1016/j.dss.2012.06.007,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868669545&doi=10.1016%2fj.dss.2012.06.007&partnerID=40&md5=78b179f67548fc68e43458e132d301b1","The real-time operation of a fleet of vehicles introduces challenging optimization problems. In this work, we propose an event-driven framework that anticipates unknown changes arising in the context of dynamic vehicle routing. The framework is intrinsically parallelized to take advantage of modern multi-core and multi-threaded computing architectures. It is also designed to be easily embeddable in decision support systems that cope with a wide range of contexts and side constraints. We illustrate the flexibility of the framework by showing how it can be adapted to tackle the dynamic vehicle routing problem with stochastic demands. © 2012 Elsevier B.V. All rights reserved.","Dynamic vehicle routing; Event-driven framework; Multiple scenario approach; Online stochastic optimization; VRPSD","Dynamic Vehicle Routing; Event-driven framework; Scenario approach; Stochastic optimizations; VRPSD; Artificial intelligence; Computer architecture; Decision support systems; Fleet operations; Optimization",Article,Scopus,2-s2.0-84868669545
"Jarrassé N., Charalambous T., Burdet E.","A Framework to Describe, Analyze and Generate Interactive Motor Behaviors",2012,"PLoS ONE",27,10.1371/journal.pone.0049945,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870619582&doi=10.1371%2fjournal.pone.0049945&partnerID=40&md5=1e2e15224d5f0787349bbfacca6743f8","While motor interaction between a robot and a human, or between humans, has important implications for society as well as promising applications, little research has been devoted to its investigation. In particular, it is important to understand the different ways two agents can interact and generate suitable interactive behaviors. Towards this end, this paper introduces a framework for the description and implementation of interactive behaviors of two agents performing a joint motor task. A taxonomy of interactive behaviors is introduced, which can classify tasks and cost functions that represent the way each agent interacts. The role of an agent interacting during a motor task can be directly explained from the cost function this agent is minimizing and the task constraints. The novel framework is used to interpret and classify previous works on human-robot motor interaction. Its implementation power is demonstrated by simulating representative interactions of two humans. It also enables us to interpret and explain the role distribution and switching between roles when performing joint motor tasks. © 2012 Jarrasse et al.",,"article; classification algorithm; conceptual framework; cost minimization analysis; human computer interaction; interactive motor behavior; locomotion; mathematical computing; motor performance; process model; robotics; taxonomy; Algorithms; Artificial Intelligence; Cognition; Computer Simulation; Humans; Motor Activity; Robotics; Social Behavior; Task Performance and Analysis",Article,Scopus,2-s2.0-84870619582
"Parkes D.C., Xia L.","A complexity-of-strategic-behavior comparison between Schulze's rule and ranked pairs",2012,"Proceedings of the National Conference on Artificial Intelligence",27,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868266567&partnerID=40&md5=cd1712ffbd0d23dbe7829f0b9dc27ca6","Schulze's rule and ranked pairs are two Condorcet methods that both satisfy many natural axiomatic properties. Schulze's rule is used in the elections of many organizations, including the Wikimedia Foundation, the Pirate Party of Sweden and Germany, the Debian project, and the Gento Project. Both rules are immune to control by cloning alternatives, but little is otherwise known about their strategic robustness, including resistance to manipulation by one or more voters, control by adding or deleting alternatives, adding or deleting votes, and bribery. Considering computational barriers, we show that these types of strategic behavior are NP-hard for ranked pairs (both constructive, in making an alternative a winner, and destructive, in precluding an alternative from being a winner). Schulze's rule, in comparison, remains vulnerable at least to constructive manipulation by a single voter and destructive manipulation by a coalition. As the first such polynomial-time rule known to resist all such manipulations, and considering also the broad axiomatic support, ranked pairs seems worthwhile to consider for practical applications. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Computational barriers; NP-hard; Polynomial-time; Strategic Behavior; Genetic engineering; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868266567
"Pan W., Xiang E.W., Yang Q.","Transfer learning in collaborative filtering with uncertain ratings",2012,"Proceedings of the National Conference on Artificial Intelligence",27,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868271003&partnerID=40&md5=2eb390b284a30f8c9ece02ae70c398bb","To solve the sparsity problem in collaborative filtering, researchers have introduced transfer learning as a viable approach to make use of auxiliary data. Most previous transfer learning works in collaborative filtering have focused on exploiting point-wise ratings such as numerical ratings, stars, or binary ratings of likes/dislikes. However, in many real-world recommender systems, many users may be unwilling or unlikely to rate items with precision. In contrast, practitioners can turn to various non-preference data to estimate a range or rating distribution of a user's preference on an item. Such a range or rating distribution is called an uncertain rating since it represents a rating spectrum of uncertainty instead of an accurate point-wise score. In this paper, we propose an efficient transfer learning solution for collaborative filtering, known as transfer by integrative factorization (TIF), to leverage such auxiliary uncertain ratings to improve the performance of recommendation. In particular, we integrate auxiliary data of uncertain ratings as additional constraints in the target matrix factorization problem, and learn an expected rating value for each uncertain rating automatically. The advantages of our proposed approach include the efficiency and the improved effectiveness of collaborative filtering, showing that incorporating the auxiliary data of uncertain ratings can really bring a benefit. Experimental results on two movie recommendation tasks show that our TIF algorithm performs significantly better over a state-of-the-art non-transfer learning method. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Auxiliary data; Collaborative filtering; Learning methods; Numerical rating; Sparsity problems; Target matrices; Transfer learning; Artificial intelligence; Salinity measurement; Rating",Conference Paper,Scopus,2-s2.0-84868271003
"Sadilek A., Krumm J.","Far out: Predicting long-term human mobility",2012,"Proceedings of the National Conference on Artificial Intelligence",27,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868278154&partnerID=40&md5=da2f4a7aca9d1f6c842aebd98e72d07c","Much work has been done on predicting where is one going to be in the immediate future, typically within the next hour. By contrast, we address the open problem of predicting human mobility far into the future, a scale of months and years. We propose an efficient nonparametric method that extracts significant and robust patterns in location data, learns their associations with contextual features (such as day of week), and subsequently leverages this information to predict the most likely location at any given time in the future. The entire process is formulated in a principled way as an eigendecomposition problem. Evaluation on a massive dataset with more than 32,000 days worth of GPS data across 703 diverse subjects shows that our model predicts the correct location with high accuracy, even years into the future. This result opens a number of interesting avenues for future research and applications. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Contextual feature; Data sets; Eigen decomposition; GPS data; Human mobility; Location data; Nonparametric methods; Research and application; Robust patterns; Artificial intelligence; Forecasting",Conference Paper,Scopus,2-s2.0-84868278154
"Doǧan B., Korürek M.","A new ECG beat clustering method based on kernelized fuzzy c-means and hybrid ant colony optimization for continuous domains",2012,"Applied Soft Computing Journal",27,10.1016/j.asoc.2012.07.007,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865850542&doi=10.1016%2fj.asoc.2012.07.007&partnerID=40&md5=a66301d1e34f28392e8ff09d086cdc6f","The kernelized fuzzy c-means algorithm uses kernel methods to improve the clustering performance of the well known fuzzy c-means algorithm by mapping a given dataset into a higher dimensional space non-linearly. Thus, the newly obtained dataset is more likely to be linearly seprable. However, to further improve the clustering performance, an optimization method is required to overcome the drawbacks of the traditional algorithms such as, sensitivity to initialization, trapping into local minima and lack of prior knowledge for optimum paramaters of the kernel functions. In this paper, to overcome these drawbacks, a new clustering method based on kernelized fuzzy c-means algorithm and a recently proposed ant based optimization algorithm, hybrid ant colony optimization for continuous domains, is proposed. The proposed method is applied to a dataset which is obtained from MIT-BIH arrhythmia database. The dataset consists of six types of ECG beats including, Normal Beat (N), Premature Ventricular Contraction (PVC), Fusion of Ventricular and Normal Beat (F), Artrial Premature Beat (A), Right Bundle Branch Block Beat (R) and Fusion of Paced and Normal Beat (f). Four time domain features are extracted for each beat type and training and test sets are formed. After several experiments it is observed that the proposed method outperforms the traditional fuzzy c-means and kernelized fuzzy c-means algorithms. © 2012 Elsevier B.V. All rights reserved.","Ant colony optimization; Arrhythmia classification; ECG classification; ECG clustering; Fuzzy c-means; Kernelized fuzzy c-means; Swarm","Ant Colony Optimization (ACO); Arrhythmia classification; Clustering methods; Continuous domain; Data sets; Fuzzy C mean; Fuzzy C-means algorithms; Higher-dimensional; Hybrid ant colony optimization; Kernel function; Kernel methods; Local minimums; Optimization algorithms; Optimization method; Premature beats; Premature ventricular contraction; Prior knowledge; Swarm; Test sets; Time domain; Algorithms; Artificial intelligence; Copying; Diseases; Electrocardiography; Fuzzy clustering; Fuzzy systems; Time domain analysis; Cluster analysis",Article,Scopus,2-s2.0-84865850542
"Napierala K., Stefanowski J.","BRACID: A comprehensive approach to learning rules from imbalanced data",2012,"Journal of Intelligent Information Systems",27,10.1007/s10844-011-0193-0,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868623433&doi=10.1007%2fs10844-011-0193-0&partnerID=40&md5=2fa66cc8a2cc783a913ba025ee08839f","In this paper we consider induction of rule-based classifiers from imbalanced data, where one class (a minority class) is under-represented in comparison to the remaining majority classes. The minority class is usually of primary interest. However, most rule-based classifiers are biased towards the majority classes and they have difficulties with correct recognition of the minority class. In this paper we discuss sources of these difficulties related to data characteristics or to an algorithm itself. Among the problems related to the data distribution we focus on the role of small disjuncts, overlapping of classes and presence of noisy examples. Then, we show that standard techniques for induction of rule-based classifiers, such as sequential covering, top-down induction of rules or classification strategies, were created with the assumption of balanced data distribution, and we explain why they are biased towards the majority classes. Some modifications of rule-based classifiers have been already introduced, but they usually concentrate on individual problems. Therefore, we propose a novel algorithm, BRACID, which more comprehensively addresses the issues associated with imbalanced data. Its main characteristics includes a hybrid representation of rules and single examples, bottom-up learning of rules and a local classification strategy using nearest rules. The usefulness of BRACID has been evaluated in experiments on several imbalanced datasets. The results show that BRACID significantly outperforms the well known rule-based classifiers C4.5rules, RIPPER, PART, CN2, MODLEM as well as other related classifiers as RISE or K-NN. Moreover, it is comparable or better than the studied approaches specialized for imbalanced data such as generalizations of rule algorithms or combinations of SMOTE + ENN preprocessing with PART. Finally, it improves the support of minority class rules, leading to better recognition of the minority class examples. © 2012 Springer Science+Business Media, LLC.","Classifiers; Imbalanced data; Nearest neighbour paradigm; Nearest rules; Rule induction","Data characteristics; Data distribution; Imbalanced data; Imbalanced Data-sets; Learning rules; Nearest neighbour; Nearest rules; Novel algorithm; Rule induction; Rule-based classifier; Small disjuncts; Topdown; Under-represented; Artificial intelligence; Classifiers; Information systems; Algorithms",Article,Scopus,2-s2.0-84868623433
"Ghiassi M., Olschimke M., Moon B., Arnaudo P.","Automated text classification using a dynamic artificial neural network model",2012,"Expert Systems with Applications",27,10.1016/j.eswa.2012.03.027,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861198947&doi=10.1016%2fj.eswa.2012.03.027&partnerID=40&md5=12be756f92da19672e37bb24b31e84a8","Widespread digitization of information in today's internet age has intensified the need for effective textual document classification algorithms. Most real life classification problems, including text classification, genetic classification, medical classification, and others, are complex in nature and are characterized by high dimensionality. Current solution strategies include Naïve Bayes (NB), Neural Network (NN), Linear Least Squares Fit (LLSF), k-Nearest-Neighbor (kNN), and Support Vector Machines (SVM); with SVMs showing better results in most cases. In this paper we introduce a new approach called dynamic architecture for artificial neural networks (DAN2) as an alternative for solving textual document classification problems. DAN2 is a scalable algorithm that does not require parameter settings or network architecture configuration. To show DAN2 as an effective and scalable alternative for text classification, we present comparative results for the Reuters-21578 benchmark dataset. Our results show DAN2 to perform very well against the current leading solutions (kNN and SVM) using established classification metrics. © 2012 Elsevier Ltd. All rights reserved.","Artificial intelligence; Classification; Dynamic artificial neural networks; Machine learning; Pattern recognition; Textual document classification","Architecture configuration; Artificial neural network models; Benchmark datasets; Dynamic architecture; Genetic classification; High dimensionality; K-nearest neighbors; Linear least squares; Medical classification; OR-networks; Parameter setting; Real life classification; Reuters-21578; Scalable algorithms; Solution strategy; Text classification; Textual documents; Algorithms; Artificial intelligence; Classification (of information); Learning systems; Medical problems; Network architecture; Neural networks; Pattern recognition; Support vector machines; Information retrieval systems",Article,Scopus,2-s2.0-84861198947
"Liu X.","Sensor deployment of wireless sensor networks based on ant colony optimization with three classes of ant transitions",2012,"IEEE Communications Letters",27,10.1109/LCOMM.2012.090312.120977,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867879174&doi=10.1109%2fLCOMM.2012.090312.120977&partnerID=40&md5=fc82fb8852b2bc1770d9c1e2ce12365e","The problem of minimum-cost and connectivity-guaranteed grid coverage (MCGC) is one of the most critical issues for the implementation of wireless sensor networks (WSNs). In this paper, a novel algorithm, ant colony optimization with three classes of ant transitions (ACO-TCAT) is proposed to decrease inferior solutions and narrow the searching range of the algorithm and finally to solve this problem. Simulation results are conducted to demonstrate the effectiveness of our proposed approach. © 2012 IEEE.","ant colony optimization; sensor deployment; three classes of ant transitions; Wireless sensor networks","Ant Colony Optimization (ACO); Critical issues; Grid coverage; Novel algorithm; Sensor deployment; Wireless sensor network (WSNs); Artificial intelligence; Sensors; Wireless sensor networks; Algorithms",Article,Scopus,2-s2.0-84867879174
"Powell W.B., George A., Simão H., Scott W., Lamont A., Stewart J.","SMART: A stochastic multiscale model for the analysis of energy resources, technology, and policy",2012,"INFORMS Journal on Computing",27,10.1287/ijoc.1110.0470,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873152819&doi=10.1287%2fijoc.1110.0470&partnerID=40&md5=4306443b4a2f398fc97e49164126e92f","We address the problem of modeling energy resource allocation, including dispatch, storage, and the longterm investments in new technologies, capturing different sources of uncertainty such as energy from wind, demands, prices, and rainfall. We also wish to model long-term investment decisions in the presence of uncertainty. Accurately modeling the value of all investments, such as wind turbines and solar panels, requires handling fine-grained temporal variability and uncertainty in wind and solar in the presence of storage. We propose a modeling and algorithmic strategy based on the framework of approximate dynamic programming (ADP) that can model these problems at hourly time increments over an entire year or several decades. We demonstrate the methodology using both spatially aggregate and disaggregate representations of energy supply and demand. This paper describes the initial proof of concept experiments for an ADP-based model called SMART; we describe the modeling and algorithmic strategy and provide comparisons against a deterministic benchmark as well as initial experiments on stochastic data sets. © 2012 INFORMS.","Analysis of algorithms; Artificial intelligence; Queues; Simulation; Statistical analysis","Analysis of algorithms; Approximate dynamic programming; Energy supplies; Long-term investment; Multiscale models; Proof of concept; Queues; Simulation; Solar panels; Sources of uncertainty; Stochastic data; Temporal variability; Time increments; Algorithms; Artificial intelligence; Economics; Energy resources; Experiments; Investments; Statistical methods; Stochastic models; Stochastic systems; Uncertainty analysis",Article,Scopus,2-s2.0-84873152819
"Mora A.M., Fernández-Ares A., Merelo J.J., García-Sánchez P., Fernandes C.M.","Effect of noisy fitness in real-time strategy games player behaviour optimisation using evolutionary algorithms",2012,"Journal of Computer Science and Technology",27,10.1007/s11390-012-1281-5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870951132&doi=10.1007%2fs11390-012-1281-5&partnerID=40&md5=39923e208a98c67ceda82cb6ef89f113","This paper investigates the performance and the results of an evolutionary algorithm (EA) specifically designed for evolving the decision engine of a program (which, in this context, is called bot) that plays Planet Wars. This game, which was chosen for the Google Artificial Intelligence Challenge in 2010, requires the bot to deal with multiple target planets, while achieving a certain degree of adaptability in order to defeat different opponents in different scenarios. The decision engine of the bot is initially based on a set of rules that have been defined after an empirical study, and a genetic algorithm (GA) is used for tuning the set of constants, weights and probabilities that those rules include, and therefore, the general behaviour of the bot. Then, the bot is supplied with the evolved decision engine and the results obtained when competing with other bots (a bot offered by Google as a sparring partner, and a scripted bot with a pre-established behaviour) are thoroughly analysed. The evaluation of the candidate solutions is based on the result of non-deterministic battles (and environmental interactions) against other bots, whose outcome depends on random draws as well as on the opponents' actions. Therefore, the proposed GA is dealing with a noisy fitness function. After analysing the effects of the noisy fitness, we conclude that tackling randomness via repeated combats and reevaluations reduces this effect and makes the GA a highly valuable approach for solving this problem. © 2012 Springer Science+Business Media, LLC & Science Press, China.","Genetic algorithm; Noisy fitness; Parameter adaptation; Player behaviour optimisation; Real-time strategy game","Candidate solution; Decision engines; Empirical studies; Environmental interactions; Fitness functions; Multiple targets; Noisy fitness; Optimisations; Parameter adaptation; Real-time strategy games; Set of rules; Sparring Partners; Artificial intelligence; Genetic algorithms; Health; Optimization; Real time systems; Behavioral research",Article,Scopus,2-s2.0-84870951132
"Rahmati P., Adler A., Hamarneh G.","Mammography segmentation with maximum likelihood active contours",2012,"Medical Image Analysis",27,10.1016/j.media.2012.05.005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866081542&doi=10.1016%2fj.media.2012.05.005&partnerID=40&md5=584b05ce4962bc255a252a5c5354f89a","We present a computer-aided approach to segmenting suspicious lesions in digital mammograms, based on a novel maximum likelihood active contour model using level sets (MLACMLS). The algorithm estimates the segmentation contour that best separates the lesion from the background using the Gamma distribution to model the intensity of both regions (foreground and background). The Gamma distribution parameters are estimated by the algorithm. We evaluate the performance of MLACMLS on real mammographic images. Our results are compared to those of two leading related methods: The adaptive level set-based segmentation method (ALSSM) and the spiculation segmentation using level sets (SSLS) approach, and show higher segmentation accuracy (MLACMLS: 86.85% vs. ALSSM: 74.32% and SSLS: 57.11%). Moreover, our results are qualitatively compared with those of the Active Contour Without Edge (ACWOE) and show a better performance. Further, the suitability of using ML as the objective function as opposed to the KL divergence and to the energy functional of the ACWOE is also demonstrated. Our algorithm is also shown to be robust to the selection of a required single seed point. © 2012 Elsevier B.V.","Active contour models; Computer-aided diagnosis; Level sets; Mammography; Maximum likelihood","Active contour model; Active contours; Digital mammograms; Energy functionals; Gamma distribution; KL-divergence; Level Set; Mammographic images; Objective functions; Seed point; Segmentation accuracy; Segmentation methods; Algorithms; Computer aided diagnosis; Grain size and shape; Image segmentation; Mammography; Maximum likelihood; X ray screens; Numerical methods; active contour without edge; adaptive level set based segmentation method; algorithm; article; computer assisted radiography; controlled study; image analysis; interferometry; mammography; maximum likelihood active contour model; maximum likelihood method; performance; priority journal; spiculation segmentation using level set; Algorithms; Artificial Intelligence; Breast Neoplasms; Female; Humans; Likelihood Functions; Mammography; Models, Biological; Models, Statistical; Pattern Recognition, Automated; Radiographic Image Enhancement; Radiographic Image Interpretation, Computer-Assisted; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84866081542
"Ramos-Pollán R., Guevara-López M.A., Suárez-Ortega C., Díaz-Herrero G., Franco-Valiente J.M., Rubio-Del-Solar M., González-De-Posada N., Vaz M.A.P., Loureiro J., Ramos I.","Discovering mammography-based machine learning classifiers for breast cancer diagnosis",2012,"Journal of Medical Systems",27,10.1007/s10916-011-9693-2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873050658&doi=10.1007%2fs10916-011-9693-2&partnerID=40&md5=777e3a59c89cc65f4b7456dad34a2765","This work explores the design of mammography-based machine learning classifiers (MLC) and proposes a new method to build MLC for breast cancer diagnosis. We massively evaluated MLC configurations to classify features vectors extracted from segmented regions (pathological lesion or normal tissue) on craniocaudal (CC) and/or mediolateral oblique (MLO) mammography image views, providing BI-RADS diagnosis. Previously, appropriate combinations of image processing and normalization techniques were applied to reduce image artifacts and increase mammograms details. The method can be used under different data acquisition circumstances and exploits computer clusters to select well performing MLC configurations. We evaluated 286 cases extracted from the repository owned by HSJ-FMUP, where specialized radiologists segmented regions on CC and/or MLO images (biopsies provided the golden standard). Around 20,000 MLC configurations were evaluated, obtaining classifiers achieving an area under the ROC curve of 0.996 when combining features vectors extracted from CC and MLO views of the same case. © 2011 Springer Science+Business Media, LLC.","Breast cancer CAD; Machine learning classifiers; Mammography classifiers","article; artifact reduction; breast cancer; classifier; cluster analysis; computer assisted diagnosis; human; image analysis; image processing; major clinical study; mammography; receiver operating characteristic; artificial intelligence; breast tumor; classification; computer assisted diagnosis; computer system; female; instrumentation; mammography; methodology; Artificial Intelligence; Breast Neoplasms; Computer Systems; Diagnosis, Computer-Assisted; Female; Humans; Image Interpretation, Computer-Assisted; Mammography",Article,Scopus,2-s2.0-84873050658
"Abdullah A., Deris S., Mohamad M.S., Hashim S.Z.M.","A new hybrid firefly algorithm for complex and nonlinear problem",2012,"Advances in Intelligent and Soft Computing",27,10.1007/978-3-642-28765-7_81,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864320135&doi=10.1007%2f978-3-642-28765-7_81&partnerID=40&md5=33ceef09d85f5eb6e8e60957bcb4d5e5","Global optimization methods play an important role to solve many real-world problems. However, the implementation of single methods is excessively preventive for high dimensionality and nonlinear problems, especially in term of the accuracy of finding best solutions and convergence speed performance. In recent years, hybrid optimization methods have shown potential achievements to overcome such challenges. In this paper, a new hybrid optimization method called Hybrid Evolutionary Firefly Algorithm (HEFA) is proposed. The method combines the standard Firefly Algorithm (FA) with the evolutionary operations of Differential Evolution (DE) method to improve the searching accuracy and information sharing among the fireflies. The HEFA method is used to estimate the parameters in a complex and nonlinear biological model to address its effectiveness in high dimensional and nonlinear problem. Experimental results showed that the accuracy of finding the best solution and convergence speed performance of the proposed method is significantly better compared to those achieved by the existing methods. © 2012 Springer-Verlag.","biological model; Differential Evolution; Firefly Algorithm; hybrid optimization; parameter estimation","Biological models; Convergence speed; Differential Evolution; Evolutionary operations; Firefly algorithms; Global optimization method; High dimensionality; High-dimensional; Hybrid optimization; Hybrid optimization method; Information sharing; Nonlinear problems; Real-world problem; Artificial intelligence; Evolutionary algorithms; Fire protection; Global optimization; Parameter estimation; Bioluminescence",Conference Paper,Scopus,2-s2.0-84864320135
"Dammak M., Mejdoub M., Zaied M., Amar C.B.","Feature vector approximation based on wavelet network",2012,"ICAART 2012 - Proceedings of the 4th International Conference on Agents and Artificial Intelligence",27,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862149161&partnerID=40&md5=c34bc1b27668202244e13156dc78fe29","Image classification is an important task in computer vision. In this paper, we propose a new image representation based on local feature vectors approximation by the wavelet networks. To extract an approximation of the feature vectors space, a Wavelet Network algorithm based on fast Wavelet is suggested. Then, the K-nearest neighbor (K-NN) classification algorithm is applied on the approximated feature vectors. The approximation of the feature space ameliorates the feature vector classification accuracy.","Approximation; Bag of words; Local feature; Wavelet network","Approximation; Bag of words; Classification accuracy; Classification algorithm; Feature space; Feature vectors; Image representations; K-nearest neighbors; Local feature; Local feature vectors; Wavelet network; Artificial intelligence; Computer vision; Face recognition; Vector spaces; Approximation algorithms",Conference Paper,Scopus,2-s2.0-84862149161
"Muise C., McIlraith S.A., Beck J.C., Hsu E.I.","DSHARP: Fast d-DNNF compilation with sharpSAT",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",27,10.1007/978-3-642-30353-1_36,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861754443&doi=10.1007%2f978-3-642-30353-1_36&partnerID=40&md5=a0ec702d062abc09a9ad9663b06e5b0e","Knowledge compilation is a compelling technique for dealing with the intractability of propositional reasoning. One particularly effective target language is Deterministic Decomposable Negation Normal Form (d-DNNF). We exploit recent advances in #SAT solving in order to produce a new state-of-the-art CNF → d-DNNF compiler: Dsharp. Empirical results demonstrate that Dsharp is generally an order of magnitude faster than c2d, the de facto standard for compiling to d-DNNF, while yielding a representation of comparable size. © 2012 Springer-Verlag.",,"De facto standard; Knowledge compilation; Normal form; SAT-solving; Target language; Program compilers; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84861754443
"Böhl F., Hofheinz D., Kraschewski D.","On definitions of selective opening security",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",27,10.1007/978-3-642-30057-8_31,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861708459&doi=10.1007%2f978-3-642-30057-8_31&partnerID=40&md5=5fc36365226d14a6e500134494c11851","Assume that an adversary observes many ciphertexts, and may then ask for openings, i.e. the plaintext and the randomness used for encryption, of some of them. Do the unopened ciphertexts remain secure? There are several ways to formalize this question, and the ensuing security notions are not known to be implied by standard notions of encryption security. In this work, we relate the two existing flavors of selective opening security. Our main result is that indistinguishability-based selective opening security and simulation-based selective opening security do not imply each other. We show our claims by counterexamples. Concretely, we construct two public-key encryption schemes. One scheme is secure under selective openings in a simulation-based sense, but not in an indistinguishability-based sense. The other scheme is secure in an indistinguishability-based sense, but not in a simulation-based sense. Our results settle an open question of Bellare et al. (Eurocrypt 2009). Also, taken together with known results about selective opening secure encryption, we get an almost complete picture how the two flavors of selective opening security relate to standard security notions. © 2012 International Association for Cryptologic Research.","public-key encryption; security definitions; selective opening security","Ciphertexts; Encryption security; Plaintext; Public-key encryption; Public-key encryption scheme; Security definitions; Security notion; selective opening security; Simulation-based; Ciphertexts; Encryption security; Indistinguishability; Public-key encryption; Public-key encryption scheme; Security definitions; Security notion; Selective opening security; Artificial intelligence; Public key cryptography; Security of data; Public key cryptography; Cryptography",Conference Paper,Scopus,2-s2.0-84861708459
"Zhang H., Zhu Y., Zou W., Yan X.","A hybrid multi-objective artificial bee colony algorithm for burdening optimization of copper strip production",2012,"Applied Mathematical Modelling",27,10.1016/j.apm.2011.09.041,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857035380&doi=10.1016%2fj.apm.2011.09.041&partnerID=40&md5=ddaa2bc7296bcf5a5750aa40c89d7198","To achieve burdening process optimization of copper strips effectively, a nonlinear constrained multi-objective model is established on the principle of the actual burdening. The problem is formulated with two objectives of minimizing the total cost of raw materials and maximizing the amount of waste material thrown into melting furnace. In this paper, a novel approach called ""hybrid multi-objective artificial bee colony"" (HMOABC) to solve this model is proposed. The HMOABC algorithm is new swarm intelligence based multi-objective optimization technique inspired by the intelligent foraging behavior of honey bees, summation of normalized objective values and diversified selection (SNOV-DS) and nondominated sorting approach. Two test examples were studied and the performance of HMOABC is evaluated in comparison with other nature inspired techniques which includes nondominated sorting genetic algorithm II (NSGAII) and multi-objective particle swarm optimization (MOPSO). The numerical results demonstrate HMOABC approach is a powerful search and optimization technique for burdening optimization of copper strips. © 2011 Elsevier Inc.","Artificial bee colony (ABC); Burdening optimization; Copper strip production; Hybrid multi-objective artificial bee colony (HMOABC); Multi-objective optimization","Artificial bee colonies; Copper strip production; Copper strips; Foraging behaviors; Honey bee; Multi objective; Multi objective particle swarm optimization; Multi-objective optimization techniques; Multiobjective models; Non-dominated Sorting; Non-dominated sorting genetic algorithm - ii; NSGA-II; Numerical results; Optimization techniques; Swarm Intelligence; Test examples; Total costs; Artificial intelligence; Cellular automata; Constrained optimization; Copper; Multiobjective optimization",Article,Scopus,2-s2.0-84857035380
"Kussul N., Mandl D., Moe K., Mund J.-P., Post J., Shelestov A., Skakun S., Szarzynski J., Van Langenhove G., Handy M.","Interoperable infrastructure for flood monitoring: SensorWeb, grid and cloud",2012,"IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing",27,10.1109/JSTARS.2012.2192417,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872066827&doi=10.1109%2fJSTARS.2012.2192417&partnerID=40&md5=199a10b21407127f4cb2dd1e229f99d3","The paper presents an international multi-disciplinary initiative, a Namibia SensorWeb Pilot Project, that was created as a testbed for evaluating and prototyping key technologies for rapid acquisition and distribution of data products for decision support systems to monitor floods. Those key technologies include SensorWebs, Grids and Computation Clouds. This pilot project aims at developing an operational trans-boundary flood management decision support system for the Southern African region to provide useful flood and water-borne disease forecasting tools for local decision makers. This effort integrates space-based and ground sensor data along with higher level geospatial data products to enable risk assessment and ultimately risk maps related to flood disaster management and water-related disease management. We present an overall architecture of the Pilot along with components and services being developed. Additionally, case-studies and results achieved so far are discussed. The presented work is being carried out within GEO 2009-2011 Work Plan as CEOS WGISS contribution. © 2008-2012 IEEE.","cloud computing; Earth observation; floods; GEOSS; Grid; remote sensing; risk analysis; sensor web","Case-studies; Data products; Earth observations; Flood disaster management; Flood management; Flood monitoring; Geo-spatial data; GEOSS; Grid; Ground sensors; Key technologies; Local decisions; Multi-disciplinary; Namibia; Pilot projects; Rapid acquisition; Risk maps; Sensor web; Space-based; Trans-boundary; Water-borne disease; Water-related disease; Work plan; Artificial intelligence; Cloud computing; Decision support systems; Disaster prevention; Flood control; Maps; Remote sensing; Risk analysis; Floods; decision support system; disaster management; EOS; flood; forecasting method; monitoring system; remote sensing; risk assessment; waterborne disease; Namibia",Article,Scopus,2-s2.0-84872066827
"Stöttinger J., Hanbury A., Sebe N., Gevers T.","Sparse color interest points for image retrieval and object categorization",2012,"IEEE Transactions on Image Processing",27,10.1109/TIP.2012.2186143,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860112203&doi=10.1109%2fTIP.2012.2186143&partnerID=40&md5=683c25ad98e960fcd68833e82226864c","Interest point detection is an important research area in the field of image processing and computer vision. In particular, image retrieval and object categorization heavily rely on interest point detection from which local image descriptors are computed for image matching. In general, interest points are based on luminance, and color has been largely ignored. However, the use of color increases the distinctiveness of interest points. The use of color may therefore provide selective search reducing the total number of interest points used for image matching. This paper proposes color interest points for sparse image representation. To reduce the sensitivity to varying imaging conditions, light-invariant interest points are introduced. Color statistics based on occurrence probability lead to color boosted points, which are obtained through saliency-based feature selection. Furthermore, a principal component analysis-based scale selection method is proposed, which gives a robust scale estimation per interest point. From large-scale experiments, it is shown that the proposed color interest point detector has higher repeatability than a luminance-based one. Furthermore, in the context of image retrieval, a reduced and predictable number of color features show an increase in performance compared to state-of-the-art interest points. Finally, in the context of object recognition, for the Pascal VOC 2007 challenge, our method gives comparable performance to state-of-the-art methods using only a small fraction of the features, reducing the computing time considerably. © 1992-2012 IEEE.","ARS-IIU; color invariance; ELI-COL; image retrieval; local features; object categorization; SMR-REP","ARS-IIU; ELI-COL; Local feature; Object categorization; SMR-REP; Computer vision; Detectors; Image matching; Image retrieval; Object recognition; Principal component analysis; Color; algorithm; article; artificial intelligence; automated pattern recognition; color; colorimetry; computer assisted diagnosis; image enhancement; methodology; reproducibility; sensitivity and specificity; Algorithms; Artificial Intelligence; Color; Colorimetry; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84860112203
"Türkan M., Guillemot C.","Image prediction based on neighbor-embedding methods",2012,"IEEE Transactions on Image Processing",27,10.1109/TIP.2011.2170700,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859042806&doi=10.1109%2fTIP.2011.2170700&partnerID=40&md5=081cb44e632efbdd3d1eb27f122bae59","This paper describes two new intraimage prediction methods based on two data dimensionality reduction methods: nonnegative matrix factorization (NMF) and locally linear embedding. These two methods aim at approximating a block to be predicted in the image as a linear combination of k-nearest neighbors determined on the known pixels in a causal neighborhood of the input block. Variable $k$ can be seen as a parameter controlling some sort of sparsity constraints of the approximation vector. The impact of this parameter as well as of the nonnegativity and sum-to-one constraints for the addressed prediction problem has been analyzed. The prediction and RD performances of these two new image prediction methods have then been evaluated in a complete image coding-and-decoding algorithm. Simulation results show gains up to 2 dB in terms of the PSNR of the reconstructed signal after coding and decoding of the prediction residue when compared with H.264/AVC intraprediction modes, up to 3 dB when compared with template matching, and up to 1 dB when compared with a sparse prediction method. © 2011 IEEE.","Image compression; image prediction; locally linear embedding (LLE); nonnegative matrix factorization (NMF); sparse prediction (SP); template matching (TM)","Coding and decoding; Data dimensionality reduction; H.264/AVC; Image prediction; Intraprediction; K-nearest neighbors; Linear combinations; Locally linear embedding; Non-negativity; Nonnegative matrix factorization; Prediction methods; Prediction problem; sparse prediction (SP); Sparsity constraints; Decoding; Image coding; Image compression; Motion Picture Experts Group standards; Template matching; Forecasting; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; image enhancement; image subtraction; methodology; reproducibility; sensitivity and specificity; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-84859042806
"Smith S., Smith G., Chen W.-H.","Disassembly sequence structure graphs: An optimal approach for multiple-target selective disassembly sequence planning",2012,"Advanced Engineering Informatics",27,10.1016/j.aei.2011.11.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859626393&doi=10.1016%2fj.aei.2011.11.003&partnerID=40&md5=d25540a479f04fce9ed2ea430f5042f1","Modern green products must be easy to disassemble. Specific target components must be accessed and removed for repair, reuse, recycling, or remanufacturing. Prior studies describe various methods for removing selective targets from a product. However, solution quality, model complexity, and searching time have not been considered thoroughly. The goal of this study is to improve solution quality, minimize model complexity, and reduce searching time. To achieve the goal, this study introduces a new 'disassembly sequence structure graph' (DSSG) model for multiple-target selective disassembly sequence planning, an approach for creating DSSGs, and methods for searching DSSGs. The DSSG model contains a minimum set of parts that must be removed to remove selected targets, with an order and direction for removing each part. The approach uses expert rules to choose parts, part order, and part disassembly directions, based upon physical constraints. The searching methods use rules to remove all parts, in order, from the DSSG. The DSSG approach is an optimal approach. The approach creates a high quality minimum-size model, in minimum time. The approach finds high quality, practical, realistic, physically feasible solutions, in minimum time. The solutions are optimized for number of removed parts, part order, part disassembly directions, and reorientations. The solutions remove parts in practical order. The solutions remove parts in realistic directions. The solutions consider contact, motion, and fastener constraints. The study also presents eight new design rules. The study results can be used to improve the product design process, increase product life-cycle quality, and reduce product environmental impact. © 2012 Elsevier Ltd. All rights reserved.","Disassembly sequence structure graph; DSSG; Multiple target; Optimal; Selective disassembly sequence planning","Disassembly sequence; DSSG; Multiple targets; Optimal; Selective-disassembly; Artificial intelligence; Information systems; Optimization",Article,Scopus,2-s2.0-84859626393
"Erdem A.S., Göen E.","Development of a decision support system for supplier evaluation and order allocation",2012,"Expert Systems with Applications",27,10.1016/j.eswa.2011.10.024,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855879730&doi=10.1016%2fj.eswa.2011.10.024&partnerID=40&md5=3ba5cae866fa9b5ce3bcd9fd9fbe0651","This study aims to develop models and generate a decision support system (DSS) for the improvement of supplier evaluation and order allocation decisions in a supply chain. Supplier evaluation and order allocation are complex, multi criteria decisions. Initially, an analytic hierarchy process (AHP) model is developed for qualitative and quantitative evaluation of suppliers. Based on these evaluations, a goal programming (GP) model is developed for order allocation among suppliers. The models are integrated into a DSS that provides a dynamic, flexible and fast decision making environment. The DSS environment is tested at the purchasing department of a manufacturer and feedbacks are obtained. © 2011 Elsevier Ltd. All rights reserved.","Analytic hierarchy process; Decision support systems; Goal programming; Order allocation; Supplier evaluation","Decision supports; Goal programming; Multicriteria decision; Order allocation; Purchasing department; Quantitative evaluation; Supplier Evaluations; Analytic hierarchy process; Artificial intelligence; Decision support systems; Purchasing; Supply chains; Quality control",Article,Scopus,2-s2.0-84855879730
"Lan Y., Wang Q., Cole J.R., Rosen G.L.","Using the RDP classifier to predict taxonomic novelty and reduce the search space for finding novel organisms",2012,"PLoS ONE",27,10.1371/journal.pone.0032491,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863288435&doi=10.1371%2fjournal.pone.0032491&partnerID=40&md5=9284ca92f99a16f8d2b56ed9ecf663ad","Background: Currently, the naïve Bayesian classifier provided by the Ribosomal Database Project (RDP) is one of the most widely used tools to classify 16S rRNA sequences, mainly collected from environmental samples. We show that RDP has 97+% assignment accuracy and is fast for 250 bp and longer reads when the read originates from a taxon known to the database. Because most environmental samples will contain organisms from taxa whose 16S rRNA genes have not been previously sequenced, we aim to benchmark how well the RDP classifier and other competing methods can discriminate these novel taxa from known taxa. Principal Findings: Because each fragment is assigned a score (containing likelihood or confidence information such as the boostrap score in the RDP classifier), we ""train"" a threshold to discriminate between novel and known organisms and observe its performance on a test set. The threshold that we determine tends to be conservative (low sensitivity but high specificity) for naïve Bayesian methods. Nonetheless, our method performs better with the RDP classifier than the other methods tested, measured by the f-measure and the area-under-the-curve on the receiver operating characteristic of the test set. By constraining the database to well-represented genera, sensitivity improves 3-15%. Finally, we show that the detector is a good predictor to determine novel abundant taxa (especially for finer levels of taxonomy where novelty is more likely to be present). Conclusions: We conclude that selecting a read-length appropriate RDP bootstrap score can significantly reduce the search space for identifying novel genera and higher levels in taxonomy. In addition, having a well-represented database significantly improves performance while having genera that are ""highly"" similar does not make a significant improvement. On a real dataset from an Amazon Terra Preta soil sample, we show that the detector can predict (or correlates to) whether novel sequences will be assigned to new taxa when the RDP database ""doubles"" in the future. © 2012 Lan et al.",,"RNA 16S; accuracy; article; Bayes theorem; bootstrapping; classifier; genus; intermethod comparison; prediction; quality control; receiver operating characteristic; ribosomal database project classifier; scoring system; sensitivity and specificity; sequence analysis; sequence database; sequence homology; species difference; taxonomic identification; artificial intelligence; base pairing; classification; discriminant analysis; genetic database; genetics; metagenomics; methodology; ribosome; soil; statistical model; time; Artificial Intelligence; Base Pairing; Classification; Databases, Genetic; Discriminant Analysis; Likelihood Functions; Metagenomics; Ribosomes; ROC Curve; Soil; Time Factors",Article,Scopus,2-s2.0-84863288435
"Ramríez-Arias A., Rodríguez F., Guzmán J.L., Berenguel M.","Multiobjective hierarchical control architecture for greenhouse crop growth",2012,"Automatica",27,10.1016/j.automatica.2012.01.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857789317&doi=10.1016%2fj.automatica.2012.01.002&partnerID=40&md5=126dbc0537aaea114b80a67ed189ba87","The problem of determining the trajectories to control greenhouse crop growth has traditionally been solved by using constrained optimization or applying artificial intelligence techniques. The economic profit has been used as the main criterion in most research on optimization to obtain adequate climatic control setpoints for the crop growth. This paper addresses the problem of greenhouse crop growth through a hierarchical control architecture governed by a high-level multiobjective optimization approach, where the solution to this problem is to find reference trajectories for diurnal and nocturnal temperatures (climate-related setpoints) and electrical conductivity (fertirrigation-related setpoints). The objectives are to maximize profit, fruit quality, and water-use efficiency, these being currently fostered by international rules. Illustrative results selected from those obtained in an industrial greenhouse during the last eight years are shown and described. © 2012 Elsevier Ltd. All rights reserved.","Agriculture; Hierarchical systems; Optimization methods; Process control; Yield optimization","Artificial intelligence techniques; Climatic controls; Crop growth; Economic profit; Electrical conductivity; Fruit quality; Greenhouse crops; Hierarchical control architecture; International rules; Multi objective; Nocturnal temperature; Optimization methods; Reference trajectories; Setpoints; Water use efficiency; Yield optimization; Agriculture; Artificial intelligence; Constrained optimization; Crops; Electric conductivity; Hierarchical systems; Multiobjective optimization; Process control; Profitability; Greenhouses",Article,Scopus,2-s2.0-84857789317
"Yuan Z., Montes de Oca M.A., Birattari M., Stützle T.","Continuous optimization algorithms for tuning real and integer parameters of swarm intelligence algorithms",2012,"Swarm Intelligence",27,10.1007/s11721-011-0065-9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855314827&doi=10.1007%2fs11721-011-0065-9&partnerID=40&md5=d97defcd386445fb4a1fc7cc0bda2417","The performance of optimization algorithms, including those based on swarm intelligence, depends on the values assigned to their parameters. To obtain high performance, these parameters must be fine-tuned. Since many parameters can take real values or integer values from a large domain, it is often possible to treat the tuning problem as a continuous optimization problem. In this article, we study the performance of a number of prominent continuous optimization algorithms for parameter tuning using various case studies from the swarm intelligence literature. The continuous optimization algorithms that we study are enhanced to handle the stochastic nature of the tuning problem. In particular, we introduce a new post-selection mechanism that uses F-Race in the final phase of the tuning process to select the best among elite parameter configurations. We also examine the parameter space of the swarm intelligence algorithms that we consider in our study, and we show that by fine-tuning their parameters one can obtain substantial improvements over default configurations. © 2011 Springer Science + Business Media, LLC.","Automated algorithm configuration; Continuous optimization algorithm; F-Race; Parameter tuning; Swarm intelligence","Automated algorithms; Continuous optimization algorithm; F-Race; Parameter-tuning; Swarm Intelligence; Algorithms; Artificial intelligence; Cellular automata; Optimization; Parameter estimation",Article,Scopus,2-s2.0-84855314827
"Goodarzi M., Jensen R., Vander Heyden Y.","QSRR modeling for diverse drugs using different feature selection methods coupled with linear and nonlinear regressions",2012,"Journal of Chromatography B: Analytical Technologies in the Biomedical and Life Sciences",27,10.1016/j.jchromb.2012.01.012,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870238114&doi=10.1016%2fj.jchromb.2012.01.012&partnerID=40&md5=2aae6933ccd3f923896b6af7c7b11d76","A Quantitative Structure-Retention Relationship (QSRR) is proposed to estimate the chromatographic retention of 83 diverse drugs on a Unisphere poly butadiene (PBD) column, using isocratic elutions at pH 11.7. Previous work has generated QSRR models for them using Classification And Regression Trees (CART). In this work, Ant Colony Optimization is used as a feature selection method to find the best molecular descriptors from a large pool. In addition, several other selection methods have been applied, such as Genetic Algorithms, Stepwise Regression and the Relief method, not only to evaluate Ant Colony Optimization as a feature selection method but also to investigate its ability to find the important descriptors in QSRR. Multiple Linear Regression (MLR) and Support Vector Machines (SVMs) were applied as linear and nonlinear regression methods, respectively, giving excellent correlation between the experimental, i.e. extrapolated to a mobile phase consisting of pure water, and predicted logarithms of the retention factors of the drugs (logkw). The overall best model was the SVM one built using descriptors selected by ACO. © 2012 Elsevier B.V.","ACO; Chromatographic retention; MLR; QSRR; Relief method; SVM","ACO; Chromatographic retention; MLR; QSRR; Relief method; SVM; Algorithms; Artificial intelligence; Butadiene; Chromatography; Linear regression; Support vector machines; water; ant colony optimization; article; correlation analysis; cost effectiveness analysis; elution; genetic algorithm; multiple linear regression analysis; pH; principal component analysis; priority journal; process optimization; quantitative structure retention relationship; structure activity relation; support vector machine; Algorithms; Chromatography, Liquid; Models, Theoretical; Pharmaceutical Preparations; Quantitative Structure-Activity Relationship; Regression Analysis",Article,Scopus,2-s2.0-84870238114
"Tsai C.-C., Lin H.-Y., Taur J., Tao C.-W.","Iris recognition using possibilistic fuzzy matching on local features",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",27,10.1109/TSMCB.2011.2163817,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856297758&doi=10.1109%2fTSMCB.2011.2163817&partnerID=40&md5=6ca385b024b513aea767abfdc33302b7","In this paper, we propose a novel possibilistic fuzzy matching strategy with invariant properties, which can provide a robust and effective matching scheme for two sets of iris feature points. In addition, the nonlinear normalization model is adopted to provide more accurate position before matching. Moreover, an effective iris segmentation method is proposed to refine the detected inner and outer boundaries to smooth curves. For feature extraction, the Gabor filters are adopted to detect the local feature points from the segmented iris image in the Cartesian coordinate system and to generate a rotation-invariant descriptor for each detected point. After that, the proposed matching algorithm is used to compute a similarity score for two sets of feature points from a pair of iris images. The experimental results show that the performance of our system is better than those of the systems based on the local features and is comparable to those of the typical systems. © 2011 IEEE.","Gabor filter; iris recognition; possibilistic fuzzy matching (PFM)","Cartesian coordinate system; Descriptors; Effective matching; Fuzzy matching; Gabor filter; Invariant properties; Iris images; Iris recognition; Iris segmentation method; Local feature; Matching algorithm; Nonlinear normalization; Possibilistic; possibilistic fuzzy matching (PFM); Rotation invariant; Similarity scores; Smooth curves; Biometrics; Image matching; Feature extraction; algorithm; article; artificial intelligence; automated pattern recognition; biometry; computer assisted diagnosis; fuzzy logic; histology; human; image subtraction; iris; methodology; retinoscopy; Algorithms; Artificial Intelligence; Biometry; Fuzzy Logic; Humans; Image Interpretation, Computer-Assisted; Iris; Pattern Recognition, Automated; Retinoscopy; Subtraction Technique",Article,Scopus,2-s2.0-84856297758
"Cutler A., Cutler D.R., Stevens J.R.","Random forests",2012,"Ensemble Machine Learning: Methods and Applications",27,10.1007/9781441993267_5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930202925&doi=10.1007%2f9781441993267_5&partnerID=40&md5=2a65ee807bcd830e448cf539a87be802","Random Forests were introduced by Leo Breiman [6] who was inspired by earlier work by Amit and Geman [2]. Although not obvious from the description in [6], Random Forests are an extension of Breiman's bagging idea [5] and were developed as a competitor to boosting. Random Forests can be used for either a categorical response variable, referred to in [6] as “classification,” or a continuous response, referred to as “regression.” Similarly, the predictor variables can be either categorical or continuous. © Springer Science+Business Media, LLC 2012. All rights reserved.",,"Artificial intelligence; Software engineering; Predictor variables; Random forests; Decision trees",Book Chapter,Scopus,2-s2.0-84930202925
"Boehm N., Wolters D., Thiel U., Lossbrand U., Wiegel N., Pfeiffer N., Grus F.H.","New insights into autoantibody profiles from immune privileged sites in the eye: A glaucoma study",2012,"Brain, Behavior, and Immunity",27,10.1016/j.bbi.2011.07.241,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-81355161325&doi=10.1016%2fj.bbi.2011.07.241&partnerID=40&md5=7a8e018223398841ac76e95b7bba6873","Glaucoma is a chronic neurodegenerative disease and one of the leading causes of blindness. Autoantibody based immune processes are assumed to be involved in its pathogenesis. However, it is still unclear to what extent autoantibody patterns found in the eye (aqueous humor) are congruent to systemic autoantibodies (blood). Consistency would underline the specificity of known serum antibody markers for glaucoma. In this study we used antigen microarrays to analyze autoantibody reactivities in sera and corresponding aqueous humor samples of primary open-angle glaucoma patients (N= 37) and non-glaucomatous controls (N= 31). Compared to control subjects several divergent immunoreactivities were identified for the glaucoma group in both body fluids. Interestingly, 20% of the tested antigens revealed increased immunoreactivities (e.g., against HSP27, MBP, and α-1-antitrypsin) and 7.5% decreased immunoreactivities (e.g., against GFAP and β- l-crystallin), thus demonstrating a significant alteration of the autoantibody profiles in glaucoma patients. Using an artificial neural network in combination with a unique serum autoantibody pattern on prospective sera we were able to detect glaucoma with a specificity and sensitivity of approximately 93%. The intraindividual comparison revealed a strong correlation of detected immunoreactivities in sera and comparative aqueous humor samples in both study groups. These results emphasize the specificity of immunoreactions found in blood samples of glaucoma patients. Furthermore they indicate the necessity of analyzing not only up-regulated but also down-regulated antibody reactivities, which might be likewise relevant for the understanding of other diseases. © 2011 Elsevier Inc.","Antigen microarray; Autoantibody profiling; Disease marker; Increased and diminished autoantibody reactivities; Primary open-angle glaucoma","autoantibody; antibody screening; antigen antibody reaction; aqueous humor; article; artificial neural network; blood analysis; clinical article; controlled study; human; immunoreactivity; microarray analysis; open angle glaucoma; priority journal; sensitivity and specificity; Aged; Algorithms; Antigens; Aqueous Humor; Area Under Curve; Artificial Intelligence; Autoantibodies; Cataract Extraction; Eye; Female; Glaucoma, Open-Angle; Humans; Immunoglobulin G; Male; Microarray Analysis; Neural Networks (Computer); Ocular Hypertension",Article,Scopus,2-s2.0-81355161325
"Zafar M., Kumar S., Kumar S., Dhiman A.K.","Artificial intelligence based modeling and optimization of poly(3-hydroxybutyrate-co-3-hydroxyvalerate) production process by using Azohydromonas lata MTCC 2311 from cane molasses supplemented with volatile fatty acids: A genetic algorithm paradigm",2012,"Bioresource Technology",27,10.1016/j.biortech.2011.10.024,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84655176471&doi=10.1016%2fj.biortech.2011.10.024&partnerID=40&md5=b46aa3acb2e8424dfcae9ca6c06470bd","The present work describes the optimization of medium variables for the production of poly(3-hydroxybutyrate-co-3-hydroxyvalerate) [P(3HB-co-3HV)] by Azohydromonas lata MTCC 2311 using cane molasses supplemented with propionic acid. Genetic algorithm (GA) has been used for the optimization of P(3HB-co-3HV) production through the simulation of artificial neural network (ANN) and response surface methodology (RSM). The predictions by ANN are better than those of RSM and in good agreement with experimental findings. The highest P(3HB-co-3HV) concentration and 3HV content have been reported as 7.35. g/l and 16.84. mol%, respectively by hybrid ANN-GA. Upon validation, 7.20. g/l and 16.30. mol% of P(3HB-co-3HV) concentration and 3HV content have been found in the shake flask, whereas 6.70. g/l and 16.35. mol%, have been observed in a 3. l bioreactor, respectively. The specific growth rate and P(3HB-co-3HV) accumulation rate of 0.29 per h and 0.16. g/l. h determined with cane molasses are comparable to those observed on pure substrates. © 2011 Elsevier Ltd.","Artificial neural network; Azohydromonas lata; Cane molasses; Genetic algorithm; Poly(3-hydroxybutyrate-co-3-hydroxyvalerate)","Accumulation rates; Artificial Neural Network; Azohydromonas lata; Cane molasses; Modeling and optimization; Poly(3-hydroxybutyrate-co-3-hydroxyvalerate); Production process; Propionic acids; Response Surface Methodology; Shake flasks; Specific growth rate; Computer simulation; Molasses; Neural networks; Optimization; Volatile fatty acids; Genetic algorithms; molasses; poly(3 hydroxybutyrate co 3 hydroxyvalerate); polymer; propionic acid; unclassified drug; artificial neural network; bacterium; bioreactor; carboxylic acid; concentration (composition); ester; experimental study; fatty acid; genetic algorithm; growth rate; numerical model; optimization; prediction; substrate; sugar cane; volatile substance; article; artificial intelligence; artificial neural network; Azohydromonas lata; bacterium; bioreactor; concentration (parameters); genetic algorithm; model; nonhuman; priority journal; process optimization; response surface method; simulation; sugarcane; synthesis; Alcaligenaceae; Algorithms; Artificial Intelligence; Bioreactors; Computer Simulation; Fatty Acids, Volatile; Models, Biological; Molasses; Polyesters; Alcaligenes latus",Article,Scopus,2-s2.0-84655176471
"Chen S., Weiss G.","An efficient and adaptive approach to negotiation in complex environments",2012,"Frontiers in Artificial Intelligence and Applications",27,10.3233/978-1-61499-098-7-228,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878796056&doi=10.3233%2f978-1-61499-098-7-228&partnerID=40&md5=d306810639218c43002804483a5eebb0","This paper studies automated bilateral negotiation among self-interested agents in complex application domains which consist of multiple issues and real-time constraints and where the agents have no prior knowledge about their opponents' preferences and strategies. We describe a novel negotiation approach called OMAC (standing for ""Opponent Modeling and Adaptive Concession"") which combines efficient opponent modeling and adaptive concession making. Opponent modeling is achieved through standard wavelet decomposition and cubic smoothing spline, and concession adaptivity is achieved through dynamically setting the concession rate on the basis of the expected utilities of forthcoming counteroffers. Experimental results are presented which demonstrate the effectiveness of our approach in both discounting and non-discounting domains. Specifically, the results show that our approach performs better than the five top agents from the 2011 Automated Negotiation Agents Competition (ANAC). © 2012 The Author(s).",,"Artificial intelligence; Wavelet decomposition; Adaptive approach; Automated negotiation agents; Bilateral negotiations; Complex applications; Complex environments; Cubic smoothing splines; Real time constraints; Self-interested agents; Intelligent agents",Conference Paper,Scopus,2-s2.0-84878796056
"Zhao B., Ding H., Lu Y., Wang G., Zhao J., Molloi S.","Dual-dictionary learning-based iterative image reconstruction for spectral computed tomography application",2012,"Physics in Medicine and Biology",26,10.1088/0031-9155/57/24/8217,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871242060&doi=10.1088%2f0031-9155%2f57%2f24%2f8217&partnerID=40&md5=cccd449eb5a2b2a9662d112b9a69b446","In this study, we investigated the effectiveness of a novel iterative reconstruction (IR) method coupled with dual-dictionary learning (DDL) for image reconstruction in a dedicated breast computed tomography (CT) system based on a cadmium-zinc-telluride (CZT) photon-counting detector and compared it to the filtered-back-projection (FBP) method with the ultimate goal of reducing the number of projections necessary for reconstruction without sacrificing the image quality. Postmortem breast samples were scanned in a fan-beam CT system and were reconstructed from 100 to 600 projections with both IR and FBP methods. The contrast-to-noise ratio (CNR) between the glandular and adipose tissues of the postmortem breast samples was calculated to compare the quality of images reconstructed from IR and FBP. The spatial resolution of the two reconstruction techniques was evaluated using aluminum (Al) wires with diameters of 643, 813, 1020, 1290 and 1630m in a plastic epoxy resin phantom with a diameter of 13cm. Both the spatial resolution and the CNR were improved with IR compared to FBP for the images reconstructed from the same number of projections. In comparison with FBP reconstruction, the CNR was improved from 3.4 to 7.5 by using the IR method with six-fold fewer projections while maintaining the same spatial resolution. The study demonstrated that the IR method coupled with DDL could significantly reduce the required number of projections for a CT reconstruction compared to the FBP method while achieving a much better CNR and maintaining the same spatial resolution. From this, the radiation dose and scanning time can potentially be reduced by a factor of approximately 6 by using this IR method for image reconstruction in a CZT-based breast CT system. © 2012 Institute of Physics and Engineering in Medicine.",,"Adipose tissue; Breast CT; Cadmium zinc tellurides; Computed Tomography; Contrast to noise ratio; CT reconstruction; Fan-beam CT; IR methods; Iterative image reconstruction; Iterative reconstruction; Photon counting; Reconstruction techniques; Scanning time; Spatial resolution; Aluminum; Cadmium; Cadmium telluride; Epoxy resins; Image quality; Image reconstruction; Image resolution; Iterative methods; Medical imaging; Tissue; Z transforms; Computerized tomography; cadmium; CdZnTe; tellurium; zinc; article; artificial intelligence; computer assisted tomography; feasibility study; human; image processing; image quality; mammography; methodology; photon; Artificial Intelligence; Cadmium; Feasibility Studies; Humans; Image Processing, Computer-Assisted; Mammography; Phantoms, Imaging; Photons; Tellurium; Tomography, X-Ray Computed; Zinc",Article,Scopus,2-s2.0-84871242060
"Eiben A.E., Kernbach S., Haasdijk E.","Embodied artificial evolution: Artificial evolutionary systems in the 21st Century",2012,"Evolutionary Intelligence",26,10.1007/s12065-012-0071-x,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868622585&doi=10.1007%2fs12065-012-0071-x&partnerID=40&md5=f90deb0c88aa6437a0f2f4c2a6bb0891","Evolution is one of the major omnipresent powers in the universe that has been studied for about two centuries. Recent scientific and technical developments make it possible to make the transition from passively understanding to actively using evolutionary processes. Today this is possible in Evolutionary Computing, where human experimenters can design and manipulate all components of evolutionary processes in digital spaces. We argue that in the near future it will be possible to implement artificial evolutionary processes outside such imaginary spaces and make them physically embodied. In other words, we envision the ""Evolution of Things"", rather than just the evolution of digital objects, leading to a new field of Embodied Artificial Evolution (EAE). The main objective of this paper is to present a unifying vision in order to aid the development of this high potential research area. To this end, we introduce the notion of EAE, discuss a few examples and applications, and elaborate on the expected benefits as well as the grand challenges this developing field will have to address. © 2012 The Author(s).","Embodied evolution; Embodiment; Evolution; Evolutionary computing; Self-reproduction","Embodied evolution; Embodiment; Evolution; Evolutionary computing; Self reproduction; Artificial intelligence; Evolutionary algorithms",Article,Scopus,2-s2.0-84868622585
"Björne J., Ginter F., Salakoski T.","University of Turku in the BioNLP'11 Shared Task.",2012,"BMC bioinformatics",26,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876211463&partnerID=40&md5=43b30cbffadbf1232e82aa580f3b3029","We present a system for extracting biomedical events (detailed descriptions of biomolecular interactions) from research articles, developed for the BioNLP'11 Shared Task. Our goal is to develop a system easily adaptable to different event schemes, following the theme of the BioNLP'11 Shared Task: generalization, the extension of event extraction to varied biomedical domains. Our system extends our BioNLP'09 Shared Task winning Turku Event Extraction System, which uses support vector machines to first detect event-defining words, followed by detection of their relationships. Our current system successfully predicts events for every domain case introduced in the BioNLP'11 Shared Task, being the only system to participate in all eight tasks and all of their subtasks, with best performance in four tasks. Following the Shared Task, we improve the system on the Infectious Diseases task from 42.57% to 53.87% F-score, bringing performance into line with the similar GENIA Event Extraction and Epigenetics and Post-translational Modifications tasks. We evaluate the machine learning performance of the system by calculating learning curves for all tasks, detecting areas where additional annotated data could be used to improve performance. Finally, we evaluate the use of system output on external articles as additional training data in a form of self-training. We show that the updated Turku Event Extraction System can easily be adapted to all presently available event extraction targets, with competitive performance in most tasks. The scope of the performance gains between the 2009 and 2011 BioNLP Shared Tasks indicates event extraction is still a new field requiring more work. We provide several analyses of event extraction methods and performance, highlighting potential future directions for continued development.",,"protein; article; artificial intelligence; bacterial gene; bacterium; classification; communicable disease; data mining; ecosystem; epigenetics; genetic epistasis; genetics; human; metabolism; natural language processing; nomenclature; protein processing; support vector machine; Artificial Intelligence; Bacteria; Communicable Diseases; Data Mining; Ecosystem; Epigenomics; Epistasis, Genetic; Genes, Bacterial; Humans; Natural Language Processing; Protein Processing, Post-Translational; Proteins; Support Vector Machines; Terminology as Topic",Article,Scopus,2-s2.0-84876211463
"Lourenço J.C., Morton A., Bana E Costa C.A.","PROBE - A multicriteria decision support system for portfolio robustness evaluation",2012,"Decision Support Systems",26,10.1016/j.dss.2012.08.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868636449&doi=10.1016%2fj.dss.2012.08.001&partnerID=40&md5=797ff752cf3f4c0c54a3d625566e3a42","This paper addresses the problem of selecting a robust portfolio of projects in the context of limited resources, multiple criteria, different project interactions and several types of uncertainty. A portfolio of projects is considered an undoubtedly robust choice if for a given uncertainty domain that affects the costs and/or the benefits of the projects there is no other portfolio that does not cost more and simultaneously may provide more overall benefit. We present a new decision support system, PROBE (Portfolio Robustness Evaluation), and the algorithms it implements. PROBE identifies all efficient portfolios and depicts the respective Pareto frontier within a given portfolio cost range, and permits users to analyze, in depth, the robustness of selecting a proposed portfolio. The robustness evaluation starts by identifying competitor portfolios to the proposed portfolio, its similarities and differences in project composition to its competitors, and the regret a decision-maker may have by selecting the proposed portfolio instead of a competitor. © 2012 Elsevier B.V. All rights reserved.","DSS; Portfolio decision analysis; Portfolio robustness; Resource allocation; Restricted efficiency","Decision makers; DSS; Efficient portfolio; Multi-criteria decision support systems; Multiple criteria; Overall benefit; Pareto frontiers; Portfolio decisions; Robustness evaluation; Uncertainty domains; Artificial intelligence; Costs; Decision support systems; Decision theory; Resource allocation; Probes",Article,Scopus,2-s2.0-84868636449
"Lin C.H., Daniel M., Weld S.","Crowdsourcing control: Moving beyond multiple choice",2012,"Uncertainty in Artificial Intelligence - Proceedings of the 28th Conference, UAI 2012",26,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882621746&partnerID=40&md5=4e875eb8c8e60a301c55b6b91a1853db","To ensure quality results from crowdsourced tasks, requesters often aggregate worker responses and use one of a plethora of strategies to infer the correct answer from the set of noisy responses. However, all current models assume prior knowledge of all possible outcomes of the task. While not an unreasonable assumption for tasks that can be posited as multiple-choice questions (e.g. n-ary classification), we observe that many tasks do not naturally fit this paradigm, but instead demand a free-response formulation where the outcome space is of infinite size (e.g. audio transcription). We model such tasks with a novel probabilistic graphical model, and design and implement LazySusan, a decision-theoretic controller that dynamically requests responses as necessary in order to infer answers to these tasks. We also design an EM algorithm to jointly learn the parameters of our model while inferring the correct answers to multiple tasks at a time. Live experiments on Amazon Mechanical Turk demonstrate the superiority of LazySusan at solving SAT Math questions, eliminating 83.2% of the error and achieving greater net utility compared to the state-ofthe- art strategy, majority-voting. We also show in live experiments that our EM algorithm outperforms majority-voting on a visualization task that we design.",,"Amazon mechanical turks; Decision-theoretic; Design and implements; Multiple choice; Multiple tasks; Multiple-choice questions; Prior knowledge; Probabilistic graphical models; Algorithms; Artificial intelligence; Experiments; Design",Conference Paper,Scopus,2-s2.0-84882621746
"Buzdalova A., Buzdalov M.","Increasing efficiency of evolutionary algorithms by choosing between auxiliary fitness functions with reinforcement learning",2012,"Proceedings - 2012 11th International Conference on Machine Learning and Applications, ICMLA 2012",26,10.1109/ICMLA.2012.32,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873587026&doi=10.1109%2fICMLA.2012.32&partnerID=40&md5=b50003ecb05f331cdfcfd0ace3989acd","In this paper further investigation of the previously proposed method of speeding up single-objective evolutionary algorithms is done. The method is based on reinforcement learning which is used to choose auxiliary fitness functions. The requirements for this method are formulated. The compliance of the method with these requirements is illustrated on model problems such as Royal Roads problem and H-IFF optimization problem. The experiments confirm that the method increases the efficiency of evolutionary algorithms. © 2012 IEEE.",,"Fitness functions; Model problems; Optimization problems; Artificial intelligence; Software engineering; Reinforcement learning",Conference Paper,Scopus,2-s2.0-84873587026
"Kumar M.N., Sujatha P., Kalva V., Nagori R., Katukojwala A.K., Kumar M.","Mitigating economic denial of sustainability (EDoS) in cloud computing using in-cloud scrubber service",2012,"Proceedings - 4th International Conference on Computational Intelligence and Communication Networks, CICN 2012",26,10.1109/CICN.2012.149,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872026350&doi=10.1109%2fCICN.2012.149&partnerID=40&md5=6c8f50c01ed5f13d78549ad0720449fb","Cloud computing is not a new technology, it is a new way of delivering computing resources. Elastic cloud computing enables services to be deployed and accessed globally on demand with little maintenance by providing QoS as per service level agreement (SLA) of customer. The Cloud-based DDoS attacks or outside DDoS attacks can make ostensibly legitimate requests for a service to generate an economic Distributed Denial of Service (eDDoS) - where the elastic nature of the cloud allows scaling of service beyond the economic means of the purveyor to pay their cloud-based service bills which leads to Economic Denial of Sustainability (EDoS). Attacks mimicking legitimate users are on the climb. For cloud computing to remain attractive, the DDoS threat is to be addressed before it triggers the billing mechanism. This problem can be addressed by using reactive/on-demand in-cloud eDDoS mitigation service (scrubber Service) for mitigating the application-layer and network-layer DDOS attacks with the help of an efficient client-puzzle approach. © 2012 IEEE.","Cloud computing; cryptographic puzzles; Distributed Denial of Service (DDoS); Economic Denial of Service (EDoS); Mitigation","Computing resource; cryptographic puzzles; DDoS Attack; Denial of Service; Distributed denial of service; Economic means; Legitimate users; Mitigation; Service Level Agreements; Artificial intelligence; Internet protocols; Quality of service; Scrubbers; Sustainable development; Cloud computing",Conference Paper,Scopus,2-s2.0-84872026350
"Chang L.-C., Chen P.-A., Chang F.-J.","Reinforced two-step-ahead weight adjustment technique for online training of recurrent neural networks",2012,"IEEE Transactions on Neural Networks and Learning Systems",26,10.1109/TNNLS.2012.2200695,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875900575&doi=10.1109%2fTNNLS.2012.2200695&partnerID=40&md5=428a32985fcacb358766fc0606255bbf","A reliable forecast of future events possesses great value. The main purpose of this paper is to propose an innovative learning technique for reinforcing the accuracy of two-step-ahead (2SA) forecasts. The real-time recurrent learning (RTRL) algorithm for recurrent neural networks (RNNs) can effectively model the dynamics of complex processes and has been used successfully in one-step-ahead forecasts for various time series. A reinforced RTRL algorithm for 2SA forecasts using RNNs is proposed in this paper, and its performance is investigated by two famous benchmark time series and a streamflow during flood events in Taiwan. Results demonstrate that the proposed reinforced 2SA RTRL algorithm for RNNs can adequately forecast the benchmark (theoretical) time series, significantly improve the accuracy of flood forecasts, and effectively reduce time-lag effects. © 2012 IEEE.","Real-time recurrent learning (RTRL) algorithm; recurrent neural network (RNN); streamflow forecast; time series forecast","Complex Processes; Innovative learning; Real-time recurrent learning; Recurrent neural network (RNN); Recurrent neural network (RNNs); Streamflow forecast; Time series forecasts; Time-lag effect; Benchmarking; Flood control; Floods; Learning algorithms; Recurrent neural networks; Reinforcement; Stream flow; Time series; Forecasting; algorithm; artificial intelligence; artificial neural network; epidemiology; forecasting; human; trends; Algorithms; Artificial Intelligence; Forecasting; Humans; Interrupted Time Series Analysis; Neural Networks (Computer)",Article,Scopus,2-s2.0-84875900575
"Peluso S., Romano P., Quaglia F.","SCORe: A scalable one-copy serializable partial replication protocol",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",26,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869816785&partnerID=40&md5=fb10e0d1de1c76ad8aac76c362714bf0","In this article we present SCORe, a scalable one-copy serializable partial replication protocol. Differently from any other literature proposal, SCORe jointly guarantees the following properties: (i) it is genuine, thus ensuring that only the replicas that maintain data accessed by a transaction are involved in its processing, and (ii) it guarantees that read operations always access consistent snapshots, thanks to a one-copy serializable multiversion scheme, which never aborts read-only transactions and spares them from any (distributed) validation phase. This makes SCORe particularly efficient in presence of read-intensive workloads, as typical of a wide range of real-world applications. We have integrated SCORe into a popular open source distributed data grid and performed a large scale experimental study with well-known benchmarks using both private and public cloud infrastructures. The experimental results demonstrate that SCORe provides stronger consistency guarantees (namely One-Copy Serializability) than existing multiversion partial replication protocols at no additional overhead. © 2012 Springer-Verlag Berlin Heidelberg.","Distributed Transactional Systems; Multiversioning; Partial Replication; Scalability","Distributed data grid; Experimental studies; Multi-version; Multi-versioning; Open sources; Partial replication; Read operation; Read-only transaction; Real-world application; Serializability; Transactional systems; Validation phase; Artificial intelligence; Scalability; Middleware",Conference Paper,Scopus,2-s2.0-84869816785
"Zhu Y., Wang X., Zhong E., Liu N.N., Li H., Yang Q.","Discovering spammers in social networks",2012,"Proceedings of the National Conference on Artificial Intelligence",26,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868290015&partnerID=40&md5=2297d13f454b385103ed99f18a3b8e02","As the popularity of the social media increases, as evidenced in Twitter, Facebook and China's Renren, spamming activities also picked up in numbers and variety. On social network sites, spammers often disguise themselves by creating fake accounts and hijacking normal users' accounts for personal gains. Different from the spammers in traditional systems such as SMS and email, spammers in social media behave like normal users and they continue to change their spamming strategies to fool anti-spamming systems. However, due to the privacy and resource concerns, many social media websites cannot fully monitor all the contents of users, making many of the previous approaches, such as topology-based and content-classification-based methods, infeasible to use. In this paper, we propose a Supervised Matrix Factorization method with Social Regularization (SMFSR) for spammer detection in social networks that exploits both social activities as well as users' social relations in an innovative and highly scalable manner. The proposed method detects spammers collectively based on users' social actions and social relations. We have empirically tested our method on data from Renren.com, which is one of the largest social networks in China, and demonstrated that our new method can improve the detection performance significantly. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Anti spamming; Detection performance; Facebook; Matrix factorizations; Social activities; Social media; Social Networks; Social relations; Spammers; Traditional systems; Artificial intelligence; Internet; Spamming; Social networking (online)",Conference Paper,Scopus,2-s2.0-84868290015
"Ivanov S.V., Kosukhin S.S., Kaluzhnaya A.V., Boukhanovsky A.V.","Simulation-based collaborative decision support for surge floods prevention in St. Petersburg",2012,"Journal of Computational Science",26,10.1016/j.jocs.2012.08.005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867219516&doi=10.1016%2fj.jocs.2012.08.005&partnerID=40&md5=95ca83694836fd74ac90efc1a2f6d23d","The paper aims to implement a simulation-based collaborative decision support approach for flood control management in application to St. Petersburg surge floods, which are prevented by a complex of dams with several large openings. Despite the evolution of the numerical hydrodynamic models, hardware performance and computer technologies the accurate forecasting of storm surges and decision support for gates maneuvering is still an important issue. The prospective architecture and principal solutions of Flood Warning System with the emphasis on the simulation-based approach and collaborative decision support system on the basis of e-Science platform CLAVIRE are considered. © 2012 Elsevier B.V.","Data assimilation; Decision support; Forecasting; Simulation scenarios; Storm surge; Urgent Computing","Data assimilation; Decision supports; Simulation scenarios; Storm surges; Urgent computing; Artificial intelligence; Decision support systems; Flood control; Forecasting; Storms; Floods",Article,Scopus,2-s2.0-84867219516
"Zeng W., Wang C.","Human gait recognition via deterministic learning",2012,"Neural Networks",26,10.1016/j.neunet.2012.07.012,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865998843&doi=10.1016%2fj.neunet.2012.07.012&partnerID=40&md5=ba392dd22e579f581f9161ad8af57902","Recognition of temporal/dynamical patterns is among the most difficult pattern recognition tasks. Human gait recognition is a typical difficulty in the area of dynamical pattern recognition. It classifies and identifies individuals by their time-varying gait signature data. Recently, a new dynamical pattern recognition method based on deterministic learning theory was presented, in which a time-varying dynamical pattern can be effectively represented in a time-invariant manner and can be rapidly recognized. In this paper, we present a new model-based approach for human gait recognition via the aforementioned method, specifically for recognizing people by gait. The approach consists of two phases: a training (learning) phase and a test (recognition) phase. In the training phase, side silhouette lower limb joint angles and angular velocities are selected as gait features. A five-link biped model for human gait locomotion is employed to demonstrate that functions containing joint angle and angular velocity state vectors characterize the gait system dynamics. Due to the quasi-periodic and symmetrical characteristics of human gait, the gait system dynamics can be simplified to be described by functions of joint angles and angular velocities of one side of the human body, thus the feature dimension is effectively reduced. Locally-accurate identification of the gait system dynamics is achieved by using radial basis function (RBF) neural networks (NNs) through deterministic learning. The obtained knowledge of the approximated gait system dynamics is stored in constant RBF networks. A gait signature is then derived from the extracted gait system dynamics along the phase portrait of joint angles versus angular velocities. A bank of estimators is constructed using constant RBF networks to represent the training gait patterns. In the test phase, by comparing the set of estimators with the test gait pattern, a set of recognition errors are generated, and the average L1 norms of the errors are taken as the similarity measure between the dynamics of the training gait patterns and the dynamics of the test gait pattern. Therefore, the test gait pattern similar to one of the training gait patterns can be rapidly recognized according to the smallest error principle. Finally, experiments are carried out on the NLPR and UCSD gait databases to demonstrate the effectiveness of the proposed approach. © 2012 Elsevier Ltd.","Deterministic learning; Dynamical pattern recognition; Feature extraction; Human gait recognition; RBF neural network; Side silhouette lower limb joint angle and angular velocity","Deterministic learning; Dynamical pattern; Feature dimensions; Gait features; Gait pattern; Human bodies; Human gait; Joint angle; Lower limb; Model based approach; Phase portrait; Quasi-periodic; Radial basis function neural networks; RBF Neural Network; Recognition error; Signature data; Similarity measure; System Dynamics; Time invariants; Time varying; Training phase; Velocity state; Angular velocity; Feature extraction; Neural networks; Radial basis function networks; System theory; Testing; Gait analysis; article; artificial neural network; gait; human; human experiment; learning; locomotion; mathematical analysis; mathematical model; pattern recognition; priority journal; training; velocity; walking; Algorithms; Artificial Intelligence; Gait; Humans; Joints; Locomotion; Models, Theoretical; Neural Networks (Computer); Pattern Recognition, Automated",Article,Scopus,2-s2.0-84865998843
"Hazan T., Jaakkola T.","On the partition function and random maximum a-posteriori perturbations",2012,"Proceedings of the 29th International Conference on Machine Learning, ICML 2012",26,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867112240&partnerID=40&md5=8250acaeec06d3462d83d29ea4ee1b4a","In this paper we relate the partition function to the max-statistics of random variables. In particular, we provide a novel framework for approximating and bounding the partition function using MAP inference on randomly perturbed models. As a result, we can use efficient MAP solvers such as graph-cuts to evaluate the corresponding partition function. We show that our method excels in the typical ""high signal - high coupling"" regime that results in ragged energy landscapes difficult for alternative approaches. Copyright 2012 by the author(s)/owner(s).",,"Alternative approach; Energy landscape; Maximum a posteriori; Partition functions; Artificial intelligence; Software engineering; Learning systems",Conference Paper,Scopus,2-s2.0-84867112240
"Bacardit J., Widera P., Márquez-chamorro A., Divina F., Aguilar-Ruiz J.S., Krasnogor N.","Contact map prediction using a large-scale ensemble of rule sets and the fusion of multiple predicted structural features",2012,"Bioinformatics",26,10.1093/bioinformatics/bts472,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867301175&doi=10.1093%2fbioinformatics%2fbts472&partnerID=40&md5=eecfb842abf40f42b4dc554ddc987f36","Motivation: The prediction of a protein's contact map has become in recent years, a crucial stepping stone for the prediction of the complete 3D structure of a protein. In this article, we describe a methodology for this problem that was shown to be successful in CASP8 and CASP9. The methodology is based on (i) the fusion of the prediction of a variety of structural aspects of protein residues, (ii) an ensemble strategy used to facilitate the training process and (iii) a rule-based machine learning system from which we can extract human-readable explanations of the predictor and derive useful information about the contact map representation. Results: The main part of the evaluation is the comparison against the sequence-based contact prediction methods from CASP9, where our method presented the best rank in five out of the six evaluated metrics. We also assess the impact of the size of the ensemble used in our predictor to show the trade-off between performance and training time of our method. Finally, we also study the rule sets generated by our machine learning system. From this analysis, we are able to estimate the contribution of the attributes in our representation and how these interact to derive contact predictions. © The Author 2012. Published by Oxford University Press.",,"CASP8 protein, human; CASP9 protein, human; caspase 8; caspase 9; protein; algorithm; article; artificial intelligence; biology; chemistry; comparative study; human; methodology; protein database; protein domain; Algorithms; Artificial Intelligence; Caspase 8; Caspase 9; Computational Biology; Databases, Protein; Humans; Protein Interaction Domains and Motifs; Proteins",Article,Scopus,2-s2.0-84867301175
"Li L.-N., Ouyang J.-H., Chen H.-L., Liu D.-Y.","A computer aided diagnosis system for thyroid disease using extreme learning machine",2012,"Journal of Medical Systems",26,10.1007/s10916-012-9825-3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867297688&doi=10.1007%2fs10916-012-9825-3&partnerID=40&md5=22e6224510a3f63b110d67902e97c0b0","In this paper, we present an effective and efficient computer aided diagnosis (CAD) system based on principle component analysis (PCA) and extreme learning machine (ELM) to assist the task of thyroid disease diagnosis. The CAD system is comprised of three stages. Focusing on dimension reduction, the first stage applies PCA to construct the most discriminative new feature set. After then, the system switches to the second stage whose target is model construction. ELM classifier is explored to train an optimal predictive model whose parameters are optimized. As we known, the number of hidden neurons has an important role in the performance of ELM, so we propose an experimental method to hunt for the optimal value. Finally, the obtained optimal ELM model proceeds to perform the thyroid disease diagnosis tasks using the most discriminative new feature set and the optimal parameters. The effectiveness of the resultant CAD system (PCA-ELM) has been rigorously estimated on a thyroid disease dataset which is taken from UCI machine learning repository. We compare it with other related methods in terms of their classification accuracy. Experimental results demonstrate that PCA-ELM outperforms other ones reported so far by 10-fold cross-validation method, with the mean accuracy of 97.73% and with the maximum accuracy of 98.1%. Besides, PCA-ELM performs much faster than support vector machines (SVM) based CAD system. Consequently, the proposed method PCA-ELM can be considered as a new powerful tools for diagnosing thyroid disease with excellent performance and less time. © 2012 Springer Science+Business Media, LLC.","Extreme learning machine (ELM); Principle component analysis (PCA); Thyroid disease diagnosis","article; back propagation; classification algorithm; computer assisted diagnosis; diagnostic accuracy; experimental design; human; hyperthyroidism; hypothyroidism; learning algorithm; machine learning; nerve cell; support vector machine; thyroid disease; algorithm; artificial intelligence; methodology; principal component analysis; reproducibility; Algorithms; Artificial Intelligence; Diagnosis, Computer-Assisted; Humans; Principal Component Analysis; Reproducibility of Results; Thyroid Diseases",Article,Scopus,2-s2.0-84867297688
"Bergeest J.-P., Rohr K.","Efficient globally optimal segmentation of cells in fluorescence microscopy images using level sets and convex energy functionals",2012,"Medical Image Analysis",26,10.1016/j.media.2012.05.012,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866436561&doi=10.1016%2fj.media.2012.05.012&partnerID=40&md5=5c156f74a70839bce1793a07693bab98","In high-throughput applications, accurate and efficient segmentation of cells in fluorescence microscopy images is of central importance for the quantification of protein expression and the understanding of cell function. We propose an approach for segmenting cell nuclei which is based on active contours using level sets and convex energy functionals. Compared to previous work, our approach determines the global solution. Thus, the approach does not suffer from local minima and the segmentation result does not depend on the initialization. We consider three different well-known energy functionals for active contour-based segmentation and introduce convex formulations of these functionals. We also suggest a numeric approach for efficiently computing the solution. The performance of our approach has been evaluated using fluorescence microscopy images from different experiments comprising different cell types. We have also performed a quantitative comparison with previous segmentation approaches. © 2012 Elsevier B.V.","Active contours; Cell segmentation; Convex optimization; Level sets","Active contours; Cell functions; Cell nucleus; Cell segmentation; Cell types; Energy functionals; Fluorescence microscopy images; Functionals; Global solutions; High-throughput; Level Set; Local minimums; Optimal segmentation; Protein expressions; Quantitative comparison; Segmentation results; Cells; Convex optimization; Cytology; Fluorescence microscopy; Image segmentation; animal cell; article; cell nucleus; cell type; computer analysis; controlled study; energy; fluorescence microscopy; human; human cell; image analysis; mouse; nonhuman; performance; priority journal; quantitative analysis; Algorithms; Artificial Intelligence; Cell Tracking; Image Enhancement; Image Interpretation, Computer-Assisted; Microscopy, Fluorescence; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84866436561
"Marqués A.I., García V., Sánchez J.S.","Two-level classifier ensembles for credit risk assessment",2012,"Expert Systems with Applications",26,10.1016/j.eswa.2012.03.033,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861183555&doi=10.1016%2fj.eswa.2012.03.033&partnerID=40&md5=2e202367f3a1e27ea10a7a5a959fd486","Many techniques have been proposed for credit risk assessment, from statistical models to artificial intelligence methods. During the last few years, different approaches to classifier ensembles have successfully been applied to credit scoring problems, demonstrating to be generally more accurate than single prediction models. The present paper goes one step beyond by introducing composite ensembles that jointly use different strategies for diversity induction. Accordingly, the combination of data resampling algorithms (bagging and AdaBoost) and attribute subset selection methods (random subspace and rotation forest) for the construction of composite ensembles is explored with the aim of improving the prediction performance. The experimental results and statistical tests show that this new two-level classifier ensemble constitutes an appropriate solution for credit scoring problems, performing better than the traditional single ensembles and very significantly better than individual classifiers. © 2012 Elsevier Ltd. All rights reserved.","Bagging; Boosting; Classifier ensemble; Credit scoring; Random subspace; Rotation forest","Bagging; Boosting; Classifier ensembles; Credit scoring; Random subspaces; Adaptive boosting; Artificial intelligence; Forestry; Mathematical models; Statistical tests; Risk assessment; Forestry; Mathematical Models; Random Processes; Risk Assessment; Statistical Analysis; Testing",Article,Scopus,2-s2.0-84861183555
"al-Rifaie M.M., Bishop J.M., Caines S.","Creativity and Autonomy in Swarm Intelligence Systems",2012,"Cognitive Computation",26,10.1007/s12559-012-9130-y,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866514313&doi=10.1007%2fs12559-012-9130-y&partnerID=40&md5=24a54b00faab3bcff1be3316d890a65b","This work introduces two swarm intelligence algorithms-one mimicking the behaviour of one species of ants (Leptothorax acervorum) foraging (a 'stochastic diffusion search', SDS) and the other algorithm mimicking the behaviour of birds flocking (a 'particle swarm optimiser', PSO)-and outlines a novel integration strategy exploiting the local search properties of the PSO with global SDS behaviour. The resulting hybrid algorithm is used to sketch novel drawings of an input image, exploiting an artistic tension between the local behaviour of the 'birds flocking'-as they seek to follow the input sketch-and the global behaviour of the 'ants foraging'-as they seek to encourage the flock to explore novel regions of the canvas. The paper concludes by exploring the putatve 'creativity' of this hybrid swarm system in the philosophical light of the 'rhizome' and Deleauze's well-known 'Orchid and Wasp' metaphor. © 2012 Springer Science+Business Media, LLC.","Autonomy; Computational creativity; PSO; SDS; Swarm intelligence","Autonomy; Computational creativity; PSO; SDS; Swarm Intelligence; Algorithms; Philosophical aspects; Artificial intelligence",Article,Scopus,2-s2.0-84866514313
"Jiang J., Wang P., Lung W.-S., Guo L., Li M.","A GIS-based generic real-time risk assessment framework and decision tools for chemical spills in the river basin",2012,"Journal of Hazardous Materials",26,10.1016/j.jhazmat.2012.05.051,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862638759&doi=10.1016%2fj.jhazmat.2012.05.051&partnerID=40&md5=21cca6c3fe71768ee04bf4ab12a40362","This paper presents a generic framework and decision tools of real-time risk assessment on Emergency Environmental Decision Support System for response to chemical spills in river basin. The generic "" 4-step-3-model"" framework is able to delineate the warning area and the impact on vulnerable receptors considering four types of hazards referring to functional area, societal impact, and human health and ecology system. Decision tools including the stand-alone system and software components were implemented on GIS platform. A detailed case study on the Songhua River nitrobenzene spill illustrated the goodness of the framework and tool Spill first responders and decision makers of catchment management will benefit from the rich, visual and dynamic hazard information output from the software. © 2012 Elsevier B.V.","Chemical spills; Early warning/emergency response system; GIS; Hazard zone determination; Real-time risk assessment","Catchment management; Chemical spill; Decision makers; Decision tool; Environmental decision support systems; First responders; Functional areas; Generic frameworks; Hazard zones; Human health; Response systems; Risk assessment framework; River basins; Societal impacts; Software component; Songhua River; Standalone systems; Artificial intelligence; Catchments; Decision support systems; Geographic information systems; Health hazards; Pollution; Risk assessment; Watersheds; Oil spills; nitrobenzene; chemical pollutant; decision making; decision support system; early warning system; GIS; real time; risk assessment; river basin; river pollution; software; article; catchment; computer program; decision support system; emergency environmental decision support system; geographic information system; health hazard; human; risk assessment; waste water management; Chemical Hazard Release; China; Decision Support Techniques; Geographic Information Systems; Hazardous Substances; Nitrobenzenes; Risk Assessment; Rivers; China; Songhua River",Article,Scopus,2-s2.0-84862638759
"Manthey N.","Coprocessor 2.0 - A flexible CNF simplifier (Tool presentation)",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",26,10.1007/978-3-642-31612-8_34,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864183504&doi=10.1007%2f978-3-642-31612-8_34&partnerID=40&md5=e8a69173cfa07cfd3b26dd701bbe0f3f","This paper presents the CNF simplifier Coprocessor 2.0, an extension of Coprocessor [1]. It implements almost all currently known simplification techniques in a modular way and provides access to each single technique to execute them independently. Disabling preprocessing for a set of variables is also possible and enables to apply simplifications also for incremental SAT solving. Experiments show that Coprocessor 2.0 performs better than its predecessor or SatElite[2]. © 2012 Springer-Verlag.",,"Co-processors; SAT-solving; Artificial intelligence; Formal logic",Conference Paper,Scopus,2-s2.0-84864183504
"Köpf B., Mauborgne L., Ochoa M.","Automatic quantification of cache side-channels",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",26,10.1007/978-3-642-31424-7_40,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864059907&doi=10.1007%2f978-3-642-31424-7_40&partnerID=40&md5=a35a18f444cc64cdc04baee7e7e04240","The latency gap between caches and main memory has been successfully exploited for recovering sensitive input to programs, such as cryptographic keys from implementation of AES and RSA. So far, there are no practical general-purpose countermeasures against this threat. In this paper we propose a novel method for automatically deriving upper bounds on the amount of information about the input that an adversary can extract from a program by observing the CPU's cache behavior. At the heart of our approach is a novel technique for efficient counting of concretizations of abstract cache states that enables us to connect state-of-the-art techniques for static cache analysis and quantitative information-flow. We implement our counting procedure on top of the AbsInt TimingExplorer, one of the most advanced engines for static cache analysis. We use our tool to perform a case study where we derive upper bounds on the cache leakage of a 128-bit AES executable on an ARM processor. We also analyze this implementation with a commonly suggested (but until now heuristic) countermeasure applied, obtaining a formal account of the corresponding increase in security. © 2012 Springer-Verlag.",,"Amount of information; ARM processor; Automatic quantification; Cache analysis; Cache behavior; Cryptographic key; Information flows; Main memory; Novel techniques; Upper Bound; Artificial intelligence; Computer aided analysis",Conference Paper,Scopus,2-s2.0-84864059907
"Soto R., Crawford B., Monfroy E., Bustos V.","Using autonomous search for generating good enumeration strategy blends in constraint programming",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",26,10.1007/978-3-642-31137-6_46,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863895599&doi=10.1007%2f978-3-642-31137-6_46&partnerID=40&md5=0401d4600468c7e85c711e97551ff0f0","In Constraint Programming, enumeration strategies play an important role, they can significantly impact the performance of the solving process. However, choosing the right strategy is not simple as its behavior is commonly unpredictable. Autonomous search aims at tackling this concern, it proposes to replace bad performing strategies by more promising ones during the resolution. This process yields a combination of enumeration strategies that worked during the search phase. In this paper, we focus on the study of this combination by carefully tracking the resolution. Our preliminary goal is to find good enumeration strategy blends for a given Constraint Satisfaction Problem. © 2012 Springer-Verlag.","Artificial Intelligence; Autonomous Search; Constraint Programming","Autonomous searches; Constraint programming; Constraint satisfaction problems; Process yield; Artificial intelligence; Computer programming; Constraint theory",Conference Paper,Scopus,2-s2.0-84863895599
"Etcheverry L., Vaisman A.A.","Enhancing OLAP analysis with web cubes",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",26,10.1007/978-3-642-30284-8_38,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861745602&doi=10.1007%2f978-3-642-30284-8_38&partnerID=40&md5=1fcb7bb42882ef4037a3c4b878efd46e","Traditional OLAP tools have proven to be successful in analyzing large sets of enterprise data. For today's business dynamics, sometimes these highly curated data is not enough. External data (particularly web data), may be useful to enhance local analysis. In this paper we discuss the extraction of multidimensional data from web sources, and their representation in RDFS. We introduce Open Cubes, an RDFS vocabulary for the specification and publication of multidimensional cubes on the Semantic Web, and show how classical OLAP operations can be implemented over Open Cubes using SPARQL 1.1, without the need of mapping the multidimensional information to the local database (the usual approach to multidimensional analysis of Semantic Web data). We show that our approach is plausible for the data sizes that can usually be retrieved to enhance local data repositories. © 2012 Springer-Verlag.",,"Business dynamics; Data size; Enterprise data; Local analysis; Local data; Multidimensional analysis; Multidimensional cube; Multidimensional data; Multidimensional information; Web data; Web sources; Artificial intelligence; Geometry",Conference Paper,Scopus,2-s2.0-84861745602
"Miwa M., Thompson P., McNaught J., Kell D.B., Ananiadou S.","Extracting semantically enriched events from biomedical literature",2012,"BMC Bioinformatics",26,10.1186/1471-2105-13-108,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861329393&doi=10.1186%2f1471-2105-13-108&partnerID=40&md5=8fb5a4a0197883b75f207712067f35e5","Background: Research into event-based text mining from the biomedical literature has been growing in popularity to facilitate the development of advanced biomedical text mining systems. Such technology permits advanced search, which goes beyond document or sentence-based retrieval. However, existing event-based systems typically ignore additional information within the textual context of events that can determine, amongst other things, whether an event represents a fact, hypothesis, experimental result or analysis of results, whether it describes new or previously reported knowledge, and whether it is speculated or negated. We refer to such contextual information as meta-knowledge. The automatic recognition of such information can permit the training of systems allowing finer-grained searching of events according to the meta-knowledge that is associated with them.Results: Based on a corpus of 1,000 MEDLINE abstracts, fully manually annotated with both events and associated meta-knowledge, we have constructed a machine learning-based system that automatically assigns meta-knowledge information to events. This system has been integrated into EventMine, a state-of-the-art event extraction system, in order to create a more advanced system (EventMine-MK) that not only extracts events from text automatically, but also assigns five different types of meta-knowledge to these events. The meta-knowledge assignment module of EventMine-MK performs with macro-averaged F-scores in the range of 57-87% on the BioNLP'09 Shared Task corpus. EventMine-MK has been evaluated on the BioNLP'09 Shared Task subtask of detecting negated and speculated events. Our results show that EventMine-MK can outperform other state-of-the-art systems that participated in this task.Conclusions: We have constructed the first practical system that extracts both events and associated, detailed meta-knowledge information from biomedical literature. The automatically assigned meta-knowledge information can be used to refine search systems, in order to provide an extra search layer beyond entities and assertions, dealing with phenomena such as rhetorical intent, speculations, contradictions and negations. This finer grained search functionality can assist in several important tasks, e.g., database curation (by locating new experimental knowledge) and pathway enrichment (by providing information for inference). To allow easy integration into text mining systems, EventMine-MK is provided as a UIMA component that can be used in the interoperable text mining infrastructure, U-Compare. © 2012 Miwa et al.; licensee BioMed Central Ltd.",,"Advanced systems; Automatic recognition; Biomedical literature; Biomedical text minings; Contextual information; Curation; Event extraction; Event-based; Event-based system; Experimental knowledge; Medline; Meta-knowledge; Practical systems; Search functionality; Search system; Sentence-based; State-of-the-art system; Subtask; Text mining; Textual contexts; U compares; Data mining; Interoperability; Mining machinery; Natural language processing systems; Search engines; article; artificial intelligence; biology; data mining; Medline; methodology; semantics; Artificial Intelligence; Computational Biology; Data Mining; MEDLINE; Semantics",Article,Scopus,2-s2.0-84861329393
"Kluza K., Kaczor K., Nalepa G.J.","Enriching business processes with rules using the Oryx BPMN editor",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",26,10.1007/978-3-642-29350-4-68,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861015535&doi=10.1007%2f978-3-642-29350-4-68&partnerID=40&md5=bacdfcd989d21d4ee31b413e3ef6549b","BPMN is a leading visual notation for modeling Business Processes. Although it can be efficiently used for modeling workflow structures, it is not suitable for modeling the low-level logic of particular tasks in the process. Recently, Business Rules have been used for this purpose. Such rules are often specified in natural language and in an informal way. In this paper, we consider an approach to the integration of Business Processes with Business Rules. As a proof of concept, we propose a framework based on the Oryx BPMN editor integrated with rule-based tools. The goal of the integration of the BPMN editor with the XTT2 rule framework is to provide an environment for visual modeling processes with formally described business rules. We also discuss execution options of the integrated model. In the future, this opens up a possibility to execute such models using the HeaRT rule engine. © 2012 Springer-Verlag Berlin Heidelberg.",,"Business Process; Business rules; Integrated models; Natural languages; Proof of concept; Rule based; Rule engine; Visual modeling; Visual notations; Workflow structures; Artificial intelligence; Soft computing; Integration",Conference Paper,Scopus,2-s2.0-84861015535
"Gabryel M., Woźniak M., K. Nowicki R.","Creating learning sets for control systems using an evolutionary method",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",26,10.1007/978-3-642-29353-5_24,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860697876&doi=10.1007%2f978-3-642-29353-5_24&partnerID=40&md5=522246f553ef068d6d5711318259d17a","The acquisition of the knowledge which is useful for developing of artificial intelligence systems is still a problem. We usually ask experts, apply historical data or reap the results of mensuration from a real simulation of the object. In the paper we propose a new algorithm to generate a representative training set. The algorithm is based on analytical or discrete model of the object with applied the k-nn and genetic algorithms. In this paper it is presented the control case of the issue illustrated by well known truck backer-upper problem. The obtained training set can be used for training many AI systems such as neural networks, fuzzy and neuro-fuzzy architectures and k-nn systems. © 2012 Springer-Verlag.","control system; genetic algorithm; training data acquisition","AI systems; Artificial intelligence systems; Discrete models; Evolutionary method; Historical data; Learning sets; Neuro-fuzzy architectures; Training data; Training sets; Artificial intelligence; Control systems; Soft computing; Genetic algorithms",Conference Paper,Scopus,2-s2.0-84860697876
"Hosseini M., Cox I.J., Milić-Frayling N., Kazai G., Vinay V.","On aggregating labels from multiple crowd workers to infer relevance of documents",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",26,10.1007/978-3-642-28997-2_16,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860206608&doi=10.1007%2f978-3-642-28997-2_16&partnerID=40&md5=e3e77962c17c74a5048026583f9a9484","We consider the problem of acquiring relevance judgements for information retrieval (IR) test collections through crowdsourcing when no true relevance labels are available. We collect multiple, possibly noisy relevance labels per document from workers of unknown labelling accuracy. We use these labels to infer the document relevance based on two methods. The first method is the commonly used majority voting (MV) which determines the document relevance based on the label that received the most votes, treating all the workers equally. The second is a probabilistic model that concurrently estimates the document relevance and the workers accuracy using expectation maximization (EM). We run simulations and conduct experiments with crowdsourced relevance labels from the INEX 2010 Book Search track to investigate the accuracy and robustness of the relevance assessments to the noisy labels. We observe the effect of the derived relevance judgments on the ranking of the search systems. Our experimental results show that the EM method outperforms the MV method in the accuracy of relevance assessments and IR systems ranking. The performance improvements are especially noticeable when the number of labels per document is small and the labels are of varied quality. © 2012 Springer-Verlag Berlin Heidelberg.",,"Book search; Crowdsourcing; Expectation Maximization; Majority voting; Noisy labels; Performance improvements; Probabilistic models; Relevance assessments; Relevance judgement; Relevance judgment; Search system; Test Collection; Artificial intelligence; Information retrieval",Conference Paper,Scopus,2-s2.0-84860206608
"Jiao H., Zhong Y., Zhang L.","Artificial DNA computing-based spectral encoding and matching algorithm for hyperspectral remote sensing data",2012,"IEEE Transactions on Geoscience and Remote Sensing",26,10.1109/TGRS.2012.2188856,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867058657&doi=10.1109%2fTGRS.2012.2188856&partnerID=40&md5=f195a85033f91034100b7f9c9c0cb377","In this paper, a spectral encoding and matching algorithm inspired by biological deoxyribonucleic acid (DNA) computing is proposed to perform the task of spectral signature classification for hyperspectral remote sensing data. As a novel branch of computational intelligence, DNA computing has the strong computing and matching capability to discriminate the tiny differences in DNA strands by DNA encoding and matching in the molecule layer. Similar to DNA discrimination, a hyperspectral remote sensing data matching approach is used to recognize the land cover material from a spectral library or image, according to the rich spectral information. However, it is difficult to apply DNA computing to hyperspectral remote sensing data processing because traditional DNA computing often relies on biochemical reactions of DNA molecules and may result in incorrect or undesirable computations. To utilize the advantages and avoid the problems of biological DNA computing, an artificial DNA computing approach is proposed for spectral encoding and matching for hyperspectral remote sensing data. A DNA computing-based spectral matching approach is used to first transform spectral signatures into DNA codewords by capturing the key spectral features with a spectral feature encoding operation. After DNA encoding, the typical DNA database for interesting classes is constructed and saved by DNA evolutionary operating mechanisms such as crossover, mutation, and structured mutation. During the course of spectral matching, each pixel of the hyperspectral image, or each signature measured in the field, is input to the constructed DNA database. By computing the distance between an unclassified spectrum and the typical DNA codewords from the database, the class property of each pixel is set as the minimum distance class. Experiments using different hyperspectral data sets were performed to evaluate the performance of the proposed artificial DNA computing-based spectral matching algorithm by comparing it with other traditional hyperspectral classifiers, including spectral matching classifiers (binary coding, spectral angle mapper and spectral derivative feature coding (SDFC) matching methods) and a novel statistical method of machine learning termed support vector machine (SVM). Experimental results demonstrate that the proposed algorithm is distinctly superior to the three traditional hyperspectral data classification algorithms. It presents excellent processing efficiency, compared to SVM, with high-dimensional data captured by the Hyperspectral Digital Imagery Collection Experiment sensor, and hence provides an effective option for spectral matching classification of hyperspectral remote sensing data. © 2012 IEEE.","Classification; Deoxyribonucleic acid (DNA) computation; Hyperspectral remote sensing; Spectral matching","Binary coding; Biochemical reactions; Class-property; Code-words; Digital imagery; DNA database; DNA encoding; DNA molecules; DNA strands; DNA-computing; Feature coding; High dimensional data; Hyper-spectral images; HyperSpectral; Hyperspectral Data; Hyperspectral data classification; Hyperspectral remote sensing; Hyperspectral remote sensing data; Land cover; Matching algorithm; Matching methods; Minimum distance; Molecule layers; Operating mechanism; Spectral angle mappers; Spectral encoding; Spectral feature; Spectral information; Spectral libraries; Spectral matching algorithms; Spectral matchings; Spectral signature; Algorithms; Artificial intelligence; Calculations; Classification (of information); Data processing; Database systems; DNA; Encoding (symbols); Experiments; Gene encoding; Molecules; Organic acids; Pixels; Remote sensing; Spectroscopy; Support vector machines; Image matching; algorithm; biochemistry; data set; database; DNA; evolutionary biology; image classification; land cover; mutation; remote sensing; sensor; spectral analysis",Article,Scopus,2-s2.0-84867058657
"Julià-Sapé M., Coronel I., Majós C., Candiota A.P., Serrallonga M., Cos M., Aguilera C., Acebes J.J., Griffiths J.R., Arús C.","Prospective diagnostic performance evaluation of single-voxel 1H MRS for typing and grading of brain tumours",2012,"NMR in Biomedicine",26,10.1002/nbm.1782,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858860261&doi=10.1002%2fnbm.1782&partnerID=40&md5=4a91e7f6100afeb6629d76aa280412ff","The purpose of this study was to evaluate whether single-voxel 1H MRS could add useful information to conventional MRI in the preoperative characterisation of the type and grade of brain tumours. MRI and MRS examinations from a prospective cohort of 40 consecutive patients were analysed double blind by radiologists and spectroscopists before the histological diagnosis was known. The spectroscopists had only the MR spectra, whereas the radiologists had both the MR images and basic clinical details (age, sex and presenting symptoms). Then, the radiologists and spectroscopists exchanged their predictions and re-evaluated their initial opinions, taking into account the new evidence. Spectroscopists used four different systems of analysis for 1H MRS data, and the efficacy of each of these methods was also evaluated. Information extracted from 1H MRS significantly improved the radiologists' MRI-based characterisation of grade IV tumours (glioblastomas, metastases, medulloblastomas and lymphomas) in the cohort [area under the curve (AUC) in the MRI re-evaluation 0.93 versus AUC in the MRI evaluation 0.85], and also of the less malignant glial tumours (AUC in the MRI re-evaluation 0.93 versus AUC in the MRI evaluation 0.81). One of the MRS analysis systems used, the INTERPRET (International Network for Pattern Recognition of Tumours Using Magnetic Resonance) decision support system, outperformed the others, as well as being better than the MRI evaluation for the characterisation of grade III astrocytomas. Thus, preoperative MRS data improve the radiologists' performance in diagnosing grade IV tumours and, for those of grade II-III, MRS data help them to recognise the glial lineage. Even in cases in which their diagnoses were not improved, the provision of MRS data to the radiologists had no negative influence on their predictions. © 2011 John Wiley &amp; Sons, Ltd.","Brain neoplasms; Diagnosis, computer-assisted; Diagnostic techniques and procedures; MRI; MRS; Receiver operating characteristic curve","Analysis system; Area under the curves; Brain neoplasms; Brain tumours; Diagnostic performance; Diagnostic techniques; Glioblastomas; International networks; MR images; MRI and MRS; MRS; Negative influence; Re-evaluation; Receiver operating characteristic curve; Artificial intelligence; Brain; Decision support systems; Forecasting; Magnetic resonance; Magnetic resonance imaging; Pattern recognition; Rating; Tumors; adult; aged; article; astrocytoma; brain lymphoma; brain metastasis; brain tumor; cancer grading; clinical article; clinical effectiveness; cohort analysis; decision support system; double blind procedure; female; glioblastoma; human; male; medulloblastoma; nuclear magnetic resonance imaging; prediction; priority journal; prospective study; proton nuclear magnetic resonance; Adult; Aged; Brain Neoplasms; Diagnosis, Computer-Assisted; Female; Humans; Magnetic Resonance Imaging; Magnetic Resonance Spectroscopy; Male; Middle Aged; Neoplasm Grading; Prospective Studies; Protons; Reproducibility of Results; Sensitivity and Specificity; Tumor Markers, Biological",Article,Scopus,2-s2.0-84858860261
"Alberti F., Bruttomesso R., Ghilardi S., Ranise S., Sharygina N.","Lazy abstraction with interpolants for arrays",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",26,10.1007/978-3-642-28717-6_7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858320671&doi=10.1007%2f978-3-642-28717-6_7&partnerID=40&md5=539a97c36cd29af8e7a7de5596ecec80","Lazy abstraction with interpolants has been shown to be a powerful technique for verifying imperative programs. In presence of arrays, however, the method shows an intrinsic limitation, due to the fact that successful invariants usually contain universally quantified variables, which are not present in the program specification. In this work we present an extension of the interpolation-based lazy abstraction in which arrays of unknown length can be handled in a natural manner. In particular, we exploit the Model Checking Modulo Theories framework, to derive a backward reachability version of lazy abstraction that embeds array reasoning. The approach is generic, in that it is valid for both parameterized systems and imperative programs. We show by means of experiments that our approach can synthesize and prove universally quantified properties over arrays in a completely automatic fashion. © 2012 Springer-Verlag.",,"Backward reachability; Imperative programs; Interpolants; Modulo theories; Parameterized system; Program specification; Artificial intelligence; Interpolation; Model checking; Abstracting",Conference Paper,Scopus,2-s2.0-84858320671
"Agahi H., Mesiar R., Ouyang Y., Pap E., Strboja M.","General Chebyshev type inequalities for universal integral",2012,"Information Sciences",26,10.1016/j.ins.2011.10.016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84155172518&doi=10.1016%2fj.ins.2011.10.016&partnerID=40&md5=a8265266add81395a8226a0abcc6f122","A new inequality for the universal integral on abstract spaces is obtained in a rather general form. As two corollaries, Minkowski's and Chebyshev's type inequalities for the universal integral are obtained. The main results of this paper generalize some previous results obtained for special fuzzy integrals, e.g., Choquet and Sugeno integrals. Furthermore, related inequalities for seminormed integral are obtained. © 2011 Elsevier Inc. All rights reserved.","Chebyshev's inequality; Comonotone functions; Minkowski's inequality; Nonadditive measure; Seminormed fuzzy integral; Universal integral","Chebyshev's inequality; Fuzzy integral; Minkowski's inequality; Non-additive measure; Universal integral; Artificial intelligence; Software engineering; Integral equations",Article,Scopus,2-s2.0-84155172518
"Busser B.W., Taher L., Kim Y., Tansey T., Bloom M.J., Ovcharenko I., Michelson A.M.","A machine learning approach for identifying novel cell type-specific transcriptional regulators of myogenesis",2012,"PLoS Genetics",26,10.1371/journal.pgen.1002531,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859235405&doi=10.1371%2fjournal.pgen.1002531&partnerID=40&md5=131dc8e2012668088d7afccba2f03dfc","Transcriptional enhancers integrate the contributions of multiple classes of transcription factors (TFs) to orchestrate the myriad spatio-temporal gene expression programs that occur during development. A molecular understanding of enhancers with similar activities requires the identification of both their unique and their shared sequence features. To address this problem, we combined phylogenetic profiling with a DNA-based enhancer sequence classifier that analyzes the TF binding sites (TFBSs) governing the transcription of a co-expressed gene set. We first assembled a small number of enhancers that are active in Drosophila melanogaster muscle founder cells (FCs) and other mesodermal cell types. Using phylogenetic profiling, we increased the number of enhancers by incorporating orthologous but divergent sequences from other Drosophila species. Functional assays revealed that the diverged enhancer orthologs were active in largely similar patterns as their D. melanogaster counterparts, although there was extensive evolutionary shuffling of known TFBSs. We then built and trained a classifier using this enhancer set and identified additional related enhancers based on the presence or absence of known and putative TFBSs. Predicted FC enhancers were over-represented in proximity to known FC genes; and many of the TFBSs learned by the classifier were found to be critical for enhancer activity, including POU homeodomain, Myb, Ets, Forkhead, and T-box motifs. Empirical testing also revealed that the T-box TF encoded by org-1 is a previously uncharacterized regulator of muscle cell identity. Finally, we found extensive diversity in the composition of TFBSs within known FC enhancers, suggesting that motif combinatorics plays an essential role in the cellular specificity exhibited by such enhancers. In summary, machine learning combined with evolutionary sequence analysis is useful for recognizing novel TFBSs and for facilitating the identification of cognate TFs that coordinate cell type-specific developmental gene expression patterns.",,"ETS protein; forkhead transcription factor; protein Myb; T box transcription factor; transcription factor; unclassified drug; transcription factor; animal tissue; article; binding site; cell type; controlled study; Drosophila melanogaster; embryo; enhancer region; gene; gene expression; gene expression profiling; gene sequence; insect cell; machine learning; mesoderm; molecular evolution; muscle development; muscle founder cell; nonhuman; org 1 gene; orthology; phylogeny; protein motif; sequence analysis; transcription regulation; animal; artificial intelligence; binding site; cell lineage; cytology; Drosophila melanogaster; enhancer region; gene expression regulation; genetic transcription; genetics; growth, development and aging; muscle; Drosophila melanogaster; Animals; Artificial Intelligence; Binding Sites; Cell Lineage; Drosophila melanogaster; Enhancer Elements, Genetic; Evolution, Molecular; Gene Expression Regulation, Developmental; Mesoderm; Muscles; Phylogeny; Transcription Factors; Transcription, Genetic",Article,Scopus,2-s2.0-84859235405
"Boyer E.W., Fletcher R., Fay R.J., Smelson D., Ziedonis D., Picard R.W.","Preliminary Efforts Directed Toward the Detection of Craving of Illicit Substances: The iHeal Project",2012,"Journal of Medical Toxicology",26,10.1007/s13181-011-0200-4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857795440&doi=10.1007%2fs13181-011-0200-4&partnerID=40&md5=e68dc99dd0f96d1cf701efe23d0e7d03","Many behavioral interventions, whether for the management of chronic pain, overeating, medication adherence, or substance abuse, are ineffective outside of the clinic or office environments in which they are taught. This lack of utility has spawned interest in enabling technologies that are capable of detecting changes in affective state that potentially herald a transition to risky behaviors. We have therefore undertaken the preliminary development of ""iHeal"", an innovative constellation of technologies that incorporates artificial intelligence, continuous biophysical monitoring, wireless connectivity, and smartphone computation. In its fully realized form, iHeal can detect developing drug cravings; as a multimedia device, it can also intervene as the cravings develop to prevent drug use. This manuscript describes preliminary data related to the iHeal Project and our experience with its use. © 2012 American College of Medical Toxicology.","Craving; Drug abuse; Drug craving; eHealth; mHealth; Mobile health; Wireless communications","adult; article; artificial intelligence; biosensor; clinical article; computer program; controlled study; drug traffic; health program; high risk behavior; human; male; mass medium; mobile phone; online system; opiate addiction; posttraumatic stress disorder; process development; risk assessment; risk benefit analysis; substance abuse; telehealth; telemonitoring; wireless communication; withdrawal syndrome; Adult; Focus Groups; Humans; Male; Middle Aged; Street Drugs; Substance-Related Disorders; Telemedicine; Wireless Technology",Article,Scopus,2-s2.0-84857795440
"Milea V., Frasincar F., Kaymak U.","TOWL: A temporal web ontology language",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",26,10.1109/TSMCB.2011.2162582,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856255474&doi=10.1109%2fTSMCB.2011.2162582&partnerID=40&md5=d008d60486f5dd2ad8318eaf1f49567e","Through its interoperability and reasoning capabilities, the Semantic Web opens a realm of possibilities for developing intelligent systems on the Web. The Web Ontology Language (OWL) is the most expressive standard language for modeling ontologies, the cornerstone of the Semantic Web. However, up until now, no standard way of expressing time and time-dependent information in OWL has been provided. In this paper, we present a temporal extension of the very expressive fragment SHIN(D) of the OWL Description Logic language, resulting in the temporal OWL language. Through a layered approach, we introduce three extensions: 1) concrete domains, which allow the representation of restrictions using concrete domain binary predicates; 2) temporal representation, which introduces time points, relations between time points, intervals, and Allen's 13 interval relations into the language; and 3) timeslices/fluents, which implement a perdurantist view on individuals and allow for the representation of complex temporal aspects, such as process state transitions. We illustrate the expressiveness of the newly introduced language by using an example from the financial domain. © 2011 IEEE.","Concrete domains; fluents; intelligent systems; machine communication; Semantic Web; time representation; Web Ontology Language (OWL)","Binary predicates; Concrete domains; Description logic; Financial domains; Fluents; Process state; Reasoning capabilities; Temporal aspects; Temporal extensions; Temporal representations; Time points; time representation; Time-dependent; Web ontology language; Data description; Intelligent systems; Interoperability; Ontology; Temporal logic; Semantic Web; algorithm; article; artificial intelligence; automated pattern recognition; computer language; computer program; Internet; methodology; natural language processing; Algorithms; Artificial Intelligence; Internet; Natural Language Processing; Pattern Recognition, Automated; Programming Languages; Software",Article,Scopus,2-s2.0-84856255474
"Ghosh S., Das S., Kundu D., Suresh K., Abraham A.","Inter-particle communication and search-dynamics of lbest particle swarm optimizers: An analysis",2012,"Information Sciences",26,10.1016/j.ins.2010.10.015,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80055036609&doi=10.1016%2fj.ins.2010.10.015&partnerID=40&md5=caa44626a23fb7aab921d3fcb94d58dc","Particle Swarm Optimization (PSO) is arguably one of the most popular nature-inspired algorithms for real parameter optimization at present. The existing theoretical research on PSO focuses on the issues like stability, convergence, and explosion of the swarm. However, all of them are based on the gbest (global best) communication topology, which usually is susceptible to false or premature convergence over multi-modal fitness landscapes. The present standard PSO (SPSO 2007) uses an lbest (local best) topology, where a particle is stochastically attracted not towards the best position found in the entire swarm, but towards the best position found by any particle in its topological neighborhood. This article presents a first step towards a probabilistic analysis of the particle interaction and information exchange in an lbest PSO with variable random neighborhood topology (as found in SPSO 2007). It addresses issues like the distribution of particles over neighborhoods, the probability distributions of the social and cognitive terms in lbest model, and the explorative power of the lbest PSO. It also presents a state-space model of the lbest PSO and draws important conclusions regarding the stability and convergence of the particle dynamics in the light of control theory. © 2011 Elsevier Inc. All rights reserved.","Global optimization; Neighborhood topologies; Particle swarm optimization; Probability distribution; State-space model; Swarm intelligence","Best position; Communication topologies; Distribution of particles; Fitness landscape; Information exchanges; Multi-modal; Nature-inspired algorithms; Neighborhood topology; Particle dynamics; Particle swarm optimizers; Premature convergence; Probabilistic analysis; Real-parameter optimization; Stability and convergence; Standard PSO; State-space models; Swarm Intelligence; Theoretical research; Artificial intelligence; Cellular automata; Global optimization; Probability distributions; Random variables; Topology; Particle swarm optimization (PSO)",Article,Scopus,2-s2.0-80055036609
"Chen X., Liu H., Carbonell J.G.","Structured sparse canonical correlation analysis",2012,"Journal of Machine Learning Research",26,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954221126&partnerID=40&md5=95127f4ec513cda5deeeed0b84cc6bc2","In this paper, we propose to apply sparse canonical correlation analysis (sparse CCA) to an important genome-wide association study problem, eQTL mapping. Existing sparse CCA models do not incorporate structural information among variables such as pathways of genes. This work extends the sparse CCA so that it could exploit either the pre-given or unknown group structure via the structured-sparsity-inducing penalty. Such structured penalty poses new challenge on optimization techniques. To address this challenge, by specializing the excessive gap framework, we develop a scalable primal-dual optimization algorithm with a fast rate of convergence. Empirical results show that the proposed optimization algorithm is more efficient than existing state-of-the-art methods. We also demonstrate the effectiveness of the structured sparse CCA on both simulated and genetic datasets.",,"Algorithms; Artificial intelligence; Correlation methods; Genome-wide association studies; Optimization algorithms; Optimization techniques; Primal-dual optimization algorithms; Sparse canonical correlation analysis; State-of-the-art methods; Structural information; Structured sparsities; Optimization",Conference Paper,Scopus,2-s2.0-84954221126
"Chen S.-M., Griffis F.H., Chen P.-H., Chang L.-M.","Simulation and analytical techniques for construction resource planning and scheduling",2012,"Automation in Construction",26,10.1016/j.autcon.2011.05.018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-81355138778&doi=10.1016%2fj.autcon.2011.05.018&partnerID=40&md5=6dbb93b7954ed3abe436145999294d53","To date, few construction methods or models in the literature have discussed about helping the project managers decide the near-optimum distributions of manpower, material, equipment and space according to their project objectives and project constraints. Thus, the traditional scheduling methods or models often result in a ""seat-of-the-pants"" style of management, rather than decision making based on an analysis of real data. This paper presents an intelligent scheduling system (ISS) that can help the project managers to find the near-optimum schedule plan according to their project objectives and project constraints. ISS uses simulation techniques to distribute resources and assign different levels of priorities to different activities in each simulation cycle to find the near-optimal solution. ISS considers and integrates most of the important construction factors (schedule, cost, space, manpower, equipment and material) simultaneously in a unified environment, which makes the resulting schedule that will be closer to optimal. Furthermore, ISS allows for what-if analyses of possible scenarios, and schedule adjustments based on unforeseen conditions (change orders, late material delivery, etc.). Finally, two sample applications and one real-world construction project are utilized to illustrate and compare the effectiveness of ISS with two widely used software packages, Primavera Project Planner and Microsoft Project. © 2011 Elsevier B.V. All Rights Reserved.","Artificial intelligence; Computerized scheduling; Optimization; Resource management; Simulation","Analytical techniques; Change order; Computerized scheduling; Construction factor; Construction method; Construction projects; Construction resource; Intelligent scheduling; MicroSoft; Near-optimal solutions; Primavera project planners; Project constraints; Project managers; Project objectives; Resource management; Scheduling methods; Simulation; Simulation technique; Artificial intelligence; Computer simulation; Construction equipment; Construction industry; Decision making; Managers; Optimization; Resource allocation; Scheduling",Article,Scopus,2-s2.0-81355138778
"Millie D.F., Weckman G.R., Young W.A., Ivey J.E., Carrick H.J., Fahnenstiel G.L.","Modeling microalgal abundance with artificial neural networks: Demonstration of a heuristic 'Grey-Box' to deconvolve and quantify environmental influences",2012,"Environmental Modelling and Software",25,10.1016/j.envsoft.2012.04.009,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861662434&doi=10.1016%2fj.envsoft.2012.04.009&partnerID=40&md5=695a22be7115e1ed0cd70edbc990eab5","An artificial neural network (ANN)-based technology - a 'Grey-Box', originating the iterative selection, depiction, and quantitation of environmental relationships for modeling microalgal abundance, as chlorophyll (CHL) a, was developed and evaluated. Due to their robust capability for reproducing the complexities underlying chaotic, non-linear systems, ANNs have become popular for the modeling of ecosystem structure and function. However, ANNs exhibit a holistic deficiency in declarative knowledge structure (i.e. a 'black-box'). The architecture of the Grey-Box provided the benefit of the ANN modeling structure, while deconvolving the interaction of prediction potentials among environmental variables upon CHL a. The influences of (pairs of) predictors upon the variance and magnitude of CHL a were depicted via pedagogical knowledge extraction (multi-dimensional response surfaces). This afforded derivation of mathematical equations for iterative predictive outcomes of CHL a and together with an algorithmic expression across iterations, corrected for the lack of declarative knowledge within conventional ANNs. Importantly, the Grey-Box 'bridged the gap' between 'white-box' parametric models and black-box ANNs in terms of performance and mathematical transparency. Grey-Box formulations are relevant to ecological niche modeling, identification of biotic response(s) to stress/disturbance thresholds, and qualitative/quantitative derivation of biota-environmental relationships for incorporation within stand-alone mechanistic models projecting ecological structure. © 2012 Elsevier Ltd.","Artificial intelligence; Ecological modeling; Environmental informatics; Output response surfaces; Pedagogical knowledge extraction","Black boxes; Declarative knowledge; Ecological modeling; Ecological niche; Ecological structure; Ecosystem structure; Environmental influences; Environmental variables; Grey-box; Informatics; Mathematical equations; Mechanistic models; Modeling structures; Output response; Parametric models; Pedagogical knowledge; Response surface; Robust capability; Artificial intelligence; Chlorophyll; Ecology; Neural networks; Surface properties; Environmental technology; abundance; algorithm; artificial neural network; chlorophyll a; complexity; ecological modeling; environmental modeling; heuristics; informatics; microalga; numerical model",Article,Scopus,2-s2.0-84861662434
"Van Meensel J., Lauwers L., Kempen I., Dessein J., Van Huylenbroeck G.","Effect of a participatory approach on the successful development of agricultural decision support systems: The case of Pigs2win",2012,"Decision Support Systems",25,10.1016/j.dss.2012.05.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868639363&doi=10.1016%2fj.dss.2012.05.002&partnerID=40&md5=1f1ac1214e7f9a8d59c6701c9f4bab8f","This paper explores how a decision support system (DSS) can be developed that complies with the critical success factors of such systems. A participatory approach is used to develop Pigs2win, a DSS for Flemish pig farms. Pigs2win uses frontier analysis for comparative farm analysis. The participatory approach influences the selection of stakeholders, objective setting and evaluation of Pigs2win. Outcomes of the participatory approach result in features of Pigs2win that positively influence its compliance with critical success factors. Based on our experience with Pigs2win, we put forward points that need attention when a participatory approach is organized. © 2012 Elsevier B.V. All rights reserved.","Critical success factors; Decision support system; Frontier analysis; Participatory approach; Pig farming","Agricultural decision support; Critical success factor; Frontier analysis; Objective setting; Participatory approach; Pig farming; Artificial intelligence; Decision support systems",Article,Scopus,2-s2.0-84868639363
"Sun X., Chen K.J., Maddock-Carlin K.R., Anderson V.L., Lepper A.N., Schwartz C.A., Keller W.L., Ilse B.R., Magolski J.D., Berg E.P.","Predicting beef tenderness using color and multispectral image texture features",2012,"Meat Science",25,10.1016/j.meatsci.2012.04.030,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865572148&doi=10.1016%2fj.meatsci.2012.04.030&partnerID=40&md5=2faaeae0db8a3e8e476c8d09e18d4b6a","The objective of this study was to investigate the usefulness of raw meat surface characteristics (texture) in predicting cooked beef tenderness. Color and multispectral texture features, including 4 different wavelengths and 217 image texture features, were extracted from 2 laboratory-based multispectral camera imaging systems. Steaks were segregated into tough and tender classification groups based on Warner-Bratzler shear force. The texture features were submitted to STEPWISE multiple regression and support vector machine (SVM) analyses to establish prediction models for beef tenderness. A subsample (80%) of tender or tough classified steaks were used to train models which were then validated on the remaining (20%) test steaks. For color images, the SVM model correctly identified tender steaks with 100% accurately while the STEPWISE equation identified 94.9% of the tender steaks correctly. For multispectral images, the SVM model predicted 91% and STEPWISE predicted 87% average accuracy of beef tender. © 2012 Elsevier Ltd.","Beef; Color; Multispectral image; Stepwise; SVM; Tenderness","Beef tenderness; Color images; Cooked beef; Multi-spectral cameras; Multispectral images; Multispectral textures; Prediction model; Shear force; Stepwise; Stepwise multiple regression; Surface characteristics; SVM; SVM model; Tenderness; Texture features; Train model; Color; Image texture; Support vector machines; Textures; Beef; animal; animal food; article; artificial intelligence; artificial neural network; cattle; chemistry; classification; controlled clinical trial; controlled study; cross breeding; female; food control; Fourier analysis; heat; image processing; meat; mechanics; methodology; pea; physical chemistry; pigmentation; plant seed; randomized controlled trial; reproducibility; shear strength; spectrophotometry; surface property; validation study; Animal Feed; Animals; Artificial Intelligence; Cattle; Crosses, Genetic; Female; Food Inspection; Fourier Analysis; Hot Temperature; Image Processing, Computer-Assisted; Meat; Mechanical Phenomena; Neural Networks (Computer); Peas; Physicochemical Phenomena; Pigmentation; Reproducibility of Results; Seeds; Shear Strength; Spectrophotometry; Surface Properties",Article,Scopus,2-s2.0-84865572148
"Silva T.C., Zhao L.","Stochastic competitive learning in complex networks",2012,"IEEE Transactions on Neural Networks and Learning Systems",25,10.1109/TNNLS.2011.2181866,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875833873&doi=10.1109%2fTNNLS.2011.2181866&partnerID=40&md5=08a01afce7598af70647d546445c5067","Competitive learning is an important machine learning approach which is widely employed in artificial neural networks. In this paper, we present a rigorous definition of a new type of competitive learning scheme realized on large-scale networks. The model consists of several particles walking within the network and competing with each other to occupy as many nodes as possible, while attempting to reject intruder particles. The particle's walking rule is composed of a stochastic combination of random and preferential movements. The model has been applied to solve community detection and data clustering problems. Computer simulations reveal that the proposed technique presents high precision of community and cluster detections, as well as low computational complexity. Moreover, we have developed an efficient method for estimating the most likely number of clusters by using an evaluator index that monitors the information generated by the competition process itself. We hope this paper will provide an alternative way to the study of competitive learning.. © 2012 IEEE.","Community detection; complex networks; data clustering; preferential walk; random walk; stochastic competitive learning","Community detection; Data clustering; Preferential walks; Random Walk; Stochastic competitive learning; Clustering algorithms; Complex networks; Neural networks; Population dynamics; Stochastic systems; Computer simulation; artificial intelligence; artificial neural network; competitive behavior; statistics; trends; Artificial Intelligence; Competitive Behavior; Neural Networks (Computer); Stochastic Processes",Article,Scopus,2-s2.0-84875833873
"Prichep L.S., Jacquin A., Filipenko J., Dastidar S.G., Zabele S., Vodencarevic A., Rothman N.S.","Classification of traumatic brain injury severity using informed data reduction in a series of binary classifier algorithms",2012,"IEEE Transactions on Neural Systems and Rehabilitation Engineering",25,10.1109/TNSRE.2012.2206609,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869381698&doi=10.1109%2fTNSRE.2012.2206609&partnerID=40&md5=1d130dfe3abed1c8b2bc814049ebe32b","Assessment of medical disorders is often aided by objective diagnostic tests which can lead to early intervention and appropriate treatment. In the case of brain dysfunction caused by head injury, there is an urgent need for quantitative evaluation methods to aid in acute triage of those subjects who have sustained traumatic brain injury (TBI). Current clinical tools to detect mild TBI (mTBI/concussion) are limited to subjective reports of symptoms and short neurocognitive batteries, offering little objective evidence for clinical decisions; or computed tomography (CT) scans, with radiation-risk, that are most often negative in mTBI. This paper describes a novel methodology for the development of algorithms to provide multi-class classification in a substantial population of brain injured subjects, across a broad age range and representative subpopulations. The method is based on age-regressed quantitative features (linear and nonlinear) extracted from brain electrical activity recorded from a limited montage of scalp electrodes. These features are used as input to a unique informed data reduction method, maximizing confidence of prospective validation and minimizing over-fitting. A training set for supervised learning was used, including: normal control, concussed, and structural injury/CT positive (CT+). The classifier function separating CT+ from the other groups demonstrated a sensitivity of 96% and specificity of 78%; the classifier separating normal controls from the other groups demonstrated a sensitivity of 81% and specificity of 74%, suggesting high utility of such classifiers in acute clinical settings. The use of a sequence of classifiers where the desired risk can be stratified further supports clinical utility. © 2001-2011 IEEE.","Genetic algorithms (GAs); informed data reduction; multiclass classification; quantitative electroencephalography (QEEG); traumatic brain injury (TBI)","Binary classifiers; Brain dysfunctions; Brain electrical activity; Clinical decision; Clinical settings; Clinical tools; Clinical utility; Computed tomography scan; Diagnostic tests; Early intervention; Head injuries; Medical disorder; Multi-class classification; Normal controls; Novel methodology; Overfitting; quantitative electroencephalography (QEEG); Quantitative evaluation methods; Quantitative features; Reduction method; Training sets; Traumatic Brain Injuries; Algorithms; Brain; Data reduction; Electrophysiology; Computerized tomography; adolescent; adult; aged; aging; algorithm; article; artifact; artificial intelligence; brain injury; classification; computer assisted tomography; electroencephalography; eye movement; female; fractal analysis; Glasgow coma scale; human; information science; male; middle aged; multivariate analysis; physiology; receiver operating characteristic; reproducibility; skeletal muscle; statistical analysis; statistical model; very elderly; Adolescent; Adult; Aged; Aged, 80 and over; Aging; Algorithms; Artifacts; Artificial Intelligence; Brain Injuries; Data Interpretation, Statistical; Electroencephalography; Eye Movements; Female; Fractals; Glasgow Coma Scale; Humans; Information Theory; Linear Models; Male; Middle Aged; Multivariate Analysis; Muscle, Skeletal; Reproducibility of Results; ROC Curve; Tomography, X-Ray Computed; Young Adult",Article,Scopus,2-s2.0-84869381698
"Armas Romero A., Cuenca Grau B., Horrocks I.","MORe: Modular combination of OWL reasoners for ontology classification",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",25,10.1007/978-3-642-35176-1-1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868556482&doi=10.1007%2f978-3-642-35176-1-1&partnerID=40&md5=c039d34582c9c6032326f3bb806b28ba","Classification is a fundamental reasoning task in ontology design, and there is currently a wide range of reasoners highly optimised for classification of OWL 2 ontologies. There are also several reasoners that are complete for restricted fragments of OWL 2, such as the OWL 2 EL profile. These reasoners are much more efficient than fully-fledged OWL 2 reasoners, but they are not complete for ontologies containing (even if just a few) axioms outside the relevant fragment. In this paper, we propose a novel classification technique that combines an OWL 2 reasoner and an efficient reasoner for a given fragment in such a way that the bulk of the workload is assigned to the latter. Reasoners are combined in a black-box modular manner, and the specifics of their implementation (and even of their reasoning technique) are irrelevant to our approach. © 2012 Springer-Verlag Berlin Heidelberg.",,"Black boxes; Classification technique; Ontology design; Reasoner; Reasoning tasks; Reasoning techniques; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868556482
"De Falco I., Della Cioppa A., Maisto D., Scafuri U., Tarantino E.","Biological invasion-inspired migration in distributed evolutionary algorithms",2012,"Information Sciences",25,10.1016/j.ins.2012.04.027,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861621768&doi=10.1016%2fj.ins.2012.04.027&partnerID=40&md5=e0f147843f15abe3d9e1c3120c943c1a","Migration strategy plays an important role in designing effective distributed evolutionary algorithms. In this work, a novel migration model inspired to the phenomenon known as biological invasion is devised. The migration strategy is implemented through a multistage process involving invading subpopulations and their competition with native individuals. Such a general approach is used within a stepping-stone parallel model adopting Differential Evolution as the local algorithm. The resulting distributed algorithm is evaluated on a wide set of classical test functions against a large number of sequential and other distributed versions of Differential Evolution available in literature. The findings show that, in most of the cases, the proposed algorithm is able to achieve better performance in terms of both solution quality and convergence rate. © 2012 Elsevier Inc. All rights reserved.","Biological invasion; Distributed evolutionary algorithm; Massive migration","Biological invasion; Classical tests; Convergence rates; Differential Evolution; Distributed evolutionary algorithms; General approach; Local algorithm; Massive migration; Migration model; Migration strategy; Multistage process; Parallel models; Solution quality; Stepping stone; Artificial intelligence; Software engineering; Evolutionary algorithms",Article,Scopus,2-s2.0-84861621768
"Ye Q., Zhao C., Gao S., Zheng H.","Weighted Twin Support Vector Machines with Local Information and its application",2012,"Neural Networks",25,10.1016/j.neunet.2012.06.010,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865559034&doi=10.1016%2fj.neunet.2012.06.010&partnerID=40&md5=4bae78bc0ac6b363dd50cc1fd7a2986e","A Twin Support Vector Machine (TWSVM), as a variant of a Multisurface Proximal Support Vector Machine via Generalized Eigenvalues (GEPSVM), attempts to improve the generalization of GEPSVM, whose solution follows from solving two quadratic programming problems (QPPs), each of which is smaller than in a standard SVM. Unfortunately, the two QPPs still lead to rather high computational costs. Moreover, although TWSVM has better classification performance than GEPSVM, a major disadvantage is it fails to exploit the underlying correlation or similarity information between any pair of data points with the same labels that may be important for classification performance as much as possible. To mitigate the above deficiencies, in this paper, we propose a novel nonparallel plane classifier, called Weighted Twin Support Vector Machines with Local Information (WLTSVM). WLTSVM mines as much underlying similarity information within samples as possible. This method not only retains the superior characteristics of TWSVM, but also has its additional advantages: (1) comparable or better classification accuracy compared to SVM, GEPSVM and TWSVM; (2) taking motivation from standard SVM, the concept of support vectors is retained; (3) more efficient than TWSVM in terms of computational costs; and (4) only one penalty parameter is considered as opposed to two in TWSVM. Finally, experiments on both simulated and real problems confirm the effectiveness of our method. © 2012 Elsevier Ltd.","GEPSVM; Similarity information; Support vector; WLTSVM","Classification accuracy; Classification performance; Computational costs; Data points; Generalized eigenvalues; GEPSVM; Local information; Multi-surface; Penalty parameters; Proximal support vector machines; Quadratic programming problems; Real problems; Similarity information; Support vector; Twin support vector machines; WLTSVM; Eigenvalues and eigenfunctions; Support vector machines; accuracy; analytical parameters; article; comparative study; information processing; multisurface proximal support vector machine via generalized eigenvalues; priority journal; statistical analysis; support vector machine; twin support vector machine; weighted twin support vector machine with local information; Algorithms; Artificial Intelligence; Computer Simulation; Pattern Recognition, Automated; Support Vector Machines",Article,Scopus,2-s2.0-84865559034
"Bouchard B., Imbeault F., Bouzouane A., Menelas B.-A.J.","Developing serious games specifically adapted to people suffering from Alzheimer",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",25,10.1007/978-3-642-33687-4_21,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867623979&doi=10.1007%2f978-3-642-33687-4_21&partnerID=40&md5=38c16400cce0382032a88581105a484e","To face new challenges caused by society aging, several researchers have initiated the experimentation of serious games as a re-education platform to help slowing down the decline of people suffering from Alzheimer. In the last few years, academic studies have been conducted and some commercial products (Nintendo's Brain Age, Big Brain Academy, etc.) have emerged. Nevertheless, these initiatives suffer from multiple important limitations since they do not really suit perceptual and interaction needs of silver-aged gamers, more specifically people suffering from Alzheimer disease. In an effort to address this important issue, we present in this paper a set of specific guidelines for designing and implementing effective serious games targeting silver-aged and Alzheimer's patients. Our guidelines cover the following aspects: (i) choosing right in-game challenges, (ii) designing appropriate interaction mechanisms for cognitively impaired people, (iii) implementing artificial intelligence for providing adequate assistive prompting and dynamic difficulty adjustments, (iv) producing effective visual and auditory assets to maximize cognitive training. Also, as a case study, we present the prototype of our new serious game for Alzheimer's patients. © 2012 Springer-Verlag.","Adaptation; Alzheimer disease; Cognitive training; personalization; Serious games","Adaptation; Alzheimer disease; Cognitive training; Personalizations; Serious games; Artificial intelligence; Neurodegenerative diseases",Conference Paper,Scopus,2-s2.0-84867623979
"Vardoulakis L.P., Ring L., Barry B., Sidner C.L., Bickmore T.","Designing relational agents as long term social companions for older adults",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",25,10.1007/978-3-642-33197-8-30,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867548147&doi=10.1007%2f978-3-642-33197-8-30&partnerID=40&md5=02e8fdc80b52ece49b2485380369a146","Older adults with strong social connections are at a reduced risk for health problems and mortality. We describe two field studies to inform the development of a virtual agent designed to provide long-term, continuous social support to isolated older adults. Findings include the topics that older adults would like to discuss with a companion agent, in addition to overall reactions to interacting with a remote-controlled companion agent installed in their home for a week. Results indicate a generally positive attitude towards companion agents and a rich research agenda for virtual companion agents. © 2012 Springer-Verlag Berlin Heidelberg.","relational agents; social dialogue; social interfaces; wizard-of-oz study","Field studies; Older adults; Overall reactions; Positive attitude; Relational agents; Research agenda; Social connection; Social dialogue; Social interfaces; Social support; Virtual agent; Wizard of Oz; Artificial intelligence; Intelligent virtual agents",Conference Paper,Scopus,2-s2.0-84867548147
"Lehmann K., Turhan A.-Y.","A framework for semantic-based similarity measures for ELH-concepts",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",25,10.1007/978-3-642-33353-8_24,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866907922&doi=10.1007%2f978-3-642-33353-8_24&partnerID=40&md5=c4ca326501c50ff3168c3e76749bdb13","Similarity measures for concepts written in Description Logics (DLs) are often devised based on the syntax of concepts or simply by adjusting them to a set of instance data. These measures do not take the semantics of the concepts into account and can thus lead to unintuitive results. It even remains unclear how these measures behave if applied to new domains or new sets of instance data. In this paper we develop a framework for similarity measures for ELH-concept descriptions based on the semantics of the DL ELH. We show that our framework ensures that the measures resulting from instantiations fulfill fundamental properties , such as equivalence invariance, yet the framework provides the flexibility to adjust measures to specifics of the modelled domain. © 2012 Springer-Verlag.",,"Description logic; Fundamental properties; Similarity measure; Artificial intelligence; Data description; Semantics",Conference Paper,Scopus,2-s2.0-84866907922
"Van Der Aalst W.M.P.","A decade of business process management conferences: Personal reflections on a developing discipline",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",25,10.1007/978-3-642-32885-5_1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866426254&doi=10.1007%2f978-3-642-32885-5_1&partnerID=40&md5=03e9ad6afb2f9996f806e8aa273725eb","The Business Process Management (BPM) conference series celebrates its tenth anniversary. This is a nice opportunity to reflect on a decade of BPM research. This paper describes the history of the conference series, enumerates twenty typical BPM use cases, and identifies six key BPM concerns: process modeling languages, process enactment infrastructures, process model analysis, process mining, process flexibility, and process reuse. Although BPM matured as a research discipline, there are still various important open problems. Moreover, despite the broad interest in BPM, the adoption of state-of-the-art results by software vendors, consultants, and end-users leaves much to be desired. Hence, the BPM discipline should not shy away from the key challenges and set clear targets for the next decade. © 2012 Springer-Verlag.",,"Business process management; End-users; Process enactment; Process flexibility; Process mining; Process model; Process modeling language; Software vendors; Artificial intelligence; Enterprise resource management",Conference Paper,Scopus,2-s2.0-84866426254
"Ermis E., Schäf M., Wies T.","Error invariants",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",25,10.1007/978-3-642-32759-9_17,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865966656&doi=10.1007%2f978-3-642-32759-9_17&partnerID=40&md5=2ae98ecc1255a78d2b661863bc699d54","Localizing the cause of an error in an error trace is one of the most time-consuming aspects of debugging. We develop a novel technique to automate this task. For this purpose, we introduce the concept of error invariants. An error invariant for a position in an error trace is a formula over program variables that over-approximates the reachable states at the given position while only capturing states that will still produce the error, if execution of the trace is continued from that position. Error invariants can be used for slicing error traces and for obtaining concise error explanations. We present an algorithm that computes error invariants from Craig interpolants, which we construct from proofs of unsatisfiability of formulas that explain why an error trace violates a particular correctness assertion. We demonstrate the effectiveness of our algorithm by using it to localize faults in real-world programs. © 2012 Springer-Verlag.",,"Craig interpolants; Error explanation; Novel techniques; Program variables; Artificial intelligence; Algorithms",Conference Paper,Scopus,2-s2.0-84865966656
"Malar E., Kandaswamy A., Chakravarthy D., Giri Dharan A.","A novel approach for detection and classification of mammographic microcalcifications using wavelet analysis and extreme learning machine",2012,"Computers in Biology and Medicine",25,10.1016/j.compbiomed.2012.07.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865540205&doi=10.1016%2fj.compbiomed.2012.07.001&partnerID=40&md5=99f46326d16848d039a03f2cb6dda7dd","The objective of this paper is to reveal the effectiveness of wavelet based tissue texture analysis for microcalcification detection in digitized mammograms using Extreme Learning Machine (ELM). Microcalcifications are tiny deposits of calcium in the breast tissue which are potential indicators for early detection of breast cancer. The dense nature of the breast tissue and the poor contrast of the mammogram image prohibit the effectiveness in identifying microcalcifications. Hence, a new approach to discriminate the microcalcifications from the normal tissue is done using wavelet features and is compared with different feature vectors extracted using Gray Level Spatial Dependence Matrix (GLSDM) and Gabor filter based techniques. A total of 120 Region of Interests (ROIs) extracted from 55 mammogram images of mini-Mias database, including normal and microcalcification images are used in the current research. The network is trained with the above mentioned features and the results denote that ELM produces relatively better classification accuracy (94%) with a significant reduction in training time than the other artificial neural networks like Bayesnet classifier, Naivebayes classifier, and Support Vector Machine. ELM also avoids problems like local minima, improper learning rate, and over fitting. © 2012 Elsevier Ltd.","Extreme learning machine; Gabor filter; GLSDM; Microcalcification; Wavelet","Breast tissues; Classification accuracy; Digitized mammograms; Early detection of breast cancer; Extreme learning machine; Feature vectors; Filter-based technique; GLSDM; Gray levels; Learning rates; Local minimums; Mammographic; Microcalcification detection; Microcalcifications; Naive-Bayes classifiers; Normal tissue; Overfitting; Potential indicators; Region of interest; Spatial dependence; Tissue textures; Training time; Wavelet; Wavelet features; Gabor filters; Learning systems; Mammography; Medical imaging; Neural networks; Tissue; Wavelet analysis; X ray screens; Calcification (biochemistry); article; artificial neural network; breast; breast calcification; breast cancer; classifier; diagnostic accuracy; disease classification; machine learning; mammography; priority journal; probability; receiver operating characteristic; support vector machine; wavelet analysis; Algorithms; Artificial Intelligence; Bayes Theorem; Breast Diseases; Calcinosis; Databases, Factual; Female; Humans; Mammography; Radiographic Image Interpretation, Computer-Assisted; ROC Curve; Wavelet Analysis",Article,Scopus,2-s2.0-84865540205
"Lahrmann H., Agerholm N., Tradisauskas N., Berthelsen K.K., Harms L.","Pay as You Speed, ISA with incentive for not speeding: Results and interpretation of speed data",2012,"Accident Analysis and Prevention",25,10.1016/j.aap.2011.03.015,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861857176&doi=10.1016%2fj.aap.2011.03.015&partnerID=40&md5=a689dabba9bdec5752f3fed108507700","To simulate a market introduction of Intelligent Speed Adaptation (ISA) and to study the effect of a Pay as You Speed (PAYS) concept, a field trial with 153 drivers was conducted during 2007-2009. The participants drove under PAYS conditions for a shorter or a longer period. The PAYS concept consisted of informative ISA linked with economic incentive for not speeding, measured through automatic count of penalty points whenever the speed limit was exceeded. The full incentive was set to 30% of a participant's insurance premium. The participants were exposed to different treatments, with and without incentive crossed with informative ISA present or absent. The results showed that ISA is an efficient tool for reducing speeding particularly on rural roads. The analysis of speed data demonstrated that the proportion of distance driven above the speed where the ISA equipment responded (PDA) was a sensitive measure for reflecting the effect of ISA, whereas mean free flow speed and the 85th percentile speed, were less sensitive to ISA effects. The PDA increased a little over time but still remained at a low level; however, when ISA was turned off, the participants' speeding relapsed to the baseline level. Both informative ISA and incentive ISA reduced the PDA, but there was no statistically significant interaction. Informative reduced it more than the incentive. © 2011 Elsevier Ltd.","Car insurance premium; Economic incentive; Field trial; Intelligent Speed Adaptation; Pay As You Drive; Speed management","Car Insurance; Economic incentive; Field trial; Intelligent speed adaptation; Speed management; Insurance; Speed control; Speed; acceleration; accident prevention; adolescent; adult; aged; article; artificial intelligence; car; car driving; clinical trial; controlled clinical trial; controlled study; Denmark; economics; female; human; instrumentation; insurance; law enforcement; legal aspect; male; methodology; middle aged; psychological aspect; randomized controlled trial; traffic accident; Acceleration; Accident Prevention; Accidents, Traffic; Adolescent; Adult; Aged; Artificial Intelligence; Automobile Driving; Automobiles; Denmark; Female; Humans; Insurance; Law Enforcement; Male; Middle Aged; Young Adult",Article,Scopus,2-s2.0-84861857176
"Kadadevaramath R.S., Chen J.C.H., Latha Shankar B., Rameshkumar K.","Application of particle swarm intelligence algorithms in supply chain network architecture optimization",2012,"Expert Systems with Applications",25,10.1016/j.eswa.2012.02.116,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862785546&doi=10.1016%2fj.eswa.2012.02.116&partnerID=40&md5=7264797f59d187b3293eaf838cfa558f","In today's globalization, the success of an industry is dependent on cost effective supply chain management under various markets, logistics and production uncertainties. Uncertainties in the supply chain usually decrease profit, i.e. increase total supply chain cost. Demand uncertainty and constraints posed by the every echelon are important factors to be considered in the supply chain design operations. Optimization is no longer a luxury but has become the order of the day. This paper specifically deals with the modeling and optimization of a three echelon supply chain network using the particle swarm optimization/intelligence algorithms. © 2012 Elsevier Ltd. All rights reserved.","Integration; Optimization; Particle swarm optimization/intelligence; Supply chain management","Architecture optimization; Cost effective; Demand uncertainty; Modeling and optimization; Particle swarm; Production uncertainty; Supply chain costs; Supply chain design; Supply chain network; Three-echelon; Algorithms; Artificial intelligence; Integration; Network architecture; Profitability; Supply chain management; Optimization",Article,Scopus,2-s2.0-84862785546
"Lo K., Raftery A.E., Dombek K.M., Zhu J., Schadt E.E., Bumgarner R.E., Yeung K.Y.","Integrating external biological knowledge in the construction of regulatory networks from time-series expression data",2012,"BMC Systems Biology",25,10.1186/1752-0509-6-101,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864946487&doi=10.1186%2f1752-0509-6-101&partnerID=40&md5=f1ad05080a22ba0ebbf455a5271b52aa","Background: Inference about regulatory networks from high-throughput genomics data is of great interest in systems biology. We present a Bayesian approach to infer gene regulatory networks from time series expression data by integrating various types of biological knowledge.Results: We formulate network construction as a series of variable selection problems and use linear regression to model the data. Our method summarizes additional data sources with an informative prior probability distribution over candidate regression models. We extend the Bayesian model averaging (BMA) variable selection method to select regulators in the regression framework. We summarize the external biological knowledge by an informative prior probability distribution over the candidate regression models.Conclusions: We demonstrate our method on simulated data and a set of time-series microarray experiments measuring the effect of a drug perturbation on gene expression levels, and show that it outperforms leading regression-based methods in the literature. © 2012 Lo et al.; licensee BioMed Central Ltd.","Data integration; Model uncertainty; Network inference; Statistics; Systems biology; Time-series expression data","transcription factor; transcriptome; article; artificial intelligence; Bayes theorem; binding site; feedback system; gene regulatory network; metabolism; methodology; probability; systems biology; time; Artificial Intelligence; Bayes Theorem; Binding Sites; Feedback, Physiological; Gene Regulatory Networks; Probability; Systems Biology; Time Factors; Transcription Factors; Transcriptome",Article,Scopus,2-s2.0-84864946487
"Chowdhary G., Kataya A.R.A., Lingner T., Reumann S.","Non-canonical peroxisome targeting signals: identification of novel PTS1 tripeptides and characterization of enhancer elements by computational permutation analysis",2012,"BMC Plant Biology",25,10.1186/1471-2229-12-142,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864820737&doi=10.1186%2f1471-2229-12-142&partnerID=40&md5=dbd9af53ba7fecd1519a0b9998118a4e","Background: High-accuracy prediction tools are essential in the post-genomic era to define organellar proteomes in their full complexity. We recently applied a discriminative machine learning approach to predict plant proteins carrying peroxisome targeting signals (PTS) type 1 from genome sequences. For Arabidopsis thaliana 392 gene models were predicted to be peroxisome-targeted. The predictions were extensively tested in vivo, resulting in a high experimental verification rate of Arabidopsis proteins previously not known to be peroxisomal.Results: In this study, we experimentally validated the predictions in greater depth by focusing on the most challenging Arabidopsis proteins with unknown non-canonical PTS1 tripeptides and prediction scores close to the threshold. By in vivo subcellular targeting analysis, three novel PTS1 tripeptides (QRL>, SQM>, and SDL>) and two novel tripeptide residues (Q at position -3 and D at pos. -2) were identified. To understand why, among many Arabidopsis proteins carrying the same C-terminal tripeptides, these proteins were specifically predicted as peroxisomal, the residues upstream of the PTS1 tripeptide were computationally permuted and the changes in prediction scores were analyzed. The newly identified Arabidopsis proteins were found to contain four to five amino acid residues of high predicted targeting enhancing properties at position -4 to -12 in front of the non-canonical PTS1 tripeptide. The identity of the predicted targeting enhancing residues was unexpectedly diverse, comprising besides basic residues also proline, hydroxylated (Ser, Thr), hydrophobic (Ala, Val), and even acidic residues.Conclusions: Our computational and experimental analyses demonstrate that the plant PTS1 tripeptide motif is more diverse than previously thought, including an increasing number of non-canonical sequences and allowed residues. Specific targeting enhancing elements can be predicted for particular sequences of interest and are far more diverse in amino acid composition and positioning than previously assumed. Machine learning methods become indispensable to predict which specific proteins, among numerous candidate proteins carrying the same non-canonical PTS1 tripeptide, contain sufficient enhancer elements in terms of number, positioning and total strength to cause peroxisome targeting. © 2012 Chowdhary et al.; licensee BioMed Central Ltd.",,"Arabidopsis; Arabidopsis thaliana; amino acid; Arabidopsis protein; signal peptide; algorithm; Arabidopsis; article; artificial intelligence; biology; enhancer region; gene locus; genetics; isolation and purification; metabolism; methodology; molecular genetics; peroxisome; plant gene; plant leaf; protein motif; proteomics; sequence alignment; validation study; Algorithms; Amino Acid Motifs; Amino Acids; Arabidopsis; Arabidopsis Proteins; Artificial Intelligence; Computational Biology; Enhancer Elements, Genetic; Genes, Plant; Genetic Loci; Molecular Sequence Data; Peroxisomes; Plant Leaves; Protein Sorting Signals; Proteomics; Sequence Alignment",Article,Scopus,2-s2.0-84864820737
"Pecora F., Cirillo M., Dell'Osa F., Ullberg J., Saffiotti A.","A constraint-based approach for proactive, context-aware human support",2012,"Journal of Ambient Intelligence and Smart Environments",25,10.3233/AIS-2012-0157,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864467200&doi=10.3233%2fAIS-2012-0157&partnerID=40&md5=483c41f221d0dbb6abebfdbafe8f16da","In this article we address the problem of realizing a service-providing reasoning infrastructure for pro-active human assistance in intelligent environments. We propose SAM, an architecture which leverages temporal knowledge represented as relations in Allen's interval algebra and constraint-based temporal planning techniques. SAM provides two key capabilities for contextualized service provision: human activity recognition and planning for controlling pervasive actuation devices. While drawing inspiration from several state-of-the-art approaches, SAM provides a unique feature which has thus far not been addressed in the literature, namely the seamless integration of these two key capabilities. It does so by leveraging a constraint-based reasoning paradigm whereby both requirements for recognition and for planning/execution are represented as constraints and reasoned upon continuously. © 2012 - IOS Press and the authors. All rights reserved.","activity recognition; constraint reasoning; Context awareness; continuous planning; temporal reasoning","Activity recognition; Constraint reasoning; Context- awareness; Continuous planning; Temporal reasoning; Artificial intelligence; Software engineering",Article,Scopus,2-s2.0-84864467200
"Kashid S.S., Maity R.","Prediction of monthly rainfall on homogeneous monsoon regions of India based on large scale circulation patterns using Genetic Programming",2012,"Journal of Hydrology",25,10.1016/j.jhydrol.2012.05.033,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863880488&doi=10.1016%2fj.jhydrol.2012.05.033&partnerID=40&md5=531b94db4b8e96c555bf40e4a89a559e","Prediction of Indian Summer Monsoon Rainfall (ISMR) is of vital importance for Indian economy, and it has been remained a great challenge for hydro-meteorologists due to inherent complexities in the climatic systems. The Large-scale atmospheric circulation patterns from tropical Pacific Ocean (ENSO) and those from tropical Indian Ocean (EQUINOO) are established to influence the Indian Summer Monsoon Rainfall. The information of these two large scale atmospheric circulation patterns in terms of their indices is used to model the complex relationship between Indian Summer Monsoon Rainfall and the ENSO as well as EQUINOO indices. However, extracting the signal from such large-scale indices for modeling such complex systems is significantly difficult. Rainfall predictions have been done for 'All India' as one unit, as well as for five 'homogeneous monsoon regions of India', defined by Indian Institute of Tropical Meteorology. Recent 'Artificial Intelligence' tool 'Genetic Programming' (GP) has been employed for modeling such problem. The Genetic Programming approach is found to capture the complex relationship between the monthly Indian Summer Monsoon Rainfall and large scale atmospheric circulation pattern indices - ENSO and EQUINOO. Research findings of this study indicate that GP-derived monthly rainfall forecasting models, that use large-scale atmospheric circulation information are successful in prediction of All India Summer Monsoon Rainfall with correlation coefficient as good as 0.866, which may appears attractive for such a complex system. A separate analysis is carried out for All India Summer Monsoon rainfall for India as one unit, and five homogeneous monsoon regions, based on ENSO and EQUINOO indices of months of March, April and May only, performed at end of month of May. In this case, All India Summer Monsoon Rainfall could be predicted with 0.70 as correlation coefficient with somewhat lesser Correlation Coefficient (C.C.) values for different 'homogeneous monsoon regions'. © 2012 Elsevier B.V.","El Niño-Southern Oscillation (ENSO); Equatorial Indian Ocean Oscillation (EQUINOO); Genetic Programming (GP); Indian Summer Monsoon Rainfall (ISMR)","Atmospheric circulation; Atmospheric circulation patterns; Climatic systems; Complex relationships; Correlation coefficient; EQUINOO; Indian summer monsoon rainfall; Inherent complexity; Large-scale circulation patterns; Rainfall forecasting; Rainfall prediction; Separate analysis; Summer monsoon rainfall; Tropical Indian ocean; Tropical meteorology; Tropical Pacific ocean; Artificial intelligence; Atmospheric pressure; Atmospheric thermodynamics; Climatology; Forecasting; Genetic programming; Large scale systems; Nickel compounds; Signal processing; Rain; artificial intelligence; atmospheric circulation; El Nino-Southern Oscillation; homogeneity; hydrometeorology; monsoon; prediction; rainfall; Indian Ocean; Indian Ocean (Tropical); Pacific Ocean; Pacific Ocean (Tropical)",Article,Scopus,2-s2.0-84863880488
"Mahanand B.S., Suresh S., Sundararajan N., Aswatha Kumar M.","Identification of brain regions responsible for Alzheimer's disease using a Self-adaptive Resource Allocation Network",2012,"Neural Networks",25,10.1016/j.neunet.2012.02.035,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861781339&doi=10.1016%2fj.neunet.2012.02.035&partnerID=40&md5=c0c15dd892e3b189cc0e7bb6556544b1","In this paper, we present a novel approach for the identification of brain regions responsible for Alzheimer's disease using the Magnetic Resonance (MR) images. The approach incorporates the recently developed Self-adaptive Resource Allocation Network (SRAN) for Alzheimer's disease classification using voxel-based morphometric features of MR images. SRAN classifier uses a sequential learning algorithm, employing self-adaptive thresholds to select the appropriate training samples and discard redundant samples to prevent over-training. These selected training samples are then used to evolve the network architecture efficiently. Since, the number of features extracted from the MR images is large, a feature selection scheme (to reduce the number of features needed) using an Integer-Coded Genetic Algorithm (ICGA) in conjunction with the SRAN classifier (referred to here as the ICGA-SRAN classifier) have been developed. In this study, different healthy/Alzheimer's disease patient's MR images from the Open Access Series of Imaging Studies data set have been used for the performance evaluation of the proposed ICGA-SRAN classifier. We have also compared the results of the ICGA-SRAN classifier with the well-known Support Vector Machine (SVM) and Extreme Learning Machine (ELM) classifiers. The study results clearly show that the ICGA-SRAN classifier produces a better generalization performance with a smaller number of features, lower misclassification rate and a compact network. The ICGA-SRAN selected features clearly indicate that the variations in the gray matter volume in the parahippocampal gyrus and amygdala brain regions may be good indicators of the onset of Alzheimer's disease in normal persons. © 2012 Elsevier Ltd.","Alzheimer's disease; Integer coded genetic algorithm; Magnetic Resonance Imaging; Self-adaptive Resource Allocation Network; Voxel-based morphometry","Alzheimer's disease; Brain regions; Data sets; Extreme learning machine; Generalization performance; Gray matter; Misclassification rates; MR images; Open Access; Performance evaluation; Redundant samples; Resource allocation networks; Self-adaptive; Sequential learning algorithm; Training sample; Voxel-based morphometry; Brain; Genetic algorithms; Learning algorithms; Magnetic resonance imaging; Network architecture; Resource allocation; Sampling; Support vector machines; Image processing; Alzheimer disease; amygdaloid nucleus; anatomical variation; article; artificial neural network; brain region; classification algorithm; classifier; genetic algorithm; gray matter; human; intermethod comparison; learning algorithm; network learning; neuroimaging; nuclear magnetic resonance imaging; parahippocampal gyrus; priority journal; sensitivity and specificity; support vector machine; voxel based morphometry; Aged; Algorithms; Alzheimer Disease; Amygdala; Artificial Intelligence; Brain; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Models, Genetic; Neural Networks (Computer); Neurons; Parahippocampal Gyrus; Resource Allocation; Support Vector Machines",Article,Scopus,2-s2.0-84861781339
"Boccato L., Lopes A., Attux R., Von Zuben F.J.","An extended echo state network using Volterra filtering and principal component analysis",2012,"Neural Networks",25,10.1016/j.neunet.2012.02.028,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861784861&doi=10.1016%2fj.neunet.2012.02.028&partnerID=40&md5=f0a51a3db05166e8f88dab058738bd36","Echo state networks (ESNs) can be interpreted as promoting an encouraging compromise between two seemingly conflicting objectives: (i) simplicity of the resulting mathematical model and (ii) capability to express a wide range of nonlinear dynamics. By imposing fixed weights to the recurrent connections, the echo state approach avoids the well-known difficulties faced by recurrent neural network training strategies, but still preserves, to a certain extent, the potential of the underlying structure due to the existence of feedback loops within the dynamical reservoir. Moreover, the overall training process is relatively simple, as it amounts essentially to adapting the readout, which usually corresponds to a linear combiner. However, the linear nature of the output layer may limit the capability of exploring the available information, since higher-order statistics of the signals are not taken into account. In this work, we present a novel architecture for an ESN in which the linear combiner is replaced by a Volterra filter structure. Additionally, the principal component analysis technique is used to reduce the number of effective signals transmitted to the output layer. This idea not only improves the processing capability of the network, but also preserves the simplicity of the training process. The proposed architecture is then analyzed in the context of a set of representative information extraction problems, more specifically supervised and unsupervised channel equalization, and blind separation of convolutive mixtures. The obtained results, when compared to those produced by already proposed ESN versions, highlight the benefits brought by the novel network proposal and characterize it as a promising tool to deal with challenging signal processing tasks. © 2012 Elsevier Ltd.","Channel equalization; Echo state networks; Principal component analysis; Source separation; Volterra filtering","Blind separation; Channel equalization; Convolutive mixture; Echo state networks; Feed-back loop; Higher order; Information Extraction; Linear combiner; Neural network training; Novel architecture; Output layer; Principal Components; Processing capability; Proposed architectures; Training process; Volterra filter; Volterra filtering; Mathematical models; Principal component analysis; Recurrent neural networks; Signal processing; Source separation; State feedback; Network architecture; article; artificial neural network; computer prediction; echo state network; feedback system; learning algorithm; linear system; mathematical computing; mathematical model; network learning; nonlinear system; prediction; principal component analysis; priority journal; signal processing; statistical analysis; structure analysis; Algorithms; Artificial Intelligence; Computer Systems; Entropy; Linear Models; Neural Networks (Computer); Nonlinear Dynamics; Principal Component Analysis; Signal Processing, Computer-Assisted; Software",Article,Scopus,2-s2.0-84861784861
"Jang J.H., Bae Y., Ra J.B.","Contrast-enhanced fusion of multisensor images using subband-decomposed multiscale retinex",2012,"IEEE Transactions on Image Processing",25,10.1109/TIP.2012.2197014,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864139944&doi=10.1109%2fTIP.2012.2197014&partnerID=40&md5=40a550d5bb968deb21a274cb187d36a2","In this paper, we propose a novel pixel-level multisensor image fusion algorithm with simultaneous contrast enhancement. In order to accomplish both image fusion and contrast enhancement simultaneously, we suggest a modified framework of the subband-decomposed multiscale retinex (SDMSR), our previous contrast enhancement algorithm. This framework is based on a fusion strategy that reflects the multiscale characteristics of the SDMSR well. We first apply two complementary intensity transfer functions to source images in order to effectively utilize hidden information in both shadows and highlights in the fusion process. We then decompose retinex outputs into nearly nonoverlapping spectral subbands. The decomposed retinex outputs are then fused subband-by-subband, by using global weighting as well as local weighting to overcome the limitations of the pixel-based fusion approach. After the fusion process, we apply a space-varying subband gain to each fused SD retinex output according to the subband characteristic so that the contrast of the fused image can be effectively enhanced. In addition, in order to effectively manage artifacts and noise, we make the degree of enhancement of fused details adjustable by improving a detail adjustment function. From experiments with various multisensor image pairs, the results clearly demonstrate that even if source images have poor contrast, the proposed algorithm makes it possible to generate a fused image with highly enhanced contrast while preserving visually salient information contained in the source images. © 1992-2012 IEEE.","Contrast enhancement; detail adjustment function; multisensor image fusion; pixel-level image fusion; subband-decomposed multiscale retinex (SDMSR)","Contrast Enhancement; Contrast-enhanced; Enhanced contrast; Fused images; Fusion process; Fusion strategies; Hidden information; Multi sensor images; Multi-scale Retinex; Multiscales; Multisensor image fusion; Nonoverlapping; Pixel-based fusion; Pixel-level image fusion; Retinex; Simultaneous contrast; Source images; Subbands; Algorithms; Pixels; Image fusion; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; image enhancement; image subtraction; methodology; multimodal imaging; reproducibility; sensitivity and specificity; automated pattern recognition; computer assisted diagnosis; image enhancement; multimodal imaging; procedures; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Multimodal Imaging; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Multimodal Imaging; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-84864139944
"Kangas L.J., Metz T.O., Isaac G., Schrom B.T., Ginovska-Pangovska B., Wang L., Tan L., Lewis R.R., Miller J.H.","In silico identification software (ISIS): A machine learning approach to tandem mass spectral identification of lipids",2012,"Bioinformatics",25,10.1093/bioinformatics/bts194,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864008587&doi=10.1093%2fbioinformatics%2fbts194&partnerID=40&md5=fc48acc1441d917df9087591f963b2e2","Motivation: Liquid chromatography-mass spectrometry-based metabolomics has gained importance in the life sciences, yet it is not supported by software tools for high throughput identification of metabolites based on their fragmentation spectra. An algorithm (ISIS: in silico identification software) and its implementation are presented and show great promise in generating in silico spectra of lipids for the purpose of structural identification. Instead of using chemical reaction rate equations or rules-based fragmentation libraries, the algorithm uses machine learning to find accurate bond cleavage rates in a mass spectrometer employing collision-induced dissociation tandem mass spectrometry. Results: A preliminary test of the algorithm with 45 lipids from a subset of lipid classes shows both high sensitivity and specificity. Published by Oxford University Press 2012. All rights reserved.",,"lipid; algorithm; article; artificial intelligence; chemistry; computer program; computer simulation; metabolomics; methodology; sensitivity and specificity; tandem mass spectrometry; Algorithms; Artificial Intelligence; Computer Simulation; Lipids; Metabolomics; Sensitivity and Specificity; Software; Tandem Mass Spectrometry",Article,Scopus,2-s2.0-84864008587
"Joo K., Lee S.J., Lee J.","Sann: Solvent accessibility prediction of proteins by nearest neighbor method",2012,"Proteins: Structure, Function and Bioinformatics",25,10.1002/prot.24074,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862184719&doi=10.1002%2fprot.24074&partnerID=40&md5=8890548461e5fdd8527d98745ab6f669","We present a method to predict the solvent accessibility of proteins which is based on a nearest neighbor method applied to the sequence profiles. Using the method, continuous real-value prediction as well as two-state and three-state discrete predictions can be obtained. The method utilizes the z-score value of the distance measure in the feature vector space to estimate the relative contribution among the k-nearest neighbors for prediction of the discrete and continuous solvent accessibility. The Solvent accessibility database is constructed from 5717 proteins extracted from PISCES culling server with the cutoff of 25% sequence identities. Using optimal parameters, the prediction accuracies (for discrete predictions) of 78.38% (two-state prediction with the threshold of 25%), 65.1% (three-state prediction with the thresholds of 9 and 36%), and the Pearson correlation coefficient (between the predicted and true RSA's for continuous prediction) of 0.676 are achieved An independent benchmark test was performed with the CASP8 targets where we find that the proposed method outperforms existing methods. The prediction accuracies are 80.89% (for two state prediction with the threshold of 25%), 67.58% (three-state prediction), and the Pearson correlation coefficient of 0.727 (for continuous prediction) with mean absolute error of 0.148. We have also investigated the effect of increasing database sizes on the prediction accuracy, where additional improvement in the accuracy is observed as the database size increases. The SANN web server is available at. © 2012 Wiley Periodicals, Inc.","Machine learning; Sequence analysis; Sequence profile; Solvent accessibility prediction","solvent; accuracy; analytic method; article; correlation coefficient; false negative result; false positive result; nearest neighbor method; prediction; priority journal; protein database; protein secondary structure; quality control; reliability; Algorithms; Amino Acid Sequence; Artificial Intelligence; Databases, Protein; Models, Chemical; Models, Molecular; Molecular Sequence Data; Proteins; Sequence Analysis, Protein; Solvents",Article,Scopus,2-s2.0-84862184719
"Martins I.F., Teixeira A.L., Pinheiro L., Falcao A.O.","A Bayesian approach to in Silico blood-brain barrier penetration modeling",2012,"Journal of Chemical Information and Modeling",25,10.1021/ci300124c,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862890235&doi=10.1021%2fci300124c&partnerID=40&md5=bae98f4ac16897dccfaf07ab69100430","The human blood-brain barrier (BBB) is a membrane that protects the central nervous system (CNS) by restricting the passage of solutes. The development of any new drug must take into account its existence whether for designing new molecules that target components of the CNS or, on the other hand, to find new substances that should not penetrate the barrier. Several studies in the literature have attempted to predict BBB penetration, so far with limited success and few, if any, application to real world drug discovery and development programs. Part of the reason is due to the fact that only about 2% of small molecules can cross the BBB, and the available data sets are not representative of that reality, being generally biased with an over-representation of molecules that show an ability to permeate the BBB (BBB positives). To circumvent this limitation, the current study aims to devise and use a new approach based on Bayesian statistics, coupled with state-of-the-art machine learning methods to produce a robust model capable of being applied in real-world drug research scenarios. The data set used, gathered from the literature, totals 1970 curated molecules, one of the largest for similar studies. Random Forests and Support Vector Machines were tested in various configurations against several chemical descriptor set combinations. Models were tested in a 5-fold cross-validation process, and the best one tested over an independent validation set. The best fitted model produced an overall accuracy of 95%, with a mean square contingency coefficient (φ) of 0.74, and showing an overall capacity for predicting BBB positives of 83% and 96% for determining BBB negatives. This model was adapted into a Web based tool made available for the whole community at http://b3pp.lasige.di.fc.ul.pt. © 2012 American Chemical Society.",,"Bayesian approaches; Bayesian statistics; Blood-brain barrier; Central nervous systems; Cross validation; Data sets; Descriptors; Development programs; Drug discovery; Drug research; In-silico; Mean square; Random forests; Robust models; Small molecules; State-of-the-art machine learning methods; Target component; Web-based tools; Bayesian networks; Blood; Decision trees; Learning systems; Molecules; article; artificial intelligence; Bayes theorem; blood brain barrier; human; probability; statistical model; support vector machine; theoretical model; validation study; Artificial Intelligence; Bayes Theorem; Blood-Brain Barrier; Humans; Likelihood Functions; Models, Theoretical; Probability; Support Vector Machines",Article,Scopus,2-s2.0-84862890235
"Li K., Chen P., Wang H.","Intelligent diagnosis method for rotating machinery using wavelet transform and ant colony optimization",2012,"IEEE Sensors Journal",25,10.1109/JSEN.2012.2191402,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862002031&doi=10.1109%2fJSEN.2012.2191402&partnerID=40&md5=dc1ec1056e6d65b834cd01c0234e293e","This paper proposes an intelligent diagnosis method for condition diagnosis of rotating machinery by using wavelet transform (WT) and ant colony optimization (ACO), in order to detect faults and distinguish fault types at an early stage. The WT is used to extract a feature signal of each machine state from a measured vibration signal for for high-accuracy condition diagnosis. The decision method of optimum frequency area for the extraction of the feature signal is discussed by using real plant data. We convert the state identification for the condition diagnosis of rotating machinery to a clustering problem of the values of the nondimensional symptom parameters (NSPs). ACO is introduced for this purpose. NSPs are calculated with the recomposed signals of each frequency level. These parameters can reflect the characteristics of the signals measured for the condition diagnosis. The synthetic detection index (SDI), on the basis of statistical theory, is defined to evaluate the applicability of the NSPs. The SDI can be used to select better NSPs for the ACO. Practical examples of diagnosis for a bearing used in the centrifugal fan system are shown to verify the effectiveness of the methods proposed in this paper. © 2012 IEEE.","Ant colony optimization; nondimensional symptom parameters; rotating machinery; wavelet transform","Ant Colony Optimization (ACO); Centrifugal fans; Clustering problems; Decision method; Fault types; High-accuracy; Intelligent diagnosis methods; Machine state; Nondimensional; Optimum frequency; Plant data; State identification; Statistical theory; Symptom parameters; Vibration signal; Algorithms; Artificial intelligence; Centrifugation; Data handling; Rotating machinery; Wavelet transforms; Signal detection",Article,Scopus,2-s2.0-84862002031
"Wang D.","Basic framework and key technology for a new generation of data center in electric power corporation based on cloud computation",2012,"Dianli Xitong Zidonghua/Automation of Electric Power Systems",25,10.3969/j.issn.1000-1026.2012.11.012,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863847152&doi=10.3969%2fj.issn.1000-1026.2012.11.012&partnerID=40&md5=60952c671c4f6905fbb2ce7cbcaf230f","In order to meet the developing demand of the new generation of data center, and to better satisfy the need of the smart grid, the basic architecture of the data center in an electric power corporation based on cloud computation is proposed. Based on an analysis of the logical structure and function of the existing data center in the electric power corporation, a server virtualization method is given to improve resource utilization efficiency, to guarantee the availability and scalability of the data center in the electric power corporation. In addition, a cloud computing platform for data center in the electric power corporation is designed to provide an environment for storage management of massive data and computation concerning the operational system, the data mining system and the auxiliary decision support system in the smart grid. Finally, to realize smooth transition from the existing data center to a new one in the electric power corporation, a multi-phase migration strategy is proposed. © 2012 State Grid Electric Power Research Institute Press.","Cloud computing; Data center; Hadoop; Smart grid; Virtualization","Computing platform; Data centers; Data mining system; Electric power corporation; Hadoop; Key technologies; Logical structure; Massive data; Migration strategy; Operational systems; Resource utilizations; Smart grid; Smooth transitions; Storage management; Virtualizations; Artificial intelligence; Cloud computing; Decision support systems; Electric energy storage; Electricity; Industry; Smart power grids; Virtual reality; Information management",Article,Scopus,2-s2.0-84863847152
"Fournier-Viger P., Wu C.-W., Tseng V.S.","Mining top-K association rules",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",25,10.1007/978-3-642-30353-1_6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861752193&doi=10.1007%2f978-3-642-30353-1_6&partnerID=40&md5=09956d085f7718471b7b300ab6183dc4","Mining association rules is a fundamental data mining task. However, depending on the choice of the parameters (the minimum confidence and minimum support), current algorithms can become very slow and generate an extremely large amount of results or generate too few results, omitting valuable information.This is a serious problem because in practice users have limited resources for analyzing the results and thus are often only interested in discovering a certain amount of results, and fine tuning the parameters is time-consuming.To address this problem, we propose an algorithm to mine the top-k association rules, where k is the number of association rules to be found and is set by the user. The algorithm utilizes a new approach for generating association rules named rule expansions and includes several optimizations. Experimental results show that the algorithm has excellent performance and scalability, and that it is an advantageous alternative to classical association rule mining algorithms when the user want to control the number of rules generated. © 2012 Springer-Verlag.","association rule mining; rule expansion; support; top-k rules","Data mining tasks; Excellent performance; Fine tuning; Minimum support; Mining associations; Rule mining algorithms; top-k rules; Algorithms; Artificial intelligence; Data mining; Supports; Association rules",Conference Paper,Scopus,2-s2.0-84861752193
"Zacharaki E.I., Morita N., Bhatt P., O'Rourke D.M., Melhem E.R., Davatzikos C.","Survival analysis of patients with high-grade gliomas based on data mining of imaging variables",2012,"American Journal of Neuroradiology",25,10.3174/ajnr.A2939,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862508110&doi=10.3174%2fajnr.A2939&partnerID=40&md5=9844197d3f7dfdf5dc8737f0eaff2a61","BACKGROUND AND PURPOSE: The prediction of prognosis in HGGs is poor in the majority of patients. Our aim was to test whether multivariate prediction models constructed by machine-learning methods provide a more accurate predictor of prognosis in HGGs than histopathologic classification. The prediction of survival was based on DTI and rCBV measurements as an adjunct to conventional imaging. MATERIALS AND METHODS: The relationship of survival to 55 variables, including clinical parameters (age, sex), categoric or continuous tumor descriptors (eg, tumor location, extent of resection, multi-focality, edema), and imaging characteristics in ROIs, was analyzed in a multivariate fashion by using data-mining techniques. A variable selection method was applied to identify the overall most important variables. The analysis was performed on 74 HGGs (18 anaplastic gliomas WHO grades III/IV and 56 GBMs or gliosarcomas WHO grades IV/IV). RESULTS: Five variables were identified as the most significant, including the extent of resection, mass effect, volume of enhancing tumor, maximum B0 intensity, and mean trace intensity in the nonenhancing/edematous region. These variables were used to construct a prediction model based on a J48 classification tree. The average classification accuracy, assessed by cross-validation, was 85.1%. Kaplan-Meier survival curves showed that the constructed prediction model classified malignant gliomas in a manner that better correlates with clinical outcome than standard histopathology. CONCLUSIONS: Prediction models based on data-mining algorithms can provide a more accurate predictor of prognosis in malignant gliomas than histopathologic classification alone.",,"accuracy; adult; aged; article; brain blood flow; cancer prognosis; data mining; diffusion tensor imaging; glioblastoma; glioma; gliosarcoma; histopathology; human; major clinical study; outcome assessment; prediction; survival rate; Adult; Aged; Aged, 80 and over; Algorithms; Artificial Intelligence; Brain Neoplasms; Data Mining; Databases, Factual; Decision Support Systems, Clinical; Female; Glioma; Humans; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Male; Middle Aged; Pattern Recognition, Automated; Pennsylvania; Prevalence; Proportional Hazards Models; Reproducibility of Results; Risk Factors; Sensitivity and Specificity; Survival Analysis; Survival Rate",Article,Scopus,2-s2.0-84862508110
"Michel L., Van Hentenryck P.","Activity-based search for black-box constraint programming solvers",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",25,10.1007/978-3-642-29828-8_15,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861449125&doi=10.1007%2f978-3-642-29828-8_15&partnerID=40&md5=32dddbc083ee79ec78f64193aeca8567","Robust search procedures are a central component in the design of black-box constraint-programming solvers. This paper proposes activity-based search which uses the activity of variables during propagation to guide the search. Activity-based search was compared experimentally to impact-based search and the wdeg heuristics but not to solution counting heuristics. Experimental results on a variety of benchmarks show that activity-based search is more robust than other heuristics and may produce significant improvements in performance. © 2012 Springer-Verlag.",,"Activity-based; Black boxes; Central component; Constraint programming; Search procedures; Artificial intelligence; Benchmarking; Combinatorial optimization; Computer programming; Constraint theory",Conference Paper,Scopus,2-s2.0-84861449125
"Asarin E., Donzé A., Maler O., Nickovic D.","Parametric identification of temporal properties",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",25,10.1007/978-3-642-29860-8_12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861214456&doi=10.1007%2f978-3-642-29860-8_12&partnerID=40&md5=1ca886bc55525006b5834e725c52e6f7","Given a dense-time real-valued signal and a parameterized temporal logic formula with both magnitude and timing parameters, we compute the subset of the parameter space that renders the formula satisfied by the trace. We provide two preliminary implementations, one which follows the exact semantics and attempts to compute the validity domain by quantifier elimination in linear arithmetics and one which conducts adaptive search in the parameter space. © 2012 Springer-Verlag.",,"Adaptive search; Dense-time; Linear arithmetic; Parameter spaces; Parameterized; Parametric identification; Quantifier elimination; Temporal logic formula; Temporal property; Timing parameters; Artificial intelligence; Semantics",Conference Paper,Scopus,2-s2.0-84861214456
"Kepski M., Kwolek B., Austvoll I.","Fuzzy inference-based reliable fall detection using kinect and accelerometer",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",25,10.1007/978-3-642-29347-4_31,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861060003&doi=10.1007%2f978-3-642-29347-4_31&partnerID=40&md5=06845f0b0751bef3fe50f1c1fd7e145c","Falls are major causes of mortality and morbidity in the elderly. However, prevalent methods only utilize accelerometers or both accelerometers and gyroscopes to separate falls from activities of daily living. This makes it not easy to distinguish real falls from fall-like activities. The existing CCD-camera based solutions require time for installation, camera calibration and are not generally cheap. In this paper we show how to achieve reliable fall detection. The detection is done by a fuzzy inference system using low-cost Kinect and a device consisting of an accelerometer and a gyroscope. The experimental results indicate high accuracy of the detection and effectiveness of the system. © 2012 Springer-Verlag Berlin Heidelberg.",,"Activities of Daily Living; Camera calibration; Fall detection; Fuzzy inference systems; Artificial intelligence; Cameras; Gyroscopes; Soft computing; Accelerometers",Conference Paper,Scopus,2-s2.0-84861060003
"Bostrom N.","The superintelligent will: Motivation and instrumental rationality in advanced artificial agents",2012,"Minds and Machines",25,10.1007/s11023-012-9281-3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865320990&doi=10.1007%2fs11023-012-9281-3&partnerID=40&md5=c90939ed3f8f619b2e04e802a19d1c62","This paper discusses the relation between intelligence and motivation in artificial agents, developing and briefly arguing for two theses. The first, the orthogonality thesis, holds (with some caveats) that intelligence and final goals (purposes) are orthogonal axes along which possible artificial intellects can freely vary-more or less any level of intelligence could be combined with more or less any final goal. The second, the instrumental convergence thesis, holds that as long as they possess a sufficient level of intelligence, agents having any of a wide range of final goals will pursue similar intermediary goals because they have instrumental reasons to do so. In combination, the two theses help us understand the possible range of behavior of superintelligent agents, and they point to some potential dangers in building such an agent. © Springer Science+Business Media, B.V. 2011.","AI; Artificial intelligence; Goal; Instrumental reason; Intelligent agent; Superintelligence","Artificial agents; Goal; In-buildings; Instrumental reason; Orthogonal axes; Orthogonality; Superintelligence; Artificial intelligence; Intelligent agents; Problem solving; Motivation",Article,Scopus,2-s2.0-84865320990
"Derrac J., Triguero I., García S., Herrera F.","Integrating instance selection, instance weighting, and feature weighting for nearest neighbor classifiers by coevolutionary algorithms",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",25,10.1109/TSMCB.2012.2191953,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866498991&doi=10.1109%2fTSMCB.2012.2191953&partnerID=40&md5=a0d06956499fc15e845bad68fca5ac2e","Cooperative coevolution is a successful trend of evolutionary computation which allows us to define partitions of the domain of a given problem, or to integrate several related techniques into one, by the use of evolutionary algorithms. It is possible to apply it to the development of advanced classification methods, which integrate several machine learning techniques into a single proposal. A novel approach integrating instance selection, instance weighting, and feature weighting into the framework of a coevolutionary model is presented in this paper. We compare it with a wide range of evolutionary and nonevolutionary related methods, in order to show the benefits of the employment of coevolution to apply the techniques considered simultaneously. The results obtained, contrasted through nonparametric statistical tests, show that our proposal outperforms other methods in the comparison, thus becoming a suitable tool in the task of enhancing the nearest neighbor classifier. © 1996-2012 IEEE.","Cooperative coevolution; feature weighting (FW); instance selection (IS); instance weighting (IW); nearest neighbor rule","Cooperative co-evolution; Feature weighting; Instance selection; instance weighting (IW); Nearest neighbor rule; Evolutionary algorithms; Statistical tests; Feature extraction; algorithm; article; artificial intelligence; automated pattern recognition; computer simulation; decision support system; methodology; theoretical model; Algorithms; Artificial Intelligence; Computer Simulation; Decision Support Techniques; Models, Theoretical; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84866498991
"Garcia I., Pajares S., Sebastia L., Onaindia E.","Preference elicitation techniques for group recommender systems",2012,"Information Sciences",25,10.1016/j.ins.2011.11.037,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855864055&doi=10.1016%2fj.ins.2011.11.037&partnerID=40&md5=bb2187459e40c4f6718d64233f57639f","A key issue in group recommendation is how to combine the individual preferences of different users that form a group and elicit a profile that accurately reflects the tastes of all members in the group. Most Group Recommender Systems (GRSs) make use of some sort of method for aggregating the preference models of individual users to elicit a recommendation that is satisfactory for the whole group. In general, most GRSs offer good results, but each of them have only been tested in one application domain. This paper describes a domain-independent GRS that has been used in two different application domains. In order to create the group preference model, we select two techniques that are widely used in other GRSs and we compare them with two novel techniques. Our aim is to come up with a model that weighs the preferences of all the individuals to the same extent in such a way that no member in the group is particularly satisfied or dissatisfied with the final recommendations. © 2011 Elsevier Inc. All rights reserved.","Group profile; Group recommender systems; Preference elicitation","Application domains; Group profile; Group recommender systems; Individual preference; Novel techniques; Preference elicitation; Preference elicitation techniques; Preference models; Artificial intelligence; Software engineering; Recommender systems",Article,Scopus,2-s2.0-84855864055
"Yager R.R.","On prioritized multiple-criteria aggregation",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",25,10.1109/TSMCB.2012.2189560,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866358268&doi=10.1109%2fTSMCB.2012.2189560&partnerID=40&md5=05522b07a5200ab130c70593bad916b7","We describe multicriteria aggregation and discuss its central role in many modern applications. The concept of aggregation imperative is introduced to indicate the description of how the individual criteria satisfactions should be combined to obtain the overall score. We focus on a particular type of aggregation imperative called prioritized aggregation that is characteristic of situations where lack of satisfaction to criteria denoted as higher priority cannot be compensated by increased satisfaction by those denoted as lower priority. We discuss two approaches to the formulation of this type of aggregation process. One of these uses the prioritized aggregation operator, and the second is based on an integral-type aggregation using a monotonic set measure to convey the prioritized imperative. © 2012 IEEE.","Aggregation operator; information fusion; integral aggregation; multiple criteria","Aggregation operator; Aggregation process; Modern applications; Multi-criteria; Multiple criteria; Prioritized aggregation; Agglomeration; Information fusion; Mathematical operators; algorithm; article; artificial intelligence; automated pattern recognition; computer simulation; decision support system; methodology; theoretical model; Algorithms; Artificial Intelligence; Computer Simulation; Decision Support Techniques; Models, Theoretical; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84866358268
"Gupta R.K., Krishnamoorthy S., Kusuma D.Y., Lee P.S., Srinivasan M.P.","Enhancing charge-storage capacity of non-volatile memory devices using template-directed assembly of gold nanoparticles",2012,"Nanoscale",25,10.1039/c2nr12134d,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861908579&doi=10.1039%2fc2nr12134d&partnerID=40&md5=670a3f306d244949063449ad79cdcf43","We demonstrate the controlled fabrication of aggregates of gold nanoparticles as a means of enhancing the charge-storage capacity of metal-insulator-semiconductor (MIS) devices by up to 300% at a low biasing voltage of ±4 V. Aggregates of citrate stabilized gold nanoparticles were obtained by directed electrostatic self-assembly onto an underlying nanopattern of positively charged centers. The underlying nanopatterns consist of amine functionalized gold nanoparticle arrays formed using amphiphilic diblock copolymer reverse micelles as templates. The hierarchical self-organization leads to a twelve-fold increase in the number density of the gold nanoparticles and therefore significantly increases the charge storage centers for the MIS device. The MIS structure showed counterclockwise C-V hysteresis curves indicating a good memory effect. A memory window of 1 V was obtained at a low biasing voltage of ±4 V. Furthermore, C-t measurements conducted after applying a charging bias of 4 V showed that the charge was retained beyond 20000 s. The proposed strategy can be readily adapted for fabricating next generation solution processible non-volatile memory devices. © 2012 The Royal Society of Chemistry.",,"Amphiphilic diblock copolymers; Biasing voltages; C-V hysteresis; Charge storage; Electrostatic self assembly; Functionalized; Gold Nanoparticles; Gold-nanoparticle arrays; Memory effects; Memory window; Metal-insulator-semiconductor devices; MIS structure; Nano pattern; Nonvolatile memory devices; Number density; Positively charged; Reverse micelles; Template-directed assembly; Aggregates; Block copolymers; Electrostatic devices; Metal nanoparticles; MIS devices; gold; nanoparticle; polymer; quantum dot; article; artificial intelligence; biological model; chemistry; equipment design; evaluation; instrumentation; methodology; microarray analysis; microtechnology; nanotechnology; productivity; static electricity; surface property; synthesis; Artificial Intelligence; Efficiency; Equipment Design; Gold; Microarray Analysis; Microtechnology; Models, Biological; Nanoparticles; Nanotechnology; Polymers; Quantum Dots; Static Electricity; Surface Properties",Article,Scopus,2-s2.0-84861908579
"Tang H., Chu S.M., Hasegawa-Johnson M., Huang T.S.","Partially supervised speaker clustering",2012,"IEEE Transactions on Pattern Analysis and Machine Intelligence",25,10.1109/TPAMI.2011.174,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859204374&doi=10.1109%2fTPAMI.2011.174&partnerID=40&md5=60d349c1eacf1d59b5a2cce8c08de60f","Content-based multimedia indexing, retrieval, and processing as well as multimedia databases demand the structuring of the media content (image, audio, video, text, etc.), one significant goal being to associate the identity of the content to the individual segments of the signals. In this paper, we specifically address the problem of speaker clustering, the task of assigning every speech utterance in an audio stream to its speaker. We offer a complete treatment to the idea of partially supervised speaker clustering, which refers to the use of our prior knowledge of speakers in general to assist the unsupervised speaker clustering process. By means of an independent training data set, we encode the prior knowledge at the various stages of the speaker clustering pipeline via 1) learning a speaker-discriminative acoustic feature transformation, 2) learning a universal speaker prior model, and 3) learning a discriminative speaker subspace, or equivalently, a speaker-discriminative distance metric. We study the directional scattering property of the Gaussian mixture model (GMM) mean supervector representation of utterances in the high-dimensional space, and advocate exploiting this property by using the cosine distance metric instead of the euclidean distance metric for speaker clustering in the GMM mean supervector space. We propose to perform discriminant analysis based on the cosine distance metric, which leads to a novel distance metric learning algorithmlinear spherical discriminant analysis (LSDA). We show that the proposed LSDA formulation can be systematically solved within the elegant graph embedding general dimensionality reduction framework. Our speaker clustering experiments on the GALE database clearly indicate that 1) our speaker clustering methods based on the GMM mean supervector representation and vector-based distance metrics outperform traditional speaker clustering methods based on the bag of acoustic features representation and statistical model-based distance metrics, 2) our advocated use of the cosine distance metric yields consistent increases in the speaker clustering performance as compared to the commonly used euclidean distance metric, 3) our partially supervised speaker clustering concept and strategies significantly improve the speaker clustering performance over the baselines, and 4) our proposed LSDA algorithm further leads to state-of-the-art speaker clustering performance. © 2012 IEEE.","distance metric learning; partial supervision; Speaker clustering","Acoustic features; Audio stream; Content-based multimedia; Dimensionality reduction; Distance Metric Learning; Distance metrics; Euclidean distance; Gaussian Mixture Model; Graph embeddings; High dimensional spaces; Media content; Multimedia database; partial supervision; Prior knowledge; Scattering property; Speaker clustering; Speech utterance; Supervector; Training data sets; Cluster analysis; Clustering algorithms; Discriminant analysis; Image segmentation; Indexing (materials working); Learning algorithms; Speech recognition; Video signal processing; Audio signal processing; article; artificial intelligence; automated pattern recognition; classification; cluster analysis; discriminant analysis; human; methodology; signal processing; speech; Artificial Intelligence; Cluster Analysis; Discriminant Analysis; Humans; Pattern Recognition, Automated; Signal Processing, Computer-Assisted; Speech",Article,Scopus,2-s2.0-84859204374
"Wu Z., Fang H., She Y.","Weighted average prediction for improving consensus performance of second-order delayed multi-agent systems",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",25,10.1109/TSMCB.2012.2189384,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866382048&doi=10.1109%2fTSMCB.2012.2189384&partnerID=40&md5=2267c21d0479aefe1df6f765389a7d4c","In this paper, the weighted average prediction (WAP) is introduced into the existing consensus protocol for simultaneously improving the robustness to communication delay and the convergence speed of achieving the consensus. The frequency-domain analysis and algebra graph theory are employed to derive the necessary and sufficient condition guaranteeing the second-order delayed multi-agent systems applying the WAP-based consensus protocol to achieve the stationary consensus. It is proved that introducing the WAP with the proper length into the existing consensus protocol can improve the robustness against communication delay. Also, we prove that for two kinds of second-order delayed multi-agent systems: 1) the IR-ones with communication delay approaching zero and 2) the ones with communication delay approaching the maximum delay, introducing the WAP with the proper length into the existing consensus protocol can accelerate the convergence speed of achieving the stationary consensus. © 2012 IEEE.","Communication delay; consensus; convergence speed; multi-agent systems; robustness; weighted average prediction (WAP)","Communication delays; consensus; Convergence speed; Multi agent system (MAS); Weighted averages; Forecasting; Frequency domain analysis; Graph theory; Multi agent systems; Robustness (control systems); Statistical methods; Cellular telephone systems; algorithm; artificial intelligence; automated pattern recognition; computer simulation; decision support system; letter; statistical model; Algorithms; Artificial Intelligence; Computer Simulation; Decision Support Techniques; Models, Statistical; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84866382048
"Coelho A.C., Labadie J.W., Fontane D.G.","Multicriteria Decision Support System for Regionalization of Integrated Water Resources Management",2012,"Water Resources Management",25,10.1007/s11269-011-9961-4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858623346&doi=10.1007%2fs11269-011-9961-4&partnerID=40&md5=c273afa18dfaa57ef7688bfe412ae4fe","Successful implementation of integrated water resources planning and management (IWRM) requires delineation of regions that are relatively homogeneous with respect to multiple criteria, including hydrographic, physical-environmental, socioeconomic, and political-administrative aspects. The water resources planning and management (WARPLAM) DSS is presented as tool for regionalization in support of IWRM through: (1) GIS processing of spatial data related to multiple criteria for defining the homogeneity of clustered base units (e.g., catchments) with respect to multiple criteria; (2) application of fuzzy set theory to development of composite measures of homogeneity over all criteria for alternative clustering of adjacent base units; and (3) development of a modified dynamic programming clustering algorithm that guarantees consistent optimal solutions based on user preferences on the relative importance of the suite of criteria considered for regionalization. The viability of WARPLAM DSS as a tool for regional delineation in support of IWRM is demonstrated through a case study application to the Tocantins-Araguaia River Basin, the second largest in Brazil. © 2012 Springer Science+Business Media B.V.","Clustering methods; Decision support systems; Dynamic programming; Fuzzy sets; Geographic information systems; Integrated water resources management; Multicriteria methods","Clustering methods; Decision supports; Geographic information systems (GIS); Integrated Water Resources Management; Multi-criteria method; Artificial intelligence; Catchments; Clustering algorithms; Decision support systems; Dynamic programming; Fuzzy set theory; Fuzzy sets; Geographic information systems; Integration; Water resources exploration; Water resources; algorithm; cluster analysis; decision support system; fuzzy mathematics; GIS; homogeneity; multicriteria analysis; regionalization; spatial data; water management; water planning; water resource; Araguaia Basin; Brazil; Tocantins River",Article,Scopus,2-s2.0-84858623346
"Kötzing T., Neumann F., Röglin H., Witt C.","Theoretical analysis of two ACO approaches for the traveling salesman problem",2012,"Swarm Intelligence",25,10.1007/s11721-011-0059-7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855338157&doi=10.1007%2fs11721-011-0059-7&partnerID=40&md5=a1d712c4e842b06152406507a8f532c8","Bioinspired algorithms, such as evolutionary algorithms and ant colony optimization, are widely used for different combinatorial optimization problems. These algorithms rely heavily on the use of randomness and are hard to understand from a theoretical point of view. This paper contributes to the theoretical analysis of ant colony optimization and studies this type of algorithm on one of the most prominent combinatorial optimization problems, namely the traveling salesperson problem (TSP). We present a new construction graph and show that it has a stronger local property than one commonly used for constructing solutions of the TSP. The rigorous runtime analysis for two ant colony optimization algorithms, based on these two construction procedures, shows that they lead to good approximation in expected polynomial time on random instances. Furthermore, we point out in which situations our algorithms get trapped in local optima and show where the use of the right amount of heuristic information is provably beneficial. © 2011 Springer Science + Business Media, LLC.","Ant colony optimization; Approximation; Run time analysis; Traveling salesperson problem","Ant Colony Optimization algorithms; Ant-colony optimization; Approximation; Bio-inspired algorithms; Combinatorial optimization problems; Construction procedures; Heuristic information; Local optima; Local property; Polynomial-time; Random instance; Run-time analysis; Theoretical points; Traveling salesperson problem; Approximation algorithms; Artificial intelligence; Combinatorial mathematics; Combinatorial optimization; Constrained optimization; Evolutionary algorithms; Heuristic algorithms; Polynomial approximation; Traveling salesman problem",Article,Scopus,2-s2.0-84855338157
"Shah J.J., Millsap R.E., Woodward J., Smith S.M.","Applied tests of design skills-part 1: Divergent thinking",2012,"Journal of Mechanical Design, Transactions of the ASME",25,10.1115/1.4005594,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863077972&doi=10.1115%2f1.4005594&partnerID=40&md5=5d1fba6c5daced02f17055e495ae73d7","A number of cognitive skills relevant to conceptual design were identified previously. They include divergent thinking (DT), visual thinking (VT), spatial reasoning (SR), qualitative reasoning (QR), and problem formulation (PF). A battery of standardized tests is being developed for these design skills. This paper focuses only on the divergent thinking test. This particular test has been given to over 500 engineering students and a smaller number of practicing engineers. It is designed to evaluate four direct measures (fluency, flexibility, originality, and quality) and four indirect measures (abstractability, afixability, detailability, and decomplexability). The eight questions on the test overlap in some measures and the responses can be used to evaluate several measures independently (e.g., fluency and originality can be evaluated separately from the same idea set). The data on the twentythree measured variables were factor analyzed using both exploratory and confirmatory procedures. A four-factor solution with correlated (oblique) factors was deemed the best available solution after examining solutions with more factors. The indirect measures did not appear to correlate strongly either among themselves or with the other direct measures. The four-factor structure was then taken into a confirmatory factor analytic procedure that adjusted for the missing data. It was found to provide a reasonable fit. Estimated correlations among the four factors (F) ranged from a high of 0.32 for F1 and F2 to a low of 0.06 for F3 and F4. All factor loadings were statistically significant. © 2012 American Society of Mechanical Engineers.",,"Cognitive skill; Design skills; Direct measures; Indirect measure; Missing data; Practicing engineers; Problem formulation; Qualitative reasoning; Spatial reasoning; Visual thinking; Artificial intelligence; Conceptual design; Rating; Testing; Quality control",Article,Scopus,2-s2.0-84863077972
"Maximov I.I., Farrher E., Grinberg F., Jon Shah N.","Spatially variable Rician noise in magnetic resonance imaging",2012,"Medical Image Analysis",25,10.1016/j.media.2011.12.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856229439&doi=10.1016%2fj.media.2011.12.002&partnerID=40&md5=ad61be51a0c11864a41c70db19fedb64","Magnetic resonance images tend to be influenced by various random factors usually referred to as "" noise"" The principal sources of noise and related artefacts can be divided into two types: arising from hardware (acquisition coil arrays, gradient coils, field inhomogeneity); and arising from the subject (physiological noise including body motion, cardiac pulsation or respiratory motion). These factors negatively affect the resolution and reproducibility of the images. Therefore, a proper noise treatment is important for improving the performance of clinical and research investigations. Noise reduction becomes especially critical for the images with a low signal-to-noise ratio, such as those typically acquired in diffusion tensor imaging at high diffusion weightings. The standard methods of signal correction usually assume a uniform distribution of the standard deviation of the noise across the image and evaluate a single correction parameter for the whole image. We pursue a more advanced approach based on the assumption of an inhomogeneous distribution of noise in space and evaluate correction factors for each voxel individually. The Rician nature of the underlying noise is considered for low and high signal-to-noise ratios. The approach developed here has been examined using numerical simulations and in vivo brain diffusion tensor imaging experiments. The efficacy and usefulness of this approach is demonstrated here and the resultant effective tool is described. © 2011 Elsevier B.V.","Diffusion weighted imaging; Gaussian noise; Low signal-to-noise ratio; Rician noise; Spatially variable noise","Body motions; Coil arrays; Correction factors; Diffusion tensor imaging; Diffusion weighted imaging; Effective tool; Field inhomogeneity; Gradient coil; High signal-to-noise ratio; In-vivo; Inhomogeneous distribution; Low signal-to-noise ratio; Magnetic resonance images; Physiological noise; Random factors; Reproducibilities; Respiratory motions; Rician noise; Signal correction; Spatially variable noise; Standard deviation; Standard method; Uniform distribution; Diffusion; Fading (radio); Gaussian noise (electronic); Signal to noise ratio; Tensors; Magnetic resonance imaging; article; clinical assessment tool; diffusion tensor imaging; evaluation; in vivo study; nuclear magnetic resonance imaging; priority journal; rician noise; signal noise ratio; simulation; voxel based morphometry; Algorithms; Artificial Intelligence; Brain; Diffusion Magnetic Resonance Imaging; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84856229439
"Modinos G., Pettersson-Yeo W., Allen P., McGuire P.K., Aleman A., Mechelli A.","Multivariate pattern classification reveals differential brain activation during emotional processing in individuals with psychosis proneness",2012,"NeuroImage",25,10.1016/j.neuroimage.2011.10.048,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855449460&doi=10.1016%2fj.neuroimage.2011.10.048&partnerID=40&md5=9d84ef4aeb759dc6c13c3fc62838ebf5","Among the general population, individuals with subthreshold psychotic-like experiences, or psychosis proneness (PP), can be psychometrically identified and are thought to have a 10-fold increased risk of psychosis. They also show impairments in measures of emotional functioning parallel to schizophrenia. Whilst previous studies have revealed altered brain activation in patients with schizophrenia during emotional processing, it is unclear whether these alterations are also expressed in individuals with high PP. Here we used Support Vector Machine (SVM) to perform multivariate pattern classification based on brain activation during emotional processing in 20 individuals with high PP and 20 comparison subjects (low PP). In addition, we performed a standard univariate analysis based on the General Linear Model (GLM) on the same data for comparison. The experimental task involved passively viewing negative and neutral pictures from the International Affective Picture System (IAPS). SVM allowed classification of the two groups with statistically significant accuracy (p= 0.017) and identified group differences within an emotional circuitry including the amygdala, insula, anterior cingulate and medial prefrontal cortex. In contrast, the standard univariate analysis did not detect any significant between-group differences. Our results reveal a distributed and subtle set of alterations in brain function within the emotional circuitry of individuals with high PP, providing neurobiological support for the notion of dysfunctional emotional circuitry in this group. In addition, these alterations are best detected using a multivariate approach rather than standard univariate methods. Further application of this approach may aid in characterising people at clinical and genetic risk of developing psychosis. © 2011 Elsevier Inc.","Emotion; FMRI; Machine learning; Multivariate; Psychosis proneness; Support Vector Machine","accuracy; adult; amygdaloid nucleus; anterior cingulate; article; brain function; controlled study; disease predisposition; emotion; female; human; human experiment; insula; intermethod comparison; male; medial prefrontal cortex; multivariate analysis; normal human; prefrontal cortex; priority journal; psychosis; stimulus response; support vector machine; univariate analysis; visual stimulation; Affect; Artificial Intelligence; Brain; Disease Susceptibility; Emotions; Female; Humans; Image Processing, Computer-Assisted; Linear Models; Magnetic Resonance Imaging; Male; Models, Neurological; Multivariate Analysis; Nerve Net; Pattern Recognition, Automated; Photic Stimulation; Psychomotor Performance; Psychotic Disorders; Risk Assessment; Schizophrenia; Support Vector Machines; Young Adult",Article,Scopus,2-s2.0-84855449460
"Nie F., Xu D., Li X.","Initialization independent clustering with actively self-training method",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",25,10.1109/TSMCB.2011.2161607,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856283959&doi=10.1109%2fTSMCB.2011.2161607&partnerID=40&md5=b7afd8ff71e08659d192b431b6abb742","The results of traditional clustering methods are usually unreliable as there is not any guidance from the data labels, while the class labels can be predicted more reliable by the semisupervised learning if the labels of partial data are given. In this paper, we propose an actively self-training clustering method, in which the samples are actively selected as training set to minimize an estimated Bayes error, and then explore semisupervised learning to perform clustering. Traditional graph-based semisupervised learning methods are not convenient to estimate the Bayes error; we develop a specific regularization framework on graph to perform semisupervised learning, in which the Bayes error can be effectively estimated. In addition, the proposed clustering algorithm can be readily applied in a semisupervised setting with partial class labels. Experimental results on toy data and real-world data sets demonstrate the effectiveness of the proposed clustering method on the unsupervised and the semisupervised setting. It is worthy noting that the proposed clustering method is free of initialization, while traditional clustering methods are usually dependent on initialization. © 2011 IEEE.","Active learning; initialization independent clustering; self-training; spectral clustering (SC)","Active Learning; Bayes error; Class labels; Clustering methods; Data labels; Graph-based; initialization independent clustering; Partial data; Real world data; Regularization framework; Self-training; Semi-supervised; Semi-supervised learning; Semi-supervised learning methods; spectral clustering (SC); Toy data; Traditional clustering; Training sets; Cluster analysis; Supervised learning; Virtual reality; Clustering algorithms; algorithm; article; artificial intelligence; automated pattern recognition; cluster analysis; computer simulation; decision support system; methodology; theoretical model; Algorithms; Artificial Intelligence; Cluster Analysis; Computer Simulation; Decision Support Techniques; Models, Theoretical; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84856283959
"Sadeghieh A., Sazgar H., Goodarzi K., Lucas C.","Identification and real-time position control of a servo-hydraulic rotary actuator by means of a neurobiologically motivated algorithm",2012,"ISA Transactions",25,10.1016/j.isatra.2011.09.006,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-83555174562&doi=10.1016%2fj.isatra.2011.09.006&partnerID=40&md5=f41fd6e4817db1fec4a92115a861af9a","This paper presents a new intelligent approach for adaptive control of a nonlinear dynamic system. A modified version of the brain emotional learning based intelligent controller (BELBIC), a bio-inspired algorithm based upon a computational model of emotional learning which occurs in the amygdala, is utilized for position controlling a real laboratorial rotary electro-hydraulic servo (EHS) system. EHS systems are known to be nonlinear and non-smooth due to many factors such as leakage, friction, hysteresis, null shift, saturation, dead zone, and especially fluid flow expression through the servo valve. The large value of these factors can easily influence the control performance in the presence of a poor design. In this paper, a mathematical model of the EHS system is derived, and then the parameters of the model are identified using the recursive least squares method. In the next step, a BELBIC is designed based on this dynamic model and utilized to control the real laboratorial EHS system. To prove the effectiveness of the modified BELBIC's online learning ability in reducing the overall tracking error, results have been compared to those obtained from an optimal PID controller, an auto-tuned fuzzy PI controller (ATFPIC), and a neural network predictive controller (NNPC) under similar circumstances. The results demonstrate not only excellent improvement in control action, but also less energy consumption. Copyright © 2011 Published by Elsevier Ltd on behalf of ISA. All rights reserved.","Auto-tuned fuzzy PI controller; Brain emotional learning based intelligent controller; Electro-hydraulic servo systems; Neural network predictive controller; Parameter identification; PID controller","Adaptive Control; Bio-inspired algorithms; Brain emotional learning; Computational model; Control performance; Dead zones; Electro hydraulic servo system; Electrohydraulic servos; Emotional learning; Fuzzy-PI controllers; In-control; Intelligent controllers; Non-linear dynamic systems; Non-smooth; Online learning; PID controllers; Predictive controller; Recursive least squares method; Rotary actuator; Servo-valve; Tracking errors; Algorithms; Controllers; Dynamical systems; Electric control equipment; Energy utilization; Flow of fluids; Hydraulic servomechanisms; Identification (control systems); Intelligent control; Leakage (fluid); Least squares approximations; Mathematical models; Neural networks; Nonlinear dynamical systems; Position control; Three term control systems; Adaptive control systems; algorithm; amygdaloid nucleus; article; artificial intelligence; artificial neural network; biological model; computer simulation; computer system; emotion; equipment design; fuzzy logic; human; industry; neurobiology; nonlinear system; physiology; Algorithms; Amygdala; Artificial Intelligence; Computer Simulation; Computer Systems; Emotions; Equipment Design; Fuzzy Logic; Humans; Industry; Models, Neurological; Neural Networks (Computer); Neurobiology; Nonlinear Dynamics",Article,Scopus,2-s2.0-83555174562
"Kim H.-C., Ghahramani Z.","Bayesian classifier combination",2012,"Journal of Machine Learning Research",25,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901947678&partnerID=40&md5=7a16b5a836aad0cd95c22f9d08e944b2","Bayesian model averaging linearly mixes the probabilistic predictions of multiple models, each weighted by its posterior probability. This is the coherent Bayesian way of combining multiple models only under certain restrictive assumptions, which we outline. We explore a general framework for Bayesian model combination (which differs from model averaging) in the context of classification. This framework explicitly models the relationship between each model's output and the unknown true label. The framework does not require that the models be probabilistic (they can even be human assessors), that they share prior information or receive the same training data, or that they be independent in their errors. Finally, the Bayesian combiner does not need to believe any of the models is in fact correct. We test several variants of this classifier combination procedure starting from a classic statistical model proposed by Dawid and Skene (1979) and using MCMC to add more complex but important features to the model. Comparisons on several data sets to simpler methods like majority voting show that the Bayesian methods not only perform well but result in interpretable diagnostics on the data points and the models.",,"Artificial intelligence; Bayesian classifier; Bayesian model averaging; Classifier combination; Important features; Posterior probability; Prior information; Probabilistic prediction; Statistical modeling; Bayesian networks",Conference Paper,Scopus,2-s2.0-84901947678
"Vasirani M., Ossowski S.","A market-inspired approach for intersection management in urban road traffic networks",2012,"Journal of Artificial Intelligence Research",25,10.1613/jair.3484,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862650528&doi=10.1613%2fjair.3484&partnerID=40&md5=cb5d6b833d0fb5eb174e14c46d51efc7","Traffic congestion in urban road networks is a costly problem that affects all major cities in developed countries. To tackle this problem, it is possible (i) to act on the supply side, increasing the number of roads or lanes in a network, (ii) to reduce the demand, restricting the access to urban areas at specific hours or to specific vehicles, or (iii) to improve the efficiency of the existing network, by means of a widespread use of so-called Intelligent Transportation Systems (ITS). In line with the recent advances in smart transportation management infrastructures, ITS has turned out to be a promising field of application for artificial intelligence techniques. In particular, multiagent systems seem to be the ideal candidates for the design and implementation of ITS. In fact, drivers can be naturally modelled as autonomous agents that interact with the transportation management infrastructure, thereby generating a large-scale, open, agent-based system. To regulate such a system and maintain a smooth and efficient flow of traffic, decentralised mechanisms for the management of the transportation infrastructure are needed. In this article we propose a distributed, market-inspired, mechanism for the management of a future urban road network, where intelligent autonomous vehicles, operated by software agents on behalf of their human owners, interact with the infrastructure in order to travel safely and efficiently through the road network. Building on the reservationbased intersection control model proposed by Dresner and Stone, we consider two different scenarios: one with a single intersection and one with a network of intersections. In the former, we analyse the performance of a novel policy based on combinatorial auctions for the allocation of reservations. In the latter, we analyse the impact that a traffic assignment strategy inspired by competitive markets has on the drivers' route choices. Finally we propose an adaptive management mechanism that integrates the auction-based traffic control policy with the competitive traffic assignment strategy. © 2012 AI Access Foundation.",,"Adaptive Management; Agent-based systems; Artificial intelligence techniques; Combinatorial auction; Competitive markets; Decentralised; Developed countries; In-line; Intelligent autonomous vehicles; Intelligent transportation systems; Intersection control; Major cities; Road network; Route choice; Specific vehicles; Traffic assignment; Transportation infrastructures; Transportation management; Urban areas; Urban road networks; Urban road traffic; Autonomous agents; Commerce; Highway planning; Intelligent systems; Motor transportation; Multi agent systems; Traffic congestion; Intelligent vehicle highway systems",Article,Scopus,2-s2.0-84862650528
"Tran-Thanh L., Stein S., Rogers A., Jennings N.R.","Efficient crowdsourcing of unknown experts using multi-armed bandits",2012,"Frontiers in Artificial Intelligence and Applications",25,10.3233/978-1-61499-098-7-768,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878812412&doi=10.3233%2f978-1-61499-098-7-768&partnerID=40&md5=e4c2e5ae13031cbe6b3c5c97aa7792eb","We address the expert crowdsourcing problem, in which an employer wishes to assign tasks to a set of available workers with heterogeneous working costs. Critically, as workers produce results of varying quality, the utility of each assigned task is unknown and can vary both between workers and individual tasks. Furthermore, in realistic settings, workers are likely to have limits on the number of tasks they can perform and the employer will have a fixed budget to spend on hiring workers. Given these constraints, the objective of the employer is to assign tasks to workers in order to maximise the overall utility achieved. To achieve this, we introduce a novel multi-armed bandit (MAB) model, the bounded MAB, that naturally captures the problem of expert crowdsourcing. We also propose an algorithm to solve it efficiently, called bounded ε-first, which uses the first sB of its total budget B to derive estimates of the workers' quality characteristics (exploration), while the remaining (1 - ε) B is used to maximise the total utility based on those estimates (exploitation). We show that using this technique allows us to derive an OB2/3) upper bound on our algorithm's performance regret (i.e. the expected difference in utility between the optimal and our algorithm). In addition, we demonstrate that our algorithm outperforms existing crowdsourcing methods by up to 155% in experiments based on real-world data from a prominent crowdsourcing site, while achieving up to 75% of a hypothetical optimal with full information. © 2012 The Author(s).",,"Algorithms; Artificial intelligence; Algorithm's performance; Crowdsourcing; Fixed budget; Full informations; Multi armed bandit; Quality characteristic; Total budget; Working costs; Budget control",Conference Paper,Scopus,2-s2.0-84878812412
"Rintanen J.","Engineering efficient planners with SAT",2012,"Frontiers in Artificial Intelligence and Applications",25,10.3233/978-1-61499-098-7-684,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878780031&doi=10.3233%2f978-1-61499-098-7-684&partnerID=40&md5=ec5af52d73ad957fdbdf303b26b8157f","Planning with SAT has long been viewed as a main approach to AI planning. In comparison to other approaches, its high memory requirements have been considered to be a main obstacle to its scalability to large planning problems. Better implementation technology, especially addressing the memory use, together with a shift of understanding about SAT-based planning during the past ten years, enables planners that radically differ from those from the late 1990s. We discuss a SAT-based planning system that implements modern versions of virtually all components of first planners that used SAT, focusing on the new implementation technology for a compact clause representation that is both simpler and more effective than ones proposed earlier. Specifically, the decreased memory requirements enable the use of top-level solution strategies that lift the performance of SAT-based planning to the same level with other search methods. © 2012 The Author(s).",,"Artificial intelligence; AI planning; Memory requirements; Memory use; Planning problem; SAT-based planning; Search method; Solution strategy; Planning",Conference Paper,Scopus,2-s2.0-84878780031
"Merlet S., Caruyer E., Deriche R.","Parametric dictionary learning for modeling EAP and ODF in diffusion MRI",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",25,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872900492&partnerID=40&md5=bbf4c537aa496af29a442b0bd9daf753","In this work, we propose an original and efficient approach to exploit the ability of Compressed Sensing (CS) to recover Diffusion MRI (dMRI) signals from a limited number of samples while efficiently recovering important diffusion features such as the Ensemble Average Propagator (EAP) and the Orientation Distribution Function (ODF). Some attempts to sparsely represent the diffusion signal have already been performed. However and contrarly to what has been presented in CS dMRI, in this work we propose and advocate the use of a well adapted learned dictionary and show that it leads to a sparser signal estimation as well as to an efficient reconstruction of very important diffusion features. We first propose to learn and design a sparse and parametric dictionary from a set of training diffusion data. Then, we propose a framework to analytically estimate in closed form two important diffusion features : the EAP and the ODF. Various experiments on synthetic, phantom and human brain data have been carried out and promising results with reduced number of atoms have been obtained on diffusion signal reconstruction, thus illustrating the added value of our method over state-of-the-art SHORE and SPF based approaches. © Springer-Verlag Berlin Heidelberg 2012.",,"Atoms; Compressed sensing; Distribution functions; Education; Magnetic resonance imaging; Medical computing; Medical imaging; Signal reconstruction; Compressive sensing; Dictionary learning; Efficient reconstruction; Ensemble averages; Learned dictionaries; Number of samples; Orientation distribution function; Signal estimation; Diffusion; algorithm; article; artificial intelligence; brain; computer assisted diagnosis; connectome; diffusion tensor imaging; histology; human; image enhancement; information processing; methodology; myelinated nerve; reproducibility; sensitivity and specificity; three dimensional imaging; ultrastructure; Algorithms; Artificial Intelligence; Brain; Connectome; Data Compression; Diffusion Tensor Imaging; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Nerve Fibers, Myelinated; Reproducibility of Results; Sensitivity and Specificity",Conference Paper,Scopus,2-s2.0-84872900492
"Verstegen J.A., Karssenberg D., Van der Hilst F., Faaij A.","Spatio-temporal uncertainty in Spatial Decision Support Systems: A case study of changing land availability for bioenergy crops in Mozambique",2012,"Computers, Environment and Urban Systems",25,10.1016/j.compenvurbsys.2011.08.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855479885&doi=10.1016%2fj.compenvurbsys.2011.08.003&partnerID=40&md5=264c5f54db54a65aa85cb34772cf00ac","Spatial Decision Support Systems (SDSSs) often include models that can be used to assess the impact of possible decisions. These models usually simulate complex spatio-temporal phenomena, with input variables and parameters that are often hard to measure. The resulting model uncertainty is, however, rarely communicated to the user, so that current SDSSs yield clear, but therefore sometimes deceptively precise outputs. Inclusion of uncertainty in SDSSs requires modeling methods to calculate uncertainty and tools to visualize indicators of uncertainty that can be understood by its users, having mostly limited knowledge of spatial statistics. This research makes an important step towards a solution of this issue. It illustrates the construction of the PCRaster Land Use Change model (PLUC) that integrates simulation, uncertainty analysis and visualization. It uses the PCRaster Python framework, which comprises both a spatio-temporal modeling framework and a Monte Carlo analysis framework that together produce stochastic maps, which can be visualized with the Aguila software, included in the PCRaster Python distribution package. This is illustrated by a case study for Mozambique in which it is evaluated where bioenergy crops can be cultivated without endangering nature areas and food production now and in the near future, when population and food intake per capita will increase and thus arable land and pasture areas are likely to expand. It is shown how the uncertainty of the input variables and model parameters effects the model outcomes. Evaluation of spatio-temporal uncertainty patterns has provided new insights in the modeled land use system about, e.g., the shape of concentric rings around cities. In addition, the visualization modes give uncertainty information in an comprehensible way for users without specialist knowledge of statistics, for example by means of confidence intervals for potential bioenergy crop yields. The coupling of spatio-temporal uncertainty analysis to the simulation model is considered a major step forward in the exposure of uncertainty in SDSSs. © 2011 Elsevier Ltd.","Bioenergy; Land use change; Spatial Decision Support Systems; Spatial modeling; Uncertainty; Visualization","Bio-energy; Land use change; Spatial Decision Support Systems; Spatial modeling; Uncertainty; Artificial intelligence; Biofuels; Computer simulation; Computer software; Crops; Decision support systems; Flow visualization; High level languages; Land use; Research; Visualization; Uncertainty analysis; bioenergy; crop production; decision support system; land use change; Monte Carlo analysis; parameterization; raster; software; spatial analysis; spatiotemporal analysis; uncertainty analysis; visualization; zoning policy; Mozambique",Article,Scopus,2-s2.0-84855479885
"Peng Z., Kurgan L.","On the complementarity of the consensus-based disorder prediction",2012,"17th Pacific Symposium on Biocomputing, PSB 2012",25,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865991020&partnerID=40&md5=e8a8de27a214ae291eed0291e0d9a109","Intrinsic disorder in proteins plays important roles in transcriptional regulation, translation, and cellular signal transduction. The experimental annotation of the disorder lags behind the rapidly accumulating number of known protein chains, which motivates the development of computational predictors of disorder. Some of these methods address predictions of certain types/flavors of the disorder and recent years show that consensusbased predictors provide a viable way to improve predictive performance. However, the selection of the base predictors in a given consensus is usually performed in an ad-hock manner, based on their availability and with a premise that more is better. We perform first-of-its-kind investigation that analyzes complementarity among a dozen recent predictors to identify characteristics of (future) predictors that would lead to further consensus-based improvements in the predictive quality. The complementarity of a given set of three base predictors is expressed by the differences in their predictions when compared with each other and with their majority vote consensus. We propose a regression-based model that quantifies/predicts quality of the majority-vote consensus of a given triplet of predictors based on their individual predictive performance and their complementarity measured at the residue and the disorder segment levels. Our model shows that improved performance is associated with higher (lower) similarity between the three base predictors at the residue (segment) level and to their consensus prediction at the segment (residue) level. We also show that better consensuses utilize higher quality base methods. We use our model to predict the best-performing consensus on an independent test dataset and our empirical evaluation shows that this consensus outperforms individual methods and other consensus-based predictors based on the area under the ROC curve measure. Our study provides insights that could lead to the development of a new generation of the consensus-based disorder predictors.",,"protein; algorithm; article; artificial intelligence; biology; chemical structure; chemistry; consensus sequence; genetics; protein conformation; protein database; protein stability; protein tertiary structure; statistical model; Algorithms; Artificial Intelligence; Computational Biology; Consensus Sequence; Databases, Protein; Linear Models; Models, Molecular; Protein Conformation; Protein Stability; Protein Structure, Tertiary; Proteins",Conference Paper,Scopus,2-s2.0-84865991020
"Cabrio E., Villata S.","Natural language arguments: A combined approach",2012,"Frontiers in Artificial Intelligence and Applications",25,10.3233/978-1-61499-098-7-205,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878790162&doi=10.3233%2f978-1-61499-098-7-205&partnerID=40&md5=01f798e390268bdac5b1002a40a4e4ed","With the growing use of the Social Web, an increasing number of applications for exchanging opinions with other people are becoming available online. These applications are widely adopted with the consequence that the number of opinions about the debated issues increases. In order to cut in on a debate, the participants need first to evaluate the opinions in favour or against the debated issue. Argumentation theory proposes algorithms and semantics to evaluate the set of accepted arguments, given the conflicts among them. The main problem is how to automatically generate the arguments from the natural language formulation of the opinions used in these applications. Our paper addresses this problem by proposing and evaluating the use of natural language techniques to generate the arguments. In particular, we adopt the textual entailment approach, a generic framework for applied semantics, where linguistic objects are mapped by means of semantic inferences at a textual level. We couple textual entailment together with a Dung-like argumentation system which allows us to identify the arguments that are accepted in the considered online debate. The originality of the proposed framework lies in the following point: natural language debates are analyzed and the arguments are automatically extracted. © 2012 The Author(s).",,"Artificial intelligence; Computational linguistics; Text processing; Argumentation systems; Argumentation theory; Generic frameworks; Natural language techniques; Natural languages; Semantic inference; Social webs; Textual entailment; Semantics",Conference Paper,Scopus,2-s2.0-84878790162
"Salmeron J.L., Gutierrez E.","Fuzzy Grey Cognitive Maps in reliability engineering",2012,"Applied Soft Computing Journal",24,10.1016/j.asoc.2012.02.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869090833&doi=10.1016%2fj.asoc.2012.02.003&partnerID=40&md5=5b5159ebf8eb950f48b3c778b7e5ab95","Current industrial equipment has become more complex and huge. In this case, the conventional reliability techniques cannot correctly support functional assessment. This paper integrates an innovative soft computing methodology, Fuzzy Grey Cognitive Map (FGCM), into a traditional reliability analysis for better knowledge. FGCMs are used for evaluating, modelling and aiding decision-making by examining causal relations among relevant domain concepts. The proposed procedure is illustrated with a reliability analysis of a transformer active part. Twenty failure causes in the transformer's active part are identified and assessed. In addition, six failure scenarios are simulated. The results revealed the potential of the combination of FGCM and failure analysis for complex systems. The proposed methodology exposes the potential benefits it could provide in order to assist electric power system decision-makers to supply its customer electrical energy with a high degree of reliability. © 2012 Elsevier B.V. All rights reserved.","Decision support systems; FMEA; Fuzzy Grey Cognitive Maps; Knowledge-based systems; Soft computing","Active parts; Causal relations; Cognitive maps; Decision makers; Degree of reliability; Domain concepts; Electrical energy; FMEA; Industrial equipment; Potential benefits; Reliability engineering; Reliability techniques; Soft computing methodologies; Artificial intelligence; Decision support systems; Electric power systems; Knowledge based systems; Reliability analysis; Soft computing; Cognitive systems",Article,Scopus,2-s2.0-84869090833
"Xu L., Cai C.B., Cui H.F., Ye Z.H., Yu X.P.","Rapid discrimination of pork in Halal and non-Halal Chinese ham sausages by Fourier transform infrared (FTIR) spectroscopy and chemometrics",2012,"Meat Science",24,10.1016/j.meatsci.2012.05.019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865550195&doi=10.1016%2fj.meatsci.2012.05.019&partnerID=40&md5=4727a291d054e91cd6eca90c1cf33424","Rapid discrimination of pork in Halal and non-Halal Chinese ham sausages was developed by Fourier transform infrared (FTIR) spectrometry combined with chemometrics. Transmittance spectra ranging from 400 to 4000cm -1 of 73 Halal and 78 non-Halal Chinese ham sausages were measured. Sample preparation involved finely grinding of samples and formation of KBr disks (under 10MPa for 5min). The influence of data preprocessing methods including smoothing, taking derivatives and standard normal variate (SNV) on partial least squares discriminant analysis (PLSDA) and least squares support vector machine (LS-SVM) was investigated. The results indicate removal of spectral background and baseline plays an important role in discrimination. Taking derivatives, SNV can improve classification accuracy and reduce the complexity of PLSDA. Possibly due to the loss of detailed high-frequency spectral information, smoothing degrades the model performance. For the best models, the sensitivity and specificity was 0.913 and 0.929 for PLSDA with SNV spectra, 0.957 and 0.929 for LS-SVM with second derivative spectra, respectively. © 2012 Elsevier Ltd.","Chinese Ham sausage; FTIR spectroscopy; Halal; LS-SVM; PLSDA","Chinese Ham sausage; FTIR spectroscopy; Halal; LS-SVM; PLSDA; Fourier transform infrared spectroscopy; Support vector machines; Meats; animal; article; artificial intelligence; biology; China; diet; discriminant analysis; ethnology; food control; food handling; human; infrared spectroscopy; meat; methodology; physical chemistry; principal component analysis; regression analysis; religion; reproducibility; standard; swine; time; validation study; Animals; Artificial Intelligence; China; Computational Biology; Diet; Discriminant Analysis; Food Inspection; Food Technology; Humans; Islam; Least-Squares Analysis; Meat; Meat Products; Physicochemical Phenomena; Principal Component Analysis; Reproducibility of Results; Spectroscopy, Fourier Transform Infrared; Sus scrofa; Time Factors",Article,Scopus,2-s2.0-84865550195
"Chen J., Low K.H., Tan C.K.-Y., Oran A., Jaillet P., Dolan J., Sukhatme G.","Decentralized data fusion and active sensing with mobile sensors for modeling and predicting spatiotemporal traffic phenomena",2012,"Uncertainty in Artificial Intelligence - Proceedings of the 28th Conference, UAI 2012",24,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886000023&partnerID=40&md5=15209bc317bd27662a2c9d06cd487110","The problem of modeling and predicting spatiotemporal traffic phenomena over an urban road network is important to many traffic applications such as detecting and forecasting congestion hotspots. This paper presents a decentralized data fusion and active sensing (D2FAS) algorithm for mobile sensors to actively explore the road network to gather and assimilate the most informative data for predicting the traffic phenomenon. We analyze the time and communication complexity of D2FAS and demonstrate that it can scale well with a large number of observations and sensors. We provide a theoretical guarantee on its predictive performance to be equivalent to that of a sophisticated centralized sparse approximation for the Gaussian process (GP) model: The computation of such a sparse approximate GP model can thus be parallelized and distributed among the mobile sensors (in a Google-like MapReduce paradigm), thereby achieving efficient and scalable prediction. We also theoretically guarantee its active sensing performance that improves under various practical environmental conditions. Empirical evaluation on real-world urban road network data shows that our D2FAS algorithm is significantly more time-efficient and scalable than state-ofthe- art centralized algorithms while achieving comparable predictive performance.",,"Centralized algorithms; Communication complexity; Decentralized data fusion; Empirical evaluations; Environmental conditions; Predictive performance; Sparse approximations; Theoretical guarantees; Algorithms; Artificial intelligence; Forecasting; Traffic congestion",Conference Paper,Scopus,2-s2.0-84886000023
"Claassen T., Heskes T.","A bayesian approach to constraint based causal inference",2012,"Uncertainty in Artificial Intelligence - Proceedings of the 28th Conference, UAI 2012",24,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885985995&partnerID=40&md5=d68905999c8a6e8695bd9e2d61e8e1e0","We target the problem of accuracy and robustness in causal inference from finite data sets. Some state-of-the-art algorithms produce clear output complete with solid theoretical guarantees but are susceptible to propagating erroneous decisions, while others are very adept at handling and representing uncertainty, but need to rely on undesirable assumptions. Our aim is to combine the inherent robustness of the Bayesian approach with the theoretical strength and clarity of constraint-based methods. We use a Bayesian score to obtain probability estimates on the input statements used in a constraint-based procedure. These are subsequently processed in decreasing order of reliability, letting more reliable decisions take precedence in case of conflicts, until a single output model is obtained. Tests show that a basic implementation of the resulting Bayesian Constraint-based Causal Discovery (BCCD) algorithm already outperforms established procedures such as FCI and Conservative PC. It can also indicate which causal decisions in the output have high reliability and which do not.",,"Bayesian approaches; Causal inferences; Constraint-based; High reliability; Probability estimate; State-of-the-art algorithms; Theoretical guarantees; Theoretical strength; Algorithms; Artificial intelligence; Statistical tests; Bayesian networks",Conference Paper,Scopus,2-s2.0-84885985995
"Levesque H.J., Davis E., Morgenstern L.","The winograd schema challenge",2012,"13th International Conference on the Principles of Knowledge Representation and Reasoning, KR 2012",24,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893428800&partnerID=40&md5=26c4a507e78fd2b6b7d1e0fc6e9d8275","In this paper, we present an alternative to the Turing Test that has some conceptual and practical advantages. A Wino-grad schema is a pair of sentences that differ only in one or two words and that contain a referential ambiguity that is resolved in opposite directions in the two sentences. We have compiled a collection of Winograd schemas, designed so that the correct answer is obvious to the human reader, but cannot easily be found using selectional restrictions or statistical techniques over text corpora. A contestant in the Winograd Schema Challenge is presented with a collection of one sentence from each pair, and required to achieve human-level accuracy in choosing the correct disambiguation. Copyright © 2012, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Human readers; Statistical techniques; Text corpora; Turing tests; Winograd; Artificial intelligence; Knowledge representation",Conference Paper,Scopus,2-s2.0-84893428800
"Mourao K., Zettlemoyer L., Petrick R.P.A., Steedman M.","Learning STRIPS operators from noisy and incomplete observations",2012,"Uncertainty in Artificial Intelligence - Proceedings of the 28th Conference, UAI 2012",24,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885976211&partnerID=40&md5=c5cfda13f869128927d3de599a0ecaa6","Agents learning to act autonomously in realworld domains must acquire a model of the dynamics of the domain in which they operate. Learning domain dynamics can be challenging, especially where an agent only has partial access to the world state, and/or noisy external sensors. Even in standard STRIPS domains, existing approaches cannot learn from noisy, incomplete observations typical of real-world domains. We propose a method which learns STRIPS action models in such domains, by decomposing the problem into first learning a transition function between states in the form of a set of classifiers, and then deriving explicit STRIPS rules from the classifiers' parameters. We evaluate our approach on simulated standard planning domains from the International Planning Competition, and show that it learns useful domain descriptions from noisy, incomplete observations.",,"Domain description; Domain dynamics; External sensors; Incomplete observation; International Planning Competitions; Planning domains; Real world domain; Transition functions; Artificial intelligence; Autonomous agents",Conference Paper,Scopus,2-s2.0-84885976211
"Kajino H., Tsuboi Y., Kashima H.","A convex formulation for learning from crowds",2012,"Proceedings of the National Conference on Artificial Intelligence",24,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868280190&partnerID=40&md5=a9eb21af7842b7df94268cfb1e310adb","Recently crowdsourcing services are often used to collect a large amount of labeled data for machine learning, since they provide us an easy way to get labels at very low cost and in a short period. The use of crowd-sourcing has introduced a new challenge in machine learning, that is, coping with the variable quality of crowd-generated data. Although there have been many recent attempts to address the quality problem of multiple workers, only a few of the existing methods consider the problem of learning classifiers directly from such noisy data. All these methods modeled the true labels as latent variables, which resulted in non-convex optimization problems. In this paper, we propose a convex optimization formulation for learning from crowds without estimating the true labels by introducing personal models of the individual crowd workers. We also devise an efficient iterative method for solving the convex optimization problems by exploiting conditional independence structures in multiple classifiers. We evaluate the proposed method against three competing methods on synthetic data sets and a real crowdsourced data set and demonstrate that the proposed method outperforms the other three methods. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Conditional independences; Convex optimization problems; Crowdsourcing; Data sets; Labeled data; Latent variable; Learning classifiers; Low costs; Multiple classifiers; Noisy data; Nonconvex optimization; Optimization formulations; Quality problems; Short periods; Synthetic datasets; Convex optimization; Iterative methods; Learning systems; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868280190
"Zungeru A.M., Ang L.-M., Seng K.P.","Termite-hill: Performance optimized swarm intelligence based routing algorithm for wireless sensor networks",2012,"Journal of Network and Computer Applications",24,10.1016/j.jnca.2012.07.014,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867497191&doi=10.1016%2fj.jnca.2012.07.014&partnerID=40&md5=9469110b313a4d95cc9d0eb5de9e9a39","A wireless sensor network (WSN) is a large collection of sensor nodes with limited power supply, constrained memory capacity, processing capability, and available bandwidth. The main problem in event gathering in wireless sensor networks is the formation of energy-holes or hot spots near the sink. Due to the restricted communication range and high network density, events forwarding in sensor networks is very challenging, and require multi-hop data forwarding. Improving network lifetime and network reliability are the main factors to consider in the research associated with WSN. In static wireless sensor networks, sensors nodes close to the sink node run out of energy much faster than nodes in other parts of the monitored area. The nodes near the sink are more likely to use up their energy because they have to forward all the traffic generated by the nodes farther away to the sink. The uneven energy consumption results in network partitioning and limit the network lifetime. To this end, we propose an on-demand and multipath routing algorithm that utilizes the behavior of real termites on hill building termed Termite-hill which support sink mobility. The main objective of our proposed algorithm is to efficiently relay all the traffic destined for the sink, and also balance the network energy. The performance of our proposed algorithm was tested on static, dynamic and mobile sink scenarios with varying speed, and compared with other state-of-the-art routing algorithms in WSN. The results of our extensive experiments on Routing Modeling Application Simulation Environment (RMASE) demonstrated that our proposed routing algorithm was able to balance the network traffic load, and prolong the network lifetime. © 2012 Elsevier Ltd. All rights reserved.","Network lifetime; Network reliability; Routing; Sink mobility; Swarm intelligence; Target tracking; Termite-hill; Wireless Sensor Network","Network lifetime; Network reliability; Routing; Sink mobility; Swarm Intelligence; Termite-hill; Algorithms; Artificial intelligence; Energy utilization; Network routing; Routing algorithms; Target tracking; Wireless sensor networks; Sensor nodes",Article,Scopus,2-s2.0-84867497191
"Kumar A., Niculescu-Mizil A., Kavukcoglu K., Daumé H.","A binary classification framework for two-stage multiple kernel learning",2012,"Proceedings of the 29th International Conference on Machine Learning, ICML 2012",24,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867128123&partnerID=40&md5=349ae6c1cd7cf8020243909bd0472cf4","With the advent of kernel methods, automating the task of specifying a suitable kernel has become increasingly important. In this context, the Multiple Kernel Learning (MKL) problem of finding a combination of prespecified base kernels that is suitable for the task at hand has received significant attention from researchers. In this paper we show that Multiple Kernel Learning can be framed as a standard binary classification problem with additional constraints that ensure the positive definiteness of the learned kernel. Framing MKL in this way has the distinct advantage that it makes it easy to leverage the extensive research in binary classification to develop better performing and more scalable MKL algorithms that are conceptually simpler, and, arguably, more accessible to practitioners. Experiments on nine data sets from different domains show that, despite its simplicity, the proposed technique compares favorably with current leading MKL approaches. Copyright 2012 by the author(s)/owner(s).",,"Binary classification; Binary classification problems; Data sets; Different domains; Kernel methods; Multiple Kernel Learning; Positive definiteness; Artificial intelligence; Software engineering; Learning systems",Conference Paper,Scopus,2-s2.0-84867128123
"Bansal M., Garg N., Gupta N.","A 5-approximation for capacitated facility location",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",24,10.1007/978-3-642-33090-2_13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866648481&doi=10.1007%2f978-3-642-33090-2_13&partnerID=40&md5=f9a8429f259030f7f96e091866100279","In this paper, we propose and analyze a local search algorithm for the capacitated facility location problem. Our algorithm is a modification of the algorithm proposed by Zhang et al. [7] and improves the approximation ratio from 5.83 to 5. We achieve this by modifying the close, open and multi operations. The idea of taking linear combinations of inequalities used in Aggarwal et al.[1] is crucial in achieving this result. The example proposed by Zhang et al. also shows that our analysis is tight. © 2012 Springer-Verlag.",,"Approximation ratios; Capacitated facility location problems; Facility locations; Linear combinations; Local search algorithm; Multi-Operation; Artificial intelligence; Algorithms",Conference Paper,Scopus,2-s2.0-84866648481
"Fritsch V., Varoquaux G., Thyreau B., Poline J.-B., Thirion B.","Detecting outliers in high-dimensional neuroimaging datasets with robust covariance estimators",2012,"Medical Image Analysis",24,10.1016/j.media.2012.05.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866444366&doi=10.1016%2fj.media.2012.05.002&partnerID=40&md5=b646a0a7ea7e64cf91bfc9134781457d","Medical imaging datasets often contain deviant observations, the so-called outliers, due to acquisition or preprocessing artifacts or resulting from large intrinsic inter-subject variability. These can undermine the statistical procedures used in group studies as the latter assume that the cohorts are composed of homogeneous samples with anatomical or functional features clustered around a central mode. The effects of outlying subjects can be mitigated by detecting and removing them with explicit statistical control. With the emergence of large medical imaging databases, exhaustive data screening is no longer possible, and automated outlier detection methods are currently gaining interest. The datasets used in medical imaging are often high-dimensional and strongly correlated. The outlier detection procedure should therefore rely on high-dimensional statistical multivariate models. However, state-of-the-art procedures, based on the Minimum Covariance Determinant (MCD) estimator, are not well-suited for such high-dimensional settings. In this work, we introduce regularization in the MCD framework and investigate different regularization schemes. We carry out extensive simulations to provide backing for practical choices in absence of ground truth knowledge. We demonstrate on functional neuroimaging datasets that outlier detection can be performed with small sample sizes and improves group studies. © 2012 Elsevier B.V.","High-dimension; Minimum covariance determinant; Neuroimaging; Outlier detection; Robust estimation","Data screening; Data sets; Extensive simulations; Functional features; Ground truth; Group study; High-dimension; High-dimensional; Homogeneous samples; Minimum covariance determinant; Multivariate models; Outlier Detection; Outlier detection procedure; Regularization schemes; Robust estimation; Small Sample Size; State-of-the-art procedures; Statistical control; Estimation; Functional neuroimaging; Medical imaging; Multivariant analysis; Neuroimaging; Statistics; article; covariance; diagnosis related group; diagnostic imaging; functional neuroimaging; kernel method; multivariate analysis; priority journal; Algorithms; Artifacts; Artificial Intelligence; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Information Storage and Retrieval; Neuroimaging; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-84866444366
"Van Westen G.J.P., Van Den Hoven O.O., Van Der Pijl R., Mulder-Krieger T., De Vries H., Wegner J.K., IJzerman A.P., Van Vlijmen H.W.T., Bender A.","Identifying novel adenosine receptor ligands by simultaneous proteochemometric modeling of rat and human bioactivity data",2012,"Journal of Medicinal Chemistry",24,10.1021/jm3003069,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866889254&doi=10.1021%2fjm3003069&partnerID=40&md5=6b324d207933f4cdd73ea057790e8293","The four subtypes of adenosine receptors form relevant drug targets in the treatment of, e.g., diabetes and Parkinson's disease. In the present study, we aimed at finding novel small molecule ligands for these receptors using virtual screening approaches based on proteochemometric (PCM) modeling. We combined bioactivity data from all human and rat receptors in order to widen available chemical space. After training and validating a proteochemometric model on this combined data set (Q2 of 0.73, RMSE of 0.61), we virtually screened a vendor database of 100910 compounds. Of 54 compounds purchased, six novel high affinity adenosine receptor ligands were confirmed experimentally, one of which displayed an affinity of 7 nM on the human adenosine A1 receptor. We conclude that the combination of rat and human data performs better than human data only. Furthermore, we conclude that proteochemometric modeling is an efficient method to quickly screen for novel bioactive compounds. © 2012 American Chemical Society.",,"adenosine A1 receptor; adenosine A2a receptor; adenosine A2b receptor; adenosine A3 receptor; adenosine receptor affecting agent; ligand; article; binding affinity; chemometric analysis; data base; drug receptor binding; drug screening; human; ligand binding; model; nonhuman; proteochemometric model; rat; structure activity relation; validation process; virtual reality; Animals; Artificial Intelligence; Binding Sites; CHO Cells; Computer Simulation; Cricetinae; Cricetulus; Databases, Chemical; Humans; Ligands; Models, Molecular; Radioligand Assay; Rats; Receptor, Adenosine A1; Receptor, Adenosine A2A; Receptor, Adenosine A2B; Receptor, Adenosine A3; Receptors, Purinergic P1; Structure-Activity Relationship",Article,Scopus,2-s2.0-84866889254
"Zhang W., Niu Y., Xiong Y., Zhao M., Yu R., Liu J.","Computational prediction of conformational B-cell epitopes from antigen primary structures by ensemble learning",2012,"PLoS ONE",24,10.1371/journal.pone.0043575,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865183573&doi=10.1371%2fjournal.pone.0043575&partnerID=40&md5=a3979c476833d72695e07e05b5c83871","Motivation: The conformational B-cell epitopes are the specific sites on the antigens that have immune functions. The identification of conformational B-cell epitopes is of great importance to immunologists for facilitating the design of peptide-based vaccines. As an attempt to narrow the search for experimental validation, various computational models have been developed for the epitope prediction by using antigen structures. However, the application of these models is undermined by the limited number of available antigen structures. In contrast to the most of available structure-based methods, we here attempt to accurately predict conformational B-cell epitopes from antigen sequences. Methods: In this paper, we explore various sequence-derived features, which have been observed to be associated with the location of epitopes or ever used in the similar tasks. These features are evaluated and ranked by their discriminative performance on the benchmark datasets. From the perspective of information science, the combination of various features can usually lead to better results than the individual features. In order to build the robust model, we adopt the ensemble learning approach to incorporate various features, and develop the ensemble model to predict conformational epitopes from antigen sequences. Results: Evaluated by the leave-one-out cross validation, the proposed method gives out the mean AUC scores of 0.687 and 0.651 on two datasets respectively compiled from the bound structures and unbound structures. When compared with publicly available servers by using the independent dataset, our method yields better or comparable performance. The results demonstrate the proposed method is useful for the sequence-based conformational epitope prediction. Availability: The web server and datasets are freely available at http://bcell.whu.edu.cn. © 2012 Zhang et al.",,"antigen; epitope; access to information; amino acid sequence; antigen structure; area under the curve; article; B lymphocyte; client server application; information science; Internet; machine learning; mathematical computing; prediction; protein conformation; Amino Acid Sequence; Artificial Intelligence; Computational Biology; Epitopes, B-Lymphocyte; Protein Conformation",Article,Scopus,2-s2.0-84865183573
"Davidović T., Šelmić M., Teodorović D., Ramljak D.","Bee colony optimization for scheduling independent tasks to identical processors",2012,"Journal of Heuristics",24,10.1007/s10732-012-9197-3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865219719&doi=10.1007%2fs10732-012-9197-3&partnerID=40&md5=c72df5a97e0a157e7d5085dba087e95c","The static scheduling of independent tasks on homogeneous multiprocessor systems is studied in this paper. This problem is treated by the Bee Colony Optimization (BCO) meta-heuristic. The BCO algorithm belongs to the class of stochastic swarm optimization methods inspired by the foraging habits of bees in nature. To investigate the performance of the proposed method extensive numerical experiments are performed. Our BCO algorithm is able to obtain the optimal value of the objective function in the majority of test examples known from literature. The deviation of non-optimal solutions from the optimal ones in our test examples is at most 2%. The CPU times required to find the best solutions by BCO are significantly smaller than the corresponding times required by the CPLEX optimization solver. Moreover, our BCO is competitive with state-of-the-art methods for similar problems, with respect to both solution quality and running time. The stability of BCO is examined through multiple executions and it is shown that solution deviation is less than 1%. © 2012 Springer Science+Business Media, LLC.","Bee colony optimization (BCO); Combinatorial optimization; Homogeneous multiprocessor systems; Scheduling problems; Swarm intelligence","Colony optimization; CPU time; Independent tasks; Metaheuristic; Multi processor systems; Numerical experiments; Objective functions; Optimal values; Optimization solvers; Running time; Scheduling problem; Solution quality; State-of-the-art methods; Static scheduling; Swarm Intelligence; Swarm optimization; Test examples; Algorithms; Artificial intelligence; Combinatorial optimization; Numerical methods; Optimal systems; Scheduling; Optimization",Article,Scopus,2-s2.0-84865219719
"Saa R., Garcia A., Gomez C., Carretero J., Garcia-Carballeira F.","An ontology-driven decision support system for high-performance and cost-optimized design of complex railway portal frames",2012,"Expert Systems with Applications",24,10.1016/j.eswa.2012.02.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859209433&doi=10.1016%2fj.eswa.2012.02.002&partnerID=40&md5=7d8e8e35f31d908649a88c6efb558fa4","Electrification structures design for railway systems is a crucial and complex process, since it compounds plenty of infrastructure elements, design decisions, and calculation conditions. In this paper, an ontology-driven decision support system for designing complex railway portal frames is presented and developed. A knowledge-rules database has been also developed relying on experts knowledge and complying with railway standards. Our system outperforms the current portal frames design methods by decreasing construction time and costs. As a result, an intelligent computer-aided design tool is provided, thus facilitating the task of seeking for the optimal portal frame, which is geometrically and structurally feasible, and cost-effective. © 2012 Elsevier Ltd. All rights reserved.","Complex structures; Expert knowledge; Intelligent CAD tools; Ontologies; Railway systems; Rule-based systems","Complex Processes; Complex structure; Construction time; Design decisions; Design method; Designing complex; Expert knowledge; Intelligent CAD tools; Intelligent computer-aided design; Portal frame; Railway standards; Railway system; Artificial intelligence; Computer aided design; Decision support systems; Electric railroads; Electric utilities; Knowledge based systems; Ontology; Optimization; Railroads; Structural design",Article,Scopus,2-s2.0-84859209433
"Colton S.","The painting fool: Stories from building an automated painter",2012,"Computers and Creativity",24,10.1007/978-3-642-31727-9_1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949179879&doi=10.1007%2f978-3-642-31727-9_1&partnerID=40&md5=297faa541be72f5783a325f9a7c031fc","The Painting Fool is software that we hope will one day be taken seriously as a creative artist in its own right. This aim is being pursued as an Artificial Intelligence (AI) project, with the hope that the technical difficulties overcome along the way will lead to new and improved generic AI techniques. It is also being pursued as a sociological project, where the effect of software which might be deemed as creative is tested in the art world and the wider public. In this chapter, we summarise our progress so far in The Painting Fool project. To do this, we first compare and contrast The Painting Fool with software of a similar nature arising from AI and graphics projects. We follow this with a discussion of the guiding principles from Computational Creativity research that we adhere to in building the software. We then describe five projects with The Painting Fool where our aim has been to produce increasingly interesting and culturally valuable pieces of art. We end by discussing the issues raised in building an automated painter, and describe further work and future prospects for the project. By studying both the technical difficulties and sociological issues involved in engineering software for creative purposes, we hope to help usher in a new era where computers routinely act as our creative collaborators, as well as independent and creative artists, musicians, writers, designers, engineers and scientists, and contribute in meaningful and interesting ways to human culture. © Springer-Verlag Berlin Heidelberg 2012. All rights are reserved.",,"Artificial intelligence; AI techniques; Computational creativities; Engineering software; Further works; Future prospects; Guiding principles; Human cultures; Technical difficulties; Software testing",Book Chapter,Scopus,2-s2.0-84949179879
"Lupashin S., D'Andrea R.","Adaptive fast open-loop maneuvers for quadrocopters",2012,"Autonomous Robots",24,10.1007/s10514-012-9289-9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862144105&doi=10.1007%2fs10514-012-9289-9&partnerID=40&md5=91f38d886bf962ca1d5dd4dd4ee58b78","We present a conceptually and computationally lightweight method for the design and iterative learning of fast maneuvers for quadrocopters. We use first-principles, reduced-order models and we do not require nor make an attempt to follow a specific state trajectory-only the initial and the final states of the vehicle are taken into account. We evaluate the adaptation scheme through experiments on quadrocopters in the ETH Flying Machine Arena that perform multi-flips and other high-performance maneuvers. © 2012 Springer Science+Business Media, LLC.","Aerial robotics; Aerobatics; Learning; Policy gradient","Adaptation scheme; Aerial robotics; Aerobatics; Final state; Flying machines; Iterative learning; Learning; Open loops; Policy gradient; Reduced order models; Specific state; Artificial intelligence; Robots",Article,Scopus,2-s2.0-84862144105
"Broquedis F., Gautier T., Danjean V.","LIBKOMP, an efficient OpenMP runtime system for both fork-join and data flow paradigms",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",24,10.1007/978-3-642-30961-8_8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862184905&doi=10.1007%2f978-3-642-30961-8_8&partnerID=40&md5=dba46631280288b3d4664cd46d41ae5d","To efficiently exploit high performance computing platforms, applications currently have to express more and more finer-grain parallelism. The OpenMP standard allows programmers to do so since version 3.0 and the introduction of task parallelism. Even if this evolution stands as a necessary step towards scalability over shared memory machines holding hundreds of cores, the current specification of OpenMP lacks ways of expressing dependencies between tasks, forcing programmers to make unnecessary use of synchronization degrading overall performance. This paper introduces libKOMP, an OpenMP runtime system based on the X-Kaapi library that outperforms popular OpenMP implementations on current task-based OpenMP benchmarks, but also provides OpenMP programmers with new ways of expressing data-flow parallelism. © 2012 Springer-Verlag.","data-flow programming; OpenMP; runtime systems; task parallelism","Data-flow paradigm; Dataflow; High performance computing; On currents; OpenMP; Runtime systems; Shared memory machines; Task parallelism; Task-based; Artificial intelligence; Application programming interfaces (API)",Conference Paper,Scopus,2-s2.0-84862184905
"Qian J., Ferguson T.M., Shinde D.N., Ramírez-Borrero A.J., Hintze A., Adami C., Niemz A.","Sequence dependence of isothermal DNA amplification via EXPAR",2012,"Nucleic Acids Research",24,10.1093/nar/gks230,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862201470&doi=10.1093%2fnar%2fgks230&partnerID=40&md5=a4a44f665d54b0cc0e27dc3cee5897e3","Isothermal nucleic acid amplification is becoming increasingly important for molecular diagnostics. Therefore, new computational tools are needed to facilitate assay design. In the isothermal EXPonential Amplification Reaction (EXPAR), template sequences with similar thermodynamic characteristics perform very differently. To understand what causes this variability, we characterized the performance of 384 template sequences, and used this data to develop two computational methods to predict EXPAR template performance based on sequence: a position weight matrix approach with support vector machine classifier, and RELIEF attribute evaluation with Nave Bayes classification. The methods identified well and poorly performing EXPAR templates with 6770 sensitivity and 7780 specificity. We combined these methods into a computational tool that can accelerate new assay design by ruling out likely poor performers. Furthermore, our data suggest that variability in template performance is linked to specific sequence motifs. Cytidine, a pyrimidine base, is over-represented in certain positions of well-performing templates. Guanosine and adenosine, both purine bases, are over-represented in similar regions of poorly performing templates, frequently as GA or AG dimers. Since polymerases have a higher affinity for purine oligonucleotides, polymerase binding to GA-rich regions of a single-stranded DNA template may promote non-specific amplification in EXPAR and other nucleic acid amplification reactions. © 2012 The Author(s).",,"adenosine; cytidine; dimer; DNA polymerase; guanosine; single stranded DNA; complementary DNA; purine nucleotide; article; Bayesian learning; controlled study; DNA sequence; DNA template; exponential amplification reaction; gene amplification; isothermal DNA amplification; position weight matrix; prediction; priority journal; protein DNA interaction; sensitivity and specificity; sequence analysis; support vector machine; thermodynamics; validation process; accuracy; amplicon; binding affinity; DNA hybridization; DNA sequence; DNA template; evaluation research; learning algorithm; process development; sequence analysis; Artificial Intelligence; Base Sequence; Bayes Theorem; Computational Biology; DNA; Nucleic Acid Amplification Techniques; Position-Specific Scoring Matrices; Software; Templates, Genetic; Thermodynamics",Article,Scopus,2-s2.0-84862201470
"Krysiak-Baltyn K., Toppari J., Skakkebaek N.E., Jensen T.S., Virtanen H.E., Schramm K.-W., Shen H., Vartiainen T., Kiviranta H., Taboureau O., Audouze K., Brunak S., Main K.M.","Association between chemical pattern in breast milk and congenital cryptorchidism: Modelling of complex human exposures",2012,"International Journal of Andrology",24,10.1111/j.1365-2605.2012.01268.x,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861459464&doi=10.1111%2fj.1365-2605.2012.01268.x&partnerID=40&md5=e8fafd753be12d040ce96ea100d93b74","During the past four decades, there has been an increase in the incidence rate of male reproductive disorders in some, but not all, Western countries. The observed increase in the prevalence of male reproductive disorders is suspected to be ascribable to environmental factors as the increase has been too rapid to be explained by genetics alone. To study the association between complex chemical exposures of humans and congenital cryptorchidism, the most common malformation of the male genitalia, we measured 121 environmental chemicals with suspected or known endocrine disrupting properties in 130 breast milk samples from Danish and Finnish mothers. Half the newborns were healthy controls, whereas the other half was boys with congenital cryptorchidism. The measured chemicals included polychlorinated biphenyls (PCBs), polybrominated diphenyl-ethers, dioxins (OCDD/PCDFs), phthalates, polybrominated biphenyls and organochlorine pesticides. Computational analysis of the data was performed using logistic regression and three multivariate machine learning classifiers. Furthermore, we performed systems biology analysis to explore the chemical influence on a molecular level. After correction for multiple testing, exposure to nine chemicals was significantly different between the cases and controls in the Danish cohort, but not in the Finnish cohort. The multivariate analysis indicated that Danish samples exhibited a stronger correlation between chemical exposure patterns in breast milk and cryptorchidism than Finnish samples. Moreover, PCBs were indicated as having a protective effect within the Danish cohort, which was supported by molecular data recovered through systems biology. Our results lend further support to the hypothesis that the mixture of environmental chemicals may contribute to observed adverse trends in male reproductive health. © 2012 The Authors. International Journal of Andrology © 2012 European Academy of Andrology.","Breast milk; Chemicals; Cryptorchidism; Exposure; Modelling; Persistent organic pollutants","amphiregulin; dioxin; diphenyl ether; hydroxymethylglutaryl coenzyme A reductase kinase; organochlorine pesticide; phthalic acid; polybrominated biphenyl; polychlorinated biphenyl; protein kinase C alpha; somatomedin C; article; breast milk; cohort analysis; controlled study; correlation analysis; cryptorchism; Denmark; disease association; environmental exposure; Finland; human; major clinical study; male; newborn; phenotype; priority journal; protein interaction; reproductive health; systems biology; Artificial Intelligence; Cryptorchidism; Denmark; Dioxins; Environmental Pollutants; Female; Finland; Halogenated Diphenyl Ethers; Humans; Logistic Models; Male; Milk, Human; Polychlorinated Biphenyls; Systems Biology",Article,Scopus,2-s2.0-84861459464
"Nearing G.S., Crow W.T., Thorp K.R., Moran M.S., Reichle R.H., Gupta H.V.","Assimilating remote sensing observations of leaf area index and soil moisture for wheat yield estimates: An observing system simulation experiment",2012,"Water Resources Research",24,10.1029/2011WR011420,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861380412&doi=10.1029%2f2011WR011420&partnerID=40&md5=1ac109efbbd2af95676b8bf019bec3de","Observing system simulation experiments were used to investigate ensemble Bayesian state-updating data assimilation of observations of leaf area index (LAI) and soil moisture () for the purpose of improving single-season wheat yield estimates with the Decision Support System for Agrotechnology Transfer (DSSAT) CropSim-Ceres model. Assimilation was conducted in an energy-limited environment and a water-limited environment. Modeling uncertainty was prescribed to weather inputs, soil parameters and initial conditions, and cultivar parameters and through perturbations to model state transition equations. The ensemble Kalman filter and the sequential importance resampling filter were tested for the ability to attenuate effects of these types of uncertainty on yield estimates. LAI and observations were synthesized according to characteristics of existing remote sensing data, and effects of observation error were tested. Results indicate that the potential for assimilation to improve end-of-season yield estimates is low. Limitations are due to a lack of root zone soil moisture information, error in LAI observations, and a lack of correlation between leaf and grain growth. © 2012. American Geophysical Union.",,"Agrotechnology transfer; Data assimilation; Ensemble Kalman Filter; Initial conditions; Leaf Area Index; Model state; Modeling uncertainties; Observation errors; Observing system simulation experiments; Remote sensing data; Resampling; Root zone; Soil parameters; Weather inputs; Wheat yield; Artificial intelligence; Computer simulation; Data processing; Decision support systems; Estimation; Experiments; Geologic models; Grain growth; Remote sensing; Soil moisture; Uncertainty analysis; agricultural modeling; crop yield; cultivar; data assimilation; decision support system; experimental study; Kalman filter; leaf area index; observational method; remote sensing; rhizosphere; soil moisture; uncertainty analysis; water availability; wheat; Ceres; Triticum aestivum",Article,Scopus,2-s2.0-84861380412
"Basin D., Harvan M., Klaedtke F., Zǎlinescu E.","MONPOLY: Monitoring usage-control policies",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",24,10.1007/978-3-642-29860-8_27,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861210843&doi=10.1007%2f978-3-642-29860-8_27&partnerID=40&md5=9b286a8f66cc4a364778350bcd889251","Determining whether the usage of sensitive, digitally stored data complies with regulations and policies is a growing concern for companies, administrations, and end users alike. Classical examples of policies used for protecting and preventing the misuse of data are history-based access-control policies like the Chinese-wall policy and separation-of-duty constraints. Other policies from more specialized areas like banking involve retention, reporting, and transaction requirements. Simplified examples from this domain are that financial reports must be approved at most a week before they are published and that transactions over $10,000 must be reported within two days. © 2012 Springer-Verlag.",,"Classical example; End users; Financial reports; Regulations and policy; Artificial intelligence; Access control",Conference Paper,Scopus,2-s2.0-84861210843
"Wee H.","Dual projective hashing and its applications-lossy trapdoor functions and more",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",24,10.1007/978-3-642-29011-4_16,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859963781&doi=10.1007%2f978-3-642-29011-4_16&partnerID=40&md5=14e85525d4fd1ab8f39e1f3326eba255","We introduce the notion of dual projective hashing. This is similar to Cramer-Shoup projective hashing, except that instead of smoothness, which stipulates that the output of the hash function looks random on no instances, we require invertibility, which stipulates that the output of the hash function on no instances uniquely determine the hashing key, and moreover, that there is a trapdoor which allows us to efficiently recover the hashing key. We show a simple construction of lossy trapdoor functions via dual projective hashing. Our construction encompasses almost all known constructions of lossy trapdoor functions, as given in the works of Peikert and Waters (STOC '08) and Freeman et al. (PKC '10). We also provide a simple construction of deterministic encryption schemes secure with respect to hard-to-invert auxiliary input, under an additional assumption about the projection map. Our construction clarifies and encompasses all of the constructions given in the recent work of Brakerski and Segev (Crypto '11). In addition, we obtain a new deterministic encryption scheme based on LWE. © 2012 International Association for Cryptologic Research.",,"Auxiliary inputs; Encryption schemes; Invertibility; Trapdoor functions; Auxiliary inputs; Deterministic encryptions; Invertibility; ITS applications; Lossy Trapdoor Functions; Artificial intelligence; Hash functions; Hash functions; Cryptography",Conference Paper,Scopus,2-s2.0-84859963781
"Abbasi M.-A., Kumar S., Filho J.A.A., Liu H.","Lessons learned in using social media for disaster relief - ASU crisis response game",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",24,10.1007/978-3-642-29047-3_34,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859154621&doi=10.1007%2f978-3-642-29047-3_34&partnerID=40&md5=d1a760a699a3f4273b17e8f2e1711edc","In disasters such as the earthquake in Haiti and the tsunami in Japan, people used social media to ask for help or report injuries. The popularity, efficiency, and ease of use of social media has led to its pervasive use during the disaster. This creates a pool of timely reports about the disaster, injuries, and help requests. This offers an alternative opportunity for first responders and disaster relief organizations to collect information about the disaster, victims, and their needs. It also presents a challenge for these organizations to aggregate and process the requests from different social media. Given the sheer volume of requests, it is necessary to filter reports and select those of high priority for decision making. Little is known about how the two phases should be smoothly integrated. In this paper we report the use of social media during a simulated crisis and crisis response process, the ASU Crisis Response Game. Its main objective is to creat a training capability to understand how to use social media in crisis. We report lessons learned from this exercise that may benefit first responders and NGOs who use social media to manage relief efforts during the disaster. © 2012 Springer-Verlag.",,"Crisis response; Disaster relief; Ease of use; First responders; Relief efforts; Social media; Artificial intelligence; Disaster prevention",Conference Paper,Scopus,2-s2.0-84859154621
"Temerinac-Ott M., Ronneberger O., Ochs P., Driever W., Brox T., Burkhardt H.","Multiview deblurring for 3-D images from light-Sheet-based fluorescence microscopy",2012,"IEEE Transactions on Image Processing",24,10.1109/TIP.2011.2181528,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859027565&doi=10.1109%2fTIP.2011.2181528&partnerID=40&md5=171a8422a253a720948a6eb43adbb775","We propose an algorithm for 3-D multiview deblurring using spatially variant point spread functions (PSFs). The algorithm is applied to multiview reconstruction of volumetric microscopy images. It includes registration and estimation of the PSFs using irregularly placed point markers (beads). We formulate multiview deblurring as an energy minimization problem subject to L1-regularization. Optimization is based on the regularized Lucy-Richardson algorithm, which we extend to deal with our more general model. The model parameters are chosen in a profound way by optimizing them on a realistic training set. We quantitatively and qualitatively compare with existing methods and show that our method provides better signal-to-noise ratio and increases the resolution of the reconstructed images. © 2011 IEEE.","3-D images; Deblurring with spatially variant point spread function (PSF); deconvolution; fluorescence microscopy; L1-regularization; PSF estimation; registration with irregularly placed point markers; tomography","3-D image; Deblurring; L1-regularization; PSF estimation; registration with irregularly placed point markers; Algorithms; Deconvolution; Fluorescence microscopy; Image enhancement; Optical design; Optical transfer function; Optimization; Tomography; Three dimensional; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; fluorescence microscopy; image enhancement; methodology; reproducibility; sensitivity and specificity; three dimensional imaging; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Microscopy, Fluorescence; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84859027565
"Katagiri H., Hayashida T., Nishizaki I., Guo Q.","A hybrid algorithm based on tabu search and ant colony optimization for k-minimum spanning tree problems",2012,"Expert Systems with Applications",24,10.1016/j.eswa.2011.11.103,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855888656&doi=10.1016%2fj.eswa.2011.11.103&partnerID=40&md5=af54e44305c3beacde45d14ccace2248","This paper considers an efficient approximate algorithm for solving k-minimum spanning tree problems which is one of the combinatorial optimization in networks. A new hybrid algorithm based on tabu search and ant colony optimization is provided. Results of numerical experiments show that the proposed method updates some of the best known values with very short time and that the proposed method provides a better performance with solution accuracy over existing algorithms. © 2011 Elsevier Ltd. All rights reserved.","Ant colony optimization; Approximate solution; Hybrid algorithm; k-Minimum spanning tree; Tabu search","Ant-colony optimization; Approximate algorithms; Approximate solution; Hybrid algorithms; k-Minimum spanning tree; Numerical experiments; Solution accuracy; Spanning tree problems; Algorithms; Artificial intelligence; Combinatorial optimization; Numerical methods; Parallel architectures; Trees (mathematics); Tabu search",Article,Scopus,2-s2.0-84855888656
"Yu C.-M., Peng H.-P., Chen I.-C., Lee Y.-C., Chen J.-B., Tsai K.-C., Chen C.-T., Chang J.-Y., Yang E.-W., Hsu P.-C., Jian J.-W., Hsu H.-J., Chang H.-J., Hsu W.-L., Huang K.-F., Ma A.C., Yang A.-S.","Rationalization and design of the complementarity determining region sequences in an antibody-antigen recognition interface",2012,"PLoS ONE",24,10.1371/journal.pone.0033340,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863338291&doi=10.1371%2fjournal.pone.0033340&partnerID=40&md5=9aaabfffc20132d81713415aaf4dab6a","Protein-protein interactions are critical determinants in biological systems. Engineered proteins binding to specific areas on protein surfaces could lead to therapeutics or diagnostics for treating diseases in humans. But designing epitope-specific protein-protein interactions with computational atomistic interaction free energy remains a difficult challenge. Here we show that, with the antibody-VEGF (vascular endothelial growth factor) interaction as a model system, the experimentally observed amino acid preferences in the antibody-antigen interface can be rationalized with 3-dimensional distributions of interacting atoms derived from the database of protein structures. Machine learning models established on the rationalization can be generalized to design amino acid preferences in antibody-antigen interfaces, for which the experimental validations are tractable with current high throughput synthetic antibody display technologies. Leave-one-out cross validation on the benchmark system yielded the accuracy, precision, recall (sensitivity) and specificity of the overall binary predictions to be 0.69, 0.45, 0.63, and 0.71 respectively, and the overall Matthews correlation coefficient of the 20 amino acid types in the 24 interface CDR positions was 0.312. The structure-based computational antibody design methodology was further tested with other antibodies binding to VEGF. The results indicate that the methodology could provide alternatives to the current antibody technologies based on animal immune systems in engineering therapeutic and diagnostic antibodies against predetermined antigen epitopes. © 2012 Yu et al.",,"amino acid; vasculotropin antibody; single chain fragment variable antibody; vasculotropin A; accuracy; antibody combining site; antibody structure; antigen antibody reaction; antigen binding; antigen recognition; article; binding affinity; complementarity determining region; controlled study; crystal structure; experimental design; high throughput screening; intermethod comparison; machine learning; mathematical computing; molecular interaction; molecular recognition; prediction; process development; process optimization; protein structure; quality control; sensitivity and specificity; sequence analysis; three dimensional imaging; antigen antibody reaction; artificial intelligence; chemical structure; chemistry; human; immunology; reproducibility; validation study; X ray crystallography; Animalia; Antigen-Antibody Reactions; Artificial Intelligence; Binding Sites, Antibody; Complementarity Determining Regions; Crystallography, X-Ray; Humans; Models, Molecular; Reproducibility of Results; Single-Chain Antibodies; Vascular Endothelial Growth Factor A",Article,Scopus,2-s2.0-84863338291
"Akdemir B., Çetinkaya N.","Long-term load forecasting based on adaptive neural fuzzy inference system using real energy data",2012,"Energy Procedia",24,10.1016/j.egypro.2011.12.1013,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858384979&doi=10.1016%2fj.egypro.2011.12.1013&partnerID=40&md5=f8135cc60961ad12ac8531b474483a20","Energy production and distributing have critical importance for all countries especially developing countries. Studies about energy consumption, distributing and planning have much importance at the present day. In order to manage any power plant or take precautions about energy subject, many kinds of observations are used for short, mid and long term forecasting. Especially long term forecasting is in need to plan and carry on future energy demand and investment such as size of energy plant and location. Long term forecasting often includes power consumption data for past years, national incoming per year, rates of civilization, increasing population rates and moreover economical parameters. Long term forecasting data vary from one month to several years. Some of the forecasting models use mathematical formulas and statistical models such as correlation and regression models. In this study, artificial intelligence is used to forecast long term energy demand. Artificial intelligences are widely used for engineering problems to solve and obtain valid solutions. Adaptive neural fuzzy inference system is one of the most famous artificial intelligence methods and has been widely used in literature. In addition to numerical inputs, Adaptive neural fuzzy inference system has linguistics inputs such as good, bad and ugly. Adaptive neural fuzzy inference system is used to obtain long term forecasting results and the results are compared to mathematical methods to show validity and error levels. In order to show error levels, mean absolute error and mean absolute error percentage are used. Mean absolute error and mean absolute error percentages are very common and practical methods in literature. The obtained error results, from 2003 to 2025, mean absolute error and mean absolute percentage error are 1.504313 and 0.82439, respectively. Success of Adaptive neural fuzzy inference system for energy demand forecasting is 99.17%. © 2011 Published by Elsevier Ltd.","Adaptive neural fuzzy inference system; Long term forecasting; Mean absolute error; Mean absolute error percentage; Real data set","Adaptive neural fuzzy inference system (ANFIS); Artificial intelligence methods; Data sets; Economical parameters; Energy data; Energy demand forecasting; Energy demands; Energy plant; Energy productions; Engineering problems; Error levels; Forecasting models; Future energies; Long-term forecasting; Long-term load forecasting; Mathematical formulas; Mathematical method; Mean absolute error; Mean absolute percentage error; Numerical inputs; Practical method; Regression model; Statistical models; Artificial intelligence; Developing countries; Electric load forecasting; Energy utilization; Forecasting; Fuzzy systems; Mathematical models; Population statistics; Profilometry; Regression analysis; Investments",Conference Paper,Scopus,2-s2.0-84858384979
"Libert B., Yung M.","Non-interactive CCA-secure threshold cryptosystems with adaptive security: New framework and constructions",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",24,10.1007/978-3-642-28914-9_5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858324813&doi=10.1007%2f978-3-642-28914-9_5&partnerID=40&md5=7c78663e471e4562a648ca00b23d6770","In threshold cryptography, private keys are divided into n shares, each one of which is given to a different server in order to avoid single points of failure. In the case of threshold public-key encryption, at least t ≤ n servers need to contribute to the decryption process. A threshold primitive is said robust if no coalition of t malicious servers can prevent remaining honest servers from successfully completing private key operations. So far, most practical non-interactive threshold cryptosystems, where no interactive conversation is required among decryption servers, were only proved secure against static corruptions. In the adaptive corruption scenario (where the adversary can corrupt servers at any time, based on its complete view), all existing robust threshold encryption schemes that also resist chosen-ciphertext attacks (CCA) till recently require interaction in the decryption phase. A specific method (in composite order groups) for getting rid of interaction was recently suggested, leaving the question of more generic frameworks and constructions with better security and better flexibility (i.e., compatibility with distributed key generation). This paper describes a general construction of adaptively secure robust non-interactive threshold cryptosystems with chosen-ciphertext security. We define the notion of all-but-one perfectly sound threshold hash proof systems that can be seen as (threshold) hash proof systems with publicly verifiable and simulation-sound proofs. We show that this notion generically implies threshold cryptosystems combining the aforementioned properties. Then, we provide efficient instantiations under well-studied assumptions in bilinear groups (e.g., in such groups of prime order). These instantiations have a tighter security proof and are indeed compatible with distributed key generation protocols. © 2012 Springer-Verlag.","adaptive corruptions; chosen-ciphertext security; non-interactivity; public-key encryption; robustness; Threshold cryptography","adaptive corruptions; Chosen ciphertext security; Non-interactivity; Public-key encryption; Threshold cryptography; Adaptive corruptions; Chosen ciphertext security; Non-interactivity; Public-key encryption; Threshold cryptography; Artificial intelligence; Robustness (control systems); Public key cryptography; Robustness (control systems); Security of data; Public key cryptography; Cryptography",Conference Paper,Scopus,2-s2.0-84858324813
"Amaral J.L.M., Lopes A.J., Jansen J.M., Faria A.C.D., Melo P.L.","Machine learning algorithms and forced oscillation measurements applied to the automatic identification of chronic obstructive pulmonary disease",2012,"Computer Methods and Programs in Biomedicine",24,10.1016/j.cmpb.2011.09.009,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856228303&doi=10.1016%2fj.cmpb.2011.09.009&partnerID=40&md5=e4be245fc238d6638087fd6820cd2502","The purpose of this study is to develop a clinical decision support system based on machine learning (ML) algorithms to help the diagnostic of chronic obstructive pulmonary disease (COPD) using forced oscillation (FO) measurements. To this end, the performances of classification algorithms based on Linear Bayes Normal Classifier, K nearest neighbor (KNN), decision trees, artificial neural networks (ANN) and support vector machines (SVM) were compared in order to the search for the best classifier. Four feature selection methods were also used in order to identify a reduced set of the most relevant parameters. The available dataset consists of 7 possible input features (FO parameters) of 150 measurements made in 50 volunteers (COPD, n=25; healthy, n=25). The performance of the classifiers and reduced data sets were evaluated by the determination of sensitivity (Se), specificity (Sp) and area under the ROC curve (AUC). Among the studied classifiers, KNN, SVM and ANN classifiers were the most adequate, reaching values that allow a very accurate clinical diagnosis (Se > 87%, Sp > 94%, and AUC > 0.95). The use of the analysis of correlation as a ranking index of the FOT parameters, allowed us to simplify the analysis of the FOT parameters, while still maintaining a high degree of accuracy. In conclusion, the results of this study indicate that the proposed classifiers may contribute to easy the diagnostic of COPD by using forced oscillation measurements. © 2011 Elsevier Ireland Ltd.","Artificial intelligence; Chronic obstructive pulmonary disease; Classification; Clinical decision support; Forced oscillation technique; Respiratory system","Area under the ROC curve; Artificial Neural Network; Automatic identification; Chronic obstructive pulmonary disease; Classification algorithm; Clinical decision support; Clinical decision support systems; Clinical diagnosis; Data sets; Feature selection methods; Forced oscillation techniques; Forced oscillations; High degree of accuracy; Input features; K-nearest neighbors; On-machines; Reduced data; Artificial intelligence; Automation; Classification (of information); Decision support systems; Decision trees; Diagnosis; Feature extraction; Learning algorithms; Measurements; Neural networks; Respiratory system; Support vector machines; Pulmonary diseases; area under the curve; article; artificial neural network; autoanalysis; Bayes theorem; chronic obstructive lung disease; decision support system; decision tree; forced oscillation; human; k nearest neighbor; learning algorithm; machine learning; oscillation; performance measurement system; sensitivity and specificity; support vector machine; Algorithms; Artificial Intelligence; Decision Support Systems, Clinical; Diagnosis, Computer-Assisted; Humans; Neural Networks (Computer); Pulmonary Disease, Chronic Obstructive; Support Vector Machines",Article,Scopus,2-s2.0-84856228303
"Persa G., Csapo A., Baranyi P.","Coginfocom systems from an interaction perspective - A pilot application for EtoCom",2012,"Journal of Advanced Computational Intelligence and Intelligent Informatics",24,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859262790&partnerID=40&md5=0a2ae053c53854817504782cc8321658","Cognitive Infocommunications (CogInfoCom) is a newly emerging research field that investigates the link between infocommunications and the cognitive sciences, with the goal of creating engineering systems in which artificial and natural cognitive systems can work together more effectively. In this paper, we describe the structure of CogInfoCom systems from an interaction perspective. Through the discussions in this paper, our goal is to further clarify the relationship between CogInfoCom and the various research areas that deal with behavioral and structural systems modeling. In order to demonstrate the theoretical aspects of the subject, we describe a pilot application which was developed during the EtoCom project.","CogInfo-Com; Cognitive infocommunications; EtoCom","CogInfo-Com; Cognitive science; Engineering systems; EtoCom; Infocommunications; Pilot applications; Research fields; Structural systems; Theoretical aspects; Artificial intelligence; Computational methods; Cognitive systems",Conference Paper,Scopus,2-s2.0-84859262790
"Sánchez-Pi N., Carbó J., Molina J.M.","A knowledge-based system approach for a context-aware system",2012,"Knowledge-Based Systems",24,10.1016/j.knosys.2011.08.017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855967351&doi=10.1016%2fj.knosys.2011.08.017&partnerID=40&md5=800fe51cf5248d1d9727e0801811c79f","Knowledge-based systems (KBS) are advanced systems for representing complex problems. Their architecture and representation formalisms are the groundwork of today's systems. The knowledge is usually derived from expertise in specific areas and has to be validated according to a different methodology than is used in conventional systems because the knowledge is symbolic. This paper describes the design, definition and evaluation of a knowledge-based system using the CommonKADS (CKADS) methodology to formally represent contextual information for the Appear platform. We also evaluate the context-aware information system from the user's point of view using a U2E system and also validate it through a simulated example in a realistic environment: an airport domain, which is a significant step towards formally building KBS applications. © 2011 Elsevier B.V. All rights reserved.","Airport domain; Context aware systems; Evaluation; Knowledge-based systems; Methodologies; Realworld applications","Advanced systems; CommonKADS; Complex problems; Context-Aware; Context-aware systems; Contextual information; Conventional systems; Evaluation; Methodologies; Real-world application; Realistic environments; Representation formalisms; Specific areas; Artificial intelligence; Software engineering; Knowledge based systems",Article,Scopus,2-s2.0-84855967351
"Topouzelis K., Psyllos A.","Oil spill feature selection and classification using decision tree forest on SAR image data",2012,"ISPRS Journal of Photogrammetry and Remote Sensing",24,10.1016/j.isprsjprs.2012.01.005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857385727&doi=10.1016%2fj.isprsjprs.2012.01.005&partnerID=40&md5=ad1c84c4976db99b7b390895a43917fb","A novel oil spill feature selection and classification technique is presented, based on a forest of decision trees. The parameters of the two-class classification problem of oil spills and look-alikes are explored. The contribution to the final classification of the 25 most commonly used features in the scientific community was examined. The work is sought in the framework of a multi-objective problem, i.e. the minimization of the used input features and, at the same time, the maximization of the overall testing classification accuracy. Results showed that the optimum forest contains 70 trees and the three most important combinations contain 4, 6 and 9 features. The latter feature combination can be seen as the most appropriate solution of the decision forest study. Examination of the robustness of the above result showed that the proposed combination achieved higher classification accuracy than other well-known statistical separation indexes. Moreover, comparisons with previous findings converge on the classification accuracy (up to 84.5%) and to the number of selected features, but diverge on the actual features. This observation leads to the conclusion that there is not a single optimum feature combination; several sets of combinations exist which contain at least some critical features. © 2012 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS).","Classification; Decision forest; Feature selection; Machine learning; Oil spill; SAR","Classification accuracy; Decision forest; Feature combination; Feature selection and classification; Input features; Machine-learning; Multi-objective problem; SAR; SAR Images; Scientific community; Two-class classification problems; Classification (of information); Decision trees; Feature extraction; Learning algorithms; Oil spills; Forestry; accuracy assessment; artificial intelligence; classification; data set; decision support system; image classification; oil spill; optimization; synthetic aperture radar; Algorithms; Classification; Forestry; Information Retrieval",Article,Scopus,2-s2.0-84857385727
"D'Acierno L., Gallo M., Montella B.","An Ant Colony Optimisation algorithm for solving the asymmetric traffic assignment problem",2012,"European Journal of Operational Research",24,10.1016/j.ejor.2011.09.035,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80755186533&doi=10.1016%2fj.ejor.2011.09.035&partnerID=40&md5=735ff742c79799315d379432df408697","In this paper we propose an Ant Colony Optimisation (ACO) algorithm for defining the signal settings on urban networks following a local approach. This consists in optimising the signal settings of each intersection of an urban network as a function only of traffic flows at the accesses to the same intersection, taking account of the effects of signal settings on costs and on user route choices. This problem, also known as Local Optimisation of Signal Settings (LOSS), has been widely studied in the literature and can be formulated as an asymmetric assignment problem. The proposed ACO algorithm is based on two kinds of behaviour of artificial ants which allow the LOSS problem to be solved: traditional behaviour based on the response to pheromones for simulating user route choice, and innovative behaviour based on the pressure of an ant stream for solving the signal setting definition problem. Our results on real-scale networks show that the proposed approach allows the solution to be obtained in less time but with the same accuracy as in traditional MSA (Method of Successive Averages) approaches. © 2011 Elsevier B.V. All rights reserved.","Ant Colony Optimisation; Signal settings design; Stochastic traffic assignment; Traffic","ACO algorithms; Ant colony optimisation; Artificial ant; Assignment problems; Asymmetric traffic; Local approaches; Method of successive averages; Optimisations; Route choice; Signal settings design; Stochastic traffic; Traffic flow; Urban networks; Artificial intelligence; Optimization; Traffic control; Traffic signals; Transportation routes; Algorithms",Article,Scopus,2-s2.0-80755186533
"Hu D., Sarosh A., Dong Y.-F.","A novel KFCM based fault diagnosis method for unknown faults in satellite reaction wheels",2012,"ISA Transactions",24,10.1016/j.isatra.2011.10.005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857045970&doi=10.1016%2fj.isatra.2011.10.005&partnerID=40&md5=a795e374301cab2d02650cb325e15db2","Reaction wheels are one of the most critical components of the satellite attitude control system, therefore correct diagnosis of their faults is quintessential for efficient operation of these spacecraft. The known faults in any of the subsystems are often diagnosed by supervised learning algorithms, however, this method fails to work correctly when a new or unknown fault occurs. In such cases an unsupervised learning algorithm becomes essential for obtaining the correct diagnosis. Kernel Fuzzy C-Means (KFCM) is one of the unsupervised algorithms, although it has its own limitations; however in this paper a novel method has been proposed for conditioning of KFCM method (C-KFCM) so that it can be effectively used for fault diagnosis of both known and unknown faults as in satellite reaction wheels. The C-KFCM approach involves determination of exact class centers from the data of known faults, in this way discrete number of fault classes are determined at the start. Similarity parameters are derived and determined for each of the fault data point. Thereafter depending on the similarity threshold each data point is issued with a class label. The high similarity points fall into one of the 'known-fault' classes while the low similarity points are labeled as 'unknown-faults'. Simulation results show that as compared to the supervised algorithm such as neural network, the C-KFCM method can effectively cluster historical fault data (as in reaction wheels) and diagnose the faults to an accuracy of more than 91%. © 2011 ISA. Published by Elsevier Ltd. All rights reserved.","Fault detection and diagnosis; Fuzzy clustering; Kernel fuzzy c-means clustering; Reaction wheel; Satellite attitude control system; Similarity","Fault detection and diagnosis; Fuzzy C means clustering; Reaction wheel; Satellite attitude control system; Similarity; Communication satellites; Fault detection; Fuzzy clustering; Fuzzy systems; Learning algorithms; Navigation; Teaching; Flight dynamics; algorithm; article; artificial intelligence; artificial neural network; cluster analysis; computer simulation; equipment; equipment design; fuzzy logic; reproducibility; space flight; statistical analysis; Algorithms; Artificial Intelligence; Cluster Analysis; Computer Simulation; Data Interpretation, Statistical; Equipment Design; Equipment Failure; Fuzzy Logic; Neural Networks (Computer); Reproducibility of Results; Spacecraft",Article,Scopus,2-s2.0-84857045970
"Jin Y., Guo H., Meng Y.","A hierarchical gene regulatory network for adaptive multirobot pattern formation",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",24,10.1109/TSMCB.2011.2178021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861191790&doi=10.1109%2fTSMCB.2011.2178021&partnerID=40&md5=a7e000881fe75a0e32aa04238beab260","Most existing multirobot systems for pattern formation rely on a predefined pattern, which is impractical for dynamic environments where the pattern to be formed should be able to change as the environment changes. In addition, adaptation to environmental changes should be realized based only on local perception of the robots. In this paper, we propose a hierarchical gene regulatory network (H-GRN) for adaptive multirobot pattern generation and formation in changing environments. The proposed model is a two-layer gene regulatory network (GRN), where the first layer is responsible for adaptive pattern generation for the given environment, while the second layer is a decentralized control mechanism that drives the robots onto the pattern generated by the first layer. An evolutionary algorithm is adopted to evolve the parameters of the GRN subnetwork in layer 1 for optimizing the generated pattern. The parameters of the GRN in layer 2 are also optimized to improve the convergence performance. Simulation results demonstrate that the H-GRN is effective in forming the desired pattern in a changing environment. Robustness of the H-GRN to robot failure is also examined. A proof-of-concept experiment using e-puck robots confirms the feasibility and effectiveness of the proposed model. © 2012 IEEE.","Dynamic environment; evolutionary algorithms; hierarchical gene regulatory network (H-GRN); multirobot pattern generation and formation; self-organization","Adaptive pattern; Changing environment; Convergence performance; Dynamic environments; Environment change; Environmental change; Gene regulatory networks; Layer 1; Layer 2; Multi-robot systems; Multirobots; Pattern formation; Pattern Generation; Proof of concept; Second layer; self-organization; Sub-network; Two layers; Evolutionary algorithms; Industrial robots; Optimization; Genes; algorithm; animal; article; artificial intelligence; automated pattern recognition; biological model; biomimetics; computer simulation; decision support system; gene expression regulation; human; methodology; robotics; Algorithms; Animals; Artificial Intelligence; Biomimetics; Computer Simulation; Decision Support Techniques; Gene Expression Regulation; Humans; Models, Genetic; Pattern Recognition, Automated; Robotics",Article,Scopus,2-s2.0-84861191790
"Akhand M.A.H., Murase K.","Ensembles of neural networks based on the alteration of input feature values",2012,"International Journal of Neural Systems",24,10.1142/S0129065712003079,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856050339&doi=10.1142%2fS0129065712003079&partnerID=40&md5=96c0759c33dbe96f056c478bc337a88d","An ensemble performs well when the component classifiers are diverse yet accurate, so that the failure of one is compensated for by others. A number of methods have been investigated for constructing ensemble in which some of them train classifiers with the generated patterns. This study investigates a new technique of training pattern generation. The method alters input feature values of some patterns using the values of other patterns to generate different patterns for different classifiers. The effectiveness of neural network ensemble based on the proposed technique was evaluated using a suite of 25 benchmark classification problems, and was found to achieve performance better than or competitive with related conventional methods. Experimental investigation of different input values alteration techniques finds that alteration with pattern values in the same class is better for generalization, although other alteration techniques may offer more diversity. © 2012 World Scientific Publishing Company.","Diversity; Generalization; Input feature values; Neural network ensemble; Pattern generation","Diversity; Generalization; Input features; Neural network ensemble; Pattern generation; Benchmarking; Neural networks; algorithm; article; artificial intelligence; artificial neural network; automated pattern recognition; computer simulation; methodology; Algorithms; Artificial Intelligence; Computer Simulation; Neural Networks (Computer); Pattern Recognition, Automated",Article,Scopus,2-s2.0-84856050339
"Dudík M., Harchaoui Z., Malick J.","Lifted coordinate descent for learning with trace-norm regularization",2012,"Journal of Machine Learning Research",24,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954222977&partnerID=40&md5=a2109244e3c982326fcfb861a0792a21","We consider the minimization of a smooth loss with trace-norm regularization, which is a natural objective in multi-class and multitask learning. Even though the problem is convex, existing approaches rely on optimizing a non-convex variational bound, which is not guaranteed to converge, or repeatedly perform singular-value decomposition, which prevents scaling beyond moderate matrix sizes. We lift the non-smooth convex problem into an infinitely dimensional smooth problem and apply coordinate descent to solve it. We prove that our approach converges to the optimum, and is competitive or outperforms state of the art.",,"Artificial intelligence; Convex problems; Coordinate descent; Matrix size; Multitask learning; State of the art; Trace-norms; Variational bounds; Singular value decomposition",Conference Paper,Scopus,2-s2.0-84954222977
"Rigamonti R., Lepetit V.","Accurate and efficient linear structure segmentation by leveraging ad hoc features with learned filters",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",24,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872519433&partnerID=40&md5=9734bbcbee130b35bf2c68c7dac7d1a1","Extracting linear structures, such as blood vessels or dendrites, from images is crucial in many medical imagery applications, and many handcrafted features have been proposed to solve this problem. However, such features rely on assumptions that are never entirely true. Learned features, on the other hand, can capture image characteristics difficult to define analytically, but tend to be much slower to compute than handcrafted features. We propose to complement handcrafted methods with features found using very recent Machine Learning techniques, and we show that even few filters are sufficient to efficiently leverage handcrafted features. We demonstrate our approach on the STARE, DRIVE, and BF2D datasets, and on 2D projections of neural images from the DIADEM challenge. Our proposal outperforms handcrafted methods, and pairs up with learning-only approaches at a fraction of their computational cost. © Springer-Verlag Berlin Heidelberg 2012.",,"Bandpass filters; Blood vessels; Education; Learning systems; Medical computing; 2D projections; Capture images; Computational costs; Linear structures; Machine learning techniques; Medical imagery; Medical imaging; algorithm; article; artificial intelligence; automated pattern recognition; blood vessel; computer program; computer simulation; diagnostic imaging; human; image processing; methodology; nerve cell; pathology; regression analysis; reproducibility; statistical model; Algorithms; Artificial Intelligence; Blood Vessels; Computer Simulation; Diagnostic Imaging; Humans; Image Processing, Computer-Assisted; Models, Statistical; Neurons; Pattern Recognition, Automated; Regression Analysis; Reproducibility of Results; Software",Conference Paper,Scopus,2-s2.0-84872519433
"Martínez-Pérez F.E., González-Fraga J.A., Cuevas-Tello J.C., Rodríguez M.D.","Activity inference for ambient intelligence through handling artifacts in a healthcare environment",2012,"Sensors",24,10.3390/s120101072,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856278436&doi=10.3390%2fs120101072&partnerID=40&md5=9a85e5e6ca153ffd27e9878b5452c246","Human activity inference is not a simple process due to distinct ways of performing it. Our proposal presents the SCAN framework for activity inference. SCAN is divided into three modules: (1) artifact recognition, (2) activity inference, and (3) activity representation, integrating three important elements of Ambient Intelligence (AmI) (artifact-behavior modeling, event interpretation and context extraction). The framework extends the roaming beat (RB) concept by obtaining the representation using three kinds of technologies for activity inference. The RB is based on both analysis and recognition from artifact behavior for activity inference. A practical case is shown in a nursing home where a system affording 91.35% effectiveness was implemented in situ. Three examples are shown using RB representation for activity representation. Framework description, RB description and CALog system overcome distinct problems such as the feasibility to implement AmI systems, and to show the feasibility for accomplishing the challenges related to activity recognition based on artifact recognition. We discuss how the use of RBs might positively impact the problems faced by designers and developers for recovering information in an easier manner and thus they can develop tools focused on the user. © 2012 by the authors; licensee MDPI, Basel, Switzerland.","Activity inference; Activity representation; Ambient intelligence; Artifact recognition","acceleration; algorithm; article; artifact; artificial intelligence; blood pressure; health care delivery; human; human activities; image processing; methodology; physiology; radiofrequency identification; telemetry; Acceleration; Algorithms; Artifacts; Artificial Intelligence; Blood Pressure; Delivery of Health Care; Human Activities; Humans; Image Processing, Computer-Assisted; Radio Frequency Identification Device; Telemetry",Article,Scopus,2-s2.0-84856278436
"Kar P., Karnick H.","Random feature maps for dot product kernels",2012,"Journal of Machine Learning Research",24,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919922637&partnerID=40&md5=f47bb2c873b4adb41aa932eb3e774ab8","Approximating non-linear kernels using feature maps has gained a lot of interest in recent years due to applications in reducing training and testing times of SVM classifiers and other kernel based learning algorithms. We extend this line of work and present low distortion embeddings for dot product kernels into linear Euclidean spaces. We base our results on a classical result in harmonic analysis characterizing all dot product kernels and use it to define randomized feature maps into explicit low dimensional Euclidean spaces in which the native dot product provides an approximation to the dot product kernel with high confidence.",,"Geometry; Euclidean spaces; High confidence; Kernel based learning algorithm; Low dimensional Euclidean spaces; Low-distortion embeddings; Non linear kernels; Random features; Training and testing; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84919922637
"Chen X., Shi X., Xu X., Wang Z., Mills R., Lee C., Xu J.","A two-graph guided multi-task lasso approach for eQTL mapping",2012,"Journal of Machine Learning Research",24,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899766043&partnerID=40&md5=78019c724482618234f9faaa457fd6e1","Learning a small number of genetic variants associated with multiple complex genetic traits is of practical importance and remains challenging due to the highdimensional nature of data. In this paper, we proposed a two-graph guided multi-task Lasso to address this issue with an emphasis on estimating subnetwork-to-subnetwork associations in expression quantitative trait loci (eQTL) mapping. The proposed model can learn such subnetworkto-subnetwork associations and therefore can be seen as a generalization of several state-of-the-art multi-task feature selection methods. Additionally, this model has a nice property of allowing flexible structured sparsity on both feature and label domains. Simulation study shows the improved performance of our model and a human eQTL data set is analyzed to further demonstrate the applications of the model.",,"Artificial intelligence; Feature selection methods; Genetic variants; High-dimensional; Practical importance; Quantitative trait locus; Simulation studies; State of the art; Structured sparsities; Mapping",Conference Paper,Scopus,2-s2.0-84899766043
"Lukasiewicz T., Martinez M.V., Simari G.I.","Inconsistency handling in Datalog+/- Ontologies",2012,"Frontiers in Artificial Intelligence and Applications",24,10.3233/978-1-61499-098-7-558,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878795095&doi=10.3233%2f978-1-61499-098-7-558&partnerID=40&md5=d6669b199de772b766dd34ff8aa4e857","The advent of the Semantic Web has made the problem of inconsistency management especially relevant. Datalog+/- is a family of ontology languages that is in particular useful for representing and reasoning over lightweight ontologies in the Semantic Web. In this paper, we study different semantics for query answering in inconsistent Datalog+/- ontologies. We develop a general framework for inconsistency management in Datalog+/- ontologies based on incision functions from belief revision, in which we can characterize several query answering semantics as special cases: (i) consistent answers, originally developed for relational databases and recently adopted for some classes of description logics (DLs), (ii) intersection semantics, a sound approximation of consistent answers, and (iii) lazy consistent answers, a novel alternative semantics that offers a good compromise between quality of answers and computation time. We also provide complexity results for query answering under the different semantics, including data tractability results. © 2012 The Author(s).",,"Artificial intelligence; Data description; Semantics; Alternative Semantics; Complexity results; Description logic; Inconsistency handling; Inconsistency management; Lightweight ontology; Ontology language; Relational Database; Semantic Web",Conference Paper,Scopus,2-s2.0-84878795095
"Brandes U., Mader M.","A quantitative comparison of stress-minimization approaches for offline dynamic graph drawing",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",24,10.1007/978-3-642-25878-7_11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84455209637&doi=10.1007%2f978-3-642-25878-7_11&partnerID=40&md5=1f380bb23b764b2190ad92354fc50a26","In dynamic graph drawing, the input is a sequence of graphs for which a sequence of layouts is to be generated such that the quality of individual layouts is balanced with layout stability over time. Qualitatively different extensions of drawing algorithms for static graphs to the dynamic case have been proposed, but little is known about their relative utility. We report on a quantitative study comparing the three prototypical extensions via their adaptation for the stress-minimization framework. While some findings are more subtle, the linking approach connecting consecutive instances of the same vertex is found to be the overall method of choice. © 2012 Springer-Verlag Berlin Heidelberg.",,"Drawing algorithms; Dynamic graph drawing; Offline; Quantitative comparison; Quantitative study; Relative utility; Drawing algorithms; Dynamic graph drawing; Offline; Quantitative comparison; Quantitative study; Relative utility; Artificial intelligence; Artificial intelligence; Computer science; Computers; Drawing (graphics); Drawing (graphics)",Conference Paper,Scopus,2-s2.0-84455209637
"Toivanen J.M., Toivonen H., Valitutti A., Gross O.","Corpus-based generation of content and form in poetry",2012,"Proceedings of the 3rd International Conference on Computational Creativity, ICCC 2012",24,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984817856&partnerID=40&md5=bb9c3393bb784087ae5a96f53e8686c9","We employ a corpus-based approach to generate content and form in poetry. The main idea is to use two different corpora, on one hand, to provide semantic content for new poems, and on the other hand, to generate a specific grammatical and poetic structure. The approach uses text mining methods, morphological analysis, and morphological synthesis to produce poetry in Finnish. We present some promising results obtained via the combination of these methods and preliminary evaluation results of poetry generated by the system.",,"Data mining; Semantics; Corpus-based approaches; Corpus-based generation; Evaluation results; Finnish; Morphological analysis; Morphological synthesis; Semantic content; Text mining; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84984817856
"Santana Á.L., Conde G.B., Rego L.P., Rocha C.A., Cardoso D.L., Costa J.C.W., Bezerra U.H., Francês C.R.L.","PREDICT - Decision support system for load forecasting and inference: A new undertaking for Brazilian power suppliers",2012,"International Journal of Electrical Power and Energy Systems",24,10.1016/j.ijepes.2011.12.018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862026861&doi=10.1016%2fj.ijepes.2011.12.018&partnerID=40&md5=14da4a62e32e9694538b4b0b268be93c","One of the most desired aspects for power suppliers is the acquisition/sale of energy for a future demand. However, power consumption forecast is characterized not only by the variables of the power system itself, but also related to social-economic and climatic factors. Hence, it is imperative for the power suppliers to project and correlate these parameters. This paper presents a study of power load forecast for power suppliers, considering the applicability of wavelets, time series analysis methods and artificial neural networks, for both mid and long term forecasts. Both the periods of forecast are of major importance for power suppliers to define the future power consumption of a given region. The paper also studies the establishment of correlations among the variables using Bayesian networks. The results obtained are much more effective when compared to those projected by the power suppliers based on specialist information. The research discussed here is implemented on a decision support system, contributing to the decision making for acquisition/sale of energy at a future demand; also providing them with new ways for inference and analyses with the correlation model presented here. © 2011 Elsevier Ltd. All rights reserved.","Bayesian networks; Decision support system; Neural networks; Power load forecast; Time series analysis; Wavelets","Artificial intelligence; Bayesian networks; Decision making; Deep neural networks; Electric power plant loads; Electric power utilization; Forecasting; Harmonic analysis; Neural networks; Time series analysis; Climatic factors; Correlation modeling; Load forecasting; Long-term forecast; Power load; Power suppliers; Social-economic; Wavelets; Decision support systems",Article,Scopus,2-s2.0-84862026861
"Grondman I., Vaandrager M., Buşoniu L., Babuška R., Schuitema E.","Efficient model learning methods for actor-critic control",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",24,10.1109/TSMCB.2011.2170565,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861185789&doi=10.1109%2fTSMCB.2011.2170565&partnerID=40&md5=25e4030b670c8b51921c1d852f46d37d","We propose two new actor-critic algorithms for reinforcement learning. Both algorithms use local linear regression (LLR) to learn approximations of the functions involved. A crucial feature of the algorithms is that they also learn a process model, and this, in combination with LLR, provides an efficient policy update for faster learning. The first algorithm uses a novel model-based update rule for the actor parameters. The second algorithm does not use an explicit actor but learns a reference model which represents a desired behavior, from which desired control actions can be calculated using the inverse of the learned process model. The two novel methods and a standard actor-critic algorithm are applied to the pendulum swing-up problem, in which the novel methods achieve faster learning than the standard algorithm. © 2012 IEEE.","Actor-critic; inverse model; local linear regression (LLR); machine learning algorithms; reinforcement learning (RL)","Actor critic; Actor-critic algorithm; Control actions; Inverse models; Local linear regression; Model learning; Novel methods; Pendulum swing-up problem; Process model; Reference models; Standard algorithms; Learning algorithms; Learning systems; Lunar surface analysis; Reinforcement learning; Approximation algorithms; algorithm; article; artificial intelligence; automated pattern recognition; computer simulation; decision support system; methodology; theoretical model; Algorithms; Artificial Intelligence; Computer Simulation; Decision Support Techniques; Models, Theoretical; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84861185789
"Procaccia A.D., Reddi S.J., Shah N.","A maximum likelihood approach for selecting sets of alternatives",2012,"Uncertainty in Artificial Intelligence - Proceedings of the 28th Conference, UAI 2012",23,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879753257&partnerID=40&md5=e386f616ac35cd8e700bef59ec52493e","We consider the problem of selecting a subset of alternatives given noisy evaluations of the relative strength of different alternatives. We wish to select a k-subset (for a given k) that provides a maximum likelihood estimate for one of several objectives, e.g., containing the strongest alternative. Although this problem is NP-hard, we show that when the noise level is sufficiently high, intuitive methods provide the optimal solution. We thus generalize classical results about singling out one alternative and identifying the hidden ranking of alternatives by strength. Extensive experiments show that our methods perform well in practical settings.",,"Maximum likelihood approaches; Maximum likelihood estimate; Noise levels; NP-hard; Optimal solutions; Ranking of alternatives; Relative strength; Maximum likelihood estimation; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84879753257
"Brafman R.I., Shani G.","Replanning in domains with partial information and sensing actions",2012,"Journal of Artificial Intelligence Research",23,10.1613/jair.3711,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875158562&doi=10.1613%2fjair.3711&partnerID=40&md5=8d2c2196e2cadf8713b266cbd4df04be","Replanning via determinization is a recent, popular approach for online planning in MDPs. In this paper we adapt this idea to classical, non-stochastic domains with partial information and sensing actions, presenting a new planner: SDR (Sample, Determinize, Replan). At each step we generate a solution plan to a classical planning problem induced by the original problem. We execute this plan as long as it is safe to do so. When this is no longer the case, we replan. The classical planning problem we generate is based on the translation-based approach for conformant planning introduced by Palacios and Geffner. The state of the classical planning problem generated in this approach captures the belief state of the agent in the original problem. Unfortunately, when this method is applied to planning problems with sensing, it yields a non-deterministic planning problem that is typically very large. Our main contribution is the introduction of state sampling techniques for overcoming these two problems. In addition, we introduce a novel, lazy, regressionbased method for querying the agent's belief state during run-time. We provide a comprehensive experimental evaluation of the planner, showing that it scales better than the state-of-the-art CLG planner on existing benchmark problems, but also highlighting its weaknesses with new domains. We also discuss its theoretical guarantees. © 2012 AI Access Foundation.",,"Bench-mark problems; Classical planning; Conformant planning; Experimental evaluation; On-line planning; Partial information; Sampling technique; Theoretical guarantees; Artificial intelligence",Article,Scopus,2-s2.0-84875158562
"Carbajal-Hernández J.J., Sánchez-Fernández L.P., Carrasco-Ochoa J.A., Martínez-Trinidad J.F.","Assessment and prediction of air quality using fuzzy logic and autoregressive models",2012,"Atmospheric Environment",23,10.1016/j.atmosenv.2012.06.004,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863769646&doi=10.1016%2fj.atmosenv.2012.06.004&partnerID=40&md5=ee0938e53b34436a71064b2616c25e09","In recent years, artificial intelligence methods have been used for the treatment of environmental problems. This work, presents two models for assessment and prediction of air quality. First, we develop a new computational model for air quality assessment in order to evaluate toxic compounds that can harm sensitive people in urban areas, affecting their normal activities. In this model we propose to use a Sigma operator to statistically asses air quality parameters using their historical data information and determining their negative impact in air quality based on toxicity limits, frequency average and deviations of toxicological tests. We also introduce a fuzzy inference system to perform parameter classification using a reasoning process and integrating them in an air quality index describing the pollution levels in five stages: excellent, good, regular, bad and danger, respectively. The second model proposed in this work predicts air quality concentrations using an autoregressive model, providing a predicted air quality index based on the fuzzy inference system previously developed. Using data from Mexico City Atmospheric Monitoring System, we perform a comparison among air quality indices developed for environmental agencies and similar models. Our results show that our models are an appropriate tool for assessing site pollution and for providing guidance to improve contingency actions in urban areas. © 2012 Elsevier Ltd.","Air quality assessment; Artificial intelligence; Pattern processing; Prediction","Air quality assessment; Air quality parameters; Artificial intelligence methods; Atmospheric monitoring systems; Auto regressive models; Computational model; Environmental agency; Environmental problems; Fuzzy inference systems; Historical data; Mexico City; Pollution level; Quality indices; Reasoning process; Similar models; Toxic compounds; Toxicological tests; Urban areas; Artificial intelligence; Fish; Forecasting; Fuzzy logic; Fuzzy systems; Air quality; air quality; artificial intelligence; assessment method; atmospheric pollution; climate prediction; fuzzy mathematics; regression analysis; air analysis; air monitoring; air pollution; air quality; article; autoregressive model; calculation; fuzzy logic; mathematical model; parameters; performance; prediction; priority journal; statistical analysis; urban area; Mexico [North America]; Equus asinus",Article,Scopus,2-s2.0-84863769646
"Xue J., Wu Y., Shi Y., Cheng S.","Brain storm optimization algorithm for multi-objective optimization problems",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",23,10.1007/978-3-642-30976-2_62,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875116741&doi=10.1007%2f978-3-642-30976-2_62&partnerID=40&md5=9aeab86689fafe45f01c08c08f8a087d","In this paper, a novel multi-objective optimization algorithm based on the brainstorming process is proposed(MOBSO). In addition to the operations used in the traditional multi-objective optimization algorithm, a clustering strategy is adopted in the objective space. Two typical mutation operators, Gaussian mutation and Cauchy mutation, are utilized in the generation process independently and their performances are compared. A group of multi-objective problems with different characteristics were tested to validate the effectiveness of the proposed algorithm. Experimental results show that MOBSO is a very promising algorithm for solving multi-objective optimization problems. © 2012 Springer-Verlag.","Brain Strom Algorithm; Clustering Strategy; Multi-objective Optimization; Mutation Operator","Algorithm for solving; Clustering strategy; Gaussian mutation; Generation process; Multi-objective optimization problem; Multi-objective problem; Mutation operators; Optimization algorithms; Artificial intelligence; Multiobjective optimization; Algorithms",Conference Paper,Scopus,2-s2.0-84875116741
"Natarajan P., Krishnan N., Kenkre N.S., Nancy S., Singh B.P.","Tumor detection using threshold operation in MRI brain images",2012,"2012 IEEE International Conference on Computational Intelligence and Computing Research, ICCIC 2012",23,10.1109/ICCIC.2012.6510299,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877945965&doi=10.1109%2fICCIC.2012.6510299&partnerID=40&md5=206f457def8fc666d7b6bba63e9194df","Medical Image Processing is a complex and challenging field nowadays. Processing of MRI images is one of the parts of this field. This paper proposes a strategy for efficient detection of a brain tumor in MRI brain images. The methodology consists of the following steps: preprocessing by using sharpening and median filters, enhancement of image is performed by histogram equalization, segmentation of the image is performed by thresholding. This approach is then followed by the further application of morphological operations. Finally the tumor region can be obtained by using the technique of image subtraction. © 2012 IEEE.","Filter; Image Subtraction; Morphology; Threshold; Tumor Identification","Efficient detection; Filter; Histogram equalizations; Image subtraction; Morphological operations; Threshold; Threshold operation; Tumor detection; Artificial intelligence; Mathematical morphology; Morphology; Neuroimaging; Tumors; Image segmentation",Conference Paper,Scopus,2-s2.0-84877945965
"Kozan E., Liu S.Q.","A demand-responsive decision support system for coal transportation",2012,"Decision Support Systems",23,10.1016/j.dss.2012.08.012,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868636309&doi=10.1016%2fj.dss.2012.08.012&partnerID=40&md5=6a8a14aff47cdf257bf24b1aa7af3522","In this paper, a demand-responsive decision support system is proposed by integrating the operations of coal shipment, coal stockpiles and coal railing within a whole system. A generic and flexible scheduling optimisation methodology is developed to identify, represent, model, solve and analyse the coal transport problem in a standard and convenient way. As a result, the integrated train-stockpile-ship timetable is created and optimised for improving overall efficiency of coal transport system. A comprehensive sensitivity analysis based on extensive computational experiments is conducted to validate the proposed methodology. The mathematical proposition and proof are concluded as technical and insightful advices for industry practice. The proposed methodology provides better decision making on how to assign rail rolling-stocks and upgrade infrastructure in order to significantly improve capacity utilisation with the best resource-effectiveness ratio. The proposed decision support system with train-stockpile-ship scheduling optimisation techniques is promising to be applied in railway or mining industry, especially as a useful quantitative decision making tool on how to use more current rolling-stocks or whether to buy additional rolling-stocks for mining transportation. © 2012 Elsevier B.V. All rights reserved.","Coal shipment; Coal stockpiles; Coal train scheduling; Decision support system; Mine transportation","Computational experiment; Flexible scheduling; Industry practices; Optimisations; Overall efficiency; Quantitative decision; Train scheduling; Transport problems; Transport systems; Artificial intelligence; Coal; Coal industry; Coal transportation; Decision making; Decision support systems; Mine transportation; Optimization; Scheduling; Ships; Commerce",Article,Scopus,2-s2.0-84868636309
"Nahar J., Imam T., Tickle K.S., Shawkat Ali A.B.M., Chen Y.-P.P.","Computational intelligence for microarray data and biomedical image analysis for the early diagnosis of breast cancer",2012,"Expert Systems with Applications",23,10.1016/j.eswa.2012.04.045,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864452005&doi=10.1016%2fj.eswa.2012.04.045&partnerID=40&md5=8981767a52633e85792b40e36380951d","The objective of this paper was to perform a comparative analysis of the computational intelligence algorithms to identify breast cancer in its early stages. Two types of data representations were considered: microarray based and medical imaging based. In contrast to previous researches, this research also considered the imbalanced nature of these data. It was observed that the SMO algorithm performed better for the majority of the test data, especially for microarray based data when accuracy was used as performance measure. Considering the imbalanced characteristic of the data, the Naive Bayes algorithm was seen to perform highly in terms of true positive rate (TPR). Regarding the influence of SMOTE, a well-known imbalanced data classification technique, it was observed that there was a notable performance improvement for J48, while the performance of SMO remained comparable for the majority of the datasets. Overall, the results indicated SMO as the most potential candidate for the microarray and image dataset considered in this research. © 2012 Elsevier Ltd. All rights reserved.","Breast cancer; Computational intelligence; Image data; Microarray data; SMO; SMOTE","Breast Cancer; Image data; Microarray data; SMO; SMOTE; Algorithms; Artificial intelligence; Classification (of information); Data processing; Diseases; Research; Medical imaging",Article,Scopus,2-s2.0-84864452005
"Sun X., Liu Y., Li J., Zhu J., Liu X., Chen H.","Using cooperative game theory to optimize the feature selection problem",2012,"Neurocomputing",23,10.1016/j.neucom.2012.05.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865328053&doi=10.1016%2fj.neucom.2012.05.001&partnerID=40&md5=1412e3f50f80189a0178ca04335578c7","Feature selection is an important preprocessing step in machine learning and pattern recognition. Recent years, various information theoretic based measurements have been proposed to remove redundant and irrelevant features from high-dimensional data set as many as possible. One of the main disadvantages of existing filter feature selection methods is that they often ignore some features which have strong discriminatory power as a group but are weak as individuals. In this work, we propose a new framework for feature evaluation and weighting to optimize the performance of feature selection. The framework first introduces a cooperative game theoretic method based on Shapley value to evaluate the weight of each feature according to its influence to the intricate and intrinsic interrelation among features, and then provides the weighted features to feature selection algorithm. We also present a flexible feature selection scheme to employ any information criterion to our framework. To verify the effectiveness of our method, experimental comparisons on a set of UCI data sets are carried out using two typical classifiers. The results show that the proposed method achieves promising improvement on feature selection and classification accuracy. © 2012 Elsevier B.V.","Feature selection; Filter method; Machine learning; Shapley value","Cooperative game; Cooperative game theory; Data sets; Discriminatory power; Experimental comparison; Feature evaluation; Feature selection algorithm; Feature selection and classification; Feature selection methods; Feature selection problem; Filter method; High dimensional data; Information criterion; Pre-processing step; Shapley value; Weighted features; Game theory; Information theory; Learning systems; Optimization; Feature extraction; accuracy; article; artificial intelligence; classification algorithm; conceptual framework; feature selection; information processing; machine learning; mathematical computing; prediction; priority journal; probability; process optimization; theory validation",Article,Scopus,2-s2.0-84865328053
"Krause S., Li H., Uszkoreit H., Xu F.","Large-scale learning of relation-extraction rules with distant supervision from the web",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",23,10.1007/978-3-642-35176-1-17,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868536333&doi=10.1007%2f978-3-642-35176-1-17&partnerID=40&md5=266d555b1b5f98ed5002c1a476515fee","We present a large-scale relation extraction (RE) system which learns grammar-based RE rules from the Web by utilizing large numbers of relation instances as seed. Our goal is to obtain rule sets large enough to cover the actual range of linguistic variation, thus tackling the long-tail problem of real-world applications. A variant of distant supervision learns several relations in parallel, enabling a new method of rule filtering. The system detects both binary and n-ary relations. We target 39 relations from Freebase, for which 3M sentences extracted from 20M web pages serve as the basis for learning an average of 40K distinctive rules per relation. Employing an efficient dependency parser, the average run time for each relation is only 19 hours. We compare these rules with ones learned from local corpora of different sizes and demonstrate that the Web is indeed needed for a good coverage of linguistic variation. © 2012 Springer-Verlag Berlin Heidelberg.","distant supervision; Freebase; IE; information extraction; RE; relation extraction; rule based RE; web scale IE","distant supervision; Freebase; IE; Information Extraction; Relation extraction; Rule based; web scale IE; Artificial intelligence; Rhenium; Linguistics",Conference Paper,Scopus,2-s2.0-84868536333
"Kolb J., Kammerer K., Reichert M.","Updatable process views for user-centered adaption of large process models",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",23,10.1007/978-3-642-34321-6-32,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868350499&doi=10.1007%2f978-3-642-34321-6-32&partnerID=40&md5=4c48e28c3c09a809f8894386dd750ed4","The increasing adoption of process-aware information systems (PAISs) has resulted in large process model collections. To support users having different perspectives on these processes and related data, a PAIS should provide personalized views on process models. Existing PAISs, however, do not provide mechanisms for creating or even changing such process views. Especially, changing process models is a frequent use case in PAISs due to changing needs or unplanned situations. While process views have been used as abstractions for visualizing large process models, no work exists on how to change process models based on respective views. This paper presents an approach for changing large process models through updates of corresponding process views, while ensuring up-to-dateness and consistency of all other process views on the process model changed. Respective update operations can be applied to a process view and corresponding changes be correctly propagated to the underlying process model. Furthermore, all other views related to this process model are then migrated to the new version of the process model as well. Overall, our view framework enables domain experts to evolve large process models over time based on appropriate model abstractions. © Springer-Verlag Berlin Heidelberg 2012.",,"Appropriate models; Change process; Domain experts; Personalized views; Process model; Process view; Process-aware information systems; User-centered; Artificial intelligence; Abstracting",Conference Paper,Scopus,2-s2.0-84868350499
"Amyot D., Shamsaei A., Kealey J., Tremblay E., Miga A., Mussbacher G., Alhaj M., Tawhid R., Braun E., Cartwright N.","Towards advanced goal model analysis with jUCMNav",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",23,10.1007/978-3-642-33999-8_25,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868316933&doi=10.1007%2f978-3-642-33999-8_25&partnerID=40&md5=744ac292468da557626038c6f16a2fe0","Goal modeling is an important part of various types of activities such as requirements engineering, business management, and compliance assessment. The Goal-oriented Requirement Language is a standard and mature goal modeling language supported by the jUCMNav tool. However, recent applications of GRL to a regulatory context highlighted several analysis issues and limitations whose resolutions are urgent, and also likely applicable to other languages and tools. This paper investigates issues related to the computation of strategy and model differences, the management of complexity and uncertainty, sensitivity analysis, and various domain-specific considerations. For each, a solution is proposed, implemented in jUCMNav, and illustrated through simple examples. These solutions greatly increase the analysis capabilities of GRL and jUCMNav in order to handle real problems. © 2012 Springer-Verlag.","Analysis; Goal-oriented Requirement Language; jUCMNav; strategies; tool support; User Requirements Notation; visualization","Analysis; Goal-oriented requirement language; jUCMNav; strategies; Tool support; User requirements notation; Artificial intelligence; Flow visualization; Data mining",Conference Paper,Scopus,2-s2.0-84868316933
"Witkowski J., Parkes D.C.","A robust Bayesian truth serum for small populations",2012,"Proceedings of the National Conference on Artificial Intelligence",23,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868274369&partnerID=40&md5=76bf2e4aa55fdacda7b168eae1406b2d","Peer prediction mechanisms allow the truthful elicitation of private signals (e.g., experiences, or opinions) in regard to a true world state when this ground truth is unobservable. The original peer prediction method is incentive compatible for any number of agents n ≥ 2, but relies on a common prior, shared by all agents and the mechanism. The Bayesian Truth Serum (BTS) relaxes this assumption. While BTS still assumes that agents share a common prior, this prior need not be known to the mechanism. However, BTS is only incentive compatible for a large enough number of agents, and the particular number of agents required is uncertain because it depends on this private prior. In this paper, we present a robust BTS for the elicitation of binary information which is incentive compatible for every n ≥ 3, taking advantage of a particularity of the quadratic scoring rule. The robust BTS is the first peer prediction mechanism to provide strict incentive compatibility for every n ≥ 3 without relying on knowledge of the common prior. Moreover, and in contrast to the original BTS, our mechanism is numerically robust and ex post individually rational. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Binary information; Ground truth; Incentive compatibility; Incentive compatible; Numerically robust; Prediction methods; Scoring rules; Small population; Unobservable; Body fluids; Forecasting; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868274369
"Hähnle R., Schaefer I.","A Liskov principle for delta-oriented programming",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",23,10.1007/978-3-642-34026-0_4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868268397&doi=10.1007%2f978-3-642-34026-0_4&partnerID=40&md5=d7a7fe6adc8925f3d2379275cded8295","In formal verification of software product families one not only analyses programs, but must act on the artifacts and components which are reused to obtain software products. As the number of products is exponential in the number of artifacts, it is crucial to perform verification in a modular way. When code reuse is based on class inheritance in OO programming, Liskov's principle is a standard device to achieve modular verification. Software families, however, employ other variability modeling techniques than inheritance. Delta-oriented programming is an approach to implement a family of programs where code reuse is achieved via gradual transformation of a core program. We define a Liskov principle for delta-oriented programming and show that it achieves modular verification of software families developed in that paradigm. © 2012 Springer-Verlag.",,"Class inheritance; Code reuse; Formal verifications; Gradual transformations; Modular verification; Software family; Software product family; Software products; Variability modeling; Artificial intelligence; Verification",Conference Paper,Scopus,2-s2.0-84868268397
"Ni B., Moulin P., Yan S.","Order-preserving sparse coding for sequence classification",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",23,10.1007/978-3-642-33709-3_13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867875912&doi=10.1007%2f978-3-642-33709-3_13&partnerID=40&md5=d783bc98bb744f6987ca68f2ed13dc4e","In this paper, we investigate order-preserving sparse coding for classifying multi-dimensional sequence data. Such a problem is often tackled by first decomposing the input sequence into individual frames and extracting features, then performing sparse coding or other processing for each frame based feature vector independently, and finally aggregating individual responses to classify the input sequence. However, this heuristic approach ignores the underlying temporal order of the input sequence frames, which in turn results in suboptimal discriminative capability. In this work, we introduce a temporal-order-preserving regularizer which aims to preserve the temporal order of the reconstruction coefficients. An efficient Nesterov-type smooth approximation method is developed for optimization of the new regularization criterion, with guaranteed error bounds. Extensive experiments for time series classification on a synthetic dataset, several machine learning benchmarks, and a challenging real-world RGB-D human activity dataset, show that the proposed coding scheme is discriminative and robust, and it outperforms previous art for sequence classification. © 2012 Springer-Verlag.",,"Coding scheme; Data sets; Error bound; Extracting features; Feature vectors; Heuristic approach; Human activities; Input sequence; Regularizer; Sequence classification; Sequence data; Smooth approximation; Sparse coding; Temporal order; Time series classifications; Extracting features; Guaranteed error bounds; Heuristic approach; Multi dimensional; Order preserving; Sequence classification; Smooth approximation; Time series classifications; Approximation theory; Classification (of information); Error analysis; Heuristic methods; Artificial intelligence; Codes (symbols); Computer vision; Error analysis; Heuristic methods; Learning systems; Computer vision; Classification (of information)",Conference Paper,Scopus,2-s2.0-84867875912
"Soria-Comas J., Domingo-Ferrer J.","Probabilistic k-anonymity through microaggregation and data swapping",2012,"IEEE International Conference on Fuzzy Systems",23,10.1109/FUZZ-IEEE.2012.6251280,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867596158&doi=10.1109%2fFUZZ-IEEE.2012.6251280&partnerID=40&md5=4928175279408388f6befdf2f471f319","k-Anonymity is a privacy property used to limit the risk of re-identification in a microdata set. A data set satisfying k-anonymity consists of groups of k records which are indistinguishable as far as their quasi-identifier attributes are concerned. Hence, the probability of re-identifying a record within a group is 1/k. We introduce the probabilistic k-anonymity property, which relaxes the indistinguishability requirement of k-anonymity and only requires that the probability of re-identification be the same as in k-anonymity. Two computational heuristics to achieve probabilistic k-anonymity based on data swapping are proposed: MDAV microaggregation on the quasi-identifiers plus swapping, and individual ranking microaggregation on individual confidential attributes plus swapping. We report experimental results, where we compare the utility of original, k-anonymous and probabilistically k-anonymous data. © 2012 IEEE.","anonymization; clustering; Computational intelligence; differential privacy; k-anonymity; microaggregation; statistical disclosure control; swapping","Anonymization; clustering; Differential privacies; K-Anonymity; Microaggregation; Statistical disclosure Control; swapping; Artificial intelligence; Fuzzy systems",Conference Paper,Scopus,2-s2.0-84867596158
"Massanet S., Torrens J.","On the characterization of Yager's implications",2012,"Information Sciences",23,10.1016/j.ins.2012.03.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860265598&doi=10.1016%2fj.ins.2012.03.008&partnerID=40&md5=0701324098b0e7a852632f9c36bbf247","In this paper, characterizations of Yager's f- and g-implications are presented. Since their introduction in 2004, the properties of these implications have been studied in detail but they have not been characterized yet. The characterizations given here are based on the law of importation, a functional equation that has been extensively studied. Moreover, particular characterizations of Reichenbach, Yager and Goguen implications are derived from these results. © 2012 Elsevier Inc. All rights reserved.","f-Implication; g-Implication; Implication function; Law of importation; Yager's implications","f-Implication; g-Implication; Implication functions; Law of importation; Yager's implications; Artificial intelligence; Software engineering; Characterization",Article,Scopus,2-s2.0-84860265598
"Châari I., Koubâa A., Bennaceur H., Trigui S., Al-Shalfan K.","smartPATH: A hybrid ACO-GA algorithm for robot path planning",2012,"2012 IEEE Congress on Evolutionary Computation, CEC 2012",23,10.1109/CEC.2012.6256142,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866877392&doi=10.1109%2fCEC.2012.6256142&partnerID=40&md5=7cc7ca08b862ddcf5c06dbc245f763cf","Path planning is a critical combinatorial problem essential for the navigation of a mobile robot. Several research initiatives, aiming at providing optimized solutions to this problem, have emerged. Ant Colony Optimization (ACO) and Genetic Algorithms (GA) are the two most widely used heuristics that have shown their effectiveness in solving such a problem. This paper presents, smartPATH, a new hybrid ACO-GA algorithm to solve the global robot path planning problem. The algorithm consists of a combination of an improved ACO algorithm (IACO) for efficient and fast path selection, and a modified crossover operator for avoiding falling into a local minimum. Our system model incorporates a Wireless Sensor Network (WSN) infrastructure to support the robot navigation, where sensor nodes are used as signposts that help locating the mobile robot, and guide it towards the target location. We found out smartPATH outperforms classical ACO (CACO) and GA algorithms (as defined in the literature without modification) for solving the path planning problem both and Bellman-Ford shortest path method. We demonstrate also that smartPATH reduces the execution time up to 64.9% in comparison with Bellman-Ford exact method and improves the solution quality up to 48.3% in comparison with CACO. © 2012 IEEE.",,"ACO algorithms; Ant Colony Optimization (ACO); Bellman-Ford; Combinatorial problem; Crossover operator; Exact methods; Execution time; Fast path; GA algorithm; Local minimums; Optimized solutions; Path planning problems; Research initiatives; Robot navigation; Robot path-planning; Shortest path method; Solution quality; System models; Target location; Artificial intelligence; Mobile robots; Motion planning; Sensor nodes; Genetic algorithms",Conference Paper,Scopus,2-s2.0-84866877392
"Ginsburg S.B., Lynch D.A., Bowler R.P., Schroeder J.D.","Automated Texture-based Quantification of Centrilobular Nodularity and Centrilobular Emphysema in Chest CT Images",2012,"Academic Radiology",23,10.1016/j.acra.2012.04.020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865767060&doi=10.1016%2fj.acra.2012.04.020&partnerID=40&md5=8851b9a3af96a607ddb7b1c5e9406dba","Rationale and Objectives: Characterization of smoking-related lung disease typically consists of visual assessment of chest computed tomographic (CT) images for the presence and extent of emphysema and centrilobular nodularity (CN). Quantitative analysis of emphysema and CN may improve the accuracy, reproducibility, and efficiency of chest CT scoring. The purpose of this study was to develop a fully automated texture-based system for the detection and quantification of centrilobular emphysema (CLE) and CN in chest CT images. Materials and Methods: A novel approach was used to prepare regions of interest (ROIs) within the lung parenchyma for representation by texture features associated with the gray-level run-length and gray-level gap-length methods. These texture features were used to train a multiple logistic regression classifier to discriminate between normal lung tissue, CN or "" smoker's lung,"" and CLE. This classifier was trained and evaluated on 24 and 71 chest CT scans, respectively. Results: During training, the classifier correctly classified 89% of ROIs depicting normal lung tissue, 74% of ROIs depicting CN, and 95% of ROIs manifesting CLE. When the performance of the classifier in quantifying extent of CN and CLE was evaluated on 71 chest CT scans, 65% of ROIs in smokers without CLE were classified as CN, compared to 31% in nonsmokers (P < .001) and 28% in smokers with CLE (P < .001). Conclusions: The texture-based framework described herein facilitates successful discrimination among normal lung tissue, CN, and CLE and can be used for the automated quantification of smoking-related lung disease. © 2012 AUR.","Centrilobular nodularity; Computer-aided diagnosis; Emphysema; Texture analysis","adult; aged; article; automated texture based system; automation; centrilobular nodularity; clinical article; cohort analysis; computer assisted tomography; controlled study; cross-sectional study; female; human; image analysis; lung emphysema; lung parenchyma; lymphadenopathy; male; priority journal; smoking; training; Algorithms; Artificial Intelligence; Female; Humans; Male; Middle Aged; Pattern Recognition, Automated; Pulmonary Emphysema; Radiographic Image Enhancement; Radiographic Image Interpretation, Computer-Assisted; Radiography, Thoracic; Reproducibility of Results; Sensitivity and Specificity; Tomography, X-Ray Computed",Article,Scopus,2-s2.0-84865767060
"Charguéraud A.","The locally nameless representation",2012,"Journal of Automated Reasoning",23,10.1007/s10817-011-9225-2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868536684&doi=10.1007%2fs10817-011-9225-2&partnerID=40&md5=b6770a29d08b58cacec3cda7a41bab07","This paper provides an introduction to the locally nameless approach to the representation of syntax with variable binding, focusing in particular on the use of this technique in formal proofs. First, we explain the benefits of representing bound variables with de Bruijn indices while retaining names for free variables. Then, we explain how to describe and manipulate syntax in that form, and show how to define and reason about judgments on locally nameless terms. © Springer Science+Business Media B.V. 2011.","Binders; Cofinite quantification; Formal proofs; Locally nameless; Metatheory","Bound variables; Cofinite quantification; De-Bruijn indices; Formal proofs; Free variable; Locally nameless; Meta-theory; Variable binding; Artificial intelligence; Automata theory; Binders; Software engineering; Syntactics",Article,Scopus,2-s2.0-84868536684
"Gomez-Romero J., Serrano M.A., Patricio M.A., Garcia J., Molina J.M.","Context-based scene recognition from visual data in smart homes: An Information Fusion approach",2012,"Personal and Ubiquitous Computing",23,10.1007/s00779-011-0450-9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869125742&doi=10.1007%2fs00779-011-0450-9&partnerID=40&md5=7693c736c60b8e24b202fba6ce1ac43e","Ambient Intelligence (AmI) aims at the development of computational systems that process data acquired by sensors embedded in the environment to support users in everyday tasks. Visual sensors, however, have been scarcely used in this kind of applications, even though they provide very valuable information about scene objects: position, speed, color, texture, etc. In this paper, we propose a cognitive framework for the implementation of AmI applications based on visual sensor networks. The framework, inspired by the Information Fusion paradigm, combines a priori context knowledge represented with ontologies with real time single camera data to support logic-based high-level local interpretation of the current situation. In addition, the system is able to automatically generate feedback recommendations to adjust data acquisition procedures. Information about recognized situations is eventually collected by a central node to obtain an overall description of the scene and consequently trigger AmI services. We show the extensible and adaptable nature of the approach with a prototype system in a smart home scenario. © Springer-Verlag London Limited 2011.","Ambient intelligence; Computer vision; Context; Data and information fusion; Ontologies","Ambient intelligence; Cognitive frameworks; Computational system; Context; Context-based; Current situation; Data and information; Local interpretation; Process data; Prototype system; Real time; Scene object; Scene recognition; Single cameras; Smart homes; Visual data; Visual sensor; Visual sensor networks; Artificial intelligence; Automation; Computer vision; Information fusion; Intelligent buildings; Ontology; Sensor networks; Digital storage",Article,Scopus,2-s2.0-84869125742
"Demangeon R., Honda K.","Nested protocols in session types",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",23,10.1007/978-3-642-32940-1_20,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866703131&doi=10.1007%2f978-3-642-32940-1_20&partnerID=40&md5=4c439a98fc0b5edbfde43e2200c75ab7","We propose an improvement to session-types, introducing nested protocols, the possibility to call a subprotocol from a parent protocol. This feature adds expressiveness and modularity to the existing session-type theory, allowing arguments to be passed and enabling higher-order protocols definition. Our theory is introduced through a new type system for protocols handling subprotocol calls, and its implementation in a session-calculus. We propose validation and satisfaction relations between specification and implementation. Sound behaviour is enforced thanks to the usage of kinds and well-formedness, allowing us to ensure progress and subject reduction. In addition, we describe an extension of our framework allowing subprotocols to send back results. © 2012 Springer-Verlag.",,"Satisfaction relation; Session types; Subject reduction; Subprotocols; Type systems; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84866703131
"Larios D.F., Barbancho J., RodríGuez G., Sevillano J.L., Molina F.J., LeóN C.","Energy efficient wireless sensor network communications based on computational intelligent data fusion for environmental monitoring",2012,"IET Communications",23,10.1049/iet-com.2011.0809,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868660731&doi=10.1049%2fiet-com.2011.0809&partnerID=40&md5=3f078772b0e1734a6b153b54dd914194","The study presents a novel computational intelligence algorithm designed to optimise energy consumption in an environmental monitoring process: specifically, water level measurements in flooded areas. This algorithm aims to obtain a trade-off between accuracy and power consumption. The implementation constitutes a data aggregation and fusion in itself. A harsh environment can make the direct measurement of flood levels a difficult task. This study proposes a flood level estimation, inferred through the measurement of other common environmental variables. The benefit of this algorithm is tested both with simulations and real experiments conducted in Doñana, a national park in southern Spain where flood level measurements have traditionally been done manually. © 2012 The Institution of Engineering and Technology.",,"Data aggregation; Direct measurement; Energy efficient; Environmental Monitoring; Environmental variables; Flooded areas; Harsh environment; Intelligent data; National parks; Southern Spain; Algorithms; Artificial intelligence; Data fusion; Energy efficiency; Energy utilization; Environmental engineering; Level measurement; Water levels; Wireless sensor networks; Floods",Article,Scopus,2-s2.0-84868660731
"Tsompanas M.-A.I., Sirakoulis G.C.","Modeling and hardware implementation of an amoeba-like cellular automaton",2012,"Bioinspiration and Biomimetics",23,10.1088/1748-3182/7/3/036013,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863634889&doi=10.1088%2f1748-3182%2f7%2f3%2f036013&partnerID=40&md5=20a011c72355d5f8144bca0df2f865e1","Over the last few years, an increasing number of publications has shown that living organisms are very effective in finding solutions to complex mathematical problems which usually demand large computation resources. The plasmodium of the slime mould Physarum polycephalum is a successful example that has been used to solve path-finding problems on graphs and combinatorial problems. Cellular automata (CAs) computational model can capture the essential features of systems in which global behavior emerges from the collective effect of simple components, which interact locally (emergent computation). We developed a CA that models exactly the Physarum's behavior and applied it in finding the minimum-length path between two points in a labyrinth, as well as in solving a path-planning problem by guiding the development of adaptive networks, as in the case of the actual rail network of Tokyo. The CA results are in very good agreement with the computation results produced by the living organism experiments in both cases. Moreover, our CA hardware implementation results in faster and more effective computation performance, because of its inherent parallel nature. Consequently, our CA, implemented both in software and hardware, can serve as a powerful and low-cost virtual laboratory that models the slime mould Physarum's computation behavior. © 2012 IOP Publishing Ltd.",,"Amoeba (genus); Physarum; Physarum polycephalum; biomimetic material; article; artificial intelligence; biological model; biomimetics; cell motion; computer aided design; computer simulation; equipment design; equipment failure analysis; instrumentation; Physarum polycephalum; physiology; robotics; Artificial Intelligence; Biomimetic Materials; Biomimetics; Cell Movement; Computer Simulation; Computer-Aided Design; Equipment Design; Equipment Failure Analysis; Models, Biological; Physarum polycephalum; Robotics",Article,Scopus,2-s2.0-84863634889
"Marqués A.I., García V., Sánchez J.S.","Exploring the behaviour of base classifiers in credit scoring ensembles",2012,"Expert Systems with Applications",23,10.1016/j.eswa.2012.02.092,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859423392&doi=10.1016%2fj.eswa.2012.02.092&partnerID=40&md5=3edca6c1a5e013d447d3b78b4155b8ee","Many techniques have been proposed for credit risk assessment, from statistical models to artificial intelligence methods. During the last few years, different approaches to classifier ensembles have successfully been applied to credit scoring problems, demonstrating to be more accurate than single prediction models. However, it is still a question what base classifiers should be employed in each ensemble in order to achieve the highest performance. Accordingly, the present paper evaluates the performance of seven individual prediction techniques when used as members of five different ensemble methods. The ultimate aim of this study is to suggest appropriate classifiers for each ensemble approach in the context of credit scoring. The experimental results and statistical tests show that the C4.5 decision tree constitutes the best solution for most ensemble methods, closely followed by the multilayer perceptron neural network and logistic regression, whereas the nearest neighbour and the naive Bayes classifiers appear to be significantly the worst. © 2012 Elsevier Ltd. All rights reserved.","Classifier ensemble; Credit scoring; Finance","Artificial intelligence methods; Base classifiers; Classifier ensembles; Credit risk assessment; Credit scoring; Ensemble methods; Individual prediction; Logistic regressions; Multi-layer perceptron neural networks; Naive Bayes classifiers; Nearest neighbour; Prediction model; Statistical models; Artificial intelligence; Finance; Logistics; Mathematical models; Risk assessment; Statistical tests; Decision trees",Article,Scopus,2-s2.0-84859423392
"Hadas Z., Kurfurst J., Ondrusek C., Singule V.","Artificial intelligence based optimization for vibration energy harvesting applications",2012,"Microsystem Technologies",23,10.1007/s00542-012-1432-1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864602125&doi=10.1007%2fs00542-012-1432-1&partnerID=40&md5=4065ad226f49e8a40af1f14c7ed313cf","This paper deals with optimization studies based on artificial intelligence methods. These modern optimization methods can be very useful for design improving of an electromagnetic vibration energy harvester. The vibration energy harvester is a complex mechatronic device which harvests electrical energy from ambient mechanical vibrations. The harvester design consists of a precise mechanical resonator, electromagnetic converter and electronics. The optimization study of such complex mechatronic device is complicated however artificial intelligence methods can be used for set up of optimal harvester parameters. Used optimization strategies are applied to optimize the design of the electro-magnetic vibration energy harvester according to multi-objective fitness functions. Optimization results of the harvester are summarized in this paper. Presented optimization algorithms can be used for a design of new energy harvesting systems or for improving on existing energy harvesting systems. © Springer-Verlag 2012.",,"Artificial intelligence methods; Electrical energy; Electromagnetic converters; Electromagnetic vibrations; Energy harvesting systems; Existing energies; Mechanical resonators; Mechatronic devices; Multi-objective fitness function; Optimization algorithms; Optimization method; Optimization strategy; Optimization studies; Vibration energies; Vibration energy harvesting; Algorithms; Artificial intelligence; Design; Electromagnetism; Energy harvesting; Vibrations (mechanical); Multiobjective optimization",Conference Paper,Scopus,2-s2.0-84864602125
"Stramandinoli F., Marocco D., Cangelosi A.","The grounding of higher order concepts in action and language: A cognitive robotics model",2012,"Neural Networks",23,10.1016/j.neunet.2012.02.012,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861766092&doi=10.1016%2fj.neunet.2012.02.012&partnerID=40&md5=0b4b9286098e9976b61cc71d3a556f3f","In this paper we present a neuro-robotic model that uses artificial neural networks for investigating the relations between the development of symbol manipulation capabilities and of sensorimotor knowledge in the humanoid robot iCub. We describe a cognitive robotics model in which the linguistic input provided by the experimenter guides the autonomous organization of the robot's knowledge. In this model, sequences of linguistic inputs lead to the development of higher-order concepts grounded on basic concepts and actions. In particular, we show that higher-order symbolic representations can be indirectly grounded in action primitives directly grounded in sensorimotor experiences. The use of recurrent neural network also permits the learning of higher-order concepts based on temporal sequences of action primitives. Hence, the meaning of a higher-order concept is obtained through the combination of basic sensorimotor knowledge. We argue that such a hierarchical organization of concepts can be a possible account for the acquisition of abstract words in cognitive robots. © 2012 Elsevier Ltd.","Abstract language; Computational modelling; Developmental cognitive robotics; Recurrent neural network; Symbol grounding","Abstract languages; Abstract word; Basic concepts; Cognitive robotics; Cognitive robots; Computational modelling; Hierarchical organizations; Higher order; Humanoid robot; Symbol grounding; Symbol manipulation; Symbolic representation; Temporal sequences; Anthropomorphic robots; Linguistics; Recurrent neural networks; Robotics; article; artificial intelligence; artificial neural network; back propagation; computer simulation; controlled study; human computer interaction; language ability; learning algorithm; mathematical computing; mathematical model; positive feedback; priority journal; probability; robotics; sensorimotor function; social interaction; systematic error; Algorithms; Artificial Intelligence; Cognition; Computer Simulation; Language; Models, Theoretical; Neural Networks (Computer); Robotics; Software",Article,Scopus,2-s2.0-84861766092
"Kapp M.N., Sabourin R., Maupin P.","A dynamic model selection strategy for support vector machine classifiers",2012,"Applied Soft Computing Journal",23,10.1016/j.asoc.2012.04.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861845640&doi=10.1016%2fj.asoc.2012.04.001&partnerID=40&md5=849a06643bcff44528d0b52527dad38f","The Support Vector Machine (SVM) is a very powerful technique for general pattern recognition purposes but its efficiency in practice relies on the optimal selection of hyper-parameters. A naïve or ad hoc choice of values for these can lead to poor performance in terms of generalization error and high complexity of the parameterized models obtained in terms of the number of support vectors identified. The task of searching for optimal hyper-parameters with respect to the aforementioned performance measures is the so-called SVM model selection problem. In this paper we propose a strategy to select optimal SVM models in a dynamic fashion in order to address this problem when knowledge about the environment is updated with new observations and previously parameterized models need to be re-evaluated, and in some cases discarded in favor of revised models. This strategy combines the power of swarm intelligence theory with the conventional grid search method in order to progressively identify and sort out potential solutions using dynamically updated training datasets. Experimental results demonstrate that the proposed method outperforms the traditional approaches tested against it, while saving considerable computational time. © 2011 Elsevier B.V.","Dynamic optimization; Model selection; Particle Swarm Optimization; Support Vector Machines","Computational time; Dynamic model selections; Dynamic optimization; Generalization Error; Grid-search method; Its efficiencies; Model Selection; Optimal selection; Parameterized model; Performance measure; Poor performance; Potential solutions; Support vector; SVM model; Swarm Intelligence; Training data sets; Artificial intelligence; Optimization; Particle swarm optimization (PSO); Pattern recognition; Support vector machines",Article,Scopus,2-s2.0-84861845640
"Yeganeh B., Motlagh M.S.P., Rashidi Y., Kamalan H.","Prediction of CO concentrations based on a hybrid Partial Least Square and Support Vector Machine model",2012,"Atmospheric Environment",23,10.1016/j.atmosenv.2012.02.092,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859918822&doi=10.1016%2fj.atmosenv.2012.02.092&partnerID=40&md5=bc1e05b5e887548e4676cf67d1b62737","Due to the health impacts caused by exposures to air pollutants in urban areas, monitoring and forecasting of air quality parameters have become popular as an important topic in atmospheric and environmental research today. The knowledge on the dynamics and complexity of air pollutants behavior has made artificial intelligence models as a useful tool for a more accurate pollutant concentration prediction. This paper focuses on an innovative method of daily air pollution prediction using combination of Support Vector Machine (SVM) as predictor and Partial Least Square (PLS) as a data selection tool based on the measured values of CO concentrations.The CO concentrations of Rey monitoring station in the south of Tehran, from Jan. 2007 to Feb. 2011, have been used to test the effectiveness of this method. The hourly CO concentrations have been predicted using the SVM and the hybrid PLS-SVM models. Similarly, daily CO concentrations have been predicted based on the aforementioned four years measured data. Results demonstrated that both models have good prediction ability; however the hybrid PLS-SVM has better accuracy. In the analysis presented in this paper, statistic estimators including relative mean errors, root mean squared errors and the mean absolute relative error have been employed to compare performances of the models. It has been concluded that the errors decrease after size reduction and coefficients of determination increase from 56 to 81% for SVM model to 65-85% for hybrid PLS-SVM model respectively. Also it was found that the hybrid PLS-SVM model required lower computational time than SVM model as expected, hence supporting the more accurate and faster prediction ability of hybrid PLS-SVM model. © 2012 Elsevier Ltd.","CO concentration; Hybrid models; Machine learning; Partial Least Square; Support Vector Machine","Air pollutants; Air quality parameters; CO concentrations; Computational time; Data Selection; Environmental researches; Health impact; Hybrid model; Innovative method; Mean errors; Measured data; Monitoring stations; Partial least square (PLS); Pollutant concentration; Relative errors; Root mean squared errors; Size reductions; Support vector; SVM model; Urban areas; Air quality; Artificial intelligence; Data reduction; Errors; Forecasting; Learning algorithms; Learning systems; Support vector machines; carbon monoxide; air quality; artificial intelligence; atmospheric pollution; carbon monoxide; learning; particle size; prediction; spatiotemporal analysis; urban area; urban atmosphere; accuracy; air pollution; article; measurement error; partial least squares regression; prediction and forecasting; priority journal; support vector machine; Iran; Tehran [Iran]",Article,Scopus,2-s2.0-84859918822
"Tang H., van Walsum T., van Onkelen R.S., Hameeteman R., Klein S., Schaap M., Tori F.L., van den Bouwhuijsen Q.J.A., Witteman J.C.M., van der Lugt A., van Vliet L.J., Niessen W.J.","Semiautomatic carotid lumen segmentation for quantification of lumen geometry in multispectral MRI",2012,"Medical Image Analysis",23,10.1016/j.media.2012.05.014,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866115490&doi=10.1016%2fj.media.2012.05.014&partnerID=40&md5=7f90befa2a61b61ca00768840f021676","Quantitative information about the geometry of the carotid artery bifurcation is relevant for investigating the onset and progression of atherosclerotic disease. This paper proposes an automatic approach for quantifying the carotid bifurcation angle, carotid area ratio, carotid bulb size and the vessel tortuosity from multispectral MRI. First, the internal and external carotid centerlines are determined by finding a minimum cost path between user-defined seed points where the local costs are based on medialness and intensity. The minimum cost path algorithm is iteratively applied after curved multi-planar reformatting to refine the centerline. Second, the carotid lumen is segmented using a topology preserving geodesic active contour which is initialized by the extracted centerlines and steered by the MR intensities. Third, the bifurcation angle and vessel tortuosity are automatically extracted from the segmented lumen. The methods for centerline tracking and lumen segmentation are evaluated by comparing their accuracy to the inter- and intra-observer variability on 48 datasets (96 carotid arteries) acquired as part of a longitudinal population study. The evaluation reveals that 94 of 96 carotid arteries are segmented successfully. The distance between the tracked centerlines and the reference standard (0.33 mm) is similar to the inter-observer variation (0.32 mm). The lumen segmentation accuracy (average DSC = 0.89, average mean absolute surface distance = 0.31. mm) is close to the inter-observer variation (average dice = 0.92, average mean surface distance = 0.23 mm). The correlation coefficient of manually and automaticly derived bifurcation angle, carotid proximal area ratio, carotid proximal bulb size and vessel totuosity quantifications are close to the correlation of these measures between observers. This demonstrates that the automated method can be used for replacing manual centerline annotation and manual contour drawing for lumen segmentation in MRIs data prior to quantifying the carotid bifurcation geometry. © 2012 Elsevier B.V.","Bifurcation geometry; Centerline tracking; CMPR iteration; Lumen segmentation; Topology preserving geodesic active contour","Area ratios; Automated methods; Carotid artery; Carotid artery bifurcation; Carotid bifurcation; Carotid bulbs; Centerlines; CMPR iteration; Correlation coefficient; Data sets; Geodesic Active Contour; Intra-observer variability; Lumen segmentations; Minimum cost; MR intensity; Multi-spectral; Quantitative information; Reference standard; Seed point; Topology preserving; Bifurcation (mathematics); Geodesy; Geometry; Topology; Costs; accuracy; algorithm; article; carotid artery; carotid artery bifurcation; controlled study; geometry; nuclear magnetic resonance imaging; priority journal; Algorithms; Artificial Intelligence; Carotid Arteries; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84866115490
"Abrial J.-R., Su W., Zhu H.","Formalizing hybrid systems with event-B",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",23,10.1007/978-3-642-30885-7_13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863957261&doi=10.1007%2f978-3-642-30885-7_13&partnerID=40&md5=17d9cc2c640f292abd0951ce399300ad","This paper contains the development of hybrid systems in Event-B and the Rodin Platform. It follows the seminal approach introduced at the turn of the century in Action Systems. Many examples illustrate our approach. © 2012 Springer-Verlag.",,"Action systems; Event-B; Artificial intelligence; Hybrid systems",Conference Paper,Scopus,2-s2.0-84863957261
"Jothiprakash V., Magar R.B.","Multi-time-step ahead daily and hourly intermittent reservoir inflow prediction by artificial intelligent techniques using lumped and distributed data",2012,"Journal of Hydrology",23,10.1016/j.jhydrol.2012.04.045,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862673732&doi=10.1016%2fj.jhydrol.2012.04.045&partnerID=40&md5=47973420e8b008ce9c9bf1a10864c06b","In this study, artificial intelligent (AI) techniques such as artificial neural network (ANN), Adaptive neuro-fuzzy inference system (ANFIS) and Linear genetic programming (LGP) are used to predict daily and hourly multi-time-step ahead intermittent reservoir inflow. To illustrate the applicability of AI techniques, intermittent Koyna river watershed in Maharashtra, India is chosen as a case study. Based on the observed daily and hourly rainfall and reservoir inflow various types of time-series, cause-effect and combined models are developed with lumped and distributed input data. Further, the model performance was evaluated using various performance criteria. From the results, it is found that the performances of LGP models are found to be superior to ANN and ANFIS models especially in predicting the peak inflows for both daily and hourly time-step. A detailed comparison of the overall performance indicated that the combined input model (combination of rainfall and inflow) performed better in both lumped and distributed input data modelling. It was observed that the lumped input data models performed slightly better because; apart from reducing the noise in the data, the better techniques and their training approach, appropriate selection of network architecture, required inputs, and also training-testing ratios of the data set. The slight poor performance of distributed data is due to large variations and lesser number of observed values. © 2012 Elsevier B.V..","Artificial intelligent techniques; Cause-effect models; Combined models; Daily and hourly; Lumped and distributed data; Time-series models","Artificial intelligent; Cause-effect; Combined model; Daily and hourly; Distributed data; Time series models; Genetic programming; Input output programs; Network architecture; Neural networks; Rain; Statistical tests; Forecasting; artificial intelligence; artificial neural network; discharge; inflow; linearity; numerical model; rainfall; reservoir; time series; watershed; India; Koyna Basin; Maharashtra",Article,Scopus,2-s2.0-84862673732
"Alviano M., Faber W., Leone N., Manna M.","Disjunctive datalog with existential quantifiers: Semantics, decidability, and complexity issues",2012,"Theory and Practice of Logic Programming",23,10.1017/S1471068412000257,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864974336&doi=10.1017%2fS1471068412000257&partnerID=40&md5=015fe19e8c064d807fe80b77774ab5a5","Abstract Datalog is one of the best-known rule-based languages, and extensions of it are used in a wide context of applications. An important Datalog extension is Disjunctive Datalog, which significantly increases the expressivity of the basic language. Disjunctive Datalog is useful in a wide range of applications, ranging from Databases (e.g., Data Integration) to Artificial Intelligence (e.g., diagnosis and planning under incomplete knowledge). However, in recent years an important shortcoming of Datalog-based languages became evident, e.g. in the context of data-integration (consistent query-answering, ontology-based data access) and Semantic Web applications: The language does not permit any generation of and reasoning with unnamed individuals in an obvious way. In general, it is weak in supporting many cases of existential quantification. To overcome this problem, Datalog3has recently been proposed, which extends traditional Datalog by existential quantification in rule heads. In this work, we propose a natural extension of Disjunctive Datalog and Datalog3, called Datalog3V, which allows both disjunctions and existential quantification in rule heads and is therefore an attractive language for knowledge representation and reasoning, especially in domains where ontology-based reasoning is needed. We formally define syntax and semantics of the language Datalog3V, and provide a notion of instantiation, which we prove to be adequate for Datalog3V. A main issue of Datalog3and hence also of Datalog3Vis that decidability is no longer guaranteed for typical reasoning tasks. In order to address this issue, we identify many decidable fragments of the language, which extend, in a natural way, analog classes defined in the non-disjunctive case. Moreover, we carry out an in-depth complexity analysis, deriving interesting results which range from Logarithmic Space to Exponential Time. © Cambridge University Press 2012.","complexity; datalog; decidability; non-monotonic reasoning","Artificial intelligence; Knowledge representation; Logic programming; complexity; Complexity analysis; Complexity issues; Data integration; Datalog; Disjunctive datalog; Existential quantifications; Existential quantifiers; Exponential time; Incomplete knowledge; Knowledge representation and reasoning; Natural extension; Non-monotonic reasoning; Ontology-based; Ontology-based data access; Reasoning tasks; Rule-based language; Semantic web applications; Computability and decidability",Article,Scopus,2-s2.0-84864974336
"Bustince H., De Baets B., Fernandez J., Mesiar R., Montero J.","A generalization of the migrativity property of aggregation functions",2012,"Information Sciences",23,10.1016/j.ins.2011.12.019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857657799&doi=10.1016%2fj.ins.2011.12.019&partnerID=40&md5=52415255f0ed06fa0669b63ceeff3ff1","This paper brings a generalization of the migrativity property of aggregation functions, suggested in earlier work of some of the present authors by imposing the α-migrativity property of Durante and Sarkoci for all values of α instead of a single one. Replacing the algebraic product by an arbitrary aggregation function B naturally leads to the properties of α-B-migrativity and B-migrativity. This generalization establishes a link between migrativity and a particular case of Aczel's general associativity equation, already considered by Cutello and Montero as a recursive formula for aggregation. Following a basic investigation, emphasis is put on aggregation functions that can be represented in terms of an additive generator, more specifically, strict t-norms, strict t-conorms and representable uninorms. © 2012 Elsevier Inc. All rights reserved.","Additive generator; Aggregation function; Associativity; Migrativity; t-Norm; Uninorm","Additive generators; Aggregation function; Associativity; Migrativity; t-Norm; Uninorms; Artificial intelligence; Software engineering; Mathematical operators",Article,Scopus,2-s2.0-84857657799
"Pozun Z.D., Hansen K., Sheppard D., Rupp M., Müller K.-R., Henkelman G.","Optimizing transition states via kernel-based machine learning",2012,"Journal of Chemical Physics",23,10.1063/1.4707167,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862891798&doi=10.1063%2f1.4707167&partnerID=40&md5=c5331b49a43eb413fa8fd556ce894a40","We present a method for optimizing transition state theory dividing surfaces with support vector machines. The resulting dividing surfaces require no a priori information or intuition about reaction mechanisms. To generate optimal dividing surfaces, we apply a cycle of machine-learning and refinement of the surface by molecular dynamics sampling. We demonstrate that the machine-learned surfaces contain the relevant low-energy saddle points. The mechanisms of reactions may be extracted from the machine-learned surfaces in order to identify unexpected chemically relevant processes. Furthermore, we show that the machine-learned surfaces significantly increase the transmission coefficient for an adatom exchange involving many coupled degrees of freedom on a (100) surface when compared to a distance-based dividing surface. © 2012 American Institute of Physics.",,"Distance-based; Dividing surfaces; Low energies; Machine-learning; Priori information; Reaction mechanism; Saddle point; Transition state; Transition state theories; Transmission coefficients; Learning systems; Molecular dynamics; Reaction kinetics; Optimization; algorithm; article; artificial intelligence; biology; computer program; molecular dynamics; support vector machine; surface property; Algorithms; Artificial Intelligence; Computational Biology; Molecular Dynamics Simulation; Software; Support Vector Machines; Surface Properties",Article,Scopus,2-s2.0-84862891798
"Hohenberger S., Lewko A., Waters B.","Detecting dangerous queries: A new approach for chosen ciphertext security",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",23,10.1007/978-3-642-29011-4_39,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859983465&doi=10.1007%2f978-3-642-29011-4_39&partnerID=40&md5=f196bd4d5947b29a90e487fa6d495d04","We present a new approach for creating chosen ciphertext secure encryption. The focal point of our work is a new abstraction that we call Detectable Chosen Ciphertext Security (DCCA). Intuitively, this notion is meant to capture systems that are not necessarily chosen ciphertext attack (CCA) secure, but where we can detect whether a certain query CT can be useful for decrypting (or distinguishing) a challenge ciphertext CT *. We show how to build chosen ciphertext secure systems from DCCA security. We motivate our techniques by describing multiple examples of DCCA systems including creating them from 1-bit CCA secure encryption-capturing the recent Myers-shelat result (FOCS 2009). Our work identifies DCCA as a new target for building CCA secure systems. © 2012 International Association for Cryptologic Research.",,"Capture system; Chosen ciphertext attack; Chosen ciphertext security; Ciphertexts; Focal points; Secure system; Capture system; Chosen ciphertext attack; Chosen ciphertext security; Ciphertexts; Focal points; New approaches; Secure system; Artificial intelligence; Security of data; Cryptography; Cryptography",Conference Paper,Scopus,2-s2.0-84859983465
"D'Silva V., Haller L., Kroening D., Tautschnig M.","Numeric bounds analysis with conflict-driven learning",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",23,10.1007/978-3-642-28756-5_5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859330082&doi=10.1007%2f978-3-642-28756-5_5&partnerID=40&md5=8ce87613d5930137c98dd538aaa47d0b","This paper presents a sound and complete analysis for determining the range of floating-point variables in control software. Existing approaches to bounds analysis either use convex abstract domains and are efficient but imprecise, or use floating-point decision procedures, and are precise but do not scale. We present a new analysis that elevates the architecture of a modern SAT solver to operate over floating-point intervals. In experiments, our analyser is consistently more precise than a state-of-the-art static analyser and significantly outperforms floating-point decision procedures. © 2012 Springer-Verlag Berlin Heidelberg.",,"Abstract domains; Decision procedure; In-control; SAT solvers; Abstract domains; Decision procedure; Floating points; Floatingpoint; In-control; SAT solvers; Sound and complete; State of the art; Algorithms; Digital arithmetic; Artificial intelligence; Computer science; Computers; Computer architecture; Digital arithmetic",Conference Paper,Scopus,2-s2.0-84859330082
"Salay R., Famelis M., Chechik M.","Language independent refinement using partial modeling",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",23,10.1007/978-3-642-28872-2_16,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859123255&doi=10.1007%2f978-3-642-28872-2_16&partnerID=40&md5=be94a89e92b5fb2384ef70b9c8d3a65e","Models express not only information about their intended domain but also about the way in which the model is incomplete, or ""partial"". This partiality supports the modeling process because it permits the expression of what is known without premature decisions about what is still unknown, until later refinements can fill in this information. A key observation of this paper is that a number of partiality types can be defined in a modeling language-independent way, and we propose a formal framework for doing so. In particular, we identify four types of partiality and show how to extend a modeling language to support their expression and refinement. This systematic approach provides a basis for reasoning as well as a framework for generic tooling support. We illustrate the framework by enhancing the UML class diagram and sequence diagram languages with partiality support and using Alloy to automate reasoning tasks. © 2012 Springer-Verlag Berlin Heidelberg.",,"Formal framework; Modeling languages; Modeling process; Partial modeling; Reasoning tasks; Sequence diagrams; UML class diagrams; Formal framework; Language independents; Modeling process; Partial modeling; Reasoning tasks; Sequence diagrams; UML class diagrams; Artificial intelligence; Computational linguistics; Software engineering; Software engineering; Modeling languages",Conference Paper,Scopus,2-s2.0-84859123255
"Barbosa R.R.R., Sadre R., Pras A.","Difficulties in modeling SCADA traffic: A comparative analysis",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",23,10.1007/978-3-642-28537-0_13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859034021&doi=10.1007%2f978-3-642-28537-0_13&partnerID=40&md5=ad4aebabd85ec24379344d24b45cf4d8","Modern critical infrastructures, such as water distribution and power generation, are large facilities that are distributed over large geographical areas. Supervisory Control and Data Acquisition (SCADA) networks are deployed to guarantee the correct operation and safety of these infrastructures. In this paper, we describe key characteristics of SCADA traffic, verifying if models developed for traffic in traditional IT networks are applicable. Our results show that SCADA traffic largely differs from traditional IT traffic, more noticeably not presenting diurnal patters or self-similar correlations in the time series. © 2012 Springer-Verlag Berlin Heidelberg.",,"Comparative analysis; Geographical area; IT networks; Key characteristics; Self-similar; Supervisory control and data acquisition; Water distributions; Artificial intelligence; Water supply systems",Conference Paper,Scopus,2-s2.0-84859034021
"Raghavan M., Sahar N.D., Kohn D.H., Morris M.D.","Age-specific profiles of tissue-level composition and mechanical properties in murine cortical bone",2012,"Bone",23,10.1016/j.bone.2011.12.026,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858077260&doi=10.1016%2fj.bone.2011.12.026&partnerID=40&md5=8cfcb91206f824221086f84f02e2d374","There is growing evidence that bone composition and tissue-level mechanical properties are significant determinants of skeletal integrity. In the current study, Raman spectroscopy and nanoindentation testing were co-localized to analyze tissue-level compositional and mechanical properties in skeletally mature young (4 or 5. months) and old (19. months) murine femora at similar spatial scales. Standard multivariate linear regression analysis revealed age-dependent patterns in the relationships between mechanical and compositional properties at the tissue scale. However, changes in bone material properties with age are often complex and nonlinear, and can be missed with linear regression and correlation-based methods. A retrospective data mining approach was implemented using non-linear multidimensional visualization and classification to identify spectroscopic and nanoindentation metrics that best discriminated bone specimens of different age-classes. The ability to classify the specimens into the correct age group increased by using combinations of Raman and nanoindentation variables (86-96% accuracy) as compared to using individual measures (59-79% accuracy). Metrics that best classified 4 or 5. month and 19. month specimens (2-age classes) were mineral to matrix ratio, crystallinity, modulus and plasticity index. Metrics that best distinguished between 4, 5 and 19. month specimens (3-age classes) were mineral to matrix ratio, crystallinity, modulus, hardness, cross-linking, carbonate to phosphate ratio, creep displacement and creep viscosity. These findings attest to the complexity of mechanisms underlying bone tissue properties and draw attention to the importance of considering non-linear interactions between tissue-level composition and mechanics that may work together to influence material properties with age. The results demonstrate that a few non-linearly combined compositional and mechanical metrics provide better discriminatory information than a single metric or a single technique. © 2012 Elsevier Inc.","Bone composition; Data mining; Machine learning; Nanoindentation; Raman spectroscopy; Tissue mechanics","carbonic acid; phosphate; age distribution; animal tissue; article; biomechanics; bone matrix; bone mineral; bone structure; bone tissue; classification; controlled study; cortical bone; cross linking; crystallization; hardness; male; mouse; nanoanalysis; nonhuman; plasticity; Raman spectrometry; retrospective study; tissue distribution; tissue injury; tissue interaction; tissue level; viscosity; Young modulus; Aging; Algorithms; Animals; Artificial Intelligence; Biomechanics; Bone and Bones; Bone Density; Bone Matrix; Hardness; Linear Models; Male; Mice; Mice, Inbred C57BL; Multivariate Analysis; Nanotechnology; Nonlinear Dynamics; ROC Curve; Spectrum Analysis, Raman",Article,Scopus,2-s2.0-84858077260
"Zhang H.","Ant colony optimization for multimode resource-constrained project scheduling",2012,"Journal of Management in Engineering",23,10.1061/(ASCE)ME.1943-5479.0000089,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870712321&doi=10.1061%2f%28ASCE%29ME.1943-5479.0000089&partnerID=40&md5=3bc481d34aa084b34ac146bc54f7e4c0","An ant colony optimization (ACO)-based methodology for solving a multimode resource-constrained project scheduling problem (MRCPSP) with the objective of minimizing project duration is presented. With regards to the need to determine sequence and mode selection of activities for the MRCPSP, two levels of pheromones for each ant are proposed to guide the search course in the ACO algorithm. The corresponding heuristics and probabilities for each type of the pheromone are considered, and their calculation algorithms are presented. The flowchart of the proposed ACO algorithm is described, where a serial schedule generation scheme is adopted to transform an ACO solution into a feasible schedule. The effectiveness and efficiency of the proposed ACO methodology are justified through a series of computational analyses. The study is expected to provide a more effective alternative methodology for solving the MRCPSP by utilizing the ACO theory. © 2012 American Society of Civil Engineers.","Ant colony optimization; Multiple execution modes; Project scheduling; Resource-constrained","ACO algorithms; Ant Colony Optimization (ACO); Computational analysis; Feasible schedule; Minimizing project duration; Mode selection; Multi-mode resource-constrained project scheduling problem; Multimodes; Multiple execution modes; Project scheduling; Resource constrained project scheduling; Resource-constrained; Serial schedule generation scheme; Artificial intelligence; Scheduling; Algorithms",Article,Scopus,2-s2.0-84870712321
"Martis R.J., Krishnan M.M.R., Chakraborty C., Pal S., Sarkar D., Mandana K.M., Ray A.K.","Automated screening of arrhythmia using wavelet based machine learning techniques",2012,"Journal of Medical Systems",23,10.1007/s10916-010-9535-7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863214019&doi=10.1007%2fs10916-010-9535-7&partnerID=40&md5=0e731b1ce168dd5f06bf3c8066905576","Arrhythmia is one of the preventive cardiac problems frequently occurs all over the globe. In order to screen such disease at early stage, this work attempts to develop a system approach based on registration, feature extraction using discrete wavelet transform (DWT), feature validation and classification of electrocardiogram (ECG). This diagnostic issue is set as a two-class pattern classification problem (normal sinus rhythm versus arrhythmia) where MIT-BIH database is considered for training, testing and clinical validation. Here DWT is applied to extract multi-resolution coefficients followed by registration using Pan Tompkins algorithm based R point detection. Moreover, feature space is compressed using sub-band principal component analysis (PCA) and statistically validated using independent sample t-test. Thereafter, the machine learning algorithms viz., Gaussian mixture model (GMM), error back propagation neural network (EBPNN) and support vector machine (SVM) are employed for pattern classification. Results are studied and compared. It is observed that both supervised classifiers EBPNN and SVM lead to higher (93.41% and 95.60% respectively) accuracy in comparison with GMM (87.36%) for arrhythmia screening. © Springer Science+Business Media, LLC 2010.","Arrhythmia; EBPNN; ECG; GMM; Normal sinus rhythm; SVM","algorithm; article; cardiovascular parameters; controlled study; diagnostic accuracy; discrete wavelet transform; early diagnosis; electrocardiogram; heart arrhythmia; kernel method; machine learning; screening; sinus rhythm; Student t test; adult; artificial intelligence; artificial neural network; decision support system; electrocardiography; female; heart arrhythmia; human; male; middle aged; organization and management; support vector machine; wavelet analysis; Adult; Arrhythmias, Cardiac; Artificial Intelligence; Decision Support Systems, Clinical; Electrocardiography, Ambulatory; Female; Humans; Male; Middle Aged; Neural Networks (Computer); Support Vector Machines; Wavelet Analysis",Article,Scopus,2-s2.0-84863214019
"Ciepiela E., Nowakowski P., Kocot J., Harȩzlak D., Gubała T., Meizner J., Kasztelnik M., Bartyński T., Malawski M., Bubak M.","Managing entire lifecycles of e-science applications in the GridSpace2 virtual laboratory - From motivation through idea to operable web-accessible environment built on top of PL-Grid e-infrastructure",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",23,10.1007/978-3-642-28267-6_18,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857752139&doi=10.1007%2f978-3-642-28267-6_18&partnerID=40&md5=16ef065c35a48e107eb2112ffd094128","The GridSpace2 environment, developed in the scope of the PL-Grid Polish National Grid Initiative, constitutes a comprehensive platform which supports e-science applications throughout their entire lifecycle. Application development may involve multiple phases, including writing, prototyping, testing and composing the application. Once the application attains maturity it becomes operable and capable of being executed, although it may still be subject to further development - including actions such as sharing with collaborating researchers or making results publicly available with the use of dedicated publishing interfaces. This paper describes each of these phases in detail, showing how the GridSpace2 platform can assist the developers and publishers of computational experiments. © 2012 Springer-Verlag Berlin Heidelberg.","application development; collaborative research; computational science; virtual laboratories","Application development; Collaborative research; Computational experiment; Computational science; e-Science; Further development; National Grid; Virtual laboratories; Artificial intelligence",Article,Scopus,2-s2.0-84857752139
"Griffith S., Sinapov J., Sukhoy V., Stoytchev A.","A behavior-grounded approach to forming object categories: Separating containers from noncontainers",2012,"IEEE Transactions on Autonomous Mental Development",23,10.1109/TAMD.2011.2157504,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858650540&doi=10.1109%2fTAMD.2011.2157504&partnerID=40&md5=d5e5e25d3f5c64640e8a7db678df26b9","This paper introduces a framework that allows a robot to form a single behavior-grounded object categorization after it uses multiple exploratory behaviors to interact with objects and multiple sensory modalities to detect the outcomes that each behavior produces. Our robot observed acoustic and visual outcomes from six different exploratory behaviors performed on 20 objects (containers and noncontainers). Its task was to learn 12 different object categorizations (one for each behavior-modality combination), and then to unify these categorizations into a single one. In the end, the object categorization acquired by the robot matched closely the object labels provided by a human. In addition, the robot acquired a visual model of containers and noncontainers based on its unified categorization, which it used to label correctly 29 out of 30 novel objects. © 2009 IEEE.","Artificial intelligence; developmental robotics; intelligent robots; learning systems; object categorization; robots","Developmental robotics; Object categories; Object categorization; Sensory modality; Visual model; Artificial intelligence; Containers; Intelligent robots; Learning systems; Robots",Article,Scopus,2-s2.0-84858650540
"Huang X.","An entropy method for diversified fuzzy portfolio selection",2012,"International Journal of Fuzzy Systems",23,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860326777&partnerID=40&md5=351188d40723706b6747b270d17dd1cf","This paper proposes an entropy method for diversified fuzzy portfolio selection. Proportion entropy is introduced and credibilistic mean-variance and mean-semivariance diversification models for fuzzy portfolio selection are proposed. The crisp forms of the proposed models are also provided when the security returns are all triangular fuzzy variables. As an illustration, an application example of mean-variance diversification model is given using real data from Shanghai Stock Exchange. The computation results show that the proposed model results in a more diversified investment than the credibilistic mean-variance model. © 2012 TFSA.","Entropy; Fuzzy portfolio selection; Mean-semivariance model; Meanvariance model","Application examples; Entropy methods; Fuzzy variable; Mean variance; Mean variance model; Model results; Portfolio selection; Stock exchange; Artificial intelligence; Software engineering; Entropy",Article,Scopus,2-s2.0-84860326777
"Cavallo B., D'Apuzzo L., Squillante M.","About a consistency index for pairwise comparison matrices over a divisible alo-group",2012,"International Journal of Intelligent Systems",23,10.1002/int.21518,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855181565&doi=10.1002%2fint.21518&partnerID=40&md5=e4668bb8c98bfd29e633fd21fef50a08","Pairwise comparison matrices (PCMs) over an Abelian linearly ordered (alo)-group G=(G, ⊙ ≤) have been introduced to generalize multiplicative, additive and fuzzy ones and remove some consistency drawbacks. Under the assumption of divisibility of G, for each PCM A=(a ij), a ⊙-mean vector w m(A) can be associated with A and a consistency measure I G(A), expressed in terms of ⊙-mean of G-distances, can be provided. In this paper, we focus on the consistency index I G(A). By using the notion of rational power and the related properties, we establish a link between w m(A)and I G(A). The relevance of this link is twofold because it gives more validity to I G(A) and more meaning to w m(A); in fact, it ensures that if I G(A)is close to the identity element then, from a side A is close to be a consistent PCM and from the other side w m(A) is close to be a consistent vector; thus, it can be chosen as a priority vector for the alternatives. Copyright © 2011 Wiley Periodicals, Inc.",,"Consistency index; Consistency measures; Mean vector; Pair-wise comparison; Priority vector; Artificial intelligence; Software engineering; Iodine",Conference Paper,Scopus,2-s2.0-84855181565
"Finke P.A.","On digital soil assessment with models and the Pedometrics agenda",2012,"Geoderma",23,10.1016/j.geoderma.2011.01.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857086642&doi=10.1016%2fj.geoderma.2011.01.001&partnerID=40&md5=c1a525e4920213f7596d25c46e7dc362","The unique selling point of pedometricians still is close to their cradle: ability to map. Irrespective of the many scientific achievements one can ask if offering mapping ability, allbeit in various contexts, is all a pedometrician can do to bring forward soil science's broader agenda. This paper identifies, within some of the hotter issues on the soil science agenda, activities that need the input of pedometricians. Digital soil mapping (DSM) has reached maturity although some issues like the optimal use of legacy data still need attention. The necessary shift from DSM to digital soil function mapping implies an increased need for process knowledge in mapping, but also an increased focus of pedometricians on the strengthening of process models. Most quality issues related to DSM have a counterpart in process modelling, and some of these issues need elaboration to construct well-calibrated and complete models, e.g. by making motivated choices between the inclusion of processes or model reduction. Reaching out to stakeholders is also an issue of increasing interest, as these are confronted with uncertain concurrent models to evaluate future scenarios. Questions are raised such as: Does the need for a state-of-the-art (SOTA) approach imply a choice for one model and how should this be made, or is SOTA implementable as some weighted average of screened concurrent models. Can observations and model results be combined in decision making and what techniques are needed. The above issues are illustrated with existing examples and new material from soil science and beyond. As DSM and process modelling share common ground, mutual benefits can be expected at the interface of both research fields such as (i) increased usage of process knowledge in DSM and (ii) probabilistic approaches to less well understood soil processes. Stakeholders will profit from the development of decision frameworks to choose applicable DSM-techniques and the increased application of ensembles of DSM-methods and models to narrow prediction error bandwidths. Additionally, they will profit from the development of decision support systems filled with outcomes of scenario studies of multiple, uncertain, and concurrent models as these provide an interesting alternative to the application of one selected model. © 2010 Elsevier B.V.","Calibration; Digital soil assessment; Digital soil mapping; Ensemble modelling; Pedometrics; Process modelling","Digital soil assessment; Digital soil mapping; Ensemble modelling; Pedometrics; Process modelling; Agriculture; Artificial intelligence; Calibration; Decision support systems; Mapping; Profitability; Rating; Soils; Geologic models; assessment method; calibration; decision support system; identification method; mapping method; numerical model; pedogenesis; soil science; soil type; stakeholder; uncertainty analysis",Article,Scopus,2-s2.0-84857086642
"Sudholt D., Thyssen C.","Running time analysis of ant colony optimization for shortest path problems",2012,"Journal of Discrete Algorithms",23,10.1016/j.jda.2011.06.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855525948&doi=10.1016%2fj.jda.2011.06.002&partnerID=40&md5=aa1e0d29f78883198a2ac2457802a765","Ant Colony Optimization (ACO) is a modern and very popular optimization paradigm inspired by the ability of ant colonies to find shortest paths between their nest and a food source. Despite its popularity, the theory of ACO is still in its infancy and a solid theoretical foundation is needed. We present bounds on the running time of different ACO systems for shortest path problems. First, we improve previous results by Attiratanasunthron and Fakcharoenphol [Information Processing Letters 105 (3) (2008) 88-92] for single-destination shortest paths and extend their results from DAGs to arbitrary directed graphs. Our upper bound is asymptotically tight for large evaporation factors, holds with high probability, and transfers to the all-pairs shortest paths problem. There, a simple mechanism for exchanging information between ants with different destinations yields a significant improvement. A comparison with evolutionary and genetic approaches indicates that ACO is among the best known metaheuristics for the all-pairs shortest paths problem. © 2011 Elsevier B.V. All rights reserved.","Ant Colony Optimization; Combinatorial optimization; Metaheuristics; Running time analysis; Shortest path problems","Ant colonies; Ant-colony optimization; Arbitrary directed graphs; Food sources; Genetic approach; High probability; Meta heuristics; Running time; Running time analysis; Shortest path; Shortest path problem; Theoretical foundations; Upper Bound; Artificial intelligence; Combinatorial optimization; Data processing; Heuristic algorithms; Optimization; Wireless sensor networks; Graph theory",Article,Scopus,2-s2.0-84855525948
"Samolejová A., Feliks J., Lenort R., Besta P.","A hybrid decision support system for iron ore supply [Hibridni sustav za podršku odlučivanja opskrbe željeznom rudom]",2012,"Metalurgija",23,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053529261&partnerID=40&md5=92d09b0619c6a3472f4d78e202c9d1a9","Many European metallurgical companies are forced to import iron ore from remote destinations. For these companies it is necessary to determine the amount of iron ore that will have to be ordered and to create such a delivery schedule so that the continuous operation of blast-furnace plant is not disrupted and there is no exceedingly large stock of this raw material. The objective of this article is to design the decision support system for iron ore supply which would efficiently reduce uncertainty and risk of that decision-making. The article proposes a hybrid intelligent system which represents a combination of different artificial intelligence methods with dynamic simulation technique for that purpose.","Fuzzy sets; Hybrid decision support system; Iron ore supply; Neural network; Simulation","Artificial intelligence methods; Continuous operation; Decision supports; Delivery schedules; Hybrid intelligent system; Import iron ore; Metallurgical companies; Remote destination; Simulation; Computer simulation; Fuzzy sets; Industry; Intelligent systems; Iron; Iron ores; Neural networks; Decision support systems",Article,Scopus,2-s2.0-80053529261
"Dembczynski K., Waegeman W., Hüllermeier E.","An analysis of chaining in multi-label classification",2012,"Frontiers in Artificial Intelligence and Applications",23,10.3233/978-1-61499-098-7-294,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878803999&doi=10.3233%2f978-1-61499-098-7-294&partnerID=40&md5=f8c8fc264232f25ddd95d7643aaa040f","The idea of classifier chains has recently been introduced as a promising technique for multi-label classification. However, despite being intuitively appealing and showing strong performance in empirical studies, still very little is known about the main principles underlying this type of method. In this paper, we provide a detailed probabilistic analysis of classifier chains from a risk minimization perspective, thereby helping to gain a better understanding of this approach. As a main result, we clarify that the original chaining method seeks to approximate the joint mode of the conditional distribution of label vectors in a greedy manner. As a result of a theoretical regret analysis, we conclude that this approach can perform quite poorly in terms of subset 0/1 loss. Therefore, we present an enhanced inference procedure for which the worst-case regret can be upper-bounded far more tightly. In addition, we show that a probabilistic variant of chaining, which can be utilized for any loss function, becomes tractable by using Monte Carlo sampling. Finally, we present experimental results confirming the validity of our theoretical findings. © 2012 The Author(s).",,"Artificial intelligence; Chains; Monte Carlo methods; Risk assessment; Classifier chains; Conditional distribution; Empirical studies; Monte Carlo sampling; Multi label classification; Probabilistic analysis; Probabilistic variant; Risk minimization; Classification (of information)",Conference Paper,Scopus,2-s2.0-84878803999
"Kaljun J., Dolšak B.","Ergonomic design knowledge built in the intelligent decision support system",2012,"International Journal of Industrial Ergonomics",23,10.1016/j.ergon.2011.11.009,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855933286&doi=10.1016%2fj.ergon.2011.11.009&partnerID=40&md5=5e41e91ad8b5c7b5a22bbaa234383aee","Existing computer tools for ergonomic design are unable to assist designers with higher level advice within design processes. Thus, design engineers need to rely on their own knowledge and experience when making crucial decisions relating to products' ergonomic parameters. An intelligent decision support system has been developed in order to overcome this bottleneck. This paper presents a knowledge base, containing ergonomic design knowledge specific for hand tool design. A pneumatic hammer handle design is used as a case study in order to show how ergonomic design knowledge built within this system is used to improve the ergonomic value of a product. Relevance to industry: Engineers in small and medium-size enterprises (SME's) or young designers are in an unenviable position as their knowledge domains and experiences are minimal. When applying decision support system discussed here, even SME's could appear on the market with optimal designed products with relatively minor influence on development budget. Consequently their role on the market will be aggrandised. © 2011 Elsevier B.V.","Ergonomics; Hand tools; Knowledge acquisition; Product development","Computer tools; Design engineers; Design process; Ergonomic design; Hand tool design; Intelligent decision support systems; Knowledge and experience; Knowledge base; Knowledge domains; Small and medium-size enterprise; Artificial intelligence; Decision support systems; Design; Engineers; Hand tools; Industry; Knowledge acquisition; Knowledge based systems; Product development; Ergonomics; article; computer aided design; computer interface; decision support system; equipment design; ergonomics; grip strength; hand grip; human; human computer interaction; learning; materials handling; materials testing; pinch strength; priority journal; process optimization; product development; static electricity; task performance; validation process",Article,Scopus,2-s2.0-84855933286
"Cheng B., Zhang D., Shen D.","Domain transfer learning for MCI conversion prediction",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",23,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872575289&partnerID=40&md5=8e568b35bb0586c89b80b640ce25b915","In recent studies of Alzheimer’s disease (AD), it has increasing attentions in identifying mild cognitive impairment (MCI) converters (MCI-C) from MCI non-converters (MCI-NC). Note that MCI is a prodromal stage of AD, with possibility to convert to AD. Most traditional methods for MCI conversion prediction learn information only from MCI subjects (including MCI-C and MCI-NC), not from other related subjects, e.g., AD and normal controls (NC), which can actually aid the classification between MCI-C and MCI-NC. In this paper, we propose a novel domain-transfer learning method for MCI conversion prediction. Different from most existing methods, we classify MCI-C and MCI-NC with aid from the domain knowledge learned with AD and NC subjects as auxiliary domain to further improve the classification performance. Our method contains two key components: (1) the cross-domain kernel learning for transferring auxiliary domain knowledge, and (2) the adapted support vector machine (SVM) decision function construction for cross-domain and auxiliary domain knowledge fusion. Experimental results on the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database show that the proposed method can significantly improve the classification performance between MCI-C and MCI-NC, with aid of domain knowledge learned from AD and NC subjects. © Springer-Verlag Berlin Heidelberg 2012.",,"Classification (of information); Forecasting; Machine components; Medical computing; Medical imaging; Neuroimaging; Support vector machines; Classification performance; Decision functions; Domain knowledge; Domain transfers; Kernel learning; Mild cognitive impairments (MCI); Normal controls; Novel domain; Education; algorithm; Alzheimer disease; area under the curve; article; artificial intelligence; brain mapping; cognitive defect; computer program; diagnostic imaging; disease course; human; knowledge; learning; methodology; mild cognitive impairment; pathology; pathophysiology; reproducibility; Algorithms; Alzheimer Disease; Area Under Curve; Artificial Intelligence; Brain Mapping; Cognition Disorders; Diagnostic Imaging; Disease Progression; Humans; Knowledge; Mild Cognitive Impairment; Reproducibility of Results; Software; Transfer (Psychology)",Conference Paper,Scopus,2-s2.0-84872575289
"Wang S., Li W., Wang Y., Jiang Y., Jiang S., Zhao R.","An improved difference of gaussian filter in face recognition",2012,"Journal of Multimedia",22,10.4304/jmm.7.6.429-433,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871209323&doi=10.4304%2fjmm.7.6.429-433&partnerID=40&md5=bab0293789881e46030342e96f53a340","In this paper, we analyze an improved DOG filter with different parameters in horizontal and vertical directions and the improved filter uses oval recognition domain instead of round recognition domain of classic DOG filter. The improved filter is used to compare with major illumination pretreatment methods. The experiment result shows the improved method has better recognition and false detection rate. © 2012 ACADEMY PUBLISHER.","Difference of gaussian filter; Face recognition; Illumination pretreatment; LDA; PCA","False detections; Gaussian filters; LDA; PCA; Pre-Treatment; Pretreatment methods; Vertical direction; Artificial intelligence; Multimedia systems; Face recognition",Article,Scopus,2-s2.0-84871209323
"Han B., Chen X.-W., Talebizadeh Z., Xu H.","Genetic studies of complex human diseases: Characterizing SNP-disease associations using Bayesian networks",2012,"BMC Systems Biology",22,10.1186/1752-0509-6-S3-S14,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877328896&doi=10.1186%2f1752-0509-6-S3-S14&partnerID=40&md5=dfd6a76ff403140b1e9575769973b33c","Background: Detecting epistatic interactions plays a significant role in improving pathogenesis, prevention, diagnosis, and treatment of complex human diseases. Applying machine learning or statistical methods to epistatic interaction detection will encounter some common problems, e.g., very limited number of samples, an extremely high search space, a large number of false positives, and ways to measure the association between disease markers and the phenotype.Results: To address the problems of computational methods in epistatic interaction detection, we propose a score-based Bayesian network structure learning method, EpiBN, to detect epistatic interactions. We apply the proposed method to both simulated datasets and three real disease datasets. Experimental results on simulation data show that our method outperforms some other commonly-used methods in terms of power and sample-efficiency, and is especially suitable for detecting epistatic interactions with weak or no marginal effects. Furthermore, our method is scalable to real disease data.Conclusions: We propose a Bayesian network-based method, EpiBN, to detect epistatic interactions. In EpiBN, we develop a new scoring function, which can reflect higher-order epistatic interactions by estimating the model complexity from data, and apply a fast Branch-and-Bound algorithm to learn the structure of a two-layer Bayesian network containing only one target node. To make our method scalable to real data, we propose the use of a Markov chain Monte Carlo (MCMC) method to perform the screening process. Applications of the proposed method to some real GWAS (genome-wide association studies) datasets may provide helpful insights into understanding the genetic basis of Age-related Macular Degeneration, late-onset Alzheimer's disease, and autism. © 2012 Han et al.; licensee BioMed Central Ltd.",,"algorithm; Alzheimer disease; article; artificial intelligence; autism; Bayes theorem; biological model; biology; computer simulation; epistasis; genetic association; genetic database; genetics; human; methodology; Monte Carlo method; probability; retina macula degeneration; single nucleotide polymorphism; Algorithms; Alzheimer Disease; Artificial Intelligence; Autistic Disorder; Bayes Theorem; Computational Biology; Computer Simulation; Databases, Genetic; Epistasis, Genetic; Genome-Wide Association Study; Humans; Macular Degeneration; Markov Chains; Models, Genetic; Monte Carlo Method; Polymorphism, Single Nucleotide",Article,Scopus,2-s2.0-84877328896
"Sabuncu M.R., Van Leemput K.","The relevance voxel machine (RVoxM): A self-tuning bayesian model for informative image-based prediction",2012,"IEEE Transactions on Medical Imaging",22,10.1109/TMI.2012.2216543,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870489618&doi=10.1109%2fTMI.2012.2216543&partnerID=40&md5=ef14a27a6647f8183cdbbc85ddc88c7f","This paper presents the relevance voxel machine (RVoxM), a dedicated Bayesian model for making predictions based on medical imaging data. In contrast to the generic machine learning algorithms that have often been used for this purpose, the method is designed to utilize a small number of spatially clustered sets of voxels that are particularly suited for clinical interpretation. RVoxM automatically tunes all its free parameters during the training phase, and offers the additional advantage of producing probabilistic prediction outcomes. We demonstrate RVoxM as a regression model by predicting age from volumetric gray matter segmentations, and as a classification model by distinguishing patients with Alzheimer's disease from healthy controls using surface-based cortical thickness data. Our results indicate that RVoxM yields biologically meaningful models, while providing state-of-The-art predictive accuracy. © 2012 IEEE.","Image classification; pattern recognition","Alzheimer's disease; Bayesian model; Classification models; Cortical thickness; Free parameters; Gray matter; Healthy controls; Image-based; Predictive accuracy; Probabilistic prediction; Regression model; Selftuning; Surface-based; Training phase; Bayesian networks; Image classification; Learning algorithms; Medical imaging; Pattern recognition; Regression analysis; Forecasting; adolescent; adult; age; aged; algorithm; Alzheimer disease; article; artificial intelligence; automated pattern recognition; Bayes theorem; brain cortex; case control study; factual database; female; histology; human; image processing; male; methodology; middle aged; nuclear magnetic resonance imaging; pathology; receiver operating characteristic; regression analysis; reproducibility; Adolescent; Adult; Age Factors; Aged; Aged, 80 and over; Algorithms; Alzheimer Disease; Artificial Intelligence; Bayes Theorem; Case-Control Studies; Cerebral Cortex; Databases, Factual; Female; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Male; Middle Aged; Pattern Recognition, Automated; Regression Analysis; Reproducibility of Results; ROC Curve",Article,Scopus,2-s2.0-84870489618
"Zhou D., Shi Y., Cheng S.","Brain storm optimization algorithm with modified step-size and individual generation",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",22,10.1007/978-3-642-30976-2_29,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875109862&doi=10.1007%2f978-3-642-30976-2_29&partnerID=40&md5=29f9bfda95188fde1119ae5230ae0a72","Brain Storm Optimization algorithm is inspired from the humans' brainstorming process. It simulates the problem-solving process of a group of people. In this paper, the original BSO algorithm is modified by amending the original BSO. First the step-size is adapted according to the dynamic range of individuals on each dimension. Second, the new individuals are generated in a batch-mode and then selected into the next generation. Experiments are conducted to demonstrate the performance of the modified BSO by testing on ten benchmark functions. The experimental results show that the modified BSO algorithm performs better than the original BSO. © 2012 Springer-Verlag.","Adaptive step-size; Brain Storm Optimization; Selection","Adaptive step size; Benchmark functions; Dynamic range; Optimization algorithms; Problem-solving process; Selection; Step size; Artificial intelligence; Benchmarking; Storms; Algorithms",Conference Paper,Scopus,2-s2.0-84875109862
"Zhang Y., Yin Y., Wu H., Guo D.","Zhang dynamics and gradient dynamics with tracking-control application",2012,"Proceedings - 2012 5th International Symposium on Computational Intelligence and Design, ISCID 2012",22,10.1109/ISCID.2012.66,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873337565&doi=10.1109%2fISCID.2012.66&partnerID=40&md5=81245774830bb90d397790f31b2e039f","Via solving time-varying linear equations, this paper shows the Zhang dynamics (ZD) method. Besides, the gradient dynamics (GD) method, which was originally designed for constant problems solving, is generalized for time-varying linear equations solving. Then, the ZD and GD methods are exploited together to solve the tracking-control problem of a nonlinear system as a new application. Simulation results on the nonlinear system further demonstrate the feasibility of the ZD and GD methods for tracking control of nonlinear systems. © 2012 IEEE.","Gradient dynamics; Nonlinear system; Time varying; Tracking control; Zhang dynamics","Gradient dynamics; New applications; Time varying; Time-varying linear equations; Tracking controls; Artificial intelligence; Linear equations; Navigation; Nonlinear systems; Dynamics",Conference Paper,Scopus,2-s2.0-84873337565
"Huang J.","Compactness and its implications for qualitative spatial and temporal reasoning",2012,"13th International Conference on the Principles of Knowledge Representation and Reasoning, KR 2012",22,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893389336&partnerID=40&md5=0a9d5e038d7e2ed75469275bbe630629","A constraint satisfaction problem has compactness if any infinite set of constraints is satisfiable whenever all its finite subsets are satisfiable. We prove a sufficient condition for compactness, which holds for a range of problems including those based on the well-known Interval Algebra (IA) and RCC8. Furthermore, we show that compactness leads to a useful necessary and sufficient condition for the recently introduced patchwork property, namely that patchwork holds exactly when every satisfiable finite network (i.e., set of constraints) has a canonical solution, that is, a solution that can be extended to a solution for any satisfiable finite extension of the network. Applying these general theorems to qualitative reasoning, we obtain important new results as well as significant strengthenings of previous results regarding IA, RCC8, and their fragments and extensions. In particular, we show that all the maximal tractable fragments of IA and RCC8 (containing the base relations) have patchwork and canonical solutions as long as networks are algebraically closed. Copyright © 2012, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Canonical solutions; Finite extension; Finite subsets; Interval algebra; New results; Qualitative reasoning; Qualitative spatial and temporal reasoning; Artificial intelligence; Knowledge representation; Mathematical techniques",Conference Paper,Scopus,2-s2.0-84893389336
"Zhang Z., Zhao M., Chow T.W.S.","Marginal semi-supervised sub-manifold projections with informative constraints for dimensionality reduction and recognition",2012,"Neural Networks",22,10.1016/j.neunet.2012.09.010,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870258142&doi=10.1016%2fj.neunet.2012.09.010&partnerID=40&md5=e2c3589addcfe7e1f66d5d5c40cf4a88","In this work, sub-manifold projections based semi-supervised dimensionality reduction (DR) problem learning from partial constrained data is discussed. Two semi-supervised DR algorithms termed Marginal Semi-Supervised Sub-Manifold Projections (MS3MP) and orthogonal MS3MP (OMS3MP) are proposed. MS3MP in the singular case is also discussed. We also present the weighted least squares view of MS3MP. Based on specifying the types of neighborhoods with pairwise constraints (PC) and the defined manifold scatters, our methods can preserve the local properties of all points and discriminant structures embedded in the localized PC. The sub-manifolds of different classes can also be separated. In PC guided methods, exploring and selecting the informative constraints is challenging and random constraint subsets significantly affect the performance of algorithms. This paper also introduces an effective technique to select the informative constraints for DR with consistent constraints. The analytic form of the projection axes can be obtained by eigen-decomposition. The connections between this work and other related work are also elaborated. The validity of the proposed constraint selection approach and DR algorithms are evaluated by benchmark problems. Extensive simulations show that our algorithms can deliver promising results over some widely used state-of-the-art semi-supervised DR techniques. © 2012 Elsevier Ltd.","Dimensionality reduction; Image recognition; Informative constraints; Marginal projections; Semi-supervised learning","Bench-mark problems; Consistent constraint; Dimensionality reduction; Eigen decomposition; Extensive simulations; Informative constraints; Local property; Marginal projections; Pairwise constraints; Performance of algorithm; Random constraints; Semi-supervised; Semi-supervised learning; Weighted least squares; Image recognition; Supervised learning; Algorithms; algorithm; article; artificial neural network; automated pattern recognition; computer simulation; controlled study; dimensionality recognition; dimensionality reduction; discriminant analysis; machine learning; pairwise constraint; priority journal; regression analysis; statistical parameters; validity; Algorithms; Artificial Intelligence; Computer Simulation; Face; Handwriting; Humans; Least-Squares Analysis; Neural Networks (Computer); Pattern Recognition, Automated",Article,Scopus,2-s2.0-84870258142
"Martens J., Sutskever I.","Training deep and recurrent networks with hessian-free optimization",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",22,10.1007/978-3-642-35289-8-27,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872565347&doi=10.1007%2f978-3-642-35289-8-27&partnerID=40&md5=6e638bb403c351fc5950c3a047db0a24","In this chapter we will first describe the basic HF approach, and then examine well-known performance-improving techniques such as preconditioning which we have found to be beneficial for neural network training, as well as others of a more heuristic nature which are harder to justify, but which we have found to work well in practice. We will also provide practical tips for creating efficient and bug-free implementations and discuss various pitfalls which may arise when designing and using an HF-type approach in a particular application. © Springer-Verlag Berlin Heidelberg 2012.",,"Bug-free; Neural network training; Recurrent networks; Artificial intelligence; Neural networks",Article,Scopus,2-s2.0-84872565347
"Li J., Tao D.","On preserving original variables in Bayesian PCA with application to image analysis",2012,"IEEE Transactions on Image Processing",22,10.1109/TIP.2012.2211372,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869422991&doi=10.1109%2fTIP.2012.2211372&partnerID=40&md5=ff68af153cf79f38c82249041f413a31","Principal component analysis (PCA) computes a succinct data representation by converting the data to a few new variables while retaining maximum variation. However, the new variables are difficult to interpret, because each one is combined with all of the original input variables and has obscure semantics. Under the umbrella of Bayesian data analysis, this paper presents a new prior to explicitly regularize combinations of input variables. In particular, the prior penalizes pair-wise products of the coefficients of PCA and encourages a sparse model. Compared to the commonly used mmbl1-regularizer, the proposed prior encourages the sparsity pattern in the resultant coefficients to be consistent with the intrinsic groups in the original input variables. Moreover, the proposed prior can be explained as recovering a robust estimation of the covariance matrix for PCA. The proposed model is suited for analyzing visual data, where it encourages the output variables to correspond to meaningful parts in the data. We demonstrate the characteristics and effectiveness of the proposed technique through experiments on both synthetic and real data. © 1992-2012 IEEE.","Feature extraction; principal component analysis (PCA); sparse learning","Bayesian data analysis; Data representations; Input variables; Output variables; Regularizer; Robust estimation; sparse learning; Sparsity patterns; Synthetic and real data; Visual data; Covariance matrix; Feature extraction; Semantics; Principal component analysis; article; artificial intelligence; Bayes theorem; computer simulation; face; factual database; finger; histology; human; image processing; methodology; principal component analysis; Artificial Intelligence; Bayes Theorem; Computer Simulation; Databases, Factual; Face; Fingers; Humans; Image Processing, Computer-Assisted; Principal Component Analysis",Article,Scopus,2-s2.0-84869422991
"Gil D., Girela J.L., De Juan J., Gomez-Torres M.J., Johnsson M.","Predicting seminal quality with artificial intelligence methods",2012,"Expert Systems with Applications",22,10.1016/j.eswa.2012.05.028,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864494677&doi=10.1016%2fj.eswa.2012.05.028&partnerID=40&md5=feb8d2ee2104f7da0465589b6bcc03f1","Fertility rates have dramatically decreased in the last two decades, especially in men. It has been described that environmental factors, as well as life habits, may affect semen quality. Artificial intelligence techniques are now an emerging methodology as decision support systems in medicine. In this paper we compare three artificial intelligence techniques, decision trees, Multilayer Perceptron and Support Vector Machines, in order to evaluate their performance in the prediction of the seminal quality from the data of the environmental factors and lifestyle. To do that we collect data by a normalized questionnaire from young healthy volunteers and then, we use the results of a semen analysis to asses the accuracy in the prediction of the three classification methods mentioned above. The results show that Multilayer Perceptron and Support Vector Machines show the highest accuracy, with prediction accuracy values of 86% for some of the seminal parameters. In contrast decision trees provide a visual and illustrative approach that can compensate the slightly lower accuracy obtained. In conclusion artificial intelligence methods are a useful tool in order to predict the seminal profile of an individual from the environmental factors and life habits. From the studied methods, Multilayer Perceptron and Support Vector Machines are the most accurate in the prediction. Therefore these tools, together with the visual help that decision trees offer, are the suggested methods to be included in the evaluation of the infertile patient. © 2012 Elsevier Ltd. All rights reserved.","Artificial neural network; Decision support system; Decision trees; Diagnosis; Expert system; Male fertility potential; Semen quality; Support Vector Machines","Artificial intelligence methods; Artificial intelligence techniques; Classification methods; Environmental factors; Male fertility; Multi layer perceptron; Prediction accuracy; Body fluids; Decision support systems; Decision trees; Diagnosis; Expert systems; Forecasting; Forestry; Neural networks; Support vector machines; Quality control; Artificial Intelligence; Decision Making; Diagnosis; Fertility; Forestry; Neural Networks; Quality Control",Article,Scopus,2-s2.0-84864494677
"Meyer-Delius D., Beinhofer M., Burgard W.","Occupancy grid models for robot mapping in changing environments",2012,"Proceedings of the National Conference on Artificial Intelligence",22,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868279944&partnerID=40&md5=5f54929b0b77abe99f8f337b76a944e7","The majority of existing approaches to mobile robot mapping assumes that the world is static, which is generally not justified in real-world applications. However, in many navigation tasks including trajectory planning, surveillance, and coverage, accurate maps are essential for the effective behavior of the robot. In this paper we present a probabilistic grid-based approach for modeling changing environments. Our method represents both, the occupancy and its changes in the corresponding area where the dynamics are characterized by the state transition probabilities of a Hidden Markov Model. We apply an offline and an online technique to learn the parameters from observed data. The advantage of the online approach is that it can dynamically adapt the parameters and at the same time does not require storing the complete observation sequences. Experimental results obtained with data acquired by real robots demonstrate that our model is well-suited for representing changing environments. Further results show that our technique can be used to substantially improve the effectiveness of path planning procedures. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Changing environment; Grid-based approach; Navigation tasks; Observed data; Occupancy grids; Offline; Online technique; Planning procedure; Real robot; Real-world application; Robot mapping; State transition probabilities; Trajectory Planning; Artificial intelligence; Hidden Markov models; Motion planning; Robots; Robot programming",Conference Paper,Scopus,2-s2.0-84868279944
"Bienvenu M.","On the complexity of consistent query answering in the presence of simple ontologies",2012,"Proceedings of the National Conference on Artificial Intelligence",22,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868288902&partnerID=40&md5=57c27e0ce105f93e6d9c5a63a98de9be","Consistent query answering is a standard approach for producing meaningful query answers when data is inconsistent. Recent work on consistent query answering in the presence of ontologies has shown this problem to be intractable in data complexity even for ontologies expressed in lightweight description logics. In order to better understand the source of this intractability, we investigate the complexity of consistent query answering for simple ontologies consisting only of class subsumption and class disjointness axioms. We show that for conjunctive queries with at most one quantified variable, the problem is first-order expressible; for queries with at most two quantified variables, the problem has polynomial data complexity but may not be first-order expressible; and for three quantified variables, the problem may become co-NP-hard in data complexity. For queries having at most two quantified variables, we further identify a necessary and sufficient condition for first-order expressibility. In order to be able to handle arbitrary conjunctive queries, we propose a novel inconsistency-tolerant semantics and show that under this semantics, first-order expressibility is always guaranteed. We conclude by extending our positive results to DL-Lite ontologies without inverse. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Conjunctive queries; Consistent query answering; Data complexity; Description logic; Disjointness; Expressibility; First-order; Polynomial data; Sufficient conditions; Data description; Semantics; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868288902
"Bouzrara K., Garna T., Ragot J., Messaoud H.","Decomposition of an ARX model on Laguerre orthonormal bases",2012,"ISA Transactions",22,10.1016/j.isatra.2012.06.005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866415900&doi=10.1016%2fj.isatra.2012.06.005&partnerID=40&md5=6e93dbecfb689dd98d6b32f7033ef2f3","In this paper, we propose a new reduced complexity model by expanding a discrete-time ARX model on Laguerre orthonormal bases. To ensure an efficient complexity reduction, the coefficients associated to the input and the output of the ARX model are expanded on independent Laguerre bases, to develop a new black-box linear ARX-Laguerre model with filters on model input and output. The parametric complexity reduction with respect to the classical ARX model is proved theoretically. The structure and parameter identification of the ARX-Laguerre model is achieved by a new proposed approach which consists in solving an optimization problem built from the ARX model without using system input/output observations. The performances of the resulting ARX-Laguerre model and the proposed identification approach are illustrated by numerical simulations and validated on benchmark manufactured by Feedback known as Process Trainer PT326. A possible extension of the proposed model to a multivariable process is formulated. © 2012 ISA.","ARX model; Laguerre bases; Optimization; Reduced parametric complexity","ARX model; Black boxes; Complexity reduction; Identification approach; Input/output; Laguerre; Laguerre basis; Model inputs; Multivariable process; Optimization problems; Orthonormal basis; Reduced complexity; Reduced parametric complexity; Computer applications; Natural sciences; Optimization; algorithm; article; artificial intelligence; automated pattern recognition; computer simulation; decision support system; methodology; theoretical model; Algorithms; Artificial Intelligence; Computer Simulation; Decision Support Techniques; Models, Theoretical; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84866415900
"Punyasena S.W., Tcheng D.K., Wesseln C., Mueller P.G.","Classifying black and white spruce pollen using layered machine learning",2012,"New Phytologist",22,10.1111/j.1469-8137.2012.04291.x,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867401961&doi=10.1111%2fj.1469-8137.2012.04291.x&partnerID=40&md5=044472f68d2a4b2e59004cadd6b28783","Pollen is among the most ubiquitous of terrestrial fossils, preserving an extended record of vegetation change. However, this temporal continuity comes with a taxonomic tradeoff. Analytical methods that improve the taxonomic precision of pollen identifications would expand the research questions that could be addressed by pollen, in fields such as paleoecology, paleoclimatology, biostratigraphy, melissopalynology, and forensics. We developed a supervised, layered, instance-based machine-learning classification system that uses leave-one-out bias optimization and discriminates among small variations in pollen shape, size, and texture. We tested our system on black and white spruce, two paleoclimatically significant taxa in the North American Quaternary. We achieved &gt; 93% grain-to-grain classification accuracies in a series of experiments with both fossil and reference material. More significantly, when applied to Quaternary samples, the learning system was able to replicate the count proportions of a human expert (R2 = 0.78, P = 0.007), with one key difference - the machine achieved these ratios by including larger numbers of grains with low-confidence identifications. Our results demonstrate the capability of machine-learning systems to solve the most challenging palynological classification problem, the discrimination of congeneric species, extending the capabilities of the pollen analyst and improving the taxonomic resolution of the palynological record. © 2012 New Phytologist Trust.","Automation; Classification; Machine learning; Palynology; Picea glauca; Picea mariana; Quaternary","automation; biostratigraphy; coniferous tree; forensic science; fossil record; optimization; paleoclimate; paleoecology; palynology; pollen; precision; preservation; Quaternary; taxonomy; vegetation dynamics; article; artificial intelligence; classification; computer program; fossil; histology; image processing; Internet; methodology; physiology; pollen; reproducibility; spruce; Artificial Intelligence; Fossils; Image Processing, Computer-Assisted; Internet; Picea; Pollen; Reproducibility of Results; Software; North America; Picea glauca; Picea mariana",Article,Scopus,2-s2.0-84867401961
"McDaniel M.W., Nishihata T., Brooks C.A., Salesses P., Iagnemma K.","Terrain classification and identification of tree stems using ground-based LiDAR",2012,"Journal of Field Robotics",22,10.1002/rob.21422,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867101105&doi=10.1002%2frob.21422&partnerID=40&md5=500412d956204a366790ee0430410b73","To operate autonomously in forested terrain, unmanned ground vehicles must be able to identify the load-bearing surface of the terrain (i.e., the ground) and obstacles in the environment. To travel long distances, they must be able to track their position even when the forest canopy obstructs GPS signals, e.g., by tracking progress relative to tree stems. This paper presents a novel, robust approach for modeling the ground plane and tree stems in forests from a single viewpoint using a lightweight LiDAR scanner. Ground plane identification is implemented using a two-stage approach. The first stage, a local height-based filter, discards most nonground points. The second stage, based on a support vector machine classifier, identifies which of the remaining points belong to the ground. Main tree stems are modeled as cylinders or cones to estimate the diameter 130 cm above the ground plane. To fit these models, candidate main stem data are selected by finding points approximately 130 cm above the ground. These points are clustered into separate point clouds for each stem. Cylinders and cones are fit to each point cloud, and heuristic filters identify which fits correspond to tree stems. Experimental results from five forested environments demonstrate the effectiveness of this approach. For ground plane estimation, the overall classification accuracy was 86.28% with a mean error for the ground height of approximately 4.7 cm. For stem estimation, up to 50% of the main stems were accurately modeled using cones, with a root mean square diameter error of 13.2 cm.© 2012 Wiley Periodicals, Inc. Copyright © 2012 Wiley Periodicals, Inc.",,"Classification accuracy; Diameter error; Forest canopies; Forested environment; GPS signals; Ground based; Ground planes; Load-bearing; Mean errors; Point cloud; Robust approaches; Root Mean Square; Terrain classification; Tree stems; Unmanned ground vehicles; Cylinders (shapes); Estimation; Intelligent vehicle highway systems; Landforms; Optical radar; Forestry; Artificial Intelligence; Cylinders; Forests; Radar; Vehicles",Article,Scopus,2-s2.0-84867101105
"Li F.-D., Wu M., He Y., Chen X.","Optimal control in microgrid using multi-agent reinforcement learning",2012,"ISA Transactions",22,10.1016/j.isatra.2012.06.010,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866406146&doi=10.1016%2fj.isatra.2012.06.010&partnerID=40&md5=a78dc29fd783ddc2a331a268e2cf1f8d","This paper presents an improved reinforcement learning method to minimize electricity costs on the premise of satisfying the power balance and generation limit of units in a microgrid with grid-connected mode. Firstly, the microgrid control requirements are analyzed and the objective function of optimal control for microgrid is proposed. Then, a state variable Average Electricity Price Trend which is used to express the most possible transitions of the system is developed so as to reduce the complexity and randomicity of the microgrid, and a multi-agent architecture including agents, state variables, action variables and reward function is formulated. Furthermore, dynamic hierarchical reinforcement learning, based on change rate of key state variable, is established to carry out optimal policy exploration. The analysis shows that the proposed method is beneficial to handle the problem of curse of dimensionality and speed up learning in the unknown large-scale world. Finally, the simulation results under JADE (Java Agent Development Framework) demonstrate the validity of the presented method in optimal control for a microgrid with grid-connected mode. © 2012 ISA.","Distributed generation; MAXQ; Microgrid; Multi-agent system; Reinforcement learning","Curse of dimensionality; Electricity costs; Electricity prices; Grid-connected modes; Hierarchical reinforcement learning; Java agent development framework; MAXQ; Micro grid; Microgrid control; Multi agent system (MAS); Multi-agent reinforcement learning; Multiagent architecture; Objective functions; Optimal controls; Optimal policies; Power balance; Randomicity; Reinforcement learning method; Reward function; State variables; Control; Multi agent systems; Reinforcement learning; Silicate minerals; Distributed power generation; algorithm; article; artificial intelligence; automated pattern recognition; computer simulation; decision support system; electricity; energy transfer; feedback system; methodology; theoretical model; Algorithms; Artificial Intelligence; Computer Simulation; Decision Support Techniques; Electricity; Energy Transfer; Feedback; Models, Theoretical; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84866406146
"Hong Z., Mei X., Tao D.","Dual-force metric learning for robust distracter-resistant tracker",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",22,10.1007/978-3-642-33718-5_37,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867860674&doi=10.1007%2f978-3-642-33718-5_37&partnerID=40&md5=c33dd087044ebee1b8600f55e32d8fbc","In this paper, we propose a robust distracter-resistant tracking approach by learning a discriminative metric that adaptively learns the importance of features on-the-fly. The proposed metric is elaborately designed for the tracking problem by forming a margin objective function which systematically includes distance margin maximization and reconstruction error constraint that acts as a force to push distracters away from the positive space and into the negative space. Due to the variety of negative samples in the tracking problem, we specifically introduce the similarity propagation technique that gives distracters a second force from the negative space. Consequently, the discriminative metric obtained helps to preserve the most discriminative information to separate the target from distracters while ensuring the stability of the optimal metric. We seamlessly combine it with the popular L1 minimization tracker. Our tracker is therefore not only resistant to distracters, but also inherits the merit of occlusion robustness from the L1 tracker. Quantitative comparisons with several state-of-the-art algorithms have been conducted in many challenging video sequences. The results show that our method resists distracters excellently and achieves superior performance. © 2012 Springer-Verlag.","distance metric; distracter; similarity propagation; Visual tracking","Distance metrics; distracter; Metric learning; Objective functions; On-the-fly; Quantitative comparison; Reconstruction error; Similarity propagation; State-of-the-art algorithms; Tracking problem; Video sequences; Visual Tracking; Artificial intelligence; Computer vision",Conference Paper,Scopus,2-s2.0-84867860674
"Traum D., Devault D., Lee J., Wang Z., Marsella S.","Incremental dialogue understanding and feedback for multiparty, multimodal conversation",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",22,10.1007/978-3-642-33197-8-29,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867526350&doi=10.1007%2f978-3-642-33197-8-29&partnerID=40&md5=59e7544a0078f123f4dda988f9cfd583","In order to provide comprehensive listening behavior, virtual humans engaged in dialogue need to incrementally listen, interpret, understand, and react to what someone is saying, in real time, as they are saying it. In this paper, we describe an implemented system for engaging in multiparty dialogue, including incremental understanding and a range of feedback. We present an FML message extension for feedback in multipary dialogue that can be connected to a feedback realizer. We also describe how the important aspects of that message are calculated by different modules involved in partial input processing as a speaker is talking in a multiparty dialogue. © 2012 Springer-Verlag Berlin Heidelberg.",,"Incremental dialogue understanding; Multi-party dialogues; Multimodal conversation; Real time; Virtual humans; Artificial intelligence; Intelligent virtual agents",Conference Paper,Scopus,2-s2.0-84867526350
"Aleksendrić D., Jakovljević Ž., Ćirović V.","Intelligent control of braking process",2012,"Expert Systems with Applications",22,10.1016/j.eswa.2012.04.076,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861822455&doi=10.1016%2fj.eswa.2012.04.076&partnerID=40&md5=11a42bd5f16afe01b572c3194ade3316","Intelligent modeling, prediction and control of the braking process are not an easy task if using classical modeling techniques, regarding its complexity. In this paper, the new approach has been proposed for easy and effective monitoring, modeling, prediction, and control of the braking process i.e. the brake performance during a braking cycle. The context based control of the disc brake actuation pressure was used for improving the dynamic control of braking process versus influence of the previous and current values of the disc brake actuation pressure, the vehicle speed, and the brake interface temperature. For these purposes, two different dynamic neural models have been developed and integrated into the microcontroller. Microcontrollers are resource intensive and cost effective platforms that offer possibilities to associate with commonly used artificial intelligence techniques. The neural models, based on recurrent dynamic neural networks, are implemented in 8-bit CMOS microcontroller for control of the disc brake actuation pressure during a braking cycle. The first neural model was used for modeling and prediction of the braking process output (braking torque). Based on such acquired knowledge about the real brake operation, the inverse neural model has been developed which was able to predict the brake actuation pressure needed for achieving previously selected (desired) braking torque value in accordance with the previous and current influence of the pressure, speed, and the brake interface temperature. Both neural models have had inherent abilities for on-line learning and prediction during each braking cycle and an intelligent adaptation to the change of influences of pressure, speed, and temperature on the braking process. © 2012 Published by Elsevier Ltd.","Braking process; Intelligent control; Microcontroller","Artificial intelligence techniques; Brake actuation; Brake performance; Braking torque; Classical modeling; Context-based; Cost effective; Disc brakes; Dynamic controls; Dynamic neural networks; Intelligent adaptation; Intelligent modeling; Interface temperatures; Neural models; Online learning; Prediction and control; Process output; Vehicle speed; Actuator disks; Braking; Braking performance; Controllers; Forecasting; Grid computing; Intelligent control; Microcontrollers; Neural networks; Process control; Brakes",Article,Scopus,2-s2.0-84861822455
"Li G., Chang K., Hoi S.C.H.","Multiview semi-supervised learning with consensus",2012,"IEEE Transactions on Knowledge and Data Engineering",22,10.1109/TKDE.2011.160,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866889623&doi=10.1109%2fTKDE.2011.160&partnerID=40&md5=f87fa5c5f137be569ce33fd0dccdd52d","Obtaining high-quality and up-to-date labeled data can be difficult in many real-world machine learning applications. Semi-supervised learning aims to improve the performance of a classifier trained with limited number of labeled data by utilizing the unlabeled ones. This paper demonstrates a way to improve the transductive SVM, which is an existing semi-supervised learning algorithm, by employing a multiview learning paradigm. Multiview learning is based on the fact that for some problems, there may exist multiple perspectives, so called views, of each data sample. For example, in text classification, the typical view contains a large number of raw content features such as term frequency, while a second view may contain a small but highly informative number of domain specific features. We propose a novel two-view transductive SVM that takes advantage of both the abundant amount of unlabeled data and their multiple representations to improve classification result. The idea is straightforward: train a classifier on each of the two views of both labeled and unlabeled data, and impose a global constraint requiring each classifier to assign the same class label to each labeled and unlabeled sample. We also incorporate manifold regularization, a kind of graph-based semi-supervised learning method into our framework. The proposed two-view transductive SVM was evaluated on both synthetic and real-life data sets. Experimental results show that our algorithm performs up to 10 percent better than a single-view learning approach, especially when the amount of labeled data is small. The other advantage of our two-view semi-supervised learning approach is its significantly improved stability, which is especially useful when dealing with noisy data in real-world applications. © 2012 IEEE.","Artificial intelligence; learning systems; multiview learning; semi-supervised learning; support vector machines","Class labels; Classification results; Data sample; Domain specific; Global constraints; Graph-based; High quality; Labeled and unlabeled data; Labeled data; Learning approach; Machine learning applications; Multi-view learning; Multi-views; Multiple representation; Noisy data; Real life datasets; Real-world application; Semi-supervised learning; Semi-supervised learning methods; Term Frequency; Text classification; Transductive SVM; Unlabeled data; Unlabeled samples; Artificial intelligence; Learning algorithms; Learning systems; Support vector machines; Supervised learning",Article,Scopus,2-s2.0-84866889623
"Lange J., Tuosto E.","Synthesising choreographies from local session types",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",22,10.1007/978-3-642-32940-1_17,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866690167&doi=10.1007%2f978-3-642-32940-1_17&partnerID=40&md5=7c1a44b490b81eec4bf4090166918e40","Designing and analysing multiparty distributed interactions can be achieved either by means of a global view (e.g. in choreography-based approaches) or by composing available computational entities (e.g. in service orchestration). This paper proposes a typing systems which allows, under some conditions, to synthesise a choreography (i.e. a multiparty global type) from a set of local session types which describe end-point behaviours (i.e. local types). © 2012 Springer-Verlag.",,"Computational entities; Distributed interaction; End points; Global view; Service orchestration; Session types; Typing systems; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84866690167
"Cheema M.A., Zhang W., Lin X., Zhang Y.","Efficiently processing snapshot and continuous reverse k nearest neighbors queries",2012,"VLDB Journal",22,10.1007/s00778-012-0265-y,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866438914&doi=10.1007%2fs00778-012-0265-y&partnerID=40&md5=cefe4ac511897a5764d8540f620bccda","Given a set of objects and a query q, a point p is called the reverse k nearest neighbor (RkNN) of q if q is one of the k closest objects of p. In this paper, we introduce the concept of influence zone that is the area such that every point inside this area is the RkNN of q and every point outside this area is not the RkNN. The influence zone has several applications in location-based services, marketing and decision support systems. It can also be used to efficiently process RkNN queries. First, we present efficient algorithm to compute the influence zone. Then, based on the influence zone, we present efficient algorithms to process RkNN queries that significantly outperform existing best-known techniques for both the snapshot and continuous RkNN queries. We also present a detailed theoretical analysis to analyze the area of the influence zone and IO costs of our RkNN processing algorithms. Our experiments demonstrate the accuracy of our theoretical analysis. This paper is an extended version of our previous work (Cheema et al. in Proceedings of ICDE, pp. 577-588, 2011). We make the following new contributions in this extended version: (1) we conduct a rigorous complexity analysis and show that the complexity of one of our proposed algorithms in Cheema et al. (Proceedings of ICDE, pp. 577-588, 2011) can be reduced from O(m 2) to O(km) where m &gt; k is the number of objects used to compute the influence zone, (2) we show that our techniques can be applied to dimensionality higher than two, and (3) we present efficient techniques to handle data updates. © 2012 Springer-Verlag.","Continuous queries; Reverse nearest neighbors; RNN; Snapshot queries; Spatial queries","Continuous queries; Reverse nearest neighbors; RNN; Snapshot queries; Spatial queries; Artificial intelligence; Decision support systems; Location based services; Motion compensation; Algorithms",Article,Scopus,2-s2.0-84866438914
"Quijano-Sánchez L., Bridge D., Díaz-Agudo B., Recio-García J.A.","A case-based solution to the cold-start problem in group recommenders",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",22,10.1007/978-3-642-32986-9_26,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866660539&doi=10.1007%2f978-3-642-32986-9_26&partnerID=40&md5=44ecba3ff57bebf2e803077fa21cd601","We extend a group recommender system with a case base of previous group recommendation events. We show that this offers a potential solution to the cold-start problem. Suppose a group recommendation is sought but one of the group members is a new user who has few item ratings. We can copy ratings into this user's profile from the profile of the most similar user in the most similar group from the case base. In other words, we copy ratings from a user who played a similar role in some previous group event. We show that copying in this way, i.e. conditioned on groups, is superior to copying nothing and also superior to copying ratings from the most similar user known to the system. © 2012 Springer-Verlag.",,"Case base; Cold start problems; Group members; Group recommendations; Group recommender systems; Potential solutions; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84866660539
"Christakis M., Müller P., Wüstholz V.","Collaborative verification and testing with explicit assumptions",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",22,10.1007/978-3-642-32759-9_13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866004623&doi=10.1007%2f978-3-642-32759-9_13&partnerID=40&md5=35c084aa83223520affc9bff370e52a3","Many mainstream static code checkers make a number of compromises to improve automation, performance, and accuracy. These compromises include not checking certain program properties as well as making implicit, unsound assumptions. Consequently, the results of such static checkers do not provide definite guarantees about program correctness, which makes it unclear which properties remain to be tested. We propose a technique for collaborative verification and testing that makes compromises of static checkers explicit such that they can be compensated for by complementary checkers or testing. Our experiments suggest that our technique finds more errors and proves more properties than static checking alone, testing alone, and combinations that do not explicitly document the compromises made by static checkers. Our technique is also useful to obtain small test suites for partially-verified programs. © 2012 Springer-Verlag.",,"Program correctness; Program properties; Static checking; Static codes; Verification and testing; Artificial intelligence; Software testing",Conference Paper,Scopus,2-s2.0-84866004623
"Sermpinis G., Laws J., Karathanasopoulos A., Dunis C.L.","Forecasting and trading the EUR/USD exchange rate with Gene Expression and Psi Sigma Neural Networks",2012,"Expert Systems with Applications",22,10.1016/j.eswa.2012.02.022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859211028&doi=10.1016%2fj.eswa.2012.02.022&partnerID=40&md5=333385764758f749100b079765e6be82","The motivation for this paper is to investigate the use of two promising classes of artificial intelligence models, the Psi Sigma Neural Network (PSI) and the Gene Expression algorithm (GEP), when applied to the task of forecasting and trading the EUR/USD exchange rate. This is done by benchmarking their results with a Multi-Layer Perceptron (MLP), a Recurrent Neural Network (RNN), a genetic programming algorithm (GP), an autoregressive moving average model (ARMA) plus a naïve strategy. We also examine if the introduction of a time-varying leverage strategy can improve the trading performance of our models. © 2012 Elsevier Ltd. All rights reserved.","Genetic Expression; Genetic programming; Multi-Layer Perceptron networks; Psi Sigma Networks; Quantitative trading strategies; Recurrent networks","Autoregressive moving average model; Exchange rates; Genetic expressions; Multi layer perceptron; Multi-layer perceptron networks; Programming algorithms; Recurrent networks; Time varying; Trading strategies; Algorithms; Artificial intelligence; Gene expression; Genetic programming; Network layers; Recurrent neural networks; Commerce",Article,Scopus,2-s2.0-84859211028
"Ciompi F., Pujol O., Gatta C., Alberti M., Balocco S., Carrillo X., Mauri-Ferre J., Radeva P.","HoliMAb: A holistic approach for Media-Adventitia border detection in intravascular ultrasound",2012,"Medical Image Analysis",22,10.1016/j.media.2012.06.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866131676&doi=10.1016%2fj.media.2012.06.008&partnerID=40&md5=9aa872ffac23c12fc5ce1fdb4defa453","We present a fully automatic methodology for the detection of the Media-Adventitia border (MAb) in human coronary artery in Intravascular Ultrasound (IVUS) images. A robust border detection is achieved by means of a holistic interpretation of the detection problem where the target object, i.e. the media layer, is considered as part of the whole vessel in the image and all the relationships between tissues are learnt. A fairly general framework exploiting multi-class tissue characterization as well as contextual information on the morphology and the appearance of the tissues is presented. The methodology is (i) validated through an exhaustive comparison with both Inter-observer variability on two challenging databases and (ii) compared with state-of-the-art methods for the detection of the MAb in IVUS. The obtained averaged values for the mean radial distance and the percentage of area difference are 0.211. mm and 10.1%, respectively. The applicability of the proposed methodology to clinical practice is also discussed. © 2012 Elsevier B.V.","Error-correcting output codes; Holistic segmentation; Intravascular ultrasound; Media-Adventitia border detection; Multi-Scale Stacked Sequential Learning","Border detection; Clinical practices; Contextual information; Detection problems; Error correcting output code; Holistic approach; Human coronary arteries; Interobserver variability; Intravascular ultrasound; Intravascular ultrasound images; Media layers; Multi-class; Radial distance; Sequential learning; State-of-the-art methods; Target object; Tissue characterization; Histology; Tissue; Ultrasonics; Image segmentation; analytic method; article; blood vessel wall; clinical practice; coronary artery; guide wire; HoliMAb; human; image analysis; intravascular ultrasound; media adventitia border; priority journal; Adventitia; Algorithms; Artificial Intelligence; Coronary Artery Disease; Coronary Vessels; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Tunica Media; Ultrasonography, Interventional",Article,Scopus,2-s2.0-84866131676
"Genuis S.K.","Constructing ""sense"" from evolving health information: A qualitative investigation of information seeking and sense making across sources",2012,"Journal of the American Society for Information Science and Technology",22,10.1002/asi.22691,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864399604&doi=10.1002%2fasi.22691&partnerID=40&md5=3c52825a1a69f2ca07659149eae2348f","Focusing on information behavior in a context where medical evidence is explicitly evolving (management of the menopause transition), this investigation explored how women interact with and make sense of uncertain health information mediated by formal and informal sources. Based on interviews with 28 information seekers and 12 health professionals (HPs), findings demonstrate that participants accessed and valued a wide range of information sources, moved fluidly between formal and informal sources, and trust was strengthened through interaction and referral between sources. Participants were motivated to seek information to prepare for formal encounters with HPs, evaluate and/or supplement information already gathered, establish that they were ""normal,"" understand and address the physical embodiment of their experiences, and prepare for future information needs. Findings revealed four strategies used to construct sense from health information mediated by the many information sources encountered and accessed on an everyday basis: women assumed analytic and experiential ""postures""; they valued social contexts for learning and knowledge construction; information consistency was used as a heuristic representing accuracy and credibility; and an important feature of sense making was source complementarity. Implications for health information literacy and patient education are discussed. © 2012 ASIS&T.","information channels; uncertainty; user studies","Health informations; Health professionals; Information behavior; Information channels; Information need; Information seeking; Information sources; Knowledge construction; Patient education; Sense making; Social context; uncertainty; User study; Artificial intelligence; Software engineering; Information science",Article,Scopus,2-s2.0-84864399604
"Cerreta M., Panaro S., Cannatella D.","Multidimensional spatial decision-making process: Local shared values in action",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",22,10.1007/978-3-642-31075-1_5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863919340&doi=10.1007%2f978-3-642-31075-1_5&partnerID=40&md5=61f304ad73a746216600c9f8f1fa3898","This paper is about developing a methodological framework for a multidimensional spatial decision-making process oriented to the identification of a territorial transformation strategy reflecting shared values. Through the empirical investigation in an operative case study, the Avellino-Rocchetta S. Antonio railway line, in the South of Italy, an integrated evaluative approach implemented in a SDSS can make us go beyond space and hierarchical limits. Taking into account the different multidimensional components of decision-making process, making clear the weights and recognizing the different priorities, fit and situated strategies have been identified, according to an interactive and dynamic dialogue among expertise and local communities. © 2012 Springer-Verlag.","Analytic Hierarchy Process (AHP); Multi- Criteria Analysis (MCA); Soft Systems Methodology (SSM); Spatial Decision Support System (SDSS); Spatial indicators","Decision making process; Empirical investigation; Local community; Methodological frameworks; Multi- Criteria Analysis (MCA); Railway line; Shared values; Soft systems methodology; Spatial decision support systems; Spatial indicators; Artificial intelligence; Decision support systems",Conference Paper,Scopus,2-s2.0-84863919340
"Longhi S., Marzioni D., Alidori E., Di Buò G., Prist M., Grisostomi M., Pirro M.","Solid waste management architecture using wireless sensor network technology",2012,"2012 5th International Conference on New Technologies, Mobility and Security - Proceedings of NTMS 2012 Conference and Workshops",22,10.1109/NTMS.2012.6208764,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863919767&doi=10.1109%2fNTMS.2012.6208764&partnerID=40&md5=ab74a9233924d9f09e597dde0778b485","In many application fields such as home, industry, environment and health, different Wireless Sensor Network (WSN) applications have been developed to solve management problems with smart implementations. This approach can be applied in the filed of solid waste management. In this paper a new architecture is proposed with the aim to improve the on-site handling and transfer optimization in the waste management process. The system architecture is based on sensor nodes and makes use of Data Transfer Nodes (DTN) in order to provide to a remote server the retrieved data measurements from the garbage bins filling. A remote monitoring solution has been implemented, providing user the possibility to interact with the system by using a web browser. Several activities with the aim to provide a Decision Support System (DSS) able to find solutions for resources organization problems linked to solid waste management have been started. © 2012 IEEE.","Decision Support System; Remote Monitoring; Solid waste management; Wireless Sensor Network","Application fields; Data measurements; Management problems; Management process; Remote monitoring; Remote servers; System architectures; Wireless sensor; Architecture; Artificial intelligence; Data transfer; Decision support systems; Remote control; Solid wastes; Waste management; Wireless sensor networks; Sensor nodes",Conference Paper,Scopus,2-s2.0-84863919767
"Manivannan K., Aggarwal P., Devabhaktuni V., Kumar A., Nims D., Bhattacharya P.","Particulate matter characterization by gray level co-occurrence matrix based support vector machines",2012,"Journal of Hazardous Materials",22,10.1016/j.jhazmat.2012.04.056,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861457954&doi=10.1016%2fj.jhazmat.2012.04.056&partnerID=40&md5=0546477b4913fff54473a91f0b0452de","An efficient and highly reliable automatic selection of optimal segmentation algorithm for characterizing particulate matter is presented in this paper. Support vector machines (SVMs) are used as a new self-regulating classifier trained by gray level co-occurrence matrix (GLCM) of the image. This matrix is calculated at various angles and the texture features are evaluated for classifying the images. Results show that the performance of GLCM-based SVMs is drastically improved over the previous histogram-based SVMs. Our proposed GLCM-based approach of training SVM predicts a robust and more accurate segmentation algorithm than the standard histogram technique, as additional information based on the spatial relationship between pixels is incorporated for image classification. Further, the GLCM-based SVM classifiers were more accurate and required less training data when compared to the artificial neural network (ANN) classifiers. © 2012 Elsevier B.V.","Gray level co-occurrence matrix; Image segmentation; Particulate matter; Support vector machines","Automatic selection; Gray level co-occurrence matrix; Histogram technique; Optimal segmentation algorithm; Particulate Matter; Segmentation algorithms; Spatial relationships; SVM classifiers; Texture features; Training data; Classification (of information); Graphic methods; Image segmentation; Image texture; Neural networks; Support vector machines; politef; algorithm; artificial neural network; data set; histogram; image classification; particulate matter; pixel; article; artificial neural network; gray level co occurrence matrix; histogram; image analysis; mathematical model; particle size; particulate matter; scanning electron microscopy; sensitivity and specificity; support vector machine; Air Pollutants; Algorithms; Artificial Intelligence; Environmental Monitoring; Microscopy, Electron, Scanning; Neural Networks (Computer); Particle Size; Particulate Matter; Signal Processing, Computer-Assisted",Article,Scopus,2-s2.0-84861457954
"Gromov V.A., Shulga A.N.","Chaotic time series prediction with employment of ant colony optimization",2012,"Expert Systems with Applications",22,10.1016/j.eswa.2012.01.171,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858339896&doi=10.1016%2fj.eswa.2012.01.171&partnerID=40&md5=a341a067052d74ac64decc6b55fa7aaa","In this study, the novel method to predict chaotic time series is proposed. The method employs the ant colony optimization paradigm to analyze topological structure of the attractor behind the given time series and to single out the typical sequences corresponding to the different part of the attractor. The typical sequences are used to predict the time series values. The method was applied to time series generated by the Lorenz system, the Mackey-Glass equation, and weather time series as well. The method is able to provide robust prognosis to the periods comparable with the horizon of prediction. © 2012 Elsevier Ltd. All rights reserved.","Ant colony optimization; Chaotic time series prediction","Ant Colony Optimization (ACO); Chaotic time series; Chaotic time series prediction; Lorenz system; Mackey-Glass equations; Topological structure; Typical sequences; Algorithms; Artificial intelligence; Diagnosis; Forecasting; Time series",Article,Scopus,2-s2.0-84858339896
"Skibbe H., Reisert M., Schmidt T., Brox T., Ronneberger O., Burkhardt H.","Fast rotation invariant 3D feature computation utilizing efficient local neighborhood operators",2012,"IEEE Transactions on Pattern Analysis and Machine Intelligence",22,10.1109/TPAMI.2011.263,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862665345&doi=10.1109%2fTPAMI.2011.263&partnerID=40&md5=db4518085293621fc9214d2e41067150","We present a method for densely computing local rotation invariant image descriptors in volumetric images. The descriptors are based on a transformation to the harmonic domain, which we compute very efficiently via differential operators. We show that this fast voxelwise computation is restricted to a family of basis functions that have certain differential relationships. Building upon this finding, we propose local descriptors based on the Gaussian Laguerre and spherical Gabor basis functions and show how the coefficients can be computed efficiently by recursive differentiation. We exemplarily demonstrate the effectiveness of such dense descriptors in a detection and classification task on biological 3D images. In a direct comparison to existing volumetric features, among them 3D SIFT, our descriptors reveal superior performance. © 2012 IEEE.","Gauss-Laguerre functions; local 3D descriptors; rotation invariants; spherical harmonics; Voxel classification","3-D image; Basis functions; Classification tasks; Descriptors; Differential operators; Fast rotation; Feature computation; Gauss-Laguerre; Gaussians; Harmonic domain; Image descriptors; Laguerre; Local descriptors; Rotation invariant; Spherical harmonics; Volumetric features; Volumetric images; Harmonic analysis; Mathematical operators; Three dimensional; Computational efficiency; algorithm; animal; Arabidopsis; article; artificial intelligence; automated pattern recognition; computer simulation; cytology; echography; factual database; image processing; meristem; methodology; normal distribution; plant cell; theoretical model; three dimensional imaging; Algorithms; Animals; Arabidopsis; Artificial Intelligence; Computer Simulation; Databases, Factual; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Meristem; Models, Theoretical; Normal Distribution; Pattern Recognition, Automated; Plant Cells",Article,Scopus,2-s2.0-84862665345
"Niijima S., Shiraishi A., Okuno Y.","Dissecting kinase profiling data to predict activity and understand cross-reactivity of kinase inhibitors",2012,"Journal of Chemical Information and Modeling",22,10.1021/ci200607f,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862018569&doi=10.1021%2fci200607f&partnerID=40&md5=0248452188c567da4e38aaf74bf3251c","The development of selective and multitargeted kinase inhibitors has received much attention, because crossreactivity with unintended targets may cause toxic side effects, while it can also give rise to efficacious multitargeted drugs. Here we describe a deconvolution approach to dissecting kinase profiling data in order to gain knowledge about cross-reactivity of inhibitors from large-scale profiling data. This approach not only enables activity predictions of given compounds on a kinome-wide scale, but also allows to extract residue-fragment pairs that are associated with activity. We demonstrate its effectiveness using a large-scale public chemogenomics data set and also apply our proposed model to a recently published bioactivity data set. We further illustrate that the preference of given compounds for kinases of interest is better understood by residue-fragment pairs, which could provide both biological and chemical insights into cross-reactivity. © 2012 American Chemical Society.",,"Chemogenomics; Cross-reactivity; Data sets; Deconvolution approach; Kinase inhibitors; Toxic side effects; Chemical compounds; Dissection; Drug interactions; Enzymes; ligand; molecular library; protein binding; protein kinase; protein kinase inhibitor; protein kinase; protein kinase inhibitor; algorithm; amino acid sequence; artificial intelligence; binding site; chemical database; chemistry; cluster analysis; human; molecular genetics; molecular library; sensitivity and specificity; statistical model; structure activity relation; article; chemistry; molecular library; protein binding; Algorithms; Amino Acid Sequence; Artificial Intelligence; Binding Sites; Cluster Analysis; Databases, Chemical; Humans; Ligands; Likelihood Functions; Molecular Sequence Data; Protein Binding; Protein Kinase Inhibitors; Protein Kinases; Sensitivity and Specificity; Small Molecule Libraries; Structure-Activity Relationship; Algorithms; Amino Acid Sequence; Artificial Intelligence; Binding Sites; Cluster Analysis; Databases, Chemical; Humans; Ligands; Likelihood Functions; Molecular Sequence Data; Protein Binding; Protein Kinase Inhibitors; Protein Kinases; Sensitivity and Specificity; Small Molecule Libraries; Structure-Activity Relationship",Article,Scopus,2-s2.0-84862018569
"Mallón R., Covelo P., Vieitez A.M.","Improving secondary embryogenesis in Quercus robur: Application of temporary immersion for mass propagation",2012,"Trees - Structure and Function",22,10.1007/s00468-011-0639-6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860434050&doi=10.1007%2fs00468-011-0639-6&partnerID=40&md5=9c388cb9636aba8823b2f24141fcc202","The effects of the culture system used for embryo proliferation were investigated with the aim of improving multiplication rates and somatic embryo quality in two embryogenic lines of Quercus robur derived from mature trees (B-17 and Sainza). Embryo proliferation medium was defined following comparison of five different semi-solid media, and the highest multiplication rates (based on the total number of embryos and number of cotyledonary-shaped embryos) were achieved with medium supplemented with 0. 44 μM benzyladenine for both lines. Embryo proliferation on semi-solid medium was compared with that obtained by a temporary immersion system (TIS), in which four cycles with immersion frequencies of 1 min every 6, 8, 12 or 24 h were tested. TIS promoted a significant increase in proliferated embryo biomass, with the growth index (GI) two and four times higher than in semi-solid medium in B-17 and Sainza genotypes, respectively. An immersion cycle of 1 min every 8 or 12 h produced approximately 700 somatic embryos (B-17) and 1,500 somatic embryos (Sainza) per RITA® bioreactor, with significant differences in the latter genotype with respect to gelled medium. TIS had also a significant effect on somatic embryo synchronization as it enabled a higher production of cotyledonary embryos (90%), which represents increases of 14% (B-17) and 20% (Sainza) with respect to gelled medium. For germination of embryos proliferated in TIS two maturation systems were applied: (1) culture in semi-solid medium containing 6% sorbitol or (2) culture by TIS (without sorbitol) at a frequency of 1 min immersion every 48 h. Germination ability was higher after maturation on sorbitol medium and plantlet conversion occurred in 48% (B-17) and 13% (Sainza) embryos. TIS produced large numbers of well-developed cotyledonary embryos, hence reduced the cost and labor. © 2011 Springer-Verlag.","Pedunculate oak; Proliferation medium; RITA®; Secondary embryogenesis; Somatic embryogenesis; Temporary immersion","Pedunculate oak; Proliferation medium; Secondary embryogenesis; Somatic embryogenesis; Temporary immersions; Alcohols; Cultivation; Gelation; Plant cell culture; Plants (botany); Artificial intelligence; Quercus robur",Article,Scopus,2-s2.0-84860434050
"Szarek A., Korytkowski M., Rutkowski L., Scherer R., Szyprowski J.","Application of neural networks in assessing changes around implant after total hip arthroplasty",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",22,10.1007/978-3-642-29350-4-40,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861047382&doi=10.1007%2f978-3-642-29350-4-40&partnerID=40&md5=28ecafd9256886bfcda3d72ea107c4ce","Bone and joint diseases afflict more and more younger people. This is due to the work habits, quality and intensity of life, diet and individual factors. Hip arthroplasty is a surgery to remove the pain and to allow the patient to return to normal functioning in society. Endoprosthesoplasty brings the desired effect, but the life span of contemporary endoprosthesis is still not satisfactory. Clinical studies have shown that the introduction of the implant to the bone causes a number of changes within the bone - implant contact. The correct prediction of changes around the implant allows to plan the surgery and to identify hazardous areas where bone decalcification and loss of primary stability in implant can occur. © 2012 Springer-Verlag Berlin Heidelberg.",,"Clinical study; Endoprosthesis; Hazardous area; Hip arthroplasties; Individual factors; Joint disease; Life span; Total hip arthroplasty; Arthroplasty; Artificial intelligence; Soft computing; Bone",Conference Paper,Scopus,2-s2.0-84861047382
"Dziwiński P., Bartczuk Ł., Starczewski J.T.","Fully controllable ant colony system for text data clustering",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",22,10.1007/978-3-642-29353-5_23,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860689502&doi=10.1007%2f978-3-642-29353-5_23&partnerID=40&md5=f2288c9dcf5a3f83c1e575d5a9fec01b","The paper presents a new Fully Controllable Ant Colony Algorithm (FCACA) for the clustering of the text documents in vector space. The proposed new FCACA is a modified version of the Lumer and Faieta Ant Colony Algorithm (LF-ACA). The algorithm introduced new version of the basic heuristic decision function significantly improves the convergence and greater control over the process of the grouping data. The proposed solution was shown in a text example proving efficiency of the proposed solution in comparison with other grouping algorithms. © 2012 Springer-Verlag.",,"Ant colony algorithms; Ant colony systems; Grouping algorithm; Heuristic decisions; Text data; Text document; Clustering algorithms; Evolutionary algorithms; Soft computing; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84860689502
"Ben-Yosef G., Ben-Shahar O.","A tangent bundle theory for visual curve completion",2012,"IEEE Transactions on Pattern Analysis and Machine Intelligence",22,10.1109/TPAMI.2011.262,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861334180&doi=10.1109%2fTPAMI.2011.262&partnerID=40&md5=a85a372c338b479202314a5c441fd419","Visual curve completion is a fundamental perceptual mechanism that completes the missing parts (e.g., due to occlusion) between observed contour fragments. Previous research into the shape of completed curves has generally followed an axiomatic approach, where desired perceptual/geometrical properties are first defined as axioms, followed by mathematical investigation into curves that satisfy them. However, determining psychophysically such desired properties is difficult and researchers still debate what they should be in the first place. Instead, here we exploit the observation that curve completion is an early visual process to formalize the problem in the unit tangent bundle R 2×S 1, which abstracts the primary visual cortex (V1) and facilitates exploration of basic principles from which perceptual properties are later derived rather than imposed. Exploring here the elementary principle of least action in V1, we show how the problem becomes one of finding minimum-length admissible curves in R 2×S 1. We formalize the problem in variational terms, we analyze it theoretically, and we formulate practical algorithms for the reconstruction of these completed curves. We then explore their induced visual properties vis-vis popular perceptual axioms and show how our theory predicts many perceptual properties reported in the corresponding perceptual literature. Finally, we demonstrate a variety of curve completions and report comparisons to psychophysical data and other completion models. © 2012 IEEE.","curve completion; inpainting; tangent bundle; Visual completion","Axiomatic approach; Basic principles; Inpainting; Perceptual mechanism; Perceptual properties; Primary visual cortex; Principle of least action; Psychophysical; tangent bundle; Visual completion; Visual process; Visual properties; Artificial intelligence; Computer vision",Article,Scopus,2-s2.0-84861334180
"Odan F.K., Reis L.F.R.","Hybrid Water Demand Forecasting Model Associating Artificial Neural Network with Fourier Series",2012,"Journal of Water Resources Planning and Management",22,10.1061/(ASCE)WR.1943-5452.0000177,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860795609&doi=10.1061%2f%28ASCE%29WR.1943-5452.0000177&partnerID=40&md5=83a52ae7d288c5d74ac729de547ccb43","This paper addressed the problem of water-demand forecasting for real-time operation of water supply systems. The present study was conducted to identify the best fit model using hourly consumption data from the water supply system of Araraquara, S̃o Paulo, Brazil. Artificial neural networks (ANNs) were used in view of their enhanced capability to match or even improve on the regression model forecasts. The ANNs used were the multilayer perceptron with the back-propagation algorithm (MLP-BP), the dynamic neural network (DAN2), and two hybrid ANNs. The hybrid models used the error produced by the Fourier series forecasting as input to the MLP-BP and DAN2, called ANN-H and DAN2-H, respectively. The tested inputs for the neural network were selected literature and correlation analysis. The results from the hybrid models were promising, DAN2 performing better than the tested MLP-BP models. DAN2-H, identified as the best model, produced a mean absolute error (MAE) of and for training and test set, respectively, for the prediction of the next hour, which represented about 12% of the average consumption. The best forecasting model for the next 24 hours was again DAN2-H, which outperformed other compared models, and produced a MAE of and for training and test set respectively, which represented about 12% of average consumption. © 2012 American Society of Civil Engineers.","Artificial intelligence; Forecasting; Fourier series; Hybrid methods; Water demand; Water supply","Best model; Best-fit models; Correlation analysis; Dynamic neural networks; Forecasting models; Hybrid method; Hybrid model; Mean absolute error; Multi layer perceptron; Real-time operation; Regression model; Test sets; Water demand; Artificial intelligence; Backpropagation algorithms; Fourier series; Neural networks; Regression analysis; Water supply; Water supply systems; Forecasting; algorithm; artificial intelligence; artificial neural network; back propagation; forecasting method; Fourier transform; regression analysis; water demand; water supply; Araraquara; Brazil; Sao Paulo [Brazil]",Article,Scopus,2-s2.0-84860795609
"Chaminade T., Rosset D., Da Fonseca D., Nazarian B., Lutcher E., Cheng G., Deruelle C.","How do we think machines think? An fMRI study of alleged competition with an artificial intelligence",2012,"Frontiers in Human Neuroscience",22,10.3389/fnhum.2012.00103,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933677033&doi=10.3389%2ffnhum.2012.00103&partnerID=40&md5=db04ad0965817b73ca6a157806e613d4","Mentalizing is defined as the inference of mental states of fellow humans, and is a particularly important skill for social interactions. Here we assessed whether activity in brain areas involved in mentalizing is specific to the processing of mental states or can be generalized to the inference of non-mental states by comparing brain responses during the interaction with an intentional and an artificial agent. Participants were scanned using fMRI during interactive rock-paper-scissors games while believing their opponent was a fellow human (Intentional agent, Int), a humanoid robot endowed with an artificial intelligence (Artificial agent, Art), or a computer playing randomly (Random agent, Rnd). Participants' subjective reports indicated that they adopted different stances against the three agents. The contrast of brain activity during interaction with the artificial and the random agents didn't yield any cluster at the threshold used, suggesting the absence of a reproducible stance when interacting with an artificial intelligence. We probed response to the artificial agent in regions of interest corresponding to clusters found in the contrast between the intentional and the random agents. In the precuneus involved in working memory, the posterior intraparietal suclus, in the control of attention and the dorsolateral prefrontal cortex, in executive functions, brain activity for Art was larger than for Rnd but lower than for Int, supporting the intrinsically engaging nature of social interactions. A similar pattern in the left premotor cortex and anterior intraparietal sulcus involved in motor resonance suggested that participants simulated human, and to a lesser extend humanoid robot actions, when playing the game. Finally, mentalizing regions, the medial prefrontal cortex and right temporoparietal junction, responded to the human only, supporting the specificity of mentalizing areas for interactions with intentional agents. © 2012 Chaminade, Rosset, Da Fonseca, Nazarian, Lutcher, Cheng and Deruelle.","Artificial intelligence; fMRI; Neuroscience; Social cognition","adult; article; artificial intelligence; BOLD signal; brain function; brain region; functional magnetic resonance imaging; human; human experiment; intraparietal sulcus; male; normal human; precuneus; prefrontal cortex; premotor cortex; working memory",Article,Scopus,2-s2.0-84933677033
"Obara B., Fricker M., Gavaghan D., Grau V.","Contrast-independent curvilinear structure detection in biomedical images",2012,"IEEE Transactions on Image Processing",22,10.1109/TIP.2012.2185938,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859512576&doi=10.1109%2fTIP.2012.2185938&partnerID=40&md5=ec761dddac71b0ab48df26123f2f030e","Many biomedical applications require detection of curvilinear structures in images and would benefit from automatic or semiautomatic segmentation to allow high-throughput measurements. Here, we propose a contrast-independent approach to identify curvilinear structures based on oriented phase congruency, i.e., the phase congruency tensor (PCT). We show that the proposed method is largely insensitive to intensity variations along the curve and provides successful detection within noisy regions. The performance of the PCT is evaluated by comparing it with state-of-the-art intensity-based approaches on both synthetic and real biological images. © 1992-2012 IEEE.","Bioimage informatics; curvilinear structure; live-wire tracing; phase congruency tensor (PCT)","Biomedical applications; Biomedical images; Curvilinear structures; High-throughput measurements; Informatics; Intensity variations; Intensity-based; live-wire tracing; Phase congruency; Semi-automatic segmentation; Image segmentation; Medical applications; Tensors; Calcification (biochemistry); algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; image enhancement; methodology; reproducibility; sensitivity and specificity; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84859512576
"Gunay O., Toreyin B.U., Kose K., Cetin A.E.","Entropy-functional-based online adaptive decision fusion framework with application to wildfire detection in video",2012,"IEEE Transactions on Image Processing",22,10.1109/TIP.2012.2183141,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860151593&doi=10.1109%2fTIP.2012.2183141&partnerID=40&md5=5e1010a3177428763fb03278eede4d92","In this paper, an entropy-functional-based online adaptive decision fusion (EADF) framework is developed for image analysis and computer vision applications. In this framework, it is assumed that the compound algorithm consists of several subalgorithms, each of which yields its own decision as a real number centered around zero, representing the confidence level of that particular subalgorithm. Decision values are linearly combined with weights that are updated online according to an active fusion method based on performing entropic projections onto convex sets describing subalgorithms. It is assumed that there is an oracle, who is usually a human operator, providing feedback to the decision fusion method. A video-based wildfire detection system was developed to evaluate the performance of the decision fusion algorithm. In this case, image data arrive sequentially, and the oracle is the security guard of the forest lookout tower, verifying the decision of the combined algorithm. The simulation results are presented. © 1992-2012 IEEE.","Active learning; decision fusion; entropy maximization; online learning; projections onto convex sets; wildfire detection using video","Active Learning; Decision fusion; Entropy maximization; Online learning; Projections onto convex sets; Wildfire detection; Algorithms; Computer vision; Fires; Set theory; Entropy; algorithm; article; artificial intelligence; automated pattern recognition; classification; computer assisted diagnosis; disaster; entropy; fire; image enhancement; image subtraction; methodology; online system; photography; reproducibility; sensitivity and specificity; videorecording; Algorithms; Artificial Intelligence; Disasters; Entropy; Fires; Image Enhancement; Image Interpretation, Computer-Assisted; Online Systems; Pattern Recognition, Automated; Photography; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique; Video Recording",Article,Scopus,2-s2.0-84860151593
"Bellare M., Kiltz E., Peikert C., Waters B.","Identity-based (lossy) trapdoor functions and applications",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",22,10.1007/978-3-642-29011-4_15,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859952486&doi=10.1007%2f978-3-642-29011-4_15&partnerID=40&md5=d2bcf8294ce393cf7c7f631a0da9cb70","We provide the first constructions of identity-based (injective) trapdoor functions. Furthermore, they are lossy. Constructions are given both with pairings (DLIN) and lattices (LWE). Our lossy identitybased trapdoor functions provide an automatic way to realize, in the identity-based setting, many functionalities previously known only in the public-key setting. In particular we obtain the first deterministic and efficiently searchable IBE schemes and the first hedged IBE schemes, which achieve best possible security in the face of bad randomness. Underlying our constructs is a new definition, namely partial lossiness, that may be of broader interest. © 2012 International Association for Cryptologic Research.",,"First constructions; Identity-based; Public keys; Trapdoor functions; First constructions; Identity-based; Lossiness; Public key settings; Trapdoor functions; Artificial intelligence; Artificial intelligence; Computers; Cryptography; Cryptography",Conference Paper,Scopus,2-s2.0-84859952486
"Yuan J., Zhao G., Fu Y., Li Z., Katsaggelos A.K., Wu Y.","Discovering thematic objects in image collections and videos",2012,"IEEE Transactions on Image Processing",22,10.1109/TIP.2011.2181952,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859024517&doi=10.1109%2fTIP.2011.2181952&partnerID=40&md5=75a9e9723fc70d9d92f817ad7e818c14","Given a collection of images or a short video sequence, we define a thematic object as the key object that frequently appears and is the representative of the visual contents. Successful discovery of the thematic object is helpful for object search and tagging, video summarization and understanding, etc. However, this task is challenging because 1) there lacks a priori knowledge of the thematic objects, such as their shapes, scales, locations, and times of re-occurrences, and 2) the thematic object of interest can be under severe variations in appearances due to viewpoint and lighting condition changes, scale variations, etc. Instead of using a top-down generative model to discover thematic visual patterns, we propose a novel bottom-up approach to gradually prune uncommon local visual primitives and recover the thematic objects. A multilayer candidate pruning procedure is designed to accelerate the image data mining process. Our solution can efficiently locate thematic objects of various sizes and can tolerate large appearance variations of the same thematic object. Experiments on challenging image and video data sets and comparisons with existing methods validate the effectiveness of our method. © 2011 IEEE.","Image data mining; thematic object discovery","Bottom up approach; Candidate pruning; Generative model; Image collections; Image data; Key object; Lighting condition change; Object of interests; Priori knowledge; thematic object discovery; Topdown; Video data; Video sequences; Video summarization; Visual content; Visual pattern; Video recording; Data mining; algorithm; article; artificial intelligence; automated pattern recognition; biological model; computer assisted diagnosis; computer simulation; image enhancement; image subtraction; methodology; photography; reproducibility; sensitivity and specificity; statistical model; videorecording; Algorithms; Artificial Intelligence; Computer Simulation; Image Enhancement; Image Interpretation, Computer-Assisted; Models, Biological; Models, Statistical; Pattern Recognition, Automated; Photography; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique; Video Recording",Article,Scopus,2-s2.0-84859024517
"Tinguaro Rodríguez J., Vitoriano B., Montero J.","A general methodology for data-based rule building and its application to natural disaster management",2012,"Computers and Operations Research",22,10.1016/j.cor.2009.11.014,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051800235&doi=10.1016%2fj.cor.2009.11.014&partnerID=40&md5=261825280632cda894556a275fd1c0d5","Risks derived from natural disasters have a deeper impact than the sole damage suffered by the affected zone and its population. Because disasters can affect geostrategic stability and international safety, developed countries invest a huge amount of funds to manage these risks. A large portion of these funds are channeled through United Nations agencies and international non-governmental organizations (NGOs), which at the same time are carrying out more and more complex operations. For these reasons, technological support for these actors is required, all the more so because the global economic crisis is placing emphasis on the need for efficiency and transparency in the management of (relatively limited) funds. Nevertheless, currently available sophisticated tools for disaster management do not fit well into these contexts because their infrastructure requirements usually exceed the capabilities of such organizations. In this paper, a general methodology for inductive rule building is described and applied to natural-disaster management. The application is a data-based, two-level knowledge decision support system (DSS) prototype which provides damage assessment for multiple disaster scenarios to support humanitarian NGOs involved in response to natural disasters. A validation process is carried out to measure the accuracy of both the methodology and the DSS. © 2009 Elsevier Ltd. All rights reserved.","Data-based inductive reasoning; Decision support systems; Emergency management; Humanitarian logistics; Natural disaster risk management","Decision supports; Emergency management; Humanitarian logistics; Inductive reasoning; Natural disaster risk management; Artificial intelligence; Damage detection; Decision making; Decision support systems; Disaster prevention; Population statistics; Risk management; Societies and institutions; Disasters",Article,Scopus,2-s2.0-80051800235
"Tassopoulos I.X., Beligiannis G.N.","Solving effectively the school timetabling problem using particle swarm optimization",2012,"Expert Systems with Applications",22,10.1016/j.eswa.2011.12.013,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855878217&doi=10.1016%2fj.eswa.2011.12.013&partnerID=40&md5=769bb0249913f125e7834fe14752f451","A new adaptive algorithm based on particle swarm optimization (PSO) is designed, developed and applied to the high school timetabling problem. The proposed PSO algorithm is used in order to create feasible and very efficient timetables for high schools in Greece. Experiments with real-world data coming from many different high schools have been conducted in order to show the efficiency of the proposed PSO algorithm. As well as that, the algorithm has been compared with four other effective techniques found in the literature in order to demonstrate its efficiency and superior performance. The proposed PSO algorithm outperforms, in most cases, other existing attempts to solve the same problem as shown by experimental results. © 2011 Elsevier Ltd. All rights reserved.","Artificial intelligence; Computer application; Educational organizations; Particle swarm optimization; School timetabling problem","Educational organizations; High school; Its efficiencies; PSO algorithms; Real world data; School timetabling problem; School timetabling problems; Adaptive algorithms; Artificial intelligence; Computer applications; Problem solving; Societies and institutions; Particle swarm optimization (PSO)",Article,Scopus,2-s2.0-84855878217
"Cheng J., Zhang G., Li Z., Li Y.","Multi-objective ant colony optimization based on decomposition for bi-objective traveling salesman problems",2012,"Soft Computing",22,10.1007/s00500-011-0759-3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858001112&doi=10.1007%2fs00500-011-0759-3&partnerID=40&md5=3486f95ee47770d01990bc2147a46366","This paper proposes a framework named multi-objective ant colony optimization based on decomposition (MoACO/D) to solve bi-objective traveling salesman problems (bTSPs). In the framework, a bTSP is first decomposed into a number of scalar optimization subproblems using Tchebycheff approach. To suit for decomposition, an ant colony is divided into many subcolonies in an overlapped manner, each of which is for one subproblem. Then each subcolony independently optimizes its corresponding subproblem using single-objective ant colony optimization algorithm and all subcolonies simultaneously work. During the iteration, each subproblem maintains an aggregated pheromone trail and an aggregated heuristic matrix. Each subcolony uses the information to solve its corresponding subproblem. After an iteration, a pheromone trail share procedure is evoked to realize the information share of those subproblems solved by common ants. Three MoACO algorithms designed by, respectively, combining MoACO/D with AS, MMAS and ACS are presented. Extensive experiments conducted on ten bTSPs with various complexities manifest that MoACO/D is both efficient and effective for solving bTSPs and the ACS version of MoACO/D outperforms three well-known MoACO algorithms on large bTSPs according to several performance measures and median attainment surfaces. © 2011 Springer-Verlag.","Ant colony optimization; Bi-objective traveling salesman problems; Decomposition; Multi-objective","Ant colonies; Ant Colony Optimization (ACO); Ant Colony Optimization algorithms; Bi objectives; Information share; Multi objective; Performance measure; Pheromone trails; Scalar optimization; Sub-problems; Tchebycheff approach; Aggregates; Algorithms; Artificial intelligence; Decomposition; Traveling salesman problem",Article,Scopus,2-s2.0-84858001112
"Periwal V., Kishtapuram S., Scaria V.","Computational models for in-vitro anti-tubercular activity of molecules based on high-throughput chemical biology screening datasets",2012,"BMC Pharmacology",22,10.1186/1471-2210-12-1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860490696&doi=10.1186%2f1471-2210-12-1&partnerID=40&md5=5b9be0bc546dcb967036259ed1f9461b","Background: The emergence of Multi-drug resistant tuberculosis in pandemic proportions throughout the world and the paucity of novel therapeutics for tuberculosis have re-iterated the need to accelerate the discovery of novel molecules with anti-tubercular activity. Though high-throughput screens for anti-tubercular activity are available, they are expensive, tedious and time-consuming to be performed on large scales. Thus, there remains an unmet need to prioritize the molecules that are taken up for biological screens to save on cost and time. Computational methods including Machine Learning have been widely employed to build classifiers for high-throughput virtual screens to prioritize molecules for further analysis. The availability of datasets based on high-throughput biological screens or assays in public domain makes computational methods a plausible proposition for building predictive models. In addition, this approach would save significantly on the cost, effort and time required to run high throughput screens.Results: We show that by using four supervised state-of-the-art classifiers (SMO, Random Forest, Naive Bayes and J48) we are able to generate in-silico predictive models on an extremely imbalanced (minority class ratio: 0.6%) large dataset of anti-tubercular molecules with reasonable AROC (0.6-0.75) and BCR (60-66%) values. Moreover, these models are able to provide 3-4 fold enrichment over random selection.Conclusions: In the present study, we have used the data from in-vitro screens for anti-tubercular activity from a high-throughput screen available in public domain to build highly accurate classifiers based on molecular descriptors of the molecules. We show that Machine Learning tools can be used to build highly effective predictive models for virtual high-throughput screens to prioritize molecules from large molecular libraries. © 2012 Periwal et al; licensee BioMed Central Ltd.",,"tuberculostatic agent; tuberculostatic agent; article; Bayesian learning; computer model; data base; drug activity; drug research; high throughput screening; machine learning; mathematical model; random forest; algorithm; artificial intelligence; Bayes theorem; chemistry; computer program; computer simulation; drug development; drug effect; genetic database; human; laboratory diagnosis; microbiology; multidrug resistant tuberculosis; Mycobacterium tuberculosis; predictive value; theoretical model; Algorithms; Antitubercular Agents; Artificial Intelligence; Bayes Theorem; Computer Simulation; Databases, Genetic; Drug Discovery; False Negative Reactions; False Positive Reactions; High-Throughput Screening Assays; Humans; Models, Theoretical; Mycobacterium tuberculosis; Predictive Value of Tests; Software; Tuberculosis, Multidrug-Resistant",Article,Scopus,2-s2.0-84860490696
"De Amo E., Díaz Carrillo M., Fernández-Sánchez J.","Characterization of all copulas associated with non-continuous random variables",2012,"Fuzzy Sets and Systems",22,10.1016/j.fss.2011.10.005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855556329&doi=10.1016%2fj.fss.2011.10.005&partnerID=40&md5=d87d865343656c261cdb17f902d09803","We introduce a constructive method, by means of a doubly stochastic measure, to describe all the copulas that, in view of Sklars Theorem, are able to connect a bivariate distribution to its marginals. We use this to give the lower and upper optimal bounds for all the copulas that extend a given subcopula. © 2011 Elsevier B.V. All rights reserved.","Copula; Subcopula","Bivariate distribution; Constructive methods; Copula; Doubly stochastic; Marginals; Subcopula; Upper optimal; Artificial intelligence; Fuzzy sets; Random variables",Article,Scopus,2-s2.0-84855556329
"Ngai E.W.T., Leung T.K.P., Wong Y.H., Lee M.C.M., Chai P.Y.F., Choi Y.S.","Design and development of a context-aware decision support system for real-time accident handling in logistics",2012,"Decision Support Systems",22,10.1016/j.dss.2011.11.016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857651823&doi=10.1016%2fj.dss.2011.11.016&partnerID=40&md5=a5f07ccbbe42f34ab4141c0966750c29","This paper describes the design and development of a context-aware fleet management system (CFMS) prototype for real-time accident handling in logistics using a design science approach. One of the most important decisions in fleet management is the optimization of vehicle scheduling during an accident, such as a vehicle breakdown and mechanical failure during delivery. The schedule planner has to assign another vehicle to take over the task; thus, accident handling needs the reassignment or re-scheduling of vehicles. The large number of available vehicles for reassignment and numerous trips in a day make rescheduling complicated and difficult to resolve. In this paper, we propose a CFMS integrated with global positioning system (GPS) for real-time vehicle positioning and eSeal enabled by the RFID technology, to help human planners with rescheduling. A CFMS prototype was built and evaluated in a real-world setting. The system prototype was satisfactory during evaluation. The system was found to be more effective by its potential users and field logistics experts in aiding real-time accident handling in logistics. The design science approach used to develop the prototype could form a basis for further research. © 2011 Elsevier B.V. All rights reserved.","Context-aware decision support system; Fleet management system prototype; Real-time accident handling","Context-Aware; Design and Development; Design science; Fleet management; Fleet management system; Mechanical failures; Potential users; Re-scheduling; Real-time accident handling; RFID Technology; System prototype; Vehicle positioning; Vehicle scheduling; Artificial intelligence; Decision support systems; Fleet operations; Global positioning system; Management; Vehicles; Accidents",Article,Scopus,2-s2.0-84857651823
"Arpaia P., Manna C., Montenero G., D'Addio G.","In-time prognosis based on swarm intelligence for home-care monitoring: A case study on pulmonary disease",2012,"IEEE Sensors Journal",22,10.1109/JSEN.2011.2158305,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856843568&doi=10.1109%2fJSEN.2011.2158305&partnerID=40&md5=bf773f7aeca3a7a32e073ab7d1414fde","A swarm intelligence-based procedure to detect critical conditions of a patient, affected by a specific disease, at an early stage in absence of clinician, is proposed. The procedure is to be integrated inside a remote health care system for patients at home, where some physiological parameters related to a specific disease are being monitored. A significant variation in the monitored parameters can lead the patient to a critical state, thus the proposed method is aimed at predicting a possible future bad condition of the patient on the basis of past measurements. Moreover, different physiological parameters contribute to diverse degrees in dissimilar diseases; consequently, a swarm intelligence-based method is proposed for optimizing the weight of each parameter for a more accurate diagnosis. The proposed approach has been validated experimentally under the framework of the industrial research project Patient Diagnosis and Monitoring at Domicile (PADIAMOND: co-funded by EU and the company Filia srl, Caserta, Italy). © 2011 IEEE.","Computer-aided diagnosis; fuzzy logic; particle swarm optimization (PSO)","Critical condition; Critical state; Health-care system; Monitored parameters; Patient diagnosis; Physiological parameters; Possible futures; Swarm Intelligence; Artificial intelligence; Cellular automata; Computer aided diagnosis; Critical current density (superconductivity); Fuzzy logic; Health care; Industrial research; Monitoring; Physiology; Physiological models",Article,Scopus,2-s2.0-84856843568
"Huang Q., Tao D., Li X., Liew A.","Parallelized evolutionary learning for detection of biclusters in gene expression data",2012,"IEEE/ACM Transactions on Computational Biology and Bioinformatics",22,10.1109/TCBB.2011.53,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863038127&doi=10.1109%2fTCBB.2011.53&partnerID=40&md5=81f272bc7891acef72d3f058e1c780a3","The analysis of gene expression data obtained from microarray experiments is important for discovering the biological process of genes. Biclustering algorithms have been proven to be able to group the genes with similar expression patterns under a number of experimental conditions. In this paper, we propose a new biclustering algorithm based on evolutionary learning. By converting the biclustering problem into a common clustering problem, the algorithm can be applied in a search space constructed by the conditions. To further reduce the size of the search space, we randomly separate the full conditions into a number of condition subsets (subspaces), each of which has a smaller number of conditions. The algorithm is applied to each subspace and is able to discover bicluster seeds within a limited computing time. Finally, an expanding and merging procedure is employed to combine the bicluster seeds into larger biclusters according to a homogeneity criterion. We test the performance of the proposed algorithm using synthetic and real microarray data sets. Compared with several previously developed biclustering algorithms, our algorithm demonstrates a significant improvement in discovering additive biclusters. © 2012 IEEE.","Biclustering; gene expression data analysis; genetic learning; subdimensional search strategy","Biclustering; Biclustering algorithm; Biclusters; Biological process; Clustering problems; Computing time; Evolutionary Learning; Experimental conditions; Expression patterns; Gene Expression Data; gene expression data analysis; Genetic learning; Merging procedure; Microarray data sets; Microarray experiments; Search spaces; Search strategies; Evolutionary algorithms; Gene expression; Microarrays; Clustering algorithms; algorithm; artificial intelligence; automated pattern recognition; biological model; biology; cluster analysis; colon tumor; computer simulation; DNA microarray; gene expression profiling; genetic database; genetics; human; metabolism; procedures; Saccharomyces cerevisiae; article; biology; gene expression profiling; methodology; Algorithms; Artificial Intelligence; Cluster Analysis; Colonic Neoplasms; Computational Biology; Computer Simulation; Databases, Genetic; Gene Expression Profiling; Humans; Models, Genetic; Oligonucleotide Array Sequence Analysis; Pattern Recognition, Automated; Saccharomyces cerevisiae; Algorithms; Artificial Intelligence; Cluster Analysis; Colonic Neoplasms; Computational Biology; Computer Simulation; Databases, Genetic; Gene Expression Profiling; Humans; Models, Genetic; Oligonucleotide Array Sequence Analysis; Pattern Recognition, Automated; Saccharomyces cerevisiae",Article,Scopus,2-s2.0-84863038127
"Wang X., Ni W., Wang X.","Leader-following formation of switching multirobot systems via internal model",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",22,10.1109/TSMCB.2011.2178022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862819399&doi=10.1109%2fTSMCB.2011.2178022&partnerID=40&md5=36e7c5d57583ac526470a43717851cac","In this paper, the leader-following formation problem of multirobot systems with switching interconnection topologies is considered. The robots are required to move in a formation with formation constrains described in terms of relative distances of the robots and the formation (as whole entity) is required to track the trajectory generated by an exosystem. The exosystem of the considered multirobot systems provides driving forces or environmental disturbance, whose dynamics is different from the dynamics of the robots. A systematic distributed design approach for the leader-following formation problem is proposed via dynamic output feedback with the help of canonical internal model. © 2012 IEEE.","Canonical internal model; formation; multirobot systems; switching topology","Distributed design; Driving forces; Dynamic output feedback; Environmental disturbances; Exosystems; formation; Interconnection topologies; Internal models; Leader following; Multi-robot systems; Relative distances; Switching topology; Dynamics; Industrial robots; Topology; algorithm; article; artificial intelligence; automated pattern recognition; computer simulation; decision support system; methodology; motion; robotics; theoretical model; Algorithms; Artificial Intelligence; Computer Simulation; Decision Support Techniques; Models, Theoretical; Motion; Pattern Recognition, Automated; Robotics",Article,Scopus,2-s2.0-84862819399
"McIntosh C., Hamarneh G.","Medial-based deformable models in nonconvex shape-spaces for medical image segmentation",2012,"IEEE Transactions on Medical Imaging",22,10.1109/TMI.2011.2162528,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855398203&doi=10.1109%2fTMI.2011.2162528&partnerID=40&md5=99a5a6efccba87781617ee13655a59b3","We explore the application of genetic algorithms (GA) to deformable models through the proposition of a novel method for medical image segmentation that combines GA with nonconvex, localized, medial-based shape statistics. We replace the more typical gradient descent optimizer used in deformable models with GA, and the convex, implicit, global shape statistics with nonconvex, explicit, localized ones. Specifically, we propose GA to reduce typical deformable model weaknesses pertaining to model initialization, pose estimation and local minima, through the simultaneous evolution of a large number of models. Furthermore, we constrain the evolution, and thus reduce the size of the search-space, by using statistically-based deformable models whose deformations are intuitive (stretch, bulge, bend) and are driven in terms of localized principal modes of variation, instead of modes of variation across the entire shape that often fail to capture localized shape changes. Although GA are not guaranteed to achieve the global optima, our method compares favorably to the prevalent optimization techniques, convex/nonconvex gradient-based optimizers and to globally optimal graph-theoretic combinatorial optimization techniques, when applied to the task of corpus callosum segmentation in 50 mid-sagittal brain magnetic resonance images. © 2011 IEEE.","Deformable models; evolutionary computing; genetic algorithms; medial-shape representation; medical image segmentation","Brain magnetic resonance images; Combinatorial optimization techniques; Corpus callosum; Deformable models; evolutionary computing; Globaloptimum; Gradient descent; Gradient-based optimizers; Graph-theoretic; Local minimums; medial-shape representation; Medical image segmentation; Model initialization; Nonconvex; Optimization techniques; Optimizers; Pose estimation; Shape change; Shape statistics; Combinatorial optimization; Deformation; Genetic algorithms; Graph theory; Magnetic resonance; Magnetic resonance imaging; Medical imaging; Optimization; Image segmentation; algorithm; article; artificial intelligence; biological model; corpus callosum; histology; human; image processing; methodology; nuclear magnetic resonance imaging; physiology; principal component analysis; reproducibility; Algorithms; Artificial Intelligence; Corpus Callosum; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Models, Genetic; Principal Component Analysis; Reproducibility of Results",Article,Scopus,2-s2.0-84855398203
"Wang Y.","Inference algebra (IA): A denotational mathematics for cognitive computing and machine reasoning (II)",2012,"International Journal of Cognitive Informatics and Natural Intelligence",22,10.4018/jcini.2012010102,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868135606&doi=10.4018%2fjcini.2012010102&partnerID=40&md5=afcac401410bdd87c42bc09fe84c2b91","Inference as the basic mechanism of thought is abilities gifted to human beings, which is a cognitive process that creates rational causations between a pair of cause and effect based on empirical arguments, formal reasoning, and/or statistical norms. It's recognized that a coherent theory and mathematical means are needed for dealing with formal causal inferences. Presented is a novel denotational mathematical means for formal inferences known as Inference Algebra (IA) and structured as a set of algebraic operators on a set of formal causations. The taxonomy and framework of formal causal inferences of IA are explored in three categories: a) Logical inferences; b) Analytic inferences; and c) Hybrid inferences. IA introduces the calculus of discrete causal differential and formal models of causations. IA enables artificial intelligence and computational intelligent systems to mimic human inference abilities by cognitive computing. A wide range of applications of IA are identified and demonstrated in cognitive informatics and computational intelligence towards novel theories and technologies for machine-enabled inferences and reasoning. This work is presented in two parts. The inference operators of IA as well as their extensions and applications will be presented in this paper; while the structure of formal inference, the framework of IA, and the mathematical models of formal causations has been published in the first part of the paper in IJCINI 5(4). © 2011, IGI Global.","Abstract Intelligence; Applications; Causal Differential; Cognitive Computers; Cognitive Computing; Cognitive Informatics; Computational Intelligence; Denotational Mathematics; Formal Causations; Inference Algebra; Inference Engine","Abstract intelligence; Causal Differential; Cognitive computers; Cognitive Computing; Cognitive informatics; Denotational mathematics; Formal Causations; Applications; Artificial intelligence; Differentiation (calculus); Inference engines; Information science; Intelligent systems; Mathematical models; Mathematical operators; Algebra",Article,Scopus,2-s2.0-84868135606
"Sohn K.-A., Kim S.","Joint estimation of structured sparsity and output structure in multiple-output regression via inverse-covariance regularization",2012,"Journal of Machine Learning Research",22,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84895998243&partnerID=40&md5=26fabe6ef7c35463d22e0252132902b9","We consider the problem of learning a sparse regression model for predicting multiple related outputs given high-dimensional inputs, where related outputs are likely to share common relevant inputs. Most of the previous methods for learning structured sparsity assumed that the structure over the outputs is known a priori, and focused on designing regularization functions that encourage structured sparsity reflecting the given output structure. In this paper, we propose a new approach for sparse multiple-output regression that can jointly learn both the output structure and regression coefficients with structured sparsity. Our approach reformulates the standard regression model into an alternative parameterization that leads to a conditional Gaussian graphical model, and employes an inverse-covariance regularization. We show that the orthant-wise quasi-Newton algorithm developed for L1-regularized log-linear model can be adopted for a fast optimization for our method. We demonstrate our method on simulated datasets and real datasets from genetics and finances applications. © Copyright 2012 by the authors.",,"Artificial intelligence; Inverse problems; Fast optimizations; Gaussian graphical models; Inverse covariance; Quasi-newton algorithm; Regression coefficient; Regularization function; Simulated datasets; Structured sparsities; Regression analysis",Conference Paper,Scopus,2-s2.0-84895998243
"Liu G., Xu H., Yan S.","Exact subspace segmentation and outlier detection by low-rank representation",2012,"Journal of Machine Learning Research",22,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954519713&partnerID=40&md5=015d50a485cc40b1fd1ca904e4fed18e","In this work, we address the following matrix recovery problem: suppose we are given a set of data points containing two parts, one part consists of samples drawn from a union of multiple subspaces and the other part consists of outliers. We do not know which data points are outliers, or how many outliers there are. The rank and number of the subspaces are unknown either. Can we detect the outliers and segment the samples into their right subspaces, efficiently and exactly? We utilize a so-called Low-Rank Representation (LRR) method to solve this problem, and prove that under mild technical conditions, any solution to LRR exactly recovers the row space of the samples and detect the outliers as well. Since the subspace membership is provably determined by the row space, this further implies that LRR can perform exact subspace segmentation and outlier detection, in an efficient way.",,"Artificial intelligence; Data handling; Data points; Low-rank representations; Matrix recovery; One parts; Outlier Detection; Sub-space segmentation; Technical conditions; Statistics",Conference Paper,Scopus,2-s2.0-84954519713
"Cao K., Yang X., Chen X., Zang Y., Liang J., Tian J.","A novel ant colony optimization algorithm for large-distorted fingerprint matching",2012,"Pattern Recognition",22,10.1016/j.patcog.2011.04.016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052704329&doi=10.1016%2fj.patcog.2011.04.016&partnerID=40&md5=43dee3372b4f84eb749da225936ba1b4","Large distortion may be introduced by non-orthogonal finger pressure and 3D2D mapping during the process of fingerprint capturing. Furthermore, large variations in resolution and geometric distortion may exist among the fingerprint images acquired from different types of sensors. This distortion greatly challenges the traditional minutiae-based fingerprint matching algorithms. In this paper, we propose a novel ant colony optimization algorithm to establish minutiae correspondences in large-distorted fingerprints. First, minutiae similarity is measured by local features, and an assignment graph is constructed by local search. Then, the minutiae correspondences are established by a pseudo-greedy rule and local propagation, and the pheromone matrix is updated by the local and global update rules. Finally, the minutiae correspondences that maximize the matching score are selected as the matching result. To compensate resolution difference of fingerprint images captured from disparate sensors, a common resolution method is adopted. The proposed method is tested on FVC2004 DB1 and a FINGERPASS cross-matching database established by our lab. The experimental results demonstrate that the proposed algorithm can effectively improve the performance of large-distorted fingerprint matching, especially for those fingerprint images acquired from different modes of acquisition. © 2011 Elsevier Ltd. All rights reserved.","Ant colony optimization; Distortion; Fingerprint matching; Minutia similarity; Minutiae pairing","Ant Colony Optimization algorithms; Ant-colony optimization; Cross-matching; Different modes; Fingerprint images; Fingerprint matching; Fingerprint matching algorithm; Geometric distortion; Local feature; Local propagation; Local search; Matching score; matrix; Minutia similarity; Minutiae pairing; Resolution methods; Algorithms; Artificial intelligence; Image matching; Optimization; Sensors; Pattern matching",Article,Scopus,2-s2.0-80052704329
"Feuerstein M., Glocker B., Kitasaka T., Nakamura Y., Iwano S., Mori K.","Mediastinal atlas creation from 3-D chest computed tomography images: Application to automated detection and station mapping of lymph nodes",2012,"Medical Image Analysis",22,10.1016/j.media.2011.05.005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-82355169980&doi=10.1016%2fj.media.2011.05.005&partnerID=40&md5=222feb58337b0056d5b1f037e92516ed","One important aspect of lung cancer staging is the assessment of mediastinal lymph nodes in 3-D chest computed tomography (CT) images. In the current clinical routine this is done manually by analyzing the 3-D CT image slice by slice to find nodes, evaluate them quantitatively, and assign labels to them for describing the clinical and pathologic extent of metastases. In this paper we present a method to automate the process of lymph node detection and labeling by creation of a mediastinal average image and a novel lymph node atlas containing probability maps for mediastinal, aortic, and N1 nodes. Utilizing a fast deformable registration approach to match the atlas with CT images of new patients, our method can maintain an acceptable runtime. In comparison to previously published methods for mediastinal lymph node detection and labeling it also shows a good sensitivity and positive predictive value. © 2011 Elsevier B.V.","Deformable registration; Lung cancer; Lymph node map; Mediastinum; Probabilistic atlas","Deformable registration; Lung Cancer; Lymph node; Mediastinum; Probabilistic atlas; Biological organs; Body fluids; Deformation; Diseases; Maps; Pathology; Three dimensional; Tomography; Computerized tomography; article; automation; cancer staging; computer assisted tomography; diagnostic test accuracy study; disease severity; human; image analysis; image processing; lung cancer; lymph node; mediastinum; mediastinum lymph node; metastasis; multidetector computed tomography; paraaortic lymph node; predictive value; priority journal; sensitivity and specificity; thorax radiography; three dimensional imaging; Algorithms; Artificial Intelligence; Computer Simulation; Humans; Imaging, Three-Dimensional; Lymph Nodes; Mediastinum; Models, Anatomic; Pattern Recognition, Automated; Radiographic Image Enhancement; Radiographic Image Interpretation, Computer-Assisted; Radiography, Thoracic; Reproducibility of Results; Sensitivity and Specificity; Tomography, X-Ray Computed",Article,Scopus,2-s2.0-82355169980
"Fernández-Navarro F., Hervás-Martínez C., Gutiérrez P.A., Peña-Barragán J.M., López-Granados F.","Parameter estimation of q-Gaussian Radial Basis Functions Neural Networks with a Hybrid Algorithm for binary classification",2012,"Neurocomputing",22,10.1016/j.neucom.2011.03.056,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-82455199144&doi=10.1016%2fj.neucom.2011.03.056&partnerID=40&md5=ff94c84588ebbe914aeb25759262fb11","A classification problem is a decision-making task that many researchers have studied. A number of techniques have been proposed to perform binary classification. Neural networks are one of the artificial intelligence techniques that has had the most successful results when applied to this problem. Our proposal is the use of q-Gaussian Radial Basis Function Neural Networks (q-Gaussian RBFNNs). This basis function includes a supplementary degree of freedom in order to adapt the model to the distribution of data. A Hybrid Algorithm (HA) is used to search for a suitable architecture for the q-Gaussian RBFNN. The use of this type of more flexible kernel could greatly improve the discriminative power of RBFNNs. In order to test performance, the RBFNN with the q-Gaussian basis functions is compared to RBFNNs with Gaussian, Cauchy and Inverse Multiquadratic RBFs, and to other recent neural networks approaches. An experimental study is presented on 11 binary-classification datasets taken from the UCI repository. Moreover, aerial imagery taken in mid-May, mid-June and mid-July was used to evaluate the potential of the methodology proposed for discriminating Ridolfia segetum patches (one of the most dominant and harmful weeds in sunflower crops) in two naturally infested fields in southern Spain. © 2011 Elsevier B.V..","Classification; Hybrid Algorithms; Q-Gaussian Radial Basis Functions Neural Networks; Remote sensing; Weed discrimination and control","Aerial imagery; Artificial intelligence techniques; Basis functions; Binary classification; Data sets; Degree of freedom; Experimental studies; Gaussians; Hybrid algorithms; Multiquadratics; Radial basis function neural networks; Radial basis functions neural networks; Southern Spain; Test performance; UCI repository; Aerial photography; Algorithms; Chaos theory; Classification (of information); Gaussian distribution; Image segmentation; Neural networks; Parameter estimation; Remote sensing; Weed control; Radial basis function networks; accuracy; article; artificial intelligence; automated pattern recognition; classification algorithm; evolutionary algorithm; intermethod comparison; kernel method; mathematical computing; mathematical model; priority journal; process development; process optimization; radial based function; remote sensing; signal processing; Spain",Article,Scopus,2-s2.0-82455199144
"Farrag M.E.A., Putrus G.A.","Design of an adaptive neurofuzzy inference control system for the unified power-flow controller",2012,"IEEE Transactions on Power Delivery",22,10.1109/TPWRD.2011.2171061,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84655161261&doi=10.1109%2fTPWRD.2011.2171061&partnerID=40&md5=24f1322c2bd6e90a3deeaa69750453f7","This paper presents a new approach to control the operation of the unified power-flow controller (UPFC) based on the adaptive neurofuzzy inference controller (ANFIC) concept. The training data for the controller are extracted from an analytical model of the transmission system incorporating a UPFC. The operating points' space is dynamically partitioned into two regions: 1) an inner region where the desired operating point can be achieved without violating any of the UPFC constraints and 2) an outer region where it is necessary to operate the UPFC beyond its limits. The controller is designed to achieve the most appropriate operating point based on the real power priority. In this study, the authors investigated and analyzed the effect of the system short-circuit level on the UPFC operating feasible region which defines the limitation of its parameters. In order to illustrate the effectiveness of the control algorithm, simulation and experimental studies have been conducted using the MATLAB/SIMULINK and dSPACE DS1103 data-acquisition board. The obtained results show a clear agreement between simulation and experimental results which verify the effective performance of the ANFIC controller. © 2006 IEEE.","Artificial intelligence; flexible ac transmission systems; fuzzy; neural networks; unified power-flow controller (UPFC)","Adaptive neuro-fuzzy inference; Analytical model; D-space; Effective performance; Experimental studies; Feasible regions; Flexible AC transmission system; fuzzy; Inner region; MATLAB /simulink; Operating points; Real power; Training data; Transmission systems; Unified power flow controllers; Adaptive control systems; Algorithms; Artificial intelligence; Controllers; Electric load dispatching; Electric power transmission; Flow control; Mathematical models; Neural networks; Power electronics; Electric control equipment",Article,Scopus,2-s2.0-84655161261
"Williams C.R., Robu V., Gerding E.H., Jennings N.R.","Negotiating concurrently with unknown opponents in complex, real-time domains",2012,"Frontiers in Artificial Intelligence and Applications",22,10.3233/978-1-61499-098-7-834,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878798747&doi=10.3233%2f978-1-61499-098-7-834&partnerID=40&md5=664e8bb145dcea0eb0c3b52c866030f9","We propose a novel strategy to enable autonomous agents to negotiate concurrently with multiple, unknown opponents in realtime, over complex multi-issue domains. We formalise our strategy as an optimisation problem, in which decisions are based on probabilistic information about the opponents' strategies acquired during negotiation. In doing so, we develop the first principled approach that enables the coordination of multiple, concurrent negotiation threads for practical negotiation settings. Furthermore, we validate our strategy using the agents and domains developed for the International Automated Negotiating Agents Competition (ANAC), and we benchmark our strategy against the state-of-the-art. We find that our approach significantly outperforms existing approaches, and this difference improves even further as the number of available negotiation opponents and the complexity of the negotiation domain increases. © 2012 The Author(s).",,"Artificial intelligence; Optimization; Concurrent negotiation; Multi-issue; Negotiating agents; Novel strategies; Optimisation problems; Probabilistic information; Real-time domains; State of the art; Autonomous agents",Conference Paper,Scopus,2-s2.0-84878798747
"Vu V.Q., Lei J.","Minimax rates of estimation for sparse PCA in high dimensions",2012,"Journal of Machine Learning Research",22,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898952632&partnerID=40&md5=3fc7d56235443799d5a9f94bc6301e08","We study sparse principal components analysis in the high-dimensional setting, where p (the number of variables) can be much larger than n (the number of observations). We prove optimal, non-asymptotic lower and upper bounds on the minimax estimation error for the leading eigenvector when it belongs to an ℓq ball for q ∈ [0, 1]. Our bounds are sharp in p and n for all q ∈ [0, 1] over a wide class of distributions. The upper bound is obtained by analyzing the performance of ℓqconstrained PCA. In particular, our results provide convergence rates for !1-constrained PCA. © Copyright 2012 by the authors.",,"Artificial intelligence; Class of distributions; Convergence rates; High dimensions; High-dimensional; Lower and upper bounds; Minimax estimations; Non-asymptotic; Principal components analysis; Principal component analysis",Conference Paper,Scopus,2-s2.0-84898952632
"Lecoutre C., Likitvivatanavong C., Yap R.H.C.","A path-optimal GAC algorithm for table constraints",2012,"Frontiers in Artificial Intelligence and Applications",22,10.3233/978-1-61499-098-7-510,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878800803&doi=10.3233%2f978-1-61499-098-7-510&partnerID=40&md5=bab721e7f74ef150243ced9fec36141e","Filtering by Generalized Arc Consistency (GAC) is a fundamental technique in Constraint Programming. Recent advances in GAC algorithms for extensional constraints rely on direct manipulation of tables during search. Simple Tabular Reduction (STR), which systematically removes invalid tuples from tables, has been shown to be a simple yet efficient approach. STR2, a refinement of STR, is considered to be among the best filtering algorithms for positive table constraints. In this paper, we introduce a new GAC algorithm called STR3 that is specifically designed to enforce GAC during search. STR3 can completely avoid unnecessary traversal of tables, making it optimal along any path of the search tree. Our experiments show that STR3 is much faster than STR2 when the average size of the tables is not reduced drastically during search. © 2012 The Author(s).",,"Artificial intelligence; Computer programming; Constraint theory; Average size; Constraint programming; Direct manipulation; Filtering algorithm; Generalized arc consistencies; Search trees; Table constraints; Algorithms",Conference Paper,Scopus,2-s2.0-84878800803
"Duru O., Bulut E., Yoshida S.","A fuzzy extended DELPHI method for adjustment of statistical time series prediction: An empirical study on dry bulk freight market case",2012,"Expert Systems with Applications",22,10.1016/j.eswa.2011.07.082,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-81855218741&doi=10.1016%2fj.eswa.2011.07.082&partnerID=40&md5=d7ce13ee262435d727eb77036a8edda3","This paper investigates the forecasting accuracy of fuzzy extended group decisions in the adjustment of statistical benchmark results. DELPHI is a frequently used method for implementing accurate group consensus decisions. The concept of consensus is subject to expert characteristics and it is sometimes ensured by a facilitator's judgment. Fuzzy set theory deals with uncertain environments and has been adapted for DELPHI, called fuzzy-DELPHI (FD). The present paper extends the recent literature via an implementation of FD for the adjustment of statistical predictions. We propose a fuzzy-DELPHI adjustment process for improvement of accuracy and introduced an empirical study to illustrate its performance in the validation of adjustments of statistical forecasts in the dry bulk shipping index. © 2011 Elsevier Ltd. All rights reserved.","Consensus forecasts; Decision support systems; Dry bulk shipping; Forecasting support systems; Fuzzy-DELPHI","Consensus forecasts; Decision supports; Delphi method; Empirical studies; Forecasting accuracy; Forecasting support systems; Freight markets; Fuzzy-DELPHI; Group decision; Statistical prediction; Time series prediction; Uncertain environments; Artificial intelligence; Decision support systems; Finite difference method; Fuzzy set theory; Fuzzy sets; Time series; Forecasting",Article,Scopus,2-s2.0-81855218741
"Aksu B., Paradkar A., De Matas M., Özer Ö., Güneri T., York P.","Quality by design approach: Application of artificial intelligence techniques of tablets manufactured by direct compression",2012,"AAPS PharmSciTech",21,10.1208/s12249-012-9836-x,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871666530&doi=10.1208%2fs12249-012-9836-x&partnerID=40&md5=b430460326e1994afc6ff1a20e20f7c3","The publication of the International Conference of Harmonization (ICH) Q8, Q9, and Q10 guidelines paved the way for the standardization of quality after the Food and Drug Administration issued current Good Manufacturing Practices guidelines in 2003. ""Quality by Design"", mentioned in the ICH Q8 guideline, offers a better scientific understanding of critical process and product qualities using knowledge obtained during the life cycle of a product. In this scope, the ""knowledge space"" is a summary of all process knowledge obtained during product development, and the ""design space"" is the area in which a product can be manufactured within acceptable limits. To create the spaces, artificial neural networks (ANNs) can be used to emphasize the multidimensional interactions of input variables and to closely bind these variables to a design space. This helps guide the experimental design process to include interactions among the input variables, along with modeling and optimization of pharmaceutical formulations. The objective of this study was to develop an integrated multivariate approach to obtain a quality product based on an understanding of the cause-effect relationships between formulation ingredients and product properties with ANNs and genetic programming on the ramipril tablets prepared by the direct compression method. In this study, the data are generated through the systematic application of the design of experiments (DoE) principles and optimization studies using artificial neural networks and neurofuzzy logic programs. © 2012 American Association of Pharmaceutical Scientists.","artificial neural networks (ANNs); gene expression programming (GEP); optimization; quality by design (QbD)","ramipril; article; artificial intelligence; artificial neural network; compression; concentration (parameters); drug formulation; drug quality; experimental design; food and drug administration; fuzzy system; knowledge; practice guideline; priority journal; process optimization; standardization; study design; tablet; Artificial Intelligence; Chemistry, Pharmaceutical; Drug Compounding; Drug Industry; Neural Networks (Computer); Quality Control; Research Design; Tablets; United States; United States Food and Drug Administration",Article,Scopus,2-s2.0-84871666530
"Sioutis M., Koubarakis M.","Consistency of chordal RCC-8 networks",2012,"Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI",21,10.1109/ICTAI.2012.66,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876866481&doi=10.1109%2fICTAI.2012.66&partnerID=40&md5=bfba321f1a9d49ec26f44d6a410e9c57","We consider chordal RCC-8 networks and show that we can check their consistency by enforcing partial path consistency with weak composition. We prove this by using the fact that RCC-8 networks with relations from the maximal tractable subsets {H}8, {C}8, } and {Q}8} of RCC-8 have the patchwork property. The use of partial path consistency has important practical consequences that we demonstrate with the implementation of the new reasoner PyRCC {8}, which is developed by extending the state of the art reasoner PyRCC8. Given an RCC-8 network with only tractable RCC-8 relations, we show that it can be solved very efficiently with PyRCC {8} by making its underlying constraint graph chordal and running path consistency on this sparse graph instead of the completion of the given network. In the same way, partial path consistency can be used as the consistency checking step in backtracking algorithms for networks with arbitrary RCC-8 relations resulting in very improved pruning for sparse networks while incurring a penalty for dense networks. © 2012 IEEE.","chordal graph; constraint network; partial path consistency; RCC-8 topological relation; weak composition","Chordal graphs; Constraint networks; Path consistency; Topological relations; Weak compositions; Artificial intelligence; Constraint theory; Graph theory",Conference Paper,Scopus,2-s2.0-84876866481
"Zook A.E., Riedl M.O.","A temporal data-driven player model for dynamic difficulty adjustment",2012,"Proceedings of the 8th AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, AIIDE 2012",21,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883109617&partnerID=40&md5=53aa9a39e89b6a556ca080e7da47ee91","Many computer games of all genres pit the player against a succession of increasingly difficult challenges such as combat with computer-controlled enemies and puzzles. Part of the fun of computer games is to master the skills necessary to complete the game. Challenge tailoring is the problem of matching the difficulty of skill-based events over the course of a game to a specific player's abilities. We present a tensor factorization approach to predicting player performance in skill-based computer games. Our tensor factorization approach is data-driven and can predict changes in players' skill mastery over time, allowing more accurate tailoring of challenges. We demonstrate the efficacy and scalability of tensor factorization models through an empirical study of human players in a simple role-playing combat game.We further find a significant correlation between these performance ratings and player subjective experiences of difficulty and discuss ways our model can be used to optimize player enjoyment. Copyright © 2012, Association for the Advancement of Artificial Intelligence.",,"Empirical studies; Human players; Performance ratings; Player modeling; Subjective experiences; Tensor factorization; Artificial intelligence; Factorization; Human computer interaction; Tensors; Computer games",Conference Paper,Scopus,2-s2.0-84883109617
"Huang G., Song S., Wu C., You K.","Robust support vector regression for uncertain input and output data",2012,"IEEE Transactions on Neural Networks and Learning Systems",21,10.1109/TNNLS.2012.2212456,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876925191&doi=10.1109%2fTNNLS.2012.2212456&partnerID=40&md5=30d6a855c3d805ae9cfa7fe04fdaeb50","In this paper, a robust support vector regression (RSVR) method with uncertain input and output data is studied. First, the data uncertainties are investigated under a stochastic framework and two linear robust formulations are derived. Linear formulations robust to ellipsoidal uncertainties are also considered from a geometric perspective. Second, kernelized RSVR formulations are established for nonlinear regression problems. Both linear and nonlinear formulations are converted to second-order cone programming problems, which can be solved efficiently by the interior point method. Simulation demonstrates that the proposed method outperforms existing RSVRs in the presence of both input and output data uncertainties. © 2012 IEEE.","Robust; second-order cone programming; support vector regression; uncertain data","Ellipsoidal uncertainties; Interior point methods; Nonlinear formulation; Nonlinear regression problems; Robust; Second-order cone programming; Support vector regression (SVR); Uncertain datas; Artificial intelligence; Computer networks; Regression analysis",Article,Scopus,2-s2.0-84876925191
"Powley E.J., Whitehouse D., Cowling P.I.","Monte Carlo Tree Search with macro-actions and heuristic route planning for the Physical Travelling Salesman Problem",2012,"2012 IEEE Conference on Computational Intelligence and Games, CIG 2012",21,10.1109/CIG.2012.6374161,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871963399&doi=10.1109%2fCIG.2012.6374161&partnerID=40&md5=ad202de6a6e1eb2e87206ee1c993df0b","We present a controller for the Physical Travelling Salesman Problem (PTSP), a path planning and steering problem in a simulated continuous real-time domain. Our approach is hierarchical, using domain-specific algorithms and heuristics to plan a coarse-grained route and Monte Carlo Tree Search (MCTS) to plan and steer along fine-grained paths. The MCTS component uses macro-actions to decrease the number of decisions to be made per unit of time and thus drastically reduce the size of the decision tree. Results from the 2012 WCCI PTSP Competition show that this approach significantly and consistently outperforms all other submitted AI controllers, and is competitive with strong human players. Our approach has potential applications to many other problems in movement planning and control, including video games. © 2012 IEEE.",,"Coarse-grained; Domain specific; Human players; MONTE CARLO; Movement planning; Per unit; Potential applications; Real-time domains; Route planning; Steering problems; Travelling salesman problem; Tree search; Video game; Artificial intelligence; Decision trees; Traveling salesman problem; Motion planning",Conference Paper,Scopus,2-s2.0-84871963399
"Hassanzadeh T., Meybodi M.R.","A new hybrid approach for data clustering using firefly algorithm and K-means",2012,"AISP 2012 - 16th CSI International Symposium on Artificial Intelligence and Signal Processing",21,10.1109/AISP.2012.6313708,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869165710&doi=10.1109%2fAISP.2012.6313708&partnerID=40&md5=4f4a20ef3a68b00fef468ec7e22be248","Data clustering is a common technique for data analysis and is used in many fields, including data mining, pattern recognition and image analysis. K-means clustering is a common and simple approach for data clustering but this method has some limitation such as local optimal convergence and initial point sensibility. Firefly algorithm is a swarm based algorithm that use for solving optimization problems. This paper presents a new approach to using firefly algorithm to cluster data. It is shown how firefly algorithm can be used to find the centroid of the user specified number of clusters. The algorithm then extended to use k-means clustering to refined centroids and clusters. This new hybrid algorithm called K-FA. The experimental results showed the accuracy and capability of proposed algorithm to data clustering. © 2012 IEEE.","clustering; firefly algorithm; k-means; optimization","clustering; Data clustering; Firefly algorithms; Hybrid algorithms; Hybrid approach; Initial point; K-means; K-means clustering; Local optimal; Number of clusters; Optimization problems; Simple approach; Artificial intelligence; Bioluminescence; Cluster analysis; Optimization; Pattern recognition; Signal processing; Clustering algorithms",Conference Paper,Scopus,2-s2.0-84869165710
"Brylinski M., Lingam D.","eThread: A Highly Optimized Machine Learning-Based Approach to Meta-Threading and the Modeling of Protein Tertiary Structures",2012,"PLoS ONE",21,10.1371/journal.pone.0050200,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869842288&doi=10.1371%2fjournal.pone.0050200&partnerID=40&md5=2d1f47e19aeda3858fdd5f29aca80ae7","Template-based modeling that employs various meta-threading techniques is currently the most accurate, and consequently the most commonly used, approach for protein structure prediction. Despite the evident progress in this field, accurate structure models cannot be constructed for a significant fraction of gene products, thus the development of new algorithms is required. Here, we describe the development, optimization and large-scale benchmarking of eThread, a highly accurate meta-threading procedure for the identification of structural templates and the construction of corresponding target-to-template alignments. eThread integrates ten state-of-the-art threading/fold recognition algorithms in a local environment and extensively uses various machine learning techniques to carry out fully automated template-based protein structure modeling. Tertiary structure prediction employs two protocols based on widely used modeling algorithms: Modeller and TASSER-Lite. As a part of eThread, we also developed eContact, which is a Bayesian classifier for the prediction of inter-residue contacts and eRank, which effectively ranks generated multiple protein models and provides reliable confidence estimates as structure quality assessment. Excluding closely related templates from the modeling process, eThread generates models, which are correct at the fold level, for >80% of the targets; 40-50% of the constructed models are of a very high quality, which would be considered accurate at the family level. Furthermore, in large-scale benchmarking, we compare the performance of eThread to several alternative methods commonly used in protein structure prediction. Finally, we estimate the upper bound for this type of approach and discuss the directions towards further improvements. © 2012 Brylinski, Lingam.",,"accuracy; algorithm; article; automation; classifier; intermethod comparison; machine learning; molecular model; prediction; process development; process optimization; protein tertiary structure; quality control; Algorithms; Artificial Intelligence; Bayes Theorem; Computer Simulation; Databases, Protein; Models, Molecular; Protein Folding; Protein Structure, Tertiary; Proteins; Sequence Alignment; Software; Structural Homology, Protein",Article,Scopus,2-s2.0-84869842288
"Duan Y.","Value modeling and calculation for everything as a service (XaaS) based on reuse",2012,"Proceedings - 13th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing, SNPD 2012",21,10.1109/SNPD.2012.30,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868533599&doi=10.1109%2fSNPD.2012.30&partnerID=40&md5=32803c400c7d89723afe7319522bb676","Value is the premier pursuing which drives a service implementation and dominates service selection. Added value will help to maximize the business profiting from existing service system centering positive reuse or redundancy reducing. In view of absence of a general calculating mechanism of service value which focus on added value in the XaaS (everything as a service), we firstly model XaaS in a multiple semantics background. Then we identified the critical target object for value computation. Hence after we propose a calculation mechanism of service value and added value for situations including data as a service (DaaS), information as a service (FaaS), composition scenarios, optimization and their mix with usage explanations. Ongoing complicated implementation which centers service contracts is outlined. © 2012 IEEE.","added value; knowledge; multiple semantics; service contract; service value","Added values; knowledge; Service contract; Service selection; Service systems; service value; Target object; Artificial intelligence; Contracts; Digital storage; Maintenance; Semantics; Software engineering",Conference Paper,Scopus,2-s2.0-84868533599
"Görlitz O., Thimm M., Staab S.","SPLODGE: Systematic generation of SPARQL benchmark queries for Linked Open Data",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",21,10.1007/978-3-642-35176-1-8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868578889&doi=10.1007%2f978-3-642-35176-1-8&partnerID=40&md5=7d960a4b0443f355587ee8d4bfe12d46","The distributed and heterogeneous nature of Linked Open Data requires flexible and federated techniques for query evaluation. In order to evaluate current federation querying approaches a general methodology for conducting benchmarks is mandatory. In this paper, we present a classification methodology for federated SPARQL queries. This methodology can be used by developers of federated querying approaches to compose a set of test benchmarks that cover diverse characteristics of different queries and allows for comparability. We further develop a heuristic called SPLODGE for automatic generation of benchmark queries that is based on this methodology and takes into account the number of sources to be queried and several complexity parameters. We evaluate the adequacy of our methodology and the query generation strategy by applying them on the 2011 billion triple challenge data set. © 2012 Springer-Verlag Berlin Heidelberg.",,"Automatic Generation; Classification methodologies; Data sets; General methodologies; Query evaluation; Query generation; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868578889
"Pietrzykowski Z., Borkowski P., Wołejsza P.","Marine integrated navigational decision support system",2012,"Communications in Computer and Information Science",21,10.1007/978-3-642-34050-5_32,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868239437&doi=10.1007%2f978-3-642-34050-5_32&partnerID=40&md5=210340eef0916c98ca54680040568980","The article presents a concept of an integrated navigational decision support system for vessel traffic control. The system is based on a joint platform for decision support systems on land and ships. The platform construction is recommended to be based on the shipboard navigational decision support system. The necessary scope of work to build such system is defined. The proposed integrated decision support system NAVDEC fits such concepts as e-navigation, e-maritime and intelligent marine transport systems that reflect global trends in the development of navigational systems, widely using modern information communication technologies. © 2012 Springer-Verlag.","decision support system; navigation; sea transport","E-Navigation; Global trends; Information communication technology; Integrated decision; Marine transport; Navigational systems; Platform construction; Sea transport; Decision support systems; Information technology; Navigation; Waterway transportation; Wireless telecommunication systems; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868239437
"Di Cosmo R., Zacchiroli S., Zavattaro G.","Towards a formal component model for the cloud",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",21,10.1007/978-3-642-33826-7_11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868259961&doi=10.1007%2f978-3-642-33826-7_11&partnerID=40&md5=217ea70a03e12aed0316ab0c7e432e0b","We consider the problem of deploying and (re)configuring resources in a ""cloud"" setting, where interconnected software components and services can be deployed on clusters of heterogeneous (virtual) machines that can be created and connected on-the-fly. We introduce the Aeolus component model to capture similar scenarii from realistic cloud deployments, and instrument automated planning of day-to-day activities such as software upgrade planning, service deployment, elastic scaling, etc. We formalize the model and characterize the feasibility and complexity of configuration achievability in Aeolus. © 2012 Springer-Verlag.",,"Achievability; Automated planning; Component model; On-the-fly; Service deployment; Software component; Software upgrades; Artificial intelligence; Software engineering",Conference Paper,Scopus,2-s2.0-84868259961
"Tan H., Bao J., Zhou X.","A novel missense-mutation-related feature extraction scheme for 'driver' mutation identification",2012,"Bioinformatics",21,10.1093/bioinformatics/bts558,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869455546&doi=10.1093%2fbioinformatics%2fbts558&partnerID=40&md5=590ee1d3f49862968e32adb5582847dc","Motivation: It becomes widely accepted that human cancer is a disease involving dynamic changes in the genome and that the missense mutations constitute the bulk of human genetic variations. A multitude of computational algorithms, especially the machine learning-based ones, has consequently been proposed to distinguish missense changes that contribute to the cancer progression ('driver' mutation) from those that do not ('passenger' mutation). However, the existing methods have multifaceted shortcomings, in the sense that they either adopt incomplete feature space or depend on protein structural databases which are usually far from integrated. Results: In this article, we investigated multiple aspects of a missense mutation and identified a novel feature space that well distinguishes cancer-associated driver mutations from passenger ones. An index (DX score) was proposed to evaluate the discriminating capability of each feature, and a subset of these features which ranks top was selected to build the SVM classifier. Cross-validation showed that the classifier trained on our selected features significantly outperforms the existing ones both in precision and robustness. We applied our method to several datasets of missense mutations culled from published database and literature and obtained more reasonable results than previous studies. © 2012 The Author.",,"algorithm; article; artificial intelligence; computer program; genetics; human; missense mutation; neoplasm; Algorithms; Artificial Intelligence; Humans; Mutation, Missense; Neoplasms; Software",Article,Scopus,2-s2.0-84869455546
"Galitsky B.A., De La Rosa J.L., Dobrocsi G.","Inferring the semantic properties of sentences by mining syntactic parse trees",2012,"Data and Knowledge Engineering",21,10.1016/j.datak.2012.07.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867574918&doi=10.1016%2fj.datak.2012.07.003&partnerID=40&md5=85553e781532432aa6df605fddf870cc","We extend the mechanism of logical generalization toward syntactic parse trees and attempt to detect semantic signals unobservable in the level of keywords. Generalization from a syntactic parse tree as a measure of syntactic similarity is defined by the obtained set of maximum common sub-trees and is performed at the level of paragraphs, sentences, phrases and individual words. We analyze the semantic features of this similarity measure and compare it with the semantics of traditional anti-unification of terms. Nearest-Neighbor machine learning is then applied to relate the sentence to a semantic class. By using a syntactic parse tree-based similarity measure instead of the bag-of-words and keyword frequency approaches, we expect to detect a subtle difference between semantic classes that is otherwise unobservable. The proposed approach is evaluated in three distinct domains in which a lack of semantic information makes the classification of sentences rather difficult. We conclude that implicit indications of semantic classes can be extracted from syntactic structures. © 2012 Elsevier B.V.","Constituency parse tree; Machine learning; Search re-ranking","Anti-unification; Bag of words; Frequency approach; Nearest-neighbors; Parse trees; Re-ranking; Semantic class; Semantic features; Semantic information; Semantic properties; Similarity measure; Syntactic parse tree; Syntactic structure; Tree-based; Unobservable; Forestry; Learning systems; Syntactics; Semantics; Artificial Intelligence; Computation; Forestry; Information Retrieval",Article,Scopus,2-s2.0-84867574918
"Amer M.R., Xie D., Zhao M., Todorovic S., Zhu S.-C.","Cost-sensitive top-down/bottom-up inference for multiscale activity recognition",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",21,10.1007/978-3-642-33765-9_14,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867859826&doi=10.1007%2f978-3-642-33765-9_14&partnerID=40&md5=697018b0f5fdcf0776f42dd7c2a791d1","This paper addresses a new problem, that of multiscale activity recognition. Our goal is to detect and localize a wide range of activities, including individual actions and group activities, which may simultaneously co-occur in high-resolution video. The video resolution allows for digital zoom-in (or zoom-out) for examining fine details (or coarser scales), as needed for recognition. The key challenge is how to avoid running a multitude of detectors at all spatiotemporal scales, and yet arrive at a holistically consistent video interpretation. To this end, we use a three-layered AND-OR graph to jointly model group activities, individual actions, and participating objects. The AND-OR graph allows a principled formulation of efficient, cost-sensitive inference via an explore-exploit strategy. Our inference optimally schedules the following computational processes: 1) direct application of activity detectors - called α process; 2) bottom-up inference based on detecting activity parts - called β process; and 3) top-down inference based on detecting activity context - called γ process. The scheduling iteratively maximizes the log-posteriors of the resulting parse graphs. For evaluation, we have compiled and benchmarked a new dataset of high-resolution videos of group and individual activities co-occurring in a courtyard of the UCLA campus. © 2012 Springer-Verlag.",,"Activity recognition; Computational process; Cost-sensitive; Data sets; Detecting activities; Group activities; High resolution; Model groups; Multiscales; Spatio-temporal scale; Top-down inference; Topdown; Video interpretation; Video resolutions; Zoom-out; Artificial intelligence; Computer vision",Conference Paper,Scopus,2-s2.0-84867859826
"Sárosi J.","New approximation algorithm for the force of fluidic muscles",2012,"SACI 2012 - 7th IEEE International Symposium on Applied Computational Intelligence and Informatics, Proceedings",21,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866786064&partnerID=40&md5=ed7cadb4e2f65854c922c3e5025773f4","The newest and most promising type of pneumatic actuators is the pneumatic artificial muscle (PAM). Different designs have been developed, but the McKibben muscle is the most popular and is made commercially available by different companies (e. g. Fluidic Muscle manufactured by Festo Company). Pneumatic artificial muscles have a wide range of use in industrial and medical fields. There are a lot of advantages of these muscles like the high strength, good power-weight ratio, low price, little maintenance needed, great compliance, compactness, inherent safety and usage in rough environments. The main disadvantage is that their dynamic behavior is highly nonlinear. The most often mentioned characteristic of PAMs is the force as a function of pressure and contraction. In this paper our newest function approximation for the force generated by Fluidic Muscles is shown that can be generally used for different muscles made by Festo Company. ©2012 IEEE.",,"Dynamic behaviors; Fluidic muscles; Function approximation; Function of pressure; High strength; Highly nonlinear; Inherent safety; McKibben muscle; Medical fields; Pneumatic artificial muscle; Approximation algorithms; Artificial intelligence; Industry; Information science; Pneumatic drives; Robotic arms; Muscle",Conference Paper,Scopus,2-s2.0-84866786064
"Schadd M.P.D., Winands M.H.M., Tak M.J.W., Uiterwijk J.W.H.M.","Single-player Monte-Carlo tree search for SameGame",2012,"Knowledge-Based Systems",21,10.1016/j.knosys.2011.08.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865607699&doi=10.1016%2fj.knosys.2011.08.008&partnerID=40&md5=5a3420f8486ed10f3c3dcf5ecc3e2527","Classic methods such as A* and IDA* are a popular and successful choice for one-player games. However, without an accurate admissible evaluation function, they fail. In this article we investigate whether Monte-Carlo tree search (MCTS) is an interesting alternative for one-player games where A* and IDA* methods do not perform well. Therefore, we propose a new MCTS variant, called single-player Monte-Carlo tree search (SP-MCTS). The selection and backpropagation strategy in SP-MCTS are different from standard MCTS. Moreover, SP-MCTS makes use of randomized restarts. We tested IDA* and SP-MCTS on the puzzle SameGame and used the cross-entropy method to tune the SP-MCTS parameters. It turned out that our SP-MCTS program is able to score a substantial number of points on the standardized test set. © 2011 Elsevier B.V. All rights reserved.","Cross-entropy method; Monte-Carlo tree search; One-player game; Puzzle; SameGame","Cross-entropy method; One-player game; Puzzle; SameGame; Tree search; Artificial intelligence; Software engineering; Software testing",Article,Scopus,2-s2.0-84865607699
"Kaâniche M.-B., Brémond F.","Recognizing gestures by learning local motion signatures of hog descriptors",2012,"IEEE Transactions on Pattern Analysis and Machine Intelligence",21,10.1109/TPAMI.2012.19,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866693713&doi=10.1109%2fTPAMI.2012.19&partnerID=40&md5=d6aebcf39e436978c64dcd722969cd93","We introduce a new gesture recognition framework based on learning local motion signatures (LMSs) of HOG descriptors introduced by [1]. Our main contribution is to propose a new probabilistic learning-classification scheme based on a reliable tracking of local features. After the generation of these LMSs computed on one individual by tracking Histograms of Oriented Gradient (HOG) [2] descriptor, we learn a codebook of video-words (i.e., clusters of LMSs) using k-means algorithm on a learning gesture video database. Then, the video-words are compacted to a code-book of codewords by the Maximization of Mutual Information (MMI) algorithm. At the final step, we compare the LMSs generated for a new gesture w.r.t. the learned code-book via the k-nearest neighbors (k-NN) algorithm and a novel voting strategy. Our main contribution is the handling of the N to N mapping between codewords and gesture labels within the proposed voting strategy. Experiments have been carried out on two public gesture databases: KTH [3] and IXMAS [4]. Results show that the proposed method outperforms recent state-of-the-art methods. © 2012 IEEE.","feature tracking; Gesture recognition; HOG descriptors; motion detection; probabilistic learning and classification","Code-words; Codebooks; Descriptors; Feature-tracking; Histograms of oriented gradients; k-Means algorithm; K-nearest neighbors (k-NN) algorithms; Local feature; Local motions; Motion detection; Mutual informations; Probabilistic Learning; State-of-the-art methods; Video database; Voting strategies; Gesture recognition; Clustering algorithms; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; gesture; human; image enhancement; methodology; movement (physiology); physiology; reproducibility; sensitivity and specificity; videorecording; whole body imaging; Algorithms; Artificial Intelligence; Gestures; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Movement; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Video Recording; Whole Body Imaging",Article,Scopus,2-s2.0-84866693713
"Li L., Jin Q.","On stratified L-convergence spaces: Pretopological axioms and diagonal axioms",2012,"Fuzzy Sets and Systems",21,10.1016/j.fss.2012.02.012,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863783766&doi=10.1016%2fj.fss.2012.02.012&partnerID=40&md5=0fe84ebae9a892470d095889e834a7f8","This paper focuses on pretopological and diagonal axioms for stratified L-convergence spaces recently defined by the authors in 2011. This notion is slightly stronger than Jäger's stratified L-generalized convergence spaces. In particular, one pretopological axiom is stated without resorting to the notion of neighborhood L-filters; some generalizations of Kowalsky's and Fischer's diagonal conditions are presented. The relationship between these axioms are investigated. © 2012 Elsevier B.V. All rights reserved.","Diagonal axiom; Fuzzy relation; Pretopological axiom; Stratified L-convergence space; Stratified L-topological space; Topology","Diagonal axioms; Fuzzy relations; L-topological space; Pretopological axiom; Stratified L-convergence space; Artificial intelligence; Fuzzy sets; Topology",Article,Scopus,2-s2.0-84863783766
"Wu Q., Ye Y., Liu Y., Ng M.K.","SNP selection and classification of genome-wide SNP data using stratified sampling random forests",2012,"IEEE Transactions on Nanobioscience",21,10.1109/TNB.2012.2214232,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866484354&doi=10.1109%2fTNB.2012.2214232&partnerID=40&md5=67a477da7a592e108dcf847f3599cb5b","For high dimensional genome-wide association (GWA) case-control data of complex disease, there are usually a large portion of single-nucleotide polymorphisms (SNPs) that are irrelevant with the disease. A simple random sampling method in random forest using default mtry parameter to choose feature subspace, will select too many subspaces without informative SNPs. Exhaustive searching an optimal mtry is often required in order to include useful and relevant SNPs and get rid of vast of non-informative SNPs. However, it is too time-consuming and not favorable in GWA for high-dimensional data. The main aim of this paper is to propose a stratified sampling method for feature subspace selection to generate decision trees in a random forest for GWA high-dimensional data. Our idea is to design an equal-width discretization scheme for informativeness to divide SNPs into multiple groups. In feature subspace selection, we randomly select the same number of SNPs from each group and combine them to form a subspace to generate a decision tree. The advantage of this stratified sampling procedure can make sure each subspace contains enough useful SNPs, but can avoid a very high computational cost of exhaustive search of an optimal mtry, and maintain the randomness of a random forest. We employ two genome-wide SNP data sets (Parkinson case-control data comprised of 408803 SNPs and Alzheimer case-control data comprised of 380157 SNPs) to demonstrate that the proposed stratified sampling method is effective, and it can generate better random forest with higher accuracy and lower error bound than those by Breiman's random forest generation method. For Parkinson data, we also show some interesting genes identified by the method, which may be associated with neurological disorders for further biological investigations. © 2002-2011 IEEE.","Genome-wide association study; random forest; SNP; stratified sampling","Alzheimer; Case-control; Complex disease; Computational costs; Data sets; Discretization scheme; Error bound; Exhaustive search; Exhaustive searching; Feature subspace; Generation method; Genome-wide association; High dimensional data; High-dimensional; Informativeness; Multiple-group; Neurological disorders; Random forests; Simple random sampling; Single nucleotide polymorphisms; SNP; Stratified sampling; Disease control; Feature extraction; Genes; Optimization; Decision trees; algorithm; Alzheimer disease; article; artificial intelligence; biological model; biology; case control study; decision tree; genetic association; genetic database; genetics; human; methodology; Parkinson disease; single nucleotide polymorphism; Algorithms; Alzheimer Disease; Artificial Intelligence; Case-Control Studies; Computational Biology; Databases, Genetic; Decision Trees; Genome-Wide Association Study; Humans; Models, Genetic; Parkinson Disease; Polymorphism, Single Nucleotide",Article,Scopus,2-s2.0-84866484354
"Wijekoon J.H.B., Dudek P.","VLSI circuits implementing computational models of neocortical circuits",2012,"Journal of Neuroscience Methods",21,10.1016/j.jneumeth.2012.01.019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865738589&doi=10.1016%2fj.jneumeth.2012.01.019&partnerID=40&md5=8154fadbe41dec6ee0a56bcfd4d0fc66","This paper overviews the design and implementation of three neuromorphic integrated circuits developed for the COLAMN ("" Novel Computing Architecture for Cognitive Systems based on the Laminar Microcircuitry of the Neocortex"" ) project. The circuits are implemented in a standard 0.35. μm CMOS technology and include spiking and bursting neuron models, and synapses with short-term (facilitating/depressing) and long-term (STDP and dopamine-modulated STDP) dynamics. They enable execution of complex nonlinear models in accelerated-time, as compared with biology, and with low power consumption. The neural dynamics are implemented using analogue circuit techniques, with digital asynchronous event-based input and output. The circuits provide configurable hardware blocks that can be used to simulate a variety of neural networks. The paper presents experimental results obtained from the fabricated devices, and discusses the advantages and disadvantages of the analogue circuit approach to computational neural modelling. © 2012 Elsevier B.V..","Computing architecture; Mixed signal VLSI; Neocortex; Neural circuits; Neuromorphic; Silicon neuron; Silicon synapse; Spiking and bursting","article; cognition; computer model; computing architecture; devices; mixed signal very large scale of integration integrated circuit; neocortex; nerve cell; neuroscience; priority journal; synapse; Action Potentials; Artificial Intelligence; Computer Simulation; Computers; Humans; Models, Neurological; Neocortex; Neural Networks (Computer); Neuronal Plasticity; Neurons; Nonlinear Dynamics; Synapses",Article,Scopus,2-s2.0-84865738589
"El-Sebakhy E.A., Asparouhov O., Abdulraheem A.-A., Al-Majed A.-A., Wu D., Latinski K., Raharja I.","Functional networks as a new data mining predictive paradigm to predict permeability in a carbonate reservoir",2012,"Expert Systems with Applications",21,10.1016/j.eswa.2012.01.157,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861186454&doi=10.1016%2fj.eswa.2012.01.157&partnerID=40&md5=af7955c63d55dd928db8041185dae4ce","Permeability prediction has been a challenge to reservoir engineers due to the lack of tools that measure it directly. The most reliable data of permeability obtained from laboratory measurements on cores do not provide a continuous profile along the depth of the formation. Recently, researchers utilized statistical regression, neural networks, and fuzzy logic to estimate both permeability and porosity from well logs. Unfortunately, due to both uncertainty and imprecision, the developed predictive modelings are less accurate compared to laboratory experimental core data. This paper presents functional networks as a novel approach to forecast permeability using well logs in a carbonate reservoir. The new intelligence paradigm helps to overcome the most common limitations of the existing modeling techniques in statistics, data mining, machine learning, and artificial intelligence communities. To demonstrate the usefulness of the functional networks modeling strategy, we briefly describe its learning algorithm through simple distinct examples. Comparative studies were carried out using real-life industry wireline logs to compare the performance of the new framework with the most popular modeling schemes, such as linear/nonlinear regression, neural networks, and fuzzy logic inference systems. The results show that the performance of functional networks (separable and generalized associativity) architecture with polynomial basis is accurate, reliable, and outperforms most of the existing predictive data mining modeling approaches. Future work can be achieved using different structure of functional networks with different basis, interaction terms, ensemble and hybrid strategies, different clustering, and outlier identification techniques within different oil and gas challenge problems, namely, 3D passive seismic, identification of lithofacies types, history matching, rock mechanics, viscosity, risk assessment, and reservoir characterization. © 2012 Elsevier Ltd. All rights reserved.","Carbonate reservoir; Data mining; Feedforward neural networks; Functional networks; Fuzzy logic; Minimum description length; Permeability; Porosity; Statistical regression","Associativity; Carbonate reservoir; Comparative studies; Continuous profile; Different structure; Functional network; Fuzzy logic inference; History matching; Hybrid strategies; Intelligence communities; Interaction term; Laboratory measurements; Lithofacies; Minimum description length; Modeling approach; Modeling strategy; Modeling technique; Oil and gas; Outlier identification; Permeability prediction; Polynomial basis; Predictive data mining; Reservoir characterization; Reservoir engineers; Statistical regression; Well logs; Artificial intelligence; Data mining; Feedforward neural networks; Forecasting; Fuzzy logic; Learning algorithms; Mechanical permeability; Petroleum reservoirs; Porosity; Rock mechanics; Well logging; Petroleum reservoir engineering",Article,Scopus,2-s2.0-84861186454
"Popa M., Kemal Koc A., Rothkrantz L.J.M., Shan C., Wiggers P.","Kinect sensing of shopping related actions",2012,"Communications in Computer and Information Science",21,10.1007/978-3-642-31479-7_16,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865484552&doi=10.1007%2f978-3-642-31479-7_16&partnerID=40&md5=533b6af004a8f226db36f512c7d4dc12","Surveillance systems in shopping malls or supermarkets are usually used for detecting abnormal behavior. We used the distributed video cameras system to design digital shopping assistants which assess the behavior of customers while shopping, detect when they need assistance, and offer their support in case there is a selling opportunity. In this paper we propose a system for analyzing human behavior patterns related to products interaction, such as browse through a set of products, examine, pick products, try on, interact with the shopping cart, and look for support by waiving one hand. We used the Kinect sensor to detect the silhouettes of people and extracted discriminative features for basic action detection. Next we analyzed different classification methods, statistical and also spatio-temporal ones, which capture relations between frames, features, and basic actions. By employing feature level fusion of appearance and movement information we obtained an accuracy of 80% for the mentioned six basic actions. © 2012 Springer-Verlag.","Action Recognition; Kinect; Shopping Behavior; Surveillance","Abnormal behavior; Action recognition; Basic actions; Classification methods; Discriminative features; Feature level fusion; Human behaviors; Kinect; Selling opportunities; Shopping behavior; Shopping carts; Spatio-temporal; Surveillance systems; Shopping centers; Space surveillance; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84865484552
"Szlapczynski R., Szlapczynska J.","On evolutionary computing in multi-ship trajectory planning",2012,"Applied Intelligence",21,10.1007/s10489-011-0319-7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865140858&doi=10.1007%2fs10489-011-0319-7&partnerID=40&md5=1453f2725b86c80422efed020c78d5b3","The paper presents the updated version of Evolutionary Sets of Safe Ship Trajectories: a method which applies evolutionary algorithms and some of the assumptions of game theory to solving ship encounter situations. For given positions and motion parameters of the ships, the method finds a near optimal set of safe trajectories of all ships involved in an encounter. The method works in real time and the solutions must be returned within one minute, which enforces speeding up the optimization process. During the development of the method we have tested extensively various formulas for fitness function, problem-dedicated specialized operators as well as methods of selection. In the course of this research it turned out that some of the classic evolutionary mechanisms had to be modified for better performance, which included the order of some operations. The results of the adaptation process are presented here. The paper includes explicit description of all evolutionary mechanisms used and accentuates the research on improving the optimization process by adjusting evolutionary mechanisms to the problem. © 2011 The Author(s).","Decision support systems; Evolutionary algorithms; Ship collision avoidance","Adaptation process; Evolutionary computing; Evolutionary mechanisms; Evolutionary set; Fitness functions; Motion parameters; Optimal sets; Optimization process; Real time; Ship collision avoidance; Trajectory Planning; Artificial intelligence; Decision support systems; Evolutionary algorithms; Game theory; Optimization; Trajectories; Ships",Article,Scopus,2-s2.0-84865140858
"Phillips C.O., Syed Y., Parthaláin N.M., Zwiggelaar R., Claypole T.C., Lewis K.E.","Machine learning methods on exhaled volatile organic compounds for distinguishing COPD patients from healthy controls",2012,"Journal of Breath Research",21,10.1088/1752-7155/6/3/036003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863815951&doi=10.1088%2f1752-7155%2f6%2f3%2f036003&partnerID=40&md5=913fb2537464faa75d8072ddfb2319bc","Exhaled volatile organic compounds (VOCs) have shown promise in diagnosing chronic obstructive pulmonary disease (COPD) but studies have been limited by small sample size and potential confounders. An investigation was conducted in order to establish whether combinations of VOCs could identify COPD patients from age and BMI matched controls. Breath samples were collected from 119 stable COPD patients and 63 healthy controls. The samples were collected with a portable apparatus, and then assayed by gas chromatography and mass spectroscopy. Machine learning approaches were applied to the data and the automatically generated models were assessed using classification accuracy and receiver operating characteristic (ROC) curves. Cross-validation of the combinations correctly predicted the diagnosis in 79% of COPD patients and 64% of controls and an optimum area under the ROC curve of 0.82 was obtained. Comparison of current and ex smokers within the COPD group showed that smoking status was likely to affect the classification; with correct prediction of smoking status in 85% of COPD subjects. When current smokers were omitted from the analysis, prediction of COPD was similar at 78% but correct prediction of controls was increased to 74%. Applying different analytical methods to the largest group of subjects so far, suggests VOC analysis holds promise for diagnosing COPD but smoking status needs to be balanced. © 2012 IOP Publishing Ltd.",,"volatile organic compound; volatile organic compound; accuracy; aged; article; body mass; chronic obstructive lung disease; controlled study; exhalation; female; human; machine learning; major clinical study; male; mass fragmentography; prediction; priority journal; receiver operating characteristic; sample; smoking; validation process; artificial intelligence; breath analysis; exhalation; gas chromatography; mass spectrometry; methodology; pathophysiology; Aged; Artificial Intelligence; Breath Tests; Chromatography, Gas; Exhalation; Female; Humans; Male; Mass Spectrometry; Pulmonary Disease, Chronic Obstructive; Volatile Organic Compounds",Article,Scopus,2-s2.0-84863815951
"Mukherjee R., Chakraborty S.","Selection of EDM process parameters using biogeography-based optimization algorithm",2012,"Materials and Manufacturing Processes",21,10.1080/10426914.2011.610089,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864703363&doi=10.1080%2f10426914.2011.610089&partnerID=40&md5=42e7d669c1580d154fad4ccefc93ded0","Amongst the nontraditional machining processes, electric discharge machining (EDM) is considered to be one of the most important processes for machining intricate and complex shapes in various electrically conductive materials, including high-strength, temperature-resistant (HSTR) alloys, especially in aeronautical and automotive industries. For achieving the best performance of the EDM process, it is imperative to carry out parametric design which involves characterization of multiple process responses, such as material removal rate, tool wear rate, surface finish and surface integrity, heat affected zone, etc., with respect to different machining parameters, like peak current, pulse-on time, duty factor, gap voltage, and dielectric flushing pressure, followed by parametric optimization of the process. This article focuses on the application of the biogeography-based optimization (BBO) algorithm for single and multiobjective optimization of the responses of two EDM processes. The optimization performance of the BBO algorithm is compared with that of other population-based algorithms, e.g., genetic algorithm (GA), ant colony optimization (ACO), and artificial bee colony (ABC) algorithm. It is observed that the BBO algorithm performs better than the others with respect to the optimal process response values. © Taylor and Francis Group, LLC.","Biogeography-based optimization; Electric discharge machining; Process parameter; Response","Ant Colony Optimization (ACO); Artificial bee colonies; Biogeography-based optimizations; Complex shapes; Electrically conductive; Gap voltage; High-strength; Machining parameters; Material removal rate; Multiple process; Non-traditional machining; Optimal process; Parametric design; Parametric optimization; Peak currents; Population-based algorithm; Process parameters; Response; Surface finishes; Surface integrity; Temperature resistant; Tool wear rate; Artificial intelligence; Automotive industry; Conductive materials; Electric network parameters; Genetic algorithms; Heuristic algorithms; High strength alloys; Machining centers; Multiobjective optimization; Electric discharge machining",Article,Scopus,2-s2.0-84864703363
"Kiran M.S., Gündüz M.","A novel artificial bee colony-based algorithm for solving the numerical optimization problems",2012,"International Journal of Innovative Computing, Information and Control",21,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866019569&partnerID=40&md5=67f15dbebc999b25c3072b0372601cc4","Artificial Bee Colony (ABC) is one of the popular algorithms of swarm intelligence. The ABC algorithm simulates foraging and dance behaviors of real honey bee colonies. It has high performance and success for numerical benchmark optimization problems. Although solution exploration of ABC algorithm is good, exploitation to found food sources is poor. In this study, inspiring Genetic Algorithm (GA), we proposed a crossover operation-based neighbor selection technique for information sharing in the hive. Local search and exploitation abilities of the ABC were herewith improved. The experimental results show that the improved ABC algorithm generates the solutions that are significantly more closed to minimal ones than the basic ABC algorithm on the numerical optimization problems and estimation of energy demand problem. © 2012 ICIC International.","Artificial bee colony; Crossover operation; Estimation of energy demand; Neighbor selection; Numerical optimization; Swarm intelligence","Artificial bee colonies; Crossover operations; Energy demands; Neighbor selection; Numerical optimizations; Swarm Intelligence; Artificial intelligence; Energy management; Optimization; Genetic algorithms",Article,Scopus,2-s2.0-84866019569
"López-Ibáñez M., Stützle T.","An experimental analysis of design choices of multi-objective ant colony optimization algorithms",2012,"Swarm Intelligence",21,10.1007/s11721-012-0070-7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866482099&doi=10.1007%2fs11721-012-0070-7&partnerID=40&md5=ec8db7f59ffe093227c380dedbc64402","There have been several proposals on how to apply the ant colony optimization (ACO) metaheuristic to multi-objective combinatorial optimization problems (MOCOPs). This paper proposes a new formulation of these multi-objective ant colony optimization (MOACO) algorithms. This formulation is based on adding specific algorithm components for tackling multiple objectives to the basic ACO metaheuristic. Examples of these components are how to represent multiple objectives using pheromone and heuristic information, how to select the best solutions for updating the pheromone information, and how to define and use weights to aggregate the different objectives. This formulation reveals more similarities than previously thought in the design choices made in existing MOACO algorithms. The main contribution of this paper is an experimental analysis of how particular design choices affect the quality and the shape of the Pareto front approximations generated by each MOACO algorithm. This study provides general guidelines to understand how MOACO algorithms work, and how to improve their design. © 2012 Springer Science + Business Media, LLC.","Ant colony optimization; Experimental analysis; Multi-objective optimization; Multi-objective traveling salesman problem","Ant Colony Optimization (ACO); Ant Colony Optimization algorithms; Experimental analysis; Heuristic information; Metaheuristic; Multi objective; Multi objective optimizations (MOO); Multiobjective combinatorial optimization; Multiple objectives; Pareto front; Approximation algorithms; Artificial intelligence; Combinatorial optimization; Constrained optimization; Design; Multiobjective optimization; Pareto principle; Traveling salesman problem; Quality control",Article,Scopus,2-s2.0-84866482099
"Zhang Y., Wang L., Wu Q.","Modified Adaptive Cuckoo Search (MACS) algorithm and formal description for global optimisation",2012,"International Journal of Computer Applications in Technology",21,10.1504/IJCAT.2012.048675,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865751881&doi=10.1504%2fIJCAT.2012.048675&partnerID=40&md5=0613ced0e2a8aa0d4be91a72e6abca32","Bio-inspired algorithms, through imitating the regular pattern of life forms, often produce unexpected results. A novel global optimisation algorithm, Cuckoo Search (CS), is an example that simulates the brood behaviour of some species of cuckoos. By using Lévy distribution, the flying pattern of cuckoos is also imitated. However, the potential of cuckoo's search pattern is not fully discovered in CS algorithm. In this article, we introduce the CS algorithm and associated Lévy flights. A Modified Adaptive Cuckoo Search (MACS) is then proposed by introducing grouping, parallel, incentive, adaptive and information-sharing characteristics. Also, the formal descriptions of improving strategies are given. The proposed algorithm improves the basic CS algorithm without losing the characteristic of high-efficiency search of Lévy flights. Experiment results show that MACS outperforms basic CS algorithm on most test problems and possesses application potential for real-world problems. Copyright © 2012 Inderscience Enterprises Ltd.","Bio-inspired algorithm; Cuckoo search; Foraging strategy; Formal description; Global optimization; Lévy flights; Population diversity; Swarm intelligence","Bio-inspired algorithms; Cuckoo searches; Foraging strategy; Formal Description; Population diversity; Swarm Intelligence; Artificial intelligence; Global optimization; Algorithms",Article,Scopus,2-s2.0-84865751881
"Ho G.T.S., Ip W.H., Wu C.H., Tse Y.K.","Using a fuzzy association rule mining approach to identify the financial data association",2012,"Expert Systems with Applications",21,10.1016/j.eswa.2012.02.047,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862783731&doi=10.1016%2fj.eswa.2012.02.047&partnerID=40&md5=e9dabb0661752ca286cf89e6c674a95f","In the rapidly changing financial market, investors always have difficulty in deciding the right time to trade. In order to enhance investment profitability, investors desire a decision support system. The proposed artificial intelligence methodology provides investors with the ability to learn the association among different parameters. After the associations are extracted, investors can apply the rules in their decision support systems. In this work, the model is built with the ultimate goal of predicting the level of the Hang Seng Index in Hong Kong. The movement of Hang Seng Index, which is associated with other economics indices including the gross domestic product (GDP) index, the consumer price index (CPI), the interest rate, and the export value of goods from Hong Kong, is learnt by the proposed method. The case study shows that the proposed method is a feasible way to provide decision support for investors who may not be able to identify the hidden rules between the Hang Seng Index and other economics indices. © 2012 Elsevier Ltd. All rights reserved.","Financial data association; Fuzzy association rule; Fuzzy set theory; Hang Seng Index","Consumer price index; Decision supports; Export value; Financial data; Financial market; Fuzzy association rule; Gross domestic products; Hang Seng Index; Hidden rules; Hong-kong; Interest rates; Artificial intelligence; Decision support systems; Fuzzy rules; Fuzzy set theory; Profitability; Economics",Article,Scopus,2-s2.0-84862783731
"Kelly J., Gooding P., Pratt D., Ainsworth J., Welford M., Tarrier N.","Intelligent real-time therapy: Harnessing the power of machine learning to optimise the delivery of momentary cognitivebehavioural interventions",2012,"Journal of Mental Health",21,10.3109/09638237.2011.638001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864214034&doi=10.3109%2f09638237.2011.638001&partnerID=40&md5=a00355d44c25f890ea793fe20c824beb","Background Experience sampling methodology (ESM) [Csikszentmihalyi, M. & Larson, R. (1987). Validity and reliability of the experience-sampling method. Journal of Nervous and Mental Disease, 175(9), 526536] has been used to elucidate the cognitivebehavioural mechanisms underlying the development and maintenance of complex mental disorders as well as mechanisms involved in resilience from such states. We present an argument for the development of intelligent real-time therapy (iRTT). Machine learning and reinforcement learning specifically may be used to optimise the delivery of interventions by observing and altering the timing of real-time therapies based on ongoing ESM measures.Aims The aims of the present article are to outline the principles of iRTT and to consider how it would be applied to complex problems such as suicide prevention.Methods Relevant literature was identified through use of PychInfo.Results iRTT may provide an important and ecologically valid adjunct to traditional CBT, providing a means of balancing population-based data with individual data, thus addressing the ""knowledgepractice gap"" [Tarrier, N. (2010b). The cognitive and behavioral treatment of PTSD, what is known and what is known to be unknown: How not to fall into the practice gap. Clinical Psychology: Science and Practice, 17(2), 134143] and facilitating the delivery of interventions in situ, thereby addressing the ""therapyreal-world gap"".Conclusions iRTT may provide a platform for the development of individualised and multifaceted momentary intervention strategies that are ecologically valid and aimed at attenuating pathological pathways to complex mental health problems and amplifying pathways associated with resilience. © 2012 Informa UK, Ltd.","Cognitive behaviour therapy (CBT); Ecological momentary intervention (EMI); Experience sampling methodology (ESM); Intelligent real-time therapy (iRTT); Smartphone; Suicide prevention","article; bootstrapping; clinical assessment tool; cognitive therapy; coping behavior; human; intelligent real time therapy; intervention study; machine learning; medical practice; mental health; methodology; mobile phone; multimedia; posttraumatic stress disorder; process optimization; psychologic assessment; Psychosocial Adjustment to Illness Scale; psychotherapy; reinforcement; sampling; suicidal behavior; suicide; Artificial Intelligence; Cellular Phone; Cognitive Therapy; Computers, Handheld; Humans; Suicide; Therapy, Computer-Assisted; User-Computer Interface",Article,Scopus,2-s2.0-84864214034
"McDonald A.W.E., Afroz S., Caliskan A., Stolerman A., Greenstadt R.","Use fewer instances of the letter ""i"": Toward writing style anonymization",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",21,10.1007/978-3-642-31680-7_16,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864225669&doi=10.1007%2f978-3-642-31680-7_16&partnerID=40&md5=8cdcac944d394e4dfb9637a735bdae2f","This paper presents Anonymouth, a novel framework for anonymizing writing style. Without accounting for style, anonymous authors risk identification. This framework is necessary to provide a tool for testing the consistency of anonymized writing style and a mechanism for adaptive attacks against stylometry techniques. Our framework defines the steps necessary to anonymize documents and implements them. A key contribution of this work is this framework, including novel methods for identifying which features of documents need to change and how they must be changed to accomplish document anonymization. In our experiment, 80% of the user study participants were able to anonymize their documents in terms of a fixed corpus and limited feature set used. However, modifying pre-written documents were found to be difficult and the anonymization did not hold up to more extensive feature sets. It is important to note that Anonymouth is only the first step toward a tool to acheive stylometric anonymity with respect to state-of-the-art authorship attribution techniques. The topic needs further exploration in order to accomplish significant anonymity. © Springer-Verlag Berlin Heidelberg 2012.","anonymity; machine learning; privacy; stylometry","anonymity; Anonymization; Authorship attribution; Feature sets; Hold up; Novel methods; Risk Identification; Stylometry; User study; Writing style; Data privacy; Learning systems; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84864225669
"Li H., Leung K.-S., Wong M.-H.","Idock: A multithreaded virtual screening tool for flexible ligand docking",2012,"2012 IEEE Symposium on Computational Intelligence and Computational Biology, CIBCB 2012",21,10.1109/CIBCB.2012.6217214,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864054616&doi=10.1109%2fCIBCB.2012.6217214&partnerID=40&md5=3eba594c1354b73b1667c2c318ba1645","AutoDock Vina is a competitive protein-ligand docking tool well known for its fast execution and high accuracy. Nevertheless, when docking a massive number of ligands, Vina has to be run multiple times, repeating receptor parsing and grid maps building over and over again. There are tremendous requests for revising Vina to reuse precalculated data and incorporate built-in support for virtual screening. Hence we developed idock, inheriting from AutoDock Vina the accurate scoring function and the efficient optimization algorithm, and significantly improving the fundamental implementation and numerical model for even faster execution. idock achieves a speedup of 3.3 in terms of CPU time and a speedup of 7.5 in terms of elapsed time on average. idock is free and open source, available at https://GitHub.com/HongjianLi/idock. © 2012 IEEE.",,"Autodock; CPU time; Flexible ligands; Grid map; Multithreaded; Open sources; Optimization algorithms; Protein-ligand docking; Scoring functions; Virtual Screening; Algorithms; Artificial intelligence; Ligands; Bioinformatics",Conference Paper,Scopus,2-s2.0-84864054616
"Kühlwein D., Van Laarhoven T., Tsivtsivadze E., Urban J., Heskes T.","Overview and evaluation of premise selection techniques for large theory mathematics",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",21,10.1007/978-3-642-31365-3_30,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863617445&doi=10.1007%2f978-3-642-31365-3_30&partnerID=40&md5=46ba0a1a5da967299917b668e78c01d9","In this paper, an overview of state-of-the-art techniques for premise selection in large theory mathematics is provided, and new premise selection techniques are introduced. Several evaluation metrics are introduced, compared and their appropriateness is discussed in the context of automated reasoning in large theory mathematics. The methods are evaluated on the MPTP2078 benchmark, a subset of the Mizar library, and a 10% improvement is obtained over the best method so far. © 2012 Springer-Verlag.",,"Automated reasoning; Evaluation metrics; Selection techniques; Artificial intelligence; Mathematical techniques",Conference Paper,Scopus,2-s2.0-84863617445
"Kumar M., Yadav S.P.","The weakest t-norm based intuitionistic fuzzy fault-tree analysis to evaluate system reliability",2012,"ISA Transactions",21,10.1016/j.isatra.2012.01.004,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861590629&doi=10.1016%2fj.isatra.2012.01.004&partnerID=40&md5=891175a3e44c497894467414f118d9a8","In this paper, a new approach of intuitionistic fuzzy fault-tree analysis is proposed to evaluate system reliability and to find the most critical system component that affects the system reliability. Here weakest t-norm based intuitionistic fuzzy fault tree analysis is presented to calculate fault interval of system components from integrating expert's knowledge and experience in terms of providing the possibility of failure of bottom events. It applies fault-tree analysis, α-cut of intuitionistic fuzzy set and T ω (the weakest t-norm) based arithmetic operations on triangular intuitionistic fuzzy sets to obtain fault interval and reliability interval of the system. This paper also modifies Tanaka et al.'s fuzzy fault-tree definition. In numerical verification, a malfunction of weapon system ""automatic gun"" is presented as a numerical example. The result of the proposed method is compared with the listing approaches of reliability analysis methods. © 2012 ISA. Published by Elsevier Ltd. All rights reserved.","Intuitionistic fuzzy fault-tree analysis; Reliability analysis; Triangular intuitionistic fuzzy sets; Weakest t-norm","Arithmetic operations; Critical systems; Fault-trees; Intuitionistic fuzzy; Intuitionistic fuzzy sets; Knowledge and experience; Numerical example; Numerical verification; Reliability analysis method; System components; System reliability; Weakest t-norm; Weapon system; Fault tree analysis; Forestry; Reliability analysis; Fuzzy sets; Computation; Fuzzy Logic; Integration; Reliability; article; artificial intelligence; computer simulation; equipment; fuzzy logic; methodology; robotics; theoretical model; Artificial Intelligence; Computer Simulation; Equipment Failure Analysis; Fuzzy Logic; Models, Theoretical; Robotics",Article,Scopus,2-s2.0-84861590629
"Gebser M., Kaufmann B., Schaub T.","Multi-threaded ASP solving with clasp",2012,"Theory and Practice of Logic Programming",21,10.1017/S1471068412000166,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871955802&doi=10.1017%2fS1471068412000166&partnerID=40&md5=56511ac14b27c2c9b00fac7c2c8af12b","Abstract We present the new multi-threaded version of the state-of-the-art answer set solver clasp. We detail its component and communication architecture and illustrate how they support the principal functionalities of clasp. Also, we provide some insights into the data representation used for different constraint types handled by clasp. All this is accompanied by an extensive experimental analysis of the major features related to multi-threading in clasp. © Cambridge University Press 2012.",,"Answer set; Communication architectures; Constraint types; Data representations; Experimental analysis; Multi-threading; Multithreaded; Artificial intelligence; Software engineering; Logic programming",Conference Paper,Scopus,2-s2.0-84871955802
"Fassi A.E., Awasthi A., Viviani M.","Evaluation of carsharing network's growth strategies through discrete event simulation",2012,"Expert Systems with Applications",21,10.1016/j.eswa.2011.11.071,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857653920&doi=10.1016%2fj.eswa.2011.11.071&partnerID=40&md5=ab8f205640576323b0f650a2b760449e","Carsharing organizations are nowadays faced with the emergence of new markets due to the growing popularity of their services. To keep up with the growing demand, they have to constantly adapt their network and balance their stations' capacities by implementing new strategies. These strategies involve creation of new carsharing stations, increasing the capacity of stations, merging or demerging carsharing stations etc. Currently, the decision makers rely on an intuitive strategy selection process which often results in inadequate decisions being made representing an immediate loss in resources, time and market penetration. This paper presents a discrete event simulation based decision support tool that assists the decision makers in selecting best network growth strategies to implement for meeting adequately the demand growth while maximizing the members' satisfaction level and minimizing the number of vehicles used. Our discrete event simulation model allows modeling the activities at any given set of carsharing stations, regardless of their number and capacities. A benchmarking comparison of different potential strategies is done. An application of the proposed model on a region of Communauto's Montréal (Québec, Canada) carsharing network is provided. © 2011 Elsevier Ltd. All rights reserved.","Carsharing; Decision support system; Discrete event simulation; Network growth strategy; Operational analysis","Carsharing; Decision supports; Discrete events; Network growth strategy; Operational analysis; Artificial intelligence; Decision making; Decision support systems; Strategic planning; Discrete event simulation",Article,Scopus,2-s2.0-84857653920
"Zhou D., Wang B., Rahimi S.M., Wang X.","A study of recommending locations on location-based social network by collaborative filtering",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",21,10.1007/978-3-642-30353-1_22,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861742526&doi=10.1007%2f978-3-642-30353-1_22&partnerID=40&md5=3c0b1e8372035b311780d99a96feb652","The development of location-based social networking (LBSN) services is growing rapidly these days. Users of LBSN services are more interested in sharing tips and experiences of their visits to various locations. In this paper, we aim at a study of recommending locations to users on LBSNs by collaborative filtering (CF) recommenders based only on users' check-in data. We first design and develop a distributed crawler to collect a large amount of check-in data from Gowalla. Then, we use three ways to utilize the check-in data, namely, the binary utilization, the FIF utilization, and the probability utilization. According to different utilizations, we introduce different CF recommenders, namely, user-based, item-based and probabilistic latent semantic analysis (PLSA), to do the location recommendation. Finally, we conduct a set of experiments to compare the performances of different recommenders using different check-in utilizations. The experimental results show that PLSA recommender with the probability utilization outperforms other combinations of recommenders and utilizations for recommending locations to users on LBSN. © 2012 Springer-Verlag.","collaborative filtering; location-based social networks; probabilistic model; recommender systems","Check-in; Collaborative filtering; Distributed crawler; Location based; Probabilistic latent semantic analysis; Probabilistic models; Social Networks; Artificial intelligence; Recommender systems; Location based services",Conference Paper,Scopus,2-s2.0-84861742526
"Mielens J.D., Hoffman M.R., Ciucci M.R., McCulloch T.M., Jiang J.J.","Application of classification models to pharyngeal high-resolution manometry",2012,"Journal of Speech, Language, and Hearing Research",21,10.1044/1092-4388(2011/11-0088),"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862010878&doi=10.1044%2f1092-4388%282011%2f11-0088%29&partnerID=40&md5=6ad97fa766475326b0b89f6d6beab738","Purpose: The authors present 3 methods of performing pattern recognition on spatiotemporal plots produced by pharyngeal high-resolution manometry (HRM). Method: Classification models, including the artificial neural networks (ANNs) multilayer perceptron (MLP) and learning vector quantization (LVQ), as well as support vector machines (SVM), were evaluated for their ability to identify disordered swallowing. Data were collected from 12 control subjects and 13 subjects with swallowing disorders; for this experiment, these subjects swallowed 5-ml water boluses. Following extraction of relevant parameters, a subset of the data was used to train the models, and the remaining swallows were then independently classified by the networks. Results: All methods produced high average classification accuracies, with MLP, SVM, and LVQ achieving accuracies of 96.44%, 91.03%, and 85.39%, respectively. When evaluating the individual contributions of each parameter and groups of parameters to the classification accuracy, parameters pertaining to the upper esophageal sphincter were most valuable. Conclusion: Classification models show high accuracy in segregating HRM data sets and represent 1 method of facilitating application of HRM to the clinical setting by eliminating the time required for some aspects of data extraction and interpretation. © American Speech-Language-Hearing Association.","Artificial neural network; Classification model; Deglutition; Dysphagia; High-resolution manometry; Pharyngeal manometry","adult; aged; article; artificial intelligence; artificial neural network; biological model; classification; dysphagia; female; human; male; manometry; methodology; middle aged; pathophysiology; pharynx; physiology; pressure; sensitivity and specificity; swallowing; Adult; Aged; Aged, 80 and over; Artificial Intelligence; Deglutition; Deglutition Disorders; Female; Humans; Male; Manometry; Middle Aged; Models, Biological; Neural Networks (Computer); Pharynx; Pressure; Sensitivity and Specificity",Article,Scopus,2-s2.0-84862010878
"Zhang H., Zhou J., Miao D., Gao C.","Bayesian rough set model: A further investigation",2012,"International Journal of Approximate Reasoning",21,10.1016/j.ijar.2011.12.006,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862817884&doi=10.1016%2fj.ijar.2011.12.006&partnerID=40&md5=a8c19aa1662c5404245173a3b65b5744","Bayesian rough set model (BRSM), as the hybrid development between rough set theory and Bayesian reasoning, can deal with many practical problems which could not be effectively handled by original rough set model. In this paper, the equivalence between two kinds of current attribute reduction models in BRSM for binary decision problems is proved. Furthermore, binary decision problems are extended to multi-decision problems in BRSM. Some monotonic measures of approximation quality for multi-decision problems are presented, with which attribute reduction models for multi-decision problems can be suitably constructed. What is more, the discernibility matrices associated with attribute reduction for binary decision and multi-decision problems are proposed, respectively. Based on them, the approaches to knowledge reduction in BRSM can be obtained which corresponds well to the original rough set methodology. © 2011 Elsevier Inc. All rights reserved.","Attribute reduction; Bayesian rough set model; Binary decision; Discernibility matrix; Multi-decision","Attribute reduction; Bayesian rough sets; Binary decision; Discernibility matrix; Multi-decision; Artificial intelligence; Software engineering; Rough set theory",Article,Scopus,2-s2.0-84862817884
"Christensen R.G., Enuameh M.S., Noyes M.B., Brodsky M.H., Wolfe S.A., Stormo G.D.","Recognition models to predict DNA-binding specificities of homeodomain proteins",2012,"Bioinformatics",21,10.1093/bioinformatics/bts202,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863514508&doi=10.1093%2fbioinformatics%2fbts202&partnerID=40&md5=971a788865f4156bba9c1e76dcb6d464","Motivation: Recognition models for protein-DNA interactions, which allow the prediction of specificity for a DNA-binding domain based only on its sequence or the alteration of specificity through rational design, have long been a goal of computational biology. There has been some progress in constructing useful models, especially for C2H2 zinc finger proteins, but it remains a challenging problem with ample room for improvement. For most families of transcription factors the best available methods utilize k-nearest neighbor (KNN) algorithms to make specificity predictions based on the average of the specificities of the k most similar proteins with defined specificities. Homeodomain (HD) proteins are the second most abundant family of transcription factors, after zinc fingers, in most metazoan genomes, and as a consequence an effective recognition model for this family would facilitate predictive models of many transcriptional regulatory networks within these genomes. Results: Using extensive experimental data, we have tested several machine learning approaches and find that both support vector machines and random forests (RFs) can produce recognition models for HD proteins that are significant improvements over KNN-based methods. Cross-validation analyses show that the resulting models are capable of predicting specificities with high accuracy. We have produced a web-based prediction tool, PreMoTF (Predicted Motifs for Transcription Factors) (http://stormo.wustl.edu/PreMoTF), for predicting position frequency matrices from protein sequence using a RF-based model. © The Author(s) 2012.",,"Animalia; Metazoa; DNA; homeodomain protein; transcription factor; zinc finger protein; algorithm; amino acid sequence; animal; article; artificial intelligence; binding site; biology; chemistry; Drosophila; human; methodology; mouse; sequence alignment; statistical model; support vector machine; Algorithms; Amino Acid Sequence; Animals; Artificial Intelligence; Binding Sites; Computational Biology; DNA; Drosophila; Homeodomain Proteins; Humans; Mice; Models, Statistical; Sequence Alignment; Support Vector Machines; Transcription Factors; Zinc Fingers",Article,Scopus,2-s2.0-84863514508
"Chen X., Liu J., Huang Y., Fu B.","Transformer fault diagnosis using improved artificial fish swarm with rough set algorithm",2012,"Gaodianya Jishu/High Voltage Engineering",21,10.3969/j.issn.1003-6520.2012.06.018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864510253&doi=10.3969%2fj.issn.1003-6520.2012.06.018&partnerID=40&md5=b6154307e9c54fc25c78c9e6e2f709f4","Facing a large number of incomplete fault data, the traditional artificial intelligence methods cannot effectively and timely analyze or accurately diagnosed because of the ill-conditioned problem caused by inefficient discretization approaches. We presented a method based on rough set theory integrated with improved artificial fish swarm algorithm (AFSA) for fault diagnosis of transformer. Firstly, the values of dissolved gas analysis (DGA) in oil were taken as conditional attributes and the type faults were taken as decision attributes. Various relations between fault and symptom were connected, and decision table was established. Then, the improved artificial fish swarm algorithm was used to discrete continuous attribute, and the rough set theory was used to reduce the decision table. Finally, the simplified decision rules were got, which greatly simplified the difficulty of diagnosis. The experimental results indicate that the method increases the diagnosis accuracy compared with the traditional algorithm.","Artificial fish swarm algorithm (AFSA); Data reduction; Decision table; Dissolved gas analysis(DGA); Fault diagnosis; Rough set; Transformer","Artificial fish; Artificial fish swarm algorithms; Artificial intelligence methods; Continuous attribute; Decision attribute; Decision rules; Discretizations; Dissolved gas analysis; Fault data; Ill conditioned problems; Rough set; Transformer; Transformer fault diagnosis; Algorithms; Artificial intelligence; Data reduction; Decision tables; Failure analysis; Rough set theory; Fish",Article,Scopus,2-s2.0-84864510253
"Amato G., Broxvall M., Chessa S., Dragone M., Gennaro C., López R., Maguire L., Mcginnity T.M., Micheli A., Renteria A., O'Hare G.M.P., Pecora F.","Robotic UBIquitous COgnitive Network",2012,"Advances in Intelligent and Soft Computing",21,10.1007/978-3-642-28783-1_23,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861218029&doi=10.1007%2f978-3-642-28783-1_23&partnerID=40&md5=3559789ae481647e76d5e3f584c5156c","Robotic ecologies are networks of heterogeneous robotic devices pervasively embedded in everyday environments, where they cooperate to perform complex tasks. While their potential makes them increasingly popular, one fundamental problem is how to make them self-adaptive, so as to reduce the amount of preparation, pre-programming and human supervision that they require in real world applications. The EU FP7 project RUBICON develops self-sustaining learning solutions yielding cheaper, adaptive and efficient coordination of robotic ecologies. The approach we pursue builds upon a unique combination of methods from cognitive robotics, agent control systems, wireless sensor networks and machine learning. This paper briefly illustrates how these techniques are being extended, integrated, and applied to AAL applications. © 2012 Springer-Verlag.",,"Agent control systems; Cognitive network; Cognitive robotics; Complex task; Fundamental problem; Human supervision; Pre-programming; Real-world application; Robotic devices; Rubicon; Self-adaptive; Artificial intelligence; Ecology; Robotics",Conference Paper,Scopus,2-s2.0-84861218029
"Fazilat H., Akhlaghi S., Shiri M.E., Sharif A.","Predicting thermal degradation kinetics of nylon6/feather keratin blends using artificial intelligence techniques",2012,"Polymer",21,10.1016/j.polymer.2012.03.053,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860265537&doi=10.1016%2fj.polymer.2012.03.053&partnerID=40&md5=516fb38e7939f85eeb12ff69ff3d008a","A multi-structured architecture of artificial intelligence techniques including artificial neural network (ANN), adaptive-neuro-fuzzy-interference system (ANFIS) and radial basis function (RBF) were developed to predict thermal degradation kinetics (TDK) of nylon6 (NY6)/feather keratin (FK) blend films. By simultaneous implementation of back-propagation ANN and feed-forward ANFIS modeling on the experimental data obtained from thermogravimetric analysis (TGA) method, thermal degradation behavior of various compositions of NY6/FK blends was successfully predicted with minimum mean square errors (MSE). RBF networks were then trained on the TGA data at one heating rate for predicting analogs information at different heating rates, providing sufficient feed for TDK modeling. According to the comparison made between experimental and predicted kinetic parameters of thermal degradation process calculated from Friedman and Kissinger methods, the proposed prediction effort could effectively contribute to the estimation of precise activation energy (E a) and reaction order (n) values with least amount of experimental work and most accuracy. © 2012 Elsevier Ltd. All rights reserved.","Artificial intelligence techniques; Feather keratin; Thermal degradation kinetics","Artificial intelligence techniques; Blend films; Degradation behavior; Experimental data; Feather keratin; Feed-Forward; Kissinger methods; Minimum mean square error (mse); Nylon-6; Radial basis functions; Reaction orders; Thermal degradation kinetics; Activation energy; Forecasting; Heating rate; Kinetics; Neural networks; Radial basis function networks; Thermogravimetric analysis; Degradation",Article,Scopus,2-s2.0-84860265537
"Horty J.F., Bench-Capon T.J.M.","A factor-based definition of precedential constraint",2012,"Artificial Intelligence and Law",21,10.1007/s10506-012-9125-8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865155937&doi=10.1007%2fs10506-012-9125-8&partnerID=40&md5=6ae4690078bdc0e8288d28476bec650f","This paper describes one way in which a precise reason model of precedent could be developed, based on the general idea that courts are constrained to reach a decision that is consistent with the assessment of the balance of reasons made in relevant earlier decisions. The account provided here has the additional advantage of showing how this reason model can be reconciled with the traditional idea that precedential constraint involves rules, as long as these rules are taken to be defeasible. The account presented is firmly based on a body of work that has emerged in AI and Law. This work is discussed, and there is a particular discussion of approaches based on theory construction, and how that work relates to the model described in this paper. © 2012 Springer Science+Business Media B.V.","Case-based reasoning; Factor-based reasoning; Precedent; Rationales; Theory construction","AI and law; Factor-based reasoning; Precedent; Rationales; Theory construction; Artificial intelligence; Management",Article,Scopus,2-s2.0-84865155937
"Dimitriadis S.I., Laskaris N.A., Tzelepi A., Economou G.","Analyzing functional brain connectivity by means of commute times: A new approach and its application to track event-related dynamics",2012,"IEEE Transactions on Biomedical Engineering",21,10.1109/TBME.2012.2186568,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860355940&doi=10.1109%2fTBME.2012.2186568&partnerID=40&md5=9ffdd66989c2cf18c4be92763ed91fde","There is growing interest in studying the association of functional connectivity patterns with particular cognitive tasks. The ability of graphs to encapsulate relational data has been exploited in many related studies, where functional networks (sketched by different neural synchrony estimators) are characterized by a rich repertoire of graph-related metrics. We introduce commute times (CTs) as an alternative way to capture the true interplay between the nodes of a functional connectivity graph (FCG). CT is a measure of the time taken for a random walk to setout and return between a pair of nodes on a graph. Its computation is considered here as a robust and accurate integration, over the FCG, of the individual pairwise measurements of functional coupling. To demonstrate the benefits from our approach, we attempted the characterization of time evolving connectivity patterns derived from EEG signals recorded while the subject was engaged in an eye-movement task. With respect to standard ways, which are currently employed to characterize connectivity, an improved detection of event-related dynamical changes is noticeable. CTs appear to be a promising technique for deriving temporal fingerprints of the brains dynamic functional organization. © 1964-2012 IEEE.","Complex networks; encephalography; graph theory; time series analysis","Brain connectivity; Cognitive task; Complex networks; Connectivity pattern; EEG signals; encephalography; Functional connectivity; Functional network; Functional organization; Neural synchrony; Random Walk; Relational data; Eye movements; Graph theory; Time series analysis; article; brain function; commute time; electroencephalogram; event related potential; eye movement; female; functional brain connectivity; human; male; nervous system parameters; Algorithms; Artificial Intelligence; Brain; Brain Mapping; Electroencephalography; Eye Movements; Female; Humans; Male; Models, Neurological; Nerve Net; Pattern Recognition, Automated; Reproducibility of Results; Signal Processing, Computer-Assisted",Article,Scopus,2-s2.0-84860355940
"Levine D.S.","Neural dynamics of affect, gist, probability, and choice",2012,"Cognitive Systems Research",21,10.1016/j.cogsys.2011.07.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856629003&doi=10.1016%2fj.cogsys.2011.07.002&partnerID=40&md5=c5c8e09c9eebf44ef89ecd826adbbfc0","Recent behavioral data show that the traditional reduction of all probabilistic choices to choices among monetary gambles is inaccurate. Specifically, while decision makers tend to overweight low probabilities of obtaining any resource, the overweighting is greater when the resource is more emotionally evocative. We present a shunting nonlinear neural network that simulates the biasing effect of emotion on probabilistic choice. The network includes analogs of parts of the amygdala, orbitofrontal cortex, ventral striatum, thalamus, and anterior cingulate as well as sensory and premotor cortices. The network classifies prospective probabilistic options by means of an adaptive resonance module with vigilance selective for those attributes that are deemed most significant for the option currently being processed. The categories into which these options are placed embody significant gists of the options in a manner consistent with fuzzy trace theory. © 2011 Elsevier B.V.","Adaptive resonance theory; Amygdala; Decision making; Emotion; Fuzzy trace theory; Orbitofrontal cortex","Adaptive resonance theory; Amygdala; Emotion; Fuzzy trace; Orbitofrontal cortex; Artificial intelligence; Cognitive systems; Decision making; affect; amygdaloid nucleus; anterior cingulate; article; artificial neural network; behavior; controlled study; corpus striatum; decision making; emotion; fuzzy trace theory; nonlinear system; orbital cortex; pathological gambling; premotor cortex; priority journal; probability; sensory cortex; simulation; thalamus; theory",Article,Scopus,2-s2.0-84856629003
"Li J., Xu D., Gao W.","Removing label ambiguity in learning-based visual saliency estimation",2012,"IEEE Transactions on Image Processing",21,10.1109/TIP.2011.2179665,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859075549&doi=10.1109%2fTIP.2011.2179665&partnerID=40&md5=b5d1bf91292aa4411b6f1724a5f42a1f","Visual saliency is a useful clue to depict visually important image/video contents in many multimedia applications. In visual saliency estimation, a feasible solution is to learn a feature-saliency mapping model from the user data obtained by manually labeling activities or eye-tracking devices. However, label ambiguities may also arise due to the inaccurate and inadequate user data. To process the noisy training data, we propose a multi-instance learning to rank approach for visual saliency estimation. In our approach, the correlations between various image patches are incorporated into an ordinal regression framework. By iteratively refining a ranking model and relabeling the image patches with respect to their mutual correlations, the label ambiguities can be effectively removed from the training data. Consequently, visual saliency can be effectively estimated by the ranking model, which can pop out real targets and suppress real distractors. Extensive experiments on two public image data sets show that our approach outperforms 11 state-of-the-art methods remarkably in visual saliency estimation. © 2011 IEEE.","Label ambiguity; learning to rank; multi-instance learning (MIL); visual saliency","Eye tracking devices; Feasible solution; Image patches; Learning to rank; Mapping model; Multi-instance learning; Multimedia applications; Mutual correlations; Ordinal regression; Public image; Ranking model; Relabeling; State-of-the-art methods; Training data; User data; Visual saliency; Estimation; Visualization; algorithm; article; artifact; artificial intelligence; automated pattern recognition; computer assisted diagnosis; documentation; human; image enhancement; information retrieval; methodology; physiology; vision; Algorithms; Artifacts; Artificial Intelligence; Documentation; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Information Storage and Retrieval; Pattern Recognition, Automated; Visual Perception",Article,Scopus,2-s2.0-84859075549
"Sutcliffe G., Schulz S., Claessen K., Baumgartner P.","The TPTP typed first-order form with arithmetic",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",21,10.1007/978-3-642-28717-6_32,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858328958&doi=10.1007%2f978-3-642-28717-6_32&partnerID=40&md5=cd93ab8804b67c16597e0574493a1783","The TPTP World is a well established infrastructure supporting research, development, and deployment of Automated Theorem Proving systems. Recently, the TPTP World has been extended to include a typed first-order logic, which in turn has enabled the integration of arithmetic. This paper describes these developments. © 2012 Springer-Verlag.",,"Automated theorem proving; First order logic; First-order form; Artificial intelligence; Theorem proving; Automata theory",Conference Paper,Scopus,2-s2.0-84858328958
"Hetzl S., Leitsch A., Weller D.","Towards algorithmic cut-introduction",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",21,10.1007/978-3-642-28717-6_19,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858320477&doi=10.1007%2f978-3-642-28717-6_19&partnerID=40&md5=37f0d4dddb23c8f2887eda22a0a34645","We describe a method for abbreviating an analytic proof in classical first-order logic by the introduction of a lemma. Our algorithm is based on first computing a compressed representation of the terms present in the analytic proof and then a cut-formula that realizes such a compression. This method can be applied to the output of automated theorem provers, which typically produce analytic proofs. © 2012 Springer-Verlag.",,"Automated theorem prover; First order logic; Artificial intelligence; Algorithms",Conference Paper,Scopus,2-s2.0-84858320477
"Ortega-Martorell S., Lisboa P.J.G., Vellido A., Julià-Sapé M., Arús C.","Non-negative matrix factorisation methods for the spectral decomposition of MRS data from human brain tumours",2012,"BMC Bioinformatics",21,10.1186/1471-2105-13-38,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857951139&doi=10.1186%2f1471-2105-13-38&partnerID=40&md5=ecd95ae1caff85caf73f2d2ea834712b","Background: In-vivo single voxel proton magnetic resonance spectroscopy (SV 1H-MRS), coupled with supervised pattern recognition (PR) methods, has been widely used in clinical studies of discrimination of brain tumour types and follow-up of patients bearing abnormal brain masses. SV 1H-MRS provides useful biochemical information about the metabolic state of tumours and can be performed at short (&lt; 45 ms) or long (&gt; 45 ms) echo time (TE), each with particular advantages. Short-TE spectra are more adequate for detecting lipids, while the long-TE provides a much flatter signal baseline in between peaks but also negative signals for metabolites such as lactate. Both, lipids and lactate, are respectively indicative of specific metabolic processes taking place. Ideally, the information provided by both TE should be of use for clinical purposes. In this study, we characterise the performance of a range of Non-negative Matrix Factorisation (NMF) methods in two respects: first, to derive sources correlated with the mean spectra of known tissue types (tumours and normal tissue); second, taking the best performing NMF method for source separation, we compare its accuracy for class assignment when using the mixing matrix directly as a basis for classification, as against using the method for dimensionality reduction (DR). For this, we used SV 1H-MRS data with positive and negative peaks, from a widely tested SV 1H-MRS human brain tumour database.Results: The results reported in this paper reveal the advantage of using a recently described variant of NMF, namely Convex-NMF, as an unsupervised method of source extraction from SV1H-MRS. Most of the sources extracted in our experiments closely correspond to the mean spectra of some of the analysed tumour types. This similarity allows accurate diagnostic predictions to be made both in fully unsupervised mode and using Convex-NMF as a DR step previous to standard supervised classification. The obtained results are comparable to, or more accurate than those obtained with supervised techniques.Conclusions: The unsupervised properties of Convex-NMF place this approach one step ahead of classical label-requiring supervised methods for the discrimination of brain tumour types, as it accounts for their increasingly recognised molecular subtype heterogeneity. The application of Convex-NMF in computer assisted decision support systems is expected to facilitate further improvements in the uptake of MRS-derived information by clinicians. © 2012 Ortega-Martorell et al; licensee BioMed Central Ltd.",,"Biochemical information; Dimensionality reduction; Non-negative matrix factorisation; Spectral decomposition; Supervised classification; Supervised methods; Supervised pattern recognition; Unsupervised method; Artificial intelligence; Brain; Decision support systems; Lipids; Matrix algebra; Metabolism; Nuclear magnetic resonance spectroscopy; Pattern recognition; Tumors; Factorization; algorithm; article; automated pattern recognition; brain tumor; classification; factual database; human; metabolism; methodology; nuclear magnetic resonance spectroscopy; pathology; Algorithms; Brain Neoplasms; Databases, Factual; Humans; Magnetic Resonance Spectroscopy; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84857951139
"Ghosh S., Das S., Kundu D., Suresh K., Panigrahi B.K., Cui Z.","An inertia-adaptive particle swarm system with particle mobility factor for improved global optimization",2012,"Neural Computing and Applications",21,10.1007/s00521-010-0356-x,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857656487&doi=10.1007%2fs00521-010-0356-x&partnerID=40&md5=fe7d9873c4cf7c09ed1eb3a14e70b73a","Particle Swarm Optimization (PSO) has recently emerged as a nature-inspired algorithm for real parameter optimization. This article describes a method for improving the final accuracy and the convergence speed of PSO by firstly adding a new coefficient (called mobility factor) to the position updating equation and secondly modulating the inertia weight according to the distance between a particle and the globally best position found so far. The two-fold modification tries to balance between the explorative and exploitative tendencies of the swarm with an objective of achieving better search performance. We also mathematically analyze the effect of the modifications on the dynamics of the PSO algorithm. The new algorithm has been shown to be statistically significantly better than the basic PSO and four of its state-of-the-art variants on a twelve-function test-suite in terms of speed, accuracy, and robustness. © 2010 Springer-Verlag London Limited.","Benchmark functions; Comprehensive learning; Dynamics of swarm; Global optimization; Particle swarm systems; Swarm intelligence","Benchmark functions; Best position; Comprehensive learning; Convergence speed; Inertia weight; Mobility factors; Nature-inspired algorithms; Particle mobility; Particle swarm systems; PSO algorithms; Real-parameter optimization; Search performance; Swarm Intelligence; Algorithms; Artificial intelligence; Cellular automata; Dynamics; Global optimization; Oscillators (electronic); Steady flow; Particle swarm optimization (PSO)",Article,Scopus,2-s2.0-84857656487
"Atasoy S., Mateus D., Meining A., Yang G.-Z., Navab N.","Endoscopic video manifolds for targeted optical biopsy",2012,"IEEE Transactions on Medical Imaging",21,10.1109/TMI.2011.2174252,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857931952&doi=10.1109%2fTMI.2011.2174252&partnerID=40&md5=ae4708ef7a390545b906c24ec99f3183","Gastro-intestinal (GI) endoscopy is a widely used clinical procedure for screening and surveillance of digestive tract diseases ranging from Barrett's Oesophagus to oesophageal cancer. Current surveillance protocol consists of periodic endoscopic examinations performed in 3-4 month intervals including expert's visual assessment and biopsies taken from suspicious tissue regions. Recent development of a new imaging technology, called probe-based confocal laser endomicroscopy (pCLE), enabled the acquisition of in vivo optical biopsies without removing any tissue sample. Besides its several advantages, i.e., noninvasiveness, real-time and in vivo feedback, optical biopsies involve a new challenge for the endoscopic expert. Due to their noninvasive nature, optical biopsies do not leave any scar on the tissue and therefore recognition of the previous optical biopsy sites in surveillance endoscopy becomes very challenging. In this work, we introduce a clustering and classification framework to facilitate retargeting previous optical biopsy sites in surveillance upper GI-endoscopies. A new representation of endoscopic videos based on manifold learning, endoscopic video manifolds (EVMs), is proposed. The low dimensional EVM representation is adapted to facilitate two different clustering tasks; i.e., clustering of informative frames and patient specific endoscopic segments, only by changing the similarity measure. Each step of the proposed framework is validated on three in vivo patient datasets containing 1834, 3445, and 1546 frames, corresponding to endoscopic videos of 73.36, 137.80, and 61.84 s, respectively. Improvements achieved by the introduced EVM representation are demonstrated by quantitative analysis in comparison to the original image representation and principal component analysis. Final experiments evaluating the complete framework demonstrate the feasibility of the proposed method as a promising step for assisting the endoscopic expert in retargeting the optical biopsy sites. © 2011 IEEE.","Classification; clustering; gastro-intestinal (GI)-endoscopy; manifold learning; optical biopsy","Classification framework; Clinical procedure; clustering; Digestive tract; Endoscopic examination; Imaging technology; In-vivo; Low dimensional; Manifold learning; Optical biopsies; Original images; Patient datasets; Patient specific; Probe-based; Similarity measure; Tissue samples; Visual assessments; Biopsy; Classification (of information); Digestive system; Feedback; Monitoring; Principal component analysis; Security systems; Tissue; Video signal processing; Endoscopy; article; artificial intelligence; biopsy; cluster analysis; confocal microscopy; endoscope; factual database; gastrointestinal endoscopy; histology; human; image processing; instrumentation; methodology; pathology; principal component analysis; upper gastrointestinal tract; videorecording; Artificial Intelligence; Biopsy; Cluster Analysis; Databases, Factual; Endoscopes; Endoscopy, Gastrointestinal; Humans; Image Processing, Computer-Assisted; Microscopy, Confocal; Principal Component Analysis; Upper Gastrointestinal Tract; Video Recording",Article,Scopus,2-s2.0-84857931952
"Lancho Barrantes B.S., Guerrero Bote V.P., Rodríguez Z.C., De Moya Anegón F.","Citation flows in the zones of influence of scientific collaborations",2012,"Journal of the American Society for Information Science and Technology",21,10.1002/asi.21682,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857655541&doi=10.1002%2fasi.21682&partnerID=40&md5=c6e0a6989b59308acbd7a4e57a20a9b5","Domestic citation to papers from the same country and the greater citation impact of documents involving international collaboration are two phenomena that have been extensively studied and contrasted. Here, however, we show that it is not so much a national bias, but that papers have a greater impact on their immediate environments, an impact that is diluted as that environment grows. For this reason, the greatest biases are observed in countries with a limited production. Papers that involve international collaboration have a greater impact in general, on the one hand, because they have multiple ""immediate environments,"" and on the other because of their greater quality or prestige. In short, one can say that science knows no frontiers. Certainly there is a greater impact on the authors' immediate environment, but this does not necessarily have to coincide with their national environments, which fade in importance as the collaborative environment expands. © 2011 ASIS&T.",,"Citation impact; Collaborative environments; Immediate environment; International collaborations; Scientific collaboration; Artificial intelligence; Software engineering; International cooperation",Article,Scopus,2-s2.0-84857655541
"Hsu F.-M., Lin Y.-T., Ho T.-K.","Design and implementation of an intelligent recommendation system for tourist attractions: The integration of EBM model, Bayesian network and Google Maps",2012,"Expert Systems with Applications",21,10.1016/j.eswa.2011.09.013,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80255123279&doi=10.1016%2fj.eswa.2011.09.013&partnerID=40&md5=9a53f3f6fe26044f8348d2d144cc9dd7","Selecting tourist attractions and collecting related site information is one of the most crucial activities for a tourist when making decisions for a trip. Although various recommendation systems have been discussed over the last decade, rarely do such systems take individual tourist preference information into consideration. Based on the Engel-Blackwell-Miniard (EBM) model, this study used data published by the Tourism Bureau of Taiwan to develop a decision support system for tourist attractions. The probability of a tourist attraction appealing to a particular tourist is calculated utilizing a Bayesian network, and the accuracy of the prediction is validated by a ROC curve test. Finally, recommended routes and tourist attractions are presented through an interactive user interface using Google Maps. This study confirms that by combining the EBM model with a Bayesian network to propose a decision support system called the Intelligent Tourist Attractions System (ITAS). It has demonstrated good prediction of tourism attractions and provides useful map information to tourists. © 2011 Elsevier Ltd. All rights reserved.","Bayesian network; EBM model; Google Maps; ROC curve; Tourist attractions","EBM model; Google maps; Interactive user interfaces; Making decision; Preference information; ROC curves; Site information; Tourist attractions; Artificial intelligence; Bayesian networks; Decision making; Decision support systems; Distributed parameter networks; Inference engines; Intelligent networks; Recommender systems; User interfaces; Statistical tests",Article,Scopus,2-s2.0-80255123279
"Guo C., Zhibin J., Zhang H., Li N.","Decomposition-based classified ant colony optimization algorithm for scheduling semiconductor wafer fabrication system",2012,"Computers and Industrial Engineering",21,10.1016/j.cie.2011.09.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-82755177955&doi=10.1016%2fj.cie.2011.09.002&partnerID=40&md5=632dac669cbc0bfc7233ed4b24e4eff9","Due to its typical features, such as large-scale, multiple re-entrant flows, and hybrid machine types, the semiconductor wafer fabrication system (SWFS) is extremely difficult to schedule. In order to cope with this difficulty, the decomposition-based classified ant colony optimization (D-CACO) method is proposed and analyzed in this paper. The D-CACO method comprises decomposition procedure and classified ant colony optimization algorithm. In the decomposition procedure, a large and complicate scheduling problem is decomposed into several subproblems and these subproblems are scheduled in sequence. The classified ACO algorithm then groups all of the operations of the subproblems and schedules them according to machine type. To test the effect of the method, a set of simulations are conducted on a virtual fab simulation platform. The test results show that the proposed D-CACO algorithm works efficiently in scheduling SWFS.","Ant colony optimization; Decomposition; Decomposition-based classified ACO (D-CACO); Scheduling; Semiconductor wafer fabrication system (SWFS)","ACO algorithms; Ant Colony Optimization algorithms; Ant-colony optimization; Decomposition-based classified ACO (D-CACO); Hybrid machine; Scheduling problem; Semiconductor wafer fabrication system; Simulation platform; Sub-problems; Artificial intelligence; Decomposition; Fabrication; Optimization; Scheduling; Silicon wafers; Scheduling algorithms",Article,Scopus,2-s2.0-82755177955
"Mladenov M., Ahmadi B., Kersting K.","Lifted linear programming",2012,"Journal of Machine Learning Research",21,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885882677&partnerID=40&md5=023acce1dc6baadcb1b23baa89a6ab11","Lifted inference approaches have rendered large, previously intractable probabilistic inference problems quickly solvable by handling whole sets of indistinguishable objects together. Triggered by this success, we show that another important AI technique is liftable, too, namely linear programming. Intuitively, given a linear program (LP), we employ a lifted variant of Gaussian belief propagation (GaBP) to solve the systems of linear equations arising when running an interiorpoint method to solve the LP. However, this naïve solution cannot make use of standard solvers for linear equations and is doomed to construct lifted networks in each iteration of the interior-point method again, an operation that can itself be quite costly. To address both issues, we show how to read off an equivalent LP from the lifted GaBP computations that can be solved using any off-the-shelf LP solver. We prove the correctness of this compilation approac and experimentally demonstrate that it can greatly reduce the cost of solving LPs.",,"Artificial intelligence; Iterative methods; Linear equations; AI techniques; Belief propagation; Gaussians; Interior-point method; Linear programs; Probabilistic inference; Systems of linear equations; Linear programming",Conference Paper,Scopus,2-s2.0-84885882677
"Coquery E., Jabbour S., Sais L., Salhi Y.","A SAT-based approach for discovering frequent, closed and maximal patterns in a sequence",2012,"Frontiers in Artificial Intelligence and Applications",21,10.3233/978-1-61499-098-7-258,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878812211&doi=10.3233%2f978-1-61499-098-7-258&partnerID=40&md5=370f445c590e5708b7102814422d28c5","In this paper we propose a satisfiability-based approach for enumerating all frequent, closed and maximal patterns with wildcards in a given sequence. In this context, since frequency is the most used criterion, we introduce a new polynomial inductive formulation of the cardinality constraint as a Boolean formula. A nogood-based formulation of the anti-monotonicity property is proposed and dynamically used for pruning. This declarative framework allows us to exploit the efficiency of modern SAT solvers and particularly their clause learning component. The experimental evaluation on real world data shows the feasibility of our proposed approach in practice. © 2012 The Author(s).",,"Artificial intelligence; Constraint theory; Anti-monotonicity; Boolean formulae; Cardinality constraints; Clause learning; Experimental evaluation; Maximal patterns; SAT solvers; Satisfiability; Boolean algebra",Conference Paper,Scopus,2-s2.0-84878812211
"Fulginei F.R., Salvini A., Pulcini G.","Metric-topological-evolutionary optimization",2012,"Inverse Problems in Science and Engineering",21,10.1080/17415977.2011.624624,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858650184&doi=10.1080%2f17415977.2011.624624&partnerID=40&md5=3f12a8243cde588a9dee863a6d9f11f2","This article shows a novel approach for optimization and inverse problems based on evolutionary computation with the aim to satisfy two opposite requirements: exploration and convergence. The proposed approach is particularly suitable for parallel computing and it gives its best both for multimodal problems and for problems in which bad initializations can occur. The proposed algorithm has been called MeTEO to point out its metric-topological and evolutionary inspiration. In fact, it is based on a hybridization of two heuristics coming from swarm intelligence: the flock-of-starlings optimization (FSO; which shows high exploration capability but a lack of convergence), the standard particle swarm optimization (which is less explorative than FSO but with a good convergence capability) and a third evolutionary heuristic: the bacterial chemotaxis algorithm (that has no collective behaviour, no exploration skill but high convergence capability). Finally, with the aim of speeding up the algorithm, a technique that we call fitness modification is proposed and implemented. Suitable tests regarding optimization benchmarks and inverse problems will be presented with the aim of pointing out the MeTEO performances compared with those of each single heuristic used for hybridization. © 2012 Taylor & Francis.","Bacterial chemotaxis algorithm; Evolutionary computation; Flock-of-starlings optimization; Multimodal optimization; Particle swarm optimization","Bacterial chemotaxis; Collective behaviour; Multimodal optimization; Multimodal problems; Particle swarm; Swarm Intelligence; Algorithms; Artificial intelligence; Biochemistry; Differential equations; Evolutionary algorithms; Inverse problems; Parallel architectures; Topology; Particle swarm optimization (PSO)",Article,Scopus,2-s2.0-84858650184
"Ouammi A., Ghigliotti V., Robba M., Mimet A., Sacile R.","A decision support system for the optimal exploitation of wind energy on regional scale",2012,"Renewable Energy",21,10.1016/j.renene.2011.06.027,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79961128692&doi=10.1016%2fj.renene.2011.06.027&partnerID=40&md5=123d10216d33bfd9c97dae545ed96fe6","Wind is a promising sustainable energy resource that can help in reducing the dependence on fossil fuels. Models and tools can be effectively used to assess the resource availability, the possible exploitation, and the environmental impact. The aim of this work is to propose an Environmental Decision Support System (EDSS) for the sustainable design of wind power plants both in terms of the site selection over a regional territory and of the optimal technology to be installed. Specifically, the proposed EDSS is suited to territories with a complex orography (such as several regions of the Mediterranean coasts), and for the installation of plants in the class of power between 500 kW and 1000 kW. The different EDSS modules are applied to a specific case study, supporting the decision maker on the exploitation of wind power plants in the Savona District, Liguria Region, Italy. © 2011 Elsevier Ltd.","Decision support system; Geographic information systems; Optimization; Wind energy","Decision makers; Environmental decision support systems; Geographic information; Optimal exploitation; Optimal technology; Regional scale; Resource availability; Sustainable design; Sustainable energy; Artificial intelligence; Decision making; Energy resources; Environmental impact; Fossil fuels; Geographic information systems; Optimization; Power plants; Site selection; Sustainable development; Wind power; Decision support systems; decision making; decision support system; energy resource; environmental impact; exploitation; fossil fuel; GIS; installation; Mediterranean environment; optimization; orography; power plant; renewable resource; resource availability; site selection; sustainability; wind power; Italy; Liguria; Savona",Article,Scopus,2-s2.0-79961128692
"Kurzhanskiy A.A., Varaiya P.","Guaranteed prediction and estimation of the state of a road network",2012,"Transportation Research Part C: Emerging Technologies",21,10.1016/j.trc.2011.08.007,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84155177036&doi=10.1016%2fj.trc.2011.08.007&partnerID=40&md5=aa05c38119bb89fdd80ed588f3b731fa","The paper presents an algorithm for the prediction and estimation of the state of a road network comprising freeways and arterials, described by a Cell Transmission Model (CTM). CTM divides the network into a collection of links. Each link is characterized by its fundamental diagram, which relates link speed to link density. The state of the network is the vector of link densities. The state is observed through measurements of speed and flow on some links. Demand is specified by the volume of vehicles entering the network at some links, and by split ratios according to which vehicles are routed through the network. There is model uncertainty: the parameters of the fundamental diagram are uncertain. There is uncertainty in the demand around the nominal forecast. Lastly, the measurements are uncertain. The uncertainty in each model parameter, demand, and measurement is specified by an interval. Given measurements over a time interval [0,. t] and a horizon τ≥ 0, the algorithm computes a set of states with the guarantee that the actual state at time (t+ τ) will lie in this set, consistent with the given measurements. In standard terminology the algorithm is a state prediction or an estimate accordingly as τ> 0 or =0. The flow exiting a link may be controlled by an open- or closed-loop controller such as a signal or ramp meter. An open-loop controller does not change the algorithm, indeed it may make the system more predictable by tightening density bounds downstream of the controller. In the feedback case, the value of the control depends on the estimated state bounds, and the algorithm is extended to compute the range of possible closed-loop control values. The algorithm is used in a proposed design of a decision support system for the I-80 integrated corridor. © 2011 Elsevier Ltd.","Cell Transmission Model; Feedback control; Nonlinear filtering; Prediction; Set-valued estimation; Uncertainty","Artificial intelligence; Controllers; Decision support systems; Feedback; Feedback control; Forecasting; Nonlinear filtering; Numerical analysis; Roads and streets; Cell transmission model; Closed loop controllers; Closed-loop control; Fundamental diagram; Model uncertainties; Open loop controllers; Set-valued; Uncertainty; Uncertainty analysis; algorithm; decision making; decision support system; design; estimation method; parameterization; prediction; traffic management; uncertainty analysis",Article,Scopus,2-s2.0-84155177036
"Lipovetzky N., Geffner H.","Width and serialization of classical planning problems",2012,"Frontiers in Artificial Intelligence and Applications",21,10.3233/978-1-61499-098-7-540,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878786771&doi=10.3233%2f978-1-61499-098-7-540&partnerID=40&md5=caa4477d6e6bf62f04435f18fdc6ef75","We introduce a width parameter that bounds the complexity of classical planning problems and domains, along with a simple but effective blind-search procedure that runs in time that is exponential in the problem width. We show that many benchmark domains have a bounded and small width provided that goals are restricted to single atoms, and hence that such problems are provably solvable in low polynomial time. We then focus on the practical value of these ideas over the existing benchmarks which feature conjunctive goals. We show that the blind-search procedure can be used for both serializing the goal into subgoals and for solving the resulting problems, resulting in a 'blind' planner that competes well with a best-first search planner guided by state-of-the-art heuristics. In addition, ideas like helpful actions and landmarks can be integrated as well, producing a planner with state-of-the-art performance. © 2012 The Author(s).",,"Artificial intelligence; Polynomial approximation; Benchmark domains; Best first search; Blind searches; Classical planning; Polynomial-time; Single atoms; State of the art; State-of-the-art performance; Problem solving",Conference Paper,Scopus,2-s2.0-84878786771
"Schulz H., Behnke S.","Learning object-class segmentation with convolutional neural networks",2012,"ESANN 2012 proceedings, 20th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning",21,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885874914&partnerID=40&md5=c364264f8319ffbcabe9a27a0457a7b3","After successes at image classification, segmentation is the next step towards image understanding for neural networks. We propose a convolutional network architecture that includes innovative elements, such as multiple output maps, suitable loss functions, supervised pretraining, multiscale inputs, reused outputs, and pairwise class location filters. Experiments on three data sets show that our method performs on par with current in computer vision methods with regards to accuracy and exceeds them in speed. © 2012, i6doc.com publication. All rights reserved.",,"Artificial intelligence; Computer vision; Convolution; Image classification; Learning systems; Network architecture; Neural networks; Convolutional networks; Convolutional neural network; Learning objects; Location filter; Loss functions; Multiple outputs; Pre-training; Image segmentation",Conference Paper,Scopus,2-s2.0-84885874914
"Wiggins A., Sawyer S.","Intellectual diversity and the faculty composition of iSchools",2012,"Journal of the American Society for Information Science and Technology",21,10.1002/asi.21619,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-83655167247&doi=10.1002%2fasi.21619&partnerID=40&md5=7ca01621d31ad7b1ed002a40b0578191","We provide evidence and discuss findings regarding the intellectual distribution and faculty composition of academic units involved in the iSchool community. To better understand the intellectual heritage and major influences shaping the development of the individual and collective identities in iSchools, we develop a classification of the intellectual domains of iSchool faculty education. We use this to develop a descriptive analysis of the community's intellectual composition. The discussion focuses on characterizing intellectual diversity in the iSchools. We conclude with a discussion of the potential implications of these trends relative to the future development of the iSchool community. © 2011 ASIS&T.",,"Collective identity; Descriptive analysis; Artificial intelligence; Software engineering; Teaching",Article,Scopus,2-s2.0-83655167247
"Busser B.W., Huang D., Rogacki K.R., Lane E.A., Shokri L., Ni T., Gamble C.E., Gisselbrecht S.S., Zhu J., Bulyk M.L., Ovcharenko I., Michelson A.M.","Integrative analysis of the zinc finger transcription factor Lame duck in the Drosophila myogenic gene regulatory network",2012,"Proceedings of the National Academy of Sciences of the United States of America",20,10.1073/pnas.1210415109,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874416769&doi=10.1073%2fpnas.1210415109&partnerID=40&md5=829f6a02c13e2a196b526adadeb67a9f","Contemporary high-throughput technologies permit the rapid identification of transcription factor (TF) target genes on a genome-wide scale, yet the functional significance of TFs requires knowledge of target gene expression patterns, cooperating TFs, and cis-regulatory element (CRE) structures. Here we investigated the myogenic regulatory network downstream of the Drosophila zinc finger TF Lame duck (Lmd) by combining both previously published and newly performed genomic data sets, including ChIP sequencing (ChIP-seq), genome-wide mRNA profiling, cell-specific expression patterns of putative transcriptional targets, analysis of histone mark signatures, studies of TF cooccupancy by additional mesodermal regulators, TF binding site determination using protein binding microarrays (PBMs), and machine learning of candidate CRE motif compositions. Our findings suggest that Lmd orchestrates an extensive myogenic regulatory network, a conclusion supported by the identification of Lmd-dependent genes, histone signatures of Lmd-bound genomic regions, and the relationship of these features to cell-specific gene expression patterns. The heterogeneous cooccupancy of Lmd-bound regions with additional mesodermal regulators revealed that different transcriptional inputs are used to mediate similar myogenic gene expression patterns. Machine learning further demonstrated diverse combinatorial motif patterns within tissue-specific Lmd-bound regions. PBM analysis established the complete spectrum of Lmd DNA binding specificities, and site-directed mutagenesis of Lmd and additional newly discovered motifs in known enhancers demonstrated the critical role of these TF binding sites in supporting full enhancer activity. Collectively, these findings provide insights into the transcriptional codes regulating muscle gene expression and offer a generalizable approach for similar studies in other systems. © 2012, National Academy of Sciences. All rights reserved.",,"histone; myocyte enhancer factor 2; transcription factor; transcription factor FKHR; transcription factor Twist; unclassified drug; zinc finger transcription factor Lame duck; DNA; Drosophila protein; lmd protein, Drosophila; myogenic factor; transcriptome; animal cell; Article; binding affinity; binding site; DNA binding; Drosophila; enhancer region; gene expression; gene regulatory network; genome-wide association study; in vitro study; machine learning; muscle development; nonhuman; priority journal; protein binding; protein motif; site directed mutagenesis; animal; article; artificial intelligence; cytology; Drosophila melanogaster; gene expression regulation; genetics; genome; growth, development and aging; mesoderm; metabolism; molecular genetics; muscle development; myoblast; nucleotide sequence; systems biology; transgenic animal; Animals; Animals, Genetically Modified; Artificial Intelligence; Base Sequence; Binding Sites; DNA; Drosophila melanogaster; Drosophila Proteins; Enhancer Elements, Genetic; Gene Expression Regulation, Developmental; Gene Regulatory Networks; Genome, Insect; Mesoderm; Molecular Sequence Data; Muscle Development; Myoblasts; Myogenic Regulatory Factors; Systems Biology; Transcriptome",Article,Scopus,2-s2.0-84874416769
"Sezgin Sezer A.","A new view to ring theory via soft union rings, ideals and bi-ideals",2012,"Knowledge-Based Systems",20,10.1016/j.knosys.2012.04.031,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867878736&doi=10.1016%2fj.knosys.2012.04.031&partnerID=40&md5=960640abe2d77724059cadf3c07213cc","In this paper, by defining soft intersection-union product, we make a new approach to the classical ring theory via soft set theory, with the concept of soft union rings, ideals and bi-ideals. Moreover, we characterize regular, regular duo, intra-regular and strongly regular rings by soft union rings and ideals. © 2012 Elsevier B.V. All rights reserved.","Regular rings; Soft intersection-union product; Soft set; Soft union bi-ideal; Soft union left (right, two-sided) ideal","Ring theory; Soft intersection-union product; Soft sets; Soft union bi-ideal; Soft union left (right, two-sided) ideal; Artificial intelligence; Software engineering; Set theory",Article,Scopus,2-s2.0-84867878736
"Gatti E., Luciani D., Stella F.","A continuous time Bayesian network model for cardiogenic heart failure",2012,"Flexible Services and Manufacturing Journal",20,10.1007/s10696-011-9131-2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872091823&doi=10.1007%2fs10696-011-9131-2&partnerID=40&md5=a708ce1fd85cb32e91130634827ef092","Continuous time Bayesian networks are used to diagnose cardiogenic heart failure and to anticipate its likely evolution. The proposed model overcomes the strong modeling and computational limitations of dynamic Bayesian networks. It consists of both unobservable physiological variables, and clinically and instrumentally observable events which might support diagnosis like myocardial infarction and the future occurrence of shock. Three case studies related to cardiogenic heart failure are presented. The model predicts the occurrence of complicating diseases and the persistence of heart failure according to variations of the evidence gathered from the patient. Predictions are shown to be consistent with current pathophysiological medical understanding of clinical pictures. © Springer Science+Business Media, LLC 2011.","Cardiogenic heart failure; Continuous time Bayesian networks; Decision support system","Bayesian network models; Computational limitations; Dynamic Bayesian networks; Heart failure; Myocardial Infarction; Pathophysiological; Unobservable; Artificial intelligence; Cardiology; Continuous time systems; Decision support systems; Diagnosis; Health care; Bayesian networks",Article,Scopus,2-s2.0-84872091823
"Tian Y., Gao W., Yan S.","An improved inertia weight firefly optimization algorithm and application",2012,"Proceedings - 2012 International Conference on Control Engineering and Communication Technology, ICCECT 2012",20,10.1109/ICCECT.2012.38,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874341754&doi=10.1109%2fICCECT.2012.38&partnerID=40&md5=8311e7779e8e32a087b95a965285d664","Firefly Optimization Algorithm (FA) is a novel heuristic stochastic algorithm based on swarm intelligence, which is inspired by the fireflies' biochemical and collective behavior. But for the increasing of attractiveness and the light intensity, it may excessively increase the convergence rates of the algorithm, thus the optimizing results are easily repeated oscillation on the position of local or global extreme value point, and the optimizing accuracy is reduced. Therefore, an improved inertia weight firefly optimization algorithm (IWFA) is proposed in this paper, through the introduction of the inertia weight, the algorithm has a better ability to go on a global search in the early, and can avoid premature convergence into a local optimum, the algorithm has a small inertia weight to carry through a local search at a later stage, and can increase the optimization accuracy. The test results of five benchmark functions' optimization and PID parameters tuning show that the algorithm optimization ability is better than FA and the particle swarm optimization (PSO) algorithm. © 2012 IEEE.","Firefly Algorithm; Inertia Weight; Performance Evaluation; PID; Swarm Intelligence","Firefly algorithms; Inertia weight; Performance evaluation; PID; Swarm Intelligence; Artificial intelligence; Bioluminescence; Communication; Fire protection; Particle swarm optimization (PSO); Algorithms",Conference Paper,Scopus,2-s2.0-84874341754
"Broadbent C., Carayol A., Hague M., Serre O.","A saturation method for collapsible pushdown systems",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",20,10.1007/978-3-642-31585-5-18,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865012155&doi=10.1007%2f978-3-642-31585-5-18&partnerID=40&md5=5887ee52cf508d6edde67950cf7bdfe9","We introduce a natural extension of collapsible pushdown systems called annotated pushdown systems that replaces collapse links with stack annotations. We believe this new model has many advantages. We present a saturation method for global backwards reachability analysis of these models that can also be used to analyse collapsible pushdown systems. Beginning with an automaton representing a set of configurations, we build an automaton accepting all configurations that can reach this set. We also improve upon previous saturation techniques for higher-order pushdown systems by significantly reducing the size of the automaton constructed and simplifying the algorithm and proofs. © 2012 Springer-Verlag Berlin Heidelberg.",,"Higher-order; Natural extension; Pushdown systems; Reachability analysis; Saturation method; Artificial intelligence; Computer science; Automata theory",Conference Paper,Scopus,2-s2.0-84865012155
"Höferlin B., Netzel R., Höferlin M., Weiskopf D., Heidemann G.","Inter-active learning of ad-hoc classifiers for video visual analytics",2012,"IEEE Conference on Visual Analytics Science and Technology 2012, VAST 2012 - Proceedings",20,10.1109/VAST.2012.6400492,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872965316&doi=10.1109%2fVAST.2012.6400492&partnerID=40&md5=773292a0d24637f631933f0e2b917e2d","Learning of classifiers to be used as filters within the analytical reasoning process leads to new and aggravates existing challenges. Such classifiers are typically trained ad-hoc, with tight time constraints that affect the amount and the quality of annotation data and, thus, also the users' trust in the classifier trained. We approach the challenges of ad-hoc training by inter-active learning, which extends active learning by integrating human experts' background knowledge to greater extent. In contrast to active learning, not only does inter-active learning include the users' expertise by posing queries of data instances for labeling, but it also supports the users in comprehending the classifier model by visualization. Besides the annotation of manually or automatically selected data instances, users are empowered to directly adjust complex classifier models. Therefore, our model visualization facilitates the detection and correction of inconsistencies between the classifier model trained by examples and the user's mental model of the class definition. Visual feedback of the training process helps the users assess the performance of the classifier and, thus, build up trust in the filter created. We demonstrate the capabilities of inter-active learning in the domain of video visual analytics and compare its performance with the results of random sampling and uncertainty sampling of training sets. © 2012 IEEE.","H.3.3 [Information Systems]: Information Storage and Retrieval - Information Search and Retrieval; I.2.6 [Computing Methodologies]: Artificial Intelligence - Learning","Active Learning; Analytical reasoning; Background knowledge; Classifier models; Computing methodologies; Human expert; Information storage and retrieval; Mental model; Model visualization; Random sampling; Time constraints; Training process; Training sets; Visual analytics; Visual feedback; Artificial intelligence; Graphical user interfaces; Visual communication; Visualization; Active filters",Conference Paper,Scopus,2-s2.0-84872965316
"Wu J.-Z., Hao X.-C., Chien C.-F., Gen M.","A novel bi-vector encoding genetic algorithm for the simultaneous multiple resources scheduling problem",2012,"Journal of Intelligent Manufacturing",20,10.1007/s10845-011-0570-0,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870906620&doi=10.1007%2fs10845-011-0570-0&partnerID=40&md5=ce1b19531af3f18cfb6ea32661091110","To improve capital effectiveness in light of demand fluctuation, it is increasingly important for hightech companies to develop effective solutions for managing multiple resources involved in the production. To model and solve the simultaneous multiple resources scheduling problem in general, this study aims to develop a genetic algorithm (bvGA) incorporating with a novel bi-vector encoding method representing the chromosomes of operation sequence and seizing rules for resource assignment in tandem. The proposed model captured the crucial characteristics that the machines were dynamic configuration among multiple resources with limited availability and sequence-dependent setup times of machine configurations between operations would eventually affect performance of a scheduling plan. With the flexibility and computational intelligence that GA empowers, schedule planners can make advanced decisions on integrated machine configuration and job scheduling. According to a number of experiments with simulated data on the basis of a real semiconductor final testing facility, the proposed bvGA has shown practical viability in terms of solution quality as well as computation time. © Springer Science+Business Media, LLC 2011.","Flexible manufacturing systems; Genetic algorithm; Manufacturing management; Scheduling; Total resource management","Computation time; Demand fluctuations; Dynamic configuration; Effective solution; Encoding methods; High tech companies; Integrated machines; Job scheduling; Manufacturing management; Multiple resources; Operation sequences; Resource assignment; Resource management; Sequence-dependent setup time; Solution quality; Testing facility; Artificial intelligence; Encoding (symbols); Flexible manufacturing systems; Genetic algorithms; Scheduling",Article,Scopus,2-s2.0-84870906620
"Kolobov A., Daniel M., Weld S.","A theory of goal-oriented MDPs with dead ends",2012,"Uncertainty in Artificial Intelligence - Proceedings of the 28th Conference, UAI 2012",20,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886011828&partnerID=40&md5=909c95939b62fd8ec10d18a12768371e","Stochastic Shortest Path (SSP) MDPs is a problem class widely studied in AI, especially in probabilistic planning. They describe a wide range of scenarios but make the restrictive assumption that the goal is reachable from any state, i.e., that dead-end states do not exist. Because of this, SSPs are unable to model various scenarios that may have catastrophic events (e.g., an airplane possibly crashing if it flies into a storm). Even though MDP algorithms have been used for solving problems with dead ends, a principled theory of SSP extensions that would allow dead ends, including theoretically sound algorithms for solving such MDPs, has been lacking. In this paper, we propose three new MDP classes that admit dead ends under increasingly weaker assumptions. We present Value Iteration-based as well as the more efficient heuristic search algorithms for optimally solving each class, and explore theoretical relationships between these classes. We also conduct a preliminary empirical study comparing the performance of our algorithms on different MDP classes, especially on scenarios with unavoidable dead ends.",,"Catastrophic event; Dead-ends; Empirical studies; Goal-oriented; Heuristic search algorithms; Present value; Probabilistic planning; Stochastic shortest paths; Artificial intelligence; Heuristic algorithms; Iterative methods",Conference Paper,Scopus,2-s2.0-84886011828
"Tarlow D., Swersky K., Zemel R.S., Adams R.P., Frey B.J.","Fast exact inference for recursive cardinality models",2012,"Uncertainty in Artificial Intelligence - Proceedings of the 28th Conference, UAI 2012",20,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886079293&partnerID=40&md5=ae489cdbb6573b0a8a7692e0da22be0c","Cardinality potentials are a generally useful class of high order potential that affect probabilities based on how many of D binary variables are active. Maximum a posteriori (MAP) inference for cardinality potential models is well-understood, with efficient computations taking O(DlogD) time. Yet efficient marginalization and sampling have not been addressed as thoroughly in the machine learning community. We show that there exists a simple algorithm for computing marginal probabilities and drawing exact joint samples that runs in O(Dlog2 D) time, and we show how to frame the algorithm as efficient belief propagation in a low order tree-structured model that includes additional auxiliary variables. We then develop a new, more general class of models, termed Recursive Cardinality models, which take advantage of this efficiency. Finally, we show how to do efficient exact inference in models composed of a tree structure and a cardinality potential. We explore the expressive power of Recursive Cardinality models and empirically demonstrate their utility.",,"Auxiliary variables; Belief propagation; Efficient computation; High-order potentials; Machine learning communities; Marginal probability; Maximum a posteriori; SIMPLE algorithm; Algorithms; Artificial intelligence; Forestry; Trees (mathematics); Algorithms; Forestry; Mathematics; Trees",Conference Paper,Scopus,2-s2.0-84886079293
"Hagelbäck J.","Potential-field based navigation in StarCraft",2012,"2012 IEEE Conference on Computational Intelligence and Games, CIG 2012",20,10.1109/CIG.2012.6374181,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871978786&doi=10.1109%2fCIG.2012.6374181&partnerID=40&md5=21b259347e971352f6ad9712e7ab006c","Real-Time Strategy (Rts) games are a sub-genre of strategy games typically taking place in a war setting. Rts games provide a rich challenge for both human- and computer players (bots). Each player has a number of workers for gathering resources to be able to construct new buildings, train additional workers, build combat units and do research to unlock more powerful units or abilities. The goal is to create a strong army and destroy the bases of the opponent(s). Armies usually consists of a large number of units which must be able to navigate around the game world. The highly dynamic and real-time aspects of Rts games make pathfinding a challenging task for bots. Typically it is handled using pathfinding algorithms such as A*, which without adaptions does not cope very well with dynamic worlds. In this paper we show how a bot for StarCraft uses a combination of A* and potential fields to better handle the dynamic aspects of the game. © 2012 IEEE.",,"Dynamic aspects; Dynamic world; Path-finding algorithms; Pathfinding; Potential field; Real time aspects; Real-time strategy games; Artificial intelligence; Computational methods; Interactive computer graphics",Conference Paper,Scopus,2-s2.0-84871978786
"Kompella V.R., Luciw M., Schmidhuber J.","Incremental slow feature analysis: Adaptive low-complexity slow feature updating from high-dimensional input streams",2012,"Neural Computation",20,10.1162/NECO_a_00344,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867687400&doi=10.1162%2fNECO_a_00344&partnerID=40&md5=2ce1b95d75483d7230dd46ca9b16737a","We introduce here an incremental version of slow feature analysis (IncSFA), combining candid covariance-free incremental principal components analysis (CCIPCA) and covariance-free incremental minor components analysis (CIMCA). IncSFA's feature updating complexity is linear with respect to the input dimensionality,while batch SFA's (BSFA) updating complexity is cubic. IncSFAdoes not need to store, or even compute, any covariance matrices. The drawback to IncSFA is data efficiency: it does not use each data point as effectively as BSFA. But IncSFA allows SFA to be tractably applied, with just a few parameters, directly on highdimensional input streams (e.g., visual input of an autonomous agent), while BSFA has to resort to hierarchical receptive-field-based architectures when the input dimension is too high. Further, IncSFA's updates have simple Hebbian and anti-Hebbian forms, extending the biologicalplausibility of SFA. Experimental results show IncSFA learns the same set of features as BSFA and can handle a few cases where BSFA fails. © 2012 Massachusetts Institute of Technology.",,"algorithm; article; artificial intelligence; biological model; computer simulation; learning; principal component analysis; Algorithms; Artificial Intelligence; Computer Simulation; Learning; Models, Neurological; Principal Component Analysis",Article,Scopus,2-s2.0-84867687400
"Fernandes F.C., Rigden D.J., Franco O.L.","Prediction of antimicrobial peptides based on the adaptive neuro-fuzzy inference system application.",2012,"Biopolymers",20,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871155207&partnerID=40&md5=fae574d17ddd0b73af10afd580caa7b1","Antimicrobial peptides (AMPs) are widely distributed defense molecules and represent a promising alternative for solving the problem of antibiotic resistance. Nevertheless, the experimental time required to screen putative AMPs makes computational simulations based on peptide sequence analysis and/or molecular modeling extremely attractive. Artificial intelligence methods acting as simulation and prediction tools are of great importance in helping to efficiently discover and design novel AMPs. In the present study, state-of-the-art published outcomes using different prediction methods and databases were compared to an adaptive neuro-fuzzy inference system (ANFIS) model. Data from our study showed that ANFIS obtained an accuracy of 96.7% and a Matthew's Correlation Coefficient (MCC) of0.936, which proved it to be an efficient model for pattern recognition in antimicrobial peptide prediction. Furthermore, a lower number of input parameters were needed for the ANFIS model, improving the speed and ease of prediction. In summary, due to the fuzzy nature ofAMP physicochemical properties, the ANFIS approach presented here can provide an efficient solution for screening putative AMP sequences and for exploration of properties characteristic of AMPs.",,"antiinfective agent; peptide; algorithm; article; artificial intelligence; fuzzy logic; Algorithms; Anti-Infective Agents; Artificial Intelligence; Fuzzy Logic; Peptides",Article,Scopus,2-s2.0-84871155207
"Hendrickson S.J., Hazra A., Chen C., Eliassen A.H., Kraft P., Rosner B.A., Willett W.C.","β-Carotene 15,15′-monooxygenase 1 single nucleotide polymorphisms in relation to plasma carotenoid and retinol concentrations in women of European descent",2012,"American Journal of Clinical Nutrition",20,10.3945/ajcn.112.034934,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869805512&doi=10.3945%2fajcn.112.034934&partnerID=40&md5=06d21d154710e9bb5d17b1f84b1cfeab","Background: Carotenoids have been hypothesized to reduce the risk of many diseases, but associations with intakes or blood concentrations may arise from other constituents of fruit and vegetables. Use of genetic variation in β-carotene 15,15′-monooxygenase 1 (BCMO1), a key enzyme in provitamin A carotenoid metabolism, as a surrogate for carotenoid exposure may aid in determining the role of carotenoids unconfounded by other carotenoid-containing food constituents, but important variants must be identified. Objective: Our goal was to select BCMO1 single nucleotide polymorphisms (SNPs) that predict plasma carotenoid concentrations for use in future epidemiologic studies. Design: We assessed the associations between 224 SNPs in BCMO1 ± 20 kb imputed from the 1000 Genomes Project EUR reference panel with plasma carotenoid and retinol concentrations by using 7 case-control data sets (n = 2344) within the Nurses' Health Study, randomly divided into training (n = 1563) and testing (n = 781) data sets. SNPs were chosen in the training data set through stepwise selection in multivariate linear regression models; β-coefficients were used as weights in weighted gene scores. Results: Two or 3 SNPs were selected as predictors of β-carotene, α-carotene, β-cryptoxanthin, and lutein/zeaxanthin. In the testing data set, the weighted gene scores were significantly associated with plasma concentrations of the corresponding carotenoid (P = 6.4 × 10-12, 3.3 × 10-3, 0.02, and 1.8 × 10-17, respectively), and concentrations differed by 48%, 15%, 15%, and 36%, respectively, across extreme score quintiles. Conclusions: SNPs in BCMO1 are associated with plasma carotenoid concentrations. Given adequate sample size, the gene scores may be useful surrogates for carotenoid exposure in future studies. © 2012 American Society for Nutrition.",,"alpha carotene; beta carotene; beta carotene 15,15' monooxygenase; beta cryptoxanthin; carotenoid; lycopene; retinol; xanthophyll; zeaxanthin; adult; alcohol consumption; allele; article; controlled study; dietary intake; fat intake; female; genetic association; genetic variability; genome; genotype; human; human tissue; major clinical study; single nucleotide polymorphism; vitamin blood level; Adult; Artificial Intelligence; beta Carotene; beta-Carotene 15,15'-Monooxygenase; Biological Markers; Carotenoids; Case-Control Studies; Cohort Studies; European Continental Ancestry Group; Female; Genetic Markers; Genome-Wide Association Study; Humans; Lutein; Middle Aged; Polymorphism, Single Nucleotide; United States; Vitamin A; Xanthophylls",Article,Scopus,2-s2.0-84869805512
"Wilbik A., Keller J.M.","A distance metric for a space of linguistic summaries",2012,"Fuzzy Sets and Systems",20,10.1016/j.fss.2012.03.010,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866030133&doi=10.1016%2fj.fss.2012.03.010&partnerID=40&md5=752f4b04df5c3a47360c0a72743c9249","Producing linguistic summaries of large databases or temporal sequences of measurements is an endeavor that is receiving increasing attention. These summaries can be used in a continuous monitoring situation, like eldercare, where it is important to ascertain if the current summaries represent an abnormal condition. It is therefore necessary to compute the distance between summaries as a basis for such a determination. In this paper, we propose a dissimilarity measure between summaries based on fuzzy protoforms, and prove that this measure is a metric. We take into account not only the linguistic meaning of the summaries, but also two quality evaluations, namely the truth values and the degrees of focus. We present examples of how the distance metric behaves and show that it corresponds with intuition. © 2012 Elsevier B.V.","Distance metric; Linguistic modeling; Linguistic summarization","Abnormal conditions; Continuous monitoring; Dissimilarity measures; Distance metrics; Eldercare; Large database; Linguistic modeling; Linguistic summaries; Linguistic summarization; Protoforms; Quality evaluation; Temporal sequences; Truth values; Artificial intelligence; Fuzzy sets; Linguistics",Article,Scopus,2-s2.0-84866030133
"Kachroudi S., Grossard M., Abroug N.","Predictive driving guidance of full electric vehicles using particle swarm optimization",2012,"IEEE Transactions on Vehicular Technology",20,10.1109/TVT.2012.2212735,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869479033&doi=10.1109%2fTVT.2012.2212735&partnerID=40&md5=9ef8df1ae60cfad9c924b3da2b4cf169","This paper focuses on the development of computational algorithms for determining online energy-based driving guidance for an electric vehicle (EV) endowed with regenerative breaking system capabilities. A predictive decision support system is designed to optimally distribute energy flow between the instantaneous power demand requested by the driver for the powertrain engine and the different auxiliaries relating to comfort performance, such as the heating system. The proposed methodology uses an online particle swarm optimization (PSO) algorithm to search for a global optimum relative to specific objective functions, which take into account battery autonomy, driving comfort indexes, and travel time. Our methodology has been validated for a heavy motorized quadricycle vehicle using hardware-in-loop (HIL) simulations, for which the energy management system has been implemented in a digital signal processing (DSP) board communicating through a controller area network (CAN) protocol. © 1967-2012 IEEE.","Decision support; energy management system (EMS); full-electric vehicle (EV); particle swarm optimization (PSO); predictive strategy","Battery autonomy; Comfort performance; Computational algorithm; Controller area network; Decision supports; Driving comfort; Energy flow; Energy management system; full-electric vehicle (EV); Global optimum; Hardware-in-loop; Heating system; Instantaneous power; Objective functions; Particle swarm optimization algorithm; predictive strategy; Regenerative breaking; System capabilities; Algorithms; Artificial intelligence; Control system synthesis; Decision support systems; Electric vehicles; Energy management; Management; Signal processing; Particle swarm optimization (PSO)",Article,Scopus,2-s2.0-84869479033
"Kothari S., Osunkoya A.O., Phan J.H., Wang M.D.","Biological interpretation of morphological patterns in histopathological whole-slide images",2012,"2012 ACM Conference on Bioinformatics, Computational Biology and Biomedicine, BCB 2012",20,10.1145/2382936.2382964,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869425939&doi=10.1145%2f2382936.2382964&partnerID=40&md5=a893002e73f4990f56fffbc060de3366","We propose a framework for studying visual morphological patterns across histopathological whole-slide images (WSIs). Image representation is an important component of computeraided decision support systems for histopathological cancer diagnosis. Such systems extract hundreds of quantitative image features from digitized tissue biopsy slides and produce models for prediction. The performance of these models depends on the identification of informative features for selection of appropriate regions-of-interest (ROIs) from heterogeneous WSIs and for development of models. However, identification of informative features is hindered by the semantic gap between human interpretation of visual morphological patterns and quantitative image features. We address this challenge by using data mining and information visualization tools to study spatial patterns formed by features extracted from sub-sections of WSIs. Using ovarian serous cystadenocarcinoma (OvCa) WSIs provided by the cancer genome atlas (TCGA), we show that (1) individual and (2) multivariate image features correspond to biologically relevant ROIs, and (3) supervised image feature selection can map histopathology domain knowledge to quantitative image features. Algorithms, Design, Experimentation, and Verification Copyright © 2012 ACM.","Information visualization; Pathological image informatics; Whole-slide histopathology images","Biological interpretation; Cancer diagnosis; Cancer genome; Domain knowledge; Image features; Image representations; Information visualization; Morphological patterns; Pathological images; Quantitative images; Regions of interest; Semantic gap; Spatial patterns; Sub-sections; Tissue biopsies; Whole-slide histopathology images; Artificial intelligence; Bioinformatics; Decision support systems; Diseases; Information analysis; Information systems; Semantics; Tissue; Image processing",Conference Paper,Scopus,2-s2.0-84869425939
"Tiberghien T., Mokhtari M., Aloulou H., Biswas J.","Semantic reasoning in context-aware assistive environments to support ageing with dementia",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",20,10.1007/978-3-642-35173-0-14,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868525381&doi=10.1007%2f978-3-642-35173-0-14&partnerID=40&md5=195388a7e50d4063dc46516776ef6c10","Robust solutions for ambient assisted living are numerous, yet predominantly specific in their scope of usability. In this paper, we describe the potential contribution of semantic web technologies to building more versatile solutions a step towards adaptable context-aware engines and simplified deployments. Our conception and deployment work in hindsight, we highlight some implementation challenges and requirements for semantic web tools that would help to ease the development of context-aware services and thus generalize real-life deployment of semantically driven assistive technologies. We also compare available tools with regard to these requirements and validate our choices by providing some results from a real-life deployment. © 2012 Springer-Verlag Berlin Heidelberg.","Ambient Assisted Living; Context Awareness; Inference Engine; Knowledge Modelling; Semantic Web","Ambient assisted living; Assistive; Assistive technology; Context aware services; Context- awareness; Context-Aware; Knowledge modelling; Robust solutions; Semantic reasoning; Semantic Web technology; Artificial intelligence; Inference engines; Semantic Web; Information services",Conference Paper,Scopus,2-s2.0-84868525381
"Cai S., Su K.","Configuration checking with aspiration in local search for SAT",2012,"Proceedings of the National Conference on Artificial Intelligence",20,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868294022&partnerID=40&md5=a4b3941c3ff5301f158cab53ac615d06","An interesting strategy called configuration checking (CC) was recently proposed to handle the cycling problem in local search for Minimum Vertex Cover. A natural question is whether this CC strategy also works for SAT. The direct application of CC did not result in stochastic local search (SLS) algorithms that can compete with the current best SLS algorithms for SAT. In this paper, we propose a new heuristic based on CC for SLS algorithms for SAT, which is called configuration checking with aspiration (CCA). It is used to develop a new SLS algorithm called Swcca. The experiments on random 3-SAT instances show that Swcca significantly outperforms Sparrow2011, the winner of the random satisfiable category of the SAT Competition 2011, which is considered to be the best local search solver for random 3-SAT instances. Moreover, the experiments on structured instances show that Swcca is competitive with Sattime, the best local search solver for the crafted benchmark in the SAT Competition 2011. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Local search; Minimum vertex cover; Stochastic local searches; Experiments; Heuristic algorithms; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868294022
"Van Haaren J., Davis J.","Markov network structure learning: A randomized feature generation approach",2012,"Proceedings of the National Conference on Artificial Intelligence",20,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868286024&partnerID=40&md5=f29befdce7978910ce4dca1eb6432d44","The structure of a Markov network is typically learned in one of two ways. The first approach is to treat this task as a global search problem. However, these algorithms are slow as they require running the expensive operation of weight (i.e., parameter) learning many times. The second approach involves learning a set of local models and then combining them into a global model. However, it can be computationally expensive to learn the local models for datasets that contain a large number of variables and/or examples. This paper pursues a third approach that views Markov network structure learning as a feature generation problem. The algorithm combines a data-driven, specific-to-general search strategy with randomization to quickly generate a large set of candidate features that all have support in the data. It uses weight learning, with L1 regularization, to select a subset of generated features to include in the model. On a large empirical study, we find that our algorithm is equivalently accurate to other state-of-the-art methods while exhibiting a much faster run time. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Data sets; Empirical studies; Feature generation; Global models; Global search; L1 regularizations; Local model; Markov networks; Runtimes; Search strategies; State-of-the-art methods; Algorithms; Digital storage; Markov processes; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868286024
"Dickerson J.P., Procaccia A.D., Sandholm T.","Dynamic matching via weighted myopia with application to kidney exchange",2012,"Proceedings of the National Conference on Artificial Intelligence",20,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868283609&partnerID=40&md5=17a455253aa4492f618642556e15bd6b","In many dynamic matching applications - especially high-stakes ones - the competitive ratios of prior-free online algorithms are unacceptably poor. The algorithm should take distributional information about possible futures into account in deciding what action to take now. This is typically done by drawing sample trajectories of possible futures at each time period, but may require a prohibitively large number of trajectories or prohibitive memory and/or computation to decide what action to take. Instead, we propose to learn potentials of elements (e.g., vertices) of the current problem. Then, at run time, we simply run an offline matching algorithm at each time period, but subtracting out in the objective the potentials of the elements used up in the matching. We apply the approach to kidney exchange. Kidney exchanges enable willing but incompatible patient-donor pairs (vertices) to swap donors. These swaps typically include cycles longer than two pairs and chains triggered by altruistic donors. Fielded exchanges currently match myopically, maximizing the number of patients who get kidneys in an offline fashion at each time period. Myopic matching is sub-optimal; the clearing problem is dynamic since patients, donors, and altruists appear and expire over time. We theoretically compare the power of using potentials on increasingly large elements: vertices, edges, cycles, and the entire graph (optimum). Then, experiments show that by learning vertex potentials, our algorithm matches more patients than the current practice of clearing myopically. It scales to exchanges orders of magnitude beyond those handled by the prior dynamic algorithm. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Competitive ratio; Distributional information; Dynamic algorithm; Dynamic matching; Kidney exchanges; Matching algorithm; Offline; On-line algorithms; Orders of magnitude; Runtimes; Time-periods; Artificial intelligence; Algorithms",Conference Paper,Scopus,2-s2.0-84868283609
"Ziari S., Ezzati R., Abbasbandy S.","Numerical solution of linear fuzzy fredholm integral equations of the second kind using fuzzy haar wavelet",2012,"Communications in Computer and Information Science",20,10.1007/978-3-642-31718-7_9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868094885&doi=10.1007%2f978-3-642-31718-7_9&partnerID=40&md5=148f39c2128b1836825f5d2ef8b75e7a","In this paper, a new approach based on fuzzy Haar wavelet is proposed to solve linear fuzzy Fredholm integral equations of the second kind (FFIE-2). Moreover, the error estimate of the proposed method is given. Finally, illustrative examples are included to show the accuracy and efficiency of the proposed method. © 2012 Springer-Verlag Berlin Heidelberg.","Error estimation; Fuzzy Fredholm Integral Equation; Fuzzy Haar Wavelet","Error estimates; Fredholm integral equations; Haar wavelets; Illustrative examples; Numerical solution; Artificial intelligence; Data processing; Error analysis; Integral equations; Knowledge based systems; Information management",Conference Paper,Scopus,2-s2.0-84868094885
"Carmona J., Gavaldà R.","Online techniques for dealing with concept drift in process mining",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",20,10.1007/978-3-642-34156-4_10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868029147&doi=10.1007%2f978-3-642-34156-4_10&partnerID=40&md5=2df866492825eeb17cf73ce86af40c67","Concept drift is an important concern for any data analysis scenario involving temporally ordered data. In the last decade Process mining arose as a discipline that uses the logs of information systems in order to mine, analyze and enhance the process dimension. There is very little work dealing with concept drift in process mining. In this paper we present the first online mechanism for detecting and managing concept drift, which is based on abstract interpretation and sequential sampling, together with recent learning techniques on data streams. © Springer-Verlag Berlin Heidelberg 2012.",,"Abstract interpretations; Concept drifts; Data stream; In-process; Learning techniques; Online mechanism; Online technique; Ordered data; Process mining; Sequential sampling; Artificial intelligence; Data mining",Conference Paper,Scopus,2-s2.0-84868029147
"Read J., Bifet A., Pfahringer B., Holmes G.","Batch-incremental versus instance-incremental learning in dynamic and evolving data",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",20,10.1007/978-3-642-34156-4_29,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868020739&doi=10.1007%2f978-3-642-34156-4_29&partnerID=40&md5=1694616c06fb3063583e97693c875b45","Many real world problems involve the challenging context of data streams, where classifiers must be incremental: able to learn from a theoretically- infinite stream of examples using limited time and memory, while being able to predict at any point. Two approaches dominate the literature: batch-incremental methods that gather examples in batches to train models; and instance-incremental methods that learn from each example as it arrives. Typically, papers in the literature choose one of these approaches, but provide insufficient evidence or references to justify their choice. We provide a first in-depth analysis comparing both approaches, including how they adapt to concept drift, and an extensive empirical study to compare several different versions of each approach. Our results reveal the respective advantages and disadvantages of the methods, which we discuss in detail. © Springer-Verlag Berlin Heidelberg 2012.","data streams; dynamic; evolving; incremental; on-line","Concept drifts; Data stream; Empirical studies; evolving; In-depth analysis; incremental; on-line; Real-world problem; Train model; Artificial intelligence; Dynamics; Data communication systems",Conference Paper,Scopus,2-s2.0-84868020739
"Baatz G., Saurer O., Köser K., Pollefeys M.","Large scale visual geo-localization of images in mountainous terrain",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",20,10.1007/978-3-642-33709-3_37,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867869384&doi=10.1007%2f978-3-642-33709-3_37&partnerID=40&md5=32f38345a754c0da5e7dc7cd0990236b","Given a picture taken somewhere in the world, automatic geo-localization of that image is a task that would be extremely useful e.g. for historical and forensic sciences, documentation purposes, organization of the world's photo material and also intelligence applications. While tremendous progress has been made over the last years in visual location recognition within a single city, localization in natural environments is much more difficult, since vegetation, illumination, seasonal changes make appearance-only approaches impractical. In this work, we target mountainous terrain and use digital elevation models to extract representations for fast visual database lookup. We propose an automated approach for very large scale visual localization that can efficiently exploit visual information (contours) and geometric constraints (consistent orientation) at the same time. We validate the system on the scale of a whole country (Switzerland, 40 000km 2) using a new dataset of more than 200 landscape query pictures with ground truth. © 2012 Springer-Verlag.",,"Automated approach; Data sets; Digital elevation model; Forensic science; Geometric constraint; Ground truth; Lookups; Mountainous terrain; Natural environments; Seasonal changes; Switzerland; Visual database; Visual information; Visual localization; Visual location; Automated approach; Consistent orientations; Digital elevation model; Geometric constraint; Mountainous terrain; Natural environments; Visual localization; Visual location recognition; Artificial intelligence; Query processing; Computer vision; Computer vision",Conference Paper,Scopus,2-s2.0-84867869384
"Rohrbach M., Regneri M., Andriluka M., Amin S., Pinkal M., Schiele B.","Script data for attribute-based recognition of composite activities",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",20,10.1007/978-3-642-33718-5_11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867726359&doi=10.1007%2f978-3-642-33718-5_11&partnerID=40&md5=1ddb4af2dd0b7234bdbcb5636c3ae994","State-of-the-art human activity recognition methods build on discriminative learning which requires a representative training set for good performance. This leads to scalability issues for the recognition of large sets of highly diverse activities. In this paper we leverage the fact that many human activities are compositional and that the essential components of the activities can be obtained from textual descriptions or scripts. To share and transfer knowledge between composite activities we model them by a common set of attributes corresponding to basic actions and object participants. This attribute representation allows to incorporate script data that delivers new variations of a composite activity or even to unseen composite activities. In our experiments on 41 composite cooking tasks, we found that script data to successfully capture the high variability of composite activities. We show improvements in a supervised case where training data for all composite cooking tasks is available, but we are also able to recognize unseen composites by just using script data and without any manual video annotation. © 2012 Springer-Verlag.",,"Basic actions; Discriminative learning; Essential component; High variability; Human activities; Human activity recognition; Scalability issue; Textual description; Training data; Training sets; Video annotations; Artificial intelligence; Computer vision",Conference Paper,Scopus,2-s2.0-84867726359
"Horikoshi H., Kikuchi A., Onoguchi M., Sjöstrand K., Edenbrandt L.","Computer-aided diagnosis system for bone scintigrams from Japanese patients: Importance of training database",2012,"Annals of Nuclear Medicine",20,10.1007/s12149-012-0620-5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867992792&doi=10.1007%2fs12149-012-0620-5&partnerID=40&md5=1529af3e1c96b58df0d7c0123000c170","Aim Computer-aided diagnosis (CAD) software for bone scintigrams have recently been introduced as a clinical quality assurance tool. The purpose of this study was to compare the diagnostic accuracy of two CAD systems, one based on a European and one on a Japanese training database, in a group of bone scans from Japanese patients. Method The two CAD software are trained to interpret bone scans using training databases consisting of bone scans with the desired interpretation, metastatic disease or not. One software was trained using 795 bone scans from European patients and the other with 904 bone scans from Japanese patients. The two CAD softwares were evaluated using the same group of 257 Japanese patients, who underwent bone scintigraphy because of suspected metastases of malignant tumors in 2009. The final diagnostic results made by clinicians were used as gold standard. Results The Japanese CAD software showed a higher specificity and accuracy compared to the European CAD software [81 vs. 57 % (p<0.05) and 82 vs. 61 % (p<0.05), respectively]. The sensitivity was 90 % for the Japanese CAD software and 83 % for the European CAD software (n.s). Conclusion The CAD software trained with a Japanese database showed significantly higher performance than the corresponding CAD software trained with a European database for the analysis of bone scans from Japanese patients. These results could at least partly be caused by the physical differences between Japanese and European patients resulting in less influence of attenuation in Japanese patients and possible different judgement of count intensities of hot spots © The Author(s) 2012.","Artificial neural networks; Bone metastases; Bone scintigram; Computer-aided diagnosis","adult; article; bone metastasis; bone scintiscanning; computer assisted diagnosis; controlled study; data analysis software; diagnostic accuracy; diagnostic test accuracy study; Europe; female; human; intermethod comparison; Japan; major clinical study; male; priority journal; sensitivity and specificity; Artificial Intelligence; Bone and Bones; Diagnosis, Computer-Assisted; Female; Humans; Japan; Male; Middle Aged; Sensitivity and Specificity; Software",Article,Scopus,2-s2.0-84867992792
"Cruz S.A.B., Monteiro A.M.V., Santos R.","Automated geospatial Web Services composition based on geodata quality requirements",2012,"Computers and Geosciences",20,10.1016/j.cageo.2011.11.020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864842149&doi=10.1016%2fj.cageo.2011.11.020&partnerID=40&md5=6da27b54a266489ce22be6b975d7f9e2","Service-Oriented Architecture and Web Services technologies improve the performance of activities involved in geospatial analysis with a distributed computing architecture. However, the design of the geospatial analysis process on this platform, by combining component Web Services, presents some open issues. The automated construction of these compositions represents an important research topic. Some approaches to solving this problem are based on AI planning methods coupled with semantic service descriptions. This work presents a new approach using AI planning methods to improve the robustness of the produced geospatial Web Services composition. For this purpose, we use semantic descriptions of geospatial data quality requirements in a rule-based form. These rules allow the semantic annotation of geospatial data and, coupled with the conditional planning method, this approach represents more precisely the situations of nonconformities with geodata quality that may occur during the execution of the Web Service composition. The service compositions produced by this method are more robust, thus improving process reliability when working with a composition of chained geospatial Web Services. © 2011 Elsevier Ltd.","Artificial intelligence planning method; Geodata quality; Geoprocessing; Geospatial analysis; Service-Oriented Architecture (SOA); Web Service composition","Artificial intelligence planning; Geo-data; Geoprocessing; Geospatial analysis; Web service composition; Artificial intelligence; Information services; Quality of service; Semantics; Service oriented architecture (SOA); Websites; Web services; artificial intelligence; data processing; data quality; planning method; spatial analysis; spatial data; World Wide Web",Article,Scopus,2-s2.0-84864842149
"Brulin D., Benezeth Y., Courtial E.","Posture recognition based on fuzzy logic for home monitoring of the elderly",2012,"IEEE Transactions on Information Technology in Biomedicine",20,10.1109/TITB.2012.2208757,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866609397&doi=10.1109%2fTITB.2012.2208757&partnerID=40&md5=4d57b1bb0519b6c084c04810ae23274f","We propose in this paper a computer vision-based posture recognition method for home monitoring of the elderly. The proposed system performs human detection prior to the posture analysis; posture recognition is performed only on a human silhouette. The human detection approach has been designed to be robust to different environmental stimuli. Thus, posture is analyzed with simple and efficient features that are not designed to manage constraints related to the environment but only designed to describe human silhouettes. The posture recognition method, based on fuzzy logic, identifies four static postures and is robust to variation in the distance between the camera and the person, and to the persons morphology. With an accuracy of 74.29% of satisfactory posture recognition, this approach can detect emergency situations such as a fall within a health smart home. © 2012 IEEE.","Decision support system; fall detection; posture recognition","Emergency situation; Environmental stimuli; Fall detection; Health smart home; Home monitoring; Human detection; Human silhouette; Posture recognition; Static postures; Vision based; Artificial intelligence; Automation; Computer vision; Decision support systems; Intelligent buildings; Fuzzy logic; aged; algorithm; ambulatory monitoring; article; automated pattern recognition; body posture; computer simulation; falling; fuzzy logic; human; image processing; methodology; physiology; Accidental Falls; Aged; Algorithms; Computer Simulation; Fuzzy Logic; Humans; Image Processing, Computer-Assisted; Monitoring, Ambulatory; Pattern Recognition, Automated; Posture",Article,Scopus,2-s2.0-84866609397
"Solar M., Concha G., Meijueiro L.","A model to assess open government data in public agencies",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",20,10.1007/978-3-642-33489-4_18,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865598454&doi=10.1007%2f978-3-642-33489-4_18&partnerID=40&md5=149927b8f9d1788f4fb78deb2bb4cd79","In this article a maturity model is proposed, named OD-MM (Open Data Maturity Model) to assess the commitment and capabilities of public agencies in pursuing the principles and practices of open data. The OD-MM model has a three level hierarchical structure, called domains, sub-domains and critical variables. Four capacity levels are defined for each of the 33 critical variables distributed in nine sub-domains in order to determine the organization maturity level. The model is a very valuable diagnosis tool for public services, given it shows all weaknesses and the way (a roadmap) to progress in the implementation of open data. © 2012 IFIP International Federation for Information Processing.",,"Critical variables; Data maturity; Diagnosis tools; Hierarchical structures; Maturity levels; Maturity model; Principles and practices; Public agencies; Public services; Roadmap; Sub-domains; Artificial intelligence; Government data processing",Conference Paper,Scopus,2-s2.0-84865598454
"Iwata T., Ohashi K., Minematsu K.","Breaking and repairing GCM security proofs",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",20,10.1007/978-3-642-32009-5_3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865459581&doi=10.1007%2f978-3-642-32009-5_3&partnerID=40&md5=08ecc5c6d9d958912afb4eabdc5fe79a","In this paper, we study the security proofs of GCM (Galois/Counter Mode of Operation). We first point out that a lemma, which is related to the upper bound on the probability of a counter collision, is invalid. Both the original privacy and authenticity proofs by the designers are based on the lemma. We further show that the observation can be translated into a distinguishing attack that invalidates the main part of the privacy proof. It turns out that the original security proofs of GCM contain a flaw, and hence the claimed security bounds are not justified. A very natural question is then whether the proofs can be repaired. We give an affirmative answer to the question by presenting new security bounds, both for privacy and authenticity. As a result, although the security bounds are larger than what were previously claimed, GCM maintains its provable security. We also show that, when the nonce length is restricted to 96 bits, GCM has better security bounds than a general case of variable length nonces. © 2012 International Association for Cryptologic Research.","counter-example; distinguishing attack; GCM; proof of security","counter-example; Distinguishing attacks; GCM; Mode of operations; proof of security; Provable security; Security proofs; Upper Bound; Variable length; Artificial intelligence; Cryptography",Conference Paper,Scopus,2-s2.0-84865459581
"Xu T., Mandal M., Long R., Cheng I., Basu A.","An edge-region force guided active shape approach for automatic lung field detection in chest radiographs",2012,"Computerized Medical Imaging and Graphics",20,10.1016/j.compmedimag.2012.04.005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863867505&doi=10.1016%2fj.compmedimag.2012.04.005&partnerID=40&md5=6a0fc4e32333a986db62f87301611ebf","Automatic and accurate lung field segmentation is an essential step for developing an automated computer-aided diagnosis system for chest radiographs. Although active shape model (ASM) has been useful in many medical imaging applications, lung field segmentation remains a challenge due to the superimposed anatomical structures. We propose an automatic lung field segmentation technique to address the inadequacy of ASM in lung field extraction. Experimental results using both normal and abnormal chest radiographs show that the proposed technique provides better performance and can achieve 3-6% improvement on accuracy, sensitivity and specificity compared to traditional ASM techniques. © 2012 Elsevier Ltd.","Active shape model; Chest radiograph; Computer-aided diagnosis; Edge and region force; Level set; Poisson inverse gradient; Shape priors","Active Shape Models; Chest radiographs; Edge and region force; Level Set; Poisson inverse gradient; Shape priors; Computer aided diagnosis; Image segmentation; Medical imaging; Radiography; Biological organs; active shape model; article; automation; computer assisted diagnosis; diagnostic accuracy; imaging system; intermethod comparison; learning; lung; model; principal component analysis; priority journal; sensitivity and specificity; thorax radiography; Algorithms; Artificial Intelligence; Humans; Lung; Lung Diseases; Pattern Recognition, Automated; Radiographic Image Enhancement; Radiographic Image Interpretation, Computer-Assisted; Radiography, Thoracic; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84863867505
"Xu Y., Hong K., Tsujii J., Chang E.I.-C.","Feature engineering combined with machine learning and rule-based methods for structured information extraction from narrative clinical discharge summaries",2012,"Journal of the American Medical Informatics Association",20,10.1136/amiajnl-2011-000776,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872242834&doi=10.1136%2famiajnl-2011-000776&partnerID=40&md5=5055f39ded0b980812a8d6c2d9695dde","Objective: A system that translates narrative text in the medical domain into structured representation is in great demand. The system performs three sub-tasks: concept extraction, assertion classification, and relation identification. Design: The overall system consists of five steps: (1) pre-processing sentences, (2) marking noun phrases (NPs) and adjective phrases (APs), (3) extracting concepts that use a dosage-unit dictionary to dynamically switch two models based on Conditional Random Fields (CRF), (4) classifying assertions based on voting of five classifiers, and (5) identifying relations using normalized sentences with a set of effective discriminating features. Measurements: Macro-averaged and micro-averaged precision, recall and F-measure were used to evaluate results. Results: The performance is competitive with the stateof- the-art systems with micro-averaged F-measure of 0.8489 for concept extraction, 0.9392 for assertion classification and 0.7326 for relation identification. Conclusions: The system exploits an array of common features and achieves state-of-the-art performance. Prudent feature engineering sets the foundation of our systems. In concept extraction, we demonstrated that switching models, one of which is especially designed for telegraphic sentences, improved extraction of the treatment concept significantly. In assertion classification, a set of features derived from a rule-based classifier were proven to be effective for the classes such as conditional and possible. These classes would suffer from data scarcity in conventional machinelearning methods. In relation identification, we use twostaged architecture, the second of which applies pairwise classifiers to possible candidate classes. This architecture significantly improves performance.",,"access to information; article; drug dose; human; information retrieval; language; machine learning; medical information system; medical record; nomenclature; support vector machine; artificial intelligence; data mining; electronic medical record; hospital discharge; linguistics; methodology; natural language processing; Artificial Intelligence; Data Mining; Electronic Health Records; Humans; Natural Language Processing; Patient Discharge; Vocabulary, Controlled",Article,Scopus,2-s2.0-84872242834
"Chang P.-C., Lin J.-J., Liu C.-H.","An attribute weight assignment and particle swarm optimization algorithm for medical database classifications",2012,"Computer Methods and Programs in Biomedicine",20,10.1016/j.cmpb.2010.12.004,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863879044&doi=10.1016%2fj.cmpb.2010.12.004&partnerID=40&md5=116b18be3c04ca6250b548a09ace17c6","In this research, a hybrid model is developed by integrating a case-based reasoning approach and a particle swarm optimization model for medical data classification. Two data sets from UCI Machine Learning Repository, i.e., Liver Disorders Data Set and Breast Cancer Wisconsin (Diagnosis), are employed for benchmark test. Initially a case-based reasoning method is applied to preprocess the data set thus a weight vector for each feature is derived. A particle swarm optimization model is then applied to construct a decision-making system for diseases identified. The PSO algorithm starts by partitioning the data set into a relatively large number of clusters to reduce the effects of initial conditions and then reducing the number of clusters into two. The average forecasting accuracy for breast cancer of CBRPSO model is 97.4% and for liver disorders is 76.8%. The proposed case-based particle swarm optimization model is able to produce more accurate and comprehensible results for medical experts in medical diagnosis. © 2010 Elsevier Ireland Ltd.","Case base reasoning; Hybrid intelligence; Medical decision making; Particle swarm optimization","Attribute weight; Benchmark tests; Breast Cancer; Case-base reasonings; Case-based reasoning approaches; Data sets; Decision-making systems; Forecasting accuracy; Hybrid intelligence; Hybrid model; Initial conditions; Liver disorder; Medical data; Medical database; Medical decision making; Medical experts; Number of clusters; Particle swarm optimization algorithm; Particle swarm optimization models; Preprocess; PSO algorithms; UCI machine learning repository; Weight vector; WISCONSIN; Algorithms; Benchmarking; Decision making; Diseases; Mathematical models; Medical computing; Particle swarm optimization (PSO); Diagnosis; alanine aminotransferase; aspartate aminotransferase; gamma glutamyltransferase; alanine aminotransferase blood level; alcohol consumption; article; aspartate aminotransferase blood level; breast cancer; cancer classification; data base; decision support system; disease classification; gamma glutamyl transferase blood level; liver disease; medical decision making; symptom; Algorithms; Artificial Intelligence; Breast Neoplasms; Cluster Analysis; Computational Biology; Data Mining; Databases, Factual; Decision Making; Diagnosis, Computer-Assisted; Female; Humans; Liver Diseases; Male; Models, Theoretical; Normal Distribution; Reproducibility of Results; Support Vector Machines",Article,Scopus,2-s2.0-84863879044
"Ashtawy H.M., Mahapatra N.R.","A comparative assessment of ranking accuracies of conventional and machine-learning-based scoring functions for protein-ligand binding affinity prediction",2012,"IEEE/ACM Transactions on Computational Biology and Bioinformatics",20,10.1109/TCBB.2012.36,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864950736&doi=10.1109%2fTCBB.2012.36&partnerID=40&md5=f3d00d8d3cdb66bda80cfd9dd9d1d7d6","Accurately predicting the binding affinities of large sets of protein-ligand complexes efficiently is a key challenge in computational biomolecular science, with applications in drug discovery, chemical biology, and structural biology. Since a scoring function (SF) is used to score, rank, and identify drug leads, the fidelity with which it predicts the affinity of a ligand candidate for a protein's binding site has a significant bearing on the accuracy of virtual screening. Despite intense efforts in developing conventional SFs, which are either force-field based, knowledge-based, or empirical, their limited ranking accuracy has been a major roadblock toward cost-effective drug discovery. Therefore, in this work, we explore a range of novel SFs employing different machine-learning (ML) approaches in conjunction with a variety of physicochemical and geometrical features characterizing protein-ligand complexes. We assess the ranking accuracies of these new ML-based SFs as well as those of conventional SFs in the context of the 2007 and 2010 PDBbind benchmark data sets on both diverse and protein-family-specific test sets. We also investigate the influence of the size of the training data set and the type and number of features used on ranking accuracy. Within clusters of protein-ligand complexes with different ligands bound to the same target protein, we find that the best ML-based SF is able to rank the ligands correctly based on their experimentally determined binding affinities 62.5 percent of the time and identify the top binding ligand 78.1 percent of the time. For this SF, the Spearman correlation coefficient between ranks of ligands ordered by predicted and experimentally determined binding affinities is 0.771. Given the challenging nature of the ranking problem and that SFs are used to screen millions of ligands, this represents a significant improvement over the best conventional SF we studied, for which the corresponding ranking performance values are 57.8 percent, 73.4 percent, and 0.677. © 2004-2012 IEEE.","Drug discovery; machine learning; protein-ligand binding affinity; ranking power; scoring function; virtual screening","Binding affinities; Drug discovery; ranking power; Scoring functions; Virtual Screening; Binding energy; Binding sites; Complexation; Knowledge based systems; Learning systems; Proteins; Ligands; ligand; protein; algorithm; article; artificial intelligence; binding site; chemistry; comparative study; metabolism; protein conformation; Algorithms; Artificial Intelligence; Binding Sites; Ligands; Protein Conformation; Proteins",Article,Scopus,2-s2.0-84864950736
"Chang N.-B., Prapinpongsanone N., Ernest A.","Optimal sensor deployment in a large-scale complex drinking water network: Comparisons between a rule-based decision support system and optimization models",2012,"Computers and Chemical Engineering",20,10.1016/j.compchemeng.2012.03.012,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861203502&doi=10.1016%2fj.compchemeng.2012.03.012&partnerID=40&md5=4971143834cf187ee72d1f2dbfea4089","Many models or algorithms have been suggested for sensor placement in the drinking water distribution networks, such as genetic algorithms, multiobjective optimization models, and heuristic methods. Because these models or algorithms have high computational demands, however, the requirement of expensive technical computing software is unavoidable. This study presents a rule-based decision support system (RBDSS) to analyze and generate a set of sensor placement locations and compares the performance against 10 optimization models based on four indexes. Our findings show that the RBDSS demands relatively lower computational time and still exhibits outstanding performance in terms of all our indexes when dealing with a large-scale complex drinking water network. © 2012 Elsevier Ltd.","Graph theory; Rule-based decision support system; Sensor deployment; Systems analysis; Water distribution system","Computational demands; Computational time; Drinking water distribution networks; Multi-objective optimization models; Optimal sensor; Optimization models; Rule-based decision support system; Sensor deployment; Sensor placement; Technical computing; Algorithms; Artificial intelligence; Decision support systems; Graph theory; Heuristic methods; Multiobjective optimization; Sensors; Systems analysis; Water distribution systems; Mathematical models",Article,Scopus,2-s2.0-84861203502
"Vasudevan A., Owusu E., Zhou Z., Newsome J., McCune J.M.","Trustworthy execution on mobile devices: What security properties can my mobile platform give me?",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",20,10.1007/978-3-642-30921-2_10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863094040&doi=10.1007%2f978-3-642-30921-2_10&partnerID=40&md5=54e5de7cc0622e52de635334f4131042","We are now in the post-PC era, yet our mobile devices are insecure. We consider the different stake-holders in today's mobile device ecosystem, and analyze why widely-deployed hardware security primitives on mobile device platforms are inaccessible to application developers and end-users. We systematize existing proposals for leveraging such primitives, and show that they can indeed strengthen the security properties available to applications and users, all without reducing the properties currently enjoyed by OEMs and network carriers. We also highlight shortcomings of existing proposals and make recommendations for future research that may yield practical, deployable results. © 2012 Springer-Verlag.",,"Application developers; End-users; Hardware security; Mobile platform; Network carriers; Security properties; Artificial intelligence; Mobile devices",Conference Paper,Scopus,2-s2.0-84863094040
"Milner S., Davis C., Zhang H., Llorca J.","Nature-Inspired self-organization, control, and optimization in heterogeneous wireless networks",2012,"IEEE Transactions on Mobile Computing",20,10.1109/TMC.2011.141,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861447525&doi=10.1109%2fTMC.2011.141&partnerID=40&md5=700736ac8d75dad00dec14a73545cdbc","In this paper, we present new models and algorithms for control and optimization of a class of next generation communication networks: Hierarchical Heterogeneous Wireless Networks (HHWNs), under real-world physical constraints. Two biology-inspired techniques, a Flocking Algorithm (FA) and a Particle Swarm Optimizer (PSO), are investigated in this context. Our model is based on the control framework at the physical layer presented previously by the authors. We first develop a nonconvex mathematical model for HHWNs. Second, we propose a new FA for self-organization and control of the backbone nodes in an HHWN by collecting local information from end users. Third, we employ PSO, a widely used artificial intelligence algorithm, to directly optimize the HHWN by collecting global information from the entire system. A comprehensive evaluation measurement during the optimization process is developed. In addition, the relationship between HHWN and FA and the comparison of FA and PSO are discussed, respectively. Our novel framework is examined in various dynamic scenarios. Experimental results demonstrate that FA and PSO both outperform current algorithms for the self-organization and optimization of HHWNs while showing different characteristics with respect to convergence speed and quality of solutions. © 2012 IEEE.","directional wireless communication; flocking algorithm; Heterogeneous wireless networks; mobile ad hoc networks; particle swarm","Backbone nodes; Comprehensive evaluation; Control and optimization; Control framework; Convergence speed; Directional wireless communications; End users; Entire system; Flocking algorithms; Global informations; Heterogeneous wireless network; Local information; Nature-inspired self-organization; New model; Next generation communication network; Nonconvex; Optimization process; Particle swarm; Particle swarm optimizers; Physical constraints; Physical layers; Quality of solution; Ad hoc networks; Algorithms; Artificial intelligence; Mathematical models; Mobile ad hoc networks; Models; Network layers; Telecommunication networks; Wireless telecommunication systems; Particle swarm optimization (PSO)",Article,Scopus,2-s2.0-84861447525
"Huynh-Thu V.A., Saeys Y., Wehenkel L., Geurts P.","Statistical interpretation of machine learning-based feature importance scores for biomarker discovery",2012,"Bioinformatics",20,10.1093/bioinformatics/bts238,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864008355&doi=10.1093%2fbioinformatics%2fbts238&partnerID=40&md5=52c0ec8ee90d7c419de023c18ba1e0f1","Motivation: Univariate statistical tests are widely used for biomarker discovery in bioinformatics. These procedures are simple, fast and their output is easily interpretable by biologists but they can only identify variables that provide a significant amount of information in isolation from the other variables. As biological processes are expected to involve complex interactions between variables, univariate methods thus potentially miss some informative biomarkers. Variable relevance scores provided by machine learning techniques, however, are potentially able to highlight multivariate interacting effects, but unlike the p-values returned by univariate tests, these relevance scores are usually not statistically interpretable. This lack of interpretability hampers the determination of a relevance threshold for extracting a feature subset from the rankings and also prevents the wide adoption of these methods by practicians. Results: We evaluated several, existing and novel, procedures that extract relevant features from rankings derived from machine learning approaches. These procedures replace the relevance scores with measures that can be interpreted in a statistical way, such as p-values, false discovery rates, or family wise error rates, for which it is easier to determine a significance level. Experiments were performed on several artificial problems as well as on real microarray datasets. Although the methods differ in terms of computing times and the tradeoff, they achieve in terms of false positives and false negatives, some of them greatly help in the extraction of truly relevant biomarkers and should thus be of great practical interest for biologists and physicians. As a side conclusion, our experiments also clearly highlight that using model performance as a criterion for feature selection is often counter-productive. © The Author 2012. Published by Oxford University Press. All rights reserved.",,"biological marker; transcriptome; article; artificial intelligence; biology; evaluation; methodology; statistical analysis; Artificial Intelligence; Biological Markers; Computational Biology; Data Interpretation, Statistical; Transcriptome",Article,Scopus,2-s2.0-84864008355
"Rubiolo M., Caliusco M.L., Stegmayer G., Coronel M., Gareli Fabrizi M.","Knowledge discovery through ontology matching: An approach based on an Artificial Neural Network model",2012,"Information Sciences",20,10.1016/j.ins.2011.08.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859165019&doi=10.1016%2fj.ins.2011.08.008&partnerID=40&md5=fa1f4a99903d7ba055f1a84c280d36a9","With the emergence of the Semantic Web several domain ontologies were developed, which varied not only in their structure but also in the natural language used to define them. The lack of an integrated view of all web nodes and the existence of heterogeneous domain ontologies drive new challenges in the discovery of knowledge resources which are relevant to a user's request. New approaches have recently appeared for developing web intelligence and helping users avoid irrelevant results on the web. However, there remains some work to be done. This work makes a contribution by presenting an ANN-based ontology matching model for knowledge source discovery on the Semantic Web. Experimental results obtained on a real case study have shown that this model provides satisfactory responses. © 2012 Elsevier Inc. All rights reserved.","Artificial Neural Network; Knowledge-source discovery; Semantic Web; WordNet","Artificial neural network models; Domain ontologies; Heterogeneous domains; Knowledge resource; Knowledge sources; Knowledge-source discovery; Natural languages; Ontology matching; Web intelligence; Wordnet; Artificial intelligence; Semantic Web; Software engineering; Neural networks",Article,Scopus,2-s2.0-84859165019
"Romero C.C., Hoogenboom G., Baigorria G.A., Koo J., Gijsman A.J., Wood S.","Reanalysis of a global soil database for crop and environmental modeling",2012,"Environmental Modelling and Software",20,10.1016/j.envsoft.2012.02.018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860192214&doi=10.1016%2fj.envsoft.2012.02.018&partnerID=40&md5=07153d6d56beffe96e0a8be4ad41ffd4","There is an increased need for detailed soil information that can be used for applications of crop and environmental modeling. The goal of this project was to conduct a reanalysis of the ISRIC-WISE 1.1 Soil Profile Dataset. As part of the procedures, the soil reanalysis database was fitted to the standard formats of the International Consortium for Agricultural Systems Application (ICASA). Thus, the soil reanalysis database tailors dynamic crop models such as the Cropping System Model (CSM) of the Decision Support System for Agrotechnology Transfer (DSSAT). During the reanalysis, the physical and chemical parameters of the soil profiles were revised and estimated, where necessary and possible, using pre-established ranges given by the literature and correlations among other more stable variable. To evaluate each of the 3404 reanalyzed soil profiles, the CSM-CERES-Maize model was run for a standard crop management scenario using both the original and the new improved soil databases. Nine hundred seventy-eight soil profiles were considered to be not useful during the reanalysis due to missing values for one or more critical variables and were, therefore, not considered for quality control procedures. A pre-diagnostic for only nitrogen and soil organic carbon in the original dataset showed 70% and 5% of missing values respectively. A sensitivity analysis based on crop simulations comparing the original and the reanalyzed soil databases, showed that 1294 soil profiles yielded different results due to improvement of either the original data or improved conversion procedures. The details and considerations for detecting missing and erroneous values and for estimating soil variable values are presented in this paper for further use. The final soil reanalysis global database contains 3404 soil profiles and is available at https://harvestchoice.wufoo.com/forms/download-wisol. © 2012 Elsevier Ltd.","DSSAT; Global soil database; Model applications; Quality control; Soil reanalysis database; WISE 1.1","DSSAT; Model application; Reanalysis; Soil database; WISE 1.1; Artificial intelligence; Crops; Data handling; Database systems; Decision support systems; Geologic models; Quality control; Soils; data set; database; estimation method; numerical model; physicochemical property; quality control; sensitivity analysis; soil organic matter; soil profile; soil type; Zea mays",Article,Scopus,2-s2.0-84860192214
"Pei Y., Huang F., Shi F., Zha H.","Unsupervised image matching based on manifold alignment",2012,"IEEE Transactions on Pattern Analysis and Machine Intelligence",20,10.1109/TPAMI.2011.229,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862668689&doi=10.1109%2fTPAMI.2011.229&partnerID=40&md5=60f29b0a4ccf2b1d67f5fa8ba80b8d93","This paper challenges the issue of automatic matching between two image sets with similar intrinsic structures and different appearances, especially when there is no prior correspondence. An unsupervised manifold alignment framework is proposed to establish correspondence between data sets by a mapping function in the mutual embedding space. We introduce a local similarity metric based on parameterized distance curves to represent the connection of one point with the rest of the manifold. A small set of valid feature pairs can be found without manual interactions by matching the distance curve of one manifold with the curve cluster of the other manifold. To avoid potential confusions in image matching, we propose an extended affine transformation to solve the nonrigid alignment in the embedding space. The comparatively tight alignments and the structure preservation can be obtained simultaneously. The point pairs with the minimum distance after alignment are viewed as the matchings. We apply manifold alignment to image set matching problems. The correspondence between image sets of different poses, illuminations, and identities can be established effectively by our approach. © 2012 IEEE.","Manifold alignment; nonrigid transformation; parameterized distance curve; unsupervised image set matching","Affine transformations; Automatic matching; Data sets; Image sets; Intrinsic structures; Local similarity; Manual interaction; Mapping functions; Matchings; Minimum distance; Non-rigid transformation; Parameterized; Structure preservation; Artificial intelligence; Computer vision; Image matching",Article,Scopus,2-s2.0-84862668689
"Zhu F., Shen B.","Combined SVM-CRFs for biological named entity recognition with maximal bidirectional squeezing",2012,"PLoS ONE",20,10.1371/journal.pone.0039230,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862880725&doi=10.1371%2fjournal.pone.0039230&partnerID=40&md5=7ed3570aebf7cf2e41810ab5cab69325","Biological named entity recognition, the identification of biological terms in text, is essential for biomedical information extraction. Machine learning-based approaches have been widely applied in this area. However, the recognition performance of current approaches could still be improved. Our novel approach is to combine support vector machines (SVMs) and conditional random fields (CRFs), which can complement and facilitate each other. During the hybrid process, we use SVM to separate biological terms from non-biological terms, before we use CRFs to determine the types of biological terms, which makes full use of the power of SVM as a binary-class classifier and the data-labeling capacity of CRFs. We then merge the results of SVM and CRFs. To remove any inconsistencies that might result from the merging, we develop a useful algorithm and apply two rules. To ensure biological terms with a maximum length are identified, we propose a maximal bidirectional squeezing approach that finds the longest term. We also add a positive gain to rare events to reinforce their probability and avoid bias. Our approach will also gradually extend the context so more contextual information can be included. We examined the performance of four approaches with GENIA corpus and JNLPBA04 data. The combination of SVM and CRFs improved performance. The macro-precision, macro-recall, and macro-F1 of the SVM-CRFs hybrid approach surpassed conventional SVM and CRFs. After applying the new algorithms, the macro-F1 reached 91.67% with the GENIA corpus and 84.04% with the JNLPBA04 data. © 2012 Zhu, Shen.",,"accuracy; article; classifier; conditional random field; data extraction; information processing; machine learning; mathematical analysis; nomenclature; probability; support vector machine; Algorithms; Artificial Intelligence; Computational Biology",Article,Scopus,2-s2.0-84862880725
"Kiefl J., Cordero C., Nicolotti L., Schieberle P., Reichenbach S.E., Bicchi C.","Performance evaluation of non-targeted peak-based cross-sample analysis for comprehensive two-dimensional gas chromatography-mass spectrometry data and application to processed hazelnut profiling",2012,"Journal of Chromatography A",20,10.1016/j.chroma.2012.04.048,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861462057&doi=10.1016%2fj.chroma.2012.04.048&partnerID=40&md5=717180efa2e9a8689cfcbc90c50561e9","The continuous interest in non-targeted profiling induced the development of tools for automated cross-sample analysis. Such tools were found to be selective or not comprehensive thus delivering a biased view on the qualitative/quantitative peak distribution across 2D sample chromatograms. Therefore, the performance of non-targeted approaches needs to be critically evaluated. This study focused on the development of a validation procedure for non-targeted, peak-based, GC×GC-MS data profiling. The procedure introduced performance parameters such as specificity, precision, accuracy, and uncertainty for a profiling method known as Comprehensive Template Matching. The performance was assessed by applying a three-week validation protocol based on CITAC/EURACHEM guidelines. Optimized 1D and 2D retention times search windows, MS match factor threshold, detection threshold, and template threshold were evolved from two training sets by a semi-automated learning process. The effectiveness of proposed settings to consistently match 2D peak patterns was established by evaluating the rate of mismatched peaks and was expressed in terms of results accuracy. The study utilized 23 different 2D peak patterns providing the chemical fingerprints of raw and roasted hazelnuts (Corylus avellana L.) from different geographical origins, of diverse varieties and different roasting degrees. The validation results show that non-targeted peak-based profiling can be reliable with error rates lower than 10% independent of the degree of analytical variance. The optimized Comprehensive Template Matching procedure was employed to study hazelnut roasting profiles and in particular to find marker compounds strongly dependent on the thermal treatment, and to establish the correlation of potential marker compounds to geographical origin and variety/cultivar and finally to reveal the characteristic release of aroma active compounds. © 2012 Elsevier B.V.","Comprehensive Template Matching fingerprinting; Corylus avellana L. volatile fraction; GC×GC-MS; Key aroma markers; Roasted hazelnut; Validation","Comprehensive Template Matching fingerprinting; Key aroma markers; Roasted hazelnut; Validation; Volatile fractions; Chromatographic analysis; Gas chromatography; Optimization; Template matching; Calcination; flavoring agent; accuracy; analytic method; analytical error; article; automation; comprehensive template matching; controlled study; gas chromatography; geographic distribution; hazelnut; high temperature procedures; mass fragmentography; nontargeted peak based cross sample analysis; nontargeted peak based fingerprint analysis; priority journal; reliability; sample; sensitivity and specificity; uncertainty; validation study; Analysis of Variance; Artificial Intelligence; Cooking; Corylus; Gas Chromatography-Mass Spectrometry; Linear Models; Odors; Organic Chemicals; Reproducibility of Results; Signal-To-Noise Ratio; Solid Phase Microextraction; Corylus; Corylus avellana",Article,Scopus,2-s2.0-84861462057
"Afshar A., Mariño M.A.","Multi-objective Coverage-based ACO Model for Quality Monitoring in Large Water Networks",2012,"Water Resources Management",20,10.1007/s11269-012-0008-2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860885150&doi=10.1007%2fs11269-012-0008-2&partnerID=40&md5=5b2016da2454de7f51452f9aa69e8e94","A numerical procedure is presented for the optimization of the position of water quality monitoring stations in a pressurized water distribution system (WDS). The procedure is based on the choice of the set of sampling stations which maximizes the monitored volume of water while keeping the number of stations at minimum. The optimization model is formulated in terms of integer programming, and the solution of the mathematical problem is efficiently approximated by means of a multi-objective multi-colony ant algorithm. A built-in routine is developed for calculation of the water fraction matrix and integrated into the general modeling structure to facilitate data entry and storage to minimize problems associated with water fraction matrix determination for varying scenarios and coverage criteria for any scenario. The proposed methodology is very robust in analyzing the effects of different scenarios and/or number of potential monitoring stations by eliminating the need of employing an off-line routine for coverage matrix identification. Robustness, ease of generalization, multi-objective nature, and computational efficiency are the main characteristics and novelty of the proposed approach. Monitoring stations are optimally located in a large-scale real-world network with 104 nodes and multiple demands using the proposed ACO models. The set of non-dominated solutions forming the Pareto front for a number of monitoring stations and the total coverage of the system are also presented. © 2012 Springer Science+Business Media B.V.","Multi-colony ant algorithm; Optimization; Water distribution network; Water quality","Ant algorithms; Coverage criteria; Mathematical problems; Matrix identification; Modeling structures; Monitoring stations; Multi objective; Nondominated solutions; Numerical procedures; Optimization models; Pareto front; Potential monitoring; Pressurized water; Quality monitoring; Real-world networks; Sampling stations; Water distribution networks; Water fraction; Water networks; Water quality monitoring stations; Algorithms; Artificial intelligence; Integer programming; Matrix algebra; Optimization; Water quality; Water distribution systems; algorithm; monitoring system; multiobjective programming; numerical model; optimization; sampling; water quality",Article,Scopus,2-s2.0-84860885150
"Briegel H.J., De Las Cuevas G.","Projective simulation for artificial intelligence",2012,"Scientific Reports",20,10.1038/srep00400,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861122307&doi=10.1038%2fsrep00400&partnerID=40&md5=56d38f30b70fde9eeef362e2bf153ebe","We propose a model of a learning agent whose interaction with the environment is governed by a simulation-based projection, which allows the agent to project itself into future situations before it takes real action. Projective simulation is based on a random walk through a network of clips, which are elementary patches of episodic memory. The network of clips changes dynamically, both due to new perceptual input and due to certain compositional principles of the simulation process. During simulation, the clips are screened for specific features which trigger factual action of the agent. The scheme is different from other, computational, notions of simulation, and it provides a new element in an embodied cognitive science approach to intelligent action and learning. Our model provides a natural route for generalization to quantum-mechanical operation and connects the fields of reinforcement learning and quantum computation. Copyright 2012 by the American Geophysical Union.",,"algorithm; article; artificial intelligence; artificial neural network; biological model; biology; computer simulation; human; learning; memory; methodology; physiology; reinforcement; reproducibility; Algorithms; Artificial Intelligence; Computational Biology; Computer Simulation; Humans; Learning; Memory; Models, Biological; Neural Networks (Computer); Reinforcement (Psychology); Reproducibility of Results",Article,Scopus,2-s2.0-84861122307
"Remeseiro B., Penas M., Mosquera A., Novo J., Penedo M.G., Yebra-Pimentel E.","Statistical comparison of classifiers applied to the interferential tear film lipid layer automatic classification",2012,"Computational and Mathematical Methods in Medicine",20,10.1155/2012/207315,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861072255&doi=10.1155%2f2012%2f207315&partnerID=40&md5=0daa20652c3e9218e794b5b0d8ea52b4","The tear film lipid layer is heterogeneous among the population. Its classification depends on its thickness and can be done using the interference pattern categories proposed by Guillon. The interference phenomena can be characterised as a colour texture pattern, which can be automatically classified into one of these categories. From a photography of the eye, a region of interest is detected and its low-level features are extracted, generating a feature vector that describes it, to be finally classified in one of the target categories. This paper presents an exhaustive study about the problem at hand using different texture analysis methods in three colour spaces and different machine learning algorithms. All these methods and classifiers have been tested on a dataset composed of 105 images from healthy subjects and the results have been statistically analysed. As a result, the manual process done by experts can be automated with the benefits of being faster and unaffected by subjective factors, with maximum accuracy over 95%. © 2012 B. Remeseiro et al.",,"lipid; algorithm; article; automation; eye photography; human; image analysis; lipid analysis; machine learning; statistical analysis; tear film; adult; artificial intelligence; chemistry; classification; color; comparative study; factual database; interferometry; lacrimal fluid; probability; statistical model; statistics; Adult; Algorithms; Artificial Intelligence; Color; Databases, Factual; Humans; Interferometry; Lipids; Markov Chains; Models, Statistical; Tears; Young Adult",Article,Scopus,2-s2.0-84861072255
"Gorzałczany M.B., Rudziński F.","Accuracy vs. interpretability of fuzzy rule-based classifiers: An evolutionary approach",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",20,10.1007/978-3-642-29353-5_26,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860650444&doi=10.1007%2f978-3-642-29353-5_26&partnerID=40&md5=e1ba37c532e3a4f8569d7db692e7106c","The paper presents a generalization of the Pittsburgh approach to learn fuzzy classification rules from data. The proposed approach allows us to obtain a fuzzy rule-based system with a predefined level of compromise between its accuracy and interpretability (transparency). The application of the proposed technique to design the fuzzy rule-based classifier for the well known benchmark data sets (Dermatology and Wine) available from the http://archive.ics.uci.edu/ ml is presented. A comparative analysis with several alternative (fuzzy) rule-based classification techniques has also been carried out. © 2012 Springer-Verlag.",,"Benchmark data; Comparative analysis; Evolutionary approach; Fuzzy classification rule; Fuzzy rule-based classifier; Fuzzy rule-based systems; Interpretability; Pittsburgh approach; Rule-based classification; Artificial intelligence; Benchmarking; Evolutionary algorithms; Soft computing; Fuzzy rules",Conference Paper,Scopus,2-s2.0-84860650444
"Nanni L., Lumini A., Brahnam S.","A classifier ensemble approach for the missing feature problem",2012,"Artificial Intelligence in Medicine",20,10.1016/j.artmed.2011.11.006,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858866778&doi=10.1016%2fj.artmed.2011.11.006&partnerID=40&md5=fa1ad24455614652f846cf9a412b0552","Objectives: Many classification problems must deal with data that contains missing values. In such cases data imputation is critical. This paper evaluates the performance of several statistical and machine learning imputation methods, including our novel multiple imputation ensemble approach, using different datasets. Materials and methods: Several state-of-the-art approaches are compared using different datasets. Some state-of-the-art classifiers (including support vector machines and input decimated ensembles) are tested with several imputation methods. The novel approach proposed in this work is a multiple imputation method based on random subspace, where each missing value is calculated considering a different cluster of the data. We have used a fuzzy clustering approach for the clustering algorithm. Results: Our experiments have shown that the proposed multiple imputation approach based on clustering and a random subspace classifier outperforms several other state-of-the-art approaches. Using the Wilcoxon signed-rank test (reject the null hypothesis, level of significance 0.05) we have shown that the proposed best approach is outperformed by the classifier trained using the original data (i.e., without missing values) only when >20% of the data are missed. Moreover, we have shown that coupling an imputation method with our cluster based imputation we outperform the base method (level of significance ∼0.05). Conclusion: Starting from the assumptions that the feature set must be partially redundant and that the redundancy is distributed randomly over the feature set, we have proposed a method that works quite well even when a large percentage of the features is missing (≥30%). Our best approach is available (MATLAB code) at bias.csr.unibo.it/nanni/MI.rar. © 2011 Elsevier B.V.","Data corruption; Equipment malfunctions; Fuzzy clustering; Imputation methods; Missing values; Support vector machine","Classifier ensembles; Cluster-based; Clustering approach; Data corruption; Data imputation; Data sets; Feature sets; Imputation methods; Matlab code; Missing values; Multiple imputation; Null hypothesis; Random subspace classifiers; Random subspaces; State-of-the-art approach; Support vector machine (SVM); Clustering algorithms; Fuzzy clustering; Support vector machines; Autocorrelation; algorithm; article; classifier; cluster analysis; data analysis; machine learning; null hypothesis; priority journal; redundancy analysis; statistics; Wilcoxon signed ranks test; Area Under Curve; Artificial Intelligence; Classification; Cluster Analysis; Computer Simulation; Data Interpretation, Statistical; Fuzzy Logic; Information Storage and Retrieval; Models, Statistical; Support Vector Machines",Article,Scopus,2-s2.0-84858866778
"Niu S.H., Ong S.K., Nee A.Y.C.","An enhanced ant colony optimiser for multi-attribute partner selection in virtual enterprises",2012,"International Journal of Production Research",20,10.1080/00207543.2011.578158,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861417648&doi=10.1080%2f00207543.2011.578158&partnerID=40&md5=0ce6fea36a92ca048398476a09df71ba","Increasing global competition drives enterprises, especially small and medium-sized enterprises, to collaborate in order to respond faster to customers needs, reduce operating costs, increase capacity, and produce customised products to reach the market quicker. A virtual enterprise (VE) is an important manufacturing paradigm to address this trend in the dynamic global economy. Partner selection is a key issue tightly coupled to the success of a VE coalition, and because of its complexity, it is considered a multi-attribute optimisation problem. In this paper, an enhanced ant colony optimiser (ACO) is proposed to address the partner selection problem. Five attributes (namely, cost, time, quality, reputation, and risk) considering both qualitative and quantitative aspects have been investigated to evaluate the candidate partners. Experiments have been conducted to validate the performance of the enhanced ACO algorithm, and the results show that the enhanced ACO algorithm can produce better results in terms of search accuracy and computing time. © 2012 Copyright Taylor and Francis Group, LLC.","ACO; partner selection; virtual enterprise","ACO; ACO algorithms; Ant colonies; Computing time; Global competition; Global economies; Manufacturing paradigm; Multi-attributes; Optimisations; Partner selection; Search accuracy; Small and medium-sized enterprise; Tightly-coupled; Virtual enterprise; Artificial intelligence; Competition; Virtual corporation; Algorithms",Article,Scopus,2-s2.0-84861417648
"Vatavu R.-D.","Nomadic gestures: A technique for reusing gesture commands for frequent ambient interactions",2012,"Journal of Ambient Intelligence and Smart Environments",20,10.3233/AIS-2012-0137,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859456568&doi=10.3233%2fAIS-2012-0137&partnerID=40&md5=aeb02df8528aea0c5d4accc277e78ed9","The age of ambient intelligence has already incorporated gestures into practical applications with the goal of delivering adaptive and personalized interactions. However, practitioners are faced with many problems when implementing gesture-based interfaces for such interactive ambient systems. Although gestures offer great opportunity for natural and intuitive interactions, there are currently little to no rules for creating the set of gestures for a given application. Therefore, designers associate gestures and functions by relying only on their own expertize and experience which leads to different systems exposing different standards. This approach does not only create the premises for confusion among users in a future world with such hundreds daily interactions but it also contradicts the goals of ambient intelligence in which interaction should be personalized and adapted to each user. This work introduces a novel concept (nomadic gestures) for reusing a set of user-defined gesture commands in the context of interacting with ambient systems. Nomadic gestures live on each user's personal mobile device and are uploaded to the ambient system prior to interaction. The concept relies on an important shift of perspective strictly adhering to the goals of ambient intelligence: it is not the users that adapt to the interface learning its commands but instead the interface employs the users' own gesture sets with their own preferred function associations. © 2012 IOS Press and the authors. All rights reserved.","Ambient displays; ambient intelligence; ambient interactions; computer vision; gesture-based interfaces; gestures; kinect; mobile phones; personalized and adapted interaction","Ambient displays; Ambient intelligence; Gesture-based interface; gestures; kinect; personalized and adapted interaction; Computer vision; Mobile devices; Mobile phones; Artificial intelligence",Article,Scopus,2-s2.0-84859456568
"Wu L., Hua X.-S., Yu N., Ma W.-Y., Li S.","Flickr distance: A relationship measure for visual concepts",2012,"IEEE Transactions on Pattern Analysis and Machine Intelligence",20,10.1109/TPAMI.2011.195,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859185173&doi=10.1109%2fTPAMI.2011.195&partnerID=40&md5=b78054c196e0042eb7222d9de2d1ccca","This paper proposes the Flickr Distance (FD) to measure the visual correlation between concepts. For each concept, a collection of related images are obtained from the Flickr website. We assume that each concept consists of several states, e.g., different views, different semantics, etc., which are considered as latent topics. Then a latent topic visual language model (LTVLM) is built to capture these states. The Flickr distance between two concepts is defined as the Jensen-Shannon (J-S) divergence between their LTVLM. Differently from traditional conceptual distance measurements, which are based on Web textual documents, FD is based on the visual information. Comparing with the WordNet distance, FD can easily scale up with the increasing size of the conceptual corpus. Comparing with the Google Distance (NGD) and Tag Concurrence Distance (TCD), FD uses the visual information and can properly measure the conceptual relations. We apply FD to multimedia-related tasks and find methods based on FD significantly outperform those based on NGD and TCD. With the FD measurement, we also construct a large-scale visual conceptual network (VCNet) to store the knowledge of conceptual relationship. Experiments show that FD is more coherent to human cognition and it also outperforms text-based distances in real-world applications. © 2012 IEEE.","Artificial intelligence; distance learning; image analysis; machine vision","Conceptual relations; Google distances; Human cognition; Real-world application; Scale-up; Textual documents; Visual concept; Visual information; Visual language model; Wordnet; Artificial intelligence; Computational linguistics; Computer vision; Distance education; Image analysis; Semantics; Finite difference method; algorithm; article; artificial intelligence; cognition; data mining; documentation; human; image processing; methodology; multimedia; semantics; social media; theoretical model; Abstracting and Indexing as Topic; Algorithms; Artificial Intelligence; Cognition; Data Mining; Humans; Image Processing, Computer-Assisted; Models, Theoretical; Multimedia; Semantics; Social Media",Article,Scopus,2-s2.0-84859185173
"Shabani M.O., Mazahery A.","Prediction performance of various numerical model training algorithms in solidification process of A356 matrix composites",2012,"Indian Journal of Engineering and Materials Sciences",20,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861364058&partnerID=40&md5=c152dc55f780baaa526a241bcbb66817","This paper reports the microstructural and mechanical properties of casting Al matrix composite such as porosity, hardness and tensile strength. The numerical model and finite element method are applied to simulate the solidification of the composites. The finite element analysis involves a number of steps such as finite-element discretization, imposition of boundary conditions and solution of assembled equations. The mathematical formulation of this solidification problem is given. The neural network predictions are directly compared with the experimentally obtained data to evaluate the learning performance. In this investigation the MAPE is used to evaluate the performance of model. The results show that Levenberg-Marquardt learning algorithm give the best prediction for UTS, hardness and porosity of A356 composite reinforced with B4C particulates.","Aluminum; Artificial intelligence; Finite element; Metal matrix composite",,Article,Scopus,2-s2.0-84861364058
"Meda-Campaña J.A., Gómez-Mancilla J.C., Castillo-Toledo B.","Exact output regulation for nonlinear systems described by Takagi-Sugeno fuzzy models",2012,"IEEE Transactions on Fuzzy Systems",20,10.1109/TFUZZ.2011.2172689,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859734214&doi=10.1109%2fTFUZZ.2011.2172689&partnerID=40&md5=fd5af0ba1f18488731f4aade6c213209","The exact output regulation for Takagi-Sugeno (T-S) fuzzy models depends on two conditions: 1) The local steady-state zero-error manifolds have to be the same for every local subsystem, and 2) the local input matrices have to be the same for every local subsystem included in the T-S fuzzy model. These conditions are difficult to satisfy in general. In this paper, those conditions are relaxed by solving the fuzzy regulation problem directly on the overall T-S fuzzy model, instead of constructing the fuzzy regulator on the basis of linear local controllers. By considering the fuzzy model as a special class of linear time-varying systems, existence conditions are rigorously derived. These new conditions, which can be solved by means of any mathematical software, depend on the solution of a set of symbolic simultaneous linear equations depending on the membership values of the plant and/or the exosystem. Two examples are given to illustrate the construction of the proposed regulator and to validate the improvement that is achieved with the proposed approach. © 2012 IEEE.","Francis equations; fuzzy output regulation; Takagi-Sugeno (T-S) fuzzy model","Existence conditions; Exosystems; Francis equations; Fuzzy models; Fuzzy output; Input matrices; Linear time-varying systems; Local controllers; Local subsystem; Mathematical software; Membership values; Output regulation; Regulation problems; Special class; T-S fuzzy models; Takagi Sugeno fuzzy models; Artificial intelligence; Fuzzy sets",Article,Scopus,2-s2.0-84859734214
"Liu Y., Ling X., Liang Y., Liu G.","Improved artificial bee colony algorithm with mutual learning",2012,"Journal of Systems Engineering and Electronics",20,10.1109/JSEE.2012.00034,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863707883&doi=10.1109%2fJSEE.2012.00034&partnerID=40&md5=8bc57168803626e614f726cb0e2fd22d","The recently invented artificial bee colony (ABC) algorithm is an optimization algorithm based on swarm intelligence that has been used to solve many kinds of numerical function optimization problems. It performs well in most cases, however, there still exists an insufficiency in the ABC algorithm that ignores the fitness of related pairs of individuals in the mechanism of finding a neighboring food source. This paper presents an improved ABC algorithm with mutual learning (MutualABC) that adjusts the produced candidate food source with the higher fitness between two individuals selected by a mutual learning factor. The performance of the improved MutualABC algorithm is tested on a set of benchmark functions and compared with the basic ABC algorithm and some classical versions of improved ABC algorithms. The experimental results show that the MutualABC algorithm with appropriate parameters outperforms other ABC algorithms in most experiments.","Artificial bee colony (ABC) algorithm; Mutual learning; Numerical function optimization; Swarm intelligence","Abc algorithms; Artificial bee colonies; Artificial bee colony algorithms; Benchmark functions; Food sources; Mutual learning; Numerical function optimization; Optimization algorithms; Swarm Intelligence; Artificial intelligence; Benchmarking; Evolutionary algorithms; Optimization; Learning algorithms",Article,Scopus,2-s2.0-84863707883
"Gilio A.","Generalizing inference rules in a coherence-based probabilistic default reasoning",2012,"International Journal of Approximate Reasoning",20,10.1016/j.ijar.2011.08.004,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858443824&doi=10.1016%2fj.ijar.2011.08.004&partnerID=40&md5=fb5a177a2baaa768f498119e16ce8bf6","In this paper we first recall some notions and results on the coherence-based probabilistic treatment of uncertainty. Then, we deepen some probabilistic aspects in nonmonotonic reasoning, by generalizing OR, CM, and Cut rules. We also illustrate the degradation of these inference rules when the number of premises increases. Finally, we show that the lower bounds obtained when applying OR and Quasi-Conjunction inference rules coincide, respectively, with Hamacher and Lukasiewicz t-norms; the upper bounds in both rules coincide with Hamacher t-conorm. © 2010 Elsevier Inc. All right reserved.","Coherence principle; Generalization of inference rules; Lower and upper bounds; Probabilistic default reasoning; t-Conorms; t-Norms","Default reasoning; Inference rules; Lower and upper bounds; T - Norm; T-conorms; Artificial intelligence; Software engineering; Mathematical operators",Article,Scopus,2-s2.0-84858443824
"Stahl D., Pickles A., Elsabbagh M., Johnson M.H.","Novel machine learning methods for ERP analysis: A validation from research on infants at risk for autism",2012,"Developmental Neuropsychology",20,10.1080/87565641.2011.650808,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862058665&doi=10.1080%2f87565641.2011.650808&partnerID=40&md5=47348b199e77875eafe965ace7ea04d6","Machine learning and other computer intensive pattern recognition methods are successfully applied to a variety of fields that deal with high-dimensional data and often small sample sizes such as genetic microarray, functional magnetic resonance imaging (fMRI) and, more recently, electroencephalogram (EEG) data. The aim of this article is to discuss the use of machine learning and discrimination methods and their possible application to the analysis of infant event-related potential (ERP) data. The usefulness of two methods, regularized discriminant function analyses and support vector machines, will be demonstrated by reanalyzing an ERP dataset from infants (Elsabbagh et al., 2009). Using cross-validation, both methods successfully discriminated above chance between groups of infants at high and low risk of a later diagnosis of autism. The suitability of machine learning methods for the use of single trial or averaged ERP data is discussed. © 2012 Copyright Taylor and Francis Group, LLC.",,"article; artificial intelligence; attention; autism; automated pattern recognition; discriminant analysis; electroencephalography; evoked response; eye fixation; human; infant; methodology; pathophysiology; physiology; reaction time; reproducibility; risk; statistical model; Artificial Intelligence; Attention; Autistic Disorder; Discriminant Analysis; Electroencephalography; Evoked Potentials; Fixation, Ocular; Humans; Infant; Logistic Models; Pattern Recognition, Automated; Reaction Time; Reproducibility of Results; Risk",Article,Scopus,2-s2.0-84862058665
"Han T.A., Saptawijaya A., Moniz Pereira L.","Moral reasoning under uncertainty",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",20,10.1007/978-3-642-28717-6_18,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858325564&doi=10.1007%2f978-3-642-28717-6_18&partnerID=40&md5=16c799f64c67c383f86d6949cd2be026","We present a Logic Programming framework for moral reasoning under uncertainty. It is enacted by a coherent combination of our two previously implemented systems, Evolution Prospection for decision making, and P-log for probabilistic inference. It allows computing available moral judgments via distinct kinds of prior and post preferences. In introducing various aspects of uncertainty into cases of classical trolley problem moral dilemmas, we show how they may appropriately influence moral judgments, allowing decision makers to opt for different choices, and for these to be externally appraised, even when subject to incomplete evidence, as in courts. © 2012 Springer-Verlag.","Evolution Prospection; Logic Programming; Moral Reasoning; P-log; Uncertainty Reasoning","Coherent combination; Decision makers; Evolution Prospection; Moral judgment; Moral reasoning; P-log; Probabilistic inference; Programming framework; Uncertainty reasoning; Decision making; Logic programming; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84858325564
"Jin D., Yang B., Liu J., Liu D.-Y., He D.-X.","Ant colony optimization based on random walk for community detection in complex networks",2012,"Ruan Jian Xue Bao/Journal of Software",20,10.3724/SP.J.1001.2012.03996,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863341695&doi=10.3724%2fSP.J.1001.2012.03996&partnerID=40&md5=4ec5ca7f37e53799600a438e369b8121","Community structure is one of the most important topological properties in complex networks. The network clustering problem (NCP) refers to the detection of network community structures, and many practical problems can be modeled as NCPs. So far, lots of network clustering algorithms have been proposed. However, further improvements in the clustering accuracy, especially when discovering reasonable community structure without prior knowledge, still constitute an open problem. Building on Markov random walks, the paper addresses this problem with a novel ant colony optimization strategy, named as RWACO, which improves prior results on the NCPs and does not require knowledge of the number of communities present on a given network. The framework of ant colony optimization is taken as the basic framework in the RWACO algorithm. In each iteration, a Markov random walk model is taken as heuristic rule. All of the ants' local solutions are aggregated to a global one through clustering ensemble, which then will be used to update a pheromone matrix. The strategy relies on the progressive strengthening of within-community links and the weakening of between-community links. Gradually, this converges to a solution where the underlying community structure of the complex network will become clearly visible. The performance of algorithm RWACO was tested against a set of benchmark computer-generated networks, and as well on real-world network data sets. Experimental results confirm the validity and improvements of this approach. © 2012 ISCAS.","Ant colony optimization; Community structure; Complex network; Ensemble learning; Network clustering; Random walk","Ant Colony Optimization (ACO); Community structures; Complex networks; Ensemble learning; Network clustering; Random Walk; Artificial intelligence; Benchmarking; Clustering algorithms; Social sciences; Topology; Random processes",Article,Scopus,2-s2.0-84863341695
"Song P., Liang J., Qian Y.","A two-grade approach to ranking interval data",2012,"Knowledge-Based Systems",20,10.1016/j.knosys.2011.10.007,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855970328&doi=10.1016%2fj.knosys.2011.10.007&partnerID=40&md5=7939d60d83bd2071e158554278b4cad7","Ranking decision for interval data is a very important issue in decision making analysis. In recent years, several ranking approaches based on dominance relations have been developed. In these approaches, a dominance degree and an entire dominance degree are employed. However, one cannot obtain the complete rank of objects. To address this problem, this work will propose a two-grade approach to ranking interval data. In this approach, we keep the ranking result induced by the entire dominance degree in the first grade, and then refine the objects that cannot be ranked through introducing a so-called entire directional distance index. An example and a real case are employed to verify the effectivity of the two-grade ranking approach proposed in this paper. © 2011 Elsevier B.V. All rights reserved.","Dominance relation; Information entropy; Interval ordered information systems; Ranking decision; Rough sets","Decision making analysis; Dominance relation; Information entropy; Interval data; Ranking approach; Ranking decision; Rough set; Artificial intelligence; Software engineering",Article,Scopus,2-s2.0-84855970328
"Buchweitz A., Shinkareva S.V., Mason R.A., Mitchell T.M., Just M.A.","Identifying bilingual semantic neural representations across languages",2012,"Brain and Language",20,10.1016/j.bandl.2011.09.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858002834&doi=10.1016%2fj.bandl.2011.09.003&partnerID=40&md5=3a5f7b62e045fe0536e436d5c08c99e9","The goal of the study was to identify the neural representation of a noun's meaning in one language based on the neural representation of that same noun in another language. Machine learning methods were used to train classifiers to identify which individual noun bilingual participants were thinking about in one language based solely on their brain activation in the other language. The study shows reliable (p<.05) pattern-based classification accuracies for the classification of brain activity for nouns across languages. It also shows that the stable voxels used to classify the brain activation were located in areas associated with encoding information about semantic dimensions of the words in the study. The identification of the semantic trace of individual nouns from the pattern of cortical activity demonstrates the existence of a multi-voxel pattern of activation across the cortex for a single noun common to both languages in bilinguals. © 2011 Elsevier Inc.","Bilingualism; FMRI; Pattern-based classification","accuracy; adult; article; brain cortex; brain function; brain region; controlled study; electroencephalogram; female; functional magnetic resonance imaging; functional neuroimaging; human; human experiment; language ability; language processing; language test; linguistics; machine learning; male; mental task; object manipulation; semantics; task performance; thinking; Adult; Artificial Intelligence; Brain Mapping; Cerebral Cortex; Female; Humans; Language; Magnetic Resonance Imaging; Male; Models, Neurological; Multilingualism; Semantics; Speech Perception; Vocabulary; Young Adult",Article,Scopus,2-s2.0-84858002834
"Bruneo D., Scarpa M., Bobbio A., Cerotti D., Gribaudo M.","Markovian agent modeling swarm intelligence algorithms in wireless sensor networks",2012,"Performance Evaluation",20,10.1016/j.peva.2010.11.007,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858155095&doi=10.1016%2fj.peva.2010.11.007&partnerID=40&md5=e7e3290f4dd1abbf9c06933d6798c2d0","Wireless Sensor Networks (WSN) are large networks of tiny sensor nodes that are usually randomly distributed over a geographical region. The network topology may vary in time in an unpredictable manner due to many different causes. For example, in order to reduce power consumption, battery operated sensors undergo cycles of sleepingactive periods; additionally, sensors may be located in hostile environments increasing their likelihood of failure; furthermore, data might also be collected from a range of sources at different times. For this reason multi-hop routing algorithms used to route messages from a sensor node to a sink should be rapidly adaptable to the changing topology. Swarm intelligence has been proposed for this purpose, since it allows the emergence of a single global behavior from the interaction of many simple local agents. Swarm intelligent routing has been traditionally studied by resorting to simulation. The present paper aims to show that the recently proposed modeling technique, known as Markovian Agent Model (MAM), is suited for implementing swarm intelligent algorithms for large networks of interacting sensors. Various experimental results and quantitative performance indices are evaluated to support this claim. The validity of this approach is given a further proof by comparing the results with those obtained by using a WSN discrete event simulator. © 2011 Elsevier B.V. All rights reserved.","Gradient-based routing; Markovian agents; Performance evaluation; Swarm intelligence; Wireless sensor networks","Gradient based; Markovian; Performance evaluation; Swarm Intelligence; Wireless sensor network (WSN); Artificial intelligence; Computer network performance evaluation; Electric network topology; Sensors; Sensor nodes",Article,Scopus,2-s2.0-84858155095
"Jeong H.-Y., Choi C.-R., Song Y.-J.","Personalized Learning Course Planner with E-learning DSS using user profile",2012,"Expert Systems with Applications",20,10.1016/j.eswa.2011.08.109,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80255123679&doi=10.1016%2fj.eswa.2011.08.109&partnerID=40&md5=b99029d79fc0f71446050ec603f4a781","Various methods of E-learning systems, based on information and communications, and geared towards improving learning effectiveness and students' attention span, have been studied. However, most E-learning systems force students to follow the learning course or content established by a teacher. These methods are convenient, but they limit the effectiveness of E-learning. To overcome this limitation and increase effective learning, new techniques that reflect alternative learning styles, such as adaptive learning and personalized learning, have been studied. In this study, we proposed a Personalized Learning Course Planner (PLCP) that allows students to easily select the learning course they desire. User profile data was collected from the students' initial priorities about learning contents as well as the test scores after their study. E-Learning Decision Support System (EL-DSS) in PLCP suggests an appropriate learning course organization, according to calculated results based on the user profile data. To verify the effectiveness of the proposed system, we implemented an English learning system consisting of PLCP. We conducted an experiment with 30 university students and evaluated students' satisfaction by questionnaire analysis. The results indicate that the proposed system improved learning effectiveness and student satisfaction. Further investigation of the participants indicated that suggesting a learning course suitable for students' previous test scores and priorities encouraged students to concentrate on the lesson. © 2011 Elsevier Ltd. All rights reserved.","Course Planner; Decision matrix; Decision Support System; E-Learning system","Adaptive learning; Course Planner; Decision matrices; Decision supports; E-Learning system; E-learning systems; Effective learning; English Learning; Improving learning; Information and communication; Learning contents; Learning course; Learning effectiveness; Learning Style; Personalized learning; Student satisfaction; Students' satisfaction; University students; User profile; Artificial intelligence; Curricula; Decision making; Decision support systems; E-learning; Learning systems; Students; Teaching",Article,Scopus,2-s2.0-80255123679
"Beriha G.S., Patnaik B., Mahapatra S.S., Padhee S.","Assessment of safety performance in Indian industries using fuzzy approach",2012,"Expert Systems with Applications",20,10.1016/j.eswa.2011.09.018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80255131399&doi=10.1016%2fj.eswa.2011.09.018&partnerID=40&md5=122040a37856e8e37a8856f6912ae45d","This paper presents an artificial intelligence approach for prediction of different types of accidents (fatal to minor) in an uncertain environment. Likelihood of occurrence of accidents in the work place is a random phenomenon but judicious investment in various attribute such as expenses in health care, safety training, up-gradation of tools and machinery, and expenses on safety equipment and tools may lead to reduction in accident rate. The relationship between type of accidents and investment is difficult to establish because they do not follow any predictable rule rather associate in a non-linear manner. In such situation, fuzzy logic helps to map inputs and outputs in an efficient manner for building the inference engine so that various types of accidents can be predicted. Prediction of various types of accidents helps the managers to formulate organizational policies for improving safety performance. © 2011 Elsevier Ltd. All rights reserved.","Expert system; Fuzzy sets; Occupational health and safety (OHS); Safety performance","Accident rate; Fuzzy approach; Non-linear; Occupational health and safety; Organizational policies; Safety equipments; Safety performance; Safety training; Uncertain environments; Work place; Accidents; Artificial intelligence; Expert systems; Fuzzy inference; Fuzzy logic; Fuzzy sets; Health care; Industrial hygiene; Investments; Machinery; Accident prevention",Article,Scopus,2-s2.0-80255131399
"Wu O., Hu W., Maybank S.J., Zhu M., Li B.","Efficient clustering aggregation based on data fragments",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",20,10.1109/TSMCB.2012.2183591,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862783105&doi=10.1109%2fTSMCB.2012.2183591&partnerID=40&md5=044208d6bd9c73affe4352eae4200507","Clustering aggregation, known as clustering ensembles, has emerged as a powerful technique for combining different clustering results to obtain a single better clustering. Existing clustering aggregation algorithms are applied directly to data points, in what is referred to as the point-based approach. The algorithms are inefficient if the number of data points is large. We define an efficient approach for clustering aggregation based on data fragments. In this fragment-based approach, a data fragment is any subset of the data that is not split by any of the clustering results. To establish the theoretical bases of the proposed approach, we prove that clustering aggregation can be performed directly on data fragments under two widely used goodness measures for clustering aggregation taken from the literature. Three new clustering aggregation algorithms are described. The experimental results obtained using several public data sets show that the new algorithms have lower computational complexity than three well-known existing point-based clustering aggregation algorithms (Agglomerative, Furthest, and LocalSearch); nevertheless, the new algorithms do not sacrifice the accuracy. © 2012 IEEE.","Clustering aggregation; comparison measure; computational complexity; data fragment; fragment-based approach; mutual information; point-based approach","Clustering aggregation; Comparison measures; Data fragments; fragment-based approach; Mutual informations; Point-based; Computational complexity; Clustering algorithms; algorithm; article; artificial intelligence; automated pattern recognition; cluster analysis; computer simulation; decision support system; factual database; information retrieval; methodology; theoretical model; Algorithms; Artificial Intelligence; Cluster Analysis; Computer Simulation; Databases, Factual; Decision Support Techniques; Information Storage and Retrieval; Models, Theoretical; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84862783105
"Toro C., Sanchez E., Carrasco E., Mancilla-Amaya L., Sanín C., Szczerbicki E., Graña M., Bonachela P., Parra C., Bueno G., Guijarro F.","Using set of Experience Knowledge Structure to extend a rule set of clinical decision support system for alzheimer's disease diagnosis",2012,"Cybernetics and Systems",20,10.1080/01969722.2012.654070,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858016044&doi=10.1080%2f01969722.2012.654070&partnerID=40&md5=03df024932d98be8d91656a5e31b8f28","In this article we present an experience-based clinical decision support system (CDSS) for the diagnosis of Alzheimer's disease, which enables the discovery of new knowledge in the system and the generation of new rules that drive reasoning. In order to evolve an initial set of production rules given by medical experts we make use of the Set of Experience Knowledge Structure (SOEKS). An illustrative case of our system is also presented. Copyright © 2012 Taylor & Francis Group, LLC.","Alzheimer's disease; clinical decision support system; set of experience knowledge structure; user experience","Alzheimer's disease; Clinical decision support systems; Medical experts; Production rules; Rule set; set of experience knowledge structure; user experience; Decision support systems; Diagnosis; Diseases; Artificial intelligence",Article,Scopus,2-s2.0-84858016044
"Alvarado A., Vedantam S., Goethals P., Nopens I.","A compartmental model to describe hydraulics in a full-scale waste stabilization pond",2012,"Water Research",20,10.1016/j.watres.2011.11.038,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84355166415&doi=10.1016%2fj.watres.2011.11.038&partnerID=40&md5=3cf7b7f71cf9dd03cacbd788ba27edb2","The advancement of experimental and computational resources has facilitated the use of computational fluid dynamics (CFD) models as a predictive tool for mixing behaviour in full-scale waste stabilization pond systems. However, in view of combining hydraulic behaviour with a biokinetic process model, the computational load is still too high for practical use. This contribution presents a method that uses a validated CFD model with tracer experiments as a platform for the development of a simpler compartmental model (CM) to describe the hydraulics in a full-scale maturation pond (7 ha) of a waste stabilization ponds complex in Cuenca (Ecuador). 3D CFD models were validated with experimental data from pulse tracer experiments, showing a sufficient agreement. Based on the CFD model results, a number of compartments were selected considering the turbulence characteristics of the flow, the residence time distribution (RTD) curves and the dominant velocity component at different pond locations. The arrangement of compartments based on the introduction of recirculation flow rate between adjacent compartments, which in turn is dependent on the turbulence diffusion coefficient, is illustrated. Simulated RTD's from a systemic tanks-in-series (TIS) model and the developed CM were compared. The TIS was unable to capture the measured RTD, whereas the CM predicted convincingly the peaks and lags of the tracer experiment using only a minimal fraction of the computational demand of the CFD model. Finally, a biokinetic model was coupled to both approaches demonstrating the impact an insufficient hydraulic model can have on the outcome of a modelling exercise. TIS and CM showed drastic differences in the output loads implying that the CM approach is to be used when modelling the biological performance of the full-scale system. © 2011 Elsevier Ltd.","Compartmental model; Computation fluid dynamics (CFD); Hydrodynamics; Mixing; Modelling; Tracer study; Waste stabilization ponds (WSP)","Biokinetic models; Biological performance; CFD models; Compartmental model; Computational demands; Computational loads; Computational resources; Ecuador; Experimental data; Full-scale system; Hydraulic behaviour; Maturation ponds; Mixing behaviour; Modelling; Output load; Predictive tools; Process model; Re-circulation flow; Residence time distribution curves; Tanks-in-series; Tracer experiment; Tracer study; Turbulence characteristics; Turbulence diffusion; Velocity components; Waste stabilization ponds; Waste stabilization ponds (WSP); Artificial intelligence; Computational fluid dynamics; Dynamics; Experiments; Hydraulic models; Hydrodynamics; Lakes; Mixing; Stabilization; Stabilization ponds; Three dimensional; Turbulence; Mathematical models; compartmentalization; computational fluid dynamics; experimental study; hydraulics; hydrodynamics; mixing ratio; pond; recirculating system; stabilization; tracer; turbulence; velocity; article; compartment model; computational fluid dynamics; diffusion coefficient; Ecuador; flow rate; hydraulics; priority journal; validation study; velocity; waste stabilization pond; waste water management; water and water related phenomena; water flow; water sampling; Ecuador; Hydrodynamics; Models, Biological; Waste Disposal, Fluid; Water Movements",Article,Scopus,2-s2.0-84355166415
"Simonov M., Mussetta M., Grimaccia F., Leva S., Zich R.E.","Artificial intelligence forecast of PV plant production for integration in smart energy systems",2012,"International Review of Electrical Engineering",20,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860135191&partnerID=40&md5=590e973bd74468da7b40afad329976ab","Computation techniques play an important role in most engineering problems in which optimization problems have to be faced. Energy management operations represent one of these cases where real-time energy production, transfer, storage and consumption need to be optimized. In this context renewable energy sources can be managed using evolutionary computation and other tools. In this light artificial neural network solution based on weather forecast can estimate energy flows combined with the event-driven variability encouraging photovoltaic integration with the electric power system. This article discusses the role of these computational tools and some issues related to the variability and uncertainty in the operations where PV plants are potentially fully connected to the power grid in a future scenario. © 2012 Praise Worthy Prize S.r.l. - All rights reserved.","Artificial intelligence; PV forecasting; Renewable energy; Smart grids",,Article,Scopus,2-s2.0-84860135191
"Darus I.Z.M., Al-Khafaji A.A.M.","Non-parametric modelling of a rectangular flexible plate structure",2012,"Engineering Applications of Artificial Intelligence",20,10.1016/j.engappai.2011.09.009,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80855132640&doi=10.1016%2fj.engappai.2011.09.009&partnerID=40&md5=fde01204fd765b600c55c06dbda6aac0","This research investigates the performance of dynamic modelling using non-parametric techniques for identification of a flexible structure system for development of active vibration control. In this paper, the implementation details are described and the experimental studies conducted in this research are analysed. The inputoutput data of the system were first acquired through the experimental studies using National Instruments (NI) data acquisition system. A sinusoidal force was applied to excite the flexible plate and the dynamic response of the system was then investigated. Non-parametric modelling of the system were developed using several artificial intelligent methodologies namely Adaptive Elman Neural Networks (ENN), Backpropagation Multi-layer Perceptron Neural Networks (MLPNN) and Adaptive Neuro-Fuzzy Inference System (ANFIS). The performance of all these methodologies were compared and discussed. Finally, validation and verification of the obtained model was conducted using One Step Ahead (OSA) prediction, mean squared error (MSE) and correlation tests. The prediction ability of the model was further observed with unseen data. The results verified that the MLPNN converge to an optimum solution faster and the dynamic model obtained described the flexible plate structure very well. The non-parametric models of the flexible plate structure thus developed and validated will be used as the representation of the transfer function of the system in subsequent investigations for the development of active vibration control strategies for vibration suppression in flexible structures. © 2011 Elsevier Ltd. All rights reserved.","Active vibration control; ANFIS; Flexible plate; Neural network; System identification","Active vibration controls; Adaptive neuro-fuzzy inference system; ANFIS; Artificial intelligent; Correlation tests; Data acquisition system; Dynamic modelling; Elman neural network; Experimental studies; Flexible plates; Input-output data; Mean squared error; Multi-layer perceptron neural networks; National Instruments; Non-parametric; Non-parametric model; Non-parametric techniques; One step; Optimum solution; System identifications; Validation and verification; Vibration suppression; Artificial intelligence; Dynamic response; Flexible structures; Fuzzy inference; Fuzzy neural networks; Fuzzy systems; Identification (control systems); Investments; Network layers; Parameter estimation; Vibration control; Plates (structural components)",Article,Scopus,2-s2.0-80855132640
"Sifre L., Mallat S.","Combined scattering for rotation invariant texture analysis",2012,"ESANN 2012 proceedings, 20th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning",20,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886934383&partnerID=40&md5=f208365a1af5ae3a7a471124af866dfa","This paper introduces a combined scattering representation for texture classification, which is invariant to rotations and stable to deformations. A combined scattering is computed with two nested cascades of wavelet transforms and complex modulus, along spatial and rotation variables. Results are compared with state-of-the-art algorithms, with a nearest neighbor classifier. © 2012, i6doc.com publication. All rights reserved.",,"Complex networks; Learning systems; Neural networks; Wavelet transforms; Complex modulus; Nearest Neighbor classifier; Rotation invariant; State-of-the-art algorithms; Texture classification; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84886934383
"Kandara O., Ozturk A.","Genetic algorithms to determine the critical values of a power energy system for different operating conditions",2012,"Energy Education Science and Technology Part A: Energy Science and Research",20,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861967499&partnerID=40&md5=45b31470cd07a1218c8c1b031f9c2063","Critical values of a power system are the values of load bus active power, load bus voltage amplitude, and load bus angle when the load bus has the highest active power value. These values depend on various factors such as the length, voltage, the number, and the serial and shunt compensation rates of the line. In this study, we take the IEEE's 6-bus power system as our base system. We first used the Newton Raphson (NR) method to determine the critical values under various operating conditions. Then, we used genetic algorithms, which is an artificial intelligence method commonly used in optimization, to determine the critical values under the same operating conditions. At the end, we compared the results. The results show that overall the critical values determined using GA is as good as with or better than those obtained using the traditional NR method. © Sila Science.","Artificial Intelligence; Genetic algorithms; Newton Raphson; Power Distribution; Power system stability","Active power; Artificial intelligence methods; Base systems; Critical value; Load bus voltage; Newton-Raphson; Operating condition; Power distributions; Power energy; Power system stability; Shunt compensation; Artificial intelligence; System stability; Genetic algorithms",Article,Scopus,2-s2.0-84861967499
"Xiao G., Ma Y.","Inconsistency measurement based on variables in minimal unsatisfiable subsets",2012,"Frontiers in Artificial Intelligence and Applications",20,10.3233/978-1-61499-098-7-864,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878803544&doi=10.3233%2f978-1-61499-098-7-864&partnerID=40&md5=f7387b516780482accaacb7a1f7264b4","Measuring inconsistency degrees of knowledge bases (KBs) provides important context information for facilitating inconsistency handling. Several semantic and syntax based measures have been proposed separately. In this paper, we propose a new way to define inconsistency measurements by combining semantic and syntax based approaches. It is based on counting the variables of minimal unsatisfiable subsets (MUSes) and minimal correction subsets (MCSes), which leads to two equivalent inconsistency degrees, named IDMUS and IDMCS. We give the theoretical and experimental comparisons between them and two purely semantic-based inconsistency degrees: 4-valued and the Quasi Classical semantics based inconsistency degrees. Moreover, the computational complexities related to our new inconsistency measurements are studied. As it turns out that computing the exact inconsistency degrees is intractable in general, we then propose and evaluate an anytime algorithm to make ID MUS and IDMCS usable in knowledge management applications. In particular, as most of syntax based measures tend to be difficult to compute in reality due to the exponential number of MUSes, our new inconsistency measures are practical because the numbers of variables in MUSes are often limited or easily to be approximated. We evaluate our approach on the DC benchmark. Our encouraging experimental results show that these new inconsistency measurements or their approximations are efficient to handle large knowledge bases and to better distinguish inconsistent knowledge bases. © 2012 The Author(s).",,"Artificial intelligence; Knowledge management; Syntactics; Experimental comparison; Inconsistency handling; Inconsistency measures; Knowledge basis (KBs); Knowledge management applications; Large knowledge basis; Measuring inconsistency; Syntax-based approach; Semantics",Conference Paper,Scopus,2-s2.0-84878803544
"Catalão J.P.S., Pousinho H.M.I., Mendes V.M.F.","Optimal offering strategies for wind power producers considering uncertainty and risk",2012,"IEEE Systems Journal",20,10.1109/JSYST.2011.2163009,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861749085&doi=10.1109%2fJSYST.2011.2163009&partnerID=40&md5=aa6ae1f6bcb566194e3f6a1027ea5b34","This paper provides a two-stage stochastic programming approach for the development of optimal offering strategies for wind power producers. Uncertainty is related to electricity market prices and wind power production. A hybrid intelligent approach, combining wavelet transform, particle swarm optimization and adaptive-network-based fuzzy inference system, is used in this paper to generate plausible scenarios. Also, risk aversion is explicitly modeled using the conditional value-at-risk methodology. Results from a realistic case study, based on a wind farm in Portugal, are provided and analyzed. Finally, conclusions are duly drawn. © 2012 IEEE.","Artificial intelligence; forecasting; risk analysis; stochastic programming; uncertainty; wind power","Adaptive network based fuzzy inference system; Conditional Value-at-Risk; Electricity market; Hybrid intelligent approach; Portugal; Power producer; Risk aversion; Two-stage stochastic programming; uncertainty; Wind farm; Wind power production; Artificial intelligence; Electric utilities; Forecasting; Optimization; Risk analysis; Stochastic programming; Wind power",Article,Scopus,2-s2.0-84861749085
"Costabello L., Villata S., Gandon F.","Context-aware access control for RDF graph stores",2012,"Frontiers in Artificial Intelligence and Applications",20,10.3233/978-1-61499-098-7-282,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878811232&doi=10.3233%2f978-1-61499-098-7-282&partnerID=40&md5=56a09fbcc0bf8961443b940570ec64a1","We present SHI3LD, an access control framework for RDF stores. Our solution supports access from mobile devices with context-aware policies and is exclusively grounded on standard Semantic Web languages. Designed as a pluggable filter for generic SPARQL endpoints, the module uses RDF named graphs and SPARQL to protect triples. Evaluation shows faster execution time for low-selective queries and less impact on larger datastores. © 2012 The Author(s).",,"Artificial intelligence; Mobile devices; Semantic Web; Context-aware access control; Context-aware policies; Control framework; Named graphs; Pluggable filters; RDF graph; Rdf stores; Semantic web languages; Access control",Conference Paper,Scopus,2-s2.0-84878811232
"Domke J.","Generic methods for optimization-based modeling",2012,"Journal of Machine Learning Research",20,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869036002&partnerID=40&md5=8666f83185dad75c1e2ea5b67bc9d04c","""Energy"" models for continuous domains can be applied to many problems, but often suffer from high computational expense in training, due to the need to repeatedly minimize the energy function to high accuracy. This paper considers a modified setting, where the model is trained in terms of results after optimization is truncated to a fixed number of iterations. We derive ""backpropagating"" versions of gradient descent, heavy-ball and LBFGS. These are simple to use, as they require as input only routines to compute the gradient of the energy with respect to the domain and parameters. Experimental results on denoising and image labeling problems show that learning with truncated optimization greatly reduces computational expense compared to ""full"" fitting.",,"Artificial intelligence; Computational expense; Continuous domain; Energy functions; Fixed numbers; Generic method; Gradient descent; High-accuracy; Image labeling; Optimization",Conference Paper,Scopus,2-s2.0-84869036002
"Rangapuram S.S., Hein M.","Constrained 1-spectral clustering",2012,"Journal of Machine Learning Research",20,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904657104&partnerID=40&md5=9d88e7aaf73dce513b49f7f17014142f","An important form of prior information in clustering comes in form of cannot-link and must-link constraints. We present a generalization of the popular spectral clustering technique which integrates such constraints. Motivated by the recently proposed 1-spectral clustering for the unconstrained problem, our method is based on a tight relaxation of the constrained normalized cut into a continuous optimization problem. Opposite to all other methods which have been suggested for constrained spectral clustering, we can always guarantee to satisfy all constraints. Moreover, our soft formulation allows to optimize a trade-off between normalized cut and the number of violated constraints. An efficient implementation is provided which scales to large datasets. We outperform consistently all other proposed methods in the experiments. © Copyright 2012 by the authors.",,"Artificial intelligence; Clustering algorithms; Economic and social effects; Optimization; Constrained spectral clustering; Continuous optimization problems; Efficient implementation; Large datasets; Normalized cuts; Prior information; Spectral clustering; Unconstrained problems; Constrained optimization",Conference Paper,Scopus,2-s2.0-84904657104
"Valverde-Rebaza J.C., De Andrade Lopes A.","Link prediction in complex networks based on cluster information",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",20,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952048979&partnerID=40&md5=82dcb125fbd4f65ff049912adca75e10","Cluster in graphs is densely connected group of vertices sparsely connected to other groups. Hence, for prediction of a future link between a pair of vertices, these vertices common neighbors may play different roles depending on if they belong or not to the same cluster. Based on that, we propose a new measure (WIC) for link prediction between a pair of vertices considering the sets of their intra-cluster or within-cluster (W) and between-cluster or inter-cluster (IC) common neighbors. Also, we propose a set of measures, referred to as W forms, using only the set given by the within-cluster common neighbors instead of using the set of all common neighbors as usually considered in the basic local similarity measures. Consequently, a previous clustering scheme must be applied on the graph. Using three different clustering algorithms, we compared WIC measure with ten basic local similarity measures and their counter- part W forms on ten real networks. Our analyses suggest that clustering information, no matter the clustering algorithm used, improves link pre- diction accuracy. © Springer-Verlag Berlin Heidelberg 2012.","Clustering; Complex Networks; Link Prediction","Artificial intelligence; Complex networks; Forecasting; Between clusters; Clustering; Clustering information; Clustering scheme; Inter clusters; Link prediction; Local similarity measure; Within clusters; Clustering algorithms",Conference Paper,Scopus,2-s2.0-84952048979
"Wooldridge M.","Does game theory work?",2012,"IEEE Intelligent Systems",19,10.1109/MIS.2012.108,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870877251&doi=10.1109%2fMIS.2012.108&partnerID=40&md5=f271891c7dcf23a89045e5a127db18ce","Given the current level of international interest in game theory and its applications in AI and computer science, it seems worth pausing to consider whether game theory actually works. This article considers the evidence available in support of the two most common interpretations of game theory: the descriptive interpretation, which considers game theory as trying to predict actual behavior, and the normative interpretation, which considers game theory as providing advice on how to act optimally. © 2011 IEEE.","behavioral economics; descriptive game theory; effectiveness of game theory; game theory; normative game theory","Behavioral economics; Current levels; Artificial intelligence; Intelligent systems; Game theory",Article,Scopus,2-s2.0-84870877251
"Iyer R., Bilmes J.","Algorithms for approximate minimization of the difference between submodular functions, with applications",2012,"Uncertainty in Artificial Intelligence - Proceedings of the 28th Conference, UAI 2012",19,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886063323&partnerID=40&md5=6f7327f32ba34bdfb29a26c33fd4bb7d","We extend the work of Narasimhan and Bilmes [30] for minimizing set functions representable as a difference between submodular functions. Similar to [30], our new algorithms are guaranteed to monotonically reduce the objective function at every step. We empirically and theoretically show that the per-iteration cost of our algorithms is much less than [30], and our algorithms can be used to efficiently minimize a difference between submodular functions under various combinatorial constraints, a problem not previously addressed. We provide computational bounds and a hardness result on the multiplicative inapproximability of minimizing the difference between submodular functions. We show, however, that it is possible to give worst-case additive bounds by providing a polynomial time computable lower-bound on the minima. Finally we show how a number of machine learning problems can be modeled as minimizing the difference between submodular functions. We experimentally show the validity of our algorithms by testing them on the problem of feature selection with submodular cost features.",,"Hardness result; Inapproximability; Lower bounds; Machine learning problem; Objective functions; Polynomial-time; Set function; Submodular functions; Algorithms; Artificial intelligence; Polynomial approximation; Iterative methods",Conference Paper,Scopus,2-s2.0-84886063323
"Kengpol A., Meethom W., Tuominen M.","The development of a decision support system in multimodal transportation routing within Greater Mekong sub-region countries",2012,"International Journal of Production Economics",19,10.1016/j.ijpe.2011.02.024,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866733369&doi=10.1016%2fj.ijpe.2011.02.024&partnerID=40&md5=9132b41bde6469009c6fa8d38cf9e7ad","The objective of this research is to develop a decision support system (DSS) which can accommodate evaluation models to optimise multimodal transportation routing within Greater Mekong sub-region countries (GMS). This model is the combination of a number of models, the Analytic Hierarchy Process (AHP) which can help to bring consistency weight whose decision criteria - both quantitative and qualitative - are expressed in subjective measures according to the point of view of users. The quantitative factors are found by multimodal transport cost model and qualitative factors by using factor analysis. Then, it is followed by the Zero-One Goal Programming (ZOGP) which can integrate weights from the AHP to achieve an optimal multimodal transportation routing based upon user constraints. The analysis results, software developed, recommendations and limitations are also presented. © 2012 Elsevier B.V. All rights reserved.","Analytic Hierarchy Process (AHP); Decision support system (DSS); Greater Mekong sub-region (GMS) countries; Multimodal transportation; Zero-One Goal Programming (ZOGP)","Evaluation models; Multimodal transport; Qualitative factors; Quantitative factors; Sub-regions; User constraints; Zero-one goal programming; Analytic hierarchy process; Artificial intelligence; Decision support systems; Hierarchical systems; Multimodal transportation",Conference Paper,Scopus,2-s2.0-84866733369
"Ritota M., Casciani L., Failla S., Valentini M.","HRMAS-NMR spectroscopy and multivariate analysis meat characterisation",2012,"Meat Science",19,10.1016/j.meatsci.2012.06.034,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865553577&doi=10.1016%2fj.meatsci.2012.06.034&partnerID=40&md5=68b57482c87c9d6a49163e4e7ef1f448","1H-high resolution magic angle spinning-nuclear magnetic resonance spectroscopy was employed to gain the metabolic profile of longissimus dorsi and semitendinosus muscles of four different breeds: Chianina, Holstein Friesian, Maremmana and Buffalo.Principal component analysis, partial least squares projection to latent structure - discriminant analysis and orthogonal partial least squares projection to latent structure - discriminant analysis were used to build models capable of discriminating the muscle type according to the breed. Data analysis led to an excellent classification for Buffalo and Chianina, while for Holstein Friesian the separation was lower. In the case of Maremmana the use of intelligent bucketing was necessary due to some resonances shifting allowed improvement of the discrimination ability. Finally, by using the Variable Importance in Projection values the metabolites relevant for the classification were identified. © 2012 Elsevier Ltd.","Intelligent bucketing; Longissimus dorsi; NMR assignment; PLS-DA; Semitendinosus","Intelligent bucketing; Longissimus; NMR assignment; PLS-DA; Semitendinosus; Discriminant analysis; Nuclear magnetic resonance spectroscopy; Friesia; animal; article; artificial intelligence; biological model; biology; cattle; chemistry; comparative study; discriminant analysis; food control; Italy; male; meat; metabolism; metabolome; methodology; nuclear magnetic resonance spectroscopy; principal component analysis; regression analysis; skeletal muscle; species difference; Animals; Artificial Intelligence; Cattle; Computational Biology; Discriminant Analysis; Food Inspection; Italy; Least-Squares Analysis; Magnetic Resonance Spectroscopy; Male; Meat; Metabolome; Models, Biological; Muscle, Skeletal; Principal Component Analysis; Species Specificity",Article,Scopus,2-s2.0-84865553577
"Maienschein-Cline M., Dinner A.R., Hlavacek W.S., Mu F.","Improved predictions of transcription factor binding sites using physicochemical features of DNA",2012,"Nucleic Acids Research",19,10.1093/nar/gks771,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871226594&doi=10.1093%2fnar%2fgks771&partnerID=40&md5=c240acc13fdd53dd68bc67f11fcd6345","Typical approaches for predicting transcription factor binding sites (TFBSs) involve use of a position-specific weight matrix (PWM) to statistically characterize the sequences of the known sites. Recently, an alternative physicochemical approach, called SiteSleuth, was proposed. In this approach, a linear support vector machine (SVM) classifier is trained to distinguish TFBSs from background sequences based on local chemical and structural features of DNA. SiteSleuth appears to generally perform better than PWM-based methods. Here, we improve the SiteSleuth approach by considering both new physicochemical features and algorithmic modifications. New features are derived from Gibbs energies of amino acid-DNA interactions and hydroxyl radical cleavage profiles of DNA. Algorithmic modifications consist of inclusion of a feature selection step, use of a nonlinear kernel in the SVM classifier, and use of a consensus-based post-processing step for predictions. We also considered SVM classification based on letter features alone to distinguish performance gains from use of SVM-based models versus use of physicochemical features. The accuracy of each of the variant methods considered was assessed by cross validation using data available in the RegulonDB database for 54 Escherichia coli TFs, as well as by experimental validation using published ChIP-chip data available for Fis and Lrp. © 2012 The Author(s).",,"amino acid; DNA; hydroxyl radical; transcription factor; accuracy; amino acid DNA interaction; article; binding site; classification algorithm; DNA cleavage; DNA structure; Escherichia coli; genetic algorithm; genetic database; interactions with DNA; intermethod comparison; kernel method; nonlinear system; physical chemistry; position weight matrix; priority journal; process optimization; regulon; support vector machine; validation process; Algorithms; Artificial Intelligence; Binding Sites; Chromatin Immunoprecipitation; DNA; Escherichia coli; Escherichia coli Proteins; Factor For Inversion Stimulation Protein; Leucine-Responsive Regulatory Protein; Nucleotide Motifs; Support Vector Machines; Transcription Factors; Escherichia coli",Article,Scopus,2-s2.0-84871226594
"Amin K., Kearns M., Key P., Schwaighofer A.","Budget optimization for sponsored search: Censored learning in MDPs",2012,"Uncertainty in Artificial Intelligence - Proceedings of the 28th Conference, UAI 2012",19,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885984188&partnerID=40&md5=6e37daee3e8f08f59ea8a977243629df","We consider the budget optimization problem faced by an advertiser participating in repeated sponsored search auctions, seeking to maximize the number of clicks attained under that budget. We cast the budget optimization problem as a Markov Decision Process (MDP) with censored observations, and propose a learning algorithm based on the wellknown Kaplan-Meier or product-limit estimator. We validate the performance of this algorithm by comparing it to several others on a large set of search auction data from Microsoft adCenter, demonstrating fast convergence to optimal performance.",,"Censored observations; Fast convergence; Markov Decision Processes; Optimal performance; Optimization problems; Product-limit estimator; Sponsored search auctions; Sponsored searches; Artificial intelligence; Budget control; Learning algorithms; Markov processes; Optimization",Conference Paper,Scopus,2-s2.0-84885984188
"Litjens G., Debats O., van de Ven W., Karssemeijer N., Huisman H.","A pattern recognition approach to zonal segmentation of the prostate on MRI.",2012,"Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention",19,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872898116&partnerID=40&md5=d815d2c2de806caa6af9301d791e8b04","Zonal segmentation of the prostate into the central gland and peripheral zone is a useful tool in computer-aided detection of prostate cancer, because occurrence and characteristics of cancer in both zones differ substantially. In this paper we present a pattern recognition approach to segment the prostate zones. It incorporates three types of features that can differentiate between the two zones: anatomical, intensity and texture. It is evaluated against a multi-parametric multi-atlas based method using 48 multi-parametric MRI studies. Three observers are used to assess inter-observer variability and we compare our results against the state of the art from literature. Results show a mean Dice coefficient of 0.89 +/- 0.03 for the central gland and 0.75 +/- 0.07 for the peripheral zone, compared to 0.87 +/- 0.04 and 0.76 +/- 0.06 in literature. Summarizing, a pattern recognition approach incorporating anatomy, intensity and texture has been shown to give good results in zonal segmentation of the prostate.",,"algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; human; image enhancement; male; methodology; nuclear magnetic resonance imaging; observer variation; pathology; prostate tumor; reproducibility; sensitivity and specificity; Algorithms; Artificial Intelligence; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Male; Observer Variation; Pattern Recognition, Automated; Prostatic Neoplasms; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84872898116
"Haro B.B., Zappella L., Vidal R.","Surgical gesture classification from video data.",2012,"Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention",19,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872545114&partnerID=40&md5=dc92c2940a3671eb85f7076745009cbc","Much of the existing work on automatic classification of gestures and skill in robotic surgery is based on kinematic and dynamic cues, such as time to completion, speed, forces, torque, or robot trajectories. In this paper we show that in a typical surgical training setup, video data can be equally discriminative. To that end, we propose and evaluate three approaches to surgical gesture classification from video. In the first one, we model each video clip from each surgical gesture as the output of a linear dynamical system (LDS) and use metrics in the space of LDSs to classify new video clips. In the second one, we use spatio-temporal features extracted from each video clip to learn a dictionary of spatio-temporal words and use a bag-of-features (BoF) approach to classify new video clips. In the third approach, we use multiple kernel learning to combine the LDS and BoF approaches. Our experiments show that methods based on video data perform equally well as the state-of-the-art approaches based on kinematic data.",,"algorithm; article; artificial intelligence; automated pattern recognition; computer assisted surgery; computer program; gesture; human; image processing; instrumentation; methodology; movement (physiology); normal distribution; reproducibility; robotics; signal processing; statistical model; time; videorecording; Algorithms; Artificial Intelligence; Gestures; Humans; Image Processing, Computer-Assisted; Linear Models; Movement; Normal Distribution; Pattern Recognition, Automated; Reproducibility of Results; Robotics; Signal Processing, Computer-Assisted; Software; Surgery, Computer-Assisted; Time Factors; Video Recording",Article,Scopus,2-s2.0-84872545114
"Holec M., Kléma J., Zelezný F., Tolar J.","Comparative evaluation of set-level techniques in predictive classification of gene expression samples.",2012,"BMC bioinformatics",19,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873884420&partnerID=40&md5=135b8ac8c342b6e69e1f6ea191ac6c81","Analysis of gene expression data in terms of a priori-defined gene sets has recently received significant attention as this approach typically yields more compact and interpretable results than those produced by traditional methods that rely on individual genes. The set-level strategy can also be adopted with similar benefits in predictive classification tasks accomplished with machine learning algorithms. Initial studies into the predictive performance of set-level classifiers have yielded rather controversial results. The goal of this study is to provide a more conclusive evaluation by testing various components of the set-level framework within a large collection of machine learning experiments. Genuine curated gene sets constitute better features for classification than sets assembled without biological relevance. For identifying the best gene sets for classification, the Global test outperforms the gene-set methods GSEA and SAM-GS as well as two generic feature selection methods. To aggregate expressions of genes into a feature value, the singular value decomposition (SVD) method as well as the SetSig technique improve on simple arithmetic averaging. Set-level classifiers learned with 10 features constituted by the Global test slightly outperform baseline gene-level classifiers learned with all original data features although they are slightly less accurate than gene-level classifiers learned with a prior feature-selection step. Set-level classifiers do not boost predictive accuracy, however, they do achieve competitive accuracy if learned with the right combination of ingredients. Open-source, publicly available software was used for classifier learning and testing. The gene expression datasets and the gene set database used are also publicly available. The full tabulation of experimental results is available at http://ida.felk.cvut.cz/CESLT.",,"algorithm; article; artificial intelligence; Bayes theorem; biology; decision tree; gene expression profiling; methodology; support vector machine; Algorithms; Artificial Intelligence; Bayes Theorem; Computational Biology; Decision Trees; Gene Expression Profiling; Support Vector Machines",Article,Scopus,2-s2.0-84873884420
"Sudholt D., Thyssen C.","A simple ant colony optimizer for stochastic shortest path problems",2012,"Algorithmica",19,10.1007/s00453-011-9606-2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864963887&doi=10.1007%2fs00453-011-9606-2&partnerID=40&md5=1cc39d1500b2bf0b01f3d91c9f4f309f","Ant Colony Optimization (ACO) is a popular optimization paradigm inspired by the capabilities of natural ant colonies of finding shortest paths between their nest and a food source. This has led to many successful applications for various combinatorial problems. The reason for the success of ACO, however, is not well understood and there is a need for a rigorous theoretical foundation. We analyze the running time of a simple ant colony optimizer for stochastic shortest path problems where edge weights are subject to noise that reflects delays and uncertainty particular, we consider various noise models, ranging from general, arbitrary noise with possible dependencies to more specific models such as independent gamma-distributed noise. The question is whether the ants can find or approximate shortest paths in the presence of noise. We characterize instances where the ants can discover the real shortest paths efficiently. For general instances we prove upper bounds for the time until the algorithm finds reasonable approximations. Contrariwise, for independent gamma-distributed noise we present a graph where the ant system needs exponential time to find a good approximation. The behavior of the ant system changes dramatically when the noise is perfectly correlated as then the ants find shortest paths efficiently. Our results shed light on trade-offs between the noise strength, approximation guarantees, and expected running times. © 2011 The Author(s).","Ant colony optimization; Combinatorial optimization; Running time analysis; Shortest path problems; Stochastic optimization","Ant colonies; Ant Colony Optimization (ACO); Ant systems; Arbitrary noise; Combinatorial problem; Edge weights; Exponential time; Food sources; Gamma-distributed; Noise models; Noise strength; Optimizers; Running time; Running time analysis; Shortest path; Shortest path problem; Stochastic optimizations; Stochastic Shortest Path Problem; Theoretical foundations; Upper Bound; Approximation algorithms; Artificial intelligence; Combinatorial optimization; Stochastic models; Stochastic systems; Uncertainty analysis; Graph theory",Article,Scopus,2-s2.0-84864963887
"Souahlia S., Bacha K., Chaari A.","MLP neural network-based decision for power transformers fault diagnosis using an improved combination of Rogers and Doernenburg ratios DGA",2012,"International Journal of Electrical Power and Energy Systems",19,10.1016/j.ijepes.2012.05.067,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864418620&doi=10.1016%2fj.ijepes.2012.05.067&partnerID=40&md5=8b8f1f0ea038704ddc6008f756be3be3","Dissolved gas analysis (DGA) is a widely-used method to detect the power transformer faults, because of its high sensitivity to small amount of electrical faults. The DGA is exploited for fault classification tools implementation using the artificial intelligence techniques. In this study, we use the Rogers ratios, the Doernenburg ratios methods and our proposed combination of Rogers and Doernenburg ratios DGA methods as gas signature. The multi-layer perceptron neural network (MLPNN) is applied for decision making. The paper presents a comparative study on one hand for the choice the most appropriate DGA method and to resolve the problem of conflict between the Rogers and Doernenburg ratios methods. On the other hand, it compares the various MLP architectures by comparing two output data types and three hidden layer types with the aim to establish the most appropriate MLP model. Before testing, the proposed structures are trained and tested by the experimental data from Tunisian Company of Electricity and Gas (STEG). The test results suggest that MLPNN ratios combination can generalize better than other MLPNN models. The approach has the advantages of high accuracy. The other advantage is that the model is practically applicable and may be utilized for an automated power transformer diagnosis. The classification accuracies of the MLPNN classifier are compared with fuzzy logic (FL), radial basis function (RBF), K-nearest neighbor (KNN) and probabilistic neural network (PNN) classifiers. The test results indicate that the developed preprocessing approach can significantly improve the diagnosis accuracies for power transformer fault classification. © 2012 Elsevier Ltd. All rights reserved.","Dissolved gas analysis; Multi-layer perceptron; Neural network; Transformer fault diagnosis","Artificial intelligence techniques; Classification accuracy; Comparative studies; Dissolved gas analysis; Electrical faults; Experimental data; Fault classification; Hidden layers; High sensitivity; K-nearest neighbors; MLP model; Multi layer perceptron; Multi-layer perceptron neural networks; Network-based; Output data; Power transformer diagnosis; Preprocessing approaches; Probabilistic neural networks; Radial basis functions; Transformer fault diagnosis; Transformer faults; Fault detection; Fuzzy logic; Neural networks; Radial basis function networks; Power transformers",Article,Scopus,2-s2.0-84864418620
"Grau B.C., Motik B.","Reasoning over ontologies with hidden content: The import-by-query approach",2012,"Journal of Artificial Intelligence Research",19,10.1613/jair.3579,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875172158&doi=10.1613%2fjair.3579&partnerID=40&md5=7142559228bb3ca16830b9addefac84b","There is currently a growing interest in techniques for hiding parts of the signature of an ontology Kh that is being reused by another ontology Kv. Towards this goal, in this paper we propose the import-by-query framework, which makes the content of Kh accessible through a limited query interface. If Kv reuses the symbols from Kh in a certain restricted way, one can reason over Kv ≊Kh by accessing only Kv and the query interface. We map out the landscape of the import-by-query problem. In particular, we outline the limitations of our framework and prove that certain restrictions on the expressivity of K h and the way in which Kv reuses symbols from K h are strictly necessary to enable reasoning in our setting. We also identify cases in which reasoning is possible and we present suitable import-by-query reasoning algorithms. © 2012 AI Access Foundation.",,"Query interfaces; Reasoning algorithms; Artificial intelligence",Article,Scopus,2-s2.0-84875172158
"Savchynskyy B., Schmidt S., Kappes J., Schnörr C.","Efficient MRF energy minimization via adaptive diminishing smoothing",2012,"Uncertainty in Artificial Intelligence - Proceedings of the 28th Conference, UAI 2012",19,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886018775&partnerID=40&md5=63c63e36041adc5d411dffae5bae47b1","We consider the linear programming relaxation of an energy minimization problem for Markov Random Fields. The dual objective of this problem can be treated as a concave and unconstrained, but non-smooth function. The idea of smoothing the objective prior to optimization was recently proposed in a series of papers. Some of them suggested the idea to decrease the amount of smoothing (so called temperature) while getting closer to the optimum. However, no theoretical substantiation was provided. We propose an adaptive smoothing diminishing algorithm based on the duality gap between relaxed primal and dual objectives and demonstrate the efficiency of our approach with a smoothed version of Sequential Tree-Reweighted Message Passing (TRW-S) algorithm. The strategy is applicable to other algorithms as well, avoids adhoc tuning of the smoothing during iterations, and provably guarantees convergence to the optimum.",,"Adaptive smoothing; Duality gap; Energy minimization; Energy minimization problem; Linear programming relaxation; Markov Random Fields; Non-smooth functions; Artificial intelligence; Markov processes; Trees (mathematics); Algorithms",Conference Paper,Scopus,2-s2.0-84886018775
"Abdallah Z.S., Gaber M.M., Srinivasan B., Krishnaswamy S.","StreamAR: Incremental and active learning with evolving sensory data for activity recognition",2012,"Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI",19,10.1109/ICTAI.2012.169,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876860179&doi=10.1109%2fICTAI.2012.169&partnerID=40&md5=c827b98e4cc55eacc355c360897f1c62","Activity recognition focuses on inferring current user activities by leveraging sensory data available on todayÕs sensor rich environment. Supervised learning has been applied pervasively for activity recognition. Typical activity recognition techniques process sensory data based on point-by-point approaches. In this paper, we propose a novel cluster-based classification for activity recognition Systems, termed StreamAR. The system incorporates incremental and active learning for mining user activities in data streams. The novel approach processes activities as clusters to build a robust classification framework. StreamAR integrates supervised, unsupervised and active learning and applies hybrid similarity measures technique for recognising activities. Extensive experimental results using real activity recognition datasets have evidenced that our new approach shows improved performance over other existing state-of-the-art learning methods. © 2012 IEEE.","activity recognition; sensory data; stream mining","Active Learning; Activity recognition; Cluster-based classifications; Learning methods; Robust classification; Sensory data; Similarity measure; Stream mining; Artificial intelligence; Pattern recognition",Conference Paper,Scopus,2-s2.0-84876860179
"Gerling K.M., Schulte F.P., Smeddinck J., Masuch M.","Game design for older adults: Effects of age-related changes on structural elements of digital games",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",19,10.1007/978-3-642-33542-6_20,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875642770&doi=10.1007%2f978-3-642-33542-6_20&partnerID=40&md5=db09cce3d855336e4d2f3180de61fb54","Recent studies report various positive effects on elderly persons playing digital games. Yet, games are rarely designed with an elderly user group in mind. In this paper, this issue is addressed by providing an overview of common age-related changes followed by a summary of game design considerations for senior audiences. The impact of age on game design is discussed based on an analysis of the most important structural elements of games. The analysis shows that age-related changes in users' cognitive and physical abilities affect the use of games on multiple levels, making the complexity of games and interrelations between different game mechanics a crucial factor when designing for older adults. © 2012 Springer-Verlag Berlin Heidelberg.","accessibility; design recommendations; Game design; older adults","accessibility; Age-related changes; Design recommendations; Elderly persons; Game design; Multiple levels; Older adults; Structural elements; Artificial intelligence; Computer games",Conference Paper,Scopus,2-s2.0-84875642770
"Bouneffouf D., Bouzeghoub A., Gançarski A.L.","A contextual-bandit algorithm for mobile context-aware recommender system",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",19,10.1007/978-3-642-34487-9_40,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869007958&doi=10.1007%2f978-3-642-34487-9_40&partnerID=40&md5=c7ef6bd45a65dfefaf8b7dda7f8e7a54","Most existing approaches in Mobile Context-Aware Recommender Systems focus on recommending relevant items to users taking into account contextual information, such as time, location, or social aspects. However, none of them has considered the problem of user's content evolution. We introduce in this paper an algorithm that tackles this dynamicity. It is based on dynamic exploration/exploitation and can adaptively balance the two aspects by deciding which user's situation is most relevant for exploration or exploitation. Within a deliberately designed offline simulation framework we conduct evaluations with real online event log data. The experimental results demonstrate that our algorithm outperforms surveyed algorithms. © 2012 Springer-Verlag.","Artificial intelligence; Exploration/exploitation dilemma; Machine learning; Recommender system","Content evolution; Context-aware recommender systems; Contextual information; Exploration/exploitation dilemmas; Log data; Off-line simulations; ON dynamics; Algorithms; Artificial intelligence; Data processing; Learning systems; Social aspects; Recommender systems",Conference Paper,Scopus,2-s2.0-84869007958
"Gonçalves R.S., Parsia B., Sattler U.","Performance heterogeneity and approximate reasoning in description logic ontologies",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",19,10.1007/978-3-642-35176-1-6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868524487&doi=10.1007%2f978-3-642-35176-1-6&partnerID=40&md5=14bbac846bbd7e964922203b000e0681","Due to the high worst case complexity of the core reasoning problem for the expressive profiles of OWL 2, ontology engineers are often surprised and confused by the performance behaviour of reasoners on their ontologies. Even very experienced modellers with a sophisticated grasp of reasoning algorithms do not have a good mental model of reasoner performance behaviour. Seemingly innocuous changes to an OWL ontology can degrade classification time from instantaneous to too long to wait for. Similarly, switching reasoners (e.g., to take advantage of specific features) can result in wildly different classification times. In this paper we investigate performance variability phenomena in OWL ontologies, and present methods to identify subsets of an ontology which are performance-degrading for a given reasoner. When such (ideally small) subsets are removed from an ontology, and the remainder is much easier for the given reasoner to reason over, we designate them hot spots. The identification of these hot spots allows users to isolate difficult portions of the ontology in a principled and systematic way. Moreover, we devise and compare various methods for approximate reasoning and knowledge compilation based on hot spots. We verify our techniques with a select set of varyingly difficult ontologies from the NCBO BioPortal, and were able to, firstly, successfully identify performance hot spots against the major freely available DL reasoners, and, secondly, significantly improve classification time using approximate reasoning based on hot spots. © 2012 Springer-Verlag Berlin Heidelberg.",,"Approximate reasoning; Classification time; Description logic; Hot spot; Knowledge compilation; Mental model; OWL ontologies; Performance variability; Reasoner; Reasoning algorithms; Reasoning problems; Worst-case complexity; Artificial intelligence; Data description",Conference Paper,Scopus,2-s2.0-84868524487
"Lehmann J., Gerber D., Morsey M., Ngonga Ngomo A.-C.","DeFacto-Deep fact validation",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",19,10.1007/978-3-642-35176-1-20,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868550351&doi=10.1007%2f978-3-642-35176-1-20&partnerID=40&md5=5b11e7ce37cd982c37eef9ac0a0e6c28","One of the main tasks when creating and maintaining knowledge bases is to validate facts and provide sources for them in order to ensure correctness and traceability of the provided knowledge. So far, this task is often addressed by human curators in a three-step process: issuing appropriate keyword queries for the statement to check using standard search engines, retrieving potentially relevant documents and screening those documents for relevant content. The drawbacks of this process are manifold. Most importantly, it is very time-consuming as the experts have to carry out several search processes and must often read several documents. In this article, we present DeFacto (Deep Fact Validation)-an algorithm for validating facts by finding trustworthy sources for it on the Web. DeFacto aims to provide an effective way of validating facts by supplying the user with relevant excerpts of webpages as well as useful additional information including a score for the confidence DeFacto has in the correctness of the input fact. © 2012 Springer-Verlag Berlin Heidelberg.",,"Keyword queries; Knowledge basis; Main tasks; Relevant documents; Search process; Three-step process; Artificial intelligence; Search engines",Conference Paper,Scopus,2-s2.0-84868550351
"Brams S.J., Feldman M., Lai J.K., Morgenstern J., Procaccia A.D.","On maxsum fair cake divisions",2012,"Proceedings of the National Conference on Artificial Intelligence",19,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868279692&partnerID=40&md5=604ebb6064ef9f97c359a8286634a961","We consider the problem of selecting fair divisions of a heterogeneous divisible good among a set of agents. Recent work (Cohler et al., AAAI 2011) focused on designing algorithms for computing maxsum - social welfare maximizing - allocations under the fairness notion of envyfreeness. Maxsum allocations can also be found under alternative notions such as equitability. In this paper, we examine the properties of these allocations. In particular, we provide conditions for when maxsum envy-free or equitable allocations are Pareto optimal and give examples where fairness with Pareto optimality is not possible. We also prove that maxsum envy-free allocations have weakly greater welfare than maxsum equitable allocations when agents have structured valuations, and we derive an approximate version of this inequality for general valuations. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Pareto-optimal; Pareto-optimality; Social welfare; Optimization; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868279692
"Serrà J., Müller M., Grosche P., Arcos J.Ll.","Unsupervised detection of music boundaries by time series structure features",2012,"Proceedings of the National Conference on Artificial Intelligence",19,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868290210&partnerID=40&md5=86a44c9a94da5740a1c96fe81d3b0ae0","Locating boundaries between coherent and/or repetitive segments of a time series is a challenging problem pervading many scientific domains. In this paper we propose an unsupervised method for boundary detection, combining three basic principles: novelty, homogeneity, and repetition. In particular, the method uses what we call structure features, a representation encapsulating both local and global properties of a time series. We demonstrate the usefulness of our approach in detecting music structure boundaries, a task that has received much attention in recent years and for which exist several benchmark datasets and publicly available annotations. We find our method to significantly outperform the best accuracies published so far. Importantly, our boundary approach is generic, thus being applicable to a wide range of time series beyond the music and audio domains. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Basic principles; Benchmark datasets; Boundary detection; Global properties; Music structures; Structure features; Unsupervised detection; Unsupervised method; Artificial intelligence; Audio acoustics; Time series",Conference Paper,Scopus,2-s2.0-84868290210
"Navigli R., Ponzetto S.P.","BabelRelate! A joint multilingual approach to computing semantic relatedness",2012,"Proceedings of the National Conference on Artificial Intelligence",19,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868283843&partnerID=40&md5=f0b6093ca57cd1d78d5116970edfa91c","We present a knowledge-rich approach to computing semantic relatedness which exploits the joint contribution of different languages. Our approach is based on the lexicon and semantic knowledge of a wide-coverage multilingual knowledge base, which is used to compute semantic graphs in a variety of languages. Complementary information from these graphs is then combined to produce a 'core' graph where disambiguated translations are connected by means of strong semantic relations. We evaluate our approach on standard monolingual and bilingual datasets, and show that: i) we outperform a graph-based approach which does not use multilinguality in a joint way; ii) we achieve uniformly competitive results for both resource-rich and resource-poor languages. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Data sets; Graph-based; Knowledge base; Multilingual approach; Multilinguality; Resource-Rich; Semantic graphs; Semantic knowledge; Semantic relatedness; Semantic relations; Artificial intelligence; Graphic methods; Knowledge based systems; Semantics; Natural language processing systems",Conference Paper,Scopus,2-s2.0-84868283843
"Yin Z., Jiang A.X., Johnson M.P., Tambe M., Kiekintveld C., Leyton-Brown K., Sandholm T., Sullivan J.P.","TRUSTS: Scheduling randomized patrols for fare inspection in transit systems",2012,"Proceedings of the National Conference on Artificial Intelligence",19,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868295022&partnerID=40&md5=2371d32a938888e516600f578b1bfc71","In proof-of-payment transit systems, passengers are legally required to purchase tickets before entering but are not physically forced to do so. Instead, patrol units move about the transit system, inspecting the tickets of passengers, who face fines if caught fare evading. The deterrence of such fines depends on the unpredictability and effectiveness of the patrols. In this paper, we present TRUSTS, an application for scheduling randomized patrols for fare inspection in transit systems. TRUSTS models the problem of computing patrol strategies as a leader-follower Stackelberg game where the objective is to deter fare evasion and hence maximize revenue. This problem differs from previously studied Stackelberg settings in that the leader strategies must satisfy massive temporal and spatial constraints; moreover, unlike in these counterterrorism- motivated Stackelberg applications, a large fraction of the ridership might realistically consider fare evasion, and so the number of followers is potentially huge. A third key novelty in our work is deliberate simplification of leader strategies to make patrols easier to be executed. We present an efficient algorithm for computing such patrol strategies and present experimental results using real-world ridership data from the Los Angeles Metro Rail system. The Los Angeles County Sheriff's department has begun trials of TRUSTS. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Counter terrorism; Leader-follower; Los angeles; Metro rail system; Spatial constraints; Stackelberg; Stackelberg Games; Transit systems; Algorithms; Artificial intelligence; Inspection; Scheduling; Mass transportation",Conference Paper,Scopus,2-s2.0-84868295022
"An B., Kempe D., Kiekintveld C., Shieh E., Singh S., Tambe M., Vorobeychik Y.","Security games with limited surveillance",2012,"Proceedings of the National Conference on Artificial Intelligence",19,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868283798&partnerID=40&md5=3f478f91c2fe574ff8fd33d75c4986fd","Randomized first-mover strategies of Stackelberg games are used in several deployed applications to allocate limited resources for the protection of critical infrastructure. Stackelberg games model the fact that a strategic attacker can surveil and exploit the defender's strategy, and randomization guards against the worst effects by making the defender less predictable. In accordance with the standard game-theoretic model of Stackelberg games, past work has typically assumed that the attacker has perfect knowledge of the defender's randomized strategy and will react correspondingly. In light of the fact that surveillance is costly, risky, and delays an attack, this assumption is clearly simplistic: attackers will usually act on partial knowledge of the defender's strategies. The attacker's imperfect estimate could present opportunities and possibly also threats to a strategic defender. In this paper, we therefore begin a systematic study of security games with limited surveillance. We propose a natural model wherein an attacker forms or updates a belief based on observed actions, and chooses an optimal response. We investigate the model both theoretically and experimentally. In particular, we give mathematical programs to compute optimal attacker and defender strategies for a fixed observation duration, and show how to use them to estimate the attacker's observation durations. Our experimental results show that the defender can achieve significant improvement in expected utility by taking the attacker's limited surveillance into account, validating the motivation of our work. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Expected utility; Game-theoretic model; Imperfect estimate; Mathematical program; Natural models; Optimal response; Partial knowledge; Randomized strategy; Stackelberg Games; Systematic study; Artificial intelligence; Game theory; Monitoring",Conference Paper,Scopus,2-s2.0-84868283798
"MacAlpine P., Barrett S., Urieli D., Vu V., Stone P.","Design and optimization of an omnidirectional humanoid walk: A winning approach at the RoboCup 2011 3D simulation competition",2012,"Proceedings of the National Conference on Artificial Intelligence",19,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868297354&partnerID=40&md5=63d8eea783f70845c9ef4192dbe1d3d4","This paper presents the design and learning architecture for an omnidirectional walk used by a humanoid robot soccer agent acting in the RoboCup 3D simulation environment. The walk, which was originally designed for and tested on an actual Nao robot before being employed in the 2011 RoboCup 3D simulation competition, was the crucial component in the UT Austin Villa team winning the competition in 2011. To the best of our knowledge, this is the first time that robot behavior has been conceived and constructed on a real robot for the end purpose of being used in simulation. The walk is based on a double linear inverted pendulum model, and multiple sets of its parameters are optimized via a novel framework. The framework optimizes parameters for different tasks in conjunction with one another, a little-understood problem with substantial practical significance. Detailed experiments show that the UT Austin Villa agent significantly outperforms all the other agents in the competition with the optimized walk being the key to its success. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"3D simulations; Design and optimization; Humanoid robot; Inverted pendulum model; Learning architectures; Multiple set; Omnidirectional walk; Real robot; RoboCup; Robot behavior; Anthropomorphic robots; Artificial intelligence; Optimization; Tracking (position); Three dimensional computer graphics",Conference Paper,Scopus,2-s2.0-84868297354
"Samadi M., Kollar T., Veloso M.","Using the web to interactively learn to find objects",2012,"Proceedings of the National Conference on Artificial Intelligence",19,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868266394&partnerID=40&md5=22290953d69caedee452f86b8afcc4f3","In order for robots to intelligently perform tasks with humans, they must be able to access a broad set of background knowledge about the environments in which they operate. Unlike other approaches, which tend to manually define the knowledge of the robot, our approach enables robots to actively query the World Wide Web (WWW) to learn background knowledge about the physical environment. We show that our approach is able to search the Web to infer the probability that an object, such as a ""coffee,"" can be found in a location, such as a ""kitchen."" Our approach, called ObjectEval, is able to dynamically instantiate a utility function using this probability, enabling robots to find arbitrary objects in indoor environments. Our experimental results show that the interactive version of ObjectEval visits 28% fewer locations than the version trained offline and 71% fewer locations than a baseline approach which uses no background knowledge. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Arbitrary objects; Background knowledge; Indoor environment; Offline; Physical environments; Utility functions; Artificial intelligence; Robots; World Wide Web",Conference Paper,Scopus,2-s2.0-84868266394
"Yang R., Wang L.","Optimal control strategy for HVAC system in building energy management",2012,"Proceedings of the IEEE Power Engineering Society Transmission and Distribution Conference",19,10.1109/TDC.2012.6281687,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867921316&doi=10.1109%2fTDC.2012.6281687&partnerID=40&md5=985a0f38baa9f6a678caca7ac35dbf97","Heating, ventilating and air conditioning (HVAC) systems have played an important role in building energy and comfort management. It is designed to provide a relatively constant and comfortable temperature in buildings and provide fresh and filtered air with a comfortable humidity level. In this paper, an optimal control strategy is proposed to control the HVAC system for maintaining building's indoor environment with high energy efficiency. The control strategy utilized swarm intelligence to determine the amount of energy dispatched to each equipment in the HVAC system. In order to study the impact of HVAC system operations in the indoor environment, both the building model and HVAC equipment models are developed. A case study is carried out to simulate the real time control process in a specified building environment. © 2012 IEEE.","Building energy management; Energy-efficient buildings; HVAC system; Multi-objective problems; Particle swarm optimization","Building energy; Building environment; Building model; Comfort management; Comfortable temperature; Control strategies; Energy-efficient buildings; High energy efficiency; Humidity levels; HVAC equipment; HVAC system; In-buildings; Indoor environment; Multi-objective problem; Optimal control strategy; Real time; Swarm Intelligence; Artificial intelligence; Buildings; Energy efficiency; Energy management; Exhibitions; Optimal control systems; Particle swarm optimization (PSO); Real time control; Air conditioning",Conference Paper,Scopus,2-s2.0-84867921316
"Minutolo A., Esposito M., De Pietro G.","A pattern-based knowledge editing system for building clinical Decision Support Systems",2012,"Knowledge-Based Systems",19,10.1016/j.knosys.2012.04.024,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866492936&doi=10.1016%2fj.knosys.2012.04.024&partnerID=40&md5=494029834878335d71a024e132230ace","Decision support in medicine is being more and more configured as an innovative and valuable way for promoting more consistent, effective, and efficient medical practices. The broad acceptance and efficient application of Decision Support Systems to medical settings strongly require some mechanisms to conveniently update and handle these systems with respect to medical progress or adaptation in the treatment of individual diseases. In this respect, this paper proposes a pattern-based knowledge editing system to guide and assist the creation and formalization of condition-action clinical recommendations to be used in knowledge-based Decision Support Systems (in the following, DSSs). This system has been devised with the aim of: (i) offering a set of patterns for easily inserting and editing such clinical recommendations; (ii) synergistically combining multiple knowledge representation techniques to instantiate these patterns within hybrid knowledge bases (KBs), made of if-then rules built on the top of ontological vocabularies; (iii) reducing the complexity of the formalization process, by graphically guiding the definition of hybrid KBs that could be functional in the context of clinical DSSs and enabling their automatic encoding into machine executable languages. A usability evaluation has been carried out, showing a good satisfaction of medical users with respect to the system implemented, and, thus, proving both the feasibility and usefulness of the approach proposed. © 2012 Elsevier B.V. All rights reserved.","Clinical decision support system; Clinical guidelines; Knowledge base editor; Ontology; Rules","Clinical decision support systems; Clinical guideline; Decision support in medicine; Editing systems; If-then rules; Knowledge base; Knowledge based decision support systems; Knowledge basis; Medical practice; Medical settings; Representation techniques; Rules; Usability evaluation; Decision support systems; Knowledge based systems; Knowledge representation; Ontology; Artificial intelligence",Article,Scopus,2-s2.0-84866492936
"Jøsang A., Pope S.","Dempster's rule as seen by little colored balls",2012,"Computational Intelligence",19,10.1111/j.1467-8640.2012.00421.x,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868142548&doi=10.1111%2fj.1467-8640.2012.00421.x&partnerID=40&md5=79cf4ff125973eb5271dd29e361f4bdf","Dempster's rule is traditionally interpreted as an operator for fusing belief functions. While there are different types of belief fusion, there has been considerable confusion regarding the exact type of operation that Dempster's rule performs. Many alternative operators for belief fusion have been proposed, where some are based on the same fundamental principle as Dempster's rule, and others have a totally different basis, such as the cumulative and averaging fusion operators. In this article, we analyze Dempster's rule from a statistical and frequentist perspective and compare it with cumulative and averaging belief fusion. We prove, and illustrate by examples on colored balls, that Dempster's rule in fact represents a method for serial combination of stochastic constraints. Consequently, Dempster's rule is not a method for cumulative fusion of belief functions under the assumption that subjective beliefs are an extension of frequentist beliefs. Having identified the true nature of Dempster's rule, appropriate applications of Dempster's rule of combination are described such as the multiplication of orthogonal belief functions, and the combination of preferences dictated by different parties. © 2012 Wiley Periodicals, Inc.","averaging operator; belief theory; cumulative operator; fusion; subjective logic","Averaging operators; Belief function; Belief fusion; Belief theory; Colored balls; cumulative operator; Dempster's rule; Dempster's rule of combination; Frequentist; Fundamental principles; Fusion operator; Stochastic constraints; Subjective Logic; Artificial intelligence; Computational methods; Fusion reactions; Uncertainty analysis",Article,Scopus,2-s2.0-84868142548
"Sheikhan M., Mohammadi N.","Neural-based electricity load forecasting using hybrid of GA and ACO for feature selection",2012,"Neural Computing and Applications",19,10.1007/s00521-011-0599-1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867693003&doi=10.1007%2fs00521-011-0599-1&partnerID=40&md5=9415b661d7dada1f18b76888a458f083","Due to deregulation of electricity industry, accurate load forecasting and predicting the future electricity demand play an important role in the regional and national power system strategy management. Electricity load forecasting is a challenging task because electric load has complex and nonlinear relationships with several factors. In this paper, two hybrid models are developed for short-term load forecasting (STLF). These models use ""ant colony optimization (ACO)"" and ""combination of genetic algorithm (GA) and ACO (GA-ACO)"" for feature selection and multi-layer perceptron (MLP) for hourly load prediction. Weather and climatic conditions, month, season, day of the week, and time of the day are considered as load-influencing factors in this study. Using load time-series of a regional power system, the performance of ACO + MLP and GA-ACO + MLP hybrid models is compared with principal component analysis (PCA) + MLP hybrid model and also with the case of no-feature selection (NFS) when using MLP and radial basis function (RBF) neural models. Experimental results and the performance comparison with similar recent researches in this field show that the proposed GA-ACO + MLP hybrid model performs better in load prediction of 24-h ahead in terms of mean absolute percentage error (MAPE). © 2011 Springer-Verlag London Limited.","Ant colony optimization; Feature selection; Genetic algorithm; Neural network; Short-term load forecasting","Ant Colony Optimization (ACO); Climatic conditions; Electricity demands; Electricity industry; Electricity load; Hourly load; Hybrid model; Load forecasting; Load predictions; Mean absolute percentage error; Multi layer perceptron; National power systems; Neural models; Non-linear relationships; Performance comparison; Radial basis functions; Regional power systems; Short term load forecasting; Strategy management; Electric load forecasting; Electricity; Feature extraction; Forecasting; Genetic algorithms; Neural networks; Principal component analysis; Radial basis function networks; Artificial intelligence",Article,Scopus,2-s2.0-84867693003
"Van Der Veen V., Dutt-Sharma N., Cavallaro L., Bos H.","Memory errors: The past, the present, and the future",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",19,10.1007/978-3-642-33338-5_5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867874938&doi=10.1007%2f978-3-642-33338-5_5&partnerID=40&md5=047debd0b4521d50108b4d51bb6d196e","Memory error exploitations have been around for over 25 years and still rank among the top 3 most dangerous software errors. Why haven't we been able to stop them? Given the host of security measures on modern machines, are we less vulnerable than before, and can we expect to eradicate memory error problems in the near future? In this paper, we present a quarter century worth of memory errors: attacks, defenses, and statistics. A historical overview provides insights in past trends and developments, while an investigation of real-world vulnerabilities and exploits allows us to answer on the significance of memory errors in the foreseeable future. © 2012 Springer-Verlag.",,"Memory error; Modern machines; Security measure; Software errors; Artificial intelligence; Errors",Conference Paper,Scopus,2-s2.0-84867874938
"Kuusinen K., Mikkonen T., Pakarinen S.","Agile user experience development in a large software organization: Good expertise but limited impact",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",19,10.1007/978-3-642-34347-6_6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867865041&doi=10.1007%2f978-3-642-34347-6_6&partnerID=40&md5=c142d5b08dc8083845f87cf48be66356","While Agile methods were originally introduced for small, tightly coupled teams, leaner ways of working are becoming a practical method to run entire enterprises. As the emphasis of user experience work has inherently been on the early phases before starting the development, it also needs to be adapted to the Agile way of working. To improve the current practices in Agile user experience work, we determined the present state of a multi-continental software development organization that already had a functioning user experience team. In this paper, we describe the most prevalent issues regarding the interaction of user experience design and software development activities, and suggest improvements to fix those. Most of the observed problems were related to communication issues and to the service mode of the user experience team. The user experience team was operating between management and development organizations trying to adapt to the dissimilar practices of both the disciplines. © 2012 Springer-Verlag.","Agile development; human-centered design (HCD); human-computer interaction (HCI); User experience (UX)","Agile development; Agile methods; Development activity; Human-centered designs; Practical method; Service mode; Software development organizations; Software organization; Tightly-coupled; User experience; User experience design; Agile development; Development activity; Human computer interaction (HCI); Human-centered designs; Software development organizations; Software organization; User experience design; User experiences (ux); Artificial intelligence; Human computer interaction; Human resource management; Software engineering; Software design; Software design",Conference Paper,Scopus,2-s2.0-84867865041
"Yang B., Nevatia R.","Online learned discriminative part-based appearance models for multi-human tracking",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",19,10.1007/978-3-642-33718-5_35,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867885237&doi=10.1007%2f978-3-642-33718-5_35&partnerID=40&md5=9e3986315cb67b43121fff0953dcf71c","We introduce an online learning approach to produce discriminative part-based appearance models (DPAMs) for tracking multiple humans in real scenes by incorporating association based and category free tracking methods. Detection responses are gradually associated into tracklets in multiple levels to produce final tracks. Unlike most previous multi-target tracking approaches which do not explicitly consider occlusions in appearance modeling, we introduce a part based model that explicitly finds unoccluded parts by occlusion reasoning in each frame, so that occluded parts are removed in appearance modeling. Then DPAMs for each tracklet is online learned to distinguish a tracklet with others as well as the background, and is further used in a conservative category free tracking approach to partially overcome the missed detection problem as well as to reduce difficulties in tracklet associations under long gaps. We evaluate our approach on three public data sets, and show significant improvements compared with state-of-art methods. © 2012 Springer-Verlag.","multi-human tracking; online learned discriminative models","Appearance modeling; Appearance models; Discriminative models; Missed detection problem; Multi-target tracking; Multiple levels; Occlusion reasoning; Online learning; Public data; State-of-art methods; Tracking method; Artificial intelligence; Computer vision",Conference Paper,Scopus,2-s2.0-84867885237
"Carpineto C., Romano G.","Consensus clustering based on a new probabilistic rand index with application to subtopic retrieval",2012,"IEEE Transactions on Pattern Analysis and Machine Intelligence",19,10.1109/TPAMI.2012.80,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867823027&doi=10.1109%2fTPAMI.2012.80&partnerID=40&md5=51469f71f3b8fd7344bfe65d93b7f17b","We introduce a probabilistic version of the well-known Rand Index (RI) for measuring the similarity between two partitions, called Probabilistic Rand Index (PRI), in which agreements and disagreements at the object-pair level are weighted according to the probability of their occurring by chance. We then cast consensus clustering as an optimization problem of the PRI value between a target partition and a set of given partitions, experimenting with a simple and very efficient stochastic optimization algorithm. Remarkable performance gains over input partitions as well as over existing related methods are demonstrated through a range of applications, including a new use of consensus clustering to improve subtopic retrieval. © 2012 IEEE.","Consensus clustering; Probabilistic Rand index; Rand index; Search results clustering; Subtopic retrieval","Consensus clustering; Optimization problems; Performance Gain; Rand index; Search results clustering; Stochastic optimization algorithm; Subtopic retrieval; Artificial intelligence; Computer vision; Algorithms",Article,Scopus,2-s2.0-84867823027
"Kayala M.A., Baldi P.","ReactionPredictor: Prediction of complex chemical reactions at the mechanistic level using machine learning",2012,"Journal of Chemical Information and Modeling",19,10.1021/ci3003039,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867780136&doi=10.1021%2fci3003039&partnerID=40&md5=e1f0713a16278692a8e0161655aef36c","Proposing reasonable mechanisms and predicting the course of chemical reactions is important to the practice of organic chemistry. Approaches to reaction prediction have historically used obfuscating representations and manually encoded patterns or rules. Here we present ReactionPredictor, a machine learning approach to reaction prediction that models elementary, mechanistic reactions as interactions between approximate molecular orbitals (MOs). A training data set of productive reactions known to occur at reasonable rates and yields and verified by inclusion in the literature or textbooks is derived from an existing rule-based system and expanded upon with manual curation from graduate level textbooks. Using this training data set of complex polar, hypervalent, radical, and pericyclic reactions, a two-stage machine learning prediction framework is trained and validated. In the first stage, filtering models trained at the level of individual MOs are used to reduce the space of possible reactions to consider. In the second stage, ranking models over the filtered space of possible reactions are used to order the reactions such that the productive reactions are the top ranked. The resulting model, ReactionPredictor, perfectly ranks polar reactions 78.1% of the time and recovers all productive reactions 95.7% of the time when allowing for small numbers of errors. Pericyclic and radical reactions are perfectly ranked 85.8% and 77.0% of the time, respectively, rising to >93% recovery for both reaction types with a small number of allowed errors. Decisions about which of the polar, pericyclic, or radical reaction type ranking models to use can be made with >99% accuracy. Finally, for multistep reaction pathways, we implement the first mechanistic pathway predictor using constrained tree-search to discover a set of reasonable mechanistic steps from given reactants to given products. Webserver implementations of both the single step and pathway versions of ReactionPredictor are available via the chemoinformatics portal http://cdb.ics.uci.edu/. © 2012 American Chemical Society.",,"Chemoinformatics; Complex chemical reactions; Curation; Filtering models; Graduate levels; Learning approach; Mechanistic pathways; Multistep reactions; Organic Chemistry; Pericyclic reaction; Polar reactions; Radical reactions; Ranking model; Reaction prediction; Single-step; Training data sets; Tree-search; Web servers; Errors; Forecasting; Molecular orbitals; Reaction kinetics; Textbooks; Learning systems; free radical; algorithm; article; artificial intelligence; chemical model; chemical phenomena; chemical reaction; chemistry; computer simulation; drug design; information science; Internet; Algorithms; Artificial Intelligence; Chemistry, Pharmaceutical; Computer Simulation; Drug Design; Free Radicals; Hydrophobic and Hydrophilic Interactions; Informatics; Internet; Models, Chemical; Organic Chemistry Processes",Article,Scopus,2-s2.0-84867780136
"Claude F., Navarro G.","Improved grammar-based compressed indexes",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",19,10.1007/978-3-642-34109-0-19,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867507372&doi=10.1007%2f978-3-642-34109-0-19&partnerID=40&md5=1d38b01a13fb2f4fac12ac6a495f7ed5","We introduce the first grammar-compressed representation of a sequence that supports searches in time that depends only logarithmically on the size of the grammar. Given a text T[1.u] that is represented by a (context-free) grammar of n (terminal and nonterminal) symbols and size N (measured as the sum of the lengths of the right hands of the rules), a basic grammar-based representation of T takes bits of space. Our representation requires bits of space, for any 0∈<∈ε∈≤∈1. It can find the positions of the occ occurrences of a pattern of length m in T in time, and extract any substring of length of T in time, where h is the height of the grammar tree. © 2012 Springer-Verlag Berlin Heidelberg.",,"Context-free; Substring; Artificial intelligence; Information retrieval",Conference Paper,Scopus,2-s2.0-84867507372
"Chaowanawatee K., Heednacram A.","Implementation of cuckoo search in RBF neural network for flood forecasting",2012,"Proceedings - 2012 4th International Conference on Computational Intelligence, Communication Systems and Networks, CICSyN 2012",19,10.1109/CICSyN.2012.15,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867356101&doi=10.1109%2fCICSyN.2012.15&partnerID=40&md5=b3129172cd3aec7a7ef635edc04facfc","The flood forecasting is the key to support the right decision making. A method to forecast flood accurately and timely are important. We propose a method based on Radial Basis Function (RBF) neural network which has the important application in flood water level forecasting. The traditional way of training of the neural network may drive the network to converge in local minima instead of global minimum. We introduce a cuckoo search algorithm to train parameters of neural network instead of a normal way. We implement our proposed algorithm where the input is the real data from Little Wabash River. In the experimental part, we choose the type of Radial Basis Function to be Gaussian and Polyharmonic. We investigate the impact of these two RBF functions and discuss the error between the forecast and the actual values. We conclude that Polyharmonic function suits to this problem better than Gaussian function. © 2012 IEEE.","computational intelligence; cuckoo search; flood forecasting; neural network","Cuckoo searches; Flood forecasting; Flood waters; Gaussian functions; Gaussians; Global minima; Local minimums; Polyharmonic functions; Radial basis function neural networks; Radial basis functions; RBF Neural Network; Algorithms; Artificial intelligence; Communication systems; Flood control; Floods; Forecasting; Radial basis function networks; Water levels; Neural networks",Conference Paper,Scopus,2-s2.0-84867356101
"Massanet S., Torrens J.","Threshold generation method of construction of a new implication from two given ones",2012,"Fuzzy Sets and Systems",19,10.1016/j.fss.2012.01.013,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864255889&doi=10.1016%2fj.fss.2012.01.013&partnerID=40&md5=c6624cc60800f4abc17e7eac343f8876","In this paper, a new construction method of a fuzzy implication from two given ones, called threshold generation method, is introduced. It is a generalization of the way of construction of the recently introduced h-implications, which are fully characterized in this work. The threshold generation method allows to control, up to a certain level, the increasingness on the second variable of the fuzzy implication through an adequate scaling on that variable of the two given implications. The natural propagation of the most usual properties of fuzzy implications from the initial ones to the constructed implication is studied and the necessary and sufficient conditions in order to ensure this propagation are presented. In particular, the preservation of the contrapositive symmetry on threshold generated implications needs of another construction method of a fuzzy implication from a given one and a fuzzy negation. © 2012 Elsevier B.V.","Contrapositive symmetry; Fuzzy connectives and aggregation operators; Fuzzy implication; h-implication; Left neutrality principle; Ordering property; Threshold generation method","Fuzzy connectives and aggregation operators; Fuzzy implications; Generation method; h-implication; Left neutrality principle; Ordering property; Artificial intelligence; Fuzzy sets; Fuzzy logic",Article,Scopus,2-s2.0-84864255889
"Wu S., Feng D., Wu W., Guo J., Dong L., Zou J.","(Pseudo) preimage attack on round-reduced Grøstl hash function and others",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",19,10.1007/978-3-642-34047-5_8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866702973&doi=10.1007%2f978-3-642-34047-5_8&partnerID=40&md5=f49b9ae8f2cc2980b68ac8fe35a1988f","The Grøstl hash function is one of the 5 final round candidates of the SHA-3 competition hosted by NIST. In this paper, we study the preimage resistance of the Grøstl hash function. We propose pseudo preimage attacks on Grøstl hash function for both 256-bit and 512-bit versions, i.e., we need to choose the initial value in order to invert the hash function. Pseudo preimage attack on 5(out of 10)-round Grøstl-256 has a complexity of (2 244.85,2 230.13) (in time and memory) and pseudo preimage attack on 8(out of 14)-round Grøstl-512 has a complexity of (2 507.32,2 507.00). To the best of our knowledge, our attacks are the first (pseudo) preimage attacks on round-reduced Grøstl hash function, including its compression function and output transformation. These results are obtained by a variant of meet-in-the-middle preimage attack framework by Aoki and Sasaki. We also improve the time complexities of the preimage attacks against 5-round Whirlpool and 7-round AES hashes by Sasaki in FSE 2011. © 2012 Springer-Verlag.","AES; Grøstl; hash function; meet-in-the-middle; preimage attack; Whirlpool","AES; Compression functions; Initial values; Meet-in-the-middle; Preimage attack; Preimages; Sha-3 competitions; Time complexity; Whirlpool; Artificial intelligence; Hash functions",Conference Paper,Scopus,2-s2.0-84866702973
"Cyriac A., Gastin P., Kumar K.N.","MSO decidability of multi-pushdown systems via split-width",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",19,10.1007/978-3-642-32940-1_38,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866714529&doi=10.1007%2f978-3-642-32940-1_38&partnerID=40&md5=b838a97f3c0f6a88cfe8779946eb6c61","Multi-threaded programs with recursion are naturally modeled as multi-pushdown systems. The behaviors are represented as multiply nested words (MNWs), which are words enriched with additional binary relations for each stack matching a push operation with the corresponding pop operation. Any MNW can be decomposed by two basic and natural operations: shuffle of two sequences of factors and merge of consecutive factors of a sequence. We say that the split-width of a MNW is k if it admits a decomposition where the number of factors in each sequence is at most k. The MSO theory of MNWs with split-width k is decidable. We introduce two very general classes of MNWs that strictly generalize known decidable classes and prove their MSO decidability via their split-width and obtain comparable or better bounds of tree-width of known classes. © 2012 Springer-Verlag.",,"Binary relation; General class; Multi-threaded programs; Natural operation; Push operation; Recursions; Tree-width; Artificial intelligence; Computability and decidability",Conference Paper,Scopus,2-s2.0-84866714529
"Gisbrecht A., Mokbel B., Schleif F.-M., Zhu X., Hammer B.","Linear time relational prototype based learning",2012,"International Journal of Neural Systems",19,10.1142/S0129065712500219,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867093070&doi=10.1142%2fS0129065712500219&partnerID=40&md5=045d464b9e466709d7973d4f7076bcef","Prototype based learning offers an intuitive interface to inspect large quantities of electronic data in supervised or unsupervised settings. Recently, many techniques have been extended to data described by general dissimilarities rather than Euclidean vectors, so-called relational data settings. Unlike the Euclidean counterparts, the techniques have quadratic time complexity due to the underlying quadratic dissimilarity matrix. Thus, they are infeasible already for medium sized data sets. The contribution of this article is twofold: On the one hand we propose a novel supervised prototype based classification technique for dissimilarity data based on popular learning vector quantization (LVQ), on the other hand we transfer a linear time approximation technique, the Nyström approximation, to this algorithm and an unsupervised counterpart, the relational generative topographic mapping (GTM). This way, linear time and space methods result. We evaluate the techniques on three examples from the biomedical domain. © 2012 World Scientific Publishing Company.","Dissimilarity data; Nystroem approximation; Supervised learning","Biomedical domain; Classification technique; Data sets; Dissimilarity data; Dissimilarity matrix; Electronic data; Euclidean; Generative topographic mapping; Intuitive interfaces; Learning Vector Quantization; Linear time; Linear-time approximation; Nystroem approximation; Quadratic time; Relational data; Supervised learning; Vector quantization; Approximation algorithms; algorithm; article; artificial intelligence; biology; chromosome; cluster analysis; computer program; genetic database; genetics; human; normal distribution; statistical analysis; statistical model; support vector machine; time; Vibrio; Algorithms; Artificial Intelligence; Chromosomes; Cluster Analysis; Computational Biology; Data Interpretation, Statistical; Databases, Genetic; Humans; Linear Models; Models, Statistical; Normal Distribution; Software; Support Vector Machines; Time Factors; Vibrio",Conference Paper,Scopus,2-s2.0-84867093070
"Uǧuz H.","Adaptive neuro-fuzzy inference system for diagnosis of the heart valve diseases using wavelet transform with entropy",2012,"Neural Computing and Applications",19,10.1007/s00521-011-0610-x,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866441990&doi=10.1007%2fs00521-011-0610-x&partnerID=40&md5=b474f11e2b25f3b275355e305671633e","Listening via stethoscope is a preferential method, being used by physicians for distinguishing normal and abnormal cardiac systems. On the other hand, listening with stethoscope has a number of constraints. The interpretation of various heart sounds depends on physician's ability of hearing, experience, and skill. Such limitations may be reduced by developing biomedical-based decision support systems. In this study, a biomedical-based decision support system was developed for the classification of heart sound signals, obtained from 120 subjects with normal, pulmonary, and mitral stenosis heart valve diseases via stethoscope. Developed system comprises of three stages. In the first stage, for feature extraction, obtained heart sound signals were separated to its sub-bands using discrete wavelet transform (DWT). In the second stage, entropy of each sub-band was calculated using Shannon entropy algorithm to reduce the dimensionality of the feature vectors via DWT. In the third stage, the reduced features of three types of heart sound signals were used as input patterns of the adaptive neuro-fuzzy inference system (ANFIS) classifiers. Developed method reached 98. 33% classification accuracy, and it was showed that purposed method is effective for detection of heart valve diseases. © 2011 Springer-Verlag London Limited.","Adaptive neuro-fuzzy inference system; Discrete wavelet transform; Entropy; Heart sound","Adaptive neuro-fuzzy inference system; Classification accuracy; Classification of heart sounds; Feature vectors; Heart sound signal; Heart sounds; Heart valve disease; Input patterns; Mitral stenosis; Shannon entropy; Subbands; Artificial intelligence; Audition; Cardiology; Decision support systems; Discrete wavelet transforms; Entropy; Feature extraction; Fuzzy systems; Heart",Article,Scopus,2-s2.0-84866441990
"Kötzing T., Molter H.","ACO beats EA on a dynamic pseudo-boolean function",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",19,10.1007/978-3-642-32937-1_12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866356355&doi=10.1007%2f978-3-642-32937-1_12&partnerID=40&md5=5a225b0aeaa32d263baa3444b71c3f22","In this paper, we contribute to the understanding of the behavior of bio-inspired algorithms when tracking the optimum of a dynamically changing fitness function over time. In particular, we are interested in the difference between a simple evolutionary algorithm (EA) and a simple ant colony optimization (ACO) system on deterministically changing fitness functions, which we call dynamic fitness patterns. Of course, the algorithms have no prior knowledge about the patterns. We construct a bit string optimization problem where we can show that the ACO system is able to follow the optimum while the EA gets lost. © 2012 Springer-Verlag.",,"Ant Colony Optimization (ACO); Bio-inspired algorithms; Bit-strings; Fitness functions; Optimization problems; Prior knowledge; Pseudo-Boolean function; Artificial intelligence; Boolean functions; Health; Algorithms",Conference Paper,Scopus,2-s2.0-84866356355
"Moyano F., Fernandez-Gago C., Lopez J.","A conceptual framework for trust models",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",19,10.1007/978-3-642-32287-7_8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866037455&doi=10.1007%2f978-3-642-32287-7_8&partnerID=40&md5=ecfd87982a09e4a6f40c85537d2bc7df","During the last twenty years, a huge amount of trust and reputation models have been proposed, each of them with their own particularities and targeting different domains. While much effort has been made in defining ever-increasing complex models, little attention has been paid to abstract away the particularities of these models into a common set of easily understandable concepts. We propose a conceptual framework for computational trust models that will be used for analyzing their features and for comparing heterogeneous and relevant trust models. © 2012 Springer-Verlag.",,"Complex model; Computational trust model; Conceptual frameworks; Different domains; Trust and reputation; Trust models; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84866037455
"Power R.","OWL simplified English: A finite-state language for ontology editing",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",19,10.1007/978-3-642-32612-7_4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865681733&doi=10.1007%2f978-3-642-32612-7_4&partnerID=40&md5=cb89ef9daeac82373f05514ccbe833da","We describe a controlled fragment of English for editing ontologies in OWL. Although this language substantially overlaps other CNLs that have been proposed for this purpose, it has a number of special features designed to simplify its learning and use. First, the language allows users to start typing in sentences with little or no preliminary effort in building a controlled vocabulary or lexicon. Second, it disallows sentences that people interpret as structurally ambiguous. Third, it employs a finite-state grammar, so facilitating fast and reliable implementation of an editing tool. These advantages are gained at the cost of severe restrictions in coverage, which mean that the majority of potential OWL axioms cannot be expressed. However, analysis of axiom patterns from several ontology repositories suggests that these constraints are almost invariably respected by ontology developers, so that in practice the loss of expressivity is rarely noticeable. © 2012 Springer-Verlag.","controlled natural language; ontology editing; semantic web","Controlled natural language; Editing tools; Finite-state; In-buildings; Semantic Web; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84865681733
"Dodis Y., Ristenpart T., Steinberger J., Tessaro S.","To hash or not to hash again? (In)differentiability results for H 2 and HMAC",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",19,10.1007/978-3-642-32009-5_21,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865526160&doi=10.1007%2f978-3-642-32009-5_21&partnerID=40&md5=0791371751fa4b93c7a5e7c7e89a4928","We show that the second iterate H 2(M) = H(H(M)) of a random oracle H cannot achieve strong security in the sense of indifferentiability from a random oracle. We do so by proving that indifferentiability for H 2 holds only with poor concrete security by providing a lower bound (via an attack) and a matching upper bound (via a proof requiring new techniques) on the complexity of any successful simulator. We then investigate HMAC when it is used as a general-purpose hash function with arbitrary keys (and not as a MAC or PRF with uniform, secret keys). We uncover that HMAC's handling of keys gives rise to two types of weak key pairs. The first allows trivial attacks against its indifferentiability; the second gives rise to structural issues similar to that which ruled out strong indifferentiability bounds in the case of H 2. However, such weak key pairs do not arise, as far as we know, in any deployed applications of HMAC. For example, using keys of any fixed length shorter than d - 1, where d is the block length in bits of the underlying hash function, completely avoids weak key pairs. We therefore conclude with a positive result: a proof that HMAC is indifferentiable from a RO (with standard, good bounds) when applications use keys of a fixed length less than d - 1. © 2012 International Association for Cryptologic Research.","Hash functions; HMAC; Indifferentiability","Block lengths; Differentiability; HMAC; Indifferentiability; Lower bounds; Random Oracle; Secret key; Structural issues; Upper Bound; Weak key; Artificial intelligence; Hash functions",Conference Paper,Scopus,2-s2.0-84865526160
"Kshirsagar M., Carbonell J., Klein-Seetharaman J.","Techniques to cope with missing data in host-pathogen protein interaction prediction",2012,"Bioinformatics",19,10.1093/bioinformatics/bts375,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866452395&doi=10.1093%2fbioinformatics%2fbts375&partnerID=40&md5=c5c234eb74f6bd9ace992ef766768a7d","Motivation: Approaches that use supervised machine learning techniques for protein-protein interaction (PPI) prediction typically use features obtained by integrating several sources of data. Often certain attributes of the data are not available, resulting in missing values. In particular, our host-pathogen PPI datasets have a large fraction, in the range of 58-85% of missing values, which makes it challenging to apply machine learning algorithms. Results: We show that specialized techniques for missing value imputation can improve the performance of the models significantly. We use cross species information in combination with machine learning techniques like Group lasso with l1/l2 regularization. We demonstrate the benefits of our approach on two PPI prediction problems. In our first example of Salmonella-human PPI prediction, we are able to obtain high prediction accuracies with 77.6% precision and 84% recall. Comparison with various other techniques shows an improvement of 9 in F1 score over the next best technique. We also apply our method to Yersinia-human PPI prediction successfully, demonstrating the generality of our approach. © The Author(s) 2012. Published by Oxford University Press.",,"Salmonella; Yersinia; bacterial protein; algorithm; article; artificial intelligence; chemistry; gene expression; genetics; host pathogen interaction; human; metabolism; methodology; protein analysis; Salmonella; sequence analysis; Yersinia pestis; Algorithms; Artificial Intelligence; Bacterial Proteins; Gene Expression; Host-Pathogen Interactions; Humans; Protein Interaction Mapping; Salmonella; Sequence Analysis, Protein; Yersinia pestis",Article,Scopus,2-s2.0-84866452395
"Verbiest N., Cornelis C., Victor P., Herrera-Viedma E.","Trust and distrust aggregation enhanced with path length incorporation",2012,"Fuzzy Sets and Systems",19,10.1016/j.fss.2012.02.007,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862176454&doi=10.1016%2fj.fss.2012.02.007&partnerID=40&md5=56f36a16d81e70a5016f57314ae0ea07","Trust networks are social networks in which users can assign trust scores to each other. In order to estimate these scores for agents that are indirectly connected through the network, a range of trust score aggregators has been proposed. Currently, none of them takes into account the length of the paths that connect users; however, this appears to be a critical factor since longer paths generally contain less reliable information. In this paper, we introduce and evaluate several path length incorporating aggregation strategies in order to strike the right balance between generating more predictions on the one hand and maintaining a high prediction accuracy on the other hand. © 2012 Elsevier B.V. All rights reserved.","Aggregation operators; Path length; Trust networks","Aggregation operator; Aggregation strategy; Critical factors; Path length; Prediction accuracy; Social Networks; Trust networks; Artificial intelligence; Fuzzy sets",Article,Scopus,2-s2.0-84862176454
"Wall D.P., Dally R., Luyster R., Jung J.-Y., DeLuca T.F.","Use of artificial intelligence to shorten the behavioral diagnosis of autism",2012,"PLoS ONE",19,10.1371/journal.pone.0043855,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865428954&doi=10.1371%2fjournal.pone.0043855&partnerID=40&md5=e2dcad2797b77e7cf1c98035e7e594e7","The Autism Diagnostic Interview-Revised (ADI-R) is one of the most commonly used instruments for assisting in the behavioral diagnosis of autism. The exam consists of 93 questions that must be answered by a care provider within a focused session that often spans 2.5 hours. We used machine learning techniques to study the complete sets of answers to the ADI-R available at the Autism Genetic Research Exchange (AGRE) for 891 individuals diagnosed with autism and 75 individuals who did not meet the criteria for an autism diagnosis. Our analysis showed that 7 of the 93 items contained in the ADI-R were sufficient to classify autism with 99.9% statistical accuracy. We further tested the accuracy of this 7-question classifier against complete sets of answers from two independent sources, a collection of 1654 individuals with autism from the Simons Foundation and a collection of 322 individuals with autism from the Boston Autism Consortium. In both cases, our classifier performed with nearly 100% statistical accuracy, properly categorizing all but one of the individuals from these two resources who previously had been diagnosed with autism through the standard ADI-R. Our ability to measure specificity was limited by the small numbers of non-spectrum cases in the research data used, however, both real and simulated data demonstrated a range in specificity from 99% to 93.8%. With incidence rates rising, the capacity to diagnose autism quickly and effectively requires careful design of behavioral assessment methods. Ours is an initial attempt to retrospectively analyze large data repositories to derive an accurate, but significantly abbreviated approach that may be used for rapid detection and clinical prioritization of individuals likely to have an autism spectrum disorder. Such a tool could assist in streamlining the clinical diagnostic process overall, leading to faster screening and earlier treatment of individuals with autism. © 2012 Wall et al.",,"article; artificial intelligence; autism; Autism Diagnostic Interview Revised; behavior; classifier; computer simulation; diagnostic accuracy; diagnostic value; machine learning; psychologic test; sensitivity and specificity; Adolescent; Adult; Artificial Intelligence; Autistic Disorder; Behavior; Case-Control Studies; Child; Child, Preschool; Decision Trees; Humans; Infant; Infant, Newborn; Middle Aged; Questionnaires; Retrospective Studies; Time Factors; Young Adult",Article,Scopus,2-s2.0-84865428954
"Wenzel M.","Isabelle/jEdit - A prover IDE within the PIDE framework",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",19,10.1007/978-3-642-31374-5_38,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864936903&doi=10.1007%2f978-3-642-31374-5_38&partnerID=40&md5=6def849739ea3d5da57708e516abdf0f","PIDE is a general framework for document-oriented prover interaction and integration, based on a bilingual architecture that combines ML and Scala [2]. The overall aim is to connect LCF-style provers like Isabelle [5, §6] (or Coq [5, §4] or HOL [5, §1]) with sophisticated front-end technology on the JVM platform, overcoming command-line interaction at last. © 2012 Springer-Verlag.",,"Isabelle; Artificial intelligence; Knowledge management",Conference Paper,Scopus,2-s2.0-84864936903
"Wright J.C., Collins M.O., Yu L., Käll L., Brosch M., Choudhary J.S.","Enhanced peptide identification by electron transfer dissociation using an improved mascot percolator",2012,"Molecular and Cellular Proteomics",19,10.1074/mcp.O111.014522,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864815807&doi=10.1074%2fmcp.O111.014522&partnerID=40&md5=5be015ffd328689b672a8d9a95412b8c","Peptide identification using tandem mass spectrometry is a core technology in proteomics. Latest generations of mass spectrometry instruments enable the use of electron transfer dissociation (ETD) to complement collision induced dissociation (CID) for peptide fragmentation. However, a critical limitation to the use of ETD has been optimal database search software. Percolator is a postsearch algorithm, which uses semi-supervised machine learning to improve the rate of peptide spectrum identifications (PSMs) together with providing reliable significance measures. We have previously interfaced the Mascot search engine with Percolator and demonstrated sensitivity and specificity benefits with CID data. Here, we report recent developments in the Mascot Percolator V2.0 software including an improved feature calculator and support for a wider range of ion series. The updated software is applied to the analysis of several CID and ETD fragmented peptide data sets. This version of Mascot Percolator increases the number of CID PSMs by up to 80% and ETD PSMs by up to 60% at a 0.01 q-value (1% false discovery rate) threshold over a standard Mascot search, notably recovering PSMs from high charge state precursor ions. The greatly increased number of PSMs and peptide coverage afforded by Mascot Percolator has enabled a fuller assessment of CID/ETD complementarity to be performed. Using a data set of CID and ETcaD spectral pairs, we find that at a 1% false discovery rate, the overlap in peptide identifications by CID and ETD is 83%, which is significantly higher than that obtained using either stand-alone Mascot (69%) or OMSSA (39%). We conclude that Mascot Percolator is a highly sensitive and accurate post-search algorithm for peptide identification and allows direct comparison of peptide identifications using multiple alternative fragmentation techniques. © 2012 by The American Society for Biochemistry and Molecular Biology, Inc.",,"algorithm; article; collisionally activated dissociation; computer program; electron transport; false positive result; machine learning; Mascot Percolator; priority journal; protein analysis; sensitivity and specificity; Algorithms; Artificial Intelligence; Chromatography, Liquid; Databases, Protein; Escherichia coli; Escherichia coli Proteins; Fungal Proteins; Humans; Peptides; Proteomics; Reproducibility of Results; Software; Tandem Mass Spectrometry; Yeasts",Article,Scopus,2-s2.0-84864815807
"Moreira Barradas J.M., Matula S., Dolezal F.","A Decision Support System-Fertigation Simulator (DSS-FS) for design and optimization of sprinkler and drip irrigation systems",2012,"Computers and Electronics in Agriculture",19,10.1016/j.compag.2012.02.015,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863443943&doi=10.1016%2fj.compag.2012.02.015&partnerID=40&md5=868198e041857342ea6afe6e997b5384","Irrigation combined with fertigation has produced unquestionable results for the last few decades. It is a rather complicated process as many factors must be controlled in order to produce good and environmentally safe fertigation practices. The efficiency and uniformity of irrigation, as well as the balance of the nutritive solution used to irrigate are highly ruled by the complex and diverse information (weather, soil, water, and crop data). The article presents the rationale and structure of a decision support system (DSS) and of a Fertigation module (FS - Fertigation Simulator). The DSS-FS system is intended to support design and optimization of irrigation and fertigation systems while increasing their environmental sustainability. The data set to be processed is stored in the DSS database and can be continuously updated according to new development results. Afterwards, the user might handle the input data through a basic and user-friendly interface while allowing the DSS-FS to retrieve default scenarios and thereby reducing the systems user's need for advanced knowledge. An advanced mode of DSS-FS, which adds an increased level of precision in exchange for human support, includes soil sample analysis and other relevant information. © 2012 Elsevier B.V.","Decision support system; Fertigation; Fertigation simulator; Fertigation software; Irrigation; Nutritive solution","Data sets; Decision supports; Design and optimization; Development results; Drip irrigation systems; Environmental sustainability; Environmentally safe; Fertigations; Human support; Input datas; Soil sample; Support design; User friendly interface; Artificial intelligence; Computer software; Decision support systems; Irrigation; Optimization; Simulators; Sprinkler systems (irrigation); agricultural soil; agricultural technology; alternative agriculture; computer simulation; data management; database; decision support system; drip irrigation; farmers knowledge; fertilizer application; nutrient; optimization; precision; software; soil analysis",Article,Scopus,2-s2.0-84863443943
"Wali W.A., Al-Shamma'A A.I., Hassan K.H., Cullen J.D.","Online genetic-ANFIS temperature control for advanced microwave biodiesel reactor",2012,"Journal of Process Control",19,10.1016/j.jprocont.2012.05.013,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864296537&doi=10.1016%2fj.jprocont.2012.05.013&partnerID=40&md5=be3924d81e46f9524fe81373671d7dfc","Biofuels, such as biodiesel, are good for the environment because they add fewer emissions to the atmosphere than petroleum-based fuel. Conventional biodiesel processes are mainly based on use of high power thermal heating to produce biodiesel from pure or waste feedstock such as virgin vegetable oils or waste cooking oils. The development of a novel continuous microwave biodiesel reactor for the conversion of waste oil and fats into biodiesel is reported. This process has the capability to enhance the production of biodiesel in a very short time as compared with conventional methods that require lengthy hours and days. Real time monitoring and control process in microwave biodiesel reactor is necessary to adjust the applied power of microwave reactor under different perturbations for the process temperature control, and full system real time monitoring. The paper focuses on an artificial intelligence technique to design online genetic-ANFIS temperature control based on LabVIEW. The designed controller was compared with error-based Adaptive controller to explore the robustness of the proposed controller in nonlinear real time application. Crown Copyright © 2012 Published by Elsevier Ltd. All rights reserved.","ANFIS; Biodiesel; Error-based adaptive controller; Genetic algorithm; LabVIEW; Microwave biodiesel reactor","Adaptive controllers; ANFIS; Artificial intelligence techniques; Conventional methods; High-power; LabViEW; Microwave reactors; Process temperature; Real time monitoring; Real-time application; Thermal heating; Waste cooking oil; Waste oil; Adaptive control systems; Genetic algorithms; Microwaves; Temperature control; Thermal processing (foods); Vegetable oils; Biodiesel",Article,Scopus,2-s2.0-84864296537
"Norton K.-A., Iyatomi H., Celebi M.E., Ishizaki S., Sawada M., Suzaki R., Kobayashi K., Tanaka M., Ogawa K.","Three-phase general border detection method for dermoscopy images using non-uniform illumination correction",2012,"Skin Research and Technology",19,10.1111/j.1600-0846.2011.00569.x,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863550895&doi=10.1111%2fj.1600-0846.2011.00569.x&partnerID=40&md5=d8fe67461a3697b5bc072d3a104120ab","Background: Computer-aided diagnosis of dermoscopy images has shown great promise in developing a quantitative, objective way of classifying skin lesions. An important step in the classification process is lesion segmentation. Many studies have been successful in segmenting melanocytic skin lesions (MSLs), but few have focused on non-melanocytic skin lesions (NoMSLs), as the wide variety of lesions makes accurate segmentation difficult. Methods: We developed an automatic segmentation program for detecting borders of skin lesions in dermoscopy images. The method consists of a pre-processing phase, general lesion segmentation phase, including illumination correction, and bright region segmentation phase. Results: We tested our method on a set of 107 NoMSLs and a set of 319 MSLs. Our method achieved precision/recall scores of 84.5% and 88.5% for NoMSLs, and 93.9% and 93.8% for MSLs, in comparison with manual extractions from four or five dermatologists. Conclusion: The accuracy of our method was competitive or better than five recently published methods. Our new method is the first method for detecting borders of both non-melanocytic and melanocytic skin lesions. © 2011 John Wiley & Sons A/S.","Melanoma; Non-melanocytic lesions; Segmentation; Skin cancer","Automatic segmentations; Border detection; Classification process; Dermoscopy images; Illumination correction; Melanocytic skin lesions; Melanoma; Non-melanocytic lesions; Non-uniform illumination; Pre-processing; Region segmentation; Skin cancers; Skin lesion; Computer aided diagnosis; Dermatology; Image segmentation; accuracy; article; basal cell carcinoma; computer assisted diagnosis; computer program; epiluminescence microscopy; hemangioma; hematoma; human; illumination; image analysis; image processing; intermethod comparison; major clinical study; melanocytic nevus; melanocytic skin lesion; melanoma; noise reduction; non melanocytic skin lesion; seborrheic keratitis; seborrheic keratosis; skin defect; Artificial Intelligence; Dermoscopy; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Lighting; Melanoma; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Skin Neoplasms",Article,Scopus,2-s2.0-84863550895
"Blanc R., Seiler C., Székely G., Nolte L.-P., Reyes M.","Statistical model based shape prediction from a combination of direct observations and various surrogates: Application to orthopaedic research",2012,"Medical Image Analysis",19,10.1016/j.media.2012.04.004,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866058771&doi=10.1016%2fj.media.2012.04.004&partnerID=40&md5=afa6ac7899607638bf3e12b42f989862","In computer-assisted orthopaedic surgery, recovering three-dimensional patient-specific anatomy from incomplete information has been focus of interest due to several factors such as less invasive surgical procedures, reduced radiation doses, and rapid intra-operative updates of the anatomy. The aim of this paper is to report results obtained combining statistical shape modeling and multivariate regression techniques for predicting bone shape from clinically and surgically relevant predictors, including sparse observations of the bone surface but also morphometric and anthropometric information. Different state of the art methods such as partial least square regression, principal component regression, canonical correlation analysis, and non-parametric kernel-based regression are compared. Clinically relevant surrogate variables and combinations are investigated on a database of 142 femur and 154 tibia shapes obtained from CT images. The results are evaluated using cross validation to quantify the prediction error. The proposed approach enables to characterize the added value of different predictors in a quantitative and localized fashion. Results indicate that complementary sources of information can be efficiently exploited to improve the accuracy of shape prediction. © 2012 Elsevier B.V.","Computer-assisted orthopaedic research; Linear regression; Shape prediction","Added values; Bone surface; Canonical correlation analysis; Computer assisted; Computer-assisted orthopaedic surgery; Cross validation; CT Image; Incomplete information; Intra-operative; Multivariate regression; Non-parametric; Partial least square regression; Prediction errors; Principal component regression; Shape prediction; State-of-the-art methods; Statistical models; Statistical shapes; Surgical procedures; Bone; Computerized tomography; Linear regression; Patient rehabilitation; Principal component analysis; Regression analysis; Forecasting; adult; aged; anthropometry; article; bone shape; computer analysis; female; human; male; medical research; morphometrics; musculoskeletal system parameters; orthopedics; priority journal; Adult; Aged; Aged, 80 and over; Algorithms; Artificial Intelligence; Biomedical Research; Female; Femur; Humans; Male; Middle Aged; Models, Biological; Models, Statistical; Orthopedics; Pattern Recognition, Automated; Radiographic Image Enhancement; Radiographic Image Interpretation, Computer-Assisted; Reproducibility of Results; Sensitivity and Specificity; Tibia; Tomography, X-Ray Computed; Young Adult",Article,Scopus,2-s2.0-84866058771
"Stahl A., Schellewald C., Stavdahl Ø., Aamo O.M., Adde L., Kirkerod H.","An optical flow-based method to predict infantile cerebral palsy",2012,"IEEE Transactions on Neural Systems and Rehabilitation Engineering",19,10.1109/TNSRE.2012.2195030,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863748997&doi=10.1109%2fTNSRE.2012.2195030&partnerID=40&md5=ad2c8e6105e6af37fa3f057f45e23db7","Cerebral palsy (CP) is a perinatally acquired nonprogressive brain damage resulting in motor impairment affecting mobility and posture. Early identification of infants with CP is desired to target early interventions and follow-up. During early infancy, distinct motion patterns occur which are highly predictive for later disability. These motor patterns can be observed and recorded. In this paper, a method to predict later CP based on early video recordings of the infants' spontaneous movements, applying optical flow and statistical pattern recognition, is presented. We extract motion information from video recordings of young infants using a total variation related optical flow method. By using wavelet analysis features from motion trajectories of points initiated on a regular grid were extracted and classified using a support vector machine. © 2012 IEEE.","Cerebral palsy (CP); optical flow; support vector machine (SVM); tracking; wavelet analysis","Brain damage; Cerebral palsy; Early intervention; Flow-based methods; Motion information; Motion pattern; Motion trajectories; Motor impairments; Nonprogressive; Optical flow methods; Regular grids; Statistical pattern recognition; Total variation; Diseases; Pattern recognition; Support vector machines; Surface discharges; Video recording; Wavelet analysis; Optical flows; article; artificial intelligence; automated pattern recognition; cerebral palsy; early diagnosis; equipment design; equipment failure analysis; female; human; infant; male; methodology; movement (physiology); pathophysiology; preschool child; reproducibility; sensitivity and specificity; three dimensional imaging; videorecording; Artificial Intelligence; Cerebral Palsy; Child, Preschool; Early Diagnosis; Equipment Design; Equipment Failure Analysis; Female; Humans; Imaging, Three-Dimensional; Infant; Male; Movement; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Video Recording",Article,Scopus,2-s2.0-84863748997
"Hon W.-K., Shah R., Thankachan S.V.","Towards an optimal space-and-query-time index for top-k document retrieval",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",19,10.1007/978-3-642-31265-6_14,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863107499&doi=10.1007%2f978-3-642-31265-6_14&partnerID=40&md5=2045b78debcf03b8cc3ed6a505576446","Let {d 1,d 2,...d D } be a given set of D string documents of total length n, our task is to index , such that the k most relevant documents for an online query pattern P of length p can be retrieved efficiently. We propose an index of size |CSA| + nlogD(2 + o(1)) bits and O(t s (p) + kloglogn + polyloglogn) query time for the basic relevance metric term-frequency, where |CSA| is the size (in bits) of a compressed full text index of , with O(t s (p)) time for searching a pattern of length p. We further reduce the space to |CSA| + nlogD(1 + o(1)) bits, however the query time will be O(t s (p) + k(logσloglogn) 1 + ε + polyloglogn), where σ is the alphabet size and ε &gt; 0 is any constant. © 2012 Springer-Verlag.",,"Alphabet size; Document Retrieval; Full-text index; Query patterns; Query time; Relevant documents; Total length; Artificial intelligence; Pattern matching",Conference Paper,Scopus,2-s2.0-84863107499
"Piperagkas G.S., Konstantaras I., Skouri K., Parsopoulos K.E.","Solving the stochastic dynamic lot-sizing problem through nature-inspired heuristics",2012,"Computers and Operations Research",19,10.1016/j.cor.2011.09.004,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-81555198352&doi=10.1016%2fj.cor.2011.09.004&partnerID=40&md5=a96315d372362fbb3e97c52ab0b744ee","We investigate the dynamic lot-size problem under stochastic and non-stationary demand over the planning horizon. The problem is tackled by using three popular heuristic methods from the fields of evolutionary computation and swarm intelligence, namely particle swarm optimization, differential evolution and harmony search. To the best of the authors knowledge, this is the first investigation of the specific problem with approaches of this type. The algorithms are properly manipulated to fit the requirements of the problem. Their performance, in terms of run-time and solution accuracy, is investigated on test cases previously used in relevant works. Specifically, the lot-size problem with normally distributed demand is considered for different planning horizons, varying from 12 up to 48 periods. The obtained results are analyzed, providing evidence on the efficiency of the employed approaches as promising alternatives to the established WagnerWhitin algorithm, as well as hints on their proper configuration. © 2011 Elsevier Ltd. All rights reserved.","Differential evolution; Dynamic lot-size problem; Harmony search; Particle swarm optimization; WagnerWhitin algorithm","Differential Evolution; Harmony search; Lot-size; Non-stationary demand; Planning horizons; Runtimes; Solution accuracy; Specific problems; Stochastic dynamics; Swarm Intelligence; Test case; Wagner-whitin algorithms; Algorithms; Artificial intelligence; Cellular automata; Normal distribution; Particle swarm optimization (PSO); Stochastic systems; Heuristic methods",Article,Scopus,2-s2.0-81555198352
"Manley M., Kim Y.S.","Modeling emergency evacuation of individuals with disabilities (exitus): An agent-based public decision support system",2012,"Expert Systems with Applications",19,10.1016/j.eswa.2012.01.169,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862823605&doi=10.1016%2fj.eswa.2012.01.169&partnerID=40&md5=c37b04d567a00f425538d2f1d2864f12","In this paper, we present a public decision support system (DSS) distinguished from various DSSs in the private business sector in terms of its ownership, data scarcity, and beneficiaries. In particular, our system is intended to play a vital role in assessing and optimizing emergency response plans for rare but catastrophic events such as the September 11th attacks. While taking an agent-based microscopic simulation approach in a hierarchical framework, we used our model to estimate the effectiveness of alternative evacuation strategies to support emergency response planning as a part of business continuity planning for all private business organizations. The presented model is unique because it considers individuals with disabilities explicitly in terms of speed, ability to negotiate the environment, and normalcy bias depending on type of disability. It is also capable of classifying the environment in terms of accessibility characteristics encompassing various conditions which have been shown to have a disproportionate effect upon the behavior of individuals with disabilities during an emergency. Through a series of simulation experiments, our system identified specific locations (e.g.; the NW landing) on the 3rd floor of the test bed building as possible bottleneck spots under certain conditions (e.g.; the sharp increase in individuals with disabilities among residents). This way, our system provides the architect with tools to test the structure's design to determine how well it meets the identified requirements for emergency evacuation to accommodate this shifting demographic. In particular, our system strongly demonstrated the effectiveness of new emergency evacuation strategies for individuals with disabilities such as assisted evacuations which allows other healthy people to play more active roles compared to traditional strategies in which individuals with disabilities are helplessly waiting for assistants such as fire fighters at the designated area. Our system also revealed that people using wheelchairs and those with lower stamina were at the greatest risk. People with lower stamina such as the elderly, people with chronic health conditions, or those with temporary injuries are at a greater risk mainly because they are not easily identifiable. Ultimately, the proposed DSS system can be used to inform public policy professionals of more effective, evidence-based evacuation planning and environmental design methods based on a better understanding of the behavior of individuals with disabilities. © 2012 Elsevier Ltd. All rights reserved.","Agent-based modeling; Emergency evacuation; Individuals with disabilities; Pedestrian simulation; Public decision support system","Agent-based modeling; Decision supports; Emergency evacuation; Individuals with disabilities; Pedestrian simulation; Artificial intelligence; Computer simulation; Decision support systems; Equipment testing; Health risks",Article,Scopus,2-s2.0-84862823605
"Tak M.J.W., Winands M.H.M., Björnsson Y.","N-grams and the last-good-reply policy applied in general game playing",2012,"IEEE Transactions on Computational Intelligence and AI in Games",19,10.1109/TCIAIG.2012.2200252,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862569316&doi=10.1109%2fTCIAIG.2012.2200252&partnerID=40&md5=32623c68fa470876cbd3f55522d32bb3","The aim of general game playing (GGP) is to create programs capable of playing a wide range of different games at an expert level, given only the rules of the game. The most successful GGP programs currently employ simulation-based Monte Carlo tree search (MCTS). The performance of MCTS depends heavily on the simulation strategy used. In this paper, we introduce improved simulation strategies for GGP that we implement and test in the GGP agent CadiaPlayer, which won the International GGP competition in both 2007 and 2008. There are two aspects to the improvements: first, we show that a simple ε-greedy exploration strategy works better in the simulation play-outs than the softmax-based Gibbs measure currently used in CadiaPlayer and, second, we introduce a general framework based on N-grams for learning promising move sequences. Collectively, these enhancements result in a much improved performance of CadiaPlayer. For example, in our test suite consisting of five different two-player turn-based games, they led to an impressive average win rate of approximately 70%. The enhancements are also shown to be effective in multiplayer and simultaneous-move games. We additionally perform experiments with the last-good-reply policy (LGRP). The LGRP combined with N-grams is also tested. The LGRP has already been shown to be successful in Go programs and we demonstrate that it also has promise in GGP. © 2012 IEEE.","General game playing (GGP); last-good-reply policy (LGRP); Monte Carlo tree search (MCTS); N-grams","General game playing; Gibbs measure; Greedy exploration; last-good-reply policy (LGRP); MONTE CARLO; Multiplayers; N-grams; Simulation strategies; Simulation-based; Tree search; Artificial intelligence; Forestry; Game theory; Artificial Intelligence; Experimentation; Simulation",Article,Scopus,2-s2.0-84862569316
"Rodil I.F., Compton T.J., Lastra M.","Exploring macroinvertebrate species distributions at regional and local scales across a sandy beach geographic continuum",2012,"PLoS ONE",19,10.1371/journal.pone.0039609,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862683312&doi=10.1371%2fjournal.pone.0039609&partnerID=40&md5=46de466b4984c67cbf2a56cb71133a28","Exposed sandy beaches are highly dynamic ecosystems where macroinvertebrate species cope with extremely variable environmental conditions. The majority of the beach ecology studies present exposed beaches as physically dominated ecosystems where abiotic factors largely determine the structure and distribution of macrobenthic communities. However, beach species patterns at different scales can be modified by the interaction between different environmental variables, including biotic interactions. In this study, we examined the role of different environmental variables for describing the regional and local scale distributions of common macrobenthic species across 39 beaches along the North coast of Spain. The analyses were carried out using boosted regression trees, a relatively new technique from the field of machine learning. Our study showed that the macroinvertebrate community on exposed beaches is not structured by a single physical factor, but instead by a complex set of drivers including the biotic compound. Thus, at a regional scale the macrobenthic community, in terms of number of species and abundance, was mainly explained by surrogates of food availability, such as chlorophyll a. The results also revealed that the local scale is a feasible way to construct general predictive species-environmental models, since relationships derived from different beaches showed similar responses for most of the species. However, additional information on aspects of beach species distribution can be obtained with large scale models. This study showed that species-environmental models should be validated against changes in spatial extent, and also illustrates the utility of BRTs as a powerful analysis tool for ecology data insight. © 2012 Rodil et al.",,"chlorophyll; article; benthos; controlled study; environmental factor; food availability; geographic distribution; machine learning; macroinvertebrate; mathematical model; nonhuman; population abundance; predictive value; probability; sandy soil; seashore; Spain; species distribution; species richness; Animals; Artificial Intelligence; Ecosystem; Environment; Environmental Monitoring; Invertebrates; Population Dynamics",Article,Scopus,2-s2.0-84862683312
"Faria B.M., Vasconcelos S., Reis L.P., Lau N.","A methodology for creating intelligent wheelchair users' profiles",2012,"ICAART 2012 - Proceedings of the 4th International Conference on Agents and Artificial Intelligence",19,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862126300&partnerID=40&md5=ca75e0a6a036cb1a9088dd02eac42917","Intelligent Wheelchair (IW) is a new concept aiming to allow higher autonomy to people with lower mobility such as disabled or elderly individuals. Some of the more recent IWs have a multimodal interface, enabling multiple command modes such as joystick, voice commands, head movements, or even facial expressions. In these IW it may be very useful to provide the user with the best way of driving it through an adaptive interface. This paper describes the foundations for creating a simple methodology for extracting user profiles, which can be used to adequately select the best IW command mode for each user. The methodology is based on an interactive wizard composed by a flexible set of simple tasks presented to the user, and a method for extracting and analyzing the user's execution of those tasks. The results achieved showed that it is possible to extract simple user profiles, using the proposed method. Thus, the approach may be further used to extract more complete user profiles, just by extending the set of tasks used, enabling the adaptation of the IW interface to each user's characteristics.","Adaptive interface; Intelligent wheelchair; Users' profile","Adaptive interface; Command mode; Facial Expressions; Head movements; Intelligent wheelchair; Multi-modal interfaces; User profile; Users' profile; Voice command; Artificial intelligence; Intelligent robots",Conference Paper,Scopus,2-s2.0-84862126300
"Paulheim H.","Generating possible interpretations for statistics from linked open data",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",19,10.1007/978-3-642-30284-8_44,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861744403&doi=10.1007%2f978-3-642-30284-8_44&partnerID=40&md5=b84a570252073de3835fbca6255bdb1c","Statistics are very present in our daily lives. Every day, new statistics are published, showing the perceived quality of living in different cities, the corruption index of different countries, and so on. Interpreting those statistics, on the other hand, is a difficult task. Often, statistics collect only very few attributes, and it is difficult to come up with hypotheses that explain, e.g., why the perceived quality of living in one city is higher than in another. In this paper, we introduce Explain-a-LOD, an approach which uses data from Linked Open Data for generating hypotheses that explain statistics. We show an implemented prototype and compare different approaches for generating hypotheses by analyzing the perceived quality of those hypotheses in a user study. © 2012 Springer-Verlag.",,"Daily lives; Perceived quality; User study; Artificial intelligence; Statistics",Conference Paper,Scopus,2-s2.0-84861744403
"Liao H., Liu D., Huang Y., Yu W., Weng J., You Y., Zhang Y.","Smart grid power transfer capability analysis considering integrated renewable energy resources and energy storage systems",2012,"Zhongguo Dianji Gongcheng Xuebao/Proceedings of the Chinese Society of Electrical Engineering",19,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864319781&partnerID=40&md5=aa919208c1dd8d28193789f18df62801","The integration of renewable energy resources and storage systems makes power delivery route reconstruction after the N-1 fault taking place more complicated. To solve the calculation problem of smart grid power transfer capability, the time-varying characteristics of renewable energy resources and storage systems were analyzed. Based on this, an N-1 recovery model was proposed on the base of the transfer capability index. Through making the calculation formula of the transfer capability index linear and adopting the artificial intelligence (AI) optimal algorithm based on topology simplification, the operation modes of the power grid, new energy generations and new energy storage systems can be optimized and the maximum power transfer capability of the grid can be achieved. Finally, taking an actual typical network as an example, the significant role of interconnection of renewable energy resources and energy storage devices for the improvement of N-1 recovery ability and the transfer of loads was analyzed. The power transfer capability index in quantitatively describing the self-healing feature was proved to be effective. © 2012 Chinese Society for Electrical Engineering.","Energy storage systems; Renewable energy resources; Smart grid; Topology simplification; Transfer capability","Artificial intelligence; Electric power transmission; Energy storage; Industrial engineering; Optimization; Smart power grids; Topology; Virtual storage; Calculation formula; Energy generations; Energy storage systems; Maximum power transfer; Operation mode; Optimal algorithm; Power delivery; Power grids; Power transfer capability; Recovery model; Self-healing; Smart grid; Storage systems; Time-varying characteristics; Transfer capability; Transfer of load; Renewable energy resources",Article,Scopus,2-s2.0-84864319781
"Chakraborty J., Mukhopadhyay S., Singla V., Khandelwal N., Bhattacharyya P.","Automatic detection of pectoral muscle using average gradient and shape based feature",2012,"Journal of Digital Imaging",19,10.1007/s10278-011-9421-y,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861634258&doi=10.1007%2fs10278-011-9421-y&partnerID=40&md5=63602ddbb22d054cdbb891b296a303c4","In medio-lateral oblique view of mammogram, pectoral muscle may sometimes affect the detection of breast cancer due to their similar characteristics with abnormal tissues. As a result pectoral muscle should be handled separately while detecting the breast cancer. In this paper, a novel approach for the detection of pectoral muscle using average gradient- and shape-based feature is proposed. The process first approximates the pectoral muscle boundary as a straight line using average gradient-, position-, and shape- based features of the pectoral muscle. Straight line is then tuned to a smooth curve which represents the pectoral margin more accurately. Finally, an enclosed region is generated which represents the pectoral muscle as a segmentation mask. The main advantage of the method is its' simplicity as well as accuracy. The method is applied on 200 mammographic images consisting 80 randomly selected scanned film images from Mammographic Image Analysis Society (mini-MIAS) database, 80 direct radiography (DR) images, and 40 computed radiography (CR) images from local database. The performance is evaluated based upon the false positive (FP), false negative (FN) pixel percentage, and mean distance closest point (MDCP). Taking all the images into consideration, the average FP and FN pixel percentages are 4.22%, 3.93%, 18.81%, and 6.71%, 6.28%, 5.12% for mini-MIAS, DR, and CR images, respectively. Obtained MDCP values for the same set of database are 3.34, 3.33, and 10.41 respectively. The method is also compared with two well-known pectoral muscle detection techniques and in most of the cases, it outperforms the other two approaches. © 2011 Society for Imaging Informatics in Medicine.","Adaptive band division; Biomedical image analysis; Breast cancer; Mammography; Pectoral muscle; Segmentation","Abnormal tissues; Automatic Detection; Average gradient; Band division; Biomedical image analysis; Breast Cancer; Computed radiography; CR Images; Detection technique; False negatives; False positive; Film images; Mammographic image analysis; Mammographic images; Mean distances; Medio-lateral oblique views; Pectoral muscles; Shape based; Shape based features; Smooth curves; Database systems; Diseases; Image analysis; Image segmentation; Mammography; Pixels; Radiography; X ray screens; Muscle; algorithm; article; artificial intelligence; automated pattern recognition; breast tumor; computer assisted diagnosis; computer assisted tomography; female; human; mammography; methodology; pectoralis major muscle; radiography; Algorithms; Artificial Intelligence; Breast Neoplasms; Female; Humans; Mammography; Pattern Recognition, Automated; Pectoralis Muscles; Radiographic Image Interpretation, Computer-Assisted; Tomography, X-Ray Computed",Article,Scopus,2-s2.0-84861634258
"Altman R.B.","Translational bioinformatics: Linking the molecular world to the clinical world",2012,"Clinical Pharmacology and Therapeutics",19,10.1038/clpt.2012.49,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861338882&doi=10.1038%2fclpt.2012.49&partnerID=40&md5=9b381fbeb089960fc590c9a2c31230c2","Translational bioinformatics represents the union of translational medicine and bioinformatics. Translational medicine moves basic biological discoveries from the research bench into the patient-care setting and uses clinical observations to inform basic biology. It focuses on patient care, including the creation of new diagnostics, prognostics, prevention strategies, and therapies based on biological discoveries. Bioinformatics involves algorithms to represent, store, and analyze basic biological data, including DNA sequence, RNA expression, and protein and small-molecule abundance within cells. Translational bioinformatics spans these two fields; it involves the development of algorithms to analyze basic molecular and cellular data with an explicit goal of affecting clinical care. © 2012 American Society for Clinical Pharmacology and Therapeutics.",,"algorithm; article; bioinformatics; DNA sequence; gene expression; genomics; patient care; priority journal; public health; translational bioinformatics; Artificial Intelligence; Computational Biology; Drug Therapy; Genetic Privacy; Genomics; Humans; Pathology, Molecular; Pharmacology; Prognosis; Public Health; PubMed; Systems Biology; Translational Medical Research; United States",Article,Scopus,2-s2.0-84861338882
"Car N.J., Christen E.W., Hornbuckle J.W., Moore G.A.","Using a mobile phone Short Messaging Service (SMS) for irrigation scheduling in Australia - Farmers' participation and utility evaluation",2012,"Computers and Electronics in Agriculture",19,10.1016/j.compag.2012.03.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860310629&doi=10.1016%2fj.compag.2012.03.003&partnerID=40&md5=637c62c05275a5c0b400a83dd5743b5c","Irrigation scheduling Decision Support Systems (DSS) have seen poor uptake despite proved usage benefits. The failures of some previous systems with proven model accuracy and water savings ability have been attributed to interface difficulties and inappropriate information for end users. Use of the mobile phone Short Messaging Service (SMS) text messages was trialed as an interface to overcome these difficulties. Irrigation system dripper run time scheduling advice was sent daily to 72 Australian irrigators' mobile phones from a water balance system called IrriSatSMS. Irrigators sent back information on irrigations and rainfall, also via SMS, to update the water balance. This trial showed that a complex, water balance-based, DSS could rely on SMS as the sole interface.All 72 irrigators involved were content to receive messages daily for the entire growing season (200 days). A measure of engagement and utility of the system was determined by those who returned their irrigation and rainfall data; 45 sent in their data all season, 13 for half the season and 14 never sent in any data. Thus we infer that 45 users (63%) found the SMS system of enough utility to use for the whole season. Also, at end of season, 6 of the 13 who had stopped half way through said that in retrospect they wished they had not. Thus overall 80% of irrigators found the system useful.User interview data showed the simplicity of use, advice and the prompting effects of intrusive delivery (phone ringing) were key features in the resultant strong engagement of irrigators. Success also relied on appreciating that irrigators will only use objective decision support advice as one element in a set of decision making tools that include subjective and unquantifiable elements, such as plant appearance.This strong uptake reverses the trend in irrigation decision support which has seen poor uptake of sophisticated systems that produce comprehensive scheduling support but which are, or are perceived to be, complex and time consuming to use. Additionally, high participation rates show that much model input data may be collected from irrigators via SMS so it can be used as a very cheap bi-directional communication channel. © 2012.","Australia; Decision Support System; Irrigation; Short Messaging Service; Utility; Winegrape","Australia; Bi-directional communication; Decision making tool; Decision supports; End users; Growing season; Irrigation scheduling; Irrigation systems; Key feature; Model accuracy; Model inputs; Participation rate; Rainfall data; Run-time scheduling; Short messaging service; Sophisticated system; Text messages; Utility; Water balance; Water saving; Winegrape; Artificial intelligence; Cellular telephones; Decision support systems; Irrigation; Mobile phones; Rain; Scheduling; Telephone systems; Message passing; accuracy assessment; agricultural technology; decision support system; farmers attitude; fruit; growing season; irrigation system; mobile communication; precision agriculture; utility sector; viticulture; water budget; Australia",Article,Scopus,2-s2.0-84860310629
"Langseth H., Nielsen T.D.","A latent model for collaborative filtering",2012,"International Journal of Approximate Reasoning",19,10.1016/j.ijar.2011.11.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858799375&doi=10.1016%2fj.ijar.2011.11.002&partnerID=40&md5=0b3563d4268a5af27b6cacd44d6ce34b","Recommender systems based on collaborative filtering have received a great deal of interest over the last two decades. In particular, recently proposed methods based on dimensionality reduction techniques and using a symmetrical representation of users and items have shown promising results. Following this line of research, we propose a probabilistic collaborative filtering model that explicitly represents all items and users simultaneously in the model. Experimental results show that the proposed system obtains significantly better results than other collaborative filtering systems (evaluated on the MovieLens data set). Furthermore, the explicit representation of all users and items allows the model to e.g. make group-based recommendations balancing the preferences of the individual users. © 2011 Elsevier Inc. All rights reserved.","Collaborative filtering; Graphical models; Latent variables; Recommender systems","Collaborative filtering; Collaborative filtering systems; Data sets; Dimensionality reduction techniques; Explicit representation; GraphicaL model; Group-based; Latent models; Latent variables; Artificial intelligence; Software engineering; Recommender systems",Article,Scopus,2-s2.0-84858799375
"Lavi O., Dror G., Shamir R.","Network-induced classification kernels for gene expression profile analysis",2012,"Journal of Computational Biology",19,10.1089/cmb.2012.0065,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862525682&doi=10.1089%2fcmb.2012.0065&partnerID=40&md5=4e68b0ac372595521929200b68416f68","Computational classification of gene expression profiles into distinct disease phenotypes has been highly successful to date. Still, robustness, accuracy, and biological interpretation of the results have been limited, and it was suggested that use of protein interaction information jointly with the expression profiles can improve the results. Here, we study three aspects of this problem. First, we show that interactions are indeed relevant by showing that co-expressed genes tend to be closer in the network of interactions. Second, we show that the improved performance of one extant method utilizing expression and interactions is not really due to the biological information in the network, while in another method this is not the case. Finally, we develop a new kernel method-called NICK-that integrates network and expression data for SVM classification, and demonstrate that overall it achieves better results than extant methods while running two orders of magnitude faster. © 2012, Mary Ann Liebert, Inc.","algorithms","algorithm; article; artificial intelligence; biology; classification; computer program; gene expression; gene expression profiling; gene regulatory network; human; methodology; Algorithms; Artificial Intelligence; Computational Biology; Gene Expression; Gene Expression Profiling; Gene Regulatory Networks; Humans; Software",Article,Scopus,2-s2.0-84862525682
"He J., Dukes M.D., Hochmuth G.J., Jones J.W., Graham W.D.","Identifying irrigation and nitrogen best management practices for sweet corn production on sandy soils using CERES-Maize model",2012,"Agricultural Water Management",19,10.1016/j.agwat.2012.02.007,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863406649&doi=10.1016%2fj.agwat.2012.02.007&partnerID=40&md5=619c4d0b30a48d2ae323b1fca49865bc","Research based crop-specific best management practices (BMPs) must be developed for sweet corn (. Zea mays L. var. . saccharata) production to reduce the amount of nitrogen (N) leaching. The objective of this study was to identify irrigation and nitrogen BMPs for sweet corn production on sandy soils in Florida using the calibrated CERES-Maize model of the Decision Support System for Agrotechnology Transfer (DSSAT). A total of 24 irrigation schedules, 21 N fertilizer levels, 30 N application splits, and 20 N application rates per split were systematically evaluated in single factor simulations. Then, a set of 324 management scenarios composed of 6 irrigation timing/amount and 54 N fertilizer application strategies selected in early single factor explorations, was explored in a multifactor analysis.Irrigation frequency had a strong influence on sweet corn yield. If irrigation events were triggered when maximum allowable depletion (MAD) of soil water content was greater than 60%, corn growth suffered water stress and the simulated yield was reduced. The increase in yield approached zero above 168kgNha -1. Splitting N fertilizer applications did not influence yield if there was an N application during the small-leaf stage or large-leaf stage; however, the lowest amount of N leaching occurred when no N was applied during the small-leaf stage. Simulated yield increased when application rates decreased from 100 to 70kgNha -1 per fertigation event, but changed only slightly at application rates less than 70kgNha -1 per fertigation. Smaller application rates per fertigation decreased N leaching substantially, especially for rates less than 70kgNha -1. Six potential BMPs were selected from the 324 management scenarios as optimizing yield while minimizing N leaching. These BMPs were composed of two irrigation schedules (depths of 5.0 and 7.5mm with MAD values of 20% and 30%), two N levels (196 and 224kgNha -1), two N split plans (0-1/4-3/4 and 0-1/3-2/3 of total N applied in the small-leaf, large-leaf, and ear development stages, respectively), and two N application rates per fertigation (30 and 40kgNha -1). It should be recognized that these results are recommendations based on modeling assumptions and should be tested in actual field production for their practical and economic validity. © 2012 Elsevier B.V.","Best management practice; BMP; CERES-Maize; DSSAT; Irrigation; Nitrogen; Sweet corn","Best management practices; BMP; CERES maizes; DSSAT; Sweet corns; Artificial intelligence; Decision support systems; Fertilizers; Grain (agricultural product); Leaching; Nitrogen; Sand; Soil moisture; Irrigation; best management practice; calibration; crop production; ecological modeling; fertilizer application; frequency analysis; identification method; irrigation; leaching; maize; nitrogen; sandy soil; soil water; water stress; Florida [United States]; United States; Zea mays",Article,Scopus,2-s2.0-84863406649
"Araya I., Trombettoni G., Neveu B.","A contractor based on convex interval Taylor",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",19,10.1007/978-3-642-29828-8_1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861435415&doi=10.1007%2f978-3-642-29828-8_1&partnerID=40&md5=8771f5c0ea7efad3e8f513601c3ff89c","Interval Taylor has been proposed in the sixties by the interval analysis community for relaxing continuous non-convex constraint systems. However, it generally produces a non-convex relaxation of the solution set. A simple way to build a convex polyhedral relaxation is to select a corner of the studied domain/box as expansion point of the interval Taylor form, instead of the usual midpoint. The idea has been proposed by Neumaier to produce a sharp range of a single function and by Lin and Stadtherr to handle n x n (square) systems of equations. This paper presents an interval Newton-like operator, called X-Newton, that iteratively calls this interval convexification based on an endpoint interval Taylor. This general-purpose contractor uses no preconditioning and can handle any system of equality and inequality constraints. It uses Hansen's variant to compute the interval Taylor form and uses two opposite corners of the domain for every constraint. The X-Newton operator can be rapidly encoded, and produces good speedups in constrained global optimization and constraint satisfaction. First experiments compare X-Newton with affine arithmetic. © 2012 Springer-Verlag.",,"Affine arithmetic; Constrained global optimization; Constraint Satisfaction; Convex polyhedral; Convexification; Inequality constraint; Interval analysis; Non-convex constraints; Solution set; Systems of equations; Artificial intelligence; Combinatorial optimization; Computer programming; Contractors; Global optimization; Relaxation processes; Constraint theory",Conference Paper,Scopus,2-s2.0-84861435415
"Ueki M., Tamiya G.","Ultrahigh-dimensional variable selection method for whole-genome gene-gene interaction analysis",2012,"BMC Bioinformatics",19,10.1186/1471-2105-13-72,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860365567&doi=10.1186%2f1471-2105-13-72&partnerID=40&md5=37638c58381f82b4e825e488b1e1be7a","Background: Genome-wide gene-gene interaction analysis using single nucleotide polymorphisms (SNPs) is an attractive way for identification of genetic components that confers susceptibility of human complex diseases. Individual hypothesis testing for SNP-SNP pairs as in common genome-wide association study (GWAS) however involves difficulty in setting overall p-value due to complicated correlation structure, namely, the multiple testing problem that causes unacceptable false negative results. A large number of SNP-SNP pairs than sample size, so-called the large p small n problem, precludes simultaneous analysis using multiple regression. The method that overcomes above issues is thus needed.Results: We adopt an up-to-date method for ultrahigh-dimensional variable selection termed the sure independence screening (SIS) for appropriate handling of numerous number of SNP-SNP interactions by including them as predictor variables in logistic regression. We propose ranking strategy using promising dummy coding methods and following variable selection procedure in the SIS method suitably modified for gene-gene interaction analysis. We also implemented the procedures in a software program, EPISIS, using the cost-effective GPGPU (General-purpose computing on graphics processing units) technology. EPISIS can complete exhaustive search for SNP-SNP interactions in standard GWAS dataset within several hours. The proposed method works successfully in simulation experiments and in application to real WTCCC (Wellcome Trust Case-control Consortium) data.Conclusions: Based on the machine-learning principle, the proposed method gives powerful and flexible genome-wide search for various patterns of gene-gene interaction. © 2012 Ueki and Tamiya; licensee BioMed Central Ltd.",,"Case-control; Coding methods; Complex disease; Correlation structure; Exhaustive search; False negatives; Gene-gene interaction; General-purpose computing; Genetic components; Genome-wide association studies; Graphics Processing Unit; Hypothesis testing; Large-p small-n; Logistic regressions; Machine-learning; Multiple regressions; Multiple testing problems; P-values; Predictor variables; Ranking strategy; Sample sizes; Simultaneous analysis; Single nucleotide polymorphisms; Software program; Variable selection; Variable selection methods; Computer graphics; Diagnosis; Logistics; Program processors; Statistical tests; Genes; algorithm; article; artificial intelligence; biological model; computer program; computer simulation; gene expression; genetic association; genetic predisposition; genetics; human; human genome; methodology; single nucleotide polymorphism; statistical model; Algorithms; Artificial Intelligence; Computer Simulation; Gene Expression; Genetic Predisposition to Disease; Genome, Human; Genome-Wide Association Study; Humans; Logistic Models; Models, Genetic; Polymorphism, Single Nucleotide; Software",Article,Scopus,2-s2.0-84860365567
"Sarailidis G., Katsavounidis I.","A multiscale error diffusion technique for digital multitoning",2012,"IEEE Transactions on Image Processing",19,10.1109/TIP.2012.2185936,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860164303&doi=10.1109%2fTIP.2012.2185936&partnerID=40&md5=c8f14d4645cc6a8ced1292d67559465c","Multitoning is the representation of digital pictures using a given set of available color intensities, which are also known as tones or quantization levels. It can be viewed as the generalization of halftoning, where only two such quantization levels are available. Its main application is for printing and, similar to halftoning, can be applied to both colored and grayscale images. In this paper, we present a method to produce multitones based on the multiscale error diffusion technique. Key characteristics of this technique are: 1) the use of an image quadtree; 2) the quantization order of the pixels being determined through maximum intensity guidance on the image quadtree; and 3) noncausal error diffusion. Special care has been given to the problem of banding, which is one of the inherent limitations in error diffusion when applied to multitoning. Banding is evident in areas of the image with values close to one of the available quantization levels; our approach is to apply a preprocessing step to alleviate part of the problem. Our results are evaluated both in terms of visual appearance and using a set of standard metrics, with the latter demonstrating the blue-noise characteristics and very low anisotropy of the proposed method. © 1992-2012 IEEE.","Anisotropy; banding; error diffusion; image quadtree; multilevel halftoning; multitoning","banding; Error diffusion; Halftoning; Image quadtree; multitoning; Anisotropy; Visualization; Diffusion; algorithm; article; artificial intelligence; automated pattern recognition; color; colorimetry; computer assisted diagnosis; diffusion; image enhancement; methodology; reproducibility; sensitivity and specificity; Algorithms; Artificial Intelligence; Color; Colorimetry; Diffusion; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84860164303
"Quiala E., Cañal M.-J., Meijón M., Rodríguez R., Chávez M., Valledor L., de Feria M., Barbón R.","Morphological and physiological responses of proliferating shoots of teak to temporary immersion and BA treatments",2012,"Plant Cell, Tissue and Organ Culture",19,10.1007/s11240-011-0088-3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859421283&doi=10.1007%2fs11240-011-0088-3&partnerID=40&md5=cb5dc0488022e40d6fac4bf332fff17d","Shoot-tips, collected from greenhouse-grown plants of Tectona grandis L. (teak), were incubated on a semi-solid Murashige and Skoog (MS) medium with 2% (w/v) sucrose, and supplemented with 4.44 μM 6-benzyladenine (BA). These were then transferred to a temporary immersion system (TIS) using liquid MS medium supplemented with 0 (CK-free medium), 2.22, 4.44, 6.66 μM BA. High mean numbers of shoots per explant were obtained when explants were grown on medium containing either 4.44 or 6.66 μM BA and yielding 7.7 and 10.3 normal shoots (NS)/explant, respectively. Moreover, these high BA levels contributed to lower accumulation of phenolic compounds and deposition of lignin in vascular cells of the teak shoots following histochemical analysis. Morphological analysis of proliferating shoots by scanning microscopy revealed that leaves of shoots incubated on either CK-free medium, 2.22, or 4.44 μM BA had elliptical stomata; whereas, stomata of leaves of shoots grown on medium containing 6.66 μM BA were primarily ring-shaped, raised, and open. Moreover, misshapen stomata with broken epidermal layers of guard cells, typical of hyperhydric leaves, were also observed. When shoots were rooted ex vitro by dipping in 492.1 μM indole-3-butyric acid (IBA) for 2 min, the frequency of rooting of shoots previously grown on either CK-free medium or 2.22 μM BA (96.7 and 91.7%, respectively) was higher than that of shoots grown on semi-solid medium (73%). Shoots from both TIS treatments developed good root systems, and all plantlets (100%) survived transfer to soil mix and acclimatization in the greenhouse. Plantlets established from shoots grown on 6.66 μM BA showed the lowest frequency of survival (60%). After 3 months, plants were transferred to field conditions. © 2011 Springer Science+Business Media B.V.","Cytokinin; Forest plant; Hyperhydricity; Liquid medium; Micropropagation; Semi-automation","Cytokinin; Forest plant; Hyperhydricity; Liquid medium; Micropropagation; Semi-automation; Artificial intelligence; Greenhouses; Liquids; Phenols; Plants (botany); Sugar (sucrose); Barium compounds; Tectona grandis",Article,Scopus,2-s2.0-84859421283
"Alvarez J.M., Labra J.E., Cifuentes F., Alor-Hérnandez G., Sánchez C., Luna J.A.G.","Towards a pan-european e-procurement platform to aggregate, publish and search public procurement notices powered by linked open data: The moldeas approach",2012,"International Journal of Software Engineering and Knowledge Engineering",19,10.1142/S0218194012400086,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863462509&doi=10.1142%2fS0218194012400086&partnerID=40&md5=2068ee8e045478ca9008666b45674ad4","This paper aims to describe a public procurement information platform which provides a unified pan-European system that exploits the aggregation of tender notices using linking open data and semantic web technologies. This platform requires a step-based method to deal with the requirements of the public procurement sector and the open government data initiative: (1) modeling the unstructured information included in public procurement notices (contracting authorities, organizations, contracts awarded, etc.); (2) enriching that information with the existing product classification systems and the linked data vocabularies; (3) publishing relevant information extracted out of the notices following the linking open data approach; (4) implementing enhanced services based on advanced algorithms and techniques like query expansion methods to exploit the information in a semantic way. Taking into account that public procurement notices contain different kinds of data like types of contract, region, duration, total amount, target enterprise, etc., various methods can be applied to expand user queries easing the access to the information and providing a more accurate information retrieval system. Nevertheless expanded user queries can involve an extra-time in the process of retrieving notices. That is why a performance evaluation is outlined to tune up the semantic methods and the generated queries providing a scalable and time-efficient system. Moreover, this platform is supposed to be especially relevant for SMEs that want to tender in the European Union (EU), easing their access to the information of the notices and fostering their participation in cross-border public procurement processes across Europe. Finally an example of use is provided to evaluate and compare the goodness and the improvement of the proposed platform with regard to the existing ones. © 2012 World Scientific Publishing Company.","commercial services; e-government; e-Procurement; linked open data; semantic technologies","Commercial services; e-Government; e-Procurement; linked open data; Semantic technologies; Artificial intelligence; Software engineering; Search engines",Article,Scopus,2-s2.0-84863462509
"Pérez J.A., Caires L., Pfenning F., Toninho B.","Linear logical relations for session-based concurrency",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",19,10.1007/978-3-642-28869-2_27,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859134524&doi=10.1007%2f978-3-642-28869-2_27&partnerID=40&md5=2b81670d4ff9bfdb14c181645176a9a4","In prior work we proposed an interpretation of intuitionistic linear logic propositions as session types for concurrent processes. The type system obtained from the interpretation ensures fundamental properties of session-based typed disciplines-most notably, type preservation, session fidelity, and global progress. In this paper, we complement and strengthen these results by developing a theory of logical relations. Our development is based on, and is remarkably similar to, that for functional languages, extended to an (intuitionistic) linear type structure. A main result is that well-typed processes always terminate (strong normalization). We also introduce a notion of observational equivalence for session-typed processes. As applications, we prove that all proof conversions induced by the logic interpretation actually express observational equivalences, and explain how type isomorphisms resulting from linear logic equivalences are realized by coercions between interface types of session-based concurrent systems. © 2012 Springer-Verlag.",,"Concurrent process; Concurrent systems; Functional languages; Fundamental properties; Intuitionistic linear logic; Linear logic; Linear types; Logical relations; Observational equivalences; Session types; Strong normalization; Type systems; Concurrent process; Concurrent systems; Functional languages; Fundamental properties; Intuitionistic linear logic; Logical relations; Observational equivalences; Strong normalization; Artificial intelligence; Computer circuits; Computational linguistics; Linear algebra; Reconfigurable hardware",Conference Paper,Scopus,2-s2.0-84859134524
"Stray B.J., van Vuuren J.H., Bezuidenhout C.N.","An optimisation-based seasonal sugarcane harvest scheduling decision support system for commercial growers in South Africa",2012,"Computers and Electronics in Agriculture",19,10.1016/j.compag.2012.01.009,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857705815&doi=10.1016%2fj.compag.2012.01.009&partnerID=40&md5=db92e0a9d127285a928a30267c62c037","An ongoing sugarcane decision support research project in South Africa is aimed at developing a decision support system capable of providing computerised support to those charged with the task of scheduling sugarcane harvesting operations in South Africa. In situations where the number of fields is large and when the conditions under which the crops are growing change frequently-for example as a result of climatic, biological or management-related events-computerised support is applicable. Commercial growers have provided data suitable for regression modelling of the parameters that govern the values and costs involved, and have participated in two consecutive preliminary system evaluation and development experiments conducted during the 2009 and 2010 harvesting seasons. The optimisation models underlying the decision support system are based on a time-dependent travelling salesman problem formulation and are solved approximately by means of a tabu search in a Microsoft Visual Basic for Applications (VBA) for Excel environment. The growers who participated in the evaluation experiments responded positively to the decision support system and stated that it may be useful to large-scale sugarcane producers as well as emerging growers. The authors' findings are that the decision support system provides support in practical sugarcane harvest scheduling and that one series of regression fits is required for each agroclimatically and management-wise homogenous area. © 2012 Elsevier B.V.","Metaheuristic; Regression analysis; Sugarcane; Travelling salesman problem","Decision supports; Evaluation experiments; Harvest scheduling; Metaheuristic; Microsoft visual basic for applications; Optimisation models; Regression modelling; South Africa; System evaluation; Time-dependent; Travelling salesman problem; Artificial intelligence; Decision support systems; Experiments; Harvesting; Regression analysis; Sugar cane; Tabu search; Traveling salesman problem; Scheduling; agricultural management; agricultural research; agroecology; crop production; decision support system; harvesting; heuristics; optimization; regression analysis; research program; software; sugar cane; South Africa",Article,Scopus,2-s2.0-84857705815
"Kung H.-Y., Chen C.-H., Ku H.-H.","Designing intelligent disaster prediction models and systems for debris-flow disasters in Taiwan",2012,"Expert Systems with Applications",19,10.1016/j.eswa.2011.11.083,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855865094&doi=10.1016%2fj.eswa.2011.11.083&partnerID=40&md5=8bc257d636068ba7506359297b6265dd","Effective disaster prediction relies on using correct disaster decision model to predict the disaster occurrence accurately. This study proposes three effective debris-flow prediction models and an inference engine to predict and decide the debris-flow occurrence in Taiwan. The proposed prediction models are based on linear regression, multivariate analysis, and back-propagation networks. To create a practical simulation environment, the decision database is the pre-analyzed 181 potential debris-flows in Taiwan. According to the simulation results, the prediction model based on back-propagation networks predicted the debris flow most accurately. Moreover, a Real-time Mobile Debris Flow Disaster Forecast System (RM(DF) 2) was implemented as a three-tier architecture consisting of mobile appliances, intelligent situation-aware agents and decision support servers based on the wireless/mobile Internet communications. The RM(DF) 2 system provides real-time communication between the disaster area and the rescue-control center, and effectively prevents and manages debris-flow disasters. © 2011 Elsevier Ltd. All rights reserved.","Back-propagation network; Debris-flow prediction models; Decision support system; Disaster prevention; Mobile multimedia communications","Backpropagation network; Debris flows; Debris-flow prediction models; Decision models; Decision supports; Disaster areas; Disaster prediction; Forecast systems; Internet communication; Mobile appliances; Mobile multimedia communications; Multi variate analysis; Prediction model; Real-time communication; Simulation environment; Situation-aware; Three-tier architecture; Artificial intelligence; Assembly; Backpropagation; Debris; Decision support systems; Disaster prevention; Disasters; Forecasting; Intelligent agents; Mathematical models; Mobile agents; Mobile telecommunication systems; Multimedia systems; Multivariant analysis; Computer simulation",Article,Scopus,2-s2.0-84855865094
"Pellegrini P., Stützle T., Birattari M.","A critical analysis of parameter adaptation in ant colony optimization",2012,"Swarm Intelligence",19,10.1007/s11721-011-0061-0,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855328718&doi=10.1007%2fs11721-011-0061-0&partnerID=40&md5=b25e8af98a42a1ffd6dcbac86b33aa40","Applying parameter adaptation means operating on parameters of an algorithm while it is tackling an instance. For ant colony optimization, several parameter adaptation methods have been proposed. In the literature, these methods have been shown to improve the quality of the results achieved in some particular contexts. In particular, they proved to be successful when applied to novel ant colony optimization algorithms for tackling problems that are not a classical testbed for optimization algorithms. In this paper, we show that the adaptation methods proposed so far do not improve, and often even worsen the performance when applied to high performing ant colony optimization algorithms for some classical combinatorial optimization problems. © 2011 Springer Science + Business Media, LLC.","Ant colony optimization; Parameter adaptation; Quadratic assignment problem; Traveling salesman problem","Adaptation methods; Ant Colony Optimization algorithms; Ant-colony optimization; Combinatorial optimization problems; Critical analysis; Optimization algorithms; Parameter adaptation; Quadratic assignment problems; Traveling salesman; Algorithms; Artificial intelligence; Combinatorial optimization; Constrained optimization; Traveling salesman problem",Article,Scopus,2-s2.0-84855328718
"Lin H.W., Nagalingam S.V., Kuik S.S., Murata T.","Design of a Global Decision Support System for a manufacturing SME: Towards participating in Collaborative Manufacturing",2012,"International Journal of Production Economics",19,10.1016/j.ijpe.2011.07.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855984150&doi=10.1016%2fj.ijpe.2011.07.001&partnerID=40&md5=3da10b5eb3654174734d9c9a12d39f32","This paper discusses the conceptual design of a Global Decision Support System for a manufacturing Small or Medium Enterprise (SM/E), which actively participates in Collaborative Manufacturing. In order to implement the proposed concept, a Web Services based system architecture is proposed to offer maximum interoperability between all the distributed participants of a Collaborative Manufacturing Network (CMN) and their management information systems. Furthermore, this conceptual design utilises a Collaborative decision-support model that effectively interacts with the decision-makers and the management information systems/tools exist in the network, and provides appropriate support to all necessary decision-making steps towards the attainment of the networks strategic goals, while making full benefits of the network resources. © 2011 Elsevier B.V.","Collaborative Manufacturing; Collaborative networks; Global Decision Support System; Multi-objective optimisation; System interoperability","Collaborative manufacturing; Collaborative network; Decision makers; Decision support models; Decision supports; Network resource; Strategic goals; System architectures; System interoperability; Artificial intelligence; Conceptual design; Decision making; Decision support systems; Knowledge management; Manufacture; Multiobjective optimization; Web services; Interoperability",Article,Scopus,2-s2.0-84855984150
"Buesing L., MacKe J.H., Sahani M.","Learning stable, regularised latent models of neural population dynamics",2012,"Network: Computation in Neural Systems",19,10.3109/0954898X.2012.677095,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861897852&doi=10.3109%2f0954898X.2012.677095&partnerID=40&md5=f69927417e69e7d578675062ae268598","Ongoing advances in experimental technique are making commonplace simultaneous recordings of the activity of tens to hundreds of cortical neurons at high temporal resolution. Latent population models, including Gaussian-process factor analysis and hidden linear dynamical system (LDS) models, have proven effective at capturing the statistical structure of such data sets. They can be estimated efficiently, yield useful visualisations of population activity, and are also integral building-blocks of decoding algorithms for brain-machine interfaces (BMI). One practical challenge, particularly to LDS models, is that when parameters are learned using realistic volumes of data the resulting models often fail to reflect the true temporal continuity of the dynamics; and indeed may describe a biologically-implausible unstable population dynamic that is, it may predict neural activity that grows without bound. We propose a method for learning LDS models based on expectation maximisation that constrains parameters to yield stable systems and at the same time promotes capture of temporal structure by appropriate regularisation. We show that when only little training data is available our method yields LDS parameter estimates which provide a substantially better statistical description of the data than alternatives, whilst guaranteeing stable dynamics. We demonstrate our methods using both synthetic data and extracellular multi-electrode recordings from motor cortex. © 2012 Informa Healthcare Ltd.","Cortical microcircuitry; Motor control","algorithm; animal; article; artificial intelligence; artificial neural network; biological model; computer interface; computer simulation; electrode implant; motor cortex; nerve cell network; normal distribution; physiology; population dynamics; rhesus monkey; statistical analysis; statistical model; Algorithms; Animals; Artificial Intelligence; Computer Simulation; Data Interpretation, Statistical; Electrodes, Implanted; Likelihood Functions; Linear Models; Macaca mulatta; Models, Neurological; Motor Cortex; Nerve Net; Neural Networks (Computer); Normal Distribution; Population Dynamics; User-Computer Interface",Article,Scopus,2-s2.0-84861897852
"Adeloye A.J., Rustum R., Kariyama I.D.","Neural computing modeling of the reference crop evapotranspiration",2012,"Environmental Modelling and Software",19,10.1016/j.envsoft.2011.10.012,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855497539&doi=10.1016%2fj.envsoft.2011.10.012&partnerID=40&md5=04123e130cf330513a4955f049fc85e0","Reference crop evapotranspiration (ET o) estimation is of importance in irrigation water management for the calculation of crop water requirements and its scheduling, in rainfall-runoff modeling and in numerous other water resources studies. Due to its importance, several direct and indirect methods have been employed to determine the reference crop evapotranspiration but success has been limited because the direct measurement methods lack in precision and accuracy due to scale issues and other problems, while some of the more accurate indirect methods, e.g. the Penman-Monteith benchmark model, are extremely non-linear and require weather input data that are not routinely monitored. In such situations, artificial intelligence (AI), neural computing techniques that are able to accurately map complex, non-linear input-output relationships offer a useful alternative. This paper has used the Kohonen Self-Organizing Map (SOM), unsupervised artificial neural networks, to develop prediction models for the ET o. This was achieved by using the powerful clustering capability of the SOM to analyze the multi-dimensional data array comprising the estimated ET o (based on the FAO Penman-Monteith model) and different subsets of climatic variables known to affect it. The findings indicate that the SOM-based ET o estimates, even when forced with fewer input data variables, were in good agreement with those obtained using the conventional FAO Penman-Monteith formulation employing the full complement of weather data. Further comparisons were carried out between the SOM model estimates of the ET o and those based on the use of feed-forward back propagation supervised artificial neural networks and the results showed that the SOM estimates were superior. Finally, the SOM-based estimates were also found to be significantly superior to those estimated using established empirical ET o methods recommended in the literature for situations where the full complement of input weather needed to drive the Penman-Monteith model are unavailable. This offers significant potential for more accurate estimation of the ET o in data scarce regions of the world. © 2011 Elsevier Ltd.","Artificial intelligence models; Crop water requirements; FAO Penman-Monteith method; Kohonen self-organizing map; Neural networks; Reference crop evapotranspiration","Accurate estimation; Benchmark models; Climatic variables; Crop evapotranspiration; Crop water requirements; Direct and indirect methods; Direct measurement method; FAO Penman-Monteith method; Feed-forward back propagation; Indirect methods; Input datas; Input-output; Irrigation water management; Kohonen self-organizing map; Kohonen self-organizing maps; Model estimates; Multidimensional data; Neural computing; Penman-Monteith; Prediction model; Rainfall-runoff modeling; Reference crop evapotranspiration; Weather data; Weather inputs; Conformal mapping; Crops; Estimation; Evapotranspiration; Input output programs; Irrigation; Mathematical models; Rain; Runoff; Scheduling; Water management; Water supply; Neural networks; accuracy assessment; artificial intelligence; artificial neural network; benchmarking; climate conditions; cluster analysis; crop plant; empirical analysis; evapotranspiration; irrigation system; precision; rainfall-runoff modeling; water management; water resource; water use",Article,Scopus,2-s2.0-84855497539
"Ruiz M.C., Romero E., Pérez M.A., Fernández I.","Development and application of a multi-criteria spatial decision support system for planning sustainable industrial areas in Northern Spain",2012,"Automation in Construction",19,10.1016/j.autcon.2011.09.009,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857369312&doi=10.1016%2fj.autcon.2011.09.009&partnerID=40&md5=2ba0773fd1fbfe875b85e276e258a3d3","The planning, design and construction of an industrial area is quite a complicated and long process due to the scope of the action itself. The drive for a new theory on sustainable industrial areas requires acting upon all of the phases of their life cycles. Industrial location poses a multivariate and semi-structured strategic decision. This article analyzes influential location factors and proposes a multicriteria evaluation model aimed at guaranteeing the viability of industrial areas with their surroundings. The spatial character intrinsic to the problem leads to the design and construction of a Spatial Decision Support System (SDSS) based on a Geographic Information System (GIS) platform and the integration of other compatible tools. The system is applied to a district of 646.2 km 2 located in Cantabria (Northern Spain). The results are discussed with digital maps which differentiate the zones according to their suitability for industrial area location using sustainability criteria. © 2011 Elsevier B.V. All rights reserved.","Industrial area; Planning; Spatial system; Sustainability","Cantabria; Design and construction; Digital map; Industrial area; Industrial location; Location factors; Multi-criteria evaluation; Multi-criteria Spatial Decision Support Systems; New theory; Semi-structured; Spatial decision support systems; Spatial system; Strategic decisions; Sustainability criteria; Artificial intelligence; Decision support systems; Geographic information systems; Planning; Regional planning; Sustainable development; Urban planning; Industry",Conference Paper,Scopus,2-s2.0-84857369312
"Jonoski A., Popescu I.","Distance Learning in Support of Water Resources Management: An Online Course on Decision Support Systems in River Basin Management",2012,"Water Resources Management",19,10.1007/s11269-011-9959-y,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858618552&doi=10.1007%2fs11269-011-9959-y&partnerID=40&md5=18dc0dda9e5587ecdf54f35d249f276a","This paper presents the conceptualisation, design and implementation of an online course on the topic of Decision Support Systems in River Basin Management. The need for development of such a course has been recognised, as activities in the field of water resources planning and management increasingly depend on decision support methods such as simulation, optimisation and Multi Criteria Analysis (MCA). The online learning approach is particularly needed for continuous professional development and life-long learning of professionals active in this field, and especially for those coming from developing countries. The course was developed and implemented following the competence-based learning approach, supported by the EU FP 7 educational research project named TenCompetence, which also provided the learning platform for deploying and delivering the course. The paper presents the course design, implementation and evaluation by the course participants, with special focus on the course content and the developed learning resources. Participants' evaluations show high appreciation for the course, but they also highlight areas for future improvements. © 2011 Springer Science+Business Media B.V.","Decision support systems (DSSs); Multi criteria analysis; Online learning; Optimisation; River basin management; Simulation","Multi Criteria Analysis; Online learning; Optimisations; River basin management; Simulation; Artificial intelligence; Curricula; Decision support systems; Developing countries; Optimization; Water management; E-learning; basin management; computer simulation; decision support system; multicriteria analysis; optimization; river basin; water management; water planning; water resource",Article,Scopus,2-s2.0-84858618552
"Memisevic R., Sigal L., Fleet D.J.","Shared kernel information embedding for discriminative inference",2012,"IEEE Transactions on Pattern Analysis and Machine Intelligence",19,10.1109/TPAMI.2011.154,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857416741&doi=10.1109%2fTPAMI.2011.154&partnerID=40&md5=0a8adf80287f029d256a993143b4e54a","Latent variable models, such as the GPLVM and related methods, help mitigate overfitting when learning from small or moderately sized training sets. Nevertheless, existing methods suffer from several problems: 1) complexity, 2) the lack of explicit mappings to and from the latent space, 3) an inability to cope with multimodality, and 4) the lack of a well-defined density over the latent space. We propose an LVM called the Kernel Information Embedding (KIE) that defines a coherent joint density over the input and a learned latent space. Learning is quadratic, and it works well on small data sets. We also introduce a generalization, the shared KIE (sKIE), that allows us to model multiple input spaces (e.g., image features and poses) using a single, shared latent representation. KIE and sKIE permit missing data during inference and partially labeled data during learning. We show that with data sets too large to learn a coherent global model, one can use the sKIE to learn local online models. We use sKIE for human pose inference. © 2012 IEEE.","inference; kernel information embedding; Latent variable models; mutual information; nonparametric","inference; Information embedding; Latent variable models; Mutual informations; Non-parametric; Artificial intelligence; Computer vision; article; automated pattern recognition; human; methodology; theoretical model; Humans; Models, Theoretical; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84857416741
"Li R., Seçkiner S.U., He D., Bechhoefer E., Menon P.","Gear fault location detection for split torque gearbox using AE sensors",2012,"IEEE Transactions on Systems, Man and Cybernetics Part C: Applications and Reviews",19,10.1109/TSMCC.2011.2182609,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871739242&doi=10.1109%2fTSMCC.2011.2182609&partnerID=40&md5=56906195575760a94bd06f6fef7b3d9a","In comparison with a traditional planetary gearbox, the split torque gearbox (STG) potentially offers lower weight, increased reliability, and improved efficiency. These benefits have driven helicopter object exchange models (OEMs) to develop products using STG. However, the unique structure of the STG creates a problem on how to locate the gear faults in an STG. As of today, only limited research on STG fault detection using vibration and acoustic emission (AE) sensors has been conducted. In this paper, an effective gear fault location detection methodology using AE sensors for STG is presented. The methodology uses wavelet transform to process AE sensor signals at different locations to determine the arrival time of the AE bursts. By analyzing the arrival time of the AE bursts, the gear fault location can be determined. The parameters of the wavelets are optimized by using an ant colony optimization algorithm. Real seeded gear fault experimental tests on a notional STG are conducted. AE signals at different locations of the gearbox with both healthy and damaged output driving gears are collected simultaneously to determine the location of the damaged gear. Experimental results have shown the effectiveness of the presented methodology. © 1998-2012 IEEE.","Acoustic emission (AE) sensor; ant colony optimization (ACO); gear fault location detection; split torque gearbox (STG); wavelet transform (WT)","Acoustic emission sensors; Ae sensors; AE signals; Ant Colony Optimization (ACO); Ant Colony Optimization algorithms; Arrival time; Experimental test; Gear faults; Location detection; Object exchange model; Planetary gearboxes; split torque gearbox (STG); Acoustic emissions; Algorithms; Artificial intelligence; Signal detection; Wavelet transforms; Sensors",Article,Scopus,2-s2.0-84871739242
"Farrell T.L., Poquet L., Dew T.P., Barber S., Williamson G.","Predicting phenolic acid absorption in Caco-2 cells: A theoretical permeability model and mechanistic study",2012,"Drug Metabolism and Disposition",19,10.1124/dmd.111.041665,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856016533&doi=10.1124%2fdmd.111.041665&partnerID=40&md5=3f2ecccc1f29292f0389ff7137ed1bd3","There is a considerable need to rationalize the membrane permeability and mechanism of transport for potential nutraceuticals. The aim of this investigation was to develop a theoretical permeability equation, based on a reported descriptive absorption model, enabling calculation of the transcellular component of absorption across Caco-2 monolayers. Published data for Caco-2 permeability of 30 drugs transported by the transcellular route were correlated with the descriptors 1-octanol/water distribution coefficient (log D, pH 7.4) and size, based on molecular mass. Nonlinear regression analysis was used to derive a set of model parameters a′, β′, and b′ with an integrated molecular mass function. The new theoretical transcellular permeability (TTP) model obtained a good fit of the published data (R 2 = 0.93) and predicted reasonably well (R 2 = 0.86) the experimental apparent permeability coefficient (P app) for nine non-training set compounds reportedly transported by the transcellular route. For the first time, the TTP model was used to predict the absorption characteristics of six phenolic acids, and this original investigation was supported by in vitro Caco-2 cell mechanistic studies, which suggested that deviation of the P app value from the predicted transcellular permeability (P app trans) may be attributed to involvement of active uptake, efflux transporters, or paracellular flux. Copyright © 2012 by The American Society for Pharmacology and Experimental Therapeutics.",,"3,4 dimethoxycinnamic acid; 3,5 di o caffeoylquinic acid; 5 feruloylquinic acid; 5 o caffeoylquinic acid; alprenolol; caffeic acid; ceftriaxone; corticosterone; coumarin; dexamethasone; diazepam; felodipine; ferulic acid; hydrocortisone; hydroxyacid; imipramine; ketoprofen; lidocaine; metoprolol; mibefradil; morphine; nitrendipine; phenazone; phenytoin; piroxicam; practolol; propranolol; remikiren; sulpiride; unclassified drug; unindexed drug; article; cell membrane permeability; cell strain CACO 2; cell transport; drug absorption; drug penetration; drug transport; human; human cell; in vitro study; molecular weight; nonlinear regression analysis; priority journal; Artificial Intelligence; Caco-2 Cells; Cell Membrane Permeability; Cinnamates; Enterocytes; Humans; Hydrophobic and Hydrophilic Interactions; Intestinal Absorption; Kinetics; Models, Biological; Molecular Conformation; Osmolar Concentration; Phenols",Article,Scopus,2-s2.0-84856016533
"Li C.-H., Ho H.-H., Liu Y.-L., Lin C.-T., Kuo B.-C., Taur J.-S.","An automatic method for selecting the parameter of the normalized kernel function to support vector machines",2012,"Journal of Information Science and Engineering",19,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862959226&partnerID=40&md5=0ac61433087b399554c0333c8992027e","Soft-margin support vector machine (SVM) is one of the most powerful techniques for supervised classification. However, the performances of SVMs are based on choosing the proper kernel functions or proper parameters of a kernel function. It is extremely time consuming by applying the k-fold cross-validation (CV) to choose the almost best parameter. Nevertheless, the searching range and fineness of the grid method should be determined in advance. In this paper, an automatic method for selecting the parameter of the normalized kernel function is proposed. In the experimental results, it costs very little time than k-fold cross-validation for selecting the parameter by our proposed method. Moreover, the corresponding soft-margin SVMs can obtain more accurate or at least equal performance than the soft-margin SVMs by applying k-fold cross-validation to determine the parameters.","K-fold cross-validation; Kernel method; Normalized kernel; Optimal kernel; Soft-margin support vector machine; SVM","Cross validation; Kernel methods; Normalized kernel; Optimal kernel; Support vector; SVM; Artificial intelligence; Support vector machines",Article,Scopus,2-s2.0-84862959226
"Mattila J., Koikkalainen J., Virkki A., Van Gils M., Lötjönen J.","Design and application of a generic clinical decision support system for multiscale data",2012,"IEEE Transactions on Biomedical Engineering",19,10.1109/TBME.2011.2170986,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84555196791&doi=10.1109%2fTBME.2011.2170986&partnerID=40&md5=d167c4f0a098aba99d1593d5a09e84c3","Medical research and clinical practice are currently being redefined by the constantly increasing amounts of multiscale patient data. New methods are needed to translate them into knowledge that is applicable in healthcare. Multiscale modeling has emerged as a way to describe systems that are the source of experimental data. Usually, a multiscale model is built by combining distinct models of several scales, integrating, e.g., genetic, molecular, structural, and neuropsychological models into a composite representation. We present a novel generic clinical decision support system, which models a patients disease state statistically from heterogeneous multiscale data. Its goal is to aid in diagnostic work by analyzing all available patient data and highlighting the relevant information to the clinician. The system is evaluated by applying it to several medical datasets and demonstrated by implementing a novel clinical decision support tool for early prediction of Alzheimers disease. © 2011 IEEE.","Clinical diagnosis; decision support systems; software architecture; supervised learning","Alzheimers disease; Clinical decision support; Clinical decision support systems; Clinical diagnosis; Clinical practices; Decision supports; Design and application; Disease state; Early prediction; Experimental data; Medical data sets; Medical research; Multi-scale Modeling; Multiscale models; Multiscales; Neuropsychological; Patient data; Artificial intelligence; Diagnosis; Health care; Hospital data processing; Software architecture; Supervised learning; Decision support systems; Alzheimer disease; article; clinical practice; computer program; decision support system; early diagnosis; health care system; human; learning; model; prediction; Alzheimer Disease; Data Mining; Decision Support Systems, Clinical; Diagnosis, Computer-Assisted; Health Records, Personal; Humans; Software; Software Design",Article,Scopus,2-s2.0-84555196791
"Virtanen S., Klami A., Khan S.A., Kaski S.","Bayesian group factor analysis",2012,"Journal of Machine Learning Research",19,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877727222&partnerID=40&md5=6264d5a1b5cc161a4b6e4d6a54344b78","We introduce a factor analysis model that summarizes the dependencies between observed variable groups, instead of dependencies between individual variables as standard factor analysis does. A group may correspond to one view of the same set of objects, one of many data sets tied by co-occurrence, or a set of alternative variables collected from statistics tables to measure one property of interest. We show that by assuming groupwise sparse factors, active in a subset of the sets, the variation can be decomposed into factors explaining relationships between the sets and factors explaining away set-specific variation. We formulate the assumptions in a Bayesian model providing the factors, and apply the model to two data analysis tasks, in neuroimaging and chemical systems biology. © Copyright 2012 by the authors.",,"Artificial intelligence; Bayesian networks; Chemical analysis; Multivariant analysis; Neuroimaging; Bayesian; Bayesian model; Chemical systems; Co-occurrence; Factor analysis model; Factor analysis",Conference Paper,Scopus,2-s2.0-84877727222
"Maienschein-Cline M., Zhou J., White K.P., Sciammas R., Dinner A.R.","Discovering transcription factor regulatory targets using gene expression and binding data",2012,"Bioinformatics",19,10.1093/bioinformatics/btr628,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862963292&doi=10.1093%2fbioinformatics%2fbtr628&partnerID=40&md5=7f2c75ac9c60d703ff404b67147e597e","Motivation: Identifying the target genes regulated by transcription factors (TFs) is the most basic step in understanding gene regulation. Recent advances in high-throughput sequencing technology, together with chromatin immunoprecipitation (ChIP), enable mapping TF binding sites genome wide, but it is not possible to infer function from binding alone. This is especially true in mammalian systems, where regulation often occurs through long-range enhancers in gene-rich neighborhoods, rather than proximal promoters, preventing straightforward assignment of a binding site to a target gene. Results: We present EMBER (Expectation Maximization of Binding and Expression pRofiles), a method that integrates high-throughput binding data (e.g. ChIP-chip or ChIP-seq) with gene expression data (e.g. DNA microarray) via an unsupervised machine learning algorithm for inferring the gene targets of sets of TF binding sites. Genes selected are those that match overrepresented expression patterns, which can be used to provide information about multiple TF regulatory modes. We apply the method to genome-wide human breast cancer data and demonstrate that EMBER confirms a role for the TFs estrogen receptor alpha, retinoic acid receptors alpha and gamma in breast cancer development, whereas the conventional approach of assigning regulatory targets based on proximity does not. Additionally, we compare several predicted target genes from EMBER to interactions inferred previously, examine combinatorial effects of TFs on gene regulation and illustrate the ability of EMBER to discover multiple modes of regulation. © The Author 2011. Published by Oxford University Press. All rights reserved.",,"Mammalia; estrogen receptor alpha; retinoic acid receptor; retinoic acid receptor alpha; transcription factor; algorithm; article; artificial intelligence; DNA microarray; gene expression profiling; gene expression regulation; human; metabolism; promoter region; protein binding; regulatory sequence; Algorithms; Artificial Intelligence; Estrogen Receptor alpha; Gene Expression Profiling; Gene Expression Regulation; Humans; Oligonucleotide Array Sequence Analysis; Promoter Regions, Genetic; Protein Binding; Receptors, Retinoic Acid; Regulatory Sequences, Nucleic Acid; Transcription Factors",Article,Scopus,2-s2.0-84862963292
"Ren Z.-G., Feng Z.-R., Zhang A.-M.","Fusing ant colony optimization with Lagrangian relaxation for the multiple-choice multidimensional knapsack problem",2012,"Information Sciences",19,10.1016/j.ins.2011.07.033,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80055030614&doi=10.1016%2fj.ins.2011.07.033&partnerID=40&md5=da80393fb755859a239bd873cdaea522","The multiple-choice multidimensional knapsack problem (MMKP) concerns a wide variety of practical problems. It is strongly constrained and NP-hard; thus searching for an efficient heuristic approach for MMKP is of great significance. In this study, we attempt to solve MMKP by fusing ant colony optimization (ACO) with Lagrangian relaxation (LR). The algorithm used here follows the algorithmic scheme of max-min ant system for its outstanding performance in solving many other combinatorial optimization problems. The Lagrangian value of the item in MMKP, obtained from LR, is used as the heuristic factor in ACO since it performs best among the six domain-based heuristic factors we define. Furthermore, a novel infeasibility index is proposed for the development of a new repair operator, which converts possibly infeasible solutions into feasible ones. The proposed algorithm was compared with four existing algorithms by applying them to three groups of instances. Computational results demonstrate that the proposed algorithm is capable of producing competitive solutions. © 2011 Elsevier Inc. All rights reserved.","Ant colony optimization; Lagrangian relaxation; Multiple-choice multidimensional knapsack problem","Ant-colony optimization; Combinatorial optimization problems; Computational results; Heuristic approach; Lagrangian; LaGrangian relaxation; Lagrangian relaxations; Max-Min Ant System; Multidimensional knapsack problems; NP-hard; Practical problems; Repair operator; Artificial intelligence; Combinatorial optimization; Heuristic methods; Integer programming; Lagrange multipliers; Mathematical operators; Optimization; Algorithms",Article,Scopus,2-s2.0-80055030614
"Ahmed B.S., Zamli K.Z., Lim C.P.","Constructing a t-way interaction test suite using the Particle Swarm Optimization approach",2012,"International Journal of Innovative Computing, Information and Control",19,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856956194&partnerID=40&md5=c3e60a2ebf206edf45250ebe62fe1130","This paper presents the design and implementation of a new t-way test generation strategy, known as the Particle Swarm Test Generator (PSTG). Complementing the existing work on t-way testing strategies, PSTG serves as our research vehicle to investigate the applicability of Particle Swarm Optimization for t-way test data generation. The experimental results demonstrate that PSTG is capable of outperforming some of the existing strategies as far as the test size is concerned. Additionally, the evaluation also indicates the effectiveness of PSTG in generating an efficient test suite for testing consideration. © 2012 ICIC International.","Artificial intelligence; Interaction testing; Particle swarm optimization; T-way testing; Testing","Interaction testing; Particle swarm; Research vehicles; T-way testing; Test data generation; Test generations; Test size; Artificial intelligence; Testing; Particle swarm optimization (PSO)",Article,Scopus,2-s2.0-84856956194
"Ji M., Han J.","A variance minimization criterion to active learning on graphs",2012,"Journal of Machine Learning Research",19,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954491151&partnerID=40&md5=5323a5693ac881fd342f0ff61af11e5f","We consider the problem of active learning over the vertices in a graph, without feature representation. Our study is based on the common graph smoothness assumption, which is formulated in a Gaussian random field model. We analyze the probability distribution over the unlabeled vertices conditioned on the label information, which is a multivariate normal with the mean being the harmonic solution over the field. Then we select the nodes to label such that the total variance of the distribution on the unlabeled data, as well as the expected prediction error, is minimized. In this way, the classifier we obtain is theoretically more robust. Compared with existing methods, our algorithm has the advantage of selecting data in a batch offline mode with solid theoretical support. We show improved performance over existing label selection criteria on several real world data sets.",,"Gaussian distribution; Probability distributions; Virtual reality; Feature representation; Gaussian random fields; Harmonic solution; Label information; Multivariate normal; Prediction errors; Selection criteria; Variance minimization; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84954491151
"Pérez J.L., Cladera A., Rabuñal J.R., Martínez-Abella F.","Optimization of existing equations using a new Genetic Programming algorithm: Application to the shear strength of reinforced concrete beams",2012,"Advances in Engineering Software",19,10.1016/j.advengsoft.2012.02.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861865268&doi=10.1016%2fj.advengsoft.2012.02.008&partnerID=40&md5=36b8a0d0ab0f2abd048ab58c10ab79fe","A method based on Genetic Programming (GP) to improve previously known empirical equations is presented. From a set of experimental data, the GP may improve the adjustment of such formulas through the symbolic regression technique. Through a set of restrictions, and the indication of the terms of the expression to be improved, GP creates new individuals. The methodology allows us to study the need of including new variables in the expression. The proposed method is applied to the shear strength of concrete beams. The results show a marked improvement using this methodology in relation to the classic GP and international code procedures. © 2012 Civil-Comp Ltd and Elsevier Ltd. All rights reserved.","Artificial intelligence; Concrete; Genetic Programming; Regression analysis; Shear strength; Structural engineering","Artificial intelligence; Concrete beams and girders; Concretes; Genetic programming; Regression analysis; Shear strength; Structural design; Empirical equations; Experimental data; International codes; Programming algorithms; Reinforced concrete beams; Strength of concrete; Symbolic regression; Genetic algorithms",Conference Paper,Scopus,2-s2.0-84861865268
"Rao V.S.H., Kumar M.N.","A new intelligence-based approach for computer-aided diagnosis of dengue fever",2012,"IEEE Transactions on Information Technology in Biomedicine",19,10.1109/TITB.2011.2171978,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856823023&doi=10.1109%2fTITB.2011.2171978&partnerID=40&md5=1368c8c261fd11202b76725b1285d81e","Identification of the influential clinical symptoms and laboratory features that help in the diagnosis of dengue fever (DF) in early phase of the illness would aid in designing effective public health management and virological surveillance strategies. Keeping this as our main objective, we develop in this paper a new computational intelligence-based methodology that predicts the diagnosis in real time, minimizing the number of false positives and false negatives. Our methodology consists of three major components: 1) a novel missing value imputation procedure that can be applied on any dataset consisting of categorical (nominal) and/or numeric (real or integer); 2) a wrapper-based feature selection method with genetic search for extracting a subset of most influential symptoms that can diagnose the illness; and 3) an alternating decision tree method that employs boosting for generating highly accurate decision rules. The predictivemodels developed using our methodology are found to be more accurate than the state-of-theart methodologies used in the diagnosis of the DF. © 2012 IEEE.","Alternating decision trees; Classification; Clinical diagnosis; Dengue fever (DF); Features selection; Genetic search; Imputation; Prediction","Alternating decision trees; Clinical diagnosis; Dengue fever (DF); Features selection; Genetic search; Imputation; Artificial intelligence; Classification (of information); Computer aided diagnosis; Decision trees; Forecasting; Forestry; Feature extraction; Classification; Diagnosis; Forecasts; Forestry; Information Retrieval; Trees; adolescent; adult; algorithm; article; child; computer assisted diagnosis; decision tree; dengue; factual database; human; methodology; nonparametric test; pathophysiology; preschool child; Adolescent; Adult; Algorithms; Child; Child, Preschool; Databases, Factual; Decision Trees; Dengue; Diagnosis, Computer-Assisted; Humans; Statistics, Nonparametric",Article,Scopus,2-s2.0-84856823023
"Bareinboim E., Pearl J.","Controlling selection bias in causal inference",2012,"Journal of Machine Learning Research",19,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894404372&partnerID=40&md5=ed794b4ede795bd074d2ff78b6f6cbe2","Selection bias, caused by preferential exclusion of samples from the data, is a major obstacle to valid causal and statistical inferences; it cannot be removed by randomized experiments and can hardly be detected in either experimental or observational studies. This paper highlights several graphical and algebraic methods capable of mitigating and sometimes eliminating this bias. These nonparametric methods generalize previously reported results, and identify the type of knowledge that is needed for reasoning in the presence of selection bias. Specifically, we derive a general condition together with a procedure for deciding recoverability of the odds ratio (OR) from s-biased data. We show that recoverability is feasible if and only if our condition holds. We further offer a new method of controlling selection bias using instrumental variables that permits the recovery of other effect measures besides OR.",,"Artificial intelligence; Algebraic method; Causal inferences; Instrumental variables; Nonparametric methods; Observational study; Randomized experiments; Selection bias; Statistical inference; Algebra",Conference Paper,Scopus,2-s2.0-84894404372
"Buşoniu L., Munos R.","Optimistic planning for Markov decision processes",2012,"Journal of Machine Learning Research",19,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933046398&partnerID=40&md5=65a6cdf6256366c51eb16655e1660fbb","The reinforcement learning community has recently intensified its interest in online planning methods, due to their relative independence on the state space size. However, tight near-optimality guarantees are not yet available for the general case of stochastic Markov decision processes and closed-loop, state-dependent planning policies. We therefore consider an algorithm related to AO∗that optimistically explores a tree representation of the space of closed-loop policies, and we analyze the near-optimality of the action it returns after n tree node expansions. While this optimistic planning requires a finite number of actions and possible next states for each transition, its asymptotic performance does not depend directly on these numbers, but only on the subset of nodes that significantly impact near-optimal policies. We characterize this set by introducing a novel measure of problem complexity, called the near-optimality exponent. Specializing the exponent and performance bound for some interesting classes of MDPs illustrates the algorithm works better when there are fewer near-optimal policies and less uniform transition probabilities.",,"Artificial intelligence; Forestry; Markov processes; Reinforcement learning; Stochastic systems; Asymptotic performance; Markov Decision Processes; Near-optimal policies; Performance bounds; Planning policies; Problem complexity; Transition probabilities; Tree representation; Trees (mathematics)",Conference Paper,Scopus,2-s2.0-84933046398
"Depeursinge A., Foncubierta-Rodriguez A., Van de Ville D., Müller H.","Multiscale lung texture signature learning using the Riesz transform",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",19,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872916361&partnerID=40&md5=8264e2b6e080f547f9fed0d4c66f3b54","Texture–based computerized analysis of high–resolution computed tomography images from patients with interstitial lung diseases is introduced to assist radiologists in image interpretation. The cornerstone of our approach is to learn lung texture signatures using a linear combination of N–th order Riesz templates at multiple scales. The weights of the linear combination are derived from one–versus–all support vector machines. Steerability and multiscale properties of Riesz wavelets allow for scale and rotation covariance of the texture descriptors with infinitesimal precision. Orientations are normalized among texture instances by locally aligning the Riesz templates, which is carried out analytically. The proposed approach is compared with state–of–the–art texture attributes and shows significant improvement in classification performance with an average area under receiver operating characteristic curves of 0.94 for five lung tissue classes. The derived lung texture signatures illustrate optimal class–wise discriminative properties. © Springer-Verlag Berlin Heidelberg 2012.","Computer–aided diagnosis; High–resolution computed tomography; Interstitial lung diseases; Riesz; Steerability; Texture analysis","Computer aided diagnosis; Computerized tomography; Diagnosis; Medical computing; Medical imaging; Pulmonary diseases; Tomography; Classification performance; Computed tomography images; Computerized analysis; Interstitial lung disease; Receiver operating characteristic curves; Riesz; Steerability; Texture analysis; Biological organs; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; computer assisted tomography; human; image quality; lung; lung disease; methodology; radiography; reproducibility; sensitivity and specificity; Algorithms; Artificial Intelligence; Humans; Lung; Lung Diseases; Pattern Recognition, Automated; Radiographic Image Enhancement; Radiographic Image Interpretation, Computer-Assisted; Reproducibility of Results; Sensitivity and Specificity; Tomography, X-Ray Computed",Conference Paper,Scopus,2-s2.0-84872916361
"Kadlec R.H.","Constructed marshes for nitrate removal",2012,"Critical Reviews in Environmental Science and Technology",19,10.1080/10643389.2010.534711,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878578113&doi=10.1080%2f10643389.2010.534711&partnerID=40&md5=56f606761f87c19e2a7da7ab51c2c7f4","Large numbers of free water surface treatment wetlands are in use for nitrate reduction. Target applications are field runoff, river and stream improvement, and enhancement of wastewater treatment plants. In total, an extensive database now exists, in many publications and operating reports.Microcosms and mesocosms are not included here because of the lack of transferability to design. A first-order areal model is appropriate, to be implemented with appropriate temperature, hydraulic efficiency, and flow pattern. Annual average rate constants at 20°C have a median of 25 m/year. Performance is better at higher water temperatures, with a modified Arrhenius temperature factor of 1.106. Measured values of the tanks-in-series (TIS) parameter average N = 4.4 TIS. Higher rate coefficients are associated with emergent soft tissue vegetation, and lower efficiencies with submergent vegetation, unvegetated open water, and forested wetlands. Carbon availability can limit denitrification at high nitrate loadings; however, wetlands produce carbon in sufficient quantities to support typical municipal and agricultural loads. Design may be for load reduction or concentration reduction, with the latter requiring larger wetlands. Significant ancillary benefits of ecological diversity and wildlife habitat are certain to accompany the project. A small negative greenhouse gas penalty, which accrues to all new wetlands, is not an important factor. Economic issues may include land cost and pumping cost. Constructed marshes are an ecologically and economically attractive method for reducing nitrate levels in surface waters. Copyright © Taylor & Francis Group, LLC.","Design; Economics; Hydrology; Nitrate; Performance; Treatment marshes","Artificial intelligence; Carbon; Design; Economics; Flow patterns; Greenhouse gases; Hydrology; Nitrates; Rate constants; Surface treatment; Surface waters; Vegetation; Wastewater treatment; Water treatment; Wetlands; Arrhenius temperature; Concentration reduction; Ecological diversity; Free-water surface treatment; Hydraulic efficiency; Performance; Treatment marshes; Wastewater treatment plants; Nitrogen removal",Article,Scopus,2-s2.0-84878578113
"Van Der Maaten L., Hendriks E.","Action unit classification using active appearance models and conditional random fields",2012,"Cognitive Processing",19,10.1007/s10339-011-0419-7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870191881&doi=10.1007%2fs10339-011-0419-7&partnerID=40&md5=09a04a30d1a059998fd72288c149adfc","In this paper, we investigate to what extent modern computer vision and machine learning techniques can assist social psychology research by automatically recognizing facial expressions. To this end, we develop a system that automatically recognizes the action units defined in the facial action coding system (FACS). The system uses a sophisticated deformable template, which is known as the active appearance model, to model the appearance of faces. The model is used to identify the location of facial feature points, as well as to extract features from the face that are indicative of the action unit states. The detection of the presence of action units is performed by a time series classification model, the linear-chain conditional random field. We evaluate the performance of our system in experiments on a large data set of videos with posed and natural facial expressions. In the experiments, we compare the action units detected by our approach with annotations made by human FACS annotators. Our results show that the agreement between the system and human FACS annotators is higher than 90% and underlines the potential of modern computer vision and machine learning techniques to social psychology research. We conclude with some suggestions on how systems like ours can play an important role in research on social signals. © The Author(s) 2012.","Active appearance models; Conditional random fields; Facial action coding system; Facial expressions","action unit; active appearance model; algorithm; article; coding; computer; Computer vision; conditional random field; controlled study; data processing; experiment; face; Facial Action Coding System; facial expression; group dynamics; human; machine learning; priority journal; sample; social psychology; social signal; statistical model; videorecording; visual memory; artificial intelligence; automated pattern recognition; methodology; Artificial Intelligence; Facial Expression; Humans; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84870191881
"Iqbal M., Browne W.N., Zhang M.","XCSR with computed continuous action",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",18,10.1007/978-3-642-35101-3_30,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871390939&doi=10.1007%2f978-3-642-35101-3_30&partnerID=40&md5=6beba8a4e359984269394569c21b89d5","Wilson extended XCS with interval based conditions to XCSR to handle real valued inputs. However, the possible actions must always be determined in advance. Yet domains such as robot control require numerical actions, so that neither XCS nor XCSR with their discrete actions can yield high performance. In the work presented here, genetic programming-based representation is used for the first time to compute continuous action in XCSR. This XCSR version has been examined on a simple one-dimensional but non-linear testbed problem - the ""frog"" problem - and compared with two continuous action based systems, GCS and XCSFCA. The proposed approach has consistently solved the frog problem and outperformed GCS and XCSFCA. © 2012 Springer-Verlag.","Code Fragment; Computed Action; Continuous Action; Learning Classifier Systems; XCS; XCSR","Code fragments; Computed Action; Continuous Action; Learning classifier system; XCS; XCSR; Genetic programming; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84871390939
"Russakovsky O., Fei-Fei L.","Attribute learning in large-scale datasets",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",18,10.1007/978-3-642-35749-7_1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871129073&doi=10.1007%2f978-3-642-35749-7_1&partnerID=40&md5=dfb0c53e9b762c4e29e912939d2c9eac","We consider the task of learning visual connections between object categories using the ImageNet dataset, which is a large-scale dataset ontology containing more than 15 thousand object classes. We want to discover visual relationships between the classes that are currently missing (such as similar colors or shapes or textures). In this work we learn 20 visual attributes and use them in a zero-shot transfer learning experiment as well as to make visual connections between semantically unrelated object categories. © 2012 Springer-Verlag.",,"Data sets; Large-scale datasets; Object categories; Object class; Transfer learning; Visual attributes; Artificial intelligence; Computer vision",Conference Paper,Scopus,2-s2.0-84871129073
"Aerts D., Sozzo S., Tapia J.","A quantum model for the Ellsberg and machina paradoxes",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",18,10.1007/978-3-642-35659-9_5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870808030&doi=10.1007%2f978-3-642-35659-9_5&partnerID=40&md5=b546cb42a688bb4eab57ca0a9a859950","The Ellsberg and Machina paradoxes reveal that expected utility theory is problematical when real subjects take decisions under uncertainty. Suitable generalizations of expected utility exist which attempt to solve the Ellsberg paradox, but none of them provides a satisfactory solution of the Machina paradox. In this paper we elaborate a quantum model in Hilbert space describing the Ellsberg situation and also the Machina situation, and show that we can model the specific aspect of the Machina situation that is unable to be modeled within the existing generalizations of expected utility. © 2012 Springer-Verlag.","ambiguity aversion; Ellsberg paradox; Machina paradox; quantum modeling","Ambiguity aversion; Ellsberg paradox; Expected utility; Expected utility theory; Machina paradox; Quantum modeling; Quantum models; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84870808030
"Van Den Broeck G., Choi A., Darwiche A.","Lifted relax, compensate and then recover: From approximate to exact lifted probabilistic inference",2012,"Uncertainty in Artificial Intelligence - Proceedings of the 28th Conference, UAI 2012",18,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879295161&partnerID=40&md5=036f6e2ebf27007272c9faa3afec3239","We propose an approach to lifted approximate inference for first-order probabilistic models, such as Markov logic networks. It is based on performing exact lifted inference in a simplified first-order model, which is found by relaxing first-order constraints, and then compensating for the relaxation. These simplified models can be incrementally improved by carefully recovering constraints that have been relaxed, also at the first-order level. This leads to a spectrum of approximations, with lifted belief propagation on one end, and exact lifted inference on the other. We discuss how relaxation, compensation, and recovery can be performed, all at the firstorder level, and show empirically that our approach substantially improves on the approximations of both propositional solvers and lifted belief propagation.",,"Approximate inference; Belief propagation; First-order; First-order models; Markov logic networks; Probabilistic inference; Probabilistic models; Artificial intelligence; Recovery",Conference Paper,Scopus,2-s2.0-84879295161
"Pelusi D.","PID and intelligent controllers for optimal timing performances of industrial actuators",2012,"International Journal of Simulation: Systems, Science and Technology",18,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879250938&partnerID=40&md5=f9fd8c99897d48d059f88b32dec27491","PID controllers are widely used in process industries due to its simplicity and robustness. The main problem sometime is tuning the PID parameters in order to improve the settling time, the rise time and the overshoot. In literature, there are procedures to obtain the PID settings which gives the better performance and robustness. Some experiments on this research line show that the controller gain is only a function of the overshoot observed in the setpoint experiment. The challenge is to improve the timing parameters to achieve optimal control performances. Remarkable findings are obtained through the use of Artificial Intelligence techniques as Fuzzy Logic, Genetic Algorithms and Neural Network. The first theory is good for decisional problems, the second one can be used in search algorithms and the Neural Networks have the capability to learn from data. The combination of these approaches can give good results in terms of settling time, rise time and overshoot. In this paper, we propose the design of suitable controllers which target is the improvement of timing performance of industrial actuators. The designed controllers are PID controller, genetic-fuzzy controller and neuro-fuzzy controller. The results show that the PID controller has good overshoot values and shows optimal robustness. The genetic-fuzzy controller gives a good value of settling time and a very good overshoot value. The neural-fuzzy controller gives the best timing parameters improving the control performances of the others two approaches.","Control systems; Fuzzy logic; Genetic algorithms; Neural networks; PID controllers","Artificial intelligence techniques; Better performance; Control performance; Intelligent controllers; Neural-fuzzy controllers; Neuro-fuzzy controller; PID controllers; Timing performance; Actuators; Control systems; Electric control equipment; Experiments; Fuzzy logic; Genetic algorithms; Industry; Neural networks; Optimization; Robust control; Robustness (control systems); Three term control systems",Article,Scopus,2-s2.0-84879250938
"Kubota N., Toda Y.","Multimodal communication for human-friendly robot partners in informationally structured space",2012,"IEEE Transactions on Systems, Man and Cybernetics Part C: Applications and Reviews",18,10.1109/TSMCC.2012.2213810,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871766484&doi=10.1109%2fTSMCC.2012.2213810&partnerID=40&md5=b8964f20a26a3cc0d7f2288fd233ad9f","This paper proposes a multimodal communication method for human-friendly robot partners based on various types of sensors. First, we explain informationally structured space to extend the cognitive capabilities of robot partners based on environmental systems. Next, we discuss the suitable measurement range for recognition technologies of touch interface, voice recognition, human detection, gesture recognition, and others. Based on the suitable measurement ranges, we propose an integration method to estimate human behaviors based on the human detection using color image and 3-D distance information, and gesture recognition by the multilayered spiking neural network using the time series of human-hand positions. Furthermore, we propose a conversation system to realize the multimodal communication with a person. Finally, we show several experimental results of the proposed method, and discuss the future direction of this research. © 1998-2012 IEEE.","Computational intelligence; human-robot interaction; information services; intelligent robots; ubiquitous computing","Cognitive capability; Color images; Conversation systems; Distance information; Environmental systems; Human behaviors; Human detection; Human hands; Human-friendly; Integration method; Measurement range; Multi-layered; Multimodal communications; Spiking neural networks; Touch interfaces; Artificial intelligence; Cognitive systems; Human computer interaction; Information services; Intelligent robots; Neural networks; Robots; Ubiquitous computing; Gesture recognition",Article,Scopus,2-s2.0-84871766484
"Shaker N., Yannakakis G.N., Togelius J., Nicolau M., O'neill M.","Evolving personalized content for super mario bros using grammatical evolution",2012,"Proceedings of the 8th AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, AIIDE 2012",18,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883079945&partnerID=40&md5=873ff59ae56ff09a6d6ad0985cdaea32","Adapting game content to a particular player's needs and expertise constitutes an important aspect in game design. Most research in this direction has focused on adapting game difficulty to keep the player engaged in the game. Dynamic difficulty adjustment, however, focuses on one aspect of the gameplay experience by adjusting the content to increase or decrease perceived challenge. In this paper, we introduce a method for automatic level generation for the platform game Super Mario Bros using grammatical evolution. The grammatical evolution-based level generator is used to generate player-adapted content by employing an adaptation mechanism as a fitness function in grammatical evolution to optimize the player experience of three emotional states: engagement, frustration and challenge. The fitness functions used are models of player experience constructed in our previous work from crowd-sourced gameplay data collected from over 1500 game sessions. Copyright © 2012, Association for the Advancement of Artificial Intelligence.",,"Adaptation mechanism; Emotional state; Fitness functions; Gameplay experiences; Grammatical evolution; Personalized content; Player experience; Super Mario Bros; Artificial intelligence; Human computer interaction; Computational grammars",Conference Paper,Scopus,2-s2.0-84883079945
"Gubern-Mérida A., Kallenberg M., Martí R., Karssemeijer N.","Segmentation of the pectoral muscle in breast MRI using atlas-based approaches.",2012,"Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention",18,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872949941&partnerID=40&md5=9b2fabb97bccc2609051b5aa79740ac0","Pectoral muscle segmentation is an important step in automatic breast image analysis methods and crucial for multi-modal image registration. In breast MRI, accurate delineation of the pectoral is important for volumetric breast density estimation and for pharmacokinetic analysis of dynamic contrast enhancement. In this paper we propose and study the performance of atlas-based segmentation methods evaluating two fully automatic breast MRI dedicated strategies on a set of 27 manually segmented MR volumes. One uses a probabilistic model and the other is a multi-atlas registration based approach. The multi-atlas approach performed slightly better, with an average Dice coefficient (DSC) of 0.74, while with the much faster probabilistic method a DSC of 0.72 was obtained.",,"algorithm; anatomic landmark; article; artificial intelligence; automated pattern recognition; breast; computer assisted diagnosis; female; histology; human; image quality; image subtraction; methodology; nuclear magnetic resonance imaging; pectoralis major muscle; reproducibility; sensitivity and specificity; Algorithms; Anatomic Landmarks; Artificial Intelligence; Breast; Female; Humans; Magnetic Resonance Imaging; Pattern Recognition, Automated; Pectoralis Muscles; Radiographic Image Enhancement; Radiographic Image Interpretation, Computer-Assisted; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-84872949941
"Hannan M.A., Arebey M., Begum R.A., Basri H.","An automated solid waste bin level detection system using a gray level aura matrix",2012,"Waste Management",18,10.1016/j.wasman.2012.06.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870255042&doi=10.1016%2fj.wasman.2012.06.002&partnerID=40&md5=61ffcf312d720b559187fd00a8010f1c","An advanced image processing approach integrated with communication technologies and a camera for waste bin level detection has been presented. The proposed system is developed to address environmental concerns associated with waste bins and the variety of waste being disposed in them. A gray level aura matrix (GLAM) approach is proposed to extract the bin image texture. GLAM parameters, such as neighboring systems, are investigated to determine their optimal values. To evaluate the performance of the system, the extracted image is trained and tested using multi-layer perceptions (MLPs) and K-nearest neighbor (KNN) classifiers. The results have shown that the accuracy of bin level classification reach acceptable performance levels for class and grade classification with rates of 98.98% and 90.19% using the MLP classifier and 96.91% and 89.14% using the KNN classifier, respectively. The results demonstrated that the system performance is robust and can be applied to a variety of waste and waste bin level detection under various conditions. © 2012 Elsevier Ltd.","Bin level detection; GLAM; KNN; MLP; Solid waste monitoring and management","Communication technologies; Detection system; Environmental concerns; GLAM; Gray levels; K-nearest neighbors; k-NN classifier; KNN; MLP; MLP classifiers; Monitoring and management; Multi-layer perception; Optimal values; Performance level; Processing approach; Waste bins; Image processing; Solid wastes; Bins; image processing; nearest neighbor analysis; solid waste; waste management; accuracy; article; classification algorithm; environmental factor; gray level aura matrix; image analysis; image processing; k nearest neighbor; learning algorithm; municipal solid waste; perceptron; priority journal; process optimization; quantitative analysis; solid waste management; waste disposal; Artificial Intelligence; Automation; Environmental Monitoring; Environmental Pollution; Image Processing, Computer-Assisted; Malaysia; Refuse Disposal",Article,Scopus,2-s2.0-84870255042
"Ting T.O., Shi Y., Cheng S., Lee S.","Exponential inertia weight for particle swarm optimization",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",18,10.1007/978-3-642-30976-2_10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875095304&doi=10.1007%2f978-3-642-30976-2_10&partnerID=40&md5=cfc0e81b5d4940593d597c9cb2794ce4","The exponential inertia weight is proposed in this work aiming to improve the search quality of Particle Swarm Optimization (PSO) algorithm. This idea is based on the adaptive crossover rate used in Differential Evolution (DE) algorithm. The same formula is adopted and applied to inertia weight, w. We further investigate the characteristics of the adaptive w graphically and careful analysis showed that there exists two important parameters in the equation for adaptive w; one acting as the local attractor and the other as the global attractor. The 23 benchmark problems are adopted as test bed in this study; consisting of both high and low dimensional problems. Simulation results showed that the proposed method achieved significant improvement compared to the linearly decreasing method technique that is used widely in literature. © 2012 Springer-Verlag.","Benchmark functions; exponential inertia weight; Particle Swarm Optimization","Adaptive crossovers; Bench-mark problems; Benchmark functions; Differential evolution algorithms; Global attractor; Inertia weight; Local attractors; Particle swarm optimization algorithm; Algorithms; Artificial intelligence; Equipment testing; Particle swarm optimization (PSO)",Conference Paper,Scopus,2-s2.0-84875095304
"Yuan C., Malone B.","An improved admissible heuristic for learning optimal bayesian networks",2012,"Uncertainty in Artificial Intelligence - Proceedings of the 28th Conference, UAI 2012",18,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886050172&partnerID=40&md5=dd94620194819e6c26c4f71010d34e37","Recently two search algorithms, a* and breadthfirst branch and bound (BFBnB), were developed based on a simple admissible heuristic for learning Bayesian network structures that optimize a scoring function. The heuristic represents a relaxation of the learning problem such that each variable chooses optimal parents independently. As a result, the heuristic may contain many directed cycles and result in a loose bound. This paper introduces an improved admissible heuristic that tries to avoid directed cycles within small groups of variables. A sparse representation is also introduced to store only the unique optimal parent choices. Empirical results show that the new techniques significantly improved the efficiency and scalability of A* and BFBnB on most of datasets tested in this paper.",,"Breadth-first; Learning Bayesian networks; Learning problem; Scoring functions; Search Algorithms; Sparse representation; Artificial intelligence; Heuristic methods; Linear programming; Optimization; Bayesian networks",Conference Paper,Scopus,2-s2.0-84886050172
"Bouchard K., Bouchard B., Bouzouane A.","Guidelines to efficient smart home design for rapid AI prototyping: A case study",2012,"ACM International Conference Proceeding Series",18,10.1145/2413097.2413134,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871943341&doi=10.1145%2f2413097.2413134&partnerID=40&md5=7fc673d34e746aecf7ae2ff1320abeb7","Advances in ubiquitous technology have moved us towards the dream of creating intelligent houses that can help human in their everyday life. The next step in the completion of this vision is to make major breakthroughs in artificial intelligence. In fact, it is the key component for allowing sensors and effectors to give useful services when it is appropriate. In consequence, researchers need to conduct more experiments in realistic setting (e.g. smart home). In order to face this challenge, many research teams try to build new experimental infrastructures without any background experience, guidance or even a real idea of their research needs and issues. Our team is composed of specialists in AI for cognitive assistance and has worked with four major smart home infrastructures. From that experience, we propose, in this paper, a set of guidelines for designing and implementing an efficient smart home architecture on both hardware and software perspective. This paper aims to be a major step toward the AI development (rapid prototyping) and smart home research. Moreover, we share our recent experience with the construction of a new smart home and clinical trials conducted at our laboratory with real Alzheimer's subjects.","Architecture; Cognitive assistance; Guidelines; Hardware; Prototyping; Smart home design; Software","Alzheimer's; Clinical trial; Cognitive assistance; Guidelines; Hardware and software; Research needs; Research teams; Smart homes; Ubiquitous technology; Architecture; Artificial intelligence; Computer hardware; Computer software; Hardware; Intelligent buildings; Rapid prototyping; Research; Software prototyping; Automation",Conference Paper,Scopus,2-s2.0-84871943341
"Yang X.-S., Deb S., Karamanoglu M., He X.","Cuckoo search for business optimization applications",2012,"2012 National Conference on Computing and Communication Systems, NCCCS 2012 - Proceeding",18,10.1109/NCCCS.2012.6412973,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874227947&doi=10.1109%2fNCCCS.2012.6412973&partnerID=40&md5=90fa78e0feb02c4633ba0020e2ac868e","Cuckoo search has become a popular and powerful metaheuristic algorithm for global optimization. In business optimization and applications, many studies have focused on support vector machine and neural networks. In this paper, we use cuckoo search to carry out optimization tasks and compare the performance of cuckoo search with support vector machine. By testing benchmarks such as project scheduling and bankruptcy predictions, we conclude that cuckoo search can perform better than support vector machine. © 2012 IEEE.","algorithm; cuckoo search; metaheuristics; optimization; swarm intelligence","Bankruptcy prediction; Business optimization; Cuckoo searches; Meta heuristic algorithm; Meta heuristics; Optimization task; Project scheduling; Swarm Intelligence; Algorithms; Artificial intelligence; Communication systems; Global optimization; Support vector machines; Optimization",Conference Paper,Scopus,2-s2.0-84874227947
"Pattem S.","Unsupervised disaggregation for non-intrusive load monitoring",2012,"Proceedings - 2012 11th International Conference on Machine Learning and Applications, ICMLA 2012",18,10.1109/ICMLA.2012.249,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873585571&doi=10.1109%2fICMLA.2012.249&partnerID=40&md5=d75c44b2eb3df418a4c11447653d9938","A method for unsupervised disaggregation of appliance signatures from smart meter data is presented. The primary feature used for unsupervised learning relates to abrupt transitions or magnitude changes in the power waveform. The method consists of a sequence of procedures for appliance signature identification, and disaggregation using hidden Markov modeling (HMM), and residual analysis. The key contributions are (a) a novel 'segmented' application of the Viterbi algorithm for sequence decoding with the HMM, (b) details of establishing observation and state transition probabilities for the HMM, and (c) procedures for careful handling of low power signatures. Results show that the method is effective for magnitude-based disaggregation, and provide insights for a more complete solution. © 2012 IEEE.","disaggregation; unsupervised machine learning","Abrupt transition; Complete solutions; Disaggregation; Hidden markov modeling(HMM); Low Power; Nonintrusive load monitoring; Power waveforms; Residual analysis; Signature identification; State transition probabilities; Unsupervised machine learning; Artificial intelligence; Software engineering; Learning systems",Conference Paper,Scopus,2-s2.0-84873585571
"Mani S., Chen Y., Elasy T., Clayton W., Denny J.","Type 2 diabetes risk forecasting from EMR data using machine learning.",2012,"AMIA ... Annual Symposium proceedings / AMIA Symposium. AMIA Symposium",18,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880788228&partnerID=40&md5=2428089af15c381d669ff907041dbd3f","To test the feasibility of using data collected in electronic medical records for development of effective models for diabetes risk forecasting. Using available demographic, clinical and lab parameters of more than two thousand patients from Electronic medical records, we applied different machine learning algorithms to assess the risk of development of type 2 diabetes (T2D) six months to one year later. We achieved an AUC greater than 0.8 for predicting type 2 diabetes 365 days and 180 days prior to diagnosis of diabetes. Diabetes risk forecasting using data from EMR is innovative and has the potential to identify, automatically, high-risk populations for early intervention with life style modifications such as diet and exercise to prevent or delay the development of T2D. Our study shows that T2D risk forecasting from EMR data is feasible.",,"algorithm; area under the curve; article; artificial intelligence; electronic medical record; human; methodology; non insulin dependent diabetes mellitus; risk assessment; Algorithms; Area Under Curve; Artificial Intelligence; Diabetes Mellitus, Type 2; Electronic Health Records; Humans; Risk Assessment",Article,Scopus,2-s2.0-84880788228
"Murray T., Matichuk D., Brassil M., Gammie P., Klein G.","Noninterference for operating system kernels",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",18,10.1007/978-3-642-35308-6_12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869842599&doi=10.1007%2f978-3-642-35308-6_12&partnerID=40&md5=cf8d95c93f488d9ff5700aeef7220b7b","While intransitive noninterference is a natural property for any secure OS kernel to enforce, proving that the implementation of any particular general-purpose kernel enforces this property is yet to be achieved. In this paper we take a significant step towards this vision by presenting a machine-checked formulation of intransitive noninterference for OS kernels, and its associated sound and complete unwinding conditions, as well as a scalable proof calculus over nondeterministic state monads for discharging these unwinding conditions across a kernel's implementation. Our ongoing experience applying this noninterference framework and proof calculus to the seL4 microkernel validates their utility and real-world applicability. © 2012 Springer-Verlag Berlin Heidelberg.","Information flow; refinement; scheduling; state monads","Information flows; Intransitive non-interference; Natural properties; Nondeterministic state; Operating system kernel; Proof calculus; refinement; state monads; Artificial intelligence; Scheduling; Calculations",Conference Paper,Scopus,2-s2.0-84869842599
"Duan S., Fokoue A., Hassanzadeh O., Kementsietsidis A., Srinivas K., Ward M.J.","Instance-based matching of large ontologies using locality-sensitive hashing",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",18,10.1007/978-3-642-35176-1-4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868524420&doi=10.1007%2f978-3-642-35176-1-4&partnerID=40&md5=e7c6bb5070bd570256abe0186b443409","In this paper, we describe a mechanism for ontology alignment using instance based matching of types (or classes). Instance-based matching is known to be a useful technique for matching ontologies that have different names and different structures. A key problem in instance matching of types, however, is scaling the matching algorithm to (a) handle types with a large number of instances, and (b) efficiently match a large number of type pairs. We propose the use of state-of-the art locality-sensitive hashing (LSH) techniques to vastly improve the scalability of instance matching across multiple types. We show the feasibility of our approach with DBpedia and Freebase, two different type systems with hundreds and thousands of types, respectively. We describe how these techniques can be used to estimate containment or equivalence relations between two type systems, and we compare two different LSH techniques for computing instance similarity. © 2012 Springer-Verlag Berlin Heidelberg.","Linked Data; Ontology Alignment; Schema Matching; Semantic Web","Different structure; Equivalence relations; Linked datum; Locality sensitive hashing; Matching algorithm; Ontology alignment; Schema matching; State of the art; Type systems; Semantic Web; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868524420
"Kumar A., Wu X., Zilberstein S.","Lagrangian relaxation techniques for scalable spatial conservation planning",2012,"Proceedings of the National Conference on Artificial Intelligence",18,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868283488&partnerID=40&md5=0680a2435bcf523175f69fe6826be547","We address the problem of spatial conservation planning in which the goal is to maximize the expected spread of cascades of an endangered species by strategically purchasing land parcels within a given budget. This problem can be solved by standard integer programming methods using the sample average approximation (SAA) scheme. Our main contribution lies in exploiting the separable structure present in this problem and using Lagrangian relaxation techniques to gain scalability over the flat representation. We also generalize the approach to allow the application of the SAA scheme to a range of stochastic optimization problems. Our iterative approach is highly efficient in terms of space requirements and it provides an upper bound over the optimal solution at each iteration. We apply our approach to the Red-cockaded Woodpecker conservation problem. The results show that it can find the optimal solution significantly faster - sometimes by an order-of-magnitude - than using the flat representation for a range of budget sizes. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Conservation planning; Endangered species; Iterative approach; Lagrangian relaxation techniques; Land parcels; Optimal solutions; Sample average approximation; Space requirements; Stochastic optimization problems; Upper Bound; Artificial intelligence; Budget control; Conservation; Integer programming; Optimal systems; Iterative methods",Conference Paper,Scopus,2-s2.0-84868283488
"Li Y.-F., Hu J.-A., Jiang Y., Zhou Z.-H.","Towards discovering what patterns trigger what labels",2012,"Proceedings of the National Conference on Artificial Intelligence",18,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868268151&partnerID=40&md5=9d4e9cc45c0dc2077e4f979a7569f796","In many real applications, especially those involving data objects with complicated semantics, it is generally desirable to discover the relation between patterns in the input space and labels corresponding to different semantics in the output space. This task becomes feasible with MIML (Multi-Instance Multi-Label learning), a recently developed learning framework, where each data object is represented by multiple instances and is allowed to be associated with multiple labels simultaneously. In this paper, we propose KISAR, an MIML algorithm that is able to discover what instances trigger what labels. By considering the fact that highly relevant labels usually share some patterns, we develop a convex optimization formulation and provide an alternating optimization solution. Experiments show that KISAR is able to discover reasonable relations between input patterns and output labels, and achieves performances that are highly competitive with many state-of-the-art MIML algorithms. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Alternating optimizations; Data objects; Input patterns; Input space; Learning frameworks; Multi-label; Multiple instances; Multiple labels; Optimization formulations; Real applications; Algorithms; Convex optimization; Semantics; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868268151
"Hu Y., John A., Wang F., Kambhampati S.","ET-LDA: Joint topic modeling for aligning events and their twitter feedback",2012,"Proceedings of the National Conference on Artificial Intelligence",18,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868283695&partnerID=40&md5=e25a3493a35b6c9335ecc33b7c69cc90","During broadcast events such as the Superbowl, the U.S. Presidential and Primary debates, etc., Twitter has become the de facto platform for crowds to share perspectives and commentaries about them. Given an event and an associated large-scale collection of tweets, there are two fundamental research problems that have been receiving increasing attention in recent years. One is to extract the topics covered by the event and the tweets; the other is to segment the event. So far these problems have been viewed separately and studied in isolation. In this work, we argue that these problems are in fact inter-dependent and should be addressed together. We develop a joint Bayesian model that performs topic modeling and event segmentation in one unified framework. We evaluate the proposed model both quantitatively and qualitatively on two large-scale tweet datasets associated with two events from different domains to show that it improves significantly over baseline models. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Baseline models; Bayesian model; Data sets; Different domains; Event segmentation; Fundamental research; Unified framework; Bayesian networks; Data processing; Social networking (online); Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868283695
"Johanson M., Bard N., Burch N., Bowling M.","Finding optimal abstract strategies in extensive-form games",2012,"Proceedings of the National Conference on Artificial Intelligence",18,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868293788&partnerID=40&md5=5a1ebd816536aeea29f0231a8429bf4c","Extensive-form games are a powerful model for representing interactions between agents. Nash equilibrium strategies are a common solution concept for extensive-form games and, in two-player zero-sum games, there are efficient algorithms for calculating such strategies. In large games, this computation may require too much memory and time to be tractable. A standard approach in such cases is to apply a lossy state-space abstraction technique to produce a smaller abstract game that can be tractably solved, while hoping that the resulting abstract game equilibrium is close to an equilibrium strategy in the unabstracted game. Recent work has shown that this assumption is unreliable, and an arbitrary Nash equilibrium in the abstract game is unlikely to be even near the least suboptimal strategy that can be represented in that space. In this work, we present for the first time an algorithm which efficiently finds optimal abstract strategies - strategies with minimal exploitability in the unabstracted game. We use this technique to find the least exploitable strategy ever reported for two-player limit Texas hold'em. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Abstract games; Abstraction techniques; Equilibrium strategy; Extensive-form games; Nash equilibria; Solution concepts; State-space; Texas Hold'em; Zero-sum game; Algorithms; Game theory; Optimization; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868293788
"Amini H., Gholami R., Monjezi M., Torabi S.R., Zadhesh J.","Evaluation of flyrock phenomenon due to blasting operation by support vector machine",2012,"Neural Computing and Applications",18,10.1007/s00521-011-0631-5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867705340&doi=10.1007%2fs00521-011-0631-5&partnerID=40&md5=dcbcb96f37ed6f94c85f1bc2e3b0741a","Flyrock is an undesirable phenomenon in the blasting operation of open pit mines. Flyrock danger zone should be taken into consideration because it is the major cause of considerable damage on the nearby structures. Even with the best care and competent personnel, flyrock may not be totally avoided. There are several empirical methods for prediction of flyrock phenomenon. Low performance of these models is due to complexity of flyrock analysis. Support vector machine (SVM) is a novel machine learning technique usually considered as a robust artificial intelligence method in classification and regression tasks. The aim of this paper is to test the capability of SVM for the prediction of flyrock in the Soungun copper mine, Iran. Comparing the obtained results of SVM with that of artificial neural network (ANN), it was concluded that SVM approach is faster and more precise than ANN method in predicting the flyrock of Soungun copper mine. © 2011 Springer-Verlag London Limited.","Artificial neural network; Blasting; Flyrock; Soungun copper mine; Support vector machine","Artificial intelligence methods; Blasting operations; Empirical method; Flyrock; Machine learning techniques; Open pit mines; Blasting; Copper mines; Forecasting; Neural networks; Support vector machines",Article,Scopus,2-s2.0-84867705340
"Kiran M.S., Gündüz M., Baykan Ö.K.","A novel hybrid algorithm based on particle swarm and ant colony optimization for finding the global minimum",2012,"Applied Mathematics and Computation",18,10.1016/j.amc.2012.06.078,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867575848&doi=10.1016%2fj.amc.2012.06.078&partnerID=40&md5=280be4f14d8c6553c19842419f17154b","This paper presents a novel hybrid algorithm based on particle swarm optimization (PSO) and ant colony optimization (ACO) and called hybrid ant particle optimization algorithm (HAP) to find global minimum. In the proposed method, ACO and PSO work separately at each iteration and produce their solutions. The best solution is selected as the global best of the system and its parameters are used to select the new position of particles and ants at the next iteration. The performance of proposed method is compared with PSO and ACO on the benchmark problems and better quality results are obtained by HAP algorithm. © 2012 Elsevier Inc. All rights reserved.","Ant colony optimization; Global minimum; Hybrid metaheuristic; Particle swarm optimization","Ant Colony Optimization (ACO); Bench-mark problems; Global minima; Hybrid algorithms; Metaheuristic; New position; Particle optimization; Particle swarm; Artificial intelligence; Benchmarking; Iterative methods; Particle swarm optimization (PSO); Algorithms",Article,Scopus,2-s2.0-84867575848
"Zubaryeva A., Thiel C., Zaccarelli N., Barbone E., Mercier A.","Spatial multi-criteria assessment of potential lead markets for electrified vehicles in Europe",2012,"Transportation Research Part A: Policy and Practice",18,10.1016/j.tra.2012.05.018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865425415&doi=10.1016%2fj.tra.2012.05.018&partnerID=40&md5=a4a52802f88f4ac83ff76b0727d86f7a","This study presents a modeling approach that focuses on the identification of potential lead markets for electric-drive vehicles (EDVs) in Europe. It is based on a combination of several selected economic, social, environmental, and transport-related factors. The modeling approach is implemented in a GIS-based multi-criteria decision support process with fuzzy measures, enabling an assessment at different spatial and temporal scales under different EDV market penetration scenarios for Europe. The decision support system embeds a multi-criteria analysis based on selected expert-weighted market penetration drivers. The spatial scale chosen for the application of the decision support process are NUTS2 regions and cities within EU27 member states. Three scenarios are investigated, a business as usual, a moderate change, and an accelerated innovation scenario. Across the scenario horizon, it is shown how lead regions for EDVs will be changing in time between first early-adopter areas towards other long-term potential lead regions, depending on the evolution of the market drivers. The European regions and cities which will have a higher lead market potential score in 2020 and 2030 are identified. Our model solution suggests that with the business-as-usual scenario there will be a few insular lead market areas in 2020 and a relatively limited number of more connected lead regions in 2030. The other two scenarios explored suggest a more positive picture leading for the case of the 2030 accelerated scenario to a wide distribution of EDVs across most of Germany, the Netherlands, France, the UK, Ireland, and Italy. The cities of London, Madrid, Berlin and Rome would show high EDV sales under this scenario. © 2012 Elsevier Ltd.","AHP; Electric vehicles; GIS; Lead markets; Multi-criteria analysis","AHP; Business-as-usual; Decision support process; Electric-drive vehicles; Fuzzy measures; Germany; Ireland; Long-term potential; Market areas; Market drivers; Market penetration; Market potential; Model solution; Modeling approach; Multi Criteria Analysis; Multi-criteria assessment; Multicriteria decision support; Netherlands; Spatial scale; Temporal scale; Artificial intelligence; Decision support systems; Electric vehicles; Geographic information systems; Commerce; decision support system; electric vehicle; fuzzy mathematics; GIS; market conditions; multicriteria analysis; spatiotemporal analysis; transportation planning; Europe",Article,Scopus,2-s2.0-84865425415
"Faktor A., Irani M.","""Clustering by composition"" - Unsupervised discovery of image categories",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",18,10.1007/978-3-642-33786-4_35,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867886448&doi=10.1007%2f978-3-642-33786-4_35&partnerID=40&md5=b2f6e6e366ecb61898c70105cd66ee9b","We define a ""good image cluster"" as one in which images can be easily composed (like a puzzle) using pieces from each other, while are difficult to compose from images outside the cluster. The larger and more statistically significant the pieces are, the stronger the affinity between the images. This gives rise to unsupervised discovery of very challenging image categories. We further show how multiple images can be composed from each other simultaneously and efficiently using a collaborative randomized search algorithm. This collaborative process exploits the ""wisdom of crowds of images"", to obtain a sparse yet meaningful set of image affinities, and in time which is almost linear in the size of the image collection. ""Clustering-by-Composition"" can be applied to very few images (where a 'cluster model' cannot be 'learned'), as well as on benchmark evaluation datasets, and yields state-of-the-art results. © 2012 Springer-Verlag.",,"Benchmark evaluation; Cluster models; Collaborative process; Data sets; Image clusters; Image collections; Multiple image; Randomized search; Artificial intelligence; Computer vision",Conference Paper,Scopus,2-s2.0-84867886448
"Augonnet C., Aumage O., Furmento N., Namyst R., Thibault S.","StarPU-MPI: Task programming over clusters of machines enhanced with accelerators",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",18,10.1007/978-3-642-33518-1_40,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867630228&doi=10.1007%2f978-3-642-33518-1_40&partnerID=40&md5=deb8cb6aac38b2f4765da0d954e24239","GPUs clusters are becoming widespread HPC platforms. Exploiting them is however challenging, as this requires two separate paradigms (MPI and CUDA or OpenCL) and careful load balancing due to node heterogeneity. Current paradigms usually either limit themselves to offload part of the computation and leave CPUs idle, or require static CPU/GPU work partitioning. We thus have previously proposed StarPU, a runtime system able to dynamically scheduling tasks within a single heterogeneous node. We show how we extended the task paradigm of StarPU with MPI to easily map the task graph on MPI clusters and automatically benefit from optimized execution. © 2012 Springer-Verlag.","Accelerators; GPUs; MPI; Task-based model","GPUs; Heterogeneous nodes; MPI; Runtime systems; Scheduling tasks; Task graph; Task programming; Task-based; Artificial intelligence; Particle accelerators; Program processors",Conference Paper,Scopus,2-s2.0-84867630228
"Heiberg S., Laud P., Willemson J.","The application of i-voting for Estonian parliamentary elections of 2011",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",18,10.1007/978-3-642-32747-6_13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867440391&doi=10.1007%2f978-3-642-32747-6_13&partnerID=40&md5=86aa8a9c2ccd1cdf022226474f90eee7","Estonia has implemented internet voting as a method to participate in various types of elections since 2005. In Riigikogu (parliament) Elections of 2011, over 140,000 voters used the internet voting method. The share of votes cast over the internet among all votes was 24.3%. In light of this popularity it is questioned by various stakeholders whether internet voting can be implemented correctly and securely to support electoral principles such as uniformity. This paper gives an overview of the Estonian Internet Voting System and analyzes events that occurred during the Riigikogu Elections of 2011. © 2012 Springer-Verlag.",,"Estonia; Internet voting; Internet voting system; Parliamentary elections; Artificial intelligence; Online systems",Conference Paper,Scopus,2-s2.0-84867440391
"Póczos B., Ghahramani Z., Schneider J.","Copula-based kernel dependency measures",2012,"Proceedings of the 29th International Conference on Machine Learning, ICML 2012",18,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867117209&partnerID=40&md5=86697381abeca1228370fb5aba17127e","The paper presents a new copula based method for measuring dependence between random variables. Our approach extends the Maximum Mean Discrepancy to the copula of the joint distribution. We prove that this approach has several advantageous properties. Similarly to Shannon mutual information, the proposed dependence measure is invariant to any strictly increasing transformation of the marginal variables. This is important in many applications, for example in feature selection. The estimator is consistent, robust to outliers, and uses rank statistics only. We derive upper bounds on the convergence rate and propose independence tests too. We illustrate the theoretical contributions through a series of experiments in feature selection and low-dimensional embedding of distributions. Copyright 2012 by the author(s)/owner(s).",,"Convergence rates; Dependence measures; Independence tests; Joint distributions; Rank Statistics; Shannon mutual information; Upper Bound; Artificial intelligence; Software engineering; Learning systems",Conference Paper,Scopus,2-s2.0-84867117209
"Britto A., Pozo A.","Using archiving methods to control convergence and diversity for Many-Objective Problems in Particle Swarm Optimization",2012,"2012 IEEE Congress on Evolutionary Computation, CEC 2012",18,10.1109/CEC.2012.6256149,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866874998&doi=10.1109%2fCEC.2012.6256149&partnerID=40&md5=840fa3ac0e4bb81c5a13c453bc0b9dfc","Multi-Objective Particle Swarm Optimization (MOPSO) is a population based multi-objective meta-heuristic inspired on animal swarm intelligence. It is used to solve several Multi-Objective Optimization Problems (MOPs), problems with more than one objective function. However, Multi-Objective Evolutionary Algorithms (MOEAs), including MOPSO, have some limitations when the number of objective grows. Many-Objective Optimization research methods to decrease the negative effect of applying MOEAs into problems with more than three objective functions. In this context, the goal of this work is to explore several archiving methods from the literature used by MOPSO to store the selected leaders into Many-Objective Problems. Moreover, new archiving methods are proposed specially for these problems. The use of the archiving methods into MOPSO is evaluated through an empirical analysis aiming to observe the impact of these methods in the convergence and the diversity to the Pareto front, in Many-Objective scenarios. © 2012 IEEE.",,"Empirical analysis; Metaheuristic; Multi objective; Multi objective evolutionary algorithms; Multi objective particle swarm optimization; Multi-objective optimization problem; Objective functions; Pareto front; Swarm Intelligence; Artificial intelligence; Particle swarm optimization (PSO); Multiobjective optimization",Conference Paper,Scopus,2-s2.0-84866874998
"Duc A., Guo J., Peyrin T., Wei L.","Unaligned rebound attack: Application to Keccak",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",18,10.1007/978-3-642-34047-5_23,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866716797&doi=10.1007%2f978-3-642-34047-5_23&partnerID=40&md5=86c6279ae800f1ff2f4794cdf05e7259","We analyze the internal permutations of Keccak, one of the NIST SHA-3 competition finalists, in regard to differential properties. By carefully studying the elements composing those permutations, we are able to derive most of the best known differential paths for up to 5 rounds. We use these differential paths in a rebound attack setting and adapt this powerful freedom degrees utilization in order to derive distinguishers for up to 8 rounds of the internal permutations of the submitted version of Keccak. The complexity of the 8 round distinguisher is 2 491.47. Our results have been implemented and verified experimentally on a small version of Keccak. © 2012 Springer-Verlag.","differential cryptanalysis; hash function; Keccak; rebound attack; SHA-3","Differential cryptanalysis; Distinguishers; Keccak; rebound attack; SHA-3; Sha-3 competitions; Small version; Artificial intelligence; Hash functions",Conference Paper,Scopus,2-s2.0-84866716797
"Kiverstein J.","The Meaning of Embodiment",2012,"Topics in Cognitive Science",18,10.1111/j.1756-8765.2012.01219.x,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867622995&doi=10.1111%2fj.1756-8765.2012.01219.x&partnerID=40&md5=a6b14e361042ce5c7a029dc59a3cd9bf","There is substantial disagreement among philosophers of embodied cognitive science about the meaning of embodiment. In what follows, I describe three different views that can be found in the current literature. I show how this debate centers around the question of whether the science of embodied cognition can retain the computer theory of mind. One view, which I will label body functionalism, takes the body to play the functional role of linking external resources for problem solving with internal biological machinery. Embodiment is thus understood in terms of the role the body plays in supporting the computational circuits that realize cognition. Body enactivism argues by contrast that no computational account of cognition can account for the role of commonsense knowledge in our everyday practical engagement with the world. I will attempt a reconciliation of these seemingly opposed views. © 2012 Cognitive Science Society, Inc.","Artificial intelligence; Commonsense knowledge; Computer theory of mind; Dynamical cognitive science; Embodiment; Emotion and cognition; Enaction; Frame problem; Functionalism; Hubert Dreyfus; Predictive coding; Radical embodiment; Symbol grounding problem","article; brain; cognition; drive; human; physiology; psychology; psychophysiology; thinking; Brain; Cognition; Cognitive Science; Humans; Instinct; Psychophysiology; Thinking",Article,Scopus,2-s2.0-84867622995
"Paulson L.C.","MetiTarski: Past and future",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",18,10.1007/978-3-642-32347-8_1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865583548&doi=10.1007%2f978-3-642-32347-8_1&partnerID=40&md5=2b0926d7e87322faf1e0a277bfda3b49","A brief overview is presented of MetiTarski [4], an automatic theorem prover for real-valued special functions: ln, exp, sin, cos, etc. MetiTarski operates through a unique interaction between decision procedures and resolution theorem proving. Its history is briefly outlined, along with current projects. A simple collision avoidance example is presented. © 2012 Springer-Verlag.",,"Current projects; Decision procedure; Resolution theorem proving; Special functions; Theorem provers; Artificial intelligence; Theorem proving",Conference Paper,Scopus,2-s2.0-84865583548
"Kishimoto A., Winands M.H.M., Mul̈ler M., Saito J.-T.","Game-tree search using proof numbers: The first twenty years",2012,"ICGA Journal",18,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874619721&partnerID=40&md5=9231031a29bc0c29bdf8d499feb8cf9a","Solving games is a challenging and attractive task in the domain of Artificial Intelligence. Despite enormous progress, solving increasingly difficult games or game positions continues to pose hard technical challenges. Over the last twenty years, algorithms based on the concept of proof and disproof numbers have become dominating techniques for game solving. Prominent examples include solving the game of checkers to be a draw, and developing checkmate solvers for shogi, which can find mates that take over a thousand moves. This article provides an overview of the research on Proof-Number Search and its many variants and enhancements.",,"Game solving; Technical challenges; Artificial intelligence",Article,Scopus,2-s2.0-84874619721
"Ciucci D., Dubois D., Prade H.","Oppositions in rough set theory",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",18,10.1007/978-3-642-31900-6_62,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864970095&doi=10.1007%2f978-3-642-31900-6_62&partnerID=40&md5=88979260329cec88660be8e289f6fe23","The role of opposition in rough set theory is laid bare. There are two sources which generate oppositions in rough sets: approximations and relations. In the former case, we outline a hexagon and a cube of oppositions. In the second case, we define a classical square of oppositions and also a tetrahedron when considering the standpoint of two agents. © 2012 Springer-Verlag.",,"Rough set; Square-of-opposition; Two sources; Artificial intelligence; Rough set theory",Conference Paper,Scopus,2-s2.0-84864970095
"Avraham G., Nisky I., Fernandes H.L., Acuna D.E., Kording K.P., Loeb G.E., Karniel A.","Toward perceiving robots as humans: Three handshake models face the turing-like handshake test",2012,"IEEE Transactions on Haptics",18,10.1109/TOH.2012.16,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864955779&doi=10.1109%2fTOH.2012.16&partnerID=40&md5=d8a99b9cd6ff82e20c3af0d290b0fc49","In the Turing test a computer model is deemed to ""think intelligently"" if it can generate answers that are indistinguishable from those of a human. We developed an analogous Turing-like handshake test to determine if a machine can produce similarly indistinguishable movements. The test is administered through a telerobotic system in which an interrogator holds a robotic stylus and interacts with another party-artificial or human with varying levels of noise. The interrogator is asked which party seems to be more human. Here, we compare the human-likeness levels of three different models for handshake: 1) Tit-for-Tat model, 2) model, and 3) Machine Learning model. The Tit-for-Tat and the Machine Learning models generated handshakes that were perceived as the most human-like among the three models that were tested. Combining the best aspects of each of the three models into a single robotic handshake algorithm might allow us to advance our understanding of the way the nervous system controls sensorimotor interactions and further improve the human-likeness of robotic handshakes. © 2008-2011 IEEE.","Handshake; psychophysics; sensorimotor control; teleoperation; turing test","Computer models; Handshake; psychophysics; Sensorimotor control; System control; Telerobotic systems; Three models; Turing tests; Artificial intelligence; Learning systems; Remote control; Robotics",Article,Scopus,2-s2.0-84864955779
"Pozna C., Minculete N., Precup R.-E., Kóczy L.T., Ballagi Á.","Signatures: Definitions, operators and applications to fuzzy modelling",2012,"Fuzzy Sets and Systems",18,10.1016/j.fss.2011.12.016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861723702&doi=10.1016%2fj.fss.2011.12.016&partnerID=40&md5=6bc8f0f0810bbca2de4e0415d25b2f8c","This paper presents a new framework for the symbolic representation of data which is referred to as signatures. The definitions of signatures and of signature trees are first given. Original operators on signatures are next presented, i.e., contraction, extension, pruning, addition, multiplication, and grafting. Attractive applications of signatures related to the modelling of fuzzy inference systems are suggested and discussed. An example is included to accompany the theoretical results. © 2012 Elsevier B.V. All rights reserved.","Fuzzy inference systems; Fuzzy signatures; Operators; Signatures; Symbolic representation","Fuzzy inference systems; Fuzzy modelling; Fuzzy signatures; Signature trees; Signatures; Symbolic representation; Theoretical result; Artificial intelligence; Fuzzy sets; Mathematical operators; Fuzzy systems",Article,Scopus,2-s2.0-84861723702
"Krüger T., Schnetter P., Placzek R., Vörsmann P.","Fault-tolerant nonlinear adaptive flight control using sliding mode online learning",2012,"Neural Networks",18,10.1016/j.neunet.2012.02.025,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861779026&doi=10.1016%2fj.neunet.2012.02.025&partnerID=40&md5=3673e7f326131f2d2c0aa1f39f4f448c","An expanded nonlinear model inversion flight control strategy using sliding mode online learning for neural networks is presented. The proposed control strategy is implemented for a small unmanned aircraft system (UAS). This class of aircraft is very susceptible towards nonlinearities like atmospheric turbulence, model uncertainties and of course system failures. Therefore, these systems mark a sensible testbed to evaluate fault-tolerant, adaptive flight control strategies. Within this work the concept of feedback linearization is combined with feed forward neural networks to compensate for inversion errors and other nonlinear effects. Backpropagation-based adaption laws of the network weights are used for online training. Within these adaption laws the standard gradient descent backpropagation algorithm is augmented with the concept of sliding mode control (SMC). Implemented as a learning algorithm, this nonlinear control strategy treats the neural network as a controlled system and allows a stable, dynamic calculation of the learning rates. While considering the system's stability, this robust online learning method therefore offers a higher speed of convergence, especially in the presence of external disturbances. The SMC-based flight controller is tested and compared with the standard gradient descent backpropagation algorithm in the presence of system failures. © 2012 Elsevier Ltd.","Adaptive flight control; Sliding mode online learning; Unmanned aircraft system; Variable learning rate","Control strategies; Controlled system; Dynamic calculations; External disturbances; Fault-tolerant; Flight control; Flight controllers; Gradient descent; Inversion errors; Learning rates; Model uncertainties; Network weights; Non linear control; Non-linear model; Nonlinear effect; On-line learning methods; Online learning; Online training; Sliding modes; Small unmanned aircrafts; Speed of convergence; System failures; Unmanned aircraft system; Variable learning rate; Aircraft; Aircraft control; Atmospheric turbulence; Backpropagation algorithms; E-learning; Feedback linearization; Flight control systems; Learning algorithms; Neural networks; Nonlinear analysis; Nonlinear optics; Systems engineering; Uncertainty analysis; Unmanned aerial vehicles (UAV); Adaptive control systems; adaptive behavior; aircraft; article; artificial neural network; control system; feedback system; learning algorithm; machine learning; mathematical computing; mathematical model; nonlinear system; online system; priority journal; signal noise ratio; simulation; sliding mode control; system analysis; Algorithms; Artificial Intelligence; Automation; Aviation; Neural Networks (Computer); Nonlinear Dynamics; Online Systems; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84861779026
"Conradsen I., Beniczky S., Wolf P., Kjaer T.W., Sams T., Sorensen H.B.D.","Automatic multi-modal intelligent seizure acquisition (MISA) system for detection of motor seizures from electromyographic data and motion data",2012,"Computer Methods and Programs in Biomedicine",18,10.1016/j.cmpb.2011.06.005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861996098&doi=10.1016%2fj.cmpb.2011.06.005&partnerID=40&md5=34e3ad1fff924a62fb0eef839b010b81","The objective is to develop a non-invasive automatic method for detection of epileptic seizures with motor manifestations. Ten healthy subjects who simulated seizures and one patient participated in the study. Surface electromyography (sEMG) and motion sensor features were extracted as energy measures of reconstructed sub-bands from the discrete wavelet transformation (DWT) and the wavelet packet transformation (WPT). Based on the extracted features all data segments were classified using a support vector machine (SVM) algorithm as simulated seizure or normal activity. A case study of the seizure from the patient showed that the simulated seizures were visually similar to the epileptic one. The multi-modal intelligent seizure acquisition (MISA) system showed high sensitivity, short detection latency and low false detection rate. The results showed superiority of the multi-modal detection system compared to the uni-modal one. The presented system has a promising potential for seizure detection based on multi-modal data. © 2011 Elsevier Ireland Ltd.","Epilepsy; Movement sensors; Seizure detection; Support vector machine learning; Surface EMG sensors; Wavelet packet","Epilepsy; Movement sensors; Seizure detection; Surface EMG; Wavelet Packet; Modal analysis; Sensors; Support vector machines; adult; article; automation; clinical article; controlled study; data processing; electromyography; epilepsy; female; human; male; multi modal intelligent seizure acquisition system; seizure; sensor; simulation; wavelet analysis; Actigraphy; Adult; Algorithms; Artificial Intelligence; Diagnosis, Computer-Assisted; Electromyography; Epilepsy, Partial, Motor; Female; Humans; Male; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Wavelet Analysis",Article,Scopus,2-s2.0-84861996098
"Ciuciu I.G., Meersman R., Dillon T.","Social network of smart-metered homes and SMEs for grid-based renewable energy exchange",2012,"IEEE International Conference on Digital Ecosystems and Technologies",18,10.1109/DEST.2012.6227922,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864266957&doi=10.1109%2fDEST.2012.6227922&partnerID=40&md5=fbbeb647759c9bd6952e4bf90a537cd8","This paper proposes a revolutionary approach to Smart Energy Grids which empowers communities of consumers as first-class citizens with a novel role in the management of their electricity by sharing excess electricity and therefore becoming energy producers (prosumers). The approach makes innovations on smart technologies and processes by building a demand-response decision support system on top of smart metering and social web technologies. This is achieved using a framework to connect dynamic, context-aware, heterogeneous virtual and real entities on the Internet of Smart Meters (IoSM) and by studying the behavior of communities on it. The smart electricity meters are transformed into fully-fledged intelligent computers on the IoSM, enabled to (i) securely collect data from heterogeneous meters and sensors and actuators, (ii) detect smart meters with similar goals, (iii) exchange and aggregate data from multiple autonomous physical or virtual meters, and (iv) manage the actual energy demand and ensure the achievement of demand response for the community involved. The approach is centered on the community and its respective DSOs, where each prosumer is represented as a node on the IoSM through their electricity meters, sensors and actuators. This allows for rational energy exchange between technical and non-technical participants by expressing their goals in a standardized language through hybrid ontologies. © 2012 IEEE.","community; demand-response; energy efficiency; prosumer; renewable energy; smart-meter; social network","community; demand-response; prosumer; Renewable energies; smart-meter; Social Networks; Actuators; Artificial intelligence; Decision support systems; Ecosystems; Electric measuring instruments; Energy efficiency; Energy management; Sensors; Smart power grids; Innovation",Article,Scopus,2-s2.0-84864266957
"Canard S., Jambert A., Lescuyer R.","Sanitizable signatures with several signers and sanitizers",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",18,10.1007/978-3-642-31410-0_3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864040800&doi=10.1007%2f978-3-642-31410-0_3&partnerID=40&md5=46b86bab08bc2481234dfc224fa6ca8c","Sanitizable signatures allow a signer of a message to give one specific receiver, called a sanitizer, the power to modify some designated parts of the signed message. Most of the existing constructions consider one single signer giving such a possibility to one single sanitizer. In this paper, we formalize the concept with n signers and m sanitizers, taking into account recent models (for 1 signer and 1 sanitizer) on the subject. We next give a generic construction based on the use of both group signatures and a new cryptographic building block, called a trapdoor or proof, that may be of independent interest. © 2012 Springer-Verlag.","anonymity; Sanitizable signatures; trapdoor or proof","anonymity; Building blockes; Generic construction; Group signatures; Sanitizable signatures; trapdoor or proof; Artificial intelligence; Cryptography",Conference Paper,Scopus,2-s2.0-84864040800
"Déharbe D., Fontaine P., Guyot Y., Voisin L.","SMT solvers for Rodin",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",18,10.1007/978-3-642-30885-7_14,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863971667&doi=10.1007%2f978-3-642-30885-7_14&partnerID=40&md5=eff6b16b58cafdbea143791df6ec97c2","Formal development in Event-B generally requires the validation of a large number of proof obligations. Some automatic tools exist to automatically discharge a significant part of them, thus augmenting the efficiency of the formal development. We here investigate the use of SMT (Satisfiability Modulo Theories) solvers in addition to the traditional tools, and detail the techniques used for the cooperation between the Rodin platform and SMT solvers. Our contribution is the definition of two approaches to use SMT solvers, their implementation in a Rodin plug-in, and an experimental evaluation on a large sample of industrial and academic projects. Adding SMT solvers to Atelier B provers reduces to one fourth the number of sequents that need to be proved interactively. © 2012 Springer-Verlag.",,"Academic projects; Automatic tools; Event-B; Experimental evaluation; Formal development; Plug-ins; Proof obligations; Satisfiability modulo Theories; Techniques used; Artificial intelligence; Formal logic",Conference Paper,Scopus,2-s2.0-84863971667
"Mausam, Kolobov A.","Planning with markov decision processes: An AI perspective",2012,"Synthesis Lectures on Artificial Intelligence and Machine Learning",18,10.2200/S00426ED1V01Y201206AIM017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863855328&doi=10.2200%2fS00426ED1V01Y201206AIM017&partnerID=40&md5=97cd7c0a91afa2d45ba4eeacb0024ada","Markov Decision Processes (MDPs) are widely popular in Artificial Intelligence for modeling sequential decision-making scenarios with probabilistic dynamics. They are the framework of choice when designing an intelligent agent that needs to act for long periods of time in an environment where its actions could have uncertain outcomes. MDPs are actively researched in two related subareas of AI, probabilistic planning and reinforcement learning. Probabilistic planning assumes known models for the agent's goals and domain dynamics, and focuses on determining how the agent should behave to achieve its objectives. On the other hand, reinforcement learning additionally learns these models based on the feedback the agent gets from the environment. This book provides a concise introduction to the use of MDPs for solving probabilistic planning problems, with an emphasis on the algorithmic perspective. It covers the whole spectrum of the field, from the basics to state-of-the-art optimal and approximation algorithms. We first describe the theoretical foundations of MDPs and the fundamental solution techniques for them. We then discuss modern optimal algorithms based on heuristic search and the use of structured representations. A major focus of the book is on the numerous approximation schemes for MDPs that have been developed in the AI literature. These include determinization-based approaches, sampling techniques, heuristic functions, dimensionality reduction, and hierarchical representations. Finally, we briefly introduce several extensions of the standard MDP classes that model and solve even more complex planning problems. Copyright © 2012 by Morgan & Claypool.","AI planning; MDP; probabilistic planning; reinforcement learning; sequential decision making under uncertainty; uncertainty in AI","AI planning; Approximation scheme; Dimensionality reduction; Domain dynamics; Fundamental solutions; Heuristic functions; Heuristic search; Hierarchical representation; Markov Decision Processes; MDP; Optimal algorithm; Planning problem; Probabilistic dynamics; Probabilistic planning; Sampling technique; Sequential decision making; Sub-areas; Theoretical foundations; Approximation algorithms; Artificial intelligence; Dynamics; Heuristic algorithms; Markov processes; Optimization; Reinforcement learning",Article,Scopus,2-s2.0-84863855328
"Bartoletti M., Tuosto E., Zunino R.","On the realizability of contracts in dishonest systems",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",18,10.1007/978-3-642-30829-1_17,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862727742&doi=10.1007%2f978-3-642-30829-1_17&partnerID=40&md5=b56ca5cce1fcf5e4f858081c4b1cdee8","We develop a theory of contracting systems, where behavioural contracts may be violated by dishonest participants after they have been agreed upon - unlike in traditional approaches based on behavioural types. We consider the contracts of [10], and we embed them in a calculus that allows distributed participants to advertise contracts, reach agreements, query the fulfilment of contracts, and realise them (or choose not to). Our contract theory makes explicit who is culpable at each step of a computation. A participant is honest in a given context S when she is not culpable in each possible interaction with S. Our main result is a sufficient criterion for classifying a participant as honest in all possible contexts. © 2012 IFIP International Federation for Information Processing.",,"Contract Theory; Realizability; Sufficient criterion; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84862727742
"Albert E., Flores-Montoya A.E., Genaim S.","Analysis of may-happen-in-parallel in concurrent objects",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",18,10.1007/978-3-642-30793-5_3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862733369&doi=10.1007%2f978-3-642-30793-5_3&partnerID=40&md5=69a55b6d2c25588ce9fc011f6c26567c","This paper presents a may-happen-in-parallel (MHP) analysis for OO languages based on concurrent objects. In this concurrency model, objects are the concurrency units such that, when a method is invoked on an object o 2 from a task executing on object o 1, statements of the current task in o 1 may run in parallel with those of the (asynchronous) call on o 2, and with those of transitively invoked methods. The goal of the MHP analysis is to identify pairs of statements in the program that may run in parallel in any execution. Our MHP analysis is formalized as a method-level (local) analysis whose information can be modularly composed to obtain application-level (global) information. © 2012 IFIP International Federation for Information Processing.",,"Concurrent objects; Object o; Artificial intelligence; Model checking",Conference Paper,Scopus,2-s2.0-84862733369
"Sundar S., Singh A.","A swarm intelligence approach to the early/tardy scheduling problem",2012,"Swarm and Evolutionary Computation",18,10.1016/j.swevo.2011.12.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858447444&doi=10.1016%2fj.swevo.2011.12.002&partnerID=40&md5=7ff1c4045e354b6ef86958d71ea2f5d8","This paper describes an application of artificial bee colony (ABC) algorithm, which is a new swarm intelligence approach, for a version of the single machine early/tardy scheduling problem where no unforced machine idle time is allowed. A local search is used inside the ABC algorithm to further improve the schedules obtained through it. A variant of the basic ABC approach is also considered in this paper where the best solution obtained through ABC algorithm is improved further via an exhaustive local search. We have compared these two approaches with 16 heuristic approaches reported in the literature on existing set of benchmark instances as well as on some large instances. Computational results show the effectiveness of our approaches. © 2011 Elsevier B.V. All rights reserved.","Artificial bee colony algorithm; Constrained optimization; Early/tardy scheduling; Heuristic; Swarm intelligence","Abc algorithms; Artificial bee colonies; Artificial bee colony algorithms; Computational results; Early/tardy scheduling; Early/tardy scheduling problem; Heuristic; Heuristic approach; Local search; Machine idle; Swarm Intelligence; Constrained optimization; Evolutionary algorithms; Heuristic methods; Scheduling; Artificial intelligence",Article,Scopus,2-s2.0-84858447444
"Aldape-Pérez M., Yáñez-Márquez C., Camacho-Nieto O., J.Argüelles-Cruz A.","An associative memory approach to medical decision support systems",2012,"Computer Methods and Programs in Biomedicine",18,10.1016/j.cmpb.2011.05.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860246796&doi=10.1016%2fj.cmpb.2011.05.002&partnerID=40&md5=b96350577dc7d7723ae596d68599b305","Classification is one of the key issues in medical diagnosis. In this paper, a novel approach to perform pattern classification tasks is presented. This model is called Associative Memory based Classifier (AMBC). Throughout the experimental phase, the proposed algorithm is applied to help diagnose diseases; particularly, it is applied in the diagnosis of seven different problems in the medical field. The performance of the proposed model is validated by comparing classification accuracy of AMBC against the performance achieved by other twenty well known algorithms. Experimental results have shown that AMBC achieved the best performance in three of the seven pattern classification problems in the medical field. Similarly, it should be noted that our proposal achieved the best classification accuracy averaged over all datasets. © 2011 Elsevier Ireland Ltd.","Associative memories; Decision support systems; Pattern classification; Supervised Machine Learning algorithms","Associative memories; Classification accuracy; Data sets; Diagnose disease; Medical decision support system; Medical fields; Pattern classification problems; Supervised machine learning; Artificial intelligence; Associative processing; Associative storage; Classification (of information); Decision support systems; Learning algorithms; Medical problems; Pattern recognition; Diagnosis; accuracy; algorithm; article; associative memory; Associative Memory based Classifier; classifier; controlled study; decision support system; diagnostic procedure; disease classification; mathematical model; medical care; medical decision making; nerve cell network; support vector machine; Algorithms; Decision Support Systems, Clinical; Disease; Humans; Memory; Mexico",Article,Scopus,2-s2.0-84860246796
"Picard D., Revel A., Cord M.","An application of swarm intelligence to distributed image retrieval",2012,"Information Sciences",18,10.1016/j.ins.2010.03.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857873639&doi=10.1016%2fj.ins.2010.03.003&partnerID=40&md5=c7d95397027052aec1fa2aa203b489e3","In this article, we introduce an application of swarm intelligence to distributed visual information retrieval distributed over networks. Based on the relevance feedback scheme, we use ant-like agents to crawl the network and to retrieve relevant images. Agents movements are influenced by markers stored on the hosts. These markers are reinforced to match the distribution of relevant images over the network. We tackle the use of the information gathered during previous search sessions. In order to match the different categories available on the network, we use several markers. Sessions searching for the same category will thus use the same makers. The system involves three learning problems: the selection of relevant markers regarding the searched category, the reinforcement of these markers and the learning of the relevance function. All of these problems are based on the relevance feedback loop. We test our system on a custom network hosting images taken from the well known TrecVid dataset. Our system shows a high improvement over classical content based image retrieval systems which do not use previous sessions information. © 2010 Elsevier Inc. All rights reserved.","Ant colony optimization; Content based image retrieval; Distributed information retrieval; Multi-agents system","Ant Colony Optimization (ACO); Content based image retrieval; Content based image retrieval systems; Data sets; Distributed information retrieval; Learning problem; Multi-agents systems; Network hosting; Relevance feedback; Search sessions; Swarm Intelligence; TRECVID; Visual information retrieval; Artificial intelligence; Information use; Statistical tests; Image retrieval",Article,Scopus,2-s2.0-84857873639
"Pietruczuk L., Duda P., Jaworski M.","A new fuzzy classifier for data streams",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",18,10.1007/978-3-642-29347-4_37,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861045855&doi=10.1007%2f978-3-642-29347-4_37&partnerID=40&md5=7e7cefdd250e8a4498a803fe6be4bcd9","Along with technological developments we observe an increasing amount of stored and processed data. It is not possible to store all incoming data and analyze it on the fly. Therefore many researchers are working on new algorithms for data stream mining. New algorithm should be fast and should use a small amount of memory. We will consider the problem of data stream classification. To increase the accuracy we propose to use an ensemble of classifiers based on a modified FID3 algorithm. The experimental results show that this algorithm is fast and accurate. Therefore it is adequate tool for data stream classification. © 2012 Springer-Verlag Berlin Heidelberg.","classification; data stream; decision tree; ensemble algorithm; FID3; fuzzy logic","Data stream; Data stream mining; Ensemble algorithms; Ensemble of classifiers; FID3; Fuzzy classifiers; On the flies; Technological development; Algorithms; Artificial intelligence; Classification (of information); Data communication systems; Decision trees; Fuzzy logic; Soft computing; Data mining",Conference Paper,Scopus,2-s2.0-84861045855
"Salazar M.R., Hook J.E., Garcia y Garcia A., Paz J.O., Chaves B., Hoogenboom G.","Estimating irrigation water use for maize in the Southeastern USA: A modeling approach",2012,"Agricultural Water Management",18,10.1016/j.agwat.2012.01.015,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857994371&doi=10.1016%2fj.agwat.2012.01.015&partnerID=40&md5=afcbe7c93ff0970776d9fb73884e3b26","Increased crop production and expansion of irrigated acreage in the southeastern USA have increased agricultural water use during the past two decades. To optimize irrigation water use, it is important to know when to irrigate and how much water should be applied. The objectives of this study were (1) to evaluate the Cropping System Model (CSM)-CERES-Maize model with measured data of the amount of water required for supplemental irrigation and (2) to apply the CSM-CERES-Maize model for estimating irrigation water use for maize in the southeastern USA. The CSM-CERES-Maize model was evaluated for 2000-2004 for five counties that represent the dominant maize production regions in South Georgia. For each county, historical daily weather data, three representative soil profiles, and specific crop management recommendations were used as input for the model. The simulated results were then compared with observed data obtained during the same period. The amount of water required for irrigation for each growing season was simulated for 58 years using historical weather data from 1950 to 2007 for 88 selected counties that corresponded to the most important agricultural production region in Georgia. Both monthly and annual water demand for maize was determined for each county. The total seasonal amount of water required for irrigation across counties and years ranged from 136 to 281. mm, with an average of 227. mm. The irrigation requirements among months varied from 10 to 79. mm, with the highest amount required for May. The results from the evaluation showed that the model was able to simulate the amount of water required for maize irrigation in good agreement with the observed data. This demonstrated the potential application of the CSM-CERES-Maize model as a tool for estimating water demand for irrigation. The estimated water requirements for supplemental irrigation can be used by both policy makers and local farmers for planning the amount of water required for supplemental irrigation as well as for improvements in irrigation management for water conservation. © 2012 Elsevier B.V.","Crop simulation models; Decision Support Systems; DSSAT; Irrigated maize; Water use","Crop simulation model; Decision supports; DSSAT; Irrigated maize; Water use; Artificial intelligence; Computer simulation; Crops; Cultivation; Decision support systems; Estimation; Meteorology; Soils; Water conservation; Water supply; Irrigation; agricultural modeling; crop production; decision support system; growing season; irrigation system; maize; optimization; policy making; soil profile; water demand; water management; water planning; water supply; water use; United States; Zea mays",Article,Scopus,2-s2.0-84857994371
"Dehni A., Lounis M.","Remote sensing techniques for salt affected soil mapping: Application to the Oran region of Algeria",2012,"Procedia Engineering",18,10.1016/j.proeng.2012.01.1193,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859960181&doi=10.1016%2fj.proeng.2012.01.1193&partnerID=40&md5=4dbeb1344ac8b7a82a47d64912f0ad02","Satellite remote sensing of land affected by salinity, is a useful tool for decision support system through digital image processing for materials delineation (crystallography, detection of rocks, mineralogy, etc.). Today, with the advent of technology integration, merger, with optical data radar (InSAR and Signal Processing) actively contributed to the modelling of radar backscattering coefficient for the quantitative and qualitative salinity (modelled coefficients for models in relation to soil moisture, surface roughness). Thus, our approach has been to exploit the multi-spectral optical data from the LANDSAT ETM + (Enhanced Thematic Mapper) to map surface states, including indices of salinity and sodicity as: (BI: Brightness Index), NDSI: Normalized Difference Salinity Index, SI: Salinity Index, ASI: Aster Salinity Index (Agriculture), Index of Salinity (using GIS Geographic Information System and remote sensing), and finally the SSSI ""Soil Salinity and Sodicity Index"". These indicators of salinity were tested for the Oran region in accordance with the spectral sensor ALI (Advanced Land Imager) satellite EO-1 (NASA from 2002 to 2006). Remote sensing helps identify salts are highly reflective and improved mapping of saline soil surface. Reports of More frequently used is the combined near infrared and visible (4 / 1 ETM), or bands in the infrared (7 / 4 or 7 / 5 ETM). Consequently, the spectral curves of the satellite ALI EO-1 show a match for saline soils and two test plots were chosen (Aquifer of Es-Sénia) to study corresponding with the measured data in-situ (electrical conductivity and pH) for the classification of saline soils [2]. The confusions that arise between the effects of salt stress and water stress are removed followed by seasonal applying the Geo-statistical analysis with the Geo-modelling approach in GIS techniques investigation and monitoring the variation of the electrical conductivity in the alluvial aquifer of Es-Sénia for the salt affected soil and segmentation accuracy model. © 2012 Published by Elsevier Ltd.","Aquifer; Modelling; Radiometric index; Reflectance; Remote sensing; Salt; Soil; Spectral; Surface","Advanced land imagers; Algeria; Alluvial aquifers; Brightness index; Electrical conductivity; Enhanced thematic mappers; In-situ; Landsat ETM+; Measured data; Modelling; Multi-spectral; Near Infrared; Normalized differences; Optical data; Radar backscattering coefficient; Radiometric index; Remote sensing techniques; Saline soil; Salinity indices; Salt stresss; Salt-affected soil; Satellite remote sensing; Segmentation accuracy; Sodicity; Soil salinity; Spectral; Spectral curves; Spectral sensors; Technology Integration; Water stress; Aquifers; Artificial intelligence; Crystallography; Decision support systems; Electric conductivity; Geodetic satellites; Geographic information systems; Image processing; Mineralogy; Minerals; NASA; Optical data processing; Radar; Reflection; Remote sensing; Salinity measurement; Salts; Soil moisture; Soils; Surface roughness; Surfaces; Geologic models",Conference Paper,Scopus,2-s2.0-84859960181
"Abe M., Haralambiev K., Ohkubo M.","Group to group commitments do not shrink",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",18,10.1007/978-3-642-29011-4_19,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859952528&doi=10.1007%2f978-3-642-29011-4_19&partnerID=40&md5=84efe7f698e50c9aa705ef33e1b5bd7e","We investigate commitment schemes whose messages, keys, commitments, and decommitments are elements of bilinear groups, and whose openings are verified by pairing product equations. Such commitments facilitate efficient zero-knowledge proofs of knowledge of a correct opening. We show two lower bounds on such schemes: a commitment cannot be shorter than the message and verifying the opening in a symmetric bilinear group setting requires evaluating at least two independent pairing product equations. We also present optimal constructions that match the lower bounds in symmetric and asymmetric bilinear group settings. © 2012 International Association for Cryptologic Research.","Homomorphic Trapdoor Commitments; Structure-Preserving Commitments","Commitment scheme; Lower bounds; Optimal construction; Structure-preserving; Trapdoor commitments; Zero-knowledge proofs of knowledge; Bilinear groups; Commitment scheme; Homomorphic trapdoor commitments; Lower bounds; Optimal construction; Structure-Preserving Commitments; Zero-knowledge proofs of knowledge; Artificial intelligence; Artificial intelligence; Computers; Cryptography; Cryptography",Conference Paper,Scopus,2-s2.0-84859952528
"Unruh D.","Quantum proofs of knowledge",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",18,10.1007/978-3-642-29011-4_10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859959109&doi=10.1007%2f978-3-642-29011-4_10&partnerID=40&md5=9174c460ae2ee52c9c86c04d684a783a","We motivate, define and construct quantum proofs of knowledge, proofs of knowledge secure against quantum adversaries. Our constructions are based on a new quantum rewinding technique that allows us to extract witnesses in many classical proofs of knowledge. We give criteria under which a classical proof of knowledge is a quantum proof of knowledge. Combining our results with Watrous' results on quantum zero-knowledge, we show that there are zero-knowledge quantum proofs of knowledge for all languages in NP (assuming quantum 1-1 one-way functions). © 2012 International Association for Cryptologic Research.",,"One-way functions; Zero knowledge; One-way functions; Zero knowledge; Artificial intelligence; Cryptography; Quantum cryptography; Quantum cryptography",Conference Paper,Scopus,2-s2.0-84859959109
"Burckhardt S., Leijen D., Fähndrich M., Sagiv M.","Eventually consistent transactions",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",18,10.1007/978-3-642-28869-2_4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859114946&doi=10.1007%2f978-3-642-28869-2_4&partnerID=40&md5=64381608f416c08ddd1b8e766d4b3b68","When distributed clients query or update shared data, eventual consistency can provide better availability than strong consistency models. However, programming and implementing such systems can be difficult unless we establish a reasonable consistency model, i.e. some minimal guarantees that programmers can understand and systems can provide effectively. To this end, we propose a novel consistency model based on eventually consistent transactions. Unlike serializable transactions, eventually consistent transactions are ordered by two order relations (visibility and arbitration) rather than a single order relation. To demonstrate that eventually consistent transactions can be effectively implemented, we establish a handful of simple operational rules for managing replicas, versions and updates, based on graphs called revision diagrams. We prove that these rules are sufficient to guarantee correct implementation of eventually consistent transactions. Finally, we present two operational models (single server and server pool) of systems that provide eventually consistent transactions. © 2012 Springer-Verlag.",,"Consistency model; Eventual consistency; Operational model; Order relation; Shared data; Single server; Strong consistency; Consistency model; Eventual consistency; Operational model; Order relation; Shared data; Single server; Strong consistency; Artificial intelligence; Artificial intelligence; Computer science; Computers",Conference Paper,Scopus,2-s2.0-84859114946
"Jourdan J.-H., Pottier F., Leroy X.","Validating LR(1) parsers",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",18,10.1007/978-3-642-28869-2_20,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859141633&doi=10.1007%2f978-3-642-28869-2_20&partnerID=40&md5=8112cd68fad44a9715342b07852807dd","An LR(1) parser is a finite-state automaton, equipped with a stack, which uses a combination of its current state and one lookahead symbol in order to determine which action to perform next. We present a validator which, when applied to a context-free grammar and an automaton , checks that and agree. Validating the parser provides the correctness guarantees required by verified compilers and other high-assurance software that involves parsing. The validation process is independent of which technique was used to construct . The validator is implemented and proved correct using the Coq proof assistant. As an application, we build a formally-verified parser for the C99 language. © 2012 Springer-Verlag.",,"Coq proof assistant; Finite-state automata; Look-ahead; Validation process; Coq proof assistant; High assurance; Validation process; Artificial intelligence; Context free grammars; Theorem proving",Conference Paper,Scopus,2-s2.0-84859141633
"Čongradac V., Kulić F.","Recognition of the importance of using artificial neural networks and genetic algorithms to optimize chiller operation",2012,"Energy and Buildings",18,10.1016/j.enbuild.2012.01.007,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857060939&doi=10.1016%2fj.enbuild.2012.01.007&partnerID=40&md5=4ef02ca4ece0164e25af0118f3715dd0","This paper presents the optimization of chillers operating using artificial neural networks and genetic algorithms. For the needs of generating chiller models, an artificial neural network was used, trained with data collected from an actual chiller. For that purpose the basic characteristics of artificial neural networks are shown as well as the process of making specific chiller models used for testing the results of application of the genetic algorithm in usage optimization. The optimal criteria with the shown steps for the use of the genetic algorithm and optimization results is also displayed in the paper. The results of use of artificial intelligence methods in optimization of chiller operation are verified through an actual office building model created in the simulation software EnergyPlus and through a series of experiments on an actual office building, equipped with a modern integrated BMS. © 2012 Elsevier B.V.","Chiller; Control; Genetic algorithm; Optimization","Artificial intelligence methods; Artificial Neural Network; Chiller; EnergyPlus; Optimal criteria; Simulation software; Computer simulation; Computer software; Control; Genetic algorithms; Neural networks; Office buildings; Optimization; Cooling systems",Article,Scopus,2-s2.0-84857060939
"Tuba M., Bacanin N., Stanarevic N.","Adjusted artificial bee colony (ABC) algorithm for engineering problems",2012,"WSEAS Transactions on Computers",18,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867012948&partnerID=40&md5=7aea5663534fa4e28eaf60624fff7c1b","In this paper we present a modified algorithm which integrates artificial bee colony (ABC) algorithm with adaptive guidance adjusted for constrained engineering optimization problems. The novel algorithm improves best found solutions in some cases and improves robustness i.e. mean value and variance for number of runs in other cases by improving the algorithm's exploitation/ exploration balance. Even though scout bee phase is used for exploration, we introduced adaptive parameter that at different stages of the algorithm narrows search space facilitating faster convergence. We tested our algorithm on four standard engineering benchmark problems. The experimental results show that our modified algorithm can outperform the pure ABC algorithm in most cases.","Artificial bee colony (ABC); Constrained optimization; Metaheuristic optimization; Swarm intelligence","Abc algorithms; Adaptive guidance; Adaptive parameters; Artificial bee colonies; Bench-mark problems; Constrained engineering optimization problems; Engineering problems; Faster convergence; Mean values; Metaheuristic optimization; Modified algorithms; Novel algorithm; Search spaces; Swarm Intelligence; Artificial intelligence; Constrained optimization; Algorithms",Article,Scopus,2-s2.0-84867012948
"Trainor L.J.","Musical experience, plasticity, and maturation: Issues in measuring developmental change using EEG and MEG",2012,"Annals of the New York Academy of Sciences",18,10.1111/j.1749-6632.2012.06444.x,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860228088&doi=10.1111%2fj.1749-6632.2012.06444.x&partnerID=40&md5=ddf5d5d3e5fd2feabcec172990ef2473","The neuroscientific study of musical behavior has become a significant field of research during the last decade, and reports of this research in the popular press have caught the imagination of the public. This enterprise has also made it evident that studying the development of musical behavior can make a significant contribution to important questions in the field, such as the evolutionary origins of music, cross-cultural similarity and diversity, the effects of experience on musical processing, and relations between music and other domains. Studying musical development brings a unique set of methodological issues. We discuss a select set of these related to measurement of the electroencephalogram (EEG) and magnetoencephalogram (MEG). We use specific examples from our laboratory to illustrate the types of questions that can be answered with different data analysis techniques. © 2012 New York Academy of Sciences.","Artifact; Beta band; EEG; Gamma band; MEG; Music development; Oscillatory responses","article; auditory cortex; electroencephalography; event related potential; functional magnetic resonance imaging; human; machine learning; magnetoencephalography; music; nerve cell plasticity; oscillation; oscillatory potential; waveform; Artificial Intelligence; Auditory Cortex; Behavior; Brain; Child, Preschool; Electroencephalography; Humans; Infant; Magnetoencephalography; Music; Neuronal Plasticity",Article,Scopus,2-s2.0-84860228088
"Silva M.J., Carvalho P., Sarmento L.","Building a sentiment lexicon for social judgement mining",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",18,10.1007/978-3-642-28885-2_25,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858412156&doi=10.1007%2f978-3-642-28885-2_25&partnerID=40&md5=459f926d48b1d12664d200519dfb74b6","We present a methodology for automatically enlarging a Portuguese sentiment lexicon for mining social judgments from text, i.e., detecting opinions on human entities. Starting from publicly-availabe language resources, the identification of human adjectives is performed through the combination of a linguistic-based strategy, for extracting human adjective candidates from corpora, and machine learning for filtering the human adjectives from the candidate list. We then create a graph of the synonymic relations among the human adjectives, which is built from multiple open thesauri. The graph provides distance features for training a model for polarity assignment. Our initial evaluation shows that this method produces results at least as good as the best that have been reported for this task. © 2012 Springer-Verlag.",,"Candidate list; Distance feature; Language resources; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84858412156
"Choi S.G., Katz J., Kumaresan R., Zhou H.-S.","On the Security of the ""free-XOR"" Technique",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",18,10.1007/978-3-642-28914-9_3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863382720&doi=10.1007%2f978-3-642-28914-9_3&partnerID=40&md5=7486c8fb3f3d3597f4d1add72705be68","Yao's garbled-circuit approach enables constant-round secure two-party computation of any function. In Yao's original construction, each gate in the circuit requires the parties to perform a constant number of encryptions/decryptions and to send/receive a constant number of ciphertexts. Kolesnikov and Schneider (ICALP 2008) proposed an improvement that allows XOR gates to be evaluated ""for free,"" incurring no cryptographic operations and zero communication. Their ""free-XOR"" technique has proven very popular, and has been shown to improve performance of garbled-circuit protocols by up to a factor of 4. Kolesnikov and Schneider proved security of their approach in the random oracle model, and claimed that (an unspecified variant of) correlation robustness suffices; this claim has been repeated in subsequent work, and similar ideas have since been used in other contexts. We show that the free-XOR technique cannot be proven secure based on correlation robustness alone; somewhat surprisingly, some form of circular security is also required. We propose an appropriate definition of security for hash functions capturing the necessary requirements, and prove security of the free-XOR approach when instantiated with any hash function satisfying our definition. Our results do not impact the security of the free-XOR technique in practice, or imply an error in the free-XOR work, but instead pin down the assumptions needed to prove security. © 2012 Springer-Verlag.",,"Ciphertexts; Cryptographic operations; Random Oracle model; Schneider; Secure two-party computations; XOR gates; Ciphertexts; Cryptographic operations; Garbled circuits; Improve performance; Random Oracle model; Schneider; Secure two-party computations; XOR gates; Artificial intelligence; Computation theory; Hash functions; Reconfigurable hardware; Hash functions; Cryptography",Conference Paper,Scopus,2-s2.0-84863382720
"Chandran N., Chase M., Vaikuntanathan V.","Functional re-encryption and collusion-resistant obfuscation",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",18,10.1007/978-3-642-28914-9_23,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858329462&doi=10.1007%2f978-3-642-28914-9_23&partnerID=40&md5=28f6261e5ad31b6b5374ab2ac3aea9dc","We introduce a natural cryptographic functionality called functional re-encryption. Informally, this functionality, for a public-key encryption scheme and a function F with n possible outputs, transforms (""re- encrypts"") an encryption of a message m under an ""input public key"" pk into an encryption of the same message m under one of the n ""output public keys"", namely the public key indexed by F(m). In many settings, one might require that the program implementing the functional re-encryption functionality should reveal nothing about both the input secret key sk as well as the function F. As an example, consider a user Alice who wants her email server to share her incoming mail with one of a set of n recipients according to an access policy specified by her function F, but who wants to keep this access policy private from the server. Furthermore, in this setting, we would ideally obtain an even stronger guarantee: that this information remains hidden even when some of the n recipients may be corrupted. To formalize these issues, we introduce the notion of collusion-resistant obfuscation and define this notion with respect to average-case secure obfuscation (Hohenberger et al. - TCC 2007). We then provide a construction of a functional re-encryption scheme for any function F with a polynomial-size domain and show that it satisfies this notion of collusion-resistant obfuscation. We note that collusion-resistant security can be viewed as a special case of dependent auxiliary input security (a setting where virtually no positive results are known), and this notion may be of independent interest. Finally, we show that collusion-resistant obfuscation of functional re-encryption for a function F gives a way to obfuscate F in the sense of Barak et al. (CRYPTO 2001), indicating that this task is impossible for arbitrary (polynomial-time computable) functions F. © 2012 Springer-Verlag.",,"Access policies; Auxiliary inputs; Average-case; E-mail servers; Polynomial-time; Public keys; Public-key encryption scheme; Re-encryption; Secret key; Access policies; Auxiliary inputs; Collusion resistant; E-mail servers; Polynomial size; Polynomial-time; Public-key encryption scheme; Re-encryption; Artificial intelligence; Polynomial approximation; Public key cryptography; Public key cryptography; Cryptography",Conference Paper,Scopus,2-s2.0-84858329462
"Maurer A., Pontil M.","Structured sparsity and generalization",2012,"Journal of Machine Learning Research",18,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859471498&partnerID=40&md5=e321b84a56f91919dfef7572340ff330","We present a data dependent generalization bound for a large class of regularized algorithms which implement structured sparsity constraints. The bound can be applied to standard squared-norm regularization, the Lasso, the group Lasso, some versions of the group Lasso with overlapping groups, multiple kernel learning and other regularization schemes. In all these cases competitive results are obtained. A novel feature of our bound is that it can be applied in an infinite dimensional setting such as the Lasso in a separable Hilbert space or multiple kernel learning with a countable number of kernels. © 2012 Andreas Maurer and Massimiliano Pontil.","Empirical processes; Rademacher average; Sparse estimation","Data dependent; Empirical process; Generalization bound; Infinite dimensional; Multiple Kernel Learning; Overlapping groups; Rademacher average; Regularization schemes; Separable Hilbert space; Sparse estimation; Sparsity constraints; Artificial intelligence; Software engineering",Article,Scopus,2-s2.0-84859471498
"Kell D.B.","Scientific discovery as a combinatorial optimisation problem: How best to navigate the landscape of possible experiments?",2012,"BioEssays",18,10.1002/bies.201100144,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857045254&doi=10.1002%2fbies.201100144&partnerID=40&md5=d4ffc09a877a6e4fc90e947db42cdbed","A considerable number of areas of bioscience, including gene and drug discovery, metabolic engineering for the biotechnological improvement of organisms, and the processes of natural and directed evolution, are best viewed in terms of a 'landscape' representing a large search space of possible solutions or experiments populated by a considerably smaller number of actual solutions that then emerge. This is what makes these problems 'hard', but as such these are to be seen as combinatorial optimisation problems that are best attacked by heuristic methods known from that field. Such landscapes, which may also represent or include multiple objectives, are effectively modelled in silico, with modern active learning algorithms such as those based on Darwinian evolution providing guidance, using existing knowledge, as to what is the 'best' experiment to do next. An awareness, and the application, of these methods can thereby enhance the scientific discovery process considerably. This analysis fits comfortably with an emerging epistemology that sees scientific reasoning, the search for solutions, and scientific discovery as Bayesian processes. © 2012 WILEY Periodicals, Inc.","Automation; Epistemology; Evolutionary computing; Heuristics; Scientific discovery","immunoglobulin enhancer binding protein; nucleic acid; article; Bayesian learning; biotechnology; combinatorial chemistry; computer model; drug targeting; epistemology; evolution; fermentation; genetic engineering; landscape; learning algorithm; metabolic engineering; reproductive fitness; scientific literature; synthetic biology; Algorithms; Artificial Intelligence; Bayes Theorem; Biological Evolution; Computational Biology; Computers; Data Mining; Drug Discovery; Humans; Metabolic Engineering; Sensitivity and Specificity; Synthetic Biology",Article,Scopus,2-s2.0-84857045254
"Kumar M., Yadav S.P.","A novel approach for analyzing fuzzy system reliability using different types of intuitionistic fuzzy failure rates of components",2012,"ISA Transactions",18,10.1016/j.isatra.2011.10.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857046337&doi=10.1016%2fj.isatra.2011.10.002&partnerID=40&md5=c1189ecd9ddd3e5138add9804a0c346a","This paper addresses the fuzzy system reliability analysis using different types of intuitionistic fuzzy numbers. Till now, in the literature, to analyze the fuzzy system reliability, it is assumed that the failure rates of all components of a system follow the same type of fuzzy set or intuitionistic fuzzy set. However, in practical problems, such type of situation rarely occurs. Therefore, in the present paper, a new algorithm has been introduced to construct the membership function and non-membership function of fuzzy reliability of a system having components following different types of intuitionistic fuzzy failure rates. Functions of intuitionistic fuzzy numbers are calculated to construct the membership function and non-membership function of fuzzy reliability via non-linear programming techniques. Using the proposed algorithm, membership functions and non-membership functions of fuzzy reliability of a series system and a parallel systems are constructed. Our study generalizes the various works of the literature. Numerical examples are given to illustrate the proposed algorithm. © 2011 ISA. Published by Elsevier Ltd. All rights reserved.","Functions of intuitionistic fuzzy numbers; Fuzzy system reliability; Intuitionistic fuzzy number","Failure rate; Fuzzy reliability; Fuzzy system reliability; Intuitionistic fuzzy; Intuitionistic Fuzzy number; Intuitionistic fuzzy sets; Numerical example; Parallel system; Practical problems; Series system; System reliability; Algorithms; Fuzzy rules; Fuzzy systems; Reliability analysis; Membership functions; algorithm; article; artificial intelligence; computer simulation; equipment; equipment design; fuzzy logic; reproducibility; statistics; Algorithms; Artificial Intelligence; Computer Simulation; Equipment Design; Equipment Failure; Fuzzy Logic; Reproducibility of Results",Article,Scopus,2-s2.0-84857046337
"Pelagotti A., Paturzo M., Locatelli M., Geltrude A., Meucci R., Finizio A., Ferraro P.","An automatic method for assembling a large synthetic aperture digital hologram",2012,"Optics Express",18,10.1364/OE.20.004830,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857579099&doi=10.1364%2fOE.20.004830&partnerID=40&md5=313d8a241e4e9cd82c3614a73d11e622","A major issue so far for digital holography is the low spatial resolution generally achieved. The numerical aperture is limited by the area of currently available detectors, such as CCD sensors, which is significantly lower than that of a holographic plate. This is an even more severe constraint when IR sensors such as microbolometers are taken into account. In order to increase the numerical aperture of such systems, we developed an automatic technique which is capable of recording several holograms and of stitching them together, obtaining a digital hologram with a synthetic but larger numerical aperture. In this way we show that more detail can be resolved and a wider parallax angle can be achieved. The method is demonstrated for visible as well IR digital holography, recording and displaying large size objects. © 2012 Optical Society of America.",,"Computer generated holography; Geometrical optics; Automatic method; Automatic technique; CCD sensors; Digital holograms; Digital holography; IR sensor; Large sizes; Micro-bolometers; Numerical aperture; Spatial resolution; Holograms; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; holography; image enhancement; image subtraction; methodology; reproducibility; sensitivity and specificity; signal processing; three dimensional imaging; Algorithms; Artificial Intelligence; Holography; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Signal Processing, Computer-Assisted; Subtraction Technique",Article,Scopus,2-s2.0-84857579099
"Subirats J.L., Franco L., Jerez J.M.","C-Mantec: A novel constructive neural network algorithm incorporating competition between neurons",2012,"Neural Networks",18,10.1016/j.neunet.2011.10.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855953161&doi=10.1016%2fj.neunet.2011.10.003&partnerID=40&md5=45eb2713c9c218469492aff79626c7d6","C-Mantec is a novel neural network constructive algorithm that combines competition between neurons with a stable modified perceptron learning rule. The neuron learning is governed by the thermal perceptron rule that ensures stability of the acquired knowledge while the architecture grows and while the neurons compete for new incoming information. Competition makes it possible that even after new units have been added to the network, existing neurons still can learn if the incoming information is similar to their stored knowledge, and this constitutes a major difference with existing constructing algorithms. The new algorithm is tested on two different sets of benchmark problems: a Boolean function set used in logic circuit design and a well studied set of real world problems. Both sets were used to analyze the size of the constructed architectures and the generalization ability obtained and to compare the results with those from other standard and well known classification algorithms. The problem of overfitting is also analyzed, and a new built-in method to avoid its effects is devised and successfully applied within an active learning paradigm that filter noisy examples. The results show that the new algorithm generates very compact neural architectures with state-of-the-art generalization capabilities. © 2011 Elsevier Ltd.","Active learning; Constructive neural network; Feed-forward network; Generalization; Incremental learning; Overfitting","Active Learning; Constructive neural network; Feed-forward network; Generalization; Incremental learning; Overfitting; Algorithms; Boolean functions; Logic circuits; Logic design; Network architecture; Neural networks; article; artificial neural network; classification algorithm; Competitive MAjority Network Trained by Error Correction; information processing; information science; learning algorithm; nerve cell; perceptron; priority journal; probability; support vector machine; synapse; synaptic potential; temperature; Algorithms; Animals; Artificial Intelligence; Generalization (Psychology); Humans; Models, Neurological; Neurons",Article,Scopus,2-s2.0-84855953161
"Yoo J.-K., Kim J.-H.","Fuzzy integral-based gaze control architecture incorporated with modified-univector field-based navigation for humanoid robots",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",18,10.1109/TSMCB.2011.2162234,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856245397&doi=10.1109%2fTSMCB.2011.2162234&partnerID=40&md5=d2d6163b5b0573ddeee569062a34b621","When a humanoid robot moves in a dynamic environment, a simple process of planning and following a path may not guarantee competent performance for dynamic obstacle avoidance because the robot acquires limited information from the environment using a local vision sensor. Thus, it is essential to update its local map as frequently as possible to obtain more information through gaze control while walking. This paper proposes a fuzzy integral-based gaze control architecture incorporated with the modified-univector field-based navigation for humanoid robots. To determine the gaze direction, four criteria based on local map confidence, waypoint, self-localization, and obstacles, are defined along with their corresponding partial evaluation functions. Using the partial evaluation values and the degree of consideration for criteria, fuzzy integral is applied to each candidate gaze direction for global evaluation. For the effective dynamic obstacle avoidance, partial evaluation functions about self-localization error and surrounding obstacles are also used for generating virtual dynamic obstacle for the modified-univector field method which generates the path and velocity of robot toward the next waypoint. The proposed architecture is verified through the comparison with the conventional weighted sum-based approach with the simulations using a developed simulator for HanSaRam-IX (HSR-IX). © 2011 IEEE.","Choquet fuzzy integral; fuzzy measure; gaze control; humanoid robot navigation; preference-based selection algorithm; univector field method; virtual obstacle","Choquet fuzzy integral; Field methods; Fuzzy measures; gaze control; humanoid robot navigation; Preference-based; Virtual obstacle; Anthropomorphic robots; Function evaluation; Integral equations; Navigation; Robot programming; algorithm; article; artificial intelligence; automated pattern recognition; biomimetics; computer simulation; feedback system; fuzzy logic; methodology; motion; robotics; theoretical model; Algorithms; Artificial Intelligence; Biomimetics; Computer Simulation; Feedback; Fuzzy Logic; Models, Theoretical; Motion; Pattern Recognition, Automated; Robotics",Article,Scopus,2-s2.0-84856245397
"Cummings M.L., Marquez J.J., Roy N.","Human-automated path planning optimization and decision support",2012,"International Journal of Human Computer Studies",18,10.1016/j.ijhcs.2011.10.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054862499&doi=10.1016%2fj.ijhcs.2011.10.001&partnerID=40&md5=9474b3883d183c70d863ab325c96a38c","Path planning is a problem encountered in multiple domains, including unmanned vehicle control, air traffic control, and future exploration missions to the Moon and Mars. Due to the voluminous and complex nature of the data, path planning in such demanding environments requires the use of automated planners. In order to better understand how to support human operators in the task of path planning with computer aids, an experiment was conducted with a prototype path planner under various conditions to assess the effect on operator performance. Participants were asked to create and optimize paths based on increasingly complex path cost functions, using different map visualizations including a novel visualization based on a numerical potential field algorithm. They also planned paths under degraded automation conditions. Participants exhibited two types of analysis strategies, which were global path regeneration and local sensitivity analysis. No main effect due to visualization was detected, but results indicated that the type of optimizing cost function affected performance, as measured by metabolic costs, sun position, path distance, and task time. Unexpectedly, participants were able to better optimize more complex cost functions as compared to a simple time-based cost function. © 2011 Elsevier Ltd. All rights reserved.","Decision support systems; Humanautomation interaction; Path planning","Automation conditions; Complex nature; Decision supports; Exploration missions; Human operator; Human-automation interactions; Local sensitivity analysis; Main effect; Map visualizations; Metabolic cost; Multiple domains; Operator performance; Path planners; Potential field; Sun position; Air traffic control; Artificial intelligence; Automation; Control system synthesis; Costs; Decision support systems; Motion planning; Optimization; Sensitivity analysis; Unmanned vehicles; Visualization; Cost functions",Article,Scopus,2-s2.0-80054862499
"Moss G.P., Shah A.J., Adams R.G., Davey N., Wilkinson S.C., Pugh W.J., Sun Y.","The application of discriminant analysis and Machine Learning methods as tools to identify and classify compounds with potential as transdermal enhancers",2012,"European Journal of Pharmaceutical Sciences",18,10.1016/j.ejps.2011.10.027,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-83655201097&doi=10.1016%2fj.ejps.2011.10.027&partnerID=40&md5=8dc592fd57204a9b0ee45f774d8ece19","Discriminant analysis (DA) has previously been shown to allow the proposal of simple guidelines for the classification of 73 chemical enhancers of percutaneous absorption. Pugh et al. employed DA to classify such enhancers into simple categories, based on the physicochemical properties of the enhancer molecules (Pugh et al., 2005). While this approach provided a reasonable accuracy of classification it was unable to provide a consistently reliable estimate of enhancement ratio (ER, defined as the amount of hydrocortisone transferred after 24 h, relative to control). Machine Learning methods, including Gaussian process (GP) regression, have recently been employed in the prediction of percutaneous absorption of exogenous chemicals (Moss et al., 2009; Lam et al., 2010; Sun et al., 2011). They have shown that they provide more accurate predictions of these phenomena. In this study several Machine Learning methods, including the K-nearest-neighbour (KNN) regression, single layer networks, radial basis function networks and the SVM classifier were applied to an enhancer dataset reported previously. The SMOTE sampling method was used to oversample chemical compounds with ER > 10 in each training set in order to improve estimation of GP and KNN. Results show that models using five physicochemical descriptors exhibit better performance than those with three features. The best classification result was obtained by using the SVM method without dealing with imbalanced data. Following over-sampling, GP gives the best result. It correctly assigned 8 of the 12 ""good"" (ER > 10) enhancers and 56 of the 59 ""poor"" enhancers (ER < 10). Overall success rates were similar. However, the pharmaceutical advantages of the Machine Learning methods are that they can provide more accurate classification of enhancer type with fewer false-positive results and that, unlike discriminant analysis, they are able to make predictions of enhancer ability. © 2011 Elsevier B.V. All rights reserved.","Discriminant analysis; Gaussian process regression; Percutaneous absorption; Transdermal enhancers","penetration enhancing agent; article; chemical analysis; discriminant analysis; false positive result; k nearest neighbor; kernel method; machine learning; perceptron; physical chemistry; priority journal; support vector machine; Adjuvants, Pharmaceutic; Administration, Cutaneous; Animals; Anti-Inflammatory Agents; Artificial Intelligence; Discriminant Analysis; Hydrocortisone; Hydrogen Bonding; Hydrophobic and Hydrophilic Interactions; Mice; Mice, Hairless; Models, Biological; Molecular Weight; Physicochemical Phenomena; Skin; Skin Absorption; Solubility; Support Vector Machines",Article,Scopus,2-s2.0-83655201097
"Rao N., Recht B., Nowak R.","Universal measurement bounds for structured sparse signal recovery",2012,"Journal of Machine Learning Research",18,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937847125&partnerID=40&md5=c62c694840a06d44f5e1c63fe5c60e3d","Standard compressive sensing results state that to exactly recover an s sparse signal in Rp, one requires O (s · log p) measurements. While this bound is extremely useful in practice, often real world signals are not only sparse, but also exhibit structure in the sparsity pattern. We focus on group-structured patterns in this paper. Under this model, groups of signal coefficients are active (or inactive) together. The groups are predefined, but the particular set of groups that are active (i.e., in the signal support) must be learned from measurements. We show that exploiting knowledge of groups can further reduce the number of measurements required for exact signal recovery, and derive universal bounds for the number of measurements needed. The bound is universal in the sense that it only depends on the number of groups under consideration, and not the particulars of the groups (e.g., compositions, sizes, extents, overlaps, etc.). Experiments show that our result holds for a variety of overlapping group configurations.",,"Artificial intelligence; Compressed sensing; Recovery; Compressive sensing; Overlapping groups; Signal recovery; Signal support; Sparse signals; Sparsity patterns; Structured patterns; Structured sparse signal recoveries; Signal reconstruction",Conference Paper,Scopus,2-s2.0-84937847125
"Knorr M., Hitzler P., Maier F.","Reconciling OWL and non-monotonic rules for the Semantic Web",2012,"Frontiers in Artificial Intelligence and Applications",18,10.3233/978-1-61499-098-7-474,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878805516&doi=10.3233%2f978-1-61499-098-7-474&partnerID=40&md5=abd6b01192f881e74bbcf515337127d4","We propose a description logic extending SROIQ (the description logic underlying OWL 2 DL) and at the same time encompassing some of the most prominent monotonic and nonmonotonic rule languages, in particular Datalog extended with the answer set semantics. Our proposal could be considered a substantial contribution towards fulfilling the quest for a unifying logic for the Semantic Web. As a case in point, two non-monotonic extensions of description logics considered to be of distinct expressiveness until now are covered in our proposal. In contrast to earlier such proposals, our language has the ""look and feel"" of a description logic and avoids hybrid or first-order syntaxes. © 2012 The Author(s).",,"Artificial intelligence; Birds; Computational linguistics; Formal languages; Semantic Web; Answer set semantics; Datalog; Description logic; First order; Monotonic rules; Nonmonotonic; Unifying logic; Data description",Conference Paper,Scopus,2-s2.0-84878805516
"Zhang Z., Feng Z.","Two-stage updating pheromone for invariant ant colony optimization algorithm",2012,"Expert Systems with Applications",18,10.1016/j.eswa.2011.07.062,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-81855221666&doi=10.1016%2fj.eswa.2011.07.062&partnerID=40&md5=ac8fc896f2e1584adabeebe9de8deba5","Ant colony optimization (ACO) is a metaheuristic approach for combinatorial optimization problems. With the introduction of hypercube framework, invariance property of ACO algorithms draws more attention. In this paper, we propose a novel two-stage updating pheromone for invariant ant colony optimization (TSIACO) algorithm. Compared with standard ACO algorithms, TSIACO algorithm uses solution order other than solution itself as independent variable for quality function. In addition, the pheromone trail is updated with two stages: in one stage, the first r iterative optimal solutions are employed to enhance search capability, and in another stage, only optimal solution is used to accelerate the speed of convergence. And besides, the pheromone value is limited to an interval. We prove that TSIACO not only has the property of linear transformational invariance but also has translational invariance. We also prove that the pheromone trail can limit to the interval (0, 1]. Computational results on the traveling salesman problem show the effectiveness of TSIACO algorithm. © 2011 Elsevier Ltd. All rights reserved.","Ant colony optimization; Invariance; Quality function; Solution order; Two-stage","ACO algorithms; Ant Colony Optimization algorithms; Ant-colony optimization; Combinatorial optimization problems; Computational results; Hypercube; Independent variables; Meta-heuristic approach; Optimal solutions; Pheromone trails; Search capabilities; Speed of convergence; Translational invariance; Two stage; Artificial intelligence; Combinatorial optimization; Invariance; Optimal systems; Traveling salesman problem; Algorithms",Article,Scopus,2-s2.0-81855221666
"Nagata K., Randall A., Baldi P.","SIDEpro: A novel machine learning approach for the fast and accurate prediction of side-chain conformations",2012,"Proteins: Structure, Function and Bioinformatics",18,10.1002/prot.23170,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-83555178443&doi=10.1002%2fprot.23170&partnerID=40&md5=5185d4c3426d226bdad89ef5cbaee25e","Accurate protein side-chain conformation prediction is crucial for protein modeling and existing methods for the task are widely used; however, faster and more accurate methods are still required. Here we present a new machine learning approach to the problem where an energy function for each rotamer in a structure is computed additively over pairs of contacting atoms. A family of 156 neural networks indexed by amino acid and contacting atom types is used to compute these rotamer energies as a function of atomic contact distances. Although direct energy targets are not available for training, the neural networks can still be optimized by converting the energies to probabilities and optimizing these probabilities using Markov Chain Monte Carlo methods. The resulting predictor SIDEpro makes predictions by initially setting the rotamer probabilities for each residue from a backbone-dependent rotamer library, then iteratively updating these probabilities using the trained neural networks. After convergences of the probabilities, the side-chains are set to the highest probability rotamer. Finally, a post processing clash reduction step is applied to the models. SIDEpro represents a significant improvement in speed and a modest, but statistically significant, improvement in accuracy when compared with the state-of-the-art for rapid side-chain prediction method SCWRL4 on the following datasets: (1) 379 protein test set of SCWRL4; (2) 94 proteins from CASP9; (3) a set of seven large protein-only complexes; and (4) a ribosome with and without the RNA. Using the SCWRL4 test set, SIDEpro's accuracy (χ 1 86.14%, χ 1+2 74.15%) is slightly better than SCWRL4-FRM (χ 1 85.43%, χ 1+2 73.47%) and it is 7.0 times faster. On the same test set SIDEpro is clearly more accurate than SCWRL4-rigid rotamer model (RRM) (χ 1 84.15%, χ 1+2 71.24%) and 2.4 times faster. Evaluation on the additional test sets yield similar accuracy results with SIDEpro being slightly more accurate than SCWRL4-flexible rotamer model (FRM) and clearly more accurate than SCWRL4-RRM; however, the gap in CPU time is much more significant when the methods are applied to large protein complexes. SIDEpro is part of the SCRATCH suite of predictors and available from: http://scratch.proteomics.ics.uci.edu/. © 2011 Wiley Periodicals, Inc.","Machine learning; Monte Carlo methods; Neural network; Protein structure prediction; Side chain conformation","amino acid; peptide library; RNA; article; atom; controlled study; machine learning; Monte Carlo method; priority journal; probability; protein conformation; protein function; protein secondary structure; protein structure; ribosome; Algorithms; Amino Acids; Artificial Intelligence; Caspase 9; Computer Simulation; Models, Molecular; Protein Structure, Secondary; Ribosomes; Software",Article,Scopus,2-s2.0-83555178443
"Banik S., Maitra S., Sarkar S.","A differential fault attack on the grain family under reasonable assumptions",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",17,10.1007/978-3-642-34931-7_12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871568714&doi=10.1007%2f978-3-642-34931-7_12&partnerID=40&md5=a71acbe18eb67578f55595a61ca8624d","In this paper we study a differential fault attack against ciphers having the same physical structure as in the Grain family. In particular we demonstrate our attack against Grain v1, Grain-128 and Grain-128a. The existing attacks by Berzati et al. (HOST 2009), Karmakar et al. (Africacrypt 2011) and Banik et al. (CHES 2012) assume a fault model that allows them to reproduce a fault at a particular register location more than once. However, we assume a realistic fault model in which the above assumption is no longer necessary, i.e., re-injecting the fault in the same location more than once is not required. In addition, towards a more practical framework, we also consider the situation in which more than one consecutive locations of the LFSR are flipped as result of a single fault injection. © Springer-Verlag 2012.","Differential fault attacks; Grain v1; Grain-128; Grain-128a; LFSR; NFSR; Stream cipher","Differential fault attack; Grain-128; Grain-128a; LFSR; NFSR; Stream Ciphers; Artificial intelligence; Cryptography",Conference Paper,Scopus,2-s2.0-84871568714
"Lambrou A., Papadopoulos H., Nouretdinov I., Gammerman A.","Reliable probability estimates based on support vector machines for large multiclass datasets",2012,"IFIP Advances in Information and Communication Technology",17,10.1007/978-3-642-33412-2_19,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870861519&doi=10.1007%2f978-3-642-33412-2_19&partnerID=40&md5=66d5575443931429ca533e03d63c1ee3","Venn Predictors (VPs) are machine learning algorithms that can provide well calibrated multiprobability outputs for their predictions. The only drawback of Venn Predictors is their computational inefficiency, especially in the case of large datasets. In this work, we propose an Inductive Venn Predictor (IVP) which overcomes the computational inefficiency problem of the original Venn Prediction framework. Each VP is defined by a taxonomy which separates the data into categories. We develop an IVP with a taxonomy derived from a multiclass Support Vector Machine (SVM), and we compare our method with other probabilistic methods for SVMs, namely Platt's method, SVM Binning, and SVM with Isotonic Regression. We show that these methods do not always provide well calibrated outputs, while our IVP will always guarantee this property under the i.i.d. assumption. © 2012 IFIP International Federation for Information Processing.","Inductive Venn Predictor; Machine Learning; multiclass; Support Vector Machine; well calibrated probabilities","Data sets; Inductive Venn Predictor; Isotonic regression; Large datasets; Multi-class; Multiclass support vector machines; Probabilistic methods; Probability estimate; Artificial intelligence; Learning algorithms; Learning systems; Taxonomies; Support vector machines",Conference Paper,Scopus,2-s2.0-84870861519
"Guo H., Cao X., Liu Z., Li H., Chen J., Zhang K.","Machine learning classifier using abnormal brain network topological metrics in major depressive disorder",2012,"NeuroReport",17,10.1097/WNR.0b013e32835a650c,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869508879&doi=10.1097%2fWNR.0b013e32835a650c&partnerID=40&md5=6921d37717c54cfb92e11408c5cdbeb8","Resting state functional brain networks have been widely studied in brain disease research. However, it is currently unclear whether abnormal resting state functional brain network metrics can be used with machine learning for the classification of brain diseases. Resting state functional brain networks were constructed for 28 healthy controls and 38 major depressive disorder patients by thresholding partial correlation matrices of 90 regions. Three nodal metrics were calculated using graph theory-based approaches. Nonparametric permutation tests were then used for group comparisons of topological metrics, which were used as classified features in six different algorithms. We used statistical significance as the threshold for selecting features and measured the accuracies of six classifiers with different number of features. A sensitivity analysis method was used to evaluate the importance of different features. The result indicated that some of the regions exhibited significantly abnormal nodal centralities, including the limbic system, basal ganglia, medial temporal, and prefrontal regions. Support vector machine with radial basis kernel function algorithm and neural network algorithm exhibited the highest average accuracy (79.27 and 78.22%, respectively) with 28 features (P<0.05). Correlation analysis between feature importance and the statistical significance of metrics was investigated, and the results revealed a strong positive correlation between them. Overall, the current study demonstrated that major depressive disorder is associated with abnormal functional brain network topological metrics and statistically significant nodal metrics can be successfully used for feature selection in classification algorithms. © 2012 Wolters Kluwer Health | Lippincott Williams & Wilkins.","brain network; classifier; depression; feature selection; graph theory; machine learning","accuracy; adolescent; adult; algorithm; angular gyrus; article; basal ganglion; brain; brain function; calcarine sulcus; classifier; clinical article; controlled study; cuneus; female; frontal gyrus; fusiform gyrus; hippocampus; human; kernel method; limbic system; major depression; male; nerve cell network; posterior cingulate; prefrontal cortex; priority journal; putamen; rest; sensitivity analysis; support vector machine; temporal lobe; thalamus; Adolescent; Adult; Artificial Intelligence; Basal Ganglia; Brain Mapping; Depressive Disorder, Major; Diagnosis, Computer-Assisted; Female; Gyrus Cinguli; Hippocampus; Humans; Limbic System; Male; Middle Aged; Models, Neurological; Nerve Net; Prefrontal Cortex; Statistics, Nonparametric; Support Vector Machines; Temporal Lobe; Young Adult",Article,Scopus,2-s2.0-84869508879
"Gupta R., Sachdeva A., Bhardwaj A.","Selection of logistic service provider using fuzzy PROMETHEE for a cement industry",2012,"Journal of Manufacturing Technology Management",17,10.1108/17410381211267727,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870274573&doi=10.1108%2f17410381211267727&partnerID=40&md5=97e22c9ba3ff4cb734ec176932cdc10b","Purpose - The purpose of this paper is to develop a method to select the best alternative in a multi-criteria decision making (MCDM) environment when the decision is taken by a group of members in an uncertain environment. Design/methodology/approach - In this paper, Fuzzy Preference Ranking Organization Method for Enrichment Evaluations (Fuzzy PROMETHEE) technique has been used for MCDM problems. The team of decision makers is constituted to integrate their opinion. The analysis is done using Geometrical Analysis for Interactive Aid (GAIA) plane, available in Decision Lab 2000 software, which provides valuable help in understanding the conflicts among criteria. Findings - The selection of best alternative is done on the basis of generally conflicting criteria. Fuzzy PROMETHEE technique has been proposed and the same is demonstrated using Decision Lab 2000 software. This software can be used for as many criteria as possible and also in a fuzzy environment, where the crisp data for criteria comparison are not available. It is found that the analysis of the results becomes very easy and effective with this software. A case study is conducted for a cement company to select the logistic service providers (LSPs) to demonstrate its ease and effectiveness of use. Originality/value - The research provides a model to choose the best alternative using Decision Lab 2000 software for Fuzzy PROMETHEE technique. The proposed methodology can be used in a fuzzy environment with ease and effectiveness. In the competitive scenario, this could help the industry in prompt and efficient decision making in MCDM problems. © Emerald Group Publishing Limited.","Computer software; Decision makers; Fuzzy analytic hierarchy process; Fuzzy PROMETHEE; Geometrical analysis for interactive aid; Group decision support systems; Multi-criteria decision making","Decision makers; Fuzzy analytic hierarchy process; Geometrical analysis; Multi-criteria decision making; PROMETHEE; Artificial intelligence; Cements; Computer software; Decision making; Decision support systems; Cement industry",Article,Scopus,2-s2.0-84870274573
"Zhang H., Wu K.","A vehicle detection algorithm based on three-frame differencing and background subtraction",2012,"Proceedings - 2012 5th International Symposium on Computational Intelligence and Design, ISCID 2012",17,10.1109/ISCID.2012.45,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873359872&doi=10.1109%2fISCID.2012.45&partnerID=40&md5=2133a17b8c955c4d1b2e5ae8005d7f38","A vehicle detection algorithm based on three-frame differencing and background subtraction is presented in this paper. Firstly, improved GMM is for background subtraction, then the moving object region is gained using background subtraction, and then the background subtraction is combined with three-frame differencing to detect the motion information. The simulation results show that the proposed algorithm runs veraciously and can lower the false detection rate, and fits for real time detection. © 2012 IEEE.","Background subtraction; Motion detection; Three-frame differencing","Background subtraction; False detections; Motion detection; Motion information; Moving objects; Real-time detection; Three-frame differencing; Vehicle detection; Artificial intelligence; Signal detection",Conference Paper,Scopus,2-s2.0-84873359872
"Bojańczyk M., Lasota Sł.","A machine-independent characterization of timed languages",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",17,10.1007/978-3-642-31585-5-12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883368723&doi=10.1007%2f978-3-642-31585-5-12&partnerID=40&md5=5bb13c3b37d80648427963bdc42748dd","We use a variant of Fraenkel-Mostowski sets (known also as nominal sets) as a framework suitable for stating and proving the following two results on timed automata. The first result is a machine-independent characterization of languages of deterministic timed automata. As a second result we define a class of automata, called by us timed register automata, that extends timed automata and is effectively closed under minimization. © 2012 Springer-Verlag Berlin Heidelberg.",,"Nominal Sets; Timed Automata; Timed languages; Artificial intelligence; Computer science; Automata theory",Conference Paper,Scopus,2-s2.0-84883368723
"Ihler A., Flerova N., Dechter R., Otten L.","Join-graph based cost-shifting schemes",2012,"Uncertainty in Artificial Intelligence - Proceedings of the 28th Conference, UAI 2012",17,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886052978&partnerID=40&md5=41e2c845d81272f51aa7794887ab973c","We develop several algorithms taking advantage of two common approaches for bounding MPE queries in graphical models: minibucket elimination and message-passing updates for linear programming relaxations. Both methods are quite similar, and offer useful perspectives for the other; our hybrid approaches attempt to balance the advantages of each. We demonstrate the power of our hybrid algorithms through extensive empirical evaluation. Most notably, a Branch and Bound search guided by the heuristic function calculated by one of our new algorithms has recently won first place in the PASCAL2 inference challenge.",,"Branch and bound search; Empirical evaluations; GraphicaL model; Heuristic functions; Hybrid algorithms; Hybrid approach; Linear programming relaxation; Artificial intelligence; Graphic methods; Heuristic algorithms; Linear programming; Message passing; Inference engines",Conference Paper,Scopus,2-s2.0-84886052978
"Geller J., Ochs C., Perl Y., Xu J.","New abstraction networks and a new visualization tool in support of auditing the SNOMED CT content.",2012,"AMIA ... Annual Symposium proceedings / AMIA Symposium. AMIA Symposium",17,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880850994&partnerID=40&md5=484e89f7088d804b9f771ad1fa66db63","Medical terminologies are large and complex. Frequently, errors are hidden in this complexity. Our objective is to find such errors, which can be aided by deriving abstraction networks from a large terminology. Abstraction networks preserve important features but eliminate many minor details, which are often not useful for identifying errors. Providing visualizations for such abstraction networks aids auditors by allowing them to quickly focus on elements of interest within a terminology. Previously we introduced area taxonomies and partial area taxonomies for SNOMED CT. In this paper, two advanced, novel kinds of abstraction networks, the relationship-constrained partial area subtaxonomy and the root-constrained partial area subtaxonomy are defined and their benefits are demonstrated. We also describe BLUSNO, an innovative software tool for quickly generating and visualizing these SNOMED CT abstraction networks. BLUSNO is a dynamic, interactive system that provides quick access to well organized information about SNOMED CT.",,"article; artificial intelligence; classification; quality control; Systematized Nomenclature of Medicine; Artificial Intelligence; Classification; Quality Control; Systematized Nomenclature of Medicine",Article,Scopus,2-s2.0-84880850994
"Huerta R., Vembu S., Amigó J.M., Nowotny T., Elkan C.","Inhibition in multiclass classification",2012,"Neural Computation",17,10.1162/NECO_a_00321,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871856618&doi=10.1162%2fNECO_a_00321&partnerID=40&md5=2b6977db78494a8a55e8c74c4cc5a523","The role of inhibition is investigated in a multiclass support vector machine formalism inspired by the brain structure of insects. The so-called mushroom bodies have a set of output neurons, or classification functions, that compete with each other to encode a particular input. Strongly active output neurons depress or inhibit the remaining outputs without knowing which is correct or incorrect. Accordingly, we propose to use a classification function that embodies unselective inhibition and train it in the large margin classifier framework. Inhibition leads to more robust classifiers in the sense that they perform better on larger areas of appropriate hyperparameters when assessed with leave-one-out strategies.We also show that the classifier with inhibition is a tight bound to probabilistic exponential models and is Bayes consistent for 3-class problems. These properties make this approach useful for data sets with a limited number of labeled examples. For larger data sets, there is no significant comparative advantage to other multiclass SVM approaches. © 2012 Massachusetts Institute of Technology.",,"algorithm; animal; article; artificial intelligence; automated pattern recognition; brain; classification; histology; human; inhibition (psychology); methodology; physiology; statistical model; statistics; synapse; Algorithms; Animals; Artificial Intelligence; Brain; Classification; Humans; Inhibition (Psychology); Models, Statistical; Pattern Recognition, Automated; Stochastic Processes; Synapses",Article,Scopus,2-s2.0-84871856618
"Rak M., Aversa R., Venticinque S., Di Martino B.","User centric service level management in mOSAIC applications",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",17,10.1007/978-3-642-29740-3-13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881454333&doi=10.1007%2f978-3-642-29740-3-13&partnerID=40&md5=c9eabdda9303918eec6387073d075228","Service Level Agreements (SLAs) aims at offering a simple and clear way to build up an agreement between the final users and the service provider in order to establish what is effectively granted by the cloud providers. In this paper we will show the SLA-related activities in mOSAIC, an european funded project that aims at exploiting a new programming model, which fully acquires the flexibility and dynamicity of the cloud environment, in order to build up a dedicated solution for SLA management. The key idea of SLA management in mOSAIC is that it is impossible to offer a single, static general purpose solution for SLA management of any kind of applications, but it is possible to offer a set of micro-functionalities that can be easily integrated among them in order to build up a dedicated solution for the application developer problem. Due to the mOSAIC API approach (which enable easy interoperability among moSAIC components) it will be possible to build up applications enriching them with user-oriented SLA management, from the very early development stages. © 2012 Springer-Verlag Berlin Heidelberg.",,"Application developers; Cloud environments; Cloud providers; Development stages; Programming models; Service level agreement (SLAs); Service provider; User-centric service; Artificial intelligence; Computer science; Parallel architectures",Conference Paper,Scopus,2-s2.0-84881454333
"Zhong S., Zeng X., Wu S., Han L.","Sensitivity-based adaptive learning rules for binary feedforward neural networks",2012,"IEEE Transactions on Neural Networks and Learning Systems",17,10.1109/TNNLS.2011.2177860,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875242151&doi=10.1109%2fTNNLS.2011.2177860&partnerID=40&md5=1748097878759555e7799e5904462c87","This paper proposes a set of adaptive learning rules for binary feedforward neural networks (BFNNs) by means of the sensitivity measure that is established to investigate the effect of a BFNN's weight variation on its output. The rules are based on three basic adaptive learning principles: the benefit principle, the minimal disturbance principle, and the burden-sharing principle. In order to follow the benefit principle and the minimal disturbance principle, a neuron selection rule and a weight adaptation rule are developed. Besides, a learning control rule is developed to follow the burden-sharing principle. The advantage of the rules is that they can effectively guide the BFNN's learning to conduct constructive adaptations and avoid destructive ones. With these rules, a sensitivity-based adaptive learning (SBALR) algorithm for BFNNs is presented. Experimental results on a number of benchmark data demonstrate that the SBALR algorithm has better learning performance than the Madaline rule II and backpropagation algorithms. © 2012 IEEE.","Adaptive learning algorithm; binary feedforward neural networks; learning rule; sensitivity","Adaptation rules; Adaptive learning; Adaptive learning algorithm; Constructive adaptations; Learning performance; Learning rules; sensitivity; Sensitivity measures; Benchmarking; Feedforward neural networks; Sensitivity analysis; Learning algorithms; artificial intelligence; artificial neural network; factual database; statistics and numerical data; Artificial Intelligence; Databases, Factual; Neural Networks (Computer)",Article,Scopus,2-s2.0-84875242151
"Liu B.","Membership functions and operational law of uncertain sets",2012,"Fuzzy Optimization and Decision Making",17,10.1007/s10700-012-9128-7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870677732&doi=10.1007%2fs10700-012-9128-7&partnerID=40&md5=7c829e1ce98dcac6910b011a9b1bf105","Uncertain set is a set-valued function on an uncertainty space, and attempts to model ""unsharp concepts"" that are essentially sets but their boundaries are not sharply described. This paper will propose a concept of membership function and define the independence of uncertain sets. This paper will also present an operational law of uncertain sets via membership functions or inverse membership functions. Finally, the linearity of expected value operator is verified. © Springer Science+Business Media, LLC 2012.","Membership function; Uncertain measure; Uncertain set; Uncertainty theory","Expected value operators; Set-valued functions; Uncertain measures; Uncertain set; Uncertainty space; Uncertainty theory; Artificial intelligence; Decision making; Membership functions",Article,Scopus,2-s2.0-84870677732
"Zhang M., Ranjan R., Haller A., Georgakopoulos D., Strazdins P.","Investigating decision support techniques for automating Cloud service selection",2012,"CloudCom 2012 - Proceedings: 2012 4th IEEE International Conference on Cloud Computing Technology and Science",17,10.1109/CloudCom.2012.6427501,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874234525&doi=10.1109%2fCloudCom.2012.6427501&partnerID=40&md5=70b2eb7ec9f4996581580cd20a727fc7","The compass of Cloud infrastructure services advances steadily leaving users in the agony of choice. To be able to select the best mix of service offering from an abundance of possibilities, users must consider complex dependencies and heterogeneous sets of criteria. Therefore, we present a PhD thesis proposal on investigating an intelligent decision support system for selecting Cloud-based infrastructure services (e.g. storage, network, CPU). The outcomes of this will be decision support tools and techniques, which will automate and map users' specified application requirements to Cloud service configurations. © 2012 IEEE.","Cloud computing; operation research; semantic technology; service computing","Application requirements; Cloud infrastructures; Cloud service configurations; Cloud service selections; Cloud-based; Decision support techniques; Decision support tools; Infrastructure services; Intelligent decision support systems; Operation research; PhD thesis; Semantic technologies; Service computing; Service offering; Artificial intelligence; Cloud computing; Distributed database systems; Semantics; Decision support systems",Conference Paper,Scopus,2-s2.0-84874234525
"Liu Z., Wang X.","A PSO-based algorithm for load balancing in virtual machines of cloud computing environment",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",17,10.1007/978-3-642-30976-2_17,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875113247&doi=10.1007%2f978-3-642-30976-2_17&partnerID=40&md5=0ab5e20b31523189be799e53aae5dd25","It is possible for IT service providers to provide computing resources in an pay-per-use way in Cloud Computing environments. At the same time, terminal users can also get satisfying services conveniently. But if we take only execution time into consideration when scheduling the cloud resources, it may occur serious load imbalance problem between Virtual Machines (VMs) in Cloud Computing environments. In addition to solve this problem, a new task scheduling model is proposed in this paper. In the model, we optimize the task execution time in view of both the task running time and the system resource utilization. Based on the model, a Particle Swarm Optimization (PSO) - based algorithm is proposed. In our algorithm, we improved the standard PSO, and introduce a simple mutation mechanism and a self-adapting inertia weight method by classifying the fitness values. In the end of this paper, the global search performance and convergence rate of our adaptive algorithm are validated by the results of the comparative experiments. © 2012 Springer-Verlag.","Cloud Computing; Load Balancing; PSO; Task Scheduling; VMs","Cloud computing environments; Comparative experiments; IT service providers; Mutation mechanism; PSO; System resource utilization; Task-scheduling; VMs; Adaptive algorithms; Artificial intelligence; Cloud computing; Computer simulation; Multitasking; Parallel architectures; Particle swarm optimization (PSO); Resource allocation; Scheduling algorithms; Computer systems",Conference Paper,Scopus,2-s2.0-84875113247
"Alqasemi U., Kumavor P., Aguirre A., Zhu Q.","Recognition algorithm for assisting ovarian cancer diagnosis from coregistered ultrasound and photoacoustic images: Ex vivo study",2012,"Journal of Biomedical Optics",17,10.1117/1.JBO.17.12.126003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878023549&doi=10.1117%2f1.JBO.17.12.126003&partnerID=40&md5=bb1022f90b3d9858f0d145b820bf20ac","Unique features and the underlining hypotheses of how these features may relate to the tumor physiology in coregistered ultrasound and photoacoustic images of ex vivo ovarian tissue are introduced. The images were first compressed with wavelet transform. The mean Radon transform of photoacoustic images was then computed and fitted with a Gaussian function to find the centroid of a suspicious area for shift-invariant recognition process. Twenty-four features were extracted from a training set by several methods, including Fourier transform, image statistics, and different composite filters. The features were chosen from more than 400 training images obtained from 33 ex vivo ovaries of 24 patients, and used to train three classifiers, including generalized linear model, neural network, and support vector machine (SVM). The SVM achieved the best training performance and was able to exclusively separate cancerous from non-cancerous cases with 100% sensitivity and specificity. At the end, the classifiers were used to test 95 new images obtained from 37 ovaries of 20 additional patients. The SVM classifier achieved 76.92% sensitivity and 95.12% specificity. Furthermore, if we assume that recognizing one image as a cancer is sufficient to consider an ovary as malignant, the SVM classifier achieves 100% sensitivity and 87.88% specificity. © 2012 Society of Photo-Optical Instrumentation Engineers (SPIE).","Biomedical image recognition; Coregistered ultrasound and photoacoustic imaging; Linear and nonlinear composite filters; Neural network; Ovarian cancer detection; Support vector machine","Generalized linear model; Nonlinear composite; Ovarian cancer diagnosis; Ovarian cancers; Photo-acoustic imaging; Photoacoustic image; Recognition algorithm; Sensitivity and specificity; Diagnosis; Diseases; Image classification; Image recognition; Neural networks; Ultrasonic applications; Ultrasonics; Support vector machines; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; elastography; equipment; female; human; image enhancement; methodology; ovary tumor; photoacoustics; reproducibility; sensitivity and specificity; Algorithms; Artificial Intelligence; Elasticity Imaging Techniques; Female; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Ovarian Neoplasms; Pattern Recognition, Automated; Photoacoustic Techniques; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84878023549
"Sengupta B., Jain A., Bhattacharya K., Truong H.-L., Dustdar S.","Who do you call? Problem resolution through social compute units",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",17,10.1007/978-3-642-34321-6-4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868365397&doi=10.1007%2f978-3-642-34321-6-4&partnerID=40&md5=25c5f61c6db9a3cc441ae2b8c6091afc","Service process orchestration using workflow technologies have led to significant improvements in generating predicable outcomes by automating tedious manual tasks but suffer from challenges related to the flexibility required in work especially when humans are involved. Recently emerging trends in enterprises to explore social computing concepts have realized value in more agile work process orchestrations but tend to be less predictable with respect to outcomes. In this paper we use IT services management, specifically, incident management for large scale systems, to investigate the interplay of workflow systems and social computing. We apply a recently introduced concept of Social Compute Units, and flexible teams sourced based on various parameters such as skills, availability, incident urgency, etc. in the context of resolution of incidents in an IT service provider organization. Results from simulationbased experiments indicate that the combination of SCUs and workflow based processes can lead to significant improvement in key service delivery outcomes, with average resolution time per incident and number of SLO violations being at times as low as 52.7% and 27.3% respectively of the corresponding values for pure workflow based incident management. © Springer-Verlag Berlin Heidelberg 2012.",,"Emerging trends; Incident Management; IT service providers; IT services; Problem resolution; Service delivery; Service process; Social computing; Work process; Work-flow systems; Workflow technology; Artificial intelligence; Information technology",Conference Paper,Scopus,2-s2.0-84868365397
"Stern R., Kalech M., Feldman A., Provan G.","Exploring the duality in conflict-directed model-based diagnosis",2012,"Proceedings of the National Conference on Artificial Intelligence",17,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868277535&partnerID=40&md5=7f1604d328cc1740be96e807b167e784","A model-based diagnosis problem occurs when an observation is inconsistent with the assumption that the diagnosed system is not faulty. The task of a diagnosis engine is to compute diagnoses, which are assumptions on the health of components in the diagnosed system that explain the observation. In this paper, we extend Reiter's well-known theory of diagnosis by exploiting the duality of the relation between conflicts and diagnoses. This duality means that a diagnosis is a hitting set of conflicts, but a conflict is also a hitting set of diagnoses. We use this property to interleave the search for diagnoses and conflicts: a set of conflicts can guide the search for diagnosis, and the computed diagnoses can guide the search for more conflicts. We provide the formal basis for this dual conflict-diagnosis relation, and propose a novel diagnosis algorithm that exploits this duality. Experimental results show that the new algorithm is able to find a minimal cardinality diagnosis faster than the well-known ConflictDirected A*. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Cardinalities; Diagnosis algorithms; Hitting sets; Model based diagnosis; Algorithms; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868277535
"Bei X., Chen N., Hua X., Tao B., Yang E.","Optimal Proportional Cake Cutting with Connected Pieces",2012,"Proceedings of the National Conference on Artificial Intelligence",17,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868290833&partnerID=40&md5=03de7cabb937fef5934e40216d61c6a4","We consider the classic cake cutting problem where one allocates a divisible cake to n participating agents. Among all valid divisions, fairness and efficiency (a.k.a. social welfare) are the most critical criteria to satisfy and optimize, respectively. We study computational complexity of computing an efficiency optimal division given the conditions that the allocation satisfies proportional fairness and assigns each agent a connected piece. For linear valuation functions, we give a polynomial time approximation scheme to compute an efficiency optimal allocation. On the other hand, we show that the problem is NP-hard to approximate within a factor of Ω (1/√n) for general piecewise constant functions, and is NP-hard to compute for normalized functions. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Cutting problems; Normalized functions; NP-hard; Optimal allocation; Optimal division; Piece-wise-constant functions; Polynomial time approximation schemes; Proportional fairness; Social welfare; Valuation function; Artificial intelligence; Computational complexity; Efficiency; Polynomial approximation; Optimization",Conference Paper,Scopus,2-s2.0-84868290833
"Kolobov A., Mausam, Weld D.S.","LRTDP versus UCT for online probabilistic planning",2012,"Proceedings of the National Conference on Artificial Intelligence",17,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868298260&partnerID=40&md5=19829f92fe7dd9ce4dea11a735443ecb","UCT, the premier method for solving games such as Go, is also becoming the dominant algorithm for probabilistic planning. Out of the five solvers at the International Probabilistic Planning Competition (IPPC) 2011, four were based on the UCT algorithm. However, while a UCT-based planner, PROST, won the contest, an LRTDP-based system, GLUTTON, came in a close second, outperforming other systems derived from UCT. These results raise a question: what are the strengths and weaknesses of LRTDP and UCT in practice? This paper starts answering this question by contrasting the two approaches in the context of finite-horizon MDPs. We demonstrate that in such scenarios, UCT's lack of a sound termination condition is a serious practical disadvantage. In order to handle an MDP with a large finite horizon under a time constraint, UCT forces an expert to guess a non-myopic lookahead value for which it should be able to converge on the encountered states. Mistakes in setting this parameter can greatly hurt UCT's performance. In contrast, LRTDP's convergence criterion allows for an iterative deepening strategy. Using this strategy, LRTDP automatically finds the largest lookahead value feasible under the given time constraint. As a result, LRTDP has better performance and stronger theoretical properties. We present an online version of GLUTTON, named GOURMAND, that illustrates this analysis and outperforms PROST on the set of IPPC-2011 problems.",,"Convergence criterion; Finite horizons; Iterative deepening; Look-ahead; Online versions; Probabilistic planning; Termination condition; Time constraints; Algorithms; Iterative methods; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868298260
"Li A., Jin X., Long M.","Topic correlation analysis for cross-domain text classification",2012,"Proceedings of the National Conference on Artificial Intelligence",17,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868283349&partnerID=40&md5=ad518907b053ca223510847c30756de1","Cross-domain text classification aims to automatically train a precise text classifier for a target domain by using labeled text data from a related source domain. To this end, the distribution gap between different domains has to be reduced. In previous works, a certain number of shared latent features (e.g., latent topics, principal components, etc.) are extracted to represent documents from different domains, and thus reduce the distribution gap. However, only relying the shared latent features as the domain bridge may limit the amount of knowledge transferred. This limitation is more serious when the distribution gap is so large that only a small number of latent features can be shared between domains. In this paper, we propose a novel approach named Topic Correlation Analysis (TCA), which extracts both the shared and the domain-specific latent features to facilitate effective knowledge transfer. In TCA, all word features are first grouped into the shared and the domain-specific topics using a joint mixture model. Then the correlations between the two kinds of topics are inferred and used to induce a mapping between the domain-specific topics from different domains. Finally, both the shared and the mapped domain-specific topics are utilized to span a new shared feature space where the supervised knowledge can be effectively transferred. The experimental results on two real-world data sets justify the superiority of the proposed method over the stat-of-the-art baselines. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Correlation analysis; Cross-domain; Different domains; Domain specific; Feature space; Knowledge transfer; Mixture model; Principal Components; Real world data; Target domain; Text classification; Text classifiers; Text data; Classification (of information); Correlation methods; Knowledge management; Virtual reality; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868283349
"Güdemann M., Salaün G., Ouederni M.","Counterexample guided synthesis of monitors for realizability enforcement",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",17,10.1007/978-3-642-33386-6_20,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868239615&doi=10.1007%2f978-3-642-33386-6_20&partnerID=40&md5=ff6a19cdeaa9ed286ba2bd294c397840","Many of today's software systems are built using distributed services, which evolve in different organizations. In order to facilitate their integration, it is necessary to provide a contract that the services participating in a composition should adhere to. A contract specifies interactions among a set of services from a global point of view. One important problem in a top-down development process is figuring out whether such a contract can be implemented by a set of services, obtained by projection and communicating via message passing. It was only recently shown, that this problem, known as realizability, is decidable if asynchronous communication (communication via FIFO buffers) is considered. It can be verified using the synchronizability property. If the system is not synchronizable, the system is not realizable either. In this paper, we propose a new, automatic approach, which enforces both synchronizability and realizability by generating local monitors through successive equivalence checks and refinement. © 2012 Springer-Verlag.",,"Asynchronous communication; Development process; Distributed service; FIFO buffer; Realizability; Software systems; Synchronizability; Topdown; Artificial intelligence; Communication",Conference Paper,Scopus,2-s2.0-84868239615
"Yager R.R.","On a view of Zadeh's Z-numbers",2012,"Communications in Computer and Information Science",17,10.1007/978-3-642-31718-7_10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868137463&doi=10.1007%2f978-3-642-31718-7_10&partnerID=40&md5=b4fda536340d41fcc476d95fb75875a5","We first recall the concept of Z-numbers introduced by Zadeh. These objects consist of an ordered pair (A, B) of fuzzy numbers. We then use these Z-numbers to provide information about an uncertain variable V in the form of a Z-valuation, which expresses the knowledge that the probability that V is A is equal to B. We show that these Z-valuations essentially induce a possibility distribution over probability distributions associated with V. We provide a simple illustration of a Z-valuation. We show how we can use this representation to make decisions and answer questions. We show how to manipulate and combine multiple Z-valuations. We show the relationship between Z-numbers and linguistic summaries. Finally we provide for a representation of Z-valuations in terms of Dempster-Shafer belief structures, which makes use of type-2 fuzzy sets. © 2012 Springer-Verlag Berlin Heidelberg.",,"Dempster-Shafer belief structure; Fuzzy numbers; Linguistic summaries; Possibility distributions; Type-2 fuzzy set; Uncertain variables; Artificial intelligence; Data processing; Fuzzy sets; Knowledge based systems; Probability distributions; Information management",Conference Paper,Scopus,2-s2.0-84868137463
"Miranda E., Zaffalon M., De Cooman G.","Conglomerable natural extension",2012,"International Journal of Approximate Reasoning",17,10.1016/j.ijar.2012.06.015,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866024310&doi=10.1016%2fj.ijar.2012.06.015&partnerID=40&md5=3292a88508d651b342e1e7bdcf9554a1","At the foundations of probability theory lies a question that has been open since de Finetti framed it in 1930: whether or not an uncertainty model should be required to be conglomerable. Conglomerability is related to accepting infinitely many conditional bets. Walley is one of the authors who have argued in favor of conglomerability, while de Finetti rejected the idea. In this paper we study the extension of the conglomerability condition to two types of uncertainty models that are more general than the ones envisaged by de Finetti: sets of desirable gambles and coherent lower previsions. We focus in particular on the weakest (i.e., the least-committal) of those extensions, which we call the conglomerable natural extension. The weakest extension that does not take conglomerability into account is simply called the natural extension. We show that taking the natural extension of assessments after imposing conglomerability - the procedure adopted in Walley's theory - does not yield, in general, the conglomerable natural extension (but it does so in the case of the marginal extension). Iterating this process of imposing conglomerability and taking the natural extension produces a sequence of models that approach the conglomerable natural extension, although it is not known, at this point, whether this sequence converges to it. We give sufficient conditions for this to happen in some special cases, and study the differences between working with coherent sets of desirable gambles and coherent lower previsions. Our results indicate that it is necessary to rethink the foundations of Walley's theory of coherent lower previsions for infinite partitions of conditioning events. © 2012 Elsevier Inc. All rights reserved.","Coherence; Conglomerability; Lower previsions; Marginal extension; Sets of desirable gambles","Coherent lower previsions; Conglomerability; Lower prevision; Marginal extension; Natural extension; Probability theory; Sets of desirable gambles; Sufficient conditions; Uncertainty models; Artificial intelligence; Coherent light; Software engineering",Conference Paper,Scopus,2-s2.0-84866024310
"Yi Q., Zhan-Ming L., Er-Chao L.","Fault detection and diagnosis for non-Gaussian stochastic distribution systems with time delays via RBF neural networks",2012,"ISA Transactions",17,10.1016/j.isatra.2012.07.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866400393&doi=10.1016%2fj.isatra.2012.07.003&partnerID=40&md5=2d5a2a440a6d8e3c5efd4fcae6dcb4ed","A new fault detection and diagnosis (FDD) problem via the output probability density functions (PDFs) for non-gausian stochastic distribution systems (SDSs) is investigated. The PDFs can be approximated by radial basis functions (RBFs) neural networks. Different from conventional FDD problems, the measured information for FDD is the output stochastic distributions and the stochastic variables involved are not confined to Gaussian ones. A (RBFs) neural network technique is proposed so that the output PDFs can be formulated in terms of the dynamic weighings of the RBFs neural network. In this work, a nonlinear adaptive observer-based fault detection and diagnosis algorithm is presented by introducing the tuning parameter so that the residual is as sensitive as possible to the fault. Stability and Convergency analysis is performed in fault detection and fault diagnosis analysis for the error dynamic system. At last, an illustrated example is given to demonstrate the efficiency of the proposed algorithm, and satisfactory results have been obtained. © 2012 ISA.","Non-Gaussian stochastic distribution system; Observer-based fault detection and diagnosis; Probability density functions; Radial basis functions(RBFs) neural network","Convergency; Fault detection and diagnosis; Gaussians; Neural network techniques; Non-Gaussian; Radial basis functions; RBF Neural Network; Stochastic distribution; Stochastic distribution systems; Stochastic variable; Tuning parameter; Algorithms; Electric fault currents; Gaussian noise (electronic); Local area networks; Neural networks; Probability density function; Radial basis function networks; Stochastic systems; Fault detection; algorithm; article; artificial intelligence; artificial neural network; automated pattern recognition; computer simulation; methodology; normal distribution; statistical model; statistics; time; Algorithms; Artificial Intelligence; Computer Simulation; Models, Statistical; Neural Networks (Computer); Normal Distribution; Pattern Recognition, Automated; Stochastic Processes; Time Factors",Article,Scopus,2-s2.0-84866400393
"Alzate C., Suykens J.A.K.","Hierarchical kernel spectral clustering",2012,"Neural Networks",17,10.1016/j.neunet.2012.06.007,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865534073&doi=10.1016%2fj.neunet.2012.06.007&partnerID=40&md5=6f67dc1612da7ad0fa06178b1e3d2820","Kernel spectral clustering fits in a constrained optimization framework where the primal problem is expressed in terms of high-dimensional feature maps and the dual problem is expressed in terms of kernel evaluations. An eigenvalue problem is solved at the training stage and projections onto the eigenvectors constitute the clustering model. The formulation allows out-of-sample extensions which are useful for model selection in a learning setting. In this work, we propose a methodology to reveal the hierarchical structure present on the data. During the model selection stage, several clustering model parameters leading to good clusterings can be found. These results are then combined to display the underlying cluster hierarchies where the optimal depth of the tree is automatically determined. Simulations with toy data and real-life problems show the benefits of the proposed approach. © 2012 Elsevier Ltd.","Hierarchical clustering; Kernel methods; Out-of-sample extensions; Spectral clustering","Clustering model; Clusterings; Dual problem; Eigenvalue problem; Feature map; Hier-archical clustering; Hierarchical structures; High-dimensional; Kernel methods; Learning settings; Model Selection; Optimal depth; Out-of-sample extension; Primal problem; Real-life problems; Spectral clustering; Toy data; Constrained optimization; Eigenvalues and eigenfunctions; algorithm; analytical parameters; article; cluster analysis; information processing; kernel method; learning; linear system; priority journal; statistical analysis; Algorithms; Artificial Intelligence; Cluster Analysis; Computer Simulation; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84865534073
"Flisberg P., Frisk M., Rönnqvist M.","FuelOpt: A decision support system for forest fuel logistics",2012,"Journal of the Operational Research Society",17,10.1057/jors.2011.157,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867183136&doi=10.1057%2fjors.2011.157&partnerID=40&md5=c0a80fd1113017c800fd18b952ac7f22","The use of forest fuel is increasing at heating plants in Sweden. Heating plants provide energy in the form of hot water for heating houses and apartments in local municipalities. Forest fuel are products obtained from harvesting in forests that cannot be used for further processing at sawmills and pulp and paper mills. Examples of such products are tree branches, tree tops and low quality logs. The optimization of the supply chain for round-wood (logs to sawmills, pulp and paper mills) and for forest fuel is similar but involves two main differences. First, forest fuel has to be converted into chips before delivery to the customer, and second, the demand for forest fuel varies over the year due to the temperature. To balance the chipping and transportation capacities over time, it is important to manage inventory levels at terminals. The optimization model developed provides decision support for questions regarding the choice of technology for chipping, where to perform the chipping operations, and the allocation of different assortments to heating plants. The system has been tested on a large case study from a Swedish forest energy company. The results show large savings and that the system is very useful for both planning and business development. © 2012 Operational Research Society Ltd. All rights reserved.","Energy; Inventory; Linear programming; Practice of OR; Transport","Business development; Decision supports; Energy; Energy companies; Forest fuel; Heating plants; Hot water; Inventory; Inventory levels; Low quality log; Optimization models; Practice of OR; Pulp and paper mill; Transport; Transportation capacity; Tree branches; Tree tops; Artificial intelligence; Decision support systems; Fuels; Linear programming; Pulp manufacture; Sawing; Sawmills; Space heating; Supply chains; Wood products; Forestry; Artificial Intelligence; Energy; Forestry; Fuels; Heating; Inventories; Saw Mills; Sawing; Supply Chain Management; Transport; Wood Products",Article,Scopus,2-s2.0-84867183136
"Bender J., Dagdelen Ö., Fischlin M., Kügler D.","Domain-specific pseudonymous signatures for the german identity card",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",17,10.1007/978-3-642-33383-5_7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866974486&doi=10.1007%2f978-3-642-33383-5_7&partnerID=40&md5=553b0d15dcaa85ca1f4ee53db33a4f41","The restricted identification protocol for the new German identity card basically provides a method to use pseudonyms such that they can be linked by individual service providers, but not across different service providers (even not malicious ones). The protocol can be augmented to allow also for signatures under the pseudonyms. In this paper, we thus view -and define- this idea more abstractly as a new cryptographic signature primitive with some form of anonymity, and use the term domain-specific pseudonymous signatures. We then analyze the restricted identification solutions in terms of the formal security requirements. © 2012 Springer-Verlag.",,"Different services; Domain specific; Formal security; Identification protocol; Identity cards; Individual service; Artificial intelligence; Network security",Conference Paper,Scopus,2-s2.0-84866974486
"Dovžan D., Logar V., Škrjanc I.","Solving the sales prediction problem with fuzzy evolving methods",2012,"2012 IEEE Congress on Evolutionary Computation, CEC 2012",17,10.1109/CEC.2012.6252856,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866847474&doi=10.1109%2fCEC.2012.6252856&partnerID=40&md5=b4cab39688bdf20bf92a5122565b63a5","This paper presents the solving of the petrol sales volume estimation problem given by the Task Force on Competitions, Fuzzy Systems Technical Committee IEEE Computational Intelligence Society. The solution using eTS, SAFIS and eFuMo method is presented. The results are compared to linear ARX model identified using RLS method. Results using static and prediction model are given. The paper also presents parts of new eFuMo method, which is based on recursive Gustafson-Kessel clustering. © 2012 IEEE.","eFuMo; evolving fuzzy model; recursive Gustafson-Kessel clustering","eFuMo; Fuzzy models; Gustafson-Kessel; Linear ARX model; Prediction model; Prediction problem; Sales volume; Task force; Technical committees; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84866847474
"Knight S., Palamidessi C., Panangaden P., Valencia F.D.","Spatial and epistemic modalities in constraint-based process calculi",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",17,10.1007/978-3-642-32940-1_23,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866655607&doi=10.1007%2f978-3-642-32940-1_23&partnerID=40&md5=3de2607198639cbc5ecb3744fcc2d58a","We introduce spatial and epistemic process calculi for reasoning about spatial information and knowledge distributed among the agents of a system. We introduce domain-theoretical structures to represent spatial and epistemic information. We provide operational and denotational techniques for reasoning about the potentially infinite behaviour of spatial and epistemic processes. We also give compact representations of infinite objects that can be used by processes to simulate announcements of common knowledge and global information. © 2012 Springer-Verlag.",,"Common knowledge; Compact representation; Constraint-based; Global informations; Process calculi; Spatial informations; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84866655607
"Mu T., Goulermas J.Y., Tsujii J., Ananiadou S.","Proximity-based frameworks for generating embeddings from multi-output data",2012,"IEEE Transactions on Pattern Analysis and Machine Intelligence",17,10.1109/TPAMI.2012.20,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860461833&doi=10.1109%2fTPAMI.2012.20&partnerID=40&md5=e7ee0bedd4cde92e22456b23402d4b00","This paper is about supervised and semi-supervised dimensionality reduction (DR) by generating spectral embeddings from multi-output data based on the pairwise proximity information. Two flexible and generic frameworks are proposed to achieve supervised DR (SDR) for multilabel classification. One is able to extend any existing single-label SDR to multilabel via sample duplication, referred to as MESD. The other is a multilabel design framework that tackles the SDR problem by computing weight (proximity) matrices based on simultaneous feature and label information, referred to as MOPE, as a generalization of many current techniques. A diverse set of different schemes for label-based proximity calculation, as well as a mechanism for combining label-based and feature-based weight information by considering information importance and prioritization, are proposed for MOPE. Additionally, we summarize many current spectral methods for unsupervised DR (UDR), single/multilabel SDR, and semi-supervised DR (SSDR) and express them under a common template representation as a general guide to researchers in the field. We also propose a general framework for achieving SSDR by combining existing SDR and UDR models, and also a procedure of reducing the computational cost via learning with a target set of relation features. The effectiveness of our proposed methodologies is demonstrated with experiments with document collections for multilabel text categorization from the natural language processing domain. © 2012 IEEE.","Dimensionality reduction; embeddings; multilabel classification; semi-supervised; supervised","Dimensionality reduction; Embeddings; Multi-label; Semi-supervised; supervised; Text processing; Natural language processing systems; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; image enhancement; image subtraction; information processing; methodology; natural language processing; reproducibility; sensitivity and specificity; Algorithms; Artificial Intelligence; Automatic Data Processing; Image Enhancement; Image Interpretation, Computer-Assisted; Natural Language Processing; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-84860461833
"Jean J., Naya-Plasencia M., Peyrin T.","Improved rebound attack on the finalist Grøstl",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",17,10.1007/978-3-642-34047-5_7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866699557&doi=10.1007%2f978-3-642-34047-5_7&partnerID=40&md5=255fb65e335824c27891b776a7abaca4","Grøstl is one of the five finalist hash functions of the SHA-3 competition. For entering this final phase, the designers have tweaked the submitted versions. This tweak renders inapplicable the best known distinguishers on the compression function presented by Peyrin [18] that exploited the internal permutation properties. Since the beginning of the final round, very few analysis have been published on Grøstl. Currently, the best known rebound-based results on the permutation and the compression function for the 256-bit version work up to 8 rounds, and up to 7 rounds for the 512-bit version. In this paper, we present new rebound distinguishers that work on a higher number of rounds for the permutations of both 256 and 512-bit versions of this finalist, that is 9 and 10 respectively. Our distinguishers make use of an algorithm that we propose for solving three fully active states in the middle of the differential characteristic, while the Super-Sbox technique only handles two. © 2012 Springer-Verlag.","Cryptanalysis; Grøstl; Hash Function; Rebound Attack; SHA-3","Active state; Compression functions; Cryptanalysis; Differential characteristic; Distinguishers; Rebound Attack; SHA-3; Sha-3 competitions; Artificial intelligence; Hash functions",Conference Paper,Scopus,2-s2.0-84866699557
"Resurreccion J., Santos J.R.","Multiobjective Prioritization Methodology and Decision Support System for Evaluating Inventory Enhancement Strategies for Disrupted Interdependent Sectors",2012,"Risk Analysis",17,10.1111/j.1539-6924.2011.01779.x,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867002286&doi=10.1111%2fj.1539-6924.2011.01779.x&partnerID=40&md5=2a804ca966cd63b2aa220ccc1d260ba9","Disruptions in the production of commodities and services resulting from disasters influence the vital functions of infrastructure and economic sectors within a region. The interdependencies inherent among these sectors trigger the faster propagation of disaster consequences that are often associated with a wider range of inoperability and amplified losses. This article evaluates the impact of inventory-enhanced policies for disrupted interdependent sectors to improve the disaster preparedness capability of dynamic inoperability input-output models (DIIM). In this article, we develop the dynamic cross-prioritization plot (DCPP)-a prioritization methodology capable of identifying and dynamically updating the critical sectors based on preference assignments to different objectives. The DCPP integrates the risk assessment metrics (e.g., economic loss and inoperability), which are independently analyzed in the DIIM. We develop a computer-based DCPP tool to determine the priority for inventory enhancement with user preference and resource availability as new dimensions. A baseline inventory case for the state of Virginia revealed a high concentration of (i) manufacturing sectors under the inoperability objective and (ii) service sectors under the economic loss objective. Simulation of enhanced inventory policies for selected critical manufacturing sectors has reduced the recovery period by approximately four days and the expected total economic loss by $33 million. Although the article focuses on enhancing inventory levels in manufacturing sectors, complementary analysis is recommended to manage the resilience of the service sectors. The flexibility of the proposed DCPP as a decision support tool can also be extended to accommodate analysis in other regions and disaster scenarios. © 2012 Society for Risk Analysis.","Hurricane preparedness; Input-output analysis; Inventory management","Assessment metrics; Complementary analysis; Decision support tools; Disaster preparedness; Disaster scenario; Economic loss; Economic sectors; High concentration; Hurricane preparedness; Inoperability input-output model; Input output analysis; Inventory levels; Inventory management; Inventory policies; Manufacturing sector; Multi objective; New dimensions; Prioritization; Resource availability; Service sectors; Virginia; Artificial intelligence; Decision support systems; Disaster prevention; Inventory control; Losses; Manufacture; Industrial applications; baseline conditions; decision support system; hurricane; input-output analysis; prioritization; resource availability; risk assessment; service sector; United States; Virginia",Article,Scopus,2-s2.0-84867002286
"Bustillo A., Correa M.","Using artificial intelligence to predict surface roughness in deep drilling of steel components",2012,"Journal of Intelligent Manufacturing",17,10.1007/s10845-011-0506-8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870927834&doi=10.1007%2fs10845-011-0506-8&partnerID=40&md5=f04fa51a70d1882b2ded200577e19ee2","A predictivemodel is presented to optimize deep drilling operations under high speed conditions for the manufacture of steel components such as moulds and dies. The input data include cutting parameters and axial cutting forces measured by sensors on the milling centres where the tests are performed. The novelty of the paper lies in the use of Bayesian Networks that consider the cooling system as an input variable for the optimization of roughness quality in deep drilling operations. Two different coolant strategies are tested: traditional working fluid and MQL (Minimum Quantity Lubrication). The model is based on a machine learning classification method known as Bayesian networks. Various measures used to assess the model demonstrate its suitability to control this type of industrial task. Its ease of interpretation is a further advantage in comparison with other artificial intelligence tools, which makes it a user-friendly application for machine operators. © Springer Science+Business Media, LLC 2010.","Bayesian networks; Deep drilling; Minimum quantity lubrication (MQL); Supervised classification; Surface roughness","Artificial intelligence tools; Cutting forces; Cutting parameters; Deep drilling; High-speed conditions; Input datas; Input variables; Machine learning classification; Machine operators; Minimum quantity lubrication; Moulds and dies; Steel components; Supervised classification; Working fluid; Artificial intelligence; Bayesian networks; Optimization; Surface roughness",Article,Scopus,2-s2.0-84870927834
"Zhang L., Jiang L., Shi Y., Luo H., Kang R., Yu Z.","Post-harvest 1-methylcyclopropene and ethephon treatments differently modify protein profiles of peach fruit during ripening",2012,"Food Research International",17,10.1016/j.foodres.2012.05.022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862747477&doi=10.1016%2fj.foodres.2012.05.022&partnerID=40&md5=a4c91f508d71e4bd1c74360d97b9aec4","A proteomic approach based on two-dimensional polyacrylamide gel electrophoresis (2-DE) and MALDI-TOF/TOF techniques has been used in this work to study the effect of 1-methylcyclopropene (1-MCP) and 2-chloroethylphosphonic acid (ethephon) on the protein profiles of melting peach fruit (cv. Huiyulu) during ripening. The extracted proteins from the 1-MCP-treated and ethephon-treated peach fruit were used. More than 600 protein spots were detected by means of 2-DE and 38 differently expressed spots (P < 0.05) were selected to be excised and analyzed using MALDI-TOF/TOF, and 35 were finally confidently identified according to the peach EST database downloaded from Genome Database for Rosaceae and NCBI database. Among the 35 successfully identified protein regulated by 1-MCP and ethephon, 26 proteins were modulated by 1-MCP and 27 proteins were regulated by ethephon. Meanwhile, 8 proteins were down-regulated by 1-MCP and up-regulated by ethephon, and 5 proteins down-regulated by ethephon but up-regulated by 1-MCP. Differently expressed proteins belonged to different metabolic pathways including energy and metabolism (34.29%), cell structure (22.83%), protein fate (17.14%), stress response and defense (14.29%) and ripening and senescence (8.57%). All these indicated that the regulation patterns by 1-MCP and ethephon at proteomic level are complicated, involving various metabolic pathways. Ethephon could induce the expressions of 1-aminocyclopropane-1-carboxylic acid oxidase (ACO), abscisic acid stress ripening-like protein (ASR) and cell structure-related proteins, such as tubulin and actin, accelerate the glycolytic metabolisms, regulate the galactose and glutamine metabolism, and modulate the methionine metabolism. 1-MCP could inhibit the degradation of starch, weaken the metabolism of glycolytic pathway, strongly depresses the expression of ACO and ASR, and induce the expression of chaperonin 60 and HSPs in ripening peach fruit. Taken together, the present study will be informative for exploring the exact roles of ethylene in peach fruit ripening and explaining the molecular mechanism of peach ripening. © 2012 Elsevier Ltd.","1-methylcyclopropene; Ethephon; Peach fruit; Proteomics; Ripening","1-Aminocyclopropane-1-carboxylic acid oxidase; 1-methylcyclopropene; 1-methylcyclopropene (1-MCP); Abscisic acid; Cell structure; Chaperonin; EST database; Ethephon; Fruit ripening; Genome database for rosaceae; MALDI-TOF/TOF; Metabolic pathways; Molecular mechanism; Postharvest; Protein profiles; Protein spots; Proteomic approaches; Proteomics; Regulation pattern; Ripening; Stress response; Structure-related; Two-dimensional polyacrylamide gel electrophoresis; Amino acids; Artificial intelligence; Carboxylic acids; Database systems; Degradation; Electrophoresis; Ethylene; Fruits; Metabolism; Molecular biology; Physiology; Plants (botany); Proteins; Prunus persica; Rosaceae",Article,Scopus,2-s2.0-84862747477
"Bench-Capon T., Araszkiewicz M., Ashley K., Atkinson K., Bex F., Borges F., Bourcier D., Bourgine P., Conrad J.G., Francesconi E., Gordon T.F., Governator G., Leidner J.L., Lewis D.D., Loui R.P., McCarty L.T., Prakken H., Schilder F., Schweighofer E., Thompson P., Tyrrell A., Verheij B., Walton D.N.","A history of ai and law in 50 papers: 25 Years of the international conference on ai and law",2012,"Artificial Intelligence and Law",17,10.1007/s10506-012-9131-x,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868356255&doi=10.1007%2fs10506-012-9131-x&partnerID=40&md5=c26590f5fafed4a3ade12f0f33098771","We provide a retrospective of 25 years of the International Conference on AI and Law, which was first held in 1987. Fifty papers have been selected from the thirteen conferences and each of them is described in a short subsection individually written by one of the 24 authors. These subsections attempt to place the paper discussed in the context of the development of AI and Law, while often offering some personal reactions and reflections. As a whole, the subsections build into a history of the last quarter century of the field, and provide some insights into where it has come from, where it is now, and where it might go. © 2012 Springer Science+Business Media B.V.","Artificial intelligence and law; Legal informatics; Models of legalReasoning","AI and law; Informatics; Management; Artificial intelligence",Article,Scopus,2-s2.0-84868356255
"Lai F., Carsten O., Tate F.","How much benefit does Intelligent Speed Adaptation deliver? - An analysis of its potential contribution to safety and environment",2012,"Accident Analysis and Prevention",17,10.1016/j.aap.2011.04.011,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861881609&doi=10.1016%2fj.aap.2011.04.011&partnerID=40&md5=c2315987a273931418968f98574cf777","The UK Intelligent Speed Adaptation (ISA) project produced a rich database with high-resolution data on driver behaviour covering a comprehensive range of road environment. The field trials provided vital information on driver behaviour in the presence of ISA. The purpose of this paper is to exploit the information gathered in the field trials to predict the impacts of various forms of ISA and to assess whether ISA is viable in terms of benefit-to-cost ratio. ISA is predicted to save up to 33% of accidents on urban roads, and to reduce CO 2 emissions by up to 5.8% on 70 mph roads. In order to investigate the long-term impacts of ISA, two hypothetical deployment scenarios were envisaged covering a 60-year appraisal period. The results indicate that ISA could deliver a very healthy benefit-to-cost ratio, ranging from 3.4 to 7.4, depending on the deployment scenarios. Under both deployment scenarios, ISA has recovered its implementation costs in less than 15 years. It can be concluded that implementation of ISA is clearly justified from a social cost and benefit perspective. Of the two deployment scenarios, the Market Driven one is substantially outperformed by the Authority Driven one. The benefits of ISA on fuel saving and emission reduction are real but not substantial, in comparison with the benefits on accident reduction; up to 98% of benefits are attributable to accident savings. Indeed, ISA is predicted to lead to a savings of 30% in fatal crashes and 25% in serious crashes over the 60-year period modelled. © 2011 Elsevier Ltd.","Accidents; Carbon dioxide emissions; Cost benefit analysis; Fuel consumption; Intelligent Speed Adaptation","Accident reduction; Accident savings; Benefit to cost ratios; Carbon dioxide emissions; Deployment scenarios; Driver behaviour; Emission reduction; Fatal crashes; Field trial; Fuel savings; High resolution; Implementation cost; Intelligent speed adaptation; Market driven; Road environment; Social cost; Urban road; Carbon dioxide; Cost benefit analysis; Emission control; Fuel consumption; Fuel economy; Global warming; Speed control; Accidents; acceleration; accident prevention; article; artificial intelligence; car; car driving; cost benefit analysis; economics; evaluation; exhaust gas; human; instrumentation; law enforcement; legal aspect; methodology; psychological aspect; safety; statistical model; theoretical model; traffic accident; United Kingdom; Acceleration; Accident Prevention; Accidents, Traffic; Artificial Intelligence; Automobile Driving; Automobiles; Cost-Benefit Analysis; Great Britain; Humans; Law Enforcement; Models, Economic; Models, Theoretical; Safety; Vehicle Emissions",Article,Scopus,2-s2.0-84861881609
"Pesquita C., Couto F.M.","Predicting the Extension of Biomedical Ontologies",2012,"PLoS Computational Biology",17,10.1371/journal.pcbi.1002630,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866932669&doi=10.1371%2fjournal.pcbi.1002630&partnerID=40&md5=89b6dd8880d58874f915f7cad2e57d58","Developing and extending a biomedical ontology is a very demanding task that can never be considered complete given our ever-evolving understanding of the life sciences. Extension in particular can benefit from the automation of some of its steps, thus releasing experts to focus on harder tasks. Here we present a strategy to support the automation of change capturing within ontology extension where the need for new concepts or relations is identified. Our strategy is based on predicting areas of an ontology that will undergo extension in a future version by applying supervised learning over features of previous ontology versions. We used the Gene Ontology as our test bed and obtained encouraging results with average f-measure reaching 0.79 for a subset of biological process terms. Our strategy was also able to outperform state of the art change capturing methods. In addition we have identified several issues concerning prediction of ontology evolution, and have delineated a general framework for ontology extension prediction. Our strategy can be applied to any biomedical ontology with versioning, to help focus either manual or semi-automated extension methods on areas of the ontology that need extension. © 2012 Pesquita, Couto.",,"article; automation; biomedical ontology; biomedicine; information processing; methodology; nomenclature; prediction; Algorithms; Artificial Intelligence; Computational Biology; Database Management Systems; Evolution, Molecular; Humans; Information Storage and Retrieval; Natural Language Processing; Vocabulary, Controlled",Article,Scopus,2-s2.0-84866932669
"Figueroa R., Zeng-Treitler Q., Ngo L.H., Goryachev S., Wiechmann E.","Active learning for clinical text classification: Is it better than random sampling?",2012,"Journal of the American Medical Informatics Association",17,10.1136/amiajnl-2011-000648,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872236807&doi=10.1136%2famiajnl-2011-000648&partnerID=40&md5=5a72f3b1e338b26703311d7da9c0d27f","Objective This study explores active learning algorithms as a way to reduce the requirements for large training sets in medical text classification tasks. Design Three existing active learning algorithms (distance-based (DIST), diversity-based (DIV), and a combination of both (CMB)) were used to classify text from five datasets. The performance of these algorithms was compared to that of passive learning on the five datasets. We then conducted a novel investigation of the interaction between dataset characteristics and the performance results. Measurements Classification accuracy and area under receiver operating characteristics (ROC) curves for each algorithm at different sample sizes were generated. The performance of active learning algorithms was compared with that of passive learning using a weighted mean of paired differences. To determine why the performance varies on different datasets, we measured the diversity and uncertainty of each dataset using relative entropy and correlated the results with the performance differences. Results The DIST and CMB algorithms performed better than passive learning. With a statistical significance level set at 0.05, DIST outperformed passive learning in all five datasets, while CMB was found to be better than passive learning in four datasets. We found strong correlations between the dataset diversity and the DIV performance, as well as the dataset uncertainty and the performance of the DIST algorithm. Conclusion For medical text classification, appropriate active learning algorithms can yield performance comparable to that of passive learning with considerably smaller training sets. In particular, our results suggest that DIV performs better on data with higher diversity and DIST on data with lower uncertainty.",,"accuracy; active learning; area under the curve; article; biomedicine; clinical classification; clinical text classification; controlled study; entropy; learning algorithm; passive learning; probability; randomization; sample size; task performance; algorithm; artificial intelligence; comparative study; data mining; human; methodology; natural language processing; receiver operating characteristic; Algorithms; Artificial Intelligence; Data Mining; Humans; Natural Language Processing; ROC Curve",Article,Scopus,2-s2.0-84872236807
"Webster D.R., Volyanskyy K.Y., Weissburg M.J.","Bioinspired algorithm for autonomous sensor-driven guidance in turbulent chemical plumes",2012,"Bioinspiration and Biomimetics",17,10.1088/1748-3182/7/3/036023,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864189212&doi=10.1088%2f1748-3182%2f7%2f3%2f036023&partnerID=40&md5=1faf6d5eabdb6ab6ef113ec421c66919","We designed and implemented a control algorithm for sensor-mediated chemical plume tracking in a turbulent flow environment. In our design, we focused on development of a signal processing strategy capable of replicating behavioral responses of actively tracking blue crabs (Callinectes sapidus) to chemical stimuli. The control algorithm is evaluated in a hardware platform that allows motion in two directions (i.e. forward-back and left-right). The geometric arrangement of the sensor array is inspired by the location of blue crab sensor populations. Upstream motion is induced by a binary response to supra-threshold spikes of concentration, and cross-stream steering is controlled by contrast between bilaterally-separated sensors. Like animal strategies, the developed control algorithm is dynamic. This property allows the algorithm to function effectively in the highly irregular turbulent environment and produces adaptive adjustments of motion to minimize the distance to the source of a plume. Tracking trials indicate that roughly 80% of the tracks successfully stop near the plume source location. Both success rate and movement patterns of the tracker compare favorably to that of blue crabs searching for odorant plume sources, thus suggesting that our sensory-mediated behavior hypothesis are generally accurate and that the associated tracking mechanisms may be successfully implemented in hardware. © 2012 IOP Publishing Ltd.",,"Animalia; Callinectes sapidus; biomimetic material; algorithm; animal; article; artificial intelligence; automated pattern recognition; biological model; biomimetics; Brachyura; computer aided design; computer simulation; equipment design; equipment failure analysis; feedback system; gas; instrumentation; locomotion; methodology; physiology; robotics; transducer; Algorithms; Animals; Artificial Intelligence; Biomimetic Materials; Biomimetics; Brachyura; Computer Simulation; Computer-Aided Design; Equipment Design; Equipment Failure Analysis; Feedback; Gases; Locomotion; Models, Biological; Pattern Recognition, Automated; Robotics; Transducers",Article,Scopus,2-s2.0-84864189212
"Nagalakshmi S., Kamaraj N.","Comparison of computational intelligence algorithms for loadability enhancement of restructured power system with FACTS devices",2012,"Swarm and Evolutionary Computation",17,10.1016/j.swevo.2012.02.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861532091&doi=10.1016%2fj.swevo.2012.02.002&partnerID=40&md5=1e4e4f789257b5dc90fbf329067470b0","This paper proposes the use of computational intelligence algorithms to determine the optimal location and control of Flexible AC Transmission System (FACTS) devices to enhance the loadability of pool and hybrid models in restructured power system. Particle Swarm Optimization (PSO), Differential Evolution (DE) and Composite Differential Evolution (CoDE) algorithms are used and their performances were compared. For this study, Thyristor Controlled Series Compensator (TCSC), Static VAR Compensator (SVC) and Thyristor Controlled Phase Shifting Transformer (TCPST) are considered. This approach uses AC load flow equations with constraints on real and reactive power generations, transmission line flow, magnitude of bus voltages and FACTS device settings. For the hybrid model, bilateral transactions are modeled using secured bilateral transaction matrix utilizing the AC distribution factor with slack bus contribution. Simulations are performed on IEEE 118 bus system. Maximum loadability, computation time and convergence characteristics are compared. The results indicate that by optimal location and control of FACTS devices, DE enhances the loadability of the pool and hybrid models with less computation time and faster convergence than PSO. Further the performance of DE is improved by using its variant, CoDE. Among the three FACTS devices, TCSC gives maximum loadability than SVC and TCPST. To conclude, for enhancing the loadability of restructured power system with FACTS devices using the computational intelligence algorithm, DE with TCSC gives maximum loadability with less computational time and faster convergence. The computational effort is further reduced by using CoDE. © 2012 Elsevier B.V. All rights reserved.","Composite differential evolution; Differential evolution; Particle swarm optimization; Static VAR compensator; Thyristor controlled phase shifting transformer; Thyristor controlled series compensator","AC distribution factors; AC load; Bilateral transaction; Bus voltage; Computation time; Computational effort; Computational time; Convergence characteristics; Differential Evolution; Facts devices; Faster convergence; Flexible AC transmission system; Hybrid model; IEEE 118-bus system; Loadability; Maximum loadability; Optimal locations; Phase shifting transformer; Real and reactive power; Restructured power systems; Slack bus; Static Var compensator; Static VAR compensators; Thyristor controlled series compensator; Algorithms; Artificial intelligence; Computational efficiency; Electric network topology; Electric switchgear; Lakes; Power electronics; Thyristors; Particle swarm optimization (PSO)",Article,Scopus,2-s2.0-84861532091
"Somarowthu S., Ondrechen M.J.","POOL server: Machine learning application for functional site prediction in proteins",2012,"Bioinformatics",17,10.1093/bioinformatics/bts321,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865150047&doi=10.1093%2fbioinformatics%2fbts321&partnerID=40&md5=47f7334efa9f7b17f6e716fd8fbe6877","Summary: We present an automated web server for partial order optimum likelihood (POOL), a machine learning application that combines computed electrostatic and geometric information for high-performance prediction of catalytic residues from 3D structures. Input features consist of THEMATICS electrostatics data and pocket information from ConCavity. THEMATICS measures deviation from typical, sigmoidal titration behavior to identify functionally important residues and ConCavity identifies binding pockets by analyzing the surface geometry of protein structures. Both THEMATICS and ConCavity (structure only) do not require the query protein to have any sequence or structure similarity to other proteins. Hence, POOL is applicable to proteins with novel folds and engineered proteins. As an additional option for cases where sequence homologues are available, users can include evolutionary information from INTREPID for enhanced accuracy in site prediction. © The Author(s) 2012. Published by Oxford University Press.",,"protein; article; artificial intelligence; biology; chemistry; computer program; Internet; methodology; protein tertiary structure; static electricity; Artificial Intelligence; Computational Biology; Internet; Protein Structure, Tertiary; Proteins; Software; Static Electricity",Article,Scopus,2-s2.0-84865150047
"Harman M.","The role of artificial intelligence in software engineering",2012,"2012 1st International Workshop on Realizing AI Synergies in Software Engineering, RAISE 2012 - Proceedings",17,10.1109/RAISE.2012.6227961,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864213804&doi=10.1109%2fRAISE.2012.6227961&partnerID=40&md5=0e48a5acb960fc08418de3092decbbed","There has been a recent surge in interest in the application of Artificial Intelligence (AI) techniques to Software Engineering (SE) problems. The work is typified by recent advances in Search Based Software Engineering, but also by long established work in Probabilistic reasoning and machine learning for Software Engineering. This paper explores some of the relationships between these strands of closely related work, arguing that they have much in common and sets out some future challenges in the area of AI for SE. © 2012 IEEE.",,"Future challenges; Probabilistic reasoning; Search-based software engineering; Artificial intelligence; Software engineering",Conference Paper,Scopus,2-s2.0-84864213804
"Mangiatordi F., Pallotti E., Del Vecchio P., Leccese F.","Power consumption scheduling for residential buildings",2012,"2012 11th International Conference on Environment and Electrical Engineering, EEEIC 2012 - Conference Proceedings",17,10.1109/EEEIC.2012.6221508,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864270295&doi=10.1109%2fEEEIC.2012.6221508&partnerID=40&md5=416f035d30e33ee2f18d91f6a0219152","The increasing growth of electricity usage in buildings points out the significant role of residential users in the programs for the efficient control and management of electrical energy. The shaving of consumption peaks in household is becoming an integral part of the national energy strategies to reduce the risk of blackouts and ensure environmental sustainability of new urban context. This paper investigates the use of the paradigm of swarm intelligence to scheduling the operation of household appliances in order to reduce to smooth the variation and reduce the peak-to-average ratio of total electricity demand at home. Simulation results confirm the proposed approach. © 2012 IEEE.","Energy management; power consumption scheduling; Smart grid","Efficient control; Electrical energy; Electricity demands; Electricity usage; Energy strategy; Environmental sustainability; In-buildings; Integral part; Peak-to-average ratio; Residential building; Residential users; Smart grid; Swarm Intelligence; Urban context; Artificial intelligence; Domestic appliances; Electrical engineering; Energy management; Scheduling",Conference Paper,Scopus,2-s2.0-84864270295
"Dubitzky W., Kötter T., Schmidt O., Berthold M.R.","Towards creative information exploration based on Koestler's concept of bisociation",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",17,10.1007/978-3-642-31830-6_2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864151681&doi=10.1007%2f978-3-642-31830-6_2&partnerID=40&md5=599cb497289ae1ffb11ecd2b3a41dc67","Creative information exploration refers to a novel framework for exploring large volumes of heterogeneous information. In particular, creative information exploration seeks to discover new, surprising and valuable relationships in data that would not be revealed by conventional information retrieval, data mining and data analysis technologies. While our approach is inspired by work in the field of computational creativity, we are particularly interested in a model of creativity proposed by Arthur Koestler in the 1960s. Koestler's model of creativity rests on the concept of bisociation. Bisociative thinking occurs when a problem, idea, event or situation is perceived simultaneously in two or more ""matrices of thought"" or domains. When two matrices of thought interact with each other, the result is either their fusion in a novel intellectual synthesis or their confrontation in a new aesthetic experience. This article discusses some of the foundational issues of computational creativity and bisociation in the context of creative information exploration. © 2012 Springer-Verlag Berlin Heidelberg.",,"Heterogeneous information; Information exploration; Artificial intelligence",Article,Scopus,2-s2.0-84864151681
"Cerreta M., De Toro P.","Assessing urban transformations: A SDSS for the master plan of Castel Capuano, Naples",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",17,10.1007/978-3-642-31075-1_13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863892230&doi=10.1007%2f978-3-642-31075-1_13&partnerID=40&md5=5507fb80b3d26c24da8f570557d7141b","The objective of this study is to present a spatial simulation modelling of real estate effects caused by urban transformations. The proposed approach extends the formalization of the ""Monte Carlo"" simulation methods in Geographical Information Systems (GIS), including spatial structure and temporal dynamics. The combined application can be useful in spatial decision making process for urban planning, supporting and modelling operations for urban land-use change. Analysing the new functions for the redevelopment of Castel Capuano, an historic building in Naples (Italy), the paper explores possible scenarios of transformations identifying the effects on the urban real estate market. © 2012 Springer-Verlag.","Monte Carlo simulation; Naples (Italy); Real estate market; Spatial Decision Support System","Decision making process; Geographical information systems; Historic buildings; Master plan; MONTE CARLO; Monte Carlo Simulation; Naples (Italy); Real estate; Real estate market; Simulation methods; Spatial decision support systems; Spatial structure; Temporal dynamics; Urban land-use change; Artificial intelligence; Computer simulation; Decision support systems; Geographic information systems; Urban planning; Monte Carlo methods",Conference Paper,Scopus,2-s2.0-84863892230
"Calabrese F., Corallo A., Margherita A., Zizzari A.A.","A knowledge-based decision support system for shipboard damage control",2012,"Expert Systems with Applications",17,10.1016/j.eswa.2012.01.146,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858334457&doi=10.1016%2fj.eswa.2012.01.146&partnerID=40&md5=fe3e4e8988617a872ce164b8d531889c","The operational complexity of modern ships requires the use of advanced applications, called damage control systems (DCSs), able to assist crew members in the effective handling of dangerous events and accidents. In this article we describe the development of a knowledge-based decision support system (KDSS) integrated within a DCS designed for a national navy. The KDSS uses a hybrid design and runtime knowledge model to assist damage control operators through a kill card function which supports damage identification, action scheduling and system reconfiguration. We report a fire fighting scenario as illustrative application and discuss a preliminary evaluation of benefits allowed by the system in terms of critical performance measures. Our work can support further research aimed to apply expert systems to improve shipboard security and suggest similar applications in other contexts where situational awareness and damage management are crucial. © 2012 Elsevier Ltd. All rights reserved.","Damage control system; Decision support system; Expert system; Kill card; Knowledge-based system; Shipboard management","Advanced applications; Crew members; Damage control; Damage control systems; Damage Identification; Decision supports; Fire fighting; Hybrid design; Kill card; Knowledge based decision support systems; Knowledge model; Operational complexity; Performance measure; Runtimes; Shipboard damage; Shipboard security; Situational awareness; System reconfiguration; Artificial intelligence; Control systems; Damage detection; Distributed parameter networks; Expert systems; Decision support systems",Article,Scopus,2-s2.0-84858334457
"Park H.-S., Tran N.-H.","An autonomous manufacturing system based on swarm of cognitive agents",2012,"Journal of Manufacturing Systems",17,10.1016/j.jmsy.2012.05.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865228099&doi=10.1016%2fj.jmsy.2012.05.002&partnerID=40&md5=ba5bb60ad5a44190de1c0825d2b31e87","Disruptions of the manufacturing systems caused by the disturbances such as the tool wear, machine breakdown, malfunction of robot or transporter, and so on reduce the productivity as well as increase the cost of product. The conventional manufacturing systems are unable to face with the disturbances by their rigid structure. These systems should be stopped when the disturbances occur. The paper presents an Autonomous Manufacturing System based on Swarm of Cognitive Agents (AMS-SCA) in order to adapt to the disturbances. In the AMS-SCA, the manufacturing system is considered as a swarm of cognitive agents where work-pieces, machines, robots, and transporters are controlled by the corresponding cognitive agents. The system reacts to disturbances autonomously based on the reaction of each agent or the cooperation among them. To develop the AMS-SCA, the disturbances happened in the machining shop were analyzed to find out the corresponding management methods. A test-bed was implemented to prove the functionality of the proposed AMS-SCA. © 2012 The Society of Manufacturing Engineers.","AMS-SCA; Cognitive agent; Self-adaptation; Swarm intelligence","AMS-SCA; Cognitive agents; Conventional manufacturing; Machine breakdown; Management method; Self adaptation; Swarm Intelligence; Tool wear; Work pieces; Artificial intelligence; Process design; Manufacture",Article,Scopus,2-s2.0-84865228099
"Ali W., Shamsuddin S.M., Ismail A.S.","Intelligent Naïve Bayes-based approaches for Web proxy caching",2012,"Knowledge-Based Systems",17,10.1016/j.knosys.2012.02.015,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859911042&doi=10.1016%2fj.knosys.2012.02.015&partnerID=40&md5=f3839c30aa4a7960529fa89793fab95e","Web proxy caching is one of the most successful solutions for improving the performance of Web-based systems. In Web proxy caching, the popular Web objects that are likely to be revisited in the near future are stored on the proxy server, which plays the key roles between users and Web sites in reducing the response time of user requests and saving the network bandwidth. However, the difficulty in determining the ideal Web objects that will be re-visited in the future is still a problem faced by existing conventional Web proxy caching techniques. In this paper, a Naïve Bayes (NB) classifier is used to enhance the performance of conventional Web proxy caching approaches such as Least-Recently-Used (LRU) and Greedy-Dual-Size (GDS). NB is intelligently incorporated with conventional Web proxy caching techniques to form intelligent and effective caching approaches known as NB-GDS, NB-LRU and NB-DA. Experimental results have revealed that the proposed NB-GDS, NB-LRU and NB-DA significantly improve the performances of the existing Web proxy caching approaches across several proxy datasets. © 2012 Elsevier Ltd. All rights reserved.","Cache replacement; Classification; Naïve Bayes classifier; Proxy server; Web caching","Bayes Classifier; Cache replacement; Data sets; Network bandwidth; Proxy server; Web caching; Web objects; Web proxy caching; Web-based system; Artificial intelligence; Classification (of information); Software engineering; Websites",Article,Scopus,2-s2.0-84859911042
"Sun M., Ma X.-X., Cao W., Du P.-P., Yang Y.-H., Xu L.","Effect of polymerization with paraformaldehyde on thermal reactivity of >300°C fraction from low temperature coal tar",2012,"Thermochimica Acta",17,10.1016/j.tca.2012.03.015,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860431977&doi=10.1016%2fj.tca.2012.03.015&partnerID=40&md5=ed7e06a71e75cbd94727bd363f08c853","&gt;300°C fraction obtained by distillation of heavy tar of low temperature coal tar was reacted with paraformaldehyde. After reaction, toluene soluble (TS-tar) and toluene insoluble (TIS-tar) fractions were collected. Their structures were confirmed by gas chromatography-mass spectroscopy (GC-MS), gel permeation chromatography (GPC) and Fourier transformed infrared (FTIR). The degradations under nitrogen atmosphere of &gt;300°C fraction and TIS-tar were investigated by thermogravimetric analyzer coupled with Fourier transform infrared spectroscopy (TG-FTIR). Based on the above analyses, polymers were generated and the carbonization yields were increased by polymerization reaction. The degradation process of TIS-tar was divided into two stages, 118-248°C and 248-700°C. In the pyrolysis process of &gt;300°C fraction, CO has no significant absorbency for infrared at 2060-2240 cm -1, but there are obvious absorption peaks in TIS-tar. Polymerization reaction of paraformaldehyde and compounds with carboxyl, short-chain fatty and ester group in &gt;300°C fraction generates polymers and fixes these compounds. © 2012 Published by Elsevier B.V.","GC-MS; Low temperature coal tar; Phenolic compounds; Polymerization; Pyrolysis; TG-FTIR","Absorption peaks; Degradation process; Ester groups; Fourier; FTIR; GC-MS; Low temperature coal tar; Nitrogen atmospheres; Paraformaldehydes; Phenolic compounds; Polymerization reaction; Pyrolysis process; TG-FT-IR; Thermal reactivity; Thermogravimetric analyzers; Acetal resins; Artificial intelligence; Carbonization; Coal tar; Distillation; Gas chromatography; Gel permeation chromatography; Mass spectrometry; Phenols; Polymerization; Polymers; Pyrolysis; Toluene; Fourier transform infrared spectroscopy",Article,Scopus,2-s2.0-84860431977
"MacHuca E., Mandow L.","Multiobjective heuristic search in road maps",2012,"Expert Systems with Applications",17,10.1016/j.eswa.2011.12.022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856963039&doi=10.1016%2fj.eswa.2011.12.022&partnerID=40&md5=5502ec9930d8778356ee8a274b451b28","This article considers the application of exact multiobjective techniques to search in large size realistic road maps. In particular, the NAMOA * algorithm is successfully applied to several road networks from the DIMACS shortest path implementation challenge with two objectives. An efficient heuristic function previously proposed by Tung and Chew is evaluated. Heuristic values are precalculated with search. The precalculation effort is shown to pay off during the multiobjective search stage. An improvement to the calculation procedure is also proposed, resulting in added improved time performance in many problem instances. © 2011 Elsevier Ltd. All rights reserved.","Artificial intelligence; Best-first search; Heuristic search; Multiobjective shortest path problem; Road networks","Best first search; Calculation procedure; Heuristic functions; Heuristic search; Large sizes; Multi objective; Multiobjective search; Multiobjective shortest path problem; Problem instances; Road network; Road-maps; Shortest path; Time performance; Artificial intelligence; Graph theory; Heuristic algorithms; Heuristic methods; Modular robots",Article,Scopus,2-s2.0-84856963039
"Wong J.-T.","DSS for 3PL provider selection in global supply chain: Combining the multi-objective optimization model with experts' opinions",2012,"Journal of Intelligent Manufacturing",17,10.1007/s10845-010-0398-z,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862272173&doi=10.1007%2fs10845-010-0398-z&partnerID=40&md5=bf3315ea30bb505c8fb5a4a5f18728a0","To provide selections of logistics outsourcing providers in a global supply chain, this paper proposes a decision support system (DSS), based on fuzzy analytic network process (FANP) and preemptive fuzzy integer goal programming (PFIGP). For decision-makers, a suitable method for selecting third-party logistics (3PL) is critical under the trends of globalization and specialization. The proposed DSS for 3PL provider selection takes into consideration flexible resource and interactions among providers. When applying multiple attribute decision-making (MADM) to selection of 3PL providers, it is difficult for decision-makers to determine the feasible solution domain under limited resources, especially if the decision problems are NP-complete. Moreover, traditional mathematical programming does not incorporate experts' evaluation of providers. This paper employs FANP to obtain experts' scores of the providers, and then integrates the scores into PFIGP to facilitate selection of a 3PL provider with flexible resources. Finally, this paper uses genetic algorithms to solve the PFIGP. An illustrative example is also given to demonstrate such DSS. The test result shows that the degree of providers' interaction affects the final weight of FANP. © Springer Science+Business Media, LLC 2010.","Fuzzy analytic network process; Genetic algorithms; Global supply chain; Logistics outsourcing; Preemptive fuzzy integer goal programming","3pl providers; Decision makers; Decision problems; Feasible solution; Flexible resources; Fuzzy analytic network process; Fuzzy integers; Global supply chain; Goal programming; Illustrative examples; Logistics outsourcing; Multi-objective optimization models; Multiple attribute decision makings (MADM); NP Complete; Third party logistics (3PL); Artificial intelligence; Decision support systems; Decision theory; Fuzzy logic; Genetic algorithms; Integer programming; Multiobjective optimization; Supply chains; Outsourcing",Article,Scopus,2-s2.0-84862272173
"Bartczuk L., Dziwiński P., Starczewski J.T.","A new method for dealing with unbalanced linguistic term set",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",17,10.1007/978-3-642-29347-4_24,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861027305&doi=10.1007%2f978-3-642-29347-4_24&partnerID=40&md5=259135c6baca54f48b67009482bf54ab","In this paper, a new method for dealing with an unbalanced linguistic term set is introduced. The proposed method is a modification of the 2-tuple linguistic model, in which we use a set of extended linguistic terms. The extended linguistic term is a pair that consists a linguistic label and a value of correction factor which describes the term shift relative to its position in an equidistant term set. This modification allows us to obtain the method that is computationally less expensive and give simpler semantics than method based on linguistic hierarchies. © 2012 Springer-Verlag Berlin Heidelberg.",,"2-Tuple; Correction factors; Linguistic hierarchies; Linguistic labels; Linguistic models; Linguistic terms; Artificial intelligence; Semantics; Soft computing; Linguistics",Conference Paper,Scopus,2-s2.0-84861027305
"Singh K.P., Gupta S.","Artificial intelligence based modeling for predicting the disinfection by-products in water",2012,"Chemometrics and Intelligent Laboratory Systems",17,10.1016/j.chemolab.2012.03.014,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860443486&doi=10.1016%2fj.chemolab.2012.03.014&partnerID=40&md5=652de206b44ea399797f83fdffcbda07","Formation of trihalomethanes (THMs) in chlorinated waters poses high risk to humans. Strategies for the THM control require pre-knowledge of their levels in the water. Determination of the THMs in the laboratory experiments is very tedious, expensive, and time consuming. Hence, the development of new predictive models for the THM formation in the chlorinated water will be of immense help. This research presents the potential of the artificial neural network (ANN), support vector machine (SVM), and gene expression programming (GEP) modeling approaches to forecast the THM formation due to chlorination. To develop the models, a total of 63 data collected from the literature were used, wherein five parameters, such as dissolved organic carbon normalized chlorine dose, water pH, temperature, bromide concentration, and contact time were used as the input variables. The predictive and generalization abilities of the models were comprehensively evaluated using several statistical tests. The results revealed that the ANN, SVM, and GEP models are capable of capturing the complex nonlinear relationship between the water disinfection conditions and the corresponding THM formation in the chlorinated water. The optimal ANN, SVM, and GEP models yielded the root mean square error and coefficient of determination values of 0.09 and 0.998; 0.70 and 0.998; and 3.07 and 0.990 for training and 4.05 and 0.918; 3.66 and 0.935; and 3.63 and 0.933 for validation set, respectively. Sensitivity analysis results revealed that initial pH, contact time and temperature were the most significant factors that influence the THM formation during chlorination process. All the three models provide fairly promising approach for the prediction of the THM formation in water during the disinfection process. SVM model performed relatively better than the ANN and GEP models. © 2012 Elsevier B.V..","Artificial neural networks; Gene expression programming; Predictive modeling; Sensitivity analysis; Support vector machines; Trihalomethanes","bromide; chlorine; organic carbon; trihalomethane; water; article; artificial intelligence; artificial neural network; chemical analysis; chlorination; concentration (parameters); controlled study; disinfection; gene expression programming; intermethod comparison; pH; prediction; priority journal; process development; process optimization; reaction time; sensitivity analysis; support vector machine; temperature sensitivity; validation study; water analysis; water disinfection; water treatment",Article,Scopus,2-s2.0-84860443486
"Gomes R., Marques A.S., Sousa J.","Decision support system to divide a large network into suitable District Metered Areas",2012,"Water Science and Technology",17,10.2166/wst.2012.061,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860805444&doi=10.2166%2fwst.2012.061&partnerID=40&md5=6cbb1666ea39a6a6edcd6d0286e2d2f4","This paper presents a new approach to divide large Water Distribution Networks (WDN) into suitable District Metered Areas (DMAs). It uses a hydraulic simulator and two operational models to identify the optimal number of DMAs, their entry points and boundary valves, and the network reinforcement/ replacement needs throughout the project plan. The first model divides the WDN into suitable DMAs based on graph theory concepts and some user-defined criteria. The second model uses a simulated annealing algorithm to identify the optimal number and location of entry points and boundary valves, and the pipes reinforcement/replacement, necessary to meet the velocity and pressure requirements. The objective function is the difference between the economic benefits in terms of water loss reduction (arising from the average pressure reduction) and the cost of implementing the DMAs. To illustrate the proposed methodology, the results from a hypothetical case study are presented and discussed. © IWA Publishing 2012.","District Metered Areas (DMAs); Network reinforcement/replacement; Simulated annealing; Water loss management","District Metered Areas (DMAs); Economic benefits; Entry point; Hydraulic simulator; Large networks; Network reinforcements; Objective functions; Operational model; Optimal number; Pressure reduction; Project plans; Simulated annealing algorithms; Theory concept; Water distribution networks; Water loss; Water loss management; Artificial intelligence; Decision support systems; Graph theory; Simulated annealing; cost-benefit analysis; decision support system; flow velocity; network design; numerical model; optimization; pipe; pressure gradient; simulated annealing; simulator; velocity profile; water supply; article; consumer; district metered area; economic aspect; methodology; model; water distribution network; water loss; water supply; Algorithms; Decision Support Systems, Management; Models, Theoretical; Water Supply",Article,Scopus,2-s2.0-84860805444
"Bolla K., Kovacs T., Fazekas G.","Gathering of fat robots with limited visibility and without global navigation",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",17,10.1007/978-3-642-29353-5_4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860694091&doi=10.1007%2f978-3-642-29353-5_4&partnerID=40&md5=74f28d931a3e770ec49248039eb9aa84","In the present paper, we introduce two different algorithms for the two dimensional gathering problem for synchronous, fat (disk-like) robots with no global navigation or communication, and with limited visibility. One of the algorithms is a slightly modified version of the local smallest enclosing circle (local SEC) algorithm. The other algorithm uses a new method of the gathering: the robots moves towards the furthest visible robot, and the robots on the perimeter of the visibility graph applies a bigger extent of move than the others. With the help of computer simulations, the two proposed algorithms are shown to be applicable for the gathering problem above and they perform better than the earlier simple SEC algorithm developed for point like robots. © 2012 Springer-Verlag.","fat robots; gathering problem; mobile robot swarm","Gathering problem; Global navigation; Limited visibility; Smallest enclosing circle; Visibility graphs; Artificial intelligence; Computer simulation; Evolutionary algorithms; Soft computing; Visibility; Robots",Conference Paper,Scopus,2-s2.0-84860694091
"Wu S., Wong H.S.","Crowd motion partitioning in a scattered motion field",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",17,10.1109/TSMCB.2012.2192267,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866502690&doi=10.1109%2fTSMCB.2012.2192267&partnerID=40&md5=799bca3fdc165bfec5fb0bf3a1313d04","In this paper, we propose a crowd motion partitioning approach based on local-translational motion approximation in a scattered motion field. To represent crowd motion in an accurate and parsimonious way, we compute optical flow at the salient locations instead of at all the pixel locations. We then transform the problem of crowd motion partitioning into a problem of scattered motion field segmentation. Based on our assumption that local crowd motion can be approximated by a translational motion field, we develop a local-translation domain segmentation (LTDS) model in which the evolution of domain boundaries is derived from the Gâteaux derivative of an objective functional and further extend LTDS to the case of scattered motion field. The experiment results on a set of synthetic vector fields and a set of videos depicting real-world crowd scenes indicate that the proposed approach is effective in identifying the homogeneous crowd motion components under different scenarios. © 1996-2012 IEEE.","Crowd motion; scattered motion field; segmentation; translation domain","Crowd motion; Domain boundary; Motion components; Motion fields; Pixel location; Synthetic vector; Translational motions; Image segmentation; algorithm; article; artificial intelligence; automated pattern recognition; crowding; human; methodology; optic flow; physiology; Algorithms; Artificial Intelligence; Crowding; Humans; Optic Flow; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84866502690
"Abdel-Dayem M.S., Annajar B.B., Hanafi H.A., Obenauer P.J.","The potential distribution of phlebotomus papatasi (Diptera: Psychodidae) in Libya based on ecological niche model",2012,"Journal of Medical Entomology",17,10.1603/ME11225,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861145123&doi=10.1603%2fME11225&partnerID=40&md5=5c752179d88e38243258a58b26be082e","The increased cases of cutaneous leishmaniasis vectored by Phlebotomus papatasi (Scopoli) in Libya have driven considerable effort to develop a predictive model for the potential geographical distribution of this disease. We collected adult P. papatasi from 17 sites in Musrata and Yefern regions of Libya using four different attraction traps. Our trap results and literature records describing the distribution of P. papatasi were incorporated into a MaxEnt algorithm prediction model that used 22 environmental variables. The model showed a high performance (AUC = 0.992 and 0.990 for training and test data, respectively). High suitability for P. papatasi was predicted to be largely confined to the coast at altitudes <600 m. Regions south of 30° N latitude were calculated as unsuitable for this species. Jackknife analysis identified precipitation as having the most significant predictive power, while temperature and elevation variables were less influential. The National Leishmaniasis Control Program in Libya may find this information useful in their efforts to control zoonotic cutaneous leishmaniasis. Existing records are strongly biased toward a few geographical regions, and therefore, further sand fly collections are warranted that should include documentation of such factors as soil texture and humidity, land cover, and normalized difference vegetation index (NDVI) data to increase the model's predictive power. © 2012 Entomological Society of America.","GIS; Leishmania; Libya; MaxEnt; Phlebotomus","algorithm; animal; article; artificial intelligence; biological model; disease carrier; geography; Libyan Arab Jamahiriya; Psychodidae; skin leishmaniasis; Algorithms; Animals; Artificial Intelligence; Geography; Insect Vectors; Leishmaniasis, Cutaneous; Libya; Models, Biological; Psychodidae; Diptera; Phlebotominae; Phlebotomus papatasi; Psychodidae",Article,Scopus,2-s2.0-84861145123
"Al-Marhoun M.A., Nizamuddin S., Raheem A.A.A., Ali S.S., Muhammadain A.A.","Prediction of crude oil viscosity curve using artificial intelligence techniques",2012,"Journal of Petroleum Science and Engineering",17,10.1016/j.petrol.2012.03.029,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860274515&doi=10.1016%2fj.petrol.2012.03.029&partnerID=40&md5=01d2c91610ee8b9263a5a38c00b8af42","Viscosity of crude oil is an important physical property that controls and influences the flow of oil through rock pores and eventually dictating oil recovery. Prediction of crude oil viscosity is one of the major challenges faced by petroleum engineers in production planning to optimize reservoir production and maximize ultimate recovery. This paper presents prediction of the complete viscosity curve as a function of pressure using artificial intelligence (AI) techniques. The viscosity curve predicted using artificial intelligence techniques derived from gas compositions of Canadian oil fields closely replicated the experimental viscosity curve above and below bubble point pressure when compared with correlations of its class. Functional Networks with Forward Selection (FNFS) outperformed all the AI techniques followed by Support Vector Machine (SVM). © 2012 Elsevier B.V.","Bubble point; Functional Networks; Support Vector Machine; Viscosity","AI techniques; Artificial intelligence techniques; Bubble point pressure; Bubble points; Crude oil viscosity; Forward selection; Function of pressure; Functional network; Gas compositions; Oil recoveries; Petroleum engineers; Production Planning; Rock pore; Viscosity curve; Artificial intelligence; Bottom hole pressure; Crude oil; Forecasting; Oil fields; Production control; Viscosity; Support vector machines; algorithm; artificial intelligence; crude oil; hydrocarbon reservoir; oil production; prediction; viscosity",Article,Scopus,2-s2.0-84860274515
"Bozga M., Iosif R., Konečný F.","Deciding conditional termination",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",17,10.1007/978-3-642-28756-5_18,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859361134&doi=10.1007%2f978-3-642-28756-5_18&partnerID=40&md5=d3437f8e3f84fdd818f3b0ff1959cdb6","This paper addresses the problem of conditional termination, which is that of defining the set of initial configurations from which a given program terminates. First we define the dual set, of initial configurations, from which a non-terminating execution exists, as the greatest fixpoint of the pre-image of the transition relation. This definition enables the representation of this set, whenever the closed form of the relation of the loop is definable in a logic that has quantifier elimination. This entails the decidability of the termination problem for such loops. Second, we present effective ways to compute the weakest precondition for non-termination for difference bounds and octagonal (non-deterministic) relations, by avoiding complex quantifier eliminations. We also investigate the existence of linear ranking functions for such loops. Finally, we study the class of linear affine relations and give a method of under-approximating the termination precondition for a non-trivial subclass of affine relations. We have performed preliminary experiments on transition systems modeling real-life systems, and have obtained encouraging results. © 2012 Springer-Verlag Berlin Heidelberg.",,"Closed form; Fixpoints; Initial configuration; Linear ranking; Non terminations; Non-trivial; Quantifier elimination; Real-life systems; Termination problems; Transition relations; Transition system; Weakest precondition; Initial configuration; Non terminations; Quantifier elimination; Real-life systems; Termination problems; Transition relations; Transition system; Weakest precondition; Algorithms; Computer science; Computers; Computability and decidability; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84859361134
"Bollig B., Cyriac A., Gastin P., Narayan Kumar K.","Model checking languages of data words",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",17,10.1007/978-3-642-28729-9_26,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859143947&doi=10.1007%2f978-3-642-28729-9_26&partnerID=40&md5=ab360b7bbad6f3325e06cf98c8b6d582","We consider the model-checking problem for data multi-pushdown automata (DMPA). DMPA generate data words, i.e, strings enriched with values from an infinite domain. The latter can be used to represent an unbounded number of process identifiers so that DMPA are suitable to model concurrent programs with dynamic process creation. To specify properties of data words, we use monadic second-order (MSO) logic, which comes with a predicate to test two word positions for data equality. While satisfiability for MSO logic is undecidable (even for weaker fragments such as first-order logic), our main result states that one can decide if all words generated by a DMPA satisfy a given formula from the full MSO logic. © 2012 Springer-Verlag Berlin Heidelberg.",,"Concurrent program; Dynamic process; First order logic; Infinite domains; Model checking problem; Monadic second-order logic; Satisfiability; Concurrent program; Dynamic process; First order logic; Infinite domains; Model checking problem; Monadic second-order logic; Push-down automata; Satisfiability; Artificial intelligence; Computation theory; Computer circuits; Formal logic; Reconfigurable hardware; Model checking; Model checking",Conference Paper,Scopus,2-s2.0-84859143947
"Kang M.H., Choi H.R., Kim H.S., Park B.J.","Development of a maritime transportation planning support system for car carriers based on genetic algorithm",2012,"Applied Intelligence",17,10.1007/s10489-011-0278-z,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862133445&doi=10.1007%2fs10489-011-0278-z&partnerID=40&md5=e551d4fb52831f1090ba211efa8f5831","Recently, the port logistics market is rapidly expanding, along with the active maritime trade. To adjust to this trend and gain a competitive advantage, competition among shipping companies at home and abroad has intensified, and many efforts are being made for the improvement of customer services and cost saving. In particular, car carriers transporting more than 80% of total car import/export volume must quickly make efforts to reduce transportation costs. Much research has been conducted to improve the efficiency of maritime transportation, but studies on car carriers, which are given relatively less importance, have been lacking. The car carrier's transportation planning is similar to the vehicle routing problem, but it is much more complicated in that cars and cargo are prepared at different points in time, and cargo can be loaded not only at the departing port but also at other ports. Therefore, in an effort to solve the problem, this study has developed a meta-heuristic algorithm based on a genetic algorithm, and we have succeeded in developing a maritime transportation planning support system with the algorithm, thus making it possible to prepare various alternatives, evaluate them, and consequently support user's decision making. © 2011 Springer-Verlag.","Decision support system; Genetic algorithm; Maritime transportation; Routing problem","Competitive advantage; Cost saving; Customer services; Maritime transportation; Meta heuristic algorithm; Port logistics; Routing problems; Shipping companies; Transportation cost; Transportation planning; Vehicle Routing Problems; Artificial intelligence; Competition; Decision support systems; Genetic algorithms; Heuristic algorithms; Ports and harbors; Waterway transportation",Article,Scopus,2-s2.0-84862133445
"Bulychev P., David A., Guldstrand Larsen K., Legay A., Li G., Bøgsted Poulsen D., Stainer A.","Monitor-based statistical model checking for weighted metric temporal logic",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",17,10.1007/978-3-642-28717-6_15,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863386728&doi=10.1007%2f978-3-642-28717-6_15&partnerID=40&md5=328fa4a81a4934f53d21b4fda720aace","We present a novel approach and implementation for analysing weighted timed automata (WTA) with respect to the weighted metric temporal logic (WMTL ≤). Based on a stochastic semantics of WTAs, we apply statistical model checking (SMC) to estimate and test probabilities of satisfaction with desired levels of confidence. Our approach consists in generation of deterministic monitors for formulas in WMTL ≤, allowing for efficient SMC by run-time evaluation of a given formula. By necessity, the deterministic observers are in general approximate (over- or under-approximations), but are most often exact and experimentally tight. The technique is implemented in the new tool Casaal. that we seamlessly connect to Uppaal-smc. in a tool chain. We demonstrate the applicability of our technique and the efficiency of our implementation through a number of case-studies. © 2012 Springer-Verlag.",,"Case-studies; Metric temporal logic; Runtimes; Statistical models; Stochastic semantics; Weighted timed automata; Artificial intelligence; Semantics; Model checking",Conference Paper,Scopus,2-s2.0-84863386728
"Ghosh N., Ghosh S.K.","A planner-based approach to generate and analyze minimal attack graph",2012,"Applied Intelligence",17,10.1007/s10489-010-0266-8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862137813&doi=10.1007%2fs10489-010-0266-8&partnerID=40&md5=728884bb4d67208acb85c98f7d053830","In the present scenario, even well administered networks are susceptible to sophisticated cyber attacks. Such attack combines vulnerabilities existing on different systems/ services and are potentially more harmful than single point attacks. One of the methods for analyzing such security vulnerabilities in an enterprise network is the use of attack graph. It is a complete graph which gives a succinct representation of different attack scenarios, depicted by attack paths. An attack path is a logical succession of exploits, where each exploit in the series satisfies the preconditions for subsequent exploits and makes a causal relationship among them. Thus analysis of the attack graph may help in assessing network security from hackers' perspective. One of the intrinsic problems with the generation and analysis of such a complete attack graph is its scalability. In this work, an approach based on Planner, a special purpose search algorithm from artificial intelligence domain, has been proposed for time-efficient, scalable representation of the attack graphs. Further, customized algorithms have been developed for automatic generation of attack paths (using Planner as a low-level module). The analysis shows that generation of attack graph using the customized algorithms can be done in polynomial time. A case study has also been presented to demonstrate the efficacy of the proposed methodology. © Springer Science+Business Media, LLC 2010.","Attack graph; Attack path; Exploit; Network security; Planner","Attack graph; Attack path; Automatic Generation; Causal relationships; Complete graphs; Cyber-attacks; Different attacks; Enterprise networks; Exploit; Planner; Polynomial-time; Search Algorithms; Security vulnerabilities; Single point; Succinct representation; Algorithms; Artificial intelligence; Network security; Personal computing; Polynomial approximation; Graph theory",Article,Scopus,2-s2.0-84862137813
"Liu X., Lao C., Li X., Liu Y., Chen Y.","An integrated approach of remote sensing, GIS and swarm intelligence for zoning protected ecological areas",2012,"Landscape Ecology",17,10.1007/s10980-011-9684-1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856427867&doi=10.1007%2fs10980-011-9684-1&partnerID=40&md5=d206e75a1c23e4a9b0319d3b6bd06145","Interest in protecting ecological areas is increasing because of land uses conflicts and environmental pressures. The optimal zoning of protected ecological areas belongs to a NP-hard problem because it is subject to both box and spatial constraints. A challenge in solving area optimization problems emerges with the increasing size of a study region. In this article, an integrated approach of remote sensing, GIS and modified ant colony optimization (ACO) is proposed for application in zoning protected ecological areas. Significant modifications have been made in the conventional ACO so that it can be further extended to solve zoning problems in large regions. An improved selection strategy is designed to accelerate the progress of sites selection for artificial ants. Another important modification in ACO is to incorporate the neighborhood diffusion strategy into pheromone updating. The optimal objective is to generate protected areas that maximize both ecological suitability and spatial compactness. The modified ACO model has been successfully applied to a case study involving an area of 25,483 cells in Dongguan, Guangdong, China. The experiments have demonstrated that the proposed model is an efficient and effective optimization technique for generating optimal protection. The modified ACO model only requires approximately 119 s for determining near-optimal solutions. Furthermore, the proposed method performs better than other methods, including simulated annealing, genetic algorithm, iterative relaxation, basic ACO, and density slicing. © 2011 Springer Science+Business Media B.V.","ACO; GIS; Protected ecological areas; Remote sensing; Zoning","artificial intelligence; experimental study; genetic algorithm; GIS; integrated approach; land use change; numerical model; optimization; pheromone; protected area; remote sensing; China; Dongguan; Guangdong; Formicidae",Article,Scopus,2-s2.0-84856427867
"Wästlund E., Angulo J., Fischer-Hübner S.","Evoking comprehensive mental models of anonymous credentials",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",17,10.1007/978-3-642-27585-2_1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857090401&doi=10.1007%2f978-3-642-27585-2_1&partnerID=40&md5=03ad7737af8b019bc3ce696101009deb","Anonymous credentials are a fundamental technology for preserving end users' privacy by enforcing data minimization for online applications. However, the design of user-friendly interfaces that convey their privacy benefits to users is still a major challenge. Users are still unfamiliar with the new and rather complex concept of anonymous credentials, since no obvious real-world analogies exists that can help them create the correct mental models. In this paper we explore different ways in which suitable mental models of the data minimization property of anonymous credentials can be evoked on end users. To achieve this, we investigate three different approaches in the context of an e-shopping scenario: a card-based approach, an attribute-based approach and an adapted card-based approach. Results show that the adapted card-based approach is a good approach towards evoking the right mental models for anonymous credential applications. However, better design paradigms are still needed to make users understand that attributes can be used to satisfy conditions without revealing the value of the attributes themselves. © 2012 Springer-Verlag.","Anonymous Credentials; Credential Selection; Mental Models; Usability","Anonymous credential; Credential Selection; Design paradigm; End users; Mental model; Mental Models; On-line applications; Usability; User friendly interface; Artificial intelligence; Network security",Conference Paper,Scopus,2-s2.0-84857090401
"Namin F.S., Shahriar K., Bascetin A., Ghodsypour S.H.","FMMSIC: A hybrid fuzzy based decision support system for MMS (in order to estimate interrelationships between criteria)",2012,"Journal of the Operational Research Society",17,10.1057/jors.2011.24,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855650674&doi=10.1057%2fjors.2011.24&partnerID=40&md5=105b4c15dd354331b373d2a871227d07","One of the main tasks in exploitation of ore-body is to select a suitable mining method. In mining method selection (MMS) problems, a decision procedure has to choose the best exploitation method that satisfies the evaluation criteria. It is generally hard to find a mining method that meets all the criteria simultaneously, therefore a good compromise solution is preferred as the final selection. Furthermore, the MMS problem is an inherently uncertain activity. To deal with the uncertainty, this paper presents an hybrid decision support system based on the fuzzy multi attribute decision making, named the fuzzy mining method selection with interrelation criteria (FMMSIC). FMMSIC models the relative weights of criteria by combining the fuzzy analytic network process and fuzzy entropy, and discusses using these hybrid techniques to determine the overall weights. Subsequently, the technique for order preference by similarity to an ideal solution method was modified by various normalization norms according to the MMS problem condition. Finally, to illustrate how the FMMSIC is used for the MMS problems, an empirical study of a real case is conducted. It shows by means of an application that the FMMSIC is well suited as a decision support system for the MMS. © 2012 Operational Research Society Ltd. All rights reserved.","FMMSIC; fuzzy ANP; fuzzy entropy; hybrid decision support system; mining method selection; modified TOPSIS","Decision supports; FMMSIC; Fuzzy ANP; Fuzzy entropy; mining method selection; Modified TOPSIS; Artificial intelligence; Entropy; Fuzzy logic; Mining; Decision support systems",Article,Scopus,2-s2.0-84855650674
"Ballester M., Oppenheimer A., D'Argent E.M., Touboul C., Antoine J.-M., Coutant C., Daraï E.","Nomogram to predict pregnancy rate after ICSI-IVF cycle in patients with endometriosis",2012,"Human Reproduction",17,10.1093/humrep/der392,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855902744&doi=10.1093%2fhumrep%2fder392&partnerID=40&md5=c1382e689b691703bdcb9d460276a99b","BACKGROUND: Although several scoring systems have been published to evaluate the pregnancy rate after ICSI-IVF in infertile patients, none of them are applicable for patients with deep infiltrating endometriosis (DIE) nor can they evaluate the chances of pregnancy for individual patients. The aim of this study was to develop a nomogram based on an association of patients characteristics to predict the clinical pregnancy rate in patients with endometriosis. METHODS: This prospective longitudinal study was conducted from January 2007 to June 2010. The nomogram was built from a training cohort of 94 consecutive patients (141 ICSI-IVF cycles) and tested on an independent validation cohort of 48 patients (83 ICSI-IVF cycles). DIE was confirmed in all participants. RESULTS: The pregnancy rate (per patient) in women with and without DIE was 58 and 83%, respectively (P = 0.03). Increased patient age (P = 0.04), serum anti-Mullerian hormone (AMH) level ≤1 ng/ml (P = 0.03) and increased number of ICSI-IVF cycles (P = 0.03) were associated with a decreased clinical pregnancy rate. The presence of DIE was the strongest determinant factor of the clinical pregnancy rate in our model [odds ratio = 0.26, 95% confidence interval (CI): 0.07-0.9 (P = 0.006)], which also included patient age, serum AMH level and number of attempts at ICSI-IVF. The nomogram showed an area under the curve (AUC) of 0.76 for the training cohort (95% CI: 0.7-0.8) and was well calibrated. The AUC for the validation cohort was 0.68 (95% CI: 0.6-0.75) and calibration was good. CONCLUSIONS: Our nomogram provides realistic and precise information about ICSI-IVF success and can be used to guide couples and practitioners. © The Author 2011. Published by Oxford University Press. All rights reserved.","endometriosis; IVF; nomogram; prediction models; pregnancy","Muellerian inhibiting factor; adult; age; article; calibration; clinical trial; confidence interval; deep infiltrating endometriosis; endometriosis; female; fertilization in vitro; hormone blood level; human; longitudinal study; major clinical study; nomogram; predictive value; pregnancy rate; pregnancy test; Adult; Age Factors; Anti-Mullerian Hormone; Artificial Intelligence; Cohort Studies; Endometriosis; Family Characteristics; Female; France; Humans; Infertility, Female; Infertility, Male; Longitudinal Studies; Male; Models, Biological; Nomograms; Pregnancy; Pregnancy Rate; Prospective Studies; ROC Curve; Severity of Illness Index; Sperm Injections, Intracytoplasmic; Young Adult",Article,Scopus,2-s2.0-84855902744
"Goel L., Gupta D., Panchal V.K.","Hybrid bio-inspired techniques for land cover feature extraction: A remote sensing perspective",2012,"Applied Soft Computing Journal",17,10.1016/j.asoc.2011.10.006,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84655163233&doi=10.1016%2fj.asoc.2011.10.006&partnerID=40&md5=b1acbf693d90a5142b11ee4c0d415274","Recent advances in the theoretical and practical implementations of biogeography have led to the exploration of new bio-inspired techniques which can prove to be the building blocks of hybrid bio-inspired techniques. This aspect was discovered while considering the exploration of bio-inspired intelligence for developing generic optimization algorithms that can be adapted for performing the given land cover feature extraction task at hand. Certain bio-inspired techniques when integrated with the existing optimization techniques can drastically improve their optimization capability hence leading to better feature extraction. In this paper, we propose a generic architectural framework of a hybrid biologically inspired technique that is characterized by its capability to adapt according to the database of expert knowledge for a more efficient, focused and refined feature extraction. Since our hybrid feature extractor possesses intelligence for selective cluster identification for application of either of the constituent techniques which is in turn based on an inefficiency analysis, we term our classifier as the hybrid bio-inspired pattern analysis based intelligent classifier. Our hybrid classifier combines the strengths of the modified BBO Technique for land cover feature extraction with the Hybrid ACO2/PSO Technique for a more refined land cover feature extraction. The algorithm has been tested for for the remote sensing application of land cover feature extraction where we have applied it to the 7-Band carto-set satellite image of size 472 × 546 of the Alwar area in Rajasthan and gives far better feature extraction results than the original biogeography based land cover feature extractor [20] and the other soft computing techniques such as ACO, Hybrid PSO-ACO2, Hybrid ACO-BBO Classifier, Fuzzy sets, Rough-Fuzzy Tie up etc. The 7-band Alwar Image is a benchmark image for testing the performance of a bio-inspired classifier on multi-spectral satellite images since this image is a complete image in the sense that it contains all the land cover features that we need to extract and hence land cover feature extraction results are demonstrated and compared using this image as the standard image. © 2011 Elsevier B.V. All rights reserved.","Ant Colony Optimization; Biogeography; Feature extraction; Image classification; Kappa Coefficient; Particle Swarm Optimization; Remote sensing","Ant-colony optimization; Architectural frameworks; Bio-inspired; Bio-inspired techniques; Biogeography; Biologically inspired techniques; Building blockes; Expert knowledge; Feature extractor; Generic optimization algorithm; Hybrid classifier; Hybrid features; Intelligent classifiers; Kappa coefficient; Land cover; Multispectral satellite image; Optimization capabilities; Optimization techniques; Particle swarm; Pattern analysis; Practical implementation; Rajasthan; Remote sensing applications; Satellite images; Softcomputing techniques; Standard images; Algorithms; Artificial intelligence; Benchmarking; Constrained optimization; Ecology; Feature extraction; Fuzzy sets; Image classification; Landforms; Refining; Remote sensing; Soft computing; Image processing",Article,Scopus,2-s2.0-84655163233
"Chen D., Wang J., Zou F., Hou W., Zhao C.","An improved group search optimizer with operation of quantum-behaved swarm and its application",2012,"Applied Soft Computing Journal",17,10.1016/j.asoc.2011.10.021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84655169227&doi=10.1016%2fj.asoc.2011.10.021&partnerID=40&md5=e567128fcc46a813570fd22b254a90ef","Group search optimizer (GSO) is a novel swarm intelligent (SI) algorithm for continuous optimization problem. The framework of the algorithm is mainly based on the producer-scrounger (PS) model. Comparing with ant colony optimization (ACO) and particle swarm optimization (PSO) algorithms, GSO emphasizes more on imitating searching behavior of animals. In standard GSO algorithm, more than 80% individuals are chosen as scroungers, and the producer is the one and only destination of them. When the producer cannot found a better position than the old one in some successive iterations, the scroungers will almost move to the same place, the group might be trapped into local optima though a small quantity of rangers are used to improve the diversity of it. To improve the convergence performance of GSO, an improved GSO optimizer with quantum-behaved operator for scroungers according to a certain probability is presented in the paper. In the method, the scroungers are divided into two parts, the scroungers in the first part update their positions with the operators of QPSO, and the remainders keep searching for opportunities to join the resources found by the producer. The operators of QPSO are utilized to improve the diversity of population for GSO. The improved GSO algorithm (IGSO) is tested on several benchmark functions and applied to train single multiplicative neuron model. The results of the experiments indicate that IGSO is competitive to some other EAs. © 2011 Elsevier B.V. All rights reserved.","Evolutionary algorithms (EAs); Group search optimizer (GSO); Improved GSO algorithm (IGSO); Quantum-behaved particle swarm optimization (QPSO); Single multiplicative neuron model","Ant-colony optimization; Benchmark functions; Continuous optimization problems; Convergence performance; Improved GSO algorithm (IGSO); Local optima; Multiplicative neuron model; Optimizers; Particle swarm optimization algorithm; Quantum-behaved particle swarm optimization; Search optimizer; Searching behavior; Successive iteration; Swarm intelligent; Animals; Artificial intelligence; Particle swarm optimization (PSO); Algorithms",Article,Scopus,2-s2.0-84655169227
"Kim S., Kwon D., Shin S., Wilbur W.J.","PIE the search: Searching PubMed literature for protein interaction information",2012,"Bioinformatics",17,10.1093/bioinformatics/btr702,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857143714&doi=10.1093%2fbioinformatics%2fbtr702&partnerID=40&md5=33e71ae9cb949c2d6fc66379e99799de","Motivation: Finding protein-protein interaction (PPI) information from literature is challenging but an important issue. However, keyword search in PubMed® is often time consuming because it requires a series of actions that refine keywords and browse search results until it reaches a goal. Due to the rapid growth of biomedical literature, it has become more difficult for biologists and curators to locate PPI information quickly. Therefore, a tool for prioritizing PPI informative articles can be a useful assistant for finding this PPI-relevant information. Results: PIE (Protein Interaction information Extraction) the search is a web service implementing a competition-winning approach utilizing word and syntactic analyses by machine learning techniques. For easy user access, PIE the search provides a PubMed-like search environment, but the output is the list of articles prioritized by PPI confidence scores. By obtaining PPI-related articles at high rank, researchers can more easily find the up-to-date PPI information, which cannot be found in manually curated PPI databases. © The Author(s) 2012. Published by Oxford University Press.",,"protein; article; artificial intelligence; Medline; metabolism; protein analysis; Artificial Intelligence; Protein Interaction Mapping; Proteins; PubMed",Article,Scopus,2-s2.0-84857143714
"Roy S., Carass A., Bazin P.-L., Resnick S., Prince J.L.","Consistent segmentation using a Rician classifier",2012,"Medical Image Analysis",17,10.1016/j.media.2011.12.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856209241&doi=10.1016%2fj.media.2011.12.001&partnerID=40&md5=0f5c49c22da0153e23ab5d93b0e78a49","Several popular classification algorithms used to segment magnetic resonance brain images assume that the image intensities, or log-transformed intensities, satisfy a finite Gaussian mixture model. In these methods, the parameters of the mixture model are estimated and the posterior probabilities for each tissue class are used directly as soft segmentations or combined to form a hard segmentation. It is suggested and shown in this paper that a Rician mixture model fits the observed data better than a Gaussian model. Accordingly, a Rician mixture model is formulated and used within an expectation maximization (EM) framework to yield a new tissue classification algorithm called Rician Classifier using EM (RiCE). It is shown using both simulated and real data that RiCE yields comparable or better performance to that of algorithms based on the finite Gaussian mixture model. As well, we show that RiCE yields more consistent segmentation results when used on images of the same individual acquired with different T1-weighted pulse sequences. Therefore, RiCE has the potential to stabilize segmentation results in brain studies involving heterogeneous acquisition sources as is typically found in both multi-center and longitudinal studies. © 2011 Elsevier B.V.","Biomedical imaging; Medical image segmentation; Rician distribution; Tissue classification","Biomedical imaging; Brain study; Classification algorithm; Expectation Maximization; Finite gaussian mixture models; Gaussian model; Image intensities; Longitudinal study; Magnetic resonance brain images; Medical image segmentation; Mixture model; Observed data; Posterior probability; Pulse sequence; Rice yield; Rician distribution; Rician mixtures; Segmentation results; Tissue classification; Algorithms; Computer simulation; Gaussian distribution; Magnetic resonance; Medical imaging; Mixtures; Tissue; Image segmentation; article; classification algorithm; classifier; controlled study; longitudinal study; normal distribution; nuclear magnetic resonance imaging; priority journal; rician classifier; statistical model; Algorithms; Artificial Intelligence; Brain; Diffusion Magnetic Resonance Imaging; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84856209241
"Klauer B., Rode M., Schiller J., Franko U., Mewes M.","Decision Support for the Selection of Measures according to the Requirements of the EU Water Framework Directive",2012,"Water Resources Management",17,10.1007/s11269-011-9944-5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856007838&doi=10.1007%2fs11269-011-9944-5&partnerID=40&md5=68b29ce7199462bb90e48c6138a6adb4","One major scientific challenge posed by the EU Water Framework Directive (WFD) is the design of a decision support process that meets the Directive's requirement to achieve ""good status"" for all water bodies using a cost-effective combination of measures. This paper presents BASINFORM, a new decision methodology for selecting cost-effective management measures, developed in close co-operation with the water authorities and tested in the 5,154 km 2 mesoscale river Weisse Elster in central Germany. BASINFORM comprises (i) a procedure for framing the specific problems in the water bodies, including quantification of the need for action, (ii) modelling tools for quantifying the impacts of management measures, and (iii) a method for selecting cost-effective combinations of measures. One innovative feature of BASINFORM is that it structures the complex decision problems appropriately for practical use and provides an easy-to-use framework for integrating scientific and practical knowledge. A trial run applying BASINFORM to the Weisse Elster catchment revealed that good surface water status with respect to nutrient levels cannot be achieved if only the ""standard"" actions of current water management are taken to reduce point sources (sewage treatment) and diffuse agricultural sources. It also became clear that the nutrient-reduction measures available will generate considerable costs. The application of BASINFORM in this case study demonstrated its practical applicability in the WFD implementation process. Beyond the case study described here BASINFORM is currently being used for practical implementation of the WFD in the German Federal State of Thuringia. © 2011 Springer Science+Business Media B.V.","BASINFORM; Cost effectiveness; Decision making; Decision support system; EU water framework directive; Meta-CANDY; Point and non-point pollution; WASP 5","BASINFORM; Decision supports; Meta-CANDY; Point and non-point pollution; WASP 5; Artificial intelligence; Catchments; Cost effectiveness; Costs; Decision support systems; Innovation; Nutrients; Sewage treatment; Surface waters; Water conservation; Water management; Decision making; cost-benefit analysis; decision making; decision support system; European Union; implementation process; nonpoint source pollution; point source pollution; water management; Germany",Article,Scopus,2-s2.0-84856007838
"Gaonkar B., Davatzikos C.","Deriving statistical significance maps for SVM based image classification and group comparisons",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",17,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872527748&partnerID=40&md5=67e2435f4177b433f12103faa6129263","Population based pattern analysis and classification for quantifying structural and functional differences between diverse groups has been shown to be a powerful tool for the study of a number of diseases, and is quite commonly used especially in neuroimaging. The alternative to these pattern analysis methods, namely mass univariate methods such as voxel based analysis and all related methods, cannot detect multivariate patterns associated with group differences, and are not particularly suitable for developing individual-based diagnostic and prognostic biomarkers. A commonly used pattern analysis tool is the support vector machine (SVM). Unlike univariate statistical frameworks for morphometry, analytical tools for statistical inference are unavailable for the SVM. In this paper, we show that null distributions ordinarily obtained by permutation tests using SVMs can be analytically approximated from the data. The analytical computation takes a small fraction of the time it takes to do an actual permutation test, thereby rendering it possible to quickly create statistical significance maps derived from SVMs. Such maps are critical for understanding imaging patterns of group differences and interpreting which anatomical regions are important in determining the classifier’s decision. © Springer-Verlag Berlin Heidelberg 2012.",,"Diagnosis; Image classification; Medical computing; Medical imaging; Neuroimaging; Analytical computations; Anatomical regions; Multivariate patterns; Permutation tests; Statistical framework; Statistical inference; Statistical significance; Voxel-based analysis; Support vector machines; algorithm; Alzheimer disease; article; artificial intelligence; automated pattern recognition; brain; brain mapping; computer program; computer simulation; diagnostic imaging; human; methodology; pathology; statistical model; support vector machine; Algorithms; Alzheimer Disease; Artificial Intelligence; Brain; Brain Mapping; Computer Simulation; Diagnostic Imaging; Humans; Models, Statistical; Pattern Recognition, Automated; Software; Support Vector Machines",Conference Paper,Scopus,2-s2.0-84872527748
"Nouretdinov I., Gammerman A., Qi Y., Klein-Seetharaman J.","Determining confidence of predicted interactions between HIV-1 and human proteins using conformal method",2012,"17th Pacific Symposium on Biocomputing, PSB 2012",17,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891456421&partnerID=40&md5=1e2046d5aad232abe321daa4faa33852","Identifying protein-protein interactions (PPI's) is critical for understanding virtually all cellular molecular mechanisms. Previously, predicting PPI's was treated as a binary classification task and has commonly been solved in a supervised setting which requires a positive labeled set of known PPI's and a negative labeled set of non-interacting protein pairs. In those methods, the learner provides the likelihood of the predicted interaction, but without a confidence level associated with each prediction. Here, we apply a conformal prediction framework to make predictions and estimate confidence of the predictions. The conformal predictor uses a function measuring relative 'strangeness' interacting pairs to check whether prediction of a new example added to the sequence of already known PPI's would conform to the 'exchangeability' assumption: distribution of interacting pairs is invariant with any permutations of the pairs. In fact, this is the only assumption we make about the data. Another advantage is that the user can control a number of errors by providing a desirable confidence level. This feature of CP is very useful for a ranking list of possible interactive pairs. In this paper, the conformal method has been developed to deal with just one class - class interactive proteins - while there is not clearly defined of 'non-interactive' pairs. The confidence level helps the biologist in the interpretation of the results, and better assists the choices of pairs for experimental validation. We apply the proposed conformal framework to improve the identification of interacting pairs between HIV-1 and human proteins.","Confident prediction; Protein-protein interaction; Suspected interactions","Human immunodeficiency virus protein; small interfering RNA; transactivator protein; algorithm; article; artificial intelligence; biology; genetics; host pathogen interaction; human; Human immunodeficiency virus 1; pathogenicity; physiology; protein analysis; protein database; statistical model; statistics; Algorithms; Artificial Intelligence; Computational Biology; Databases, Protein; HIV-1; Host-Pathogen Interactions; Human Immunodeficiency Virus Proteins; Humans; Models, Statistical; Protein Interaction Mapping; RNA, Small Interfering; tat Gene Products, Human Immunodeficiency Virus",Conference Paper,Scopus,2-s2.0-84891456421
"Campanella G., Pereira A., Ribeiro R.A., Varela M.L.R.","Collaborative dynamic decision making: A case study from B2B supplier selection",2012,"Lecture Notes in Business Information Processing",17,10.1007/978-3-642-32191-7_7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865983305&doi=10.1007%2f978-3-642-32191-7_7&partnerID=40&md5=8e33d30745a304633b7b348729fa19a6","The problem of supplier selection can be easily modeled as a multiple-criteria decision making (MCDM) problem: businesses express their preferences with respect to suppliers, which can then be ranked and selected. This approach has two major pitfalls: first, it does not consider a dynamic scenario, in which suppliers and their ratings are constantly changing; second, it only addressed the problem from the point of view of a single business, and cannot be easily applied when considering more than one business. To overcome these problems, we introduce a method for supplier selection that builds upon the dynamic MCDM framework of [1] and, by means of a linear programming model, can be used in the case of multiple collaborating businesses planning their next batch of orders together. © 2012 Springer-Verlag.",,"Artificial intelligence; Computer programming; Decision support systems; Dynamic decision making; Linear programming models; Multiple criteria decision making; Supplier selection; Decision making",Conference Paper,Scopus,2-s2.0-84865983305
"Patil S.G., Mandal S., Hegde A.V.","Genetic algorithm based support vector machine regression in predicting wave transmission of horizontally interlaced multi-layer moored floating pipe breakwater",2012,"Advances in Engineering Software",17,10.1016/j.advengsoft.2011.09.026,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-83555174962&doi=10.1016%2fj.advengsoft.2011.09.026&partnerID=40&md5=8ac27078d73e2a8b06c530cb8cc44d4a","Planning and design of coastal protection works like floating pipe breakwater require information about the performance characteristics of the structure in reducing the wave energy. Several researchers have carried out analytical and numerical studies on floating breakwaters in the past but failed to give a simple mathematical model to predict the wave transmission through floating breakwaters by considering all the boundary conditions. Computational intelligence techniques, such as, Artificial Neural Networks (ANN), fuzzy logic, genetic programming and Support Vector Machine (SVM) are successfully used to solve complex problems. In the present paper, a hybrid Genetic Algorithm Tuned Support Vector Machine Regression (GA-SVMR) model is developed to predict wave transmission of horizontally interlaced multilayer moored floating pipe breakwater (HIMMFPB). Furthermore, optimal SVM and kernel parameters of GA-SVMR models are determined by genetic algorithm. The GA-SVMR model is trained on the data set obtained from experimental wave transmission of HIMMFPB using regular wave flume at Marine Structure Laboratory, National Institute of Technology, Karnataka, Surathkal, Mangalore, India. The results are compared with ANN and Adaptive Neuro-Fuzzy Inference System (ANFIS) models in terms of correlation coefficient, root mean square error and scatter index. Performance of GA-SVMR is found to be reliably superior. b-spline kernel function performs better than other kernel functions for the given set of data. © 2011 Elsevier Ltd. All rights reserved.","ANFIS; Artificial neural network; Floating breakwater; Genetic algorithm; HIMMFPB; Support vector machine; Wave transmission","Artificial intelligence; Floating breakwaters; Forecasting; Fuzzy inference; Fuzzy logic; Fuzzy neural networks; Fuzzy systems; Genetic programming; Hydraulic structures; Mathematical models; Mean square error; Offshore structures; Shore protection; Support vector machines; Vectors; Wave energy conversion; Wave transmission; Adaptive neuro-fuzzy inference system; ANFIS; Artificial Neural Network; B-spline; Coastal protection; Complex problems; Computational intelligence techniques; Correlation coefficient; Data sets; HIMMFPB; Hybrid genetic algorithms; Karnataka; Kernel function; Kernel parameter; Numerical studies; Performance characteristics; Planning and design; Regular waves; Root mean square errors; Scatter index; Support vector; Support vector machine regressions; Wave energy; Genetic algorithms",Article,Scopus,2-s2.0-83555174962
"Zazzi M., Incardona F., Rosen-Zvi M., Prosperi M., Lengauer T., Altmann A., Sonnerborg A., Lavee T., Schülter E., Kaiser R.","Predicting response to antiretroviral treatment by machine learning: The euresist project",2012,"Intervirology",17,10.1159/000332008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856433482&doi=10.1159%2f000332008&partnerID=40&md5=c5c95e83b0aa9d0b151ae1029c233835","For a long time, the clinical management of antiretroviral drug resistance was based on sequence analysis of the HIV genome followed by estimating drug susceptibility from the mutational pattern that was detected. The large number of anti-HIV drugs and HIV drug resistance mutations has prompted the development of computer-aided genotype interpretation systems, typically comprising rules handcrafted by experts via careful examination of in vitro and in vivo resistance data. More recently, machine learning approaches have been applied to establish data-driven engines able to indicate the most effective treatments for any patient and virus combination. Systems of this kind, currently including the Resistance Response Database Initiative and the EuResist engine, must learn from the large data sets of patient histories and can provide an objective and accurate estimate of the virological response to different antiretroviral regimens. The EuResist engine was developed by a European consortium of HIV and bioinformatics experts and compares favorably with the most commonly used genotype interpretation systems and HIV drug resistance experts. Next-generation treatment response prediction engines may valuably assist the HIV specialist in the challenging task of establishing effective regimens for patients harboring drug-resistant virus strains. The extensive collection and accurate processing of increasingly large patient data sets are eagerly awaited to further train and translate these systems from prototype engines into real-life treatment decision support tools. Copyright © 2012 S. Karger AG, Basel.","Antiretroviral therapy; EuResist; HIV resistance; Interpretation system; Machine learning","antiretrovirus agent; darunavir; etravirine; tipranavir; age; antiviral resistance; antiviral therapy; article; case based reasoning; CD4 lymphocyte count; clinical decision making; decision support system; drug response; ethnicity; EuResist engine; follow up; fuzzy logic; genotype; human; Human immunodeficiency virus infection; machine learning; medical history; prediction; priority journal; probability; quality control; random forest; reference database; support vector machine; virus load; Anti-HIV Agents; Artificial Intelligence; Drug Resistance, Viral; Genotype; HIV Infections; HIV-1; Humans; Microbial Sensitivity Tests",Article,Scopus,2-s2.0-84856433482
"Bai Y., Xu B., Ma Y., Sun G., Zhao Y.","Will you have a good sleep tonight? Sleep quality prediction with mobile phone",2012,"BODYNETS 2012 - 7th International Conference on Body Area Networks",17,10.4108/icst.bodynets.2012.250091,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908577272&doi=10.4108%2ficst.bodynets.2012.250091&partnerID=40&md5=251a9b500d595a529c11a9a01be2a538","Understanding the relationship between sleep and daily life can provide insights into a healthy life style since the sleep quality is one of the most important indicators of people's health status. This paper studies the extent to which a person's sleep quality can be predicted by his/her daily context information. A combination of the machine learning technology and medical knowledge is used to study the relation between context and sleep quality, so that sleep quality can be predicted in real time according to the relation. We propose a novel sleep quality predicting framework from user context data, without requiring users to wear special devices. We develop a data collecting and analyzing prototype system called SleepMiner, which uses on-phone data such as mobile sensor data and communication data to extract human contexts. Then the relationship between context data and sleep quality is analyzed and a learning model based on factor graph model is proposed to predict sleep quality. From experimental results we demonstrate that it is possible to accurately infer sleep quality (around 78%) from user context information. A set of solutions are proposed to address the practical problems of Android phone in data collection, making SleepMiner work with minimal impact on the phone's resources. We finally carry out experiments to evaluate our design in effectiveness and efficiency. Copyright © 2012 ICST.","Factor graph model; Mobile phone; Sleep quality prediction","Artificial intelligence; Cellular telephone systems; Cellular telephones; Data acquisition; E-learning; Forecasting; Graph theory; Learning systems; Mobile phones; Networks (circuits); Semantics; Telephone sets; Communication data; Context information; Effectiveness and efficiencies; Factor graphs; Machine learning technology; Medical knowledge; Practical problems; Sleep quality; Sleep research",Conference Paper,Scopus,2-s2.0-84908577272
"Zare H., Bashashati A., Kridel R., Aghaeepour N., Haffari G., Connors J.M., Gascoyne R.D., Gupta A., Brinkman R.R., Weng A.P.","Automated analysis of multidimensional flow cytometry data improves diagnostic accuracy between mantle cell lymphoma and small lymphocytic lymphoma",2012,"American Journal of Clinical Pathology",17,10.1309/AJCPMMLQ67YOMGEW,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855257738&doi=10.1309%2fAJCPMMLQ67YOMGEW&partnerID=40&md5=ebdd154d7bab00515f7e022cb79d134c","Mantle cell lymphoma (MCL) and small lymphocytic lymphoma (SLL) exhibit similar but distinct immunophenotypic profiles. Many cases can be diagnosed readily by flow cytometry (FCM) alone; however, ambiguous cases are frequently encountered and necessitate additional studies, including immunohistochemical staining for cyclin D1 and fluorescence in situ hybridization for IgH-CCND1 rearrangement. To determine if greater diagnostic accuracy could be achieved from FCM data alone, we developed an unbiased, machine-based algorithm to identify features that best distinguish between the 2 diseases. By applying conventional diagnostic criteria to the flow cytometry data, we were able to assign 28 of 44 (64%) MCL and 48 of 70 (69%) SLL cases correctly. In contrast, we were able to assign all 44 (100%) MCL and 68 of 70 (97%) SLL cases correctly using a novel set of criteria, as identified by our automated approach. The most discriminating feature was the CD20/CD23 mean fluorescence intensity ratio, and we found unexpectedly that inclusion of FMC7 expression in the diagnostic algorithm actually reduced its accuracy. This study demonstrates that computational methods can be used on existing clinical FCM data to improve diagnostic accuracy and suggests similar computational approaches could be used to identify novel prognostic markers and perhaps subdivide existing or define new diagnostic entities. © American Society for Clinical Pathology.","Automated data analysis; Bioinformatics; Diagnostic algorithm; Flow cytometry; Immunopathology; Mantle cell lymphoma; Small lymphocytic lymphoma","antigen; biological marker; CD11 antigen; CD19 antigen; CD20 antigen; CD23 antigen; CD5 antigen; cyclin D1; FMC7 antigen; immunoglobulin light chain; unclassified drug; aged; algorithm; antigen expression; article; autoanalysis; controlled study; diagnostic accuracy; differential diagnosis; false positive result; female; flow cytometry; fluorescence in situ hybridization; human; human tissue; immunohistochemistry; immunophenotyping; lymph node biopsy; lymphocytoma; major clinical study; male; mantle cell lymphoma; priority journal; prognosis; receiver operating characteristic; sensitivity and specificity; Aged; Algorithms; Antigens, CD20; Artificial Intelligence; Female; Flow Cytometry; Humans; Immunophenotyping; Leukemia, Lymphocytic, Chronic, B-Cell; Lymphoma, Mantle-Cell; Male; Pattern Recognition, Automated; Receptors, IgE; Reproducibility of Results",Article,Scopus,2-s2.0-84855257738
"den Herder M., Kolström M., Lindner M., Suominen T., Tuomasjukka D., Pekkanen M.","Sustainability impact assessment on the production and use of different wood and fossil fuels employed for energy production in North Karelia, Finland",2012,"Energies",17,10.3390/en5114870,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870739473&doi=10.3390%2fen5114870&partnerID=40&md5=02a7b477dfcc1c8b240cc02dd1f7a8b9","The utilization rate of woody biomass in eastern Finland is high and expected to increase further in the near future as set out in several regional, national and European policies and strategies. The aim of this study was to assess the sustainability impacts of changes in fuel consumption patterns. We investigated fossil and woody biomass-based energy production chains in the region of North Karelia, focusing on some economic, environmental and social indicators. Indicators were selected based on stakeholder preferences and evaluated using the Tool for Sustainability Impact Assessment (ToSIA). The analysis was based on representative values from National Forest Inventory data, scientific publications, national and regional statistics, databases, published policy targets and expert opinion. From the results it became evident that shifting from fossil to wood-based energy production implies some trade-offs. Replacing oil with woody biomass in energy production would increase the local value added remaining in the region, create employment opportunities and would reduce total GHG emissions. However, firewood, wood chips from small-diameter trees from early thinning and wood pellets have high production costs. Moreover, large greenhouse gas emission resulted from wood pellet production. The case study generated valuable reference data for future sustainability assessments and demonstrated the usefulness of ToSIA as a tool presenting existing knowledge on sustainability impacts of alternative energy supply chains to inform decision making. © 2012 by the authors.","Decision support systems; Economic and environmental indicators; Energy production; Heavy fuel oil; Light heating oil; Regional decision making; Social; Sustainability indicators; Woody biomass","Artificial intelligence; Biomass; Decision making; Decision support systems; Economic and social effects; Forestry; Fossil fuels; Fuels; Greenhouse gases; Heavy oil production; Pelletizing; Residual fuels; Supply chains; Wood; Wood fuels; Wood products; Energy productions; Environmental indicators; Light heating; Social; Sustainability indicators; Woody biomass; Sustainable development; Artificial Intelligence; Decision Making; Economic Analysis; Energy Production; Forestry; Fossil Fuels; Greenhouse Gases; Indicators; Pelleting",Article,Scopus,2-s2.0-84870739473
"Farivar F., Shoorehdeli M.A.","Fault tolerant synchronization of chaotic heavy symmetric gyroscope systems versus external disturbances via Lyapunov rule-based fuzzy control",2012,"ISA Transactions",17,10.1016/j.isatra.2011.07.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-83555174846&doi=10.1016%2fj.isatra.2011.07.002&partnerID=40&md5=b63116432a40ae128e79fa17f6a7a5a2","In this paper, fault tolerant synchronization of chaotic gyroscope systems versus external disturbances via Lyapunov rule-based fuzzy control is investigated. Taking the general nature of faults in the slave system into account, a new synchronization scheme, namely, fault tolerant synchronization, is proposed, by which the synchronization can be achieved no matter whether the faults and disturbances occur or not. By making use of a slave observer and a Lyapunov rule-based fuzzy control, fault tolerant synchronization can be achieved. Two techniques are considered as control methods: classic Lyapunov-based control and Lyapunov rule-based fuzzy control. On the basis of Lyapunov stability theory and fuzzy rules, the nonlinear controller and some generic sufficient conditions for global asymptotic synchronization are obtained. The fuzzy rules are directly constructed subject to a common Lyapunov function such that the error dynamics of two identical chaotic motions of symmetric gyros satisfy stability in the Lyapunov sense. Two proposed methods are compared. The Lyapunov rule-based fuzzy control can compensate for the actuator faults and disturbances occurring in the slave system. Numerical simulation results demonstrate the validity and feasibility of the proposed method for fault tolerant synchronization. Copyright © 2011 Published by Elsevier Ltd on behalf of ISA. All rights reserved.","Chaotic gyroscope; Fault tolerant; Lyapunov rule-based fuzzy control; Nonlinear control; Synchronization","Actuator fault; Asymptotic synchronization; Chaotic motions; Common Lyapunov functions; Control methods; Error dynamics; External disturbances; Fault tolerant; General nature; Lyapunov; Lyapunov sense; Lyapunov stability theory; Lyapunov-based control; Non linear control; Non-linear controllers; Rule based; Slave systems; Sufficient conditions; Synchronization scheme; Chaotic systems; Computer simulation; Fuzzy control; Fuzzy rules; Gyroscopes; Lyapunov functions; Numerical methods; Synchronization; algorithm; article; artificial intelligence; computer simulation; fuzzy logic; industry; instrumentation; mechanics; nonlinear system; reproducibility; Algorithms; Artificial Intelligence; Computer Simulation; Fuzzy Logic; Industry; Mechanics; Nonlinear Dynamics; Reproducibility of Results",Article,Scopus,2-s2.0-83555174846
"Eslami S., de Keizer N.F., Dongelmans D.A., de Jonge E., Schultz M.J., Abu-Hanna A.","Effects of two different levels of computerized decision support on blood glucose regulation in critically ill patients",2012,"International Journal of Medical Informatics",17,10.1016/j.ijmedinf.2011.10.004,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-83555176336&doi=10.1016%2fj.ijmedinf.2011.10.004&partnerID=40&md5=f8806daea24432373d0e7ac506285e09","Introduction: Although the use of computerized decision support systems (CDSS) in glucose control in the ICU has been reported, little is known about the effect of the systems' operating modes on the quality of glucose control. The objective of this study was to evaluate the effect of providing patient-specific and patient non-specific computerized advice on timing of blood glucose level (BGL) measurements. Our hypothesis was that both levels of support would be effective for improving the quality of glucose regulation and safety, with patient specific advice being the most effective strategy. Patients and methods: A prospective study was performed in a 30-bed mixed medical-surgical intensive care unit (ICU) of a university hospital. In phase 1 the CDSS provided non-specific advice and thereafter, in phase 2, the system provided specific advice on timing of BGL measurements. The primary outcome measure was delay in BGL measurements before and after the two levels of support. Secondary endpoints were sampling frequency, mean BGL, BGL within pre-defined targets, time to capture target, incidences of severe hypoglycemia and hyperglycemia. These indicators were analyzed over the course of time using Statistical Control Charts. The analysis was restricted to patients with at least two blood glucose measurements. Results: Data of 3934 patient admissions were evaluated, which corresponded to 119,116 BGL measurements. The BGL sampling interval, delays in BG sampling, and percentage of hypoglycemia all decreased after introducing either of the two levels of decision support. The effect was however larger for the patient specific CDSS. Mean BGL, time to capture target, hyperglycemia index, percentage of hyperglycemia events and ""in range"" measurements remained unchanged and stable after introducing both patient non-specific and patient specific decision support. Conclusion: Adherence to protocol sampling rules increased by using decision support with a larger effect at the patient specific level. This led to a decrease in the percentage of hypoglycemia events and improved safety. The use of the CDSS at both levels, however, did not improve the quality of glucose control as measured by our indicators. More research is needed to investigate whether other socio-technical factors are in play. © 2011 Elsevier Ireland Ltd.","Computerized decision support system; Evaluation; Glucose regulation; Safety statistical process control","Blood glucose; Blood glucose level; Blood glucose measurements; Computerized decision; Critically-ill patients; Decision supports; Evaluation; Glucose control; Glucose regulation; In-phase; Operating modes; Patient specific; Prospective study; Sampling frequencies; Sampling interval; Sociotechnical; Statistical control; Time-to-capture; Artificial intelligence; Blood; Decision support systems; Glucose; Glucose sensors; Insulin; Intensive care units; Statistical process control; Quality control; glucose; adult; aged; article; blood glucose monitoring; blood sampling; clinical effectiveness; controlled clinical trial; controlled study; critical illness; critically ill patient; decision support system; female; glucose blood level; health care quality; human; hyperglycemia; hypoglycemia; intensive care unit; major clinical study; male; outcome assessment; patient safety; priority journal; Aged; APACHE; Blood Glucose; Critical Illness; Decision Support Systems, Clinical; Female; Humans; Male; Middle Aged; Prospective Studies",Article,Scopus,2-s2.0-83555176336
"Torreño A., Onaindia E., Sapena Ó.","An approach to multi-agent planning with incomplete information",2012,"Frontiers in Artificial Intelligence and Applications",17,10.3233/978-1-61499-098-7-762,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878811894&doi=10.3233%2f978-1-61499-098-7-762&partnerID=40&md5=de2d6c224e26e02b69bb1d7e703bf69b","Multi-agent planning (MAP) approaches have been typically conceived for independent or loosely-coupled problems to enhance the benefits of distributed planning between autonomous agents as solving this type of problems require less coordination between the agents' sub-plans. However, when it comes to tightly-coupled agents' tasks, MAP has been relegated in favour of centralized approaches and little work has been done in this direction. In this paper, we present a general-purpose MAP capable to efficiently handle planning problems with any level of coupling between agents. We propose a cooperative refinement planning approach, built upon the partial-order planning paradigm, that allows agents to work with incomplete information and to have incomplete views of the world, i.e. being ignorant of other agents' information, as well as maintaining their own private information. We show various experiments to compare the performance of our system with a distributed CSP-based MAP approach over a suite of problems. © 2012 The Author(s).",,"Artificial intelligence; Intelligent agents; Multi agent systems; Centralized approaches; Distributed planning; Incomplete information; Loosely coupled; Multi-agent planning; Partial order planning; Planning problem; Private information; Autonomous agents",Conference Paper,Scopus,2-s2.0-84878811894
"Akay A.E., Wing M.G., Sivrikaya F., Sakar D.","A GIS-based decision support system for determining the shortest and safest route to forest fires: A case study in Mediterranean Region of Turkey",2012,"Environmental Monitoring and Assessment",17,10.1007/s10661-011-2049-z,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027958815&doi=10.1007%2fs10661-011-2049-z&partnerID=40&md5=9f628eefc29b4705217b42196e8792e7","The ability of firefighting vehicles and staff to reach a fire area as quickly as possible is critical in fighting against forest fires. In this study, a Geographical Information System-based decision support system was developed to assist fire managers in determining the fastest and the safest or more reliable access routes from firefighting headquarters to fire areas. The decision support system was tested in the Kahramanmaras Forestry Regional Directoratein the Mediterranean region of Turkey. The study area consisted of forested lands which had been classified according to fire sensitivity. The fire response routing simulations considered firefighting teams located in 20 firefighting headquarter locations. The road network, the locations of the firefighting headquarters, and possible fire locations were mapped for simulation analysis. In alternative application simulations, inaccessible roads which might be closed due to fire or other reasons were indicated in the network analysis so that the optimum route was not only the fastest but also the safest and most reliable path. The selection of which firefighting headquarters to use was evaluated by considering critical response time to potential fire areas based on fire sensitivity levels. Results indicated that new firefighting headquarters should be established in the region in order to provide sufficient firefighting response to all forested lands. In addition, building new fire access roads and increasing the design speed on current roads could also increase firefighting response capabilities within the study area. © 2011 Springer Science+Business Media B.V.","Firefighting teams; Forest fires; GIS; Network analysis; Safest route; Shortest path","Artificial intelligence; Decision support systems; Deforestation; Electric network analysis; Fire extinguishers; Fire fighting equipment; Fire hazards; Forestry; Geographic information systems; Location; Transportation; Application simulation; Firefighting teams; Firefighting vehicles; Forest fires; GIS based decision support systems; Mediterranean region; Safest routes; Shortest path; Fires; accessibility; decision support system; forest fire; GIS; hazard assessment; network analysis; reliability analysis; safety; article; controlled study; decision support system; fire; fire protection; forest fire; forest structure; geographic information system; simulation; traffic; travel; Turkey (republic); Artificial Intelligence; Decision Making; Deforestation; Fire Fighting; Forest Fires; GIS; Safety Equipment; Turkey",Article,Scopus,2-s2.0-85027958815
"Nasir A.N.K., Tokhi M.O., Abd Ghani N.M., Ahmad M.A.","A novel hybrid spiral-dynamics bacterial-foraging algorithm for global optimization with application to control design",2012,"2012 12th UK Workshop on Computational Intelligence, UKCI 2012",16,10.1109/UKCI.2012.6335764,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870322287&doi=10.1109%2fUKCI.2012.6335764&partnerID=40&md5=02ea7a16c8075aa8a5ebe984f726d279","This paper presents a hybrid optimization algorithm referred to as Hybrid spiral dynamics bacterial foraging (HSDBF). The algorithm synergizes spiral adaptive simplified bacterial foraging algorithm (BFA) and spiral dynamics inspired optimization algorithm (SDA). The standard BFA has better exploitation strategy while SDA has superior exploration approach and stable convergence when approaching the optimum value. The hybrid algorithm preserves the strengths of BFA and SDA, thus producing better results. Moreover, it has simple structure and involves less computational burden. Several unimodal and multimodal benchmark functions are employed to test the algorithm in determining the global optimum point. Furthermore, the proposed method is applied to a proportional-derivative (PD) controller optimization for a flexible manipulator system (FMS). The results show that HSDBF outperforms BFA in all test functions and successfully optimizes the PD controller. © 2012 IEEE.","bacterial foraging; flexible manipulator; optimization algorithm; PD control; Spiral dynamics","Bacterial foraging; Bacterial foraging algorithm; Computational burden; Control design; Global optimum; Hybrid algorithms; Hybrid optimization algorithm; Multimodal benchmark; Optimization algorithms; Optimum value; PD control; PD controllers; Proportional-derivative controllers; Simple structures; Spiral dynamics; Stable convergence; Test functions; Unimodal; Artificial intelligence; Dynamics; Flexible manipulators; Global optimization; Algorithms",Conference Paper,Scopus,2-s2.0-84870322287
"Lieder F., Griffiths T.L., Goodman N.D.","Burn-in, bias, and the rationality of anchoring",2012,"Advances in Neural Information Processing Systems",16,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877775445&partnerID=40&md5=8e24de1d43f09fd4ffa5ed661780928a","Bayesian inference provides a unifying framework for addressing problems in machine learning, artificial intelligence, and robotics, as well as the problems facing the human mind. Unfortunately, exact Bayesian inference is intractable in all but the simplest models. Therefore minds and machines have to approximate Bayesian inference. Approximate inference algorithms can achieve a wide range of time-accuracy tradeoffs, but what is the optimal tradeoff? We investigate timeaccuracy tradeoffs using the Metropolis-Hastings algorithm as a metaphor for the mind's inference algorithm(s). We find that reasonably accurate decisions are possible long before the Markov chain has converged to the posterior distribution, i.e. during the period known as ""burn-in"". Therefore the strategy that is optimal subject to the mind's bounded processing speed and opportunity costs may perform so few iterations that the resulting samples are biased towards the initial value. The resulting cognitive process model provides a rational basis for the anchoringand- adjustment heuristic. The model's quantitative predictions are tested against published data on anchoring in numerical estimation tasks.",,"Approximate Bayesian inference; Approximate inference; Cognitive process models; Inference algorithm; Metropolis-Hastings algorithm; Numerical estimation; Posterior distributions; Quantitative prediction; Algorithms; Artificial intelligence; Bayesian networks; Markov processes; Optimization; Inference engines",Conference Paper,Scopus,2-s2.0-84877775445
"Caron S., Kveton B., Lelarge M., Bhagat S.","Leveraging side observations in stochastic bandits",2012,"Uncertainty in Artificial Intelligence - Proceedings of the 28th Conference, UAI 2012",16,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886053230&partnerID=40&md5=581fd511e3314fff8c3aa4878f5e9adb","This paper considers stochastic bandits with side observations, a model that accounts for both the exploration/exploitation dilemma and relationships between arms. In this setting, after pulling an arm i, the decision maker also observes the rewards for some other actions related to i. We will see that this model is suited to content recommendation in social networks, where users' reactions may be endorsed or not by their friends. We provide efficient algorithms based on upper confidence bounds (UCBs) to leverage this additional information and derive new bounds improving on standard regret guarantees. We also evaluate these policies in the context of movie recommendation in social networks: experiments on real datasets show substantial learning rate speedups ranging from 2.2× to 14× on dense networks.",,"Content recommendations; Decision makers; Dense network; Exploration/exploitation dilemmas; Learning rates; Movie recommendations; Real data sets; Upper confidence bound; Algorithms; Artificial intelligence; Data processing; Social networking (online); Stochastic systems; Stochastic models",Conference Paper,Scopus,2-s2.0-84886053230
"Muller J., Hunter A.","An argumentation-based approach for decision making",2012,"Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI",16,10.1109/ICTAI.2012.82,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876848776&doi=10.1109%2fICTAI.2012.82&partnerID=40&md5=72e9bf24def4b0d9ca3852b5212a381c","The formalisation of design decisions serves two purposes: To support the decision maker in choosing which decision to take (decision analysis), and to document the reasons behind decisions for future reference (decision documentation). Approaches which solve the latter task involve a semi-formal pattern of documenting the reasons for and against each of the options, but they generally do not allow an automation of the decision making process. Approaches which solve the former task use a mathematical model of the problem, in which each option is evaluated numerically with respect to some relevant criteria, but they do not support documentation. We investigate the use of argumentation to both analyse and document decisions, solving both tasks with the same method. Additionally, the system we present is able to generate decisions for analysis, instead of relying on a predefined input of options. We collaborated with an aerospace manufacturer to identify common problems in the industry and to create realistic examples from the engineering domain. We show that our system subsumes a certain class of multi criteria decision making problems and that it improves upon previous argumentation-based decision making systems by adding the capability to generate decisions and by clearly defining the semantics used to choose accepted arguments. © 2012 IEEE.","argumentation; decision analysis; decision documentation; decision making; generating decisions","argumentation; Argumentation-based decision making; Decision making process; Design decisions; Engineering domains; generating decisions; Multi-criteria decision making problems; Support documentation; Artificial intelligence; Decision theory; Mathematical models; Semantics; Decision making",Conference Paper,Scopus,2-s2.0-84876848776
"Portela F., Pinto F., Santos M.F.","Data mining predictive models for pervasive intelligent decision support in intensive care medicine",2012,"KMIS 2012 - Proceedings of the International Conference on Knowledge Management and Information Sharing",16,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881564566&partnerID=40&md5=c2dff8e4c8b955f9060705cf4c7c784b","The introduction of an Intelligent Decision Support System (IDSS) in a critical area like the Intensive Medicine is a complex and difficult process. In this area, their professionals don't have much time to document the cases, because the patient direct care is always first. With the objective to reduce significantly the manual records and, enabling, at the same time, the possibility of developing an IDSS which can help in the decision making process, all data acquisition process and knowledge discovery in database phases were automated. From the data acquisition to the knowledge discovering, the entire process is autonomous and executed in real-time. On-line induced data mining models were used to predict organ failure and outcome. Preliminary results obtained with a limited population of patients showed that this approach can be applied successfully.","Data mining; Intelligent decision support system; Intensive care; KDD; Pervasive; Real-time","Intelligent decision support systems; Intensive care; KDD; Pervasive; Real-time; Artificial intelligence; Automobile drivers; Data acquisition; Decision support systems; Knowledge management; Data mining",Conference Paper,Scopus,2-s2.0-84881564566
"Huang W., Dietl W., Milanova A., Ernst M.D.","Inference and checking of object ownership",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,10.1007/978-3-642-31057-7-9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879705992&doi=10.1007%2f978-3-642-31057-7-9&partnerID=40&md5=9baaf99e25de7c3f96f52d43551adafe","Ownership type systems describe a heap topology and enforce an encapsulation discipline; they aid in various program correctness and understanding tasks. However, the annotation overhead of ownership type systems has hindered their widespread use. We present a unified framework for specification, type inference and type checking of ownership type systems, and instantiate the framework for two such systems: Universe Types and Ownership Types. We present an objective metric defining a ""best typing"" for these type systems, and develop an inference approach that maximizes the metric. The programmer can influence the inference by adding partial annotations to the program. We implemented the approach on top of the Checker Framework and present the results of an experimental evaluation. © 2012 Springer-Verlag Berlin Heidelberg.",,"Experimental evaluation; Ownership type; Program correctness; Type inferences; Type systems; Typechecking; Unified framework; Universe types; Artificial intelligence; Computer science; Object oriented programming",Conference Paper,Scopus,2-s2.0-84879705992
"Koryakin D., Lohmann J., Butz M.V.","Balanced echo state networks",2012,"Neural Networks",16,10.1016/j.neunet.2012.08.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870271229&doi=10.1016%2fj.neunet.2012.08.008&partnerID=40&md5=68858b646dc74447b13056214bb5c377","This paper investigates the interaction between the driving output feedback and the internal reservoir dynamics in echo state networks (ESNs). The interplay is studied experimentally on the multiple superimposed oscillators (MSOs) benchmark. The experimental data reveals a dual effect of the output feedback strength on the network dynamics: it drives the dynamic reservoir but it can also block suitable reservoir dynamics. Moreover, the data shows that the reservoir size crucially co-determines the likelihood of generating an effective ESN. We show that dependent on the complexity of the MSO dynamics somewhat smaller networks can yield better performance. Optimizing the output feedback weight range and the network size is thus crucial for generating an effective ESN. With proper parameter choices, we show that it is possible to generate ESNs that approximate MSOs with several orders of magnitude smaller errors than those previously reported. We conclude that there appears to be still much more potential in ESNs than previously thought and sketch-out some promising future research directions. © 2012 Elsevier Ltd.","Dynamic reservoir; Echo state network; Multiple superimposed oscillator; Output feedback; Recurrent neural networks","Dual effect; Echo state networks; Future research directions; Network dynamics; Network size; Orders of magnitude; Output feedback; Parameter choice; Reservoir dynamics; Weight range; Digital storage; Dynamics; Recurrent neural networks; Feedback; article; artificial neural network; brain function; dynamics; echo state network; feedback system; information processing; measurement error; memory; oscillator; priority journal; process optimization; Algorithms; Artificial Intelligence; Feedback; Neural Networks (Computer); Nonlinear Dynamics",Article,Scopus,2-s2.0-84870271229
"Lee E.K., Yuan F., Hirsh D.A., Mallory M.D., Simon H.K.","A clinical decision tool for predicting patient care characteristics: patients returning within 72 hours in the emergency department.",2012,"AMIA ... Annual Symposium proceedings / AMIA Symposium. AMIA Symposium",16,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880840247&partnerID=40&md5=cbc5dd61d3c293c30e6f5a0f976c0b7b","The primary purpose of this study was to develop a clinical tool capable of identifying discriminatory characteristics that can predict patients who will return within 72 hours to the Pediatric emergency department (PED). We studied 66,861 patients who were discharged from the EDs during the period from May 1 2009 to December 31 2009. We used a classification model to predict return visits based on factors extracted from patient demographic information, chief complaint, diagnosis, treatment, and hospital real-time ED statistics census. We began with a large pool of potentially important factors, and used particle swarm optimization techniques for feature selection coupled with an optimization-based discriminant analysis model (DAMIP) to identify a classification rule with relatively small subsets of discriminatory factors that can be used to predict - with 80% accuracy or greater - return within 72 hours. The analysis involves using a subset of the patient cohort for training and establishment of the predictive rule, and blind predicting the return of the remaining patients. Good candidate factors for revisit prediction are obtained where the accuracy of cross validation and blind prediction are over 80%. Among the predictive rules, the most frequent discriminatory factors identified include diagnosis (> 97%), patient complaint (>97%), and provider type (> 57%). There are significant differences in the readmission characteristics among different acuity levels. For Level 1 patients, critical readmission factors include patient complaint (>57%), time when the patient arrived until he/she got an ED bed (> 64%), and type/number of providers (>50%). For Level 4/5 patients, physician diagnosis (100%), patient complaint (99%), disposition type when patient arrives and leaves the ED (>30%), and if patient has lab test (>33%) appear to be significant. The model was demonstrated to be consistent and predictive across multiple PED sites.The resulting tool could enable ED staff and administrators to use patient specific values for each of a small number of discriminatory factors, and in return receive a prediction as to whether the patient will return to the ED within 72 hours. Our prediction accuracy can be as high as over 85%. This provides an opportunity for improving care and offering additional care or guidance to reduce ED readmission.",,"algorithm; article; artificial intelligence; decision support system; emergency health service; hospital readmission; human; mathematical phenomena; organization and management; pediatrics; risk factor; Algorithms; Artificial Intelligence; Decision Support Techniques; Emergency Service, Hospital; Humans; Mathematical Concepts; Patient Readmission; Pediatrics; Risk Factors; Triage",Article,Scopus,2-s2.0-84880840247
"Liu J., Wang J., Zheng Q., Zhang W., Jiang L.","Topological analysis of knowledge maps",2012,"Knowledge-Based Systems",16,10.1016/j.knosys.2012.07.011,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867875695&doi=10.1016%2fj.knosys.2012.07.011&partnerID=40&md5=0e9cc66809abd22dc54b9578a03f41b9","A knowledge map can be viewed as a directed graph, in which each node is a knowledge unit (KU), and each edge is a learning-dependency between two KUs. Understanding the topological properties of knowledge map can help us gain better insights into human cognition structure and its mechanism, design better knowledge map construction algorithms, and guide learners' navigational learning through knowledge map. In this paper, we perform topological analysis on 12 knowledge maps from computer science, mathematics, and physics. We discover that they exhibit small-world and scale-free properties like many other networks. Specifically, we show the locality of learning-dependency and hierarchical modular structure in the 12 knowledge maps. In addition, we study how KUs affect the network efficiency by removing KUs based on different centrality measures. We find that the importance of KUs varies greatly. © 2012 Elsevier B.V. All rights reserved.","Complex network; Knowledge map; Learning-dependency; Topological analysis; Topological property","Complex networks; Knowledge map; Learning-dependency; Topological analysis; Topological properties; Artificial intelligence; Software engineering; Topology",Article,Scopus,2-s2.0-84867875695
"Liu G., Wu J., Zhou Z.-H.","Key instance detection in multi-instance learning",2012,"Journal of Machine Learning Research",16,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876818045&partnerID=40&md5=e3b0400b280143f835f176403e32d15a","The goal of traditional multi-instance learning (MIL) is to predict the labels of the bags, whereas in many real applications, it is desirable to get the instance labels, especially the labels of key instances that trigger the bag labels, in addition to getting bag labels. Such a problem has been largely unexplored before. In this paper, we formulate the Key Instance Detection (KID) problem, and propose a voting framework (VF) solution to KID. The key of VF is to exploit the relationship among instances, represented by a citer kNN graph. This graph is different from commonly used nearest neighbor graphs, but is suitable for KID. Experiments validate the effectiveness of VF for KID. Additionally, VF also outperforms state-of-the-art MIL approaches on the performance of bag label prediction. © 2012 G. Liu, J. Wu & Z.-H. Zhou.","Iterative rejection; Key instance detection; Multi-instance learning; Neighborhood relation; Voting framework","Iterative rejection; k-NN graphs; Label predictions; Multi-instance learning; Nearest neighbors; Neighborhood relation; Real applications; Voting framework; Artificial intelligence; Software engineering; Learning systems",Conference Paper,Scopus,2-s2.0-84876818045
"Bouma H., Hanckmann P., Marck J.-W., Penning L., Den Hollander R., Ten Hove J.-M., Van Den Broek S., Schutte K., Burghouts G.","Automatic human action recognition in a scene from visual inputs",2012,"Proceedings of SPIE - The International Society for Optical Engineering",16,10.1117/12.918582,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862526661&doi=10.1117%2f12.918582&partnerID=40&md5=9504a54c1aee58afcc21e6ae8e2ef7bc","Surveillance is normally performed by humans, since it requires visual intelligence. However, this can be dull and dangerous, especially for military operations. Therefore, unmanned autonomous visual-intelligence systems are desired. In this paper, we present a novel system that can recognize human actions, which are relevant to detect operationally significant activity. Central to the system is a break-down of high-level perceptual concepts (verbs) in simpler observable events. The system is trained on 3482 videos and evaluated on 2589 videos from the DARPA Mind's Eye program, with for each video human annotations indicating the presence or absence of 48 different actions. The results show that our system reaches good performance approaching the human average response. © 2012 SPIE.","action recognition; artificial intelligence; computer vision; retrieval; Visual intelligence","Action recognition; Human actions; Human annotations; Human-action recognition; Perceptual concepts; retrieval; Visual intelligence; Artificial intelligence; Computer vision; Gesture recognition; Image recognition; Military operations; Sensors; Motion estimation",Conference Paper,Scopus,2-s2.0-84862526661
"Cohen D.A., Cooper M.C., Creed P., Marx D., Salamon A.Z.","The tractability of CSP classes defined by forbidden patterns",2012,"Journal of Artificial Intelligence Research",16,10.1613/jair.3651,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875138324&doi=10.1613%2fjair.3651&partnerID=40&md5=052a070c017242a1486649c42c4d5754","The constraint satisfaction problem (CSP) is a general problem central to computer science and artificial intelligence. Although the CSP is NP-hard in general, considerable effort has been spent on identifying tractable subclasses. The main two approaches consider structural properties (restrictions on the hypergraph of constraint scopes) and relational properties (restrictions on the language of constraint relations). Recently, some authors have considered hybrid properties that restrict the constraint hypergraph and the relations simultaneously. Our key contribution is the novel concept of a CSP pattern and classes of problems defined by forbidden patterns (which can be viewed as forbidding generic sub-problems). We describe the theoretical framework which can be used to reason about classes of problems defined by forbidden patterns. We show that this framework generalises certain known hybrid tractable classes. Although we are not close to obtaining a complete characterisation concerning the tractability of general forbidden patterns, we prove a dichotomy in a special case: classes of problems that arise when we can only forbid binary negative patterns (generic subproblems in which only disallowed tuples are specified). In this case we show that all (finite sets of) forbidden patterns define either polynomial-time solvable or NP-complete classes of instances. © 2012 AI Access Foundation.",,"Constraint relations; Forbidden pattern; Novel concept; Polynomial-time; Relational properties; Theoretical framework; Tractable class; Two Approaches; Artificial intelligence; Computational complexity; Constraint satisfaction problems",Article,Scopus,2-s2.0-84875138324
"Hayes T., Palomar O., Unsal O., Cristal A., Valero M.","Vector extensions for decision support DBMS acceleration",2012,"Proceedings - 2012 IEEE/ACM 45th International Symposium on Microarchitecture, MICRO 2012",16,10.1109/MICRO.2012.24,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876515848&doi=10.1109%2fMICRO.2012.24&partnerID=40&md5=44431d8b0230cd2eafbc2241632dabd7","Database management systems (DBMS) have become an essential tool for industry and research and are often a significant component of data centres. As a result of this criticality, efficient execution of DBMS engines has become an important area of investigation. This work takes a top-down approach to accelerating decision support systems (DSS) on x86-64 microprocessors using vector ISA extensions. In the first step, a leading DSS DBMS is analysed for potential data-level parallelism. We discuss why the existing multimedia SIMD extensions (SSE/AVX) are not suitable for capturing this parallelism and propose a complementary instruction set reminiscent of classical vector architectures. The instruction set is implemented using unintrusive modifications to a modern x86-64 micro architecture tailored for DSS DBMS. The ISA and micro architecture are evaluated using a cycle-accurate x86-64 micro architectural simulator coupled with a highly-detailed memory simulator. We have found a single operator is responsible for 41% of total execution time for the TPC-H DSS benchmark. Our results show performance speedups between 1.94x and 4.56x for an implementation of this operator run with our proposed hardware modifications. © 2012 IEEE.","database; dbms; decision; dlp; microarchitecture; parallelism; simd; support; vector","decision; dlp; Micro architectures; parallelism; simd; Artificial intelligence; Computer architecture; Database systems; Decision support systems; Supports; Vectors",Conference Paper,Scopus,2-s2.0-84876515848
"Groce A., Katz J., Thiruvengadam A., Zikas V.","Byzantine agreement with a rational adversary",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,10.1007/978-3-642-31585-5-50,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884200633&doi=10.1007%2f978-3-642-31585-5-50&partnerID=40&md5=67858cd18c2b72101f84a5380cbe0aaa","Traditionally, cryptographers assume a worst-case adversary who can act arbitrarily. More recently, they have begun to consider rational adversaries who can be expected to act in a utility-maximizing way. Here we apply this model for the first time to the problem of Byzantine agreement (BA) and the closely related problem of broadcast, for natural classes of utilities. Surprisingly, we show that many known results (e.g., equivalence of these problems, or the impossibility of tolerating t∈≥∈n/2 corruptions) do not hold in the rational model. We study the feasibility of information-theoretic (both perfect and statistical) BA assuming complete or partial knowledge of the adversary's preferences. We show that perfectly secure BA is possible for t∈<∈n corruptions given complete knowledge of the adversary's preferences, and characterize when statistical security is possible with only partial knowledge. Our protocols have the added advantage of being more efficient than BA protocols secure in the traditional adversarial model. © 2012 Springer-Verlag Berlin Heidelberg.",,"Byzantine Agreement; Partial knowledge; Rational models; Statistical securities; Artificial intelligence; Computer science; Automata theory",Conference Paper,Scopus,2-s2.0-84884200633
"Gandhewar N., Patel R.","Detection and prevention of sinkhole attack on AODV protocol in mobile adhoc network",2012,"Proceedings - 4th International Conference on Computational Intelligence and Communication Networks, CICN 2012",16,10.1109/CICN.2012.96,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872039906&doi=10.1109%2fCICN.2012.96&partnerID=40&md5=bb74963df1a66041e10be6d832e358a9","MANET is formed by a collection of mobile nodes, with no preset infrastructure where each node plays a role of router, It is getting fame day by day due to wide use of mobile and handheld devices. Dynamic nature of this network, makes routing protocols to play a prominent role in setting up efficient route among pair of nodes. Therefore many proactive, reactive &amp; hybrid routing protocols have been proposed, among which one of well known is AODV due to its high performance gain. Cooperative nature of nodes exposes MANET to various kinds of passive &amp; active attacks. Sinkhole is one of severe kind of attack which attempts to attract most of network traffic towards it &amp; degrade the performance of network. AODV is mainly analyzed under blakhole, wormhole &amp; flooding attack, which needs to analyze under other kinds of attack also. This paper mainly focuses on sinkhole problem, its consequences &amp; presents mechanism for detection &amp; prevention of it on the context of AODV protocol. It also shows performance of AODV with no sinkhole attack, under attack &amp; after applying our mechanism in the form of simulation result obtained for certain variation of nodes in network, by considering performance metrics as throughput, PDR, End to end delay &amp; Packet loss. Simulation is carried out using widely used simulator NS2. © 2012 IEEE.","Blackhole; End to end delay; Hybrid; MANET; PDR; Proactive; Reactive; Throughput; Wormhole","Black holes; End to end delay; Hybrid; MANET; PDR; Proactive; Reactive; Wormhole; Artificial intelligence; Routers; Routing protocols; Telecommunication networks; Throughput; Mobile ad hoc networks",Conference Paper,Scopus,2-s2.0-84872039906
"Rustad M., Ghosh K.","Why and how does native topology dictate the folding speed of a protein?",2012,"Journal of Chemical Physics",16,10.1063/1.4767567,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870535360&doi=10.1063%2f1.4767567&partnerID=40&md5=035fc260c15fde4fb5a62805c0b0f52c","Since the pioneering work of Plaxco, Simons, and Baker, it is now well known that the rates of protein folding strongly correlate with the average sequence separation (absolute contact order (ACO)) of native contacts. In spite of multitude of papers, our understanding to the basis of the relation between folding speed and ACO is still lacking. We model the transition state as a Gaussian polymer chain decorated with weak springs between native contacts while the unfolded state is modeled as a Gaussian chain only. Using these hamiltonians, our perturbative calculation explicitly shows folding speed and ACO are linearly related when only the first order term in the series is considered. However, to the second order, we notice the existence of two new topological metrics, termed COC1 and COC2 (COC stands for contact order correction). These additional correction terms are needed to properly account for the entropy loss due to overlapping (nested or linked) loops that are not well described by simple addition of entropies in ACO. COC1 and COC2 are related to fluctuations and correlations among different sequence separations. The new metric combining ACO, COC 1, and COC2 improves folding speed dependence on native topology when applied to three different databases: (i) two-state proteins with only αβ and β proteins, (ii) two-state proteins (αβ, β and purely helical proteins all combined), and (iii) master set (multi-state and two-state) folding proteins. Furthermore, the first principle calculation provides us direct physical insights to the meaning of the fit parameters. The coefficient of ACO, for example, is related to the average strength of the contacts, while the constant term is related to the protein folding speed limit. With the new scaling law, our estimate of the folding speed limit is in close agreement with the widely accepted value of 1 μs observed in proteins and RNA. Analyzing an exhaustive set (7367) of monomeric proteins from protein data bank, we find our new topology based metric (combining ACO, COC1, and COC2) scales as N0.54, N being the number of amino acids in a protein. This is in remarkable agreement with a previous argument based on random systems that predict protein folding speed depends on exp (- N0.5). The first principle calculation presented here provides deeper insights to the role of topology in protein folding and unifies many parallel arguments, seemingly disconnected, demonstrating the existence of universal mechanism in protein folding kinetics that can be understood from simple polymer physics based principles. © 2012 American Institute of Physics.",,"Contact orders; Correction terms; Entropy loss; First order; First principle calculations; Fit parameters; Gaussian chains; Gaussian polymers; Helical proteins; Metric combining; Monomeric proteins; Multi-state; Native topology; Perturbative calculations; Polymer physics; Protein data bank; Protein folding kinetics; Random systems; Second orders; Speed dependence; Speed limit; Topological metrics; Transition state; Two-state; Universal mechanisms; Amino acids; Artificial intelligence; Entropy; Protein folding; RNA; Separation; Speed; Topology; Proteins; protein; article; biological model; chemistry; protein folding; surface property; thermodynamics; time; Models, Biological; Protein Folding; Proteins; Surface Properties; Thermodynamics; Time Factors",Article,Scopus,2-s2.0-84870535360
"Persson A., Axelsson E., Svenningsson J.","Generic monadic constructs for embedded languages",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,10.1007/978-3-642-34407-7_6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864968718&doi=10.1007%2f978-3-642-34407-7_6&partnerID=40&md5=178d098b9c339ed9f94fe1fc2170d080","We present a library of generic monadic constructs for embedded languages. It is an extension of Syntactic, a Haskell library for defining and processing generic abstract syntax. Until now, Syntactic has been mostly suited to implement languages based on pure, side effect free, expressions. The presented extension allows the pure expressions to also contain controlled side effects, enabling the representation of expressions that rely on destructive updates for efficiency. We demonstrate the usefulness of the extension by giving examples from the embedded language Feldspar which is implemented using Syntactic. © 2012 Springer-Verlag.",,"Abstract syntax; Embedded Languages; Haskell; Side effect; Artificial intelligence; Syntactics",Conference Paper,Scopus,2-s2.0-84864968718
"Rogers P.H., Benkstein K.D., Semancik S.","Machine learning applied to chemical analysis: Sensing multiple biomarkers in simulated breath using a temperature-pulsed electronic-nose",2012,"Analytical Chemistry",16,10.1021/ac301687j,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869383110&doi=10.1021%2fac301687j&partnerID=40&md5=7a8afb9207a0a1c7dfc82980e7e2a9b5","Monitoring of chemical species in breath offers an approach for the detection of disease and other conditions that cause homeostatic imbalance. Here, we demonstrate the use of microsensor-based devices for detecting select biomarkers in simulated exhaled breath as a step toward enabling fast and inexpensive breath-screening technology. Microhotplate elements functionalized with three chemiresistive metal-oxide films (SnO2, In 2O3, and CuO) were used to acquire data in simulated breath containing single targets [(5 to 20) μmol/mol ammonia, methanol, and acetone], as well as mixtures of those species. All devices were operated with programmed thermal cycles featuring rapid temperature excursions, during which film resistances were measured. Material-specific temperature programs were optimized to achieve temperature-dependent metal-oxide sensing film conductance levels and target selectivity. A supervised hierarchical machine-learning algorithm using linear discriminant analysis for dimensional reduction of sensing data and discrimination was developed. This algorithm was employed in the classification and quantification of biomarkers. This approach to microsensor data collection and processing was successful in classifying and quantifying the model biomarkers in validation-set mixtures. © This article not subject to U.S. Copyright. Published 2012 by the American Chemical Society.",,"Chemical species; Data collection; Dimensional reduction; Exhaled breaths; Film resistance; Functionalized; Linear discriminant analysis; Metal oxide film; Metal-oxide; Micro hotplate; Quantifying the models; Rapid temperature; Sensing data; Sensing films; Temperature dependent; Thermal cycle; Acetone; Chemical detection; Diagnosis; Learning systems; Methanol; Microsensors; Oxide films; Learning algorithms; biological marker; metal; oxide; article; artificial intelligence; breath analysis; chemistry; discriminant analysis; electronic nose; methodology; microtechnology; temperature; Artificial Intelligence; Biological Markers; Breath Tests; Discriminant Analysis; Electronic Nose; Metals; Microtechnology; Oxides; Temperature",Article,Scopus,2-s2.0-84869383110
"Cho M., Mahalanobis A., Javidi B.","3D passive integral imaging using compressive sensing",2012,"Optics Express",16,10.1364/OE.20.026624,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870580101&doi=10.1364%2fOE.20.026624&partnerID=40&md5=3e32af68dd49323afc2f6ec65450297c","Passive 3D sensing using integral imaging techniques has been well studied in the literature. It has been shown that a scene can be reconstructed at various depths using several 2D elemental images. This provides the ability to reconstruct objects in the presence of occlusions, and passively estimate their 3D profile. However, high resolution 2D elemental images are required for high quality 3D reconstruction. Compressive Sensing (CS) provides a way to dramatically reduce the amount of data that needs to be collected to form the elemental images, which in turn can reduce the storage and bandwidth requirements. In this paper, we explore the effects of CS in acquisition of the elemental images, and ultimately on passive 3D scene reconstruction and object recognition. Our experiments show that the performance of passive 3D sensing systems remains robust even when elemental images are recovered from very few compressive measurements. © 2012 Optical Society of America.",,"3-D sensing; 3D profile; 3D reconstruction; 3D scene reconstruction; Bandwidth requirement; Compressive sensing; Elemental images; High quality; High resolution; Integral imaging; Image reconstruction; Imaging techniques; Object recognition; Sensors; Three dimensional computer graphics; Three dimensional; algorithm; article; artificial intelligence; automated pattern recognition; comparative study; human; image enhancement; image processing; methodology; reproducibility; three dimensional imaging; Algorithms; Artificial Intelligence; Humans; Image Enhancement; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Pattern Recognition, Automated; Reproducibility of Results",Conference Paper,Scopus,2-s2.0-84870580101
"Kangaspunta J., Liesiö J., Salo A.","Cost-efficiency analysis of weapon system portfolios",2012,"European Journal of Operational Research",16,10.1016/j.ejor.2012.05.042,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864422515&doi=10.1016%2fj.ejor.2012.05.042&partnerID=40&md5=c96a55a9ae00a1c48a606d893ecf22e4","Decisions about the acquisition and maintenance of military equipment serve to build long-term capabilities in preparation of military conflicts. Typically, these decisions involve large investments which need to be supported by adequate cost-efficiency analyses. Yet the cost-efficiency analysis of weapon systems involves several challenges: for example, it is necessary to account for the possible interactions among different weapon systems; the relevance of several impact criteria; and the variety of combat situations in which these systems may be used. In this paper, we develop a portfolio methodology where these challenges are addressed by evaluating the cost-efficiencies of entire portfolios consisting of individual weapon systems. Our methodology accounts for possible interactions among systems by synthesizing impact assessment results that are either generated by combat simulation models or elicited from experts. It also admits incomplete preference information about the relative importance of different impact criteria. This methodology guides decision making by identifying which combinations of weapon systems are efficient with respect to multiple evaluation criteria in different combat situations at different cost levels. It can also be extended to settings where multiple combat situations are addressed simultaneously. The methodology is generic and can therefore be applied also in civilian settings when portfolios of activities (such as mitigation of harmful environmental emissions) may exhibit interactions. © 2012 Elsevier B.V. All rights reserved.","Cost-efficiency analysis; Decision analysis; Decision support systems; Multiple criteria analysis; OR in military","Combat simulation; Cost-efficiency; Environmental emissions; Evaluation criteria; Impact assessments; Military conflicts; Multiple criteria analysis; OR in military; Preference information; Weapon system; Artificial intelligence; Computer simulation; Costs; Decision making; Decision support systems; Decision theory; Efficiency; Cost benefit analysis",Article,Scopus,2-s2.0-84864422515
"Lécué F., Schumann A., Sbodio M.L.","Applying semantic web technologies for diagnosing road traffic congestions",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,10.1007/978-3-642-35173-0-8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868548063&doi=10.1007%2f978-3-642-35173-0-8&partnerID=40&md5=e3dedc70bbf02b6db15b375de9b00cf6","Diagnosis, or the method to connect causes to its effects, is an important reasoning task for obtaining insight on cities and reaching the concept of sustainable and smarter cities that is envisioned nowadays. This paper, focusing on transportation and its road traffic, presents how road traffic congestions can be detected and diagnosed in quasi real-time. We adapt pure Artificial Intelligence diagnosis techniques to fully exploit knowledge which is captured through relevant semantics-augmented stream and static data from various domains. Our prototype of semantic-aware diagnosis of road traffic congestions, experimented in Dublin Ireland, works efficiently with large, heterogeneous information sources and delivers value-added services to citizens and city managers in quasi real-time. © 2012 Springer-Verlag Berlin Heidelberg.",,"Diagnosis techniques; Heterogeneous information sources; Ireland; Reasoning tasks; Road traffic; Semantic Web technology; Value added service; Artificial intelligence; Traffic congestion",Conference Paper,Scopus,2-s2.0-84868548063
"Rosati R., Ruzzi M., Graziosi M., Masotti G.","Evaluation of techniques for inconsistency handling in OWL 2 QL ontologies",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,10.1007/978-3-642-35173-0-23,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868570908&doi=10.1007%2f978-3-642-35173-0-23&partnerID=40&md5=3aeb616734aa9ccab34569f719ed8bdc","In this paper we present the Quonto Inconsistent Data handler (QuID). QuID is a reasoner for OWL 2 QL that is based on the system Quonto and is able to deal with inconsistent ontologies. The central aspect of QuID is that it implements two different, orthogonal strategies for dealing with inconsistency: ABox repairing techniques, based on data manipulation, and consistent query answering techniques, based on query rewriting. Moreover, by exploiting the ability of Quonto to delegate the management of the ABox to a relational database system (DBMS), such techniques are potentially able to handle very large inconsistent ABoxes. For the above reasons, QuID allows for experimentally comparing the above two different strategies for inconsistency handling in the context of OWL 2 QL. We thus report on the experimental evaluation that we have conducted using QuID. Our results clearly point out that inconsistency-tolerance in OWL 2 QL ontologies is feasible in practical cases. Moreover, our evaluation singles out the different sources of complexity for the data manipulation technique and the query rewriting technique, and allows for identifying the conditions under which one method is more efficient than the other. © 2012 Springer-Verlag Berlin Heidelberg.",,"Consistent query answering; Data manipulations; Experimental evaluation; Inconsistency handling; Inconsistent data; Query rewriting techniques; Query rewritings; Reasoner; Artificial intelligence; Relational database systems",Conference Paper,Scopus,2-s2.0-84868570908
"Bellemare M.G., Veness J., Bowling M.","Investigating contingency awareness using Atari 2600 games",2012,"Proceedings of the National Conference on Artificial Intelligence",16,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868289914&partnerID=40&md5=926f2e0260c74a216a29267fb75a324c","Contingency awareness is the recognition that some aspects of a future observation are under an agent's control while others are solely determined by the environment. This paper explores the idea of contingency awareness in reinforcement learning using the platform of Atari 2600 games. We introduce a technique for accurately identifying contingent regions and describe how to exploit this knowledge to generate improved features for value function approximation. We evaluate the performance of our techniques empirically, using 46 unseen, diverse, and challenging games for the Atari 2600 console. Our results suggest that contingency awareness is a generally useful concept for model-free reinforcement learning agents. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Future observations; Model free; Reinforcement learning agent; Value function approximation; Intelligent agents; Reinforcement learning; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868289914
"Carfì D., Musolino F.","A coopetitive approach to financial markets stabilization and risk management",2012,"Communications in Computer and Information Science",16,10.1007/978-3-642-31724-8_62,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868272935&doi=10.1007%2f978-3-642-31724-8_62&partnerID=40&md5=f667727e84e50001ab71b237048dfdb1","The aim of this paper is to propose a methodology to stabilize the financial markets by adopting Game Theory, and in particular the Complete Study of a Differentiable Game and the new mathematical model of Coopetitive Game, proposed recently in the literature by D. Carfì. Specifically, we will focus on two economic operators: a real economic subject and a financial institute (a bank, for example) with a big economic availability. At this purpose, we examine an interaction between the above economic subjects: the Enterprise, our first player, and the Financial Institute, our second player. The unique solution which allows both players to win something, and therefore the only one collectively desirable, is represented by an agreement between the two subjects. So the Enterprise artificially causes an inconsistency between spot and future markets, and the Financial Institute takes the opportunity to win the maximum possible collective (social) sum, which later will be divided with the Enterprise by contract. In fact, the Financial Institute is unable to make arbitrages alone because of the introduction, by the normative authority, of an economic transactions tax (that we propose to stabilize the financial market, in order to protect it from speculations). We propose hereunder two kinds of agreement: a fair transferable utility agreement on the initial interaction and a same type of compromise in a coopetitive context. © 2012 Springer-Verlag.","Arbitrages; Coopetition; Financial Crisis; Financial Markets and Institutions; Financing Policy; Games; Risk","Arbitrages; Co-opetition; Financial crisis; Financial market; Games; Artificial intelligence; Commerce; Data processing; Game theory; Industry; Information management; Knowledge based systems; Risk management; Risks; Finance",Conference Paper,Scopus,2-s2.0-84868272935
"Leucker M., Thoma D.","A formal approach to software product families",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,10.1007/978-3-642-34026-0_11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868268252&doi=10.1007%2f978-3-642-34026-0_11&partnerID=40&md5=7c147d0baaf20e5d19b408a656a3ab5b","Software product line engineering deals with the combined development of a family of similar software systems. These systems provide a similar set of features and should therefore share a large number of common components. We study the user perspective of features and the engineering perspective of components and present a formal notion of features, component-based product families and their interaction. We then demonstrate using Milner's CCS how our formalism can be applied to extend an arbitrary modelling formalism with support for product lines. To verify that certain products indeed realize certain features, we propose μ-calculus model-checking for multi-valued Kripke-structures. The model checking result in that case no longer is a simple truth-value, but a set of products, conforming to a certain property. © 2012 Springer-Verlag.",,"Component based; Engineering perspective; Formal approach; Product families; Product-lines; Software product family; Software product line engineerings; Software systems; Truth values; Artificial intelligence; Model checking",Conference Paper,Scopus,2-s2.0-84868268252
"Damiani F., Schaefer I.","Family-based analysis of type safety for delta-oriented software product lines",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,10.1007/978-3-642-34026-0_15,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868282555&doi=10.1007%2f978-3-642-34026-0_15&partnerID=40&md5=1c6adaee602c3ed0c65be6c336d19bc5","Delta-oriented programming (DOP) is a modular, yet flexible approach for implementing software product lines extending feature-oriented programming. Delta modules allow adding, modifying and removing code for generating product variants. The connection between code modifications and product features and the application ordering of delta modules is less restrictive than in FOP. However, the additional flexibility of DOP increases the complexity for ensuring that all possible product variants of a DOP SPL are well-typed. In previous work, we presented a constraint-based type system which allows analyzing each delta module in isolation, but requires a subsequent analysis step for each product variant. Some FOP SPL type systems generate a representation of all possible product variants and use a family-based analysis to ensure that all possible product variants are type safe. In this paper, we enhance the existing constraint-based type checking approach for DOP by providing a family-based analysis step which improves the product-based analysis of our previous work by making it possible to reuse the intermediate results of the analysis associated to the product variants. © 2012 Springer-Verlag.",,"Additional flexibilities; Code modifications; Constraint-based; Feature-oriented programming; Intermediate results; Product feature; Product variants; Software Product Line; Type safety; Type systems; Typechecking; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868282555
"Liu X., Datta A.","Modeling context aware dynamic trust using hidden markov model",2012,"Proceedings of the National Conference on Artificial Intelligence",16,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868298607&partnerID=40&md5=0ff524c706b698acbc6b7b29a92b2cab","Modeling trust in complex dynamic environments is an important yet challenging issue since an intelligent agent may strategically change its behavior to maximize its profits. In this paper, we propose a context aware trust model to predict dynamic trust by using a Hidden Markov Model (HMM) to model an agent's interactions. Although HMMs have already been applied in the past to model an agent's dynamic behavior to greatly improve the traditional static probabilistic trust approaches, most HMM based trust models only focus on outcomes of the past interactions without considering interaction context, which we believe, reflects immensely on the dynamic behavior or intent of an agent. Interaction contextual information is comprehensively studied and integrated into the model to more precisely approximate an agent's dynamic behavior. Evaluation using real auction data and synthetic data demonstrates the efficacy of our approach in comparison with previous state-of-the-art trust mechanisms. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Complex dynamics; Context-Aware; Contextual information; Dynamic behaviors; Interaction context; Synthetic data; Trust mechanism; Trust models; Hidden Markov models; Profitability; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868298607
"Bonet B., Geffner H.","Action selection for MDPs: Anytime AO* versus UCT",2012,"Proceedings of the National Conference on Artificial Intelligence",16,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868269234&partnerID=40&md5=77fdd5fd05359c664664e835b9e62c60","In the presence of non-admissible heuristics, A* and other best-first algorithms can be converted into anytime optimal algorithms over OR graphs, by simply continuing the search after the first solution is found. The same trick, however, does not work for best-first algorithms over AND/OR graphs, that must be able to expand leaf nodes of the explicit graph that are not necessarily part of the best partial solution. Anytime optimal variants of AO* must thus address an exploration-exploitation tradeoff: they cannot just ""exploit"", they must keep exploring as well. In this work, we develop one such variant of AO* and apply it to finite-horizon MDPs. This Anytime AO* algorithm eventually delivers an optimal policy while using non-admissible random heuristics that can be sampled, as when the heuristic is the cost of a base policy that can be sampled with rollouts. We then test Anytime AO* for action selection over large infinite-horizon MDPs that cannot be solved with existing off-line heuristic search and dynamic programming algorithms, and compare it with UCT. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Action selection; AND/OR graphs; Dynamic programming algorithm; Explicit graphs; Heuristic search; Infinite horizons; Optimal algorithm; Optimal policies; Optimal variants; Heuristic algorithms; Optimization; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868269234
"Agostini P., Pizzol L., Critto A., D'Alessandro M., Zabeo A., Marcomini A.","Regional risk assessment for contaminated sites Part 3: Spatial decision support system",2012,"Environment International",16,10.1016/j.envint.2012.07.005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864956003&doi=10.1016%2fj.envint.2012.07.005&partnerID=40&md5=cf3b92ae868fdfbb31818d340ce947aa","Large scale (e.g. regional or national) assessments of contaminated sites may be very costly in terms of investigation and methodological (i.e. risk assessment procedures) requirements and may produce a quantity of information that usually discourages examination by decision-makers. Moreover, most of the existing tools effectively support local environmental risk assessment and management, but lack the capabilities of larger scale analysis, not mentioning the absence of the relevant component of socio-economic prioritization. To respond to the concerns and the management needs of experts and decision makers, the Spatial decision support sYstem for Regional rIsk Assessment of Degraded land (SYRIADE DSS) was developed and is presented according to its three modules: Regional Risk Assessment, Socio-economic Assessment and Integrated Assessment, respectively. The system allows to rank potentially contaminated sites for priority of investigation, when no information on characterization and risk by site specific methodologies is available. This GIS-based system embeds an innovative spatial and relative risk assessment procedure, and proposes the integrated analysis of different data (environmental and socio-economic) for the concerned sites, eliciting when necessary experts' knowledge and stakeholders' values (through Multi Criteria Decision Analysis, MCDA, methodologies). The application to a Polish case-study shows the performance and the flexibility of the system in investigating and mapping (potentially) contaminated sites at the regional scale. © 2012 Elsevier Ltd.","Contaminated sites; Decision support systems; Environmental risk assessment; Risk management; Socio-economic assessment","Artificial intelligence; Decision making; Decision support systems; Economic analysis; Environmental management; Risk management; Assessment procedure; Contaminated sites; Decision makers; Environmental risk assessment; Integrated analysis; Integrated assessment; Multi-criteria decision analysis; Prioritization; Regional risk assessment; Regional scale; Relative risks; Relevant components; Scale analysis; Site-specific; Socio-economics; Spatial decision support systems; Risk assessment; decision making; decision support system; environmental assessment; environmental management; GIS; prioritization; risk assessment; spatial analysis; article; contamination; decision support system; environmental exploitation; environmental monitoring; geographic information system; land degradation; Poland; priority journal; risk assessment; risk management; socioeconomics",Article,Scopus,2-s2.0-84864956003
"Zaffalon M., Corani G., Mauá D.","Evaluating credal classifiers by utility-discounted predictive accuracy",2012,"International Journal of Approximate Reasoning",16,10.1016/j.ijar.2012.06.022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866049873&doi=10.1016%2fj.ijar.2012.06.022&partnerID=40&md5=17410ddbec75da03e14d9aa63ac57e63","Predictions made by imprecise-probability models are often indeterminate (that is, set-valued). Measuring the quality of an indeterminate prediction by a single number is important to fairly compare different models, but a principled approach to this problem is currently missing. In this paper we derive, from a set of assumptions, a metric to evaluate the predictions of credal classifiers. These are supervised learning models that issue set-valued predictions. The metric turns out to be made of an objective component, and another that is related to the decision-maker's degree of risk aversion to the variability of predictions. We discuss when the measure can be rendered independent of such a degree, and provide insights as to how the comparison of classifiers based on the new measure changes with the number of predictions to be made. Finally, we make extensive empirical tests of credal, as well as precise, classifiers by using the new metric. This shows the practical usefulness of the metric, while yielding a first insightful and extensive comparison of credal classifiers. © 2012 Elsevier Inc. All rights reserved.",,"Comparison of classifiers; Decision makers; Degree of risks; Empirical test; Predictive accuracy; Artificial intelligence; Software engineering; Forecasting",Conference Paper,Scopus,2-s2.0-84866049873
"Tsogkas S., Kokkinos I.","Learning-based symmetry detection in natural images",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,10.1007/978-3-642-33786-4_4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867873585&doi=10.1007%2f978-3-642-33786-4_4&partnerID=40&md5=b17f1cee4ce7d90da4df14538b67a861","In this work we propose a learning-based approach to symmetry detection in natural images. We focus on ribbon-like structures, i.e. contours marking local and approximate reflection symmetry and make three contributions to improve their detection. First, we create and make publicly available a ground-truth dataset for this task by building on the Berkeley Segmentation Dataset. Second, we extract features representing multiple complementary cues, such as grayscale structure, color, texture, and spectral clustering information. Third, we use supervised learning to learn how to combine these cues, and employ MIL to accommodate the unknown scale and orientation of the symmetric structures. We systematically evaluate the performance contribution of each individual component in our pipeline, and demonstrate that overall we consistently improve upon results obtained using existing alternatives. © 2012 Springer-Verlag.",,"Data sets; Gray scale; Individual components; Learning-based approach; Natural images; Reflection symmetry; Ribbon-like structures; Spectral clustering; Symmetric structures; Symmetry detection; Artificial intelligence; Computer vision",Conference Paper,Scopus,2-s2.0-84867873585
"Surynek P.","Towards optimal cooperative path planning in hard setups through satisfiability solving",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,10.1007/978-3-642-32695-0_50,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867662418&doi=10.1007%2f978-3-642-32695-0_50&partnerID=40&md5=6bc6a5874f198aade199ba1b0c465b7b","A novel approach to cooperative path-planning is presented. A SAT solver is used not to solve the whole instance but for optimizing the makespan of a sub-optimal solution. This approach is trying to exploit the ability of stateof- the-art SAT solvers to give a solution to relatively small instance quickly. A sub-optimal solution to the instance is obtained by some existent method first. It is then submitted to the optimization process which decomposes it into small subsequences for which optimal solutions are found by a SAT solver. The new shorter solution is subsequently obtained as concatenation of optimal subsolutions. The process is iterated until a fixed point is reached. This is the first method to produce near optimal solutions for densely populated environments; it can be also applied to domain-independent planning supposed that suboptimal planner is available. © 2012 Springer-Verlag.",,"Domain-independent planning; Fixed points; Makespan; Near-optimal solutions; Optimal solutions; Optimization process; SAT solvers; Satisfiability solving; Suboptimal solution; Artificial intelligence; Decision theory; Formal logic; Motion planning; Optimal systems; Scheduling algorithms; Optimization",Conference Paper,Scopus,2-s2.0-84867662418
"Mavrovouniotis M., Yang S.","Ant colony optimization with memory-based immigrants for the dynamic vehicle routing problem",2012,"2012 IEEE Congress on Evolutionary Computation, CEC 2012",16,10.1109/CEC.2012.6252885,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866847277&doi=10.1109%2fCEC.2012.6252885&partnerID=40&md5=d47a8fb49942ff4c260282ad677dcb5c","A recent integration showed that ant colony optimization (ACO) algorithms with immigrants schemes perform well on different variations of the dynamic travelling salesman problem. In this paper, we address ACO for the dynamic vehicle routing problem (DVRP) with traffic factor where the changes occur in a cyclic pattern. In other words, previous environments will re-appear in the future. Memory-based immigrants are used with ACO in order to collect the best solutions from the environments and use them to generate diversity and transfer knowledge when a dynamic change occurs. The results show that the proposed algorithm, with an appropriate size of memory and immigrant replacement rate, outperforms other peer ACO algorithms on different DVRP test cases. © 2012 IEEE.",,"ACO algorithms; Ant Colony Optimization (ACO); Ant Colony Optimization algorithms; Cyclic patterns; Dynamic changes; Dynamic vehicle routing problems; Replacement rates; Test case; Traffic factors; Travelling salesman problem; Evolutionary algorithms; Traveling salesman problem; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84866847277
"Hsu C.-C., Chen H.-C., Su Y.-N., Huang K.-K., Huang Y.-M.","Developing a reading concentration monitoring system by applying an artificial bee colony algorithm to e-books in an intelligent classroom",2012,"Sensors (Switzerland)",16,10.3390/s121014158,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868250714&doi=10.3390%2fs121014158&partnerID=40&md5=1b23eea22d6a4bb2a09d8fcc66cf8541","A growing number of educational studies apply sensors to improve student learning in real classroom settings. However, how can sensors be integrated into classrooms to help instructors find out students' reading concentration rates and thus better increase learning effectiveness? The aim of the current study was to develop a reading concentration monitoring system for use with e-books in an intelligent classroom and to help instructors find out the students' reading concentration rates. The proposed system uses three types of sensor technologies, namely a webcam, heartbeat sensor, and blood oxygen sensor to detect the learning behaviors of students by capturing various physiological signals. An artificial bee colony (ABC) optimization approach is applied to the data gathered from these sensors to help instructors understand their students' reading concentration rates in a classroom learning environment. The results show that the use of the ABC algorithm in the proposed system can effectively obtain near-optimal solutions.The system has a user-friendly graphical interface, making it easy for instructors to clearly understand the reading status of their students © 2012 by the authors; licensee MDPI, Basel, Switzerland.","Artificial bee colony algorithm;e-books; Intelligent classroom; Reading concentration; Sensor technology","Artificial bee colonies (ABC); Artificial bee colony algorithms; Concentration monitoring; E-books; Intelligent classroom; Learning effectiveness; Optimization approach; Sensor technologies; Computer aided instruction; Electronic publishing; Evolutionary algorithms; Monitoring; Optimization; Students; Sensors; algorithm; animal; article; artificial intelligence; attention; bee; book; equipment; human; methodology; mobile application; physiologic monitoring; physiology; psychological aspect; reading; school; student; Algorithms; Animals; Artificial Intelligence; Attention; Bees; Books; Humans; Mobile Applications; Monitoring, Physiologic; Reading; Schools; Students",Article,Scopus,2-s2.0-84868250714
"Risi S., Stanley K.O.","An enhanced hypercube-based encoding for evolving the placement, density, and connectivity of neurons",2012,"Artificial Life",16,10.1162/ARTL-a-00071,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868281827&doi=10.1162%2fARTL-a-00071&partnerID=40&md5=155b2e45e2da2cb82e6ae011d5d695fc","Intelligence in nature is the product of living brains, which are themselves the product of natural evolution. Although researchers in the field of neuroevolution (NE) attempt to recapitulate this process, artificial neural networks (ANNs) so far evolved through NE algorithms do not match the distinctive capabilities of biological brains. The recently introduced hypercube-based neuroevolution of augmenting topologies (HyperNEAT) approach narrowed this gap by demonstrating that the pattern of weights across the connectivity of an ANN can be generated as a function of its geometry, thereby allowing large ANNs to be evolved for high-dimensional problems. Yet the positions and number of the neurons connected through this approach must be decided a priori by the user and, unlike in living brains, cannot change during evolution. Evolvable-substrate HyperNEAT (ES-HyperNEAT), introduced in this article, addresses this limitation by automatically deducing the node geometry from implicit information in the pattern of weights encoded by HyperNEAT, thereby avoiding the need to evolve explicit placement. This approach not only can evolve the location of every neuron in the network, but also can represent regions of varying density, which means resolution can increase holistically over evolution. ES-HyperNEAT is demonstrated through multi-task, maze navigation, and modular retina domains, revealing that the ANNs generated by this new approach assume natural properties such as neural topography and geometric regularity. Also importantly, ES-HyperNEAT's compact indirect encoding can be seeded to begin with a bias toward a desired class of ANN topographies, which facilitates the evolutionary search. The main conclusion is that ES-HyperNEAT significantly expands the scope of neural structures that evolution can discover. © 2012 Massachusetts Institute of Technology.","Artificial neural networks; Compositional pattern-producing networks; Generative and developmental systems; HyperNEAT; Indirect encoding; Neuroevolution","Developmental systems; Evolutionary search; Geometric regularity; High-dimensional problems; HyperNEAT; Natural evolution; Natural properties; Neural structures; Neuro evolutions; Neuroevolution of augmenting topologies; Encoding (symbols); Evolutionary algorithms; Geometry; Neural networks; Neurons; Biology; algorithm; article; artificial intelligence; artificial neural network; computer simulation; evolution; human; learning; metabolism; nerve cell; nerve cell network; Algorithms; Artificial Intelligence; Biological Evolution; Computer Simulation; Humans; Learning; Nerve Net; Neural Networks (Computer); Neurons",Article,Scopus,2-s2.0-84868281827
"Wang Y.","On abstract intelligence and brain informatics: Mapping cognitive functions of the brain onto its neural structures",2012,"International Journal of Cognitive Informatics and Natural Intelligence",16,10.4018/jcini.2012100103,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877899492&doi=10.4018%2fjcini.2012100103&partnerID=40&md5=bbf500f6e1f4689ce407f9dabd256f93","A key notion in abstract intelligence and cognitive informatics is that the brain and natural intelligence may only be explained by a hierarchical and reductive theory that maps the brain through the embodied neurological, physiological, cognitive, and logical levels from bottom-up induction and top-down deduction. This paper presents an abstract intelligence framework for modeling the structures and functions of the brain across these four levels. A set of abstract intelligent model, cognitive functional model, and neurophysiological model of the brain is systematically developed. On the basis of the abstract intelligent models of the brain at different levels, the conventionally highly overlapped, redundant, and even contradicted empirical observations in brain studies and cognitive psychology may be rigorously clarified and neatly explained. The improved understanding about the brain has led to the development of a wide range of novel technologies and systems such as cognitive computers, cognitive robots, and other applied cognitive systems. Copyright © 2012, IGI Global.","Abstract intelligence; Brain informatics; Cognitive computing; Cognitive informatics; Computational intelligence; Denotational mathematics; Natural intelligence; Neurocomputing; Neuroinformatics","Abstract intelligence; Brain informatics; Cognitive Computing; Cognitive informatics; Denotational mathematics; Natural intelligence; Neurocomputing; Neuroinformatics; Artificial intelligence; Brain models; Cognitive systems; Information science; Psychophysiology; Brain mapping",Article,Scopus,2-s2.0-84877899492
"Lv X., Li H., Wang B.","Group key agreement for secure group communication in dynamic peer systems",2012,"Journal of Parallel and Distributed Computing",16,10.1016/j.jpdc.2012.06.004,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865071791&doi=10.1016%2fj.jpdc.2012.06.004&partnerID=40&md5=713ab149b8e9ef53419226ee95e63a45","Self-organizing group key agreement protocols without a centralized administrator are essential to secure group communication in dynamic peer systems. In this paper, we propose a generic construction of a one-round self-organizing group key agreement protocol based on the Chinese Remainder Theorem. In the proposed construction, all group members contribute their own public keys to negotiate a shared encryption public key, which corresponds to all different decryption keys. Using his/her own secret key, each group member is able to decrypt any ciphertext encrypted by the shared encryption key. Following the generic construction, we instantiate a one-round self-organizing group key agreement protocol using the efficient and computationally inexpensive public key cryptosystem NTRU. Both the public key and the message in this protocol are secure against the known lattice attacks. Furthermore, we also briefly describe another concrete scheme with our generic idea, based on the ElGamal public key cryptosystem. © 2012 Elsevier Inc. All rights reserved.","Dynamic peer system; One-round group key agreement; Secure group communication","Chinese remainder theorem; Ciphertexts; Decryption keys; ElGamal; Encryption key; Generic construction; Group key agreement; Group Key Agreement protocols; Group members; Public key cryptosystems; Public keys; Secret key; Secure group communications; Self organizing; Artificial intelligence; Computer programming; Public key cryptography",Article,Scopus,2-s2.0-84865071791
"Chia A.Y.-S., Rajan D., Leung M.K., Rahardja S.","Object recognition by discriminative combinations of line segments, ellipses, and appearance features",2012,"IEEE Transactions on Pattern Analysis and Machine Intelligence",16,10.1109/TPAMI.2011.220,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865588666&doi=10.1109%2fTPAMI.2011.220&partnerID=40&md5=9745bcc3a89dcfb03ba76136785d3f56","We present a novel contour-based approach that recognizes object classes in real-world scenes using simple and generic shape primitives of line segments and ellipses. Compared to commonly used contour fragment features, these primitives support more efficient representation since their storage requirements are independent of object size. Additionally, these primitives are readily described by their geometrical properties and hence afford very efficient feature comparison. We pair these primitives as shape-tokens and learn discriminative combinations of shape-tokens. Here, we allow each combination to have a variable number of shape-tokens. This, coupled with the generic nature of primitives, enables a variety of class-specific shape structures to be learned. Building on the contour-based method, we propose a new hybrid recognition method that combines shape and appearance features. Each discriminative combination can vary in the number and the types of features, where these two degrees of variability empower the hybrid method with even more flexibility and discriminative potential. We evaluate our methods across a large number of challenging classes, and obtain very competitive results against other methods. These results show the proposed shape primitives are indeed sufficiently powerful to recognize object classes in complex real-world scenes. © 2012 IEEE.","appearance features; category-level object detection.; image classification; Shape primitives","appearance features; Contour-based methods; Generic nature; Generic shapes; Geometrical property; Hybrid method; Line segment; Object class; Object Detection; Object size; Recognition methods; Shape primitives; Storage requirements; Variable number; Artificial intelligence; Computer vision; Image classification; Object recognition",Article,Scopus,2-s2.0-84865588666
"Lahrmann H., Agerholm N., Tradisauskas N., Næss T., Juhl J., Harms L.","Pay as You Speed, ISA with incentives for not speeding: A case of test driver recruitment",2012,"Accident Analysis and Prevention",16,10.1016/j.aap.2011.03.014,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861880039&doi=10.1016%2fj.aap.2011.03.014&partnerID=40&md5=7ea23675275a53a32023e5245b84edde","The Intelligent Speed Adaptation (ISA) project we describe in this article is based on Pay as You Drive principles. These principles assume that the ISA equipment informs a driver of the speed limit, warns the driver when speeding and calculates penalty points. Each penalty point entails the reduction of a 30% discount on the driver's car insurance premium, which therefore produced the name, Pay as You Speed. The ISA equipment consists of a GPS-based On Board Unit with a mobile phone connection to a web server. The project was planned for a three-year test period with 300 young car drivers, but it never succeeded in recruiting that number of drivers. After several design changes, the project eventually went forward with 153 test drivers of all ages. This number represents approximately one thousandth of all car owners in the proving ground of North Jutland in Denmark. Furthermore the project was terminated before its scheduled closing date. This article describes the project with an emphasis on recruitment efforts and the project's progress. We include a discussion of possible explanations for the failure to recruit volunteers for the project and reflect upon the general barriers to using ISA with ordinary drivers. © 2011 Elsevier Ltd.","Economic incentives; Field trial; Intelligent Speed Adaptation; Speeding; Traffic safety","acceleration; accident prevention; adolescent; adult; article; artificial intelligence; car; car driving; Denmark; economics; equipment design; evaluation; geographic information system; human; instrumentation; insurance; Internet; interview; law enforcement; legal aspect; methodology; mobile phone; motivation; politics; psychological aspect; traffic accident; Acceleration; Accident Prevention; Accidents, Traffic; Adolescent; Adult; Artificial Intelligence; Automobile Driving; Automobiles; Cellular Phone; Denmark; Equipment Design; Geographic Information Systems; Humans; Insurance; Internet; Interviews as Topic; Law Enforcement; Motivation; Politics; Young Adult; Economic incentive; Field trial; Intelligent speed adaptation; Speeding; Traffic safety; Insurance; Speed control",Article,Scopus,2-s2.0-84861880039
"Le Page M., Berjamy B., Fakir Y., Bourgin F., Jarlan L., Abourida A., Benrhanem M., Jacob G., Huber M., Sghrer F., Simonneaux V., Chehbouni G.","An Integrated DSS for Groundwater Management Based on Remote Sensing. The Case of a Semi-arid Aquifer in Morocco",2012,"Water Resources Management",16,10.1007/s11269-012-0068-3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865585359&doi=10.1007%2fs11269-012-0068-3&partnerID=40&md5=534c5cb3fa3860be985c77a3288eef9b","A Decision Support System has been set up as the result of a fruitful cooperation between several public and research institutions in the framework of a large cooperation program. The DSS aims to compare spatially and temporally sectorial water demands of the Haouz-Mejjate plain (Morocco) in regard to available surface and groundwater resources. It is composed of a tool for satellite estimation of Agricultural Water Demand (SAMIR), a tool for integrated water resources planning (WEAP) and a groundwater model (MODFLOW) each of them relying on a common Geographical Information System not described here. The DSS is operating on a monthly time scale. Agricultural water demand accounts for about 80% of the total demand. In areas where groundwater abstraction is difficult to quantify by direct methods, multitemporal remote sensing associated to the FAO methodology is a simple and efficient alternative to estimate Evapotranspiration (ET). In this work, a monthly estimate of ET from irrigated areas is derived from freely available MODIS NDVI for the 2001-2009 period. An important part of the paper deals with the validation of these estimates with eddy covariance flux measurements installed on different irrigated crops of the region. Results are satisfactory with a minus 6.5% error per year on the monthly time scale. This preprocessing allows to dichotomize irrigated versus non-irrigated areas, and then, to estimate groundwater abstraction in subareas distinguishing by their operating modes: traditional, dam or privately irrigated. A dynamic linkage between MODFLOW and WEAP transfers the results of one model as input data to the other. The model restitutes both spatial and temporal variations in head charges and allows the calculation of the ground water balance. After calibration, piezometric validation is acceptable for the majority of the 21 head control points. © 2012 Springer Science+Business Media B.V.","Decision support system; Evapotranspiration; Groundwater; Remote sensing; Semi-arid","Agricultural water; Direct method; Eddy covariance; Flux measurements; Geographical Information System; Groundwater abstraction; Groundwater management; Groundwater models; Head control; Input datas; Irrigated crops; MODFLOW; Multi-temporal remote sensing; Operating modes; Research institutions; Semi arid; Spatial and temporal variation; Sub-areas; Time-scales; Water balance; Water demand; Water resources planning; Agriculture; Aquifers; Artificial intelligence; Decision support systems; Evapotranspiration; Geographic information systems; Groundwater; Groundwater resources; Remote sensing; Surface water resources; Water management; Water supply; Estimation; aquifer; decision support system; evapotranspiration; flux measurement; GIS; groundwater; groundwater abstraction; model validation; remote sensing; semiarid region; spatiotemporal analysis; waste management; water demand; water planning; Morocco",Article,Scopus,2-s2.0-84865585359
"Bejan C.A., Xia F., Vanderwende L., Wurfe M.M., Yetisgen-Yildiz M.","Pneumonia identification using statistical feature selection",2012,"Journal of the American Medical Informatics Association",16,10.1136/amiajnl-2011-000752,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872222809&doi=10.1136%2famiajnl-2011-000752&partnerID=40&md5=868beb978642045a90f4c9b5a146899f","Objective This paper describes a natural language processing system for the task of pneumonia identification. Based on the information extracted from the narrative reports associated with a patient, the task is to identify whether or not the patient is positive for pneumonia. Design A binary classifier was employed to identify pneumonia from a dataset of multiple types of clinical notes created for 426 patients during their stay in the intensive care unit. For this purpose, three types of features were considered: (1) word n-grams, (2) Unified Medical Language System (UMLS) concepts, and (3) assertion values associated with pneumonia expressions. System performance was greatly increased by a feature selection approach which uses statistical significance testing to rank features based on their association with the two categories of pneumonia identification. Results Besides testing our system on the entire cohort of 426 patients (unrestricted dataset), we also used a smaller subset of 236 patients (restricted dataset). The performance of the system was compared with the results of a baseline previously proposed for these two datasets. The best results achieved by the system (85.71 and 81.67 F1-measure) are significantly better than the baseline results (50.70 and 49.10 F1-measure) on the restricted and unrestricted datasets, respectively. Conclusion Using a statistical feature selection approach that allows the feature extractor to consider only the most informative features from the feature space significantly improves the performance over a baseline that uses all the features from the same feature space. Extracting the assertion value for pneumonia expressions further improves the system performance.",,"algorithm; article; classifier; computer program; data processing; electronic medical record; hospitalization; human; intensive care unit; major clinical study; medical information system; pneumonia; artificial intelligence; computer assisted diagnosis; data mining; electronic medical record; methodology; natural language processing; pneumonia; sensitivity and specificity; Unified Medical Language System; Artificial Intelligence; Data Mining; Diagnosis, Computer-Assisted; Electronic Health Records; Humans; Intensive Care Units; Natural Language Processing; Pneumonia; Sensitivity and Specificity; Unified Medical Language System",Article,Scopus,2-s2.0-84872222809
"Suner A., Çelikoĝlu C.C., Dicle O., Sökmen S.","Sequential decision tree using the analytic hierarchy process for decision support in rectal cancer",2012,"Artificial Intelligence in Medicine",16,10.1016/j.artmed.2012.05.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865566398&doi=10.1016%2fj.artmed.2012.05.003&partnerID=40&md5=54e46d8b31ee8c725b27fcc77b1dc48d","Objective: The aim of the study is to determine the most appropriate method for construction of a sequential decision tree in the management of rectal cancer, using various patient-specific criteria and treatments such as surgery, chemotherapy, and radiotherapy. Methods: An analytic hierarchy process (AHP) was used to determine the priorities of variables. Relevant criteria used in two decision steps and their relative priorities were established by a panel of five general surgeons. Data were collected via a web-based application and analyzed using the ""Expert Choice"" software specifically developed for the AHP. Consistency ratios in the AHP method were calculated for each set of judgments, and the priorities of sub-criteria were determined. A sequential decision tree was constructed for the best treatment decision process, using priorities determined by the AHP method. Results: Consistency ratios in the AHP method were calculated for each decision step, and the judgments were considered consistent. The tumor-related criterion ""presence of perforation"" (0.331) and the patient-surgeon-related criterion ""surgeon's experience"" (0.630) had the highest priority in the first decision step. In the second decision step, the tumor-related criterion ""the stage of the disease"" (0.230) and the patient-surgeon-related criterion ""surgeon's experience"" (0.281) were the paramount criteria. The results showed some variation in the ranking of criteria between the decision steps. In the second decision step, for instance, the tumor-related criterion ""presence of perforation"" was just the fifth. Conclusion: The consistency of decision support systems largely depends on the quality of the underlying decision tree. When several choices and variables have to be considered in a decision, it is very important to determine priorities. The AHP method seems to be effective for this purpose. The decision algorithm developed by this method is more realistic and will improve the quality of the decision tree. © 2012 Elsevier B.V.","Analytic hierarchy process; Medical decision making; Rectal cancer; Sequential decision tree","AHP method; Consistency ratio; Decision algorithms; Decision process; Decision supports; Medical decision making; Rectal cancer; Relative priorities; Web-based applications; Analytic hierarchy process; Artificial intelligence; Chemotherapy; Decision support systems; Decision trees; Diseases; Hierarchical systems; Tumors; Decision making; antineoplastic agent; analytic hierarchy process; analytic method; analytical parameters; article; cancer chemotherapy; cancer radiotherapy; cancer surgery; clinical decision making; decision support system; decision tree; human; priority journal; rectum cancer; sequential analysis; treatment planning; Algorithms; Decision Support Techniques; Decision Trees; Fluorodeoxyglucose F18; Humans; Positron-Emission Tomography and Computed Tomography; Rectal Neoplasms",Article,Scopus,2-s2.0-84865566398
"Cai K., Zhang J., Zhou C., Cao X., Tang K.","Using computational intelligence for large scale air route networks design",2012,"Applied Soft Computing Journal",16,10.1016/j.asoc.2012.03.063,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863457965&doi=10.1016%2fj.asoc.2012.03.063&partnerID=40&md5=1a5ca7401c9a430a61308ec35d854125","Due to the rapid development of air transportation, Air Route Networks (ARNs) need to be carefully designed to improve both efficiency and safety of air traffic service. The Crossing Waypoints Location Problem (CWLP) plays a crucial role in the design of an ARN. This paper investigates this problem in the context of designing the national ARN of China. Instead of adopting the single-objective formulation established in previous research, we propose to formulate CWLP as a bi-objective optimization problem. An algorithm named Memetic Algorithm with Pull-Push operator (MAPP) is proposed to tackle it. MAPP employs the Pull-Push operator, which is specifically designed for CWLP, for local search and the Comprehensive Learning Particle Swarm Optimizer for global search. Empirical studies using real data of the current national ARN of China showed that MAPP outperformed an existing approach to CWLP as well as three well-known Multi-Objective Evolutionary Algorithms (MOEAs). Moreover, MAPP not only managed to reduce the cost of the current ARN, but also improved the airspace safety. Hence, it has been implemented as a module in the software that is currently used for ARN planning in China. The data used in our experimental studies have been made available online and can be used as a benchmark problem for research on both ARN design and evolutionary multi-objective optimization. © 2012 Elsevier B.V. All rights reserved.","Air Route Network; Crossing Waypoints Location; Evolutionary Multi-objective Optimization; Memetic Algorithms","Air routes; Air traffic services; Bench-mark problems; Bi-objective optimization; Comprehensive learning; Empirical studies; Evolutionary multiobjective optimization; Experimental studies; Global search; Local search; Location problems; Memetic algorithms; Multi objective evolutionary algorithms; Particle swarm optimizers; Rapid development; Waypoints; Artificial intelligence; Design; Evolutionary algorithms; Multiobjective optimization; Air transportation",Article,Scopus,2-s2.0-84863457965
"Baudiš P., Gailly J.-L.","PACHI: State of the art open source Go program",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,10.1007/978-3-642-31866-5_3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864964150&doi=10.1007%2f978-3-642-31866-5_3&partnerID=40&md5=516b6f42c4c4576e6db1acb00cef4f01","We present a state of the art implementation of the Monte Carlo Tree Search algorithm for the game of Go. Our Pachi software is currently one of the strongest open source Go programs, competing at the top level with other programs and playing evenly against advanced human players. We describe our implementation and choice of published algorithms as well as three notable original improvements: (1) an adaptive time control algorithm, (2) dynamic komi, and (3) the usage of the criticality statistic. We also present new methods to achieve efficient scaling both in terms of multiple threads and multiple machines in a cluster. © 2012 Springer-Verlag.",,"Human players; MONTE CARLO; Multiple machine; Multiple threads; Open sources; State of the art; Tree search algorithm; Adaptive algorithms; Artificial intelligence; Computer software; Human computer interaction; Open systems",Conference Paper,Scopus,2-s2.0-84864964150
"Grafsgaard J.F., Boyer K.E., Wiebe E.N., Lester J.C.","Analyzing posture and affect in task-oriented tutoring",2012,"Proceedings of the 25th International Florida Artificial Intelligence Research Society Conference, FLAIRS-25",16,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865017522&partnerID=40&md5=0dab10058e15030646af9dd318eb9e77","Intelligent tutoring systems research aims to produce systems that meet or exceed the effectiveness of one-on-one expert human tutoring. Theory and empirical study suggest that affective states of the learner must be addressed to achieve this goal. While many affective measures can be utilized, posture offers the advantages of non-intrusiveness and ease of interpretation. This paper presents an accurate posture estimation algorithm applied to a computer-mediated tutoring corpus of depth recordings. Analyses of posture and session-level student reports of engagement and cognitive load identified significant patterns. The results indicate that disengagement and frustration may coincide with closer postural positions and more movement, while focused attention and less frustration occur with more distant, stable postural positions. It is hoped that this work will lead to intelligent tutoring systems that recognize a greater breadth of affective expression through channels of posture and gesture. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Affective state; Cognitive loads; Empirical studies; Intelligent tutoring system; Posture estimation; Significant patterns; Through channel; Artificial intelligence; Computer aided instruction; Research",Conference Paper,Scopus,2-s2.0-84865017522
"Coulom R.","CLOP: Confident local optimization for noisy black-box parameter tuning",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,10.1007/978-3-642-31866-5_13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865023981&doi=10.1007%2f978-3-642-31866-5_13&partnerID=40&md5=2c27bf25bd542f8d0cb2ff6876abda80","Artificial intelligence in games often leads to the problem of parameter tuning. Some heuristics may have coefficients, and they should be tuned to maximize the win rate of the program. A possible approach is to build local quadratic models of the win rate as a function of program parameters. Many local regression algorithms have already been proposed for this task, but they are usually not sufficiently robust to deal automatically and efficiently with very noisy outputs and non-negative Hessians. The CLOP principle, which stands for Confident Local OPtimization, is a new approach to local regression that overcomes all these problems in a straightforward and efficient way. CLOP discards samples of which the estimated value is confidently inferior to the mean of all samples. Experiments demonstrate that, when the function to be optimized is smooth, this method outperforms all other tested algorithms. © 2012 Springer-Verlag.",,"Black boxes; Local optimizations; Local regression; Parameter-tuning; Program parameters; Quadratic models; Algorithms; Artificial intelligence; Computer software; Human computer interaction; Optimization",Conference Paper,Scopus,2-s2.0-84865023981
"Pang H., George S.L., Hui K., Tong T.","Gene selection using iterative feature elimination random forests for survival outcomes",2012,"IEEE/ACM Transactions on Computational Biology and Bioinformatics",16,10.1109/TCBB.2012.63,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864947491&doi=10.1109%2fTCBB.2012.63&partnerID=40&md5=e425c22eb1e3b1f11fc08d7ab155bd2f","Although many feature selection methods for classification have been developed, there is a need to identify genes in high-dimensional data with censored survival outcomes. Traditional methods for gene selection in classification problems have several drawbacks. First, the majority of the gene selection approaches for classification are single-gene based. Second, many of the gene selection procedures are not embedded within the algorithm itself. The technique of random forests has been found to perform well in high-dimensional data settings with survival outcomes. It also has an embedded feature to identify variables of importance. Therefore, it is an ideal candidate for gene selection in high-dimensional data with survival outcomes. In this paper, we develop a novel method based on the random forests to identify a set of prognostic genes. We compare our method with several machine learning methods and various node split criteria using several real data sets. Our method performed well in both simulations and real data analysis. Additionally, we have shown the advantages of our approach over single-gene-based approaches. Our method incorporates multivariate correlations in microarray data for survival outcomes. The described method allows us to better utilize the information available from microarray data with survival outcomes. © 2004-2012 IEEE.","Cancer; gene selection; iterative feature elimination; microarrays; random forest; survival","Cancer; Gene selection; iterative feature elimination; Random forests; survival; Decision trees; Feature extraction; Learning systems; Microarrays; Genes; algorithm; article; artificial intelligence; automated pattern recognition; DNA microarray; gene expression profiling; methodology; Algorithms; Artificial Intelligence; Gene Expression Profiling; Oligonucleotide Array Sequence Analysis; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84864947491
"Kruijff G.-J.M., Colas F., Svoboda T., Van Diggelen J., Balmer P., Pirri F., Worst R.","Designing intelligent robots for human-robot teaming in urban search & rescue",2012,"AAAI Spring Symposium - Technical Report",16,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864870012&partnerID=40&md5=c31a00aa10446029b8579dc9ca3ba4d2","The paper describes ongoing integrated research on designing intelligent robots that can assist humans in making a situation assessment during Urban Search & Rescue (USAR) missions. These robots (rover, microcopter) are deployed during the early phases of an emergency response. The aim is to explore those areas of the disaster hotzone which are too dangerous or too difficult for a human to enter at that point. This requires the robots to be ""intelligent"" in the sense of being capable of various degrees of autonomy in acting and perceiving in the environment. At the same time, their intelligence needs to go beyond mere task-work. Robots and humans are inter-dependent. Human operators are dependent on these robots to provide information for a situation assessment. And robots are dependent on humans to help them operate (shared control) and perceive (shared assessment) in what are typically highly dynamic, largely unknown environments. Robots and humans need to form a team. The paper describes how various insights from robotics and Artificial Intelligence are combined, to develop new approaches for modeling human robot teaming. These approaches range from new forms of modeling situation awareness (to model distributed acting in dynamic space), human robot interaction (to model communication in teams), flexible planning (to model team coordination and joint action), and cognitive system design (to integrate different forms of functionality in a single system). Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Emergency response; Flexible planning; Human operator; Human robots; Integrated research; Joint actions; New forms; Shared control; Situation assessment; Situation awareness; Team coordination; Urban search; Artificial intelligence; Cognitive systems; Intelligent robots",Conference Paper,Scopus,2-s2.0-84864870012
"Chen Y., Mazlack L., Lu L.","Learning fuzzy cognitive maps from data by ant colony optimization",2012,"GECCO'12 - Proceedings of the 14th International Conference on Genetic and Evolutionary Computation",16,10.1145/2330163.2330166,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864714121&doi=10.1145%2f2330163.2330166&partnerID=40&md5=58510e8cd624a457137a4aa5f46d04e0","Fuzzy Cognitive Maps (FCMs) are a flexible modeling technique with the goal of modeling causal relationships. Traditionally FCMs are developed by experts. We need to learn FCMs directly from data when expert knowledge is not available. The FCM learning problem can be described as the minimization of the difference between the desired response of the system and the estimated response of the learned FCM model. Learning FCMs from data can be a difficult task because of the large number of candidate FCMs. A FCM learning algorithm based on Ant Colony Optimization (ACO) is presented in order to learn FCM models from multiple observed response sequences. Experiments on simulated data suggest that the proposed ACO based FCM learning algorithm is capable of learning FCM with at least 40 nodes. The performance of the algorithm was tested on both single response sequence and multiple response sequences. The test results are compared to several algorithms, such as genetic algorithms and nonlinear Hebbian learning rule based algorithms. The performance of the ACO algorithm is better than these algorithms in several different experiment scenarios in terms of model errors, sensitivities and specificities. The effect of number of response sequences and number of nodes is discussed. © 2012 ACM.","ant colony optimization; data-driven learning algorithm; fuzzy cognitive maps; numerical optimization","ACO algorithms; Ant Colony Optimization (ACO); Causal relationships; Expert knowledge; Fuzzy cognitive map; Hebbian learning; Learning problem; Model errors; Modeling technique; Multiple response; Numerical optimizations; Simulated data; Experiments; Fuzzy rules; Fuzzy systems; Learning algorithms; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84864714121
"Loyola P., Román P.E., Velásquez J.D.","Predicting web user behavior using learning-based ant colony optimization",2012,"Engineering Applications of Artificial Intelligence",16,10.1016/j.engappai.2011.10.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862130639&doi=10.1016%2fj.engappai.2011.10.008&partnerID=40&md5=986cf9fe295ba854a0752ab60afa974f","An ant colony optimization-based algorithm to predict web usage patterns is presented. Our methodology incorporates multiple data sources, such as web content and structure, as well as web usage. The model is based on a continuous learning strategy based on previous usage in which artificial ants try to fit their sessions with real usage through the modification of a text preference vector. Subsequently, trained ants are released onto a new web graph and the new artificial sessions are compared with real sessions, previously captured via web log processing. The main results of this work are related to an effective prediction of the aggregated patterns of real usage, reaching approximately 80%. In the second place, this approach allows the obtaining of a quantitative representation of the keywords that influence the navigational sessions. © 2011 Elsevier Ltd. All rights reserved.","Ant colony optimization; Multi-agent simulation; Text preferences; Web usage mining","Ant colonies; Ant Colony Optimization (ACO); Artificial ant; Continuous learning; Multi agent simulation; Multiple data sources; Optimization-based algorithm; Text preferences; Web content; Web graphs; Web usage; Web usage mining; Web usage patterns; Web user behaviors; Weblogs; Algorithms; Forecasting; Artificial intelligence",Article,Scopus,2-s2.0-84862130639
"Hooghe B., Broos S., Van Roy F., De Bleser P.","A flexible integrative approach based on random forest improves prediction of transcription factor binding sites",2012,"Nucleic Acids Research",16,10.1093/nar/gks283,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864960989&doi=10.1093%2fnar%2fgks283&partnerID=40&md5=3583ed52b3f327ffc8b6956fceeee842","Transcription factor binding sites (TFBSs) are DNA sequences of 6-15 base pairs. Interaction of these TFBSs with transcription factors (TFs) is largely responsible for most spatiotemporal gene expression patterns. Here, we evaluate to what extent sequence-based prediction of TFBSs can be improved by taking into account the positional dependencies of nucleotides (NPDs) and the nucleotide sequence-dependent structure of DNA. We make use of the random forest algorithm to flexibly exploit both types of information. Results in this study show that both the structural method and the NPD method can be valuable for the prediction of TFBSs. Moreover, their predictive values seem to be complementary, even to the widely used position weight matrix (PWM) method. This led us to combine all three methods. Results obtained for five eukaryotic TFs with different DNA-binding domains show that our method improves classification accuracy for all five eukaryotic TFs compared with other approaches. Additionally, we contrast the results of seven smaller prokaryotic sets with high-quality data and show that with the use of high-quality data we can significantly improve prediction performance. Models developed in this study can be of great use for gaining insight into the mechanisms of TF binding. © The Author(s) 2012.",,"DNA; hypoxia inducible factor 1; protein p53; STAT1 protein; TATA binding protein related factor; transcription factor; transcription factor Sp1; accuracy; article; binding site; classification; DNA sequence; genetic algorithm; genetic procedures; human; intermethod comparison; nonhuman; nucleotide positional dependency method; position weight matrix; predictive value; priority journal; protein DNA binding; random forest; scoring system; Algorithms; Animals; Artificial Intelligence; Binding Sites; DNA; Humans; Mice; Nucleotides; Position-Specific Scoring Matrices; Rats; Sequence Analysis, DNA; Transcription Factors",Article,Scopus,2-s2.0-84864960989
"Zhang Y., Ng S.T.","An ant colony system based decision support system for construction time-cost optimization",2012,"Journal of Civil Engineering and Management",16,10.3846/13923730.2012.704164,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866362979&doi=10.3846%2f13923730.2012.704164&partnerID=40&md5=492c9491b9a4f66599bf5e44b0a8c355","Time and cost are the two most important factors to be considered in every construction project. In order to maximize the profit, both the client and contractor would strive to minimize the project duration and cost concurrently. In the past, most of the research studies related to construction time and cost assumed time to be constant, leaving the analyses based purely on a single objective of cost. Acknowledging this limitation, an evolutionary-based optimization algorithm known as an ant colony system is applied in this study to solve the multi-objective time-cost optimization problems. In this paper, a model is developed using Visual Basic for Application which is integrated with Microsoft Project. Through a test study, the performance of the proposed model is compared against other analytical methods previously used for time-cost modeling. The results show that the model based on the ant colony system techniques can generate better solutions without utilizing excessive computational resources. The model, therefore, provides an efficient means to support planners and managers in making better time-cost decisions efficiently. © 2012 Copyright Vilnius Gediminas Technical University (VGTU) Press Technika.","ant colony system; construction project; Pareto solution; time-cost optimization","Analytical method; Ant colony systems; Computational resources; Construction projects; Construction time; MicroSoft; Model-based OPC; Multi objective; Optimization algorithms; Pareto solution; Project duration; Research studies; Single objective; Test study; Time cost; Time-cost optimization; Visual basic for applications; Algorithms; Artificial intelligence; Costs; Decision support systems; Multiobjective optimization; Profitability; Cost benefit analysis",Article,Scopus,2-s2.0-84866362979
"Fifield D., Hardison N., Ellithorpe J., Stark E., Boneh D., Dingledine R., Porras P.","Evading censorship with browser-based proxies",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,10.1007/978-3-642-31680-7_13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864185968&doi=10.1007%2f978-3-642-31680-7_13&partnerID=40&md5=908e6948e0fbe99cde40499fedf6d9fb","While Internet access to certain sites is blocked in some parts of the world, these restrictions are often circumvented using proxies outside the censored region. Often these proxies are blocked as soon as they are discovered. In this paper we propose a browser-based proxy creation system that generates a large number of short-lived proxies. Clients using the system seamlessly hop from one proxy to the next as these browser-based proxies appear and disappear. We discuss a number of technical challenges that had to be overcome for this system to work and report on its performance and security. We show that browser-based short-lived proxies provide adequate bandwidth for video delivery and argue that blocking them can be challenging. © Springer-Verlag Berlin Heidelberg 2012.",,"Internet access; Technical challenges; Video delivery; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84864185968
"Li C.M., Li Y.","Satisfying versus falsifying in local search for satisfiability (Poster presentation)",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,10.1007/978-3-642-31612-8_43,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864211029&doi=10.1007%2f978-3-642-31612-8_43&partnerID=40&md5=4a1050edec1245f3ef82561e188d777c","During local search, clauses may frequently be satisfied or falsified. Modern SLS algorithms often exploit the falsifying history of clauses to select a variable to flip, together with variable properties such as score and age. The score of a variable x refers to the decrease in the number of unsatisfied clauses if x is flipped. The age of x refers to the number of steps done since the last time when x was flipped. © 2012 Springer-Verlag.",,"Local search; Poster presentations; Satisfiability; Variable property; Artificial intelligence; Formal logic",Conference Paper,Scopus,2-s2.0-84864211029
"Li Y., Ngom A.","A new Kernel non-negative matrix factorization and its application in microarray data analysis",2012,"2012 IEEE Symposium on Computational Intelligence and Computational Biology, CIBCB 2012",16,10.1109/CIBCB.2012.6217254,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864069021&doi=10.1109%2fCIBCB.2012.6217254&partnerID=40&md5=6cb20c0342d247fb0a74cde6cdccb10f","Non-negative factorization (NMF) has been a popular machine learning method for analyzing microarray data. Kernel approaches can capture more non-linear discriminative features than linear ones. In this paper, we propose a novel kernel NMF (KNMF) approach for feature extraction and classification of microarray data. Our approach is also generalized to kernel high-order NMF (HONMF). Extensive experiments on eight microarray datasets show that our approach generally outperforms the traditional NMF and existing KNMFs. Preliminary experiment on a high-order microarray data shows that our KHONMF is a promising approach given a suitable kernel function. © 2012 IEEE.","Classification; Feature Extraction; Kernel Non-Negative Matrix Factorization; Microarray Data","Discriminative features; Feature extraction and classification; High-order; Kernel approaches; Kernel function; Machine learning methods; Microarray data; Microarray data analysis; Microarray data sets; Non-negative factorization; Nonnegative matrix factorization; Artificial intelligence; Bioinformatics; Classification (of information); Data reduction; Feature extraction; Learning systems; Face recognition",Conference Paper,Scopus,2-s2.0-84864069021
"Chekol M.W., Euzenat J., Genevès P., Layaïda N.","SPARQL query containment under RDFS entailment regime",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,10.1007/978-3-642-31365-3_13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863621044&doi=10.1007%2f978-3-642-31365-3_13&partnerID=40&md5=8a1aa1035215bf5d221207355b6f3bba","The problem of SPARQL query containment is defined as determining if the result of one query is included in the result of another for any RDF graph. Query containment is important in many areas, including information integration, query optimization, and reasoning about Entity-Relationship diagrams. We encode this problem into an expressive logic called μ-calculus: where RDF graphs become transition systems, queries and schema axioms become formulas. Thus, the containment problem is reduced to formula satisfiability test. Beyond the logic's expressive power, satisfiability solvers are available for it. Hence, this study allows to exploit these advantages. © 2012 Springer-Verlag.",,"Entity relationship diagrams; Expressive power; Formula satisfiability; Information integration; Query containment; Query optimization; RDF graph; Satisfiability solvers; Transition system; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84863621044
"Gómez L.I., Gómez S.A., Vaisman A.A.","A generic data model and query language for spatiotemporal OLAP cube analysis",2012,"ACM International Conference Proceeding Series",16,10.1145/2247596.2247632,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863524931&doi=10.1145%2f2247596.2247632&partnerID=40&md5=4e2fbe74f193759d45e833a75681fa87","Nowadays, organizations need to use OLAP (On Line Analytical Processing) tools together with geographical information. To support this, the notion of SOLAP (Spatial OLAP) arouse, aimed at exploring spatial data in the same way as OLAP operates over tables. SOLAP however, only accounts for discrete spatial data. More sophisticated GIS-based decision support systems are increasingly being needed, to handle more complex types of data, like continuous fields. Fields describe physical phenomena that change continuously in time and/or space (e.g., temperature). Although many models have been proposed for adding spatial information to OLAP tools, no one allows the user to perceive data as a cube, and analyze any type of spatial data, continuous or discrete, together with typical alphanumerical discrete OLAP data, using only the classic OLAP operators (e.g., Roll-up, Drill-down). In this paper we propose an algebra that operates over data cubes, independently of the underlying data types and physical data representation. That means, in our approach, the final user only sees the typical OLAP operators at the query level. At lower abstraction levels we provide discrete and continuous spatial data support as well as different ways of partitioning the space. We also describe a proof-of-concept implementation to illustrate the ideas presented in the paper. As far as we are aware of, this is the first proposal that allows analyzing discrete and continuous spatiotemporal data and OLAP cubes together, using just the traditional OLAP operations, thus providing a very general framework for spatiotemporal data analysis. © 2012 ACM.","fields; map algebra; OLAP; SOLAP","Abstraction level; Continuous fields; Data cube; Data type; Drill-down; fields; Generic data; Geographical information; Map algebra; OLAP; On-line analytical processing; Physical data; Physical phenomena; Proof of concept; SOLAP; Spatial data; Spatial informations; Spatial OLAP; Spatio-temporal data; Algebra; Artificial intelligence; Decision support systems; Geographic information systems; Query languages; Geometry",Conference Paper,Scopus,2-s2.0-84863524931
"Yetilmezsoy K.","Fuzzy-logic modeling of Fenton's oxidation of anaerobically pretreated poultry manure wastewater",2012,"Environmental Science and Pollution Research",16,10.1007/s11356-011-0726-7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864106695&doi=10.1007%2fs11356-011-0726-7&partnerID=40&md5=05897ce5ea6de8d6546c6629062ff1dc","Purpose: A multiple inputs and multiple outputs (MIMO) fuzzy-logic-based model was proposed to estimate color and chemical oxygen demand (COD) removal efficiencies in the post-treatment of anaerobically pretreated poultry manure wastewater effluent using Fenton's oxidation process. Three main input variables including initial pH, Fe+2, and H2O2 dosages were fuzzified in a new numerical modeling scheme by the use of an artificial intelligence-based approach. Materials and methods: Trapezoidal membership functions with eight levels were conducted for the fuzzy subsets, and a Mamdani-type fuzzy inference system was used to implement a total of 70 rules in the IF-THEN format. The product (prod) and the center of gravity (centroid) methods were applied as the inference operator and defuzzification methods, respectively. Fuzzy-logic predicted results were compared with the outputs of two first-order polynomial regression models derived in the scope of this study. Estimated results were also compared to the multiple regression approach by means of various descriptive statistical indicators, such as root mean-squared error, index of agreement, fractional variance, proportion of systematic error, etc. Results and discussion: Results of the statistical analysis clearly revealed that, compared to conventional regression models, the proposed MIMO fuzzy-logic model produced very smaller deviations and demonstrated a superior predictive performance on forecasting of color and COD removal efficiencies with satisfactory determination coefficients over 0. 98. Conclusions: Due to high capability of the fuzzy-logic methodology in capturing the non-linear interactions, it was demonstrated that a complex dynamic system, such as Fenton's oxidation, could be easily modeled. © 2012 Springer-Verlag.","Anaerobic digestion; COD removal; Fenton's oxidation; Fuzzy-logic; Poultry manure wastewater; Regression model","hydrogen peroxide; iron; anoxic conditions; artificial intelligence; chemical oxygen demand; color; fuzzy mathematics; manure; oxidation; poultry; regression analysis; waste treatment; wastewater; anaerobic growth; animal; article; chemistry; fuzzy logic; manure; methodology; oxidation reduction reaction; pH; poultry; sewage; theoretical model; Anaerobiosis; Animals; Fuzzy Logic; Hydrogen Peroxide; Hydrogen-Ion Concentration; Iron; Manure; Models, Theoretical; Oxidation-Reduction; Poultry; Waste Disposal, Fluid",Article,Scopus,2-s2.0-84864106695
"Barbucha D.","Search modes for the cooperative multi-agent system solving the vehicle routing problem",2012,"Neurocomputing",16,10.1016/j.neucom.2011.07.032,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860242268&doi=10.1016%2fj.neucom.2011.07.032&partnerID=40&md5=2aae3d810ed1539355ec489c563e1b44","Cooperation as a problem-solving strategy is widely used to build methods addressing complex hard optimization problems. It involves a set of highly autonomous programs (agents), each implementing a particular solution method, and a cooperation scheme combining these autonomous programs into a single problem-solving strategy. Possible form of such cooperation may be based, for example, on adaptive memory methods, where partial elements of good solutions are stored and next combined to create new complete solutions. Alternative approach is based on central memory, where complete elite solutions are exchanged among various agents and/or heuristics. Moreover, cooperatively solving a task is often combined with learning mechanism, where agents adapt their behavior to the new states of environment during the process of solving the problem.The main goal of the paper is to evaluate to what extent a mode of cooperation (synchronous or asynchronous) between a number of optimization agents cooperating through sharing a central memory influences the quality of solutions while solving instances of the Vehicle Routing Problem. The investigated search modes are evaluated using a dedicated cooperative multi-agent system allowing for various modes of cooperation and with the reinforcement learning mechanism implemented in it. © 2012 Elsevier B.V.","Cooperative problem solving; Heuristics; Multi-agent learning; Multi-agent systems; Vehicle routing problem","Cooperative problem solving; Heuristics; Multi agent system (MAS); Multi-agent learning; Vehicle Routing Problems; Multi agent systems; Network routing; Reinforcement learning; Routing algorithms; Quality control; algorithm; article; artificial intelligence; computer program; cooperative problem solving; information system; machine learning; mathematical computing; priority journal; problem solving; process optimization; statistical analysis; vehicle routing problem",Article,Scopus,2-s2.0-84860242268
"Segura J., Jones P.F., Fernandez-Fuentes N.","A holistic in silico approach to predict functional sites in protein structures",2012,"Bioinformatics",16,10.1093/bioinformatics/bts269,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864151560&doi=10.1093%2fbioinformatics%2fbts269&partnerID=40&md5=698fcb01bdf381c17468fba6b9b0f36d","Motivation: Proteins execute and coordinate cellular functions by interacting with other biomolecules. Among these interactions, protein-protein (including peptide-mediated), protein-DNA and protein-RNA interactions cover a wide range of critical processes and cellular functions. The functional characterization of proteins requires the description and mapping of functional biomolecular interactions and the identification and characterization of functional sites is an important step towards this end.Results: We have developed a novel computational method, Multi-VORFFIP (MV), a tool to predicts protein-, peptide-, DNA- and RNA-binding sites in proteins. MV utilizes a wide range of structural, evolutionary, experimental and energy-based information that is integrated into a common probabilistic framework by means of a Random Forest ensemble classifier. While remaining competitive when compared with current methods, MV is a centralized resource for the prediction of functional sites and is interfaced by a powerful web application tailored to facilitate the use of the method and analysis of predictions to non-expert end-users. © The Author 2012. Published by Oxford University Press. All rights reserved.",,"DNA; protein; RNA; algorithm; article; artificial intelligence; binding site; biology; chemistry; computer program; computer simulation; Internet; methodology; protein analysis; protein tertiary structure; Algorithms; Artificial Intelligence; Binding Sites; Computational Biology; Computer Simulation; DNA; Internet; Protein Interaction Mapping; Protein Structure, Tertiary; Proteins; RNA; Software",Article,Scopus,2-s2.0-84864151560
"Akay A.E., Ertek G., Büyüközkan G.","Analyzing the solutions of DEA through information visualization and data mining techniques: SmartDEA framework",2012,"Expert Systems with Applications",16,10.1016/j.eswa.2012.01.059,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858339895&doi=10.1016%2fj.eswa.2012.01.059&partnerID=40&md5=dd42ce5b332d068639fa1d03c75c17e9","Data envelopment analysis (DEA) has proven to be a useful tool for assessing efficiency or productivity of organizations, which is of vital practical importance in managerial decision making. DEA provides a significant amount of information from which analysts and managers derive insights and guidelines to promote their existing performances. Regarding to this fact, effective and methodologic analysis and interpretation of DEA results are very critical. The main objective of this study is then to develop a general decision support system (DSS) framework to analyze the results of basic DEA models. The paper formally shows how the results of DEA models should be structured so that these solutions can be examined and interpreted by analysts through information visualization and data mining techniques effectively. An innovative and convenient DEA solver, SmartDEA, is designed and developed in accordance with the proposed analysis framework. The developed software provides DEA results which are consistent with the framework and are ready-to-analyze with data mining tools, thanks to their specially designed table-based structures. The developed framework is tested and applied in a real world project for benchmarking the vendors of a leading Turkish automotive company. The results show the effectiveness and the efficacy of the proposed framework. © 2012 Elsevier Ltd. All rights reserved.","Data envelopment analysis (DEA); Data mining; Decision support system framework; Information visualization","Amount of information; Automotive companies; Data Envelopment Analysis (DEA); Data mining techniques; Data-mining tools; DEA models; Decision support system framework; Information visualization; Managerial decision making; Practical importance; Real world projects; Turkishs; Artificial intelligence; Data mining; Decision support systems; Information science; Information systems; Managers; Data envelopment analysis",Article,Scopus,2-s2.0-84858339895
"Bull J.M., Reid F., McDonnell N.","A microbenchmark suite for OpenMP tasks",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,10.1007/978-3-642-30961-8_24,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862208231&doi=10.1007%2f978-3-642-30961-8_24&partnerID=40&md5=eedbc5d7275d3a517d3e0e823784b61f","We present a set of extensions to an existing microbenchmark suite for OpenMP. The new benchmarks measure the overhead of the task construct introduced in the OpenMP 3.0 standard, and associated task synchronisation constructs. We present the results from a variety of compilers and hardware platforms, which demonstrate some significant differences in performance between different OpenMP implementations. © 2012 Springer-Verlag.",,"Hardware platform; Micro-benchmark; Artificial intelligence; Application programming interfaces (API)",Conference Paper,Scopus,2-s2.0-84862208231
"Armando A., Pellegrino G., Carbone R., Merlo A., Balzarotti D.","From model-checking to automated testing of security protocols: Bridging the gap",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,10.1007/978-3-642-30473-6_3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862186686&doi=10.1007%2f978-3-642-30473-6_3&partnerID=40&md5=db26f0e3d203a1734bda0372c8875a0a","Model checkers have been remarkably successful in finding flaws in security protocols. In this paper we present an approach to binding specifications of security protocols to actual implementations and show how it can be effectively used to automatically test implementations against putative attack traces found by the model checker. By using our approach we have been able to automatically detect and reproduce an attack witnessing an authentication flaw in the SAML-based Single Sign-On for Google Apps. © 2012 Springer-Verlag.",,"Automated testing; Automatically test; Model checker; Security protocols; Single sign on; Artificial intelligence; Model checking",Conference Paper,Scopus,2-s2.0-84862186686
"Terboven C., Schmidl D., Cramer T., An Mey D.","Assessing OpenMP tasking implementations on NUMA architectures",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,10.1007/978-3-642-30961-8_14,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862237960&doi=10.1007%2f978-3-642-30961-8_14&partnerID=40&md5=064b7cef236161bb3b0e35de696c100d","The introduction of task-level parallelization promises to raise the level of abstraction compared to thread-centric expression of parallelism. However, tasks might exhibit poor performance on NUMA systems if locality cannot be maintained. In contrast to traditional OpenMP worksharing constructs for which threads can be bound, the behavior of tasks is much less predetermined by the OpenMP specification and implementations have a high degree of freedom implementing task scheduling. Employing different approaches to express task-parallelism, namely the single-producer and parallel-producer patterns with different data initialization strategies, we compare the behavior and quality of OpenMP implementations with task-parallel codes on NUMA architectures. For the programmer, we propose recipies to express parallelism with tasks allowing to preserve data locality while optimizing the degree of parallelism. Our proposals are evaluated on reasonably large NUMA systems with both important application kernels as well as a real-world simulation code. © 2012 Springer-Verlag.",,"Data locality; Degree of parallelism; High Degree of Freedom; Level of abstraction; NUMA systems; Parallelizations; Poor performance; Real-world simulation; Task parallelism; Task-scheduling; Work-sharing; Artificial intelligence; Application programming interfaces (API)",Conference Paper,Scopus,2-s2.0-84862237960
"Calvert G.A., Brammer M.J.","Predicting consumer behavior: Using novel mind-reading approaches",2012,"IEEE Pulse",16,10.1109/MPUL.2012.2189167,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862121029&doi=10.1109%2fMPUL.2012.2189167&partnerID=40&md5=e3f308e3b0d61b5255099ca3012aaa73","Advances in machine learning as applied to functional magnetic resonance imaging (fMRI) data offer the possibility of pretesting and classifying marketing communications using unbiased pattern recognition algorithms. By using these algorithms to analyze brain responses to brands, products, or existing marketing communications that either failed or succeeded in the marketplace and identifying the patterns of brain activity that characterize success or failure, future planned campaigns or new products can now be pretested to determine how well the resulting brain responses match the desired (successful) pattern of brain activity without the need for verbal feedback. This major advance in signal processing is poised to revolutionize the application of these brain-imaging techniques in the marketing sector by offering greater accuracy of prediction in terms of consumer acceptance of new brands, products, and campaigns at a speed that makes them accessible as routine pretesting tools that will clearly demonstrate return on investment. © 2012 IEEE.",,"Brain activity; Brain response; Consumer acceptance; Functional magnetic resonance imaging; Marketing communications; Marketing sectors; New product; Pattern recognition algorithms; Return on investments; Algorithms; Brain; Magnetic resonance imaging; Pattern recognition; Profitability; Signal processing; Marketing; algorithm; article; artificial intelligence; automated pattern recognition; consumer; decision making; human; marketing; methodology; nuclear magnetic resonance imaging; physiology; Algorithms; Artificial Intelligence; Choice Behavior; Consumer Satisfaction; Humans; Magnetic Resonance Imaging; Marketing; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84862121029
"Ramanna S.C., Chatterjee S., Sarkar P.","Variants of waters' dual system primitives using asymmetric pairings (extended abstract)",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,10.1007/978-3-642-30057-8_18,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861710747&doi=10.1007%2f978-3-642-30057-8_18&partnerID=40&md5=1e1119e019e9f49e4733c2782527b027","Waters, in 2009, introduced an important technique, called dual system encryption, to construct identity-based encryption (IBE) and related schemes. The resulting IBE scheme was described in the setting of symmetric pairing. A key feature of the construction is the presence of random tags in the ciphertext and decryption key. Later work by Lewko and Waters removed the tags and proceeding through composite-order pairings led to a more efficient dual system IBE scheme using asymmetric pairings whose security is based on non-standard but static assumptions. In this work, we have systematically simplified Waters 2009 IBE scheme in the setting of asymmetric pairing. The simplifications retain tags used in the original description. This leads to several variants, the first one of which is based on standard assumptions and in comparison to Waters' original scheme reduces ciphertexts and keys by two elements each. Going through several stages of simplifications, we finally obtain a simple scheme whose security can be based on two standard assumptions and a natural and minimal extension of the decision Diffie-Hellman problem for asymmetric pairing groups. The scheme itself is also minimal in the sense that apart from the tags, both encryption and key generation use exactly one randomiser each. This final scheme is more efficient than both the previous dual system IBE scheme in the asymmetric setting due to Lewko and Waters and the more recent dual system IBE scheme due to Lewko. We extend the IBE scheme to hierarchical IBE (HIBE) and broadcast encryption (BE) schemes. Both primitives are secure in their respective full models and have better efficiencies compared to previously known schemes offering the same level and type of security. © 2012 International Association for Cryptologic Research.","asymmetric pairing; dual system encryption; identity-based encryption","asymmetric pairing; Broadcast encryption; Ciphertexts; Decision Diffie-Hellman problem; Dual system; Extended abstracts; Full model; Identity Based Encryption; Key feature; Key generation; Standard assumptions; Asymmetric pairing; Broadcast encryption; Decision Diffie-Hellman problem; Dual system encryptions; Extended abstracts; Identity Based Encryption; Key generation; Standard assumptions; Artificial intelligence; Public key cryptography; Security of data; Public key cryptography; Cryptography",Conference Paper,Scopus,2-s2.0-84861710747
"Vermolen S.D., Wachsmuth G., Visser E.","Reconstructing complex metamodel evolution",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,10.1007/978-3-642-28830-2_11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861616525&doi=10.1007%2f978-3-642-28830-2_11&partnerID=40&md5=36c029df2783f80fa13931a02aea272a","Metamodel evolution requires model migration. To correctly migrate models, evolution needs to be made explicit. Manually describing evolution is error-prone and redundant. Metamodel matching offers a solution by automatically detecting evolution, but is only capable of detecting primitive evolution steps. In practice, primitive evolution steps are jointly applied to form a complex evolution step, which has the same effect on a metamodel as the sum of its parts, yet generally has a different effect in migration. Detection of complex evolution is therefore needed. In this paper, we present an approach to reconstruct complex evolution between two metamodel versions, using a matching result as input. It supports operator dependencies and mixed, overlapping, and incorrectly ordered complex operator components. It also supports interference between operators, where the effect of one operator is partially or completely hidden from the target metamodel by other operators. © 2012 Springer-Verlag.",,"Complex evolutions; Different effects; Error prones; Meta model; Meta-model matching; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84861616525
"Zhou X., Wang S., Chen H., Hara T., Yokoyama R., Kanematsu M., Fujita H.","Automatic localization of solid organs on 3D CT images by a collaborative majority voting decision based on ensemble learning",2012,"Computerized Medical Imaging and Graphics",16,10.1016/j.compmedimag.2011.12.004,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862776760&doi=10.1016%2fj.compmedimag.2011.12.004&partnerID=40&md5=f10798410117897ddc6b8b2f876aa272","Purpose: Organ segmentation is an essential step in the development of computer-aided diagnosis/surgery systems based on computed tomography (CT) images. A universal segmentation approach/scheme that can adapt to different organ segmentations can substantially increase the efficiency and robustness of such computer-aided systems. However, this is a very challenging problem. An initial determination of the approximate position and range of a target organ in CT images is prerequisite for precise organ segmentations. In this study, we have proposed a universal approach that enables automatic localization of the approximate position and range of different solid organs in the torso region on three-dimensional (3D) CT scans. Methods: The location of a target organ in a 3D CT scan is presented as a 3D rectangle that bounds the organ region tightly and accurately. Our goal was to automatically and effectively detect such a target organ-specific 3D rectangle. In our proposed approach, multiple 2D detectors are trained using ensemble learning and their outputs are combined using a collaborative majority voting in 3D to accomplish the robust organ localizations. Results: We applied this approach to localize the heart, liver, spleen, left-kidney, and right-kidney regions independently using a CT image database that includes 660 torso CT scans. In the experiment, we manually labeled the abovementioned target organs from 101 3D CT scans as training samples and used our proposed approach to localize the 5 kinds of target organs separately on the remaining 559 torso CT scans. The localization results of each organ were evaluated quantitatively by comparing with the corresponding ground truths obtained from the target organs that were manually labeled by human operators. Experimental results showed that success rates of such organ localizations were distributed from 99% to 75% of the 559 test CT scans. We compared the performance of our approach with an atlas-based approach. The errors of the detected organ-center-positions in the successful CT scans by our approach had a mean value of 5.14 voxels, and those errors were much smaller than the results (mean value about 25 voxels) from the atlas-based approach. The potential usefulness of the proposed organ localization was also shown in a preliminary investigation of left kidney segmentation in non-contrast CT images. Conclusions: We proposed an approach to accomplish automatic localizations of major solid organs on torso CT scans. The accuracy of localizations, flexibility of localizations of different organs, robustness to contrast and non-contrast CT images, and normal and abnormal patient cases, and computing efficiency were validated on the basis of a large number of torso CT scans. © 2012 Elsevier Ltd.","3D CT torso images; Collaborative majority voting; Ensemble learning; Inner organ localization","2D detector; 3D CT torso images; Automatic localization; Computed Tomography; Computer-aided systems; Computing efficiency; CT Image; Ensemble learning; Ground truth; Human operator; Inner organs; Kidney segmentation; Majority voting; Mean values; Organ segmentation; Patient case; Target organs; Training sample; Universal approach; Errors; Image segmentation; Three dimensional; Computerized tomography; adult; article; atlas; computer assisted tomography; data base; female; heart; human; kidney; learning; liver; male; normal human; organ; priority journal; quantitative analysis; target organ; three dimensional imaging; tissue slice; trunk; validation study; Adult; Aged; Aged, 80 and over; Algorithms; Artificial Intelligence; Decision Support Techniques; Female; Heart; Humans; Imaging, Three-Dimensional; Kidney; Liver; Male; Middle Aged; Pattern Recognition, Automated; Radiographic Image Interpretation, Computer-Assisted; Spleen; Tomography, X-Ray Computed",Article,Scopus,2-s2.0-84862776760
"Vellido A., Romero E., Julià-Sapé M., Majós C., Moreno-Torres À., Pujol J., Arús C.","Robust discrimination of glioblastomas from metastatic brain tumors on the basis of single-voxel 1H MRS",2012,"NMR in Biomedicine",16,10.1002/nbm.1797,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860618323&doi=10.1002%2fnbm.1797&partnerID=40&md5=18c609e5cb4cc0c9ae2d3e75b32f5ee5","This article investigates methods for the accurate and robust differentiation of metastases from glioblastomas on the basis of single-voxel 1H MRS information. Single-voxel 1H MR spectra from a total of 109 patients (78 glioblastomas and 31 metastases) from the multicenter, international INTERPRET database, plus a test set of 40 patients (30 glioblastomas and 10 metastases) from three different centers in the Barcelona (Spain) metropolitan area, were analyzed using a robust method for feature (spectral frequency) selection coupled with a linear-in-the-parameters single-layer perceptron classifier. For the test set, a parsimonious selection of five frequencies yielded an area under the receiver operating characteristic curve of 0.86, and an area under the convex hull of the receiver operating characteristic curve of 0.91. Moreover, these accurate results for the discrimination between glioblastomas and metastases were obtained using a small number of frequencies that are amenable to metabolic interpretation, which should ease their use as diagnostic markers. Importantly, the prediction can be expressed as a simple formula based on a linear combination of these frequencies. As a result, new cases could be straightforwardly predicted by integrating this formula into a computer-based medical decision support system. This work also shows that the combination of spectra acquired at different TEs (short TE, 20-32ms; long TE, 135-144ms) is key to the successful discrimination between glioblastomas and metastases from single-voxel 1H MRS. © 2011 John Wiley &amp; Sons, Ltd.","Feature selection; Glioblastomas; High-grade malignant tumors; Medical decision support system; Metastases; Pattern recognition; SV 1H MRS","Glioblastomas; Malignant tumors; Medical decision support system; Metastases; SV <sup>1</sup>H MRS; Artificial intelligence; Decision support systems; Feature extraction; Pattern recognition; Tumors; Pathology; article; artificial neural network; brain metastasis; decision support system; diagnostic accuracy; false positive result; glioblastoma; human; major clinical study; medical decision making; perceptron; priority journal; proton nuclear magnetic resonance; sensitivity and specificity; Algorithms; Brain Chemistry; Brain Neoplasms; Diagnosis, Computer-Assisted; Glioblastoma; Humans; Magnetic Resonance Spectroscopy; Pattern Recognition, Automated; Protons; Reproducibility of Results; Sensitivity and Specificity; Tumor Markers, Biological",Article,Scopus,2-s2.0-84860618323
"Malitsky Y., Sellmann M.","Instance-specific algorithm configuration as a method for non-model-based portfolio generation",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,10.1007/978-3-642-29828-8_16,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861443940&doi=10.1007%2f978-3-642-29828-8_16&partnerID=40&md5=ef7d6dea1f7007a72ae6838140507c37","Instance-specific algorithm configuration generalizes both instance-oblivious algorithm tuning as well as algorithm portfolio generation. ISAC is a recently proposed non-model-based approach for tuning solver parameters dependent on the specific instance that needs to be solved. While ISAC has been compared with instance-oblivious algorithm tuning systems before, to date a comparison with portfolio generators and other instance-specific algorithm configurators is crucially missing. In this paper, among others, we provide a comparison with SATzilla, as well as three other algorithm configurators: Hydra, DCM and ArgoSmart. Our experimental comparison shows that non-model-based ISAC significantly outperforms prior state-of-the-art algorithm selectors and configurators. The following study was the foundation for the best sequential portfolio at the 2011 SAT Competition. © 2012 Springer-Verlag.",,"Experimental comparison; State-of-the-art algorithms; Tuning system; Artificial intelligence; Combinatorial optimization; Computer programming; Constraint theory; Algorithms",Conference Paper,Scopus,2-s2.0-84861443940
"Knoop J., Kovács L., Zwirchmayr J.","Symbolic loop bound computation for WCET analysis",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,10.1007/978-3-642-29709-0_20,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862071463&doi=10.1007%2f978-3-642-29709-0_20&partnerID=40&md5=2b6ccae9e6e47951d0e92e74a2d62bf7","We present an automatic method for computing tight upper bounds on the iteration number of special classes of program loops. These upper bounds are further used in the WCET analysis of programs. To do so, we refine program flows using SMT reasoning and rewrite multi-path loops into single-path ones. Single-path loops are further translated into a set of recurrence relations over program variables. Recurrence relations are solved and iteration bounds of program loops are derived from the computed closed forms. For solving recurrences we deploy a pattern-based recurrence solving algorithm and compute closed forms only for a restricted class of recurrence equations. However, in practice, these recurrences describe the behavior of a large set of program loops. Our technique is implemented in the r-TuBound tool and was successfully tried out on a number of challenging WCET benchmarks. © 2012 Springer-Verlag Berlin Heidelberg.",,"Automatic method; Closed form; Iteration bound; Iteration numbers; Program flow; Program variables; Recurrence equation; Recurrence relations; Single path; Solving algorithm; Special class; Upper Bound; Artificial intelligence; Information science",Conference Paper,Scopus,2-s2.0-84862071463
"Zhang N., Behera P.K.","Solar radiation prediction based on recurrent neural networks trained by Levenberg-Marquardt backpropagation learning algorithm",2012,"2012 IEEE PES Innovative Smart Grid Technologies, ISGT 2012",16,10.1109/ISGT.2012.6175757,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860860444&doi=10.1109%2fISGT.2012.6175757&partnerID=40&md5=504eda9c87c1a9b0700ab53c12723a91","In response to the growing concern over the use of fossil fuels, renewable energy industries have been significant economic drivers in many parts of the United States. In the recent years there is a strong growth in solar power generation industries that requires prediction of solar energy to develop highly efficient stand-alone photovoltaic systems as well as hybrid power systems. In order to accomplish the goal, we propose a predictive model that is based on recurrent neural networks trained with the Levenberg-Marquardt backpropagation learning algorithm to forecast the solar radiation using the past solar radiation and solar energy. This computational intelligence modeling tool explored the impact of solar radiation and solar energy in forecasting reliable long-run solar energy. Based on the excellent experimental results including the mean squared error analysis, error autocorrelation function analysis, regression analysis, and time series response, it demonstrated that the proposed neural network structure and the learning algorithm could be very useful in training the recurrent neural network for the solar radiation prediction. © 2012 IEEE.","backpropagation learning algorithm; neural networks; Solar radiation prediction; time series prediction","Autocorrelation function analysis; Backpropagation learning algorithm; Hybrid power systems; Levenberg-Marquardt; Mean squared error; Neural network structures; Power generation industries; Predictive models; Radiation prediction; Renewable Energy industries; Stand alone photovoltaic system; Strong growth; Time series prediction; Artificial intelligence; Error analysis; Forecasting; Fossil fuels; Learning algorithms; Neural networks; Recurrent neural networks; Regression analysis; Smart power grids; Solar power generation; Sun; Time series; Solar radiation",Conference Paper,Scopus,2-s2.0-84860860444
"Fischer J., Gagie T., Kopelowitz T., Lewenstein M., Mäkinen V., Salmela L., Välimäki N.","Forbidden patterns",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,10.1007/978-3-642-29344-3_28,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860800524&doi=10.1007%2f978-3-642-29344-3_28&partnerID=40&md5=d917300d2bf57e70822849ca0979bbab","We consider the problem of indexing a collection of documents (a.k.a. strings) of total length n such that the following kind of queries are supported: given two patterns P + and P -, list all documents containing P + but not P -. This is a natural extension of the classic problem of document listing as considered by Muthukrishnan [SODA'02], where only the positive pattern P + is given. Our main solution is an index of size O(n 3/2) bits that supports queries in O(|P +| + |P -| + n match + √n) time. © 2012 Springer-Verlag Berlin Heidelberg.",,"Collection of documents; Forbidden pattern; Natural extension; Total length; Artificial intelligence; Information science",Conference Paper,Scopus,2-s2.0-84860800524
"El Dor A., Clerc M., Siarry P.","Hybridization of differential evolution and particle swarm optimization in a new algorithm: DEPSO-2S",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,10.1007/978-3-642-29353-5_7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860697167&doi=10.1007%2f978-3-642-29353-5_7&partnerID=40&md5=b1e21cfb6ccb79f6b99ff69a48b008ff","PSO-2S is a multi-swarm PSO algorithm using charged particles in a partitioned search space for continuous optimization problems. This algorithm uses two kinds of swarms, a main one that gathers the best particles of auxiliary ones. In this paper, we present a new variant of PSO-2S, called DEPSO-2S, which is a hybridization of DE and PSO. DE was used, in this variant, to construct the main swarm. We analyze the performance of the proposed approach on seven real problems. The obtained results show the efficiency of the proposed algorithm. © 2012 Springer-Verlag.","Differential evolution; Global optimization; Multi-swarm; Particle swarm optimization; Partitioned search space","Continuous optimization problems; Differential Evolution; Multi-swarms; PSO algorithms; Real problems; Search spaces; Algorithms; Artificial intelligence; Global optimization; Soft computing; Particle swarm optimization (PSO)",Conference Paper,Scopus,2-s2.0-84860697167
"Huang Z., Lu X., Duan H., Zhao C.","Collaboration-based medical knowledge recommendation",2012,"Artificial Intelligence in Medicine",16,10.1016/j.artmed.2011.10.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858859776&doi=10.1016%2fj.artmed.2011.10.002&partnerID=40&md5=21efc02fcdbdc9be08c77e2ee3cf4fc4","Purpose: Clinicians rely on a large amount of medical knowledge when performing clinical work. In clinical environment, clinical organizations must exploit effective methods of seeking and recommending appropriate medical knowledge in order to help clinicians perform their work. Method: Aiming at supporting medical knowledge search more accurately and realistically, this paper proposes a collaboration-based medical knowledge recommendation approach. In particular, the proposed approach generates clinician trust profile based on the measure of trust factors implicitly from clinicians' past rating behaviors on knowledge items. And then the generated clinician trust profile is incorporated into collaborative filtering techniques to improve the quality of medical knowledge recommendation, to solve the information-overload problem by suggesting knowledge items of interest to clinicians. Results: Two case studies are conducted at Zhejiang Huzhou Central Hospital of China. One case study is about the drug recommendation hold in the endocrinology department of the hospital. The experimental dataset records 16 clinicians' drug prescribing tracks in six months. This case study shows a proof-of-concept of the proposed approach. The other case study addresses the problem of radiological computed tomography (CT)-scan report recommendation. In particular, 30 pieces of CT-scan examinational reports about cerebral hemorrhage patients are collected from electronic medical record systems of the hospital, and are evaluated and rated by 19 radiologists of the radiology department and 7 clinicians of the neurology department, respectively. This case study provides some confidence the proposed approach will scale up. Conclusion: The experimental results show that the proposed approach performs well in recommending medical knowledge items of interest to clinicians, which indicates that the proposed approach is feasible in clinical practice. © 2011 Elsevier B.V.","Collaborative filtering; Knowledge retrieval; Medical knowledge recommendation; Trust","Central hospitals; Cerebral hemorrhage; Clinical environments; Clinical practices; Clinical work; Collaborative filtering; Collaborative filtering techniques; Computed Tomography; CT-scan; Data sets; Electronic medical record system; Knowledge items; Knowledge retrieval; Medical knowledge; Proof of concept; Scale-up; Trust; Trust factor; Zhejiang; Hospitals; Medical computing; Radiology; Rating; Research; Computerized tomography; insulin; metformin; nateglinide; sulfonylurea; article; brain hemorrhage; case study; China; clinical practice; computer assisted tomography; electronic medical record; endocrinology; feasibility study; hospital; human; medical information system; medical practice; medical record; non insulin dependent diabetes mellitus; physician; prescription; priority journal; radiologist; radiology; trust; Artificial Intelligence; Computer-Assisted Instruction; Cooperative Behavior; Humans; Medical Records; Medical Records Systems, Computerized; Problem-Based Learning; Software; Total Quality Management",Article,Scopus,2-s2.0-84858859776
"De Ferrari L., Aitken S., van Hemert J., Goryanin I.","EnzML: Multi-label prediction of enzyme classes using InterPro signatures",2012,"BMC Bioinformatics",16,10.1186/1471-2105-13-61,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860154266&doi=10.1186%2f1471-2105-13-61&partnerID=40&md5=054b0a98964a1853395bc35ca42e9a83","Background: Manual annotation of enzymatic functions cannot keep up with automatic genome sequencing. In this work we explore the capacity of InterPro sequence signatures to automatically predict enzymatic function.Results: We present EnzML, a multi-label classification method that can efficiently account also for proteins with multiple enzymatic functions: 50,000 in UniProt. EnzML was evaluated using a standard set of 300,747 proteins for which the manually curated Swiss-Prot and KEGG databases have agreeing Enzyme Commission (EC) annotations. EnzML achieved more than 98% subset accuracy (exact match of all correct Enzyme Commission classes of a protein) for the entire dataset and between 87 and 97% subset accuracy in reannotating eight entire proteomes: human, mouse, rat, mouse-ear cress, fruit fly, the S. pombe yeast, the E. coli bacterium and the M. jannaschii archaebacterium. To understand the role played by the dataset size, we compared the cross-evaluation results of smaller datasets, either constructed at random or from specific taxonomic domains such as archaea, bacteria, fungi, invertebrates, plants and vertebrates. The results were confirmed even when the redundancy in the dataset was reduced using UniRef100, UniRef90 or UniRef50 clusters.Conclusions: InterPro signatures are a compact and powerful attribute space for the prediction of enzymatic function. This representation makes multi-label machine learning feasible in reasonable time (30 minutes to train on 300,747 instances with 10,852 attributes and 2,201 class values) using the Mulan Binary Relevance Nearest Neighbours algorithm implementation (BR-kNN). © 2012 Ferrari et al.; licensee BioMed Central Ltd.",,"Algorithm implementation; Archaea; Binary relevances; Data set size; E. coli; Enzymatic functions; Enzyme commissions; Fruit flies; Genome sequencing; Manual annotation; Multi-label; Multi-label classifications; Nearest neighbour; Proteomes; SWISS-PROT; Uniprot; Enzymes; Escherichia coli; Mammals; Proteins; Forecasting; Arabidopsis thaliana; Archaea; Bacteria (microorganisms); Escherichia coli; Fungi; Invertebrata; Rattus; Schizosaccharomyces pombe; Vertebrata; enzyme; proteome; algorithm; animal; Arabidopsis; article; artificial intelligence; biology; classification; computer program; Drosophila; Escherichia coli; genetics; human; Methanococcus; methodology; mouse; protein database; rat; Schizosaccharomyces; Algorithms; Animals; Arabidopsis; Artificial Intelligence; Computational Biology; Databases, Protein; Drosophila; Enzymes; Escherichia coli; Humans; Methanococcus; Mice; Proteome; Rats; Schizosaccharomyces; Software",Article,Scopus,2-s2.0-84860154266
"Datta K., Rathi G., Sengupta I., Rahaman H.","Synthesis of reversible circuits using heuristic search method",2012,"Proceedings of the IEEE International Conference on VLSI Design",16,10.1109/VLSID.2012.92,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859906966&doi=10.1109%2fVLSID.2012.92&partnerID=40&md5=9ff4ed90180626b31dbb72c6bab66a48","Reversible circuits are of vital importance in many applications involving low power design. One of the principle areas where reversible circuits play great role is quantum computing. One of the foremost requirements of quantum computation is that it requires all the circuits that are used should be reversible in nature. Reversible circuit is one which maps an individual input vector to a singular output vector. Because of its application in many areas including quantum computing, many synthesis approaches have been developed. In this paper we focus on a synthesis approach which is based on permutation theory and heuristic search. An artificial intelligence based search technique A* is used to find near optimal solutions. Experimental results demonstrate that the proposed approach provides solutions within a very reasonable span of time. © 2012 IEEE.",,"Heuristic search; Heuristic search methods; Input vector; Low-power design; Near-optimal solutions; Output vectors; Quantum Computing; Reversible circuits; Search technique; Artificial intelligence; Computational linguistics; Electric power supplies to apparatus; Heuristic methods; Low power electronics; Modular robots; Quantum computers; Quantum theory; Embedded systems",Conference Paper,Scopus,2-s2.0-84859906966
"Toni F., Torroni P.","Bottom-up argumentation",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,10.1007/978-3-642-29184-5_16,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859560861&doi=10.1007%2f978-3-642-29184-5_16&partnerID=40&md5=a28860a592199c36165519e3b44ec9ff","Online social platforms, e-commerce sites and technical fora support the unfolding of informal exchanges, e.g. debates or discussions, that may be topic-driven or serendipitous. We outline a methodology for analysing these exchanges in computational argumentation terms, thus allowing a formal assessment of the dialectical validity of the positions debated in or emerging from the exchanges. Our methodology allows users to be engaged in this formal analysis and the assessment, within a dynamic process where comments, opinions, objections, as well as links connecting them, can all be contributed by users. © 2012 Springer-Verlag.",,"AS-links; Dynamic process; E-commerce sites; Formal analysis; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84859560861
"Thüm T., Schaefer I., Kuhlemann M., Apel S., Saake G.","Applying design by contract to feature-oriented programming",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,10.1007/978-3-642-28872-2_18,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859118443&doi=10.1007%2f978-3-642-28872-2_18&partnerID=40&md5=04fbf65746344dcdfa4526184d3df237","Feature-oriented programming (FOP) is an extension of ob- ject-oriented programming to support software variability by refining existing classes and methods. In order to increase the reliability of all implemented program variants, we integrate design by contract (DbC) with FOP. DbC is an approach to build reliable object-oriented software by specifying methods with contracts. Contracts are annotations that document and formally specify behavior, and can be used for formal verification of correctness or as test oracles. We present and discuss five approaches to define contracts of methods and their refinements in FOP. Furthermore, we share our insights gained by performing five case studies. This work is a foundation for research on the analysis of feature-oriented programs (e.g., for verifying functional correctness or for detecting feature interactions). © 2012 Springer-Verlag Berlin Heidelberg.",,"Design by contracts; Feature interactions; Feature-oriented; Feature-oriented programming; Formal verifications; Object oriented software; Test oracles; Design by contracts; Feature interactions; Feature-oriented programming; Feature-oriented programming (FOP); Functional correctness; Integrate designs; Object oriented software; Software variabilities; Artificial intelligence; Engineering research; Software engineering; Software engineering; Object oriented programming",Conference Paper,Scopus,2-s2.0-84859118443
"Atig M.F., Bouajjani A., Burckhardt S., Musuvathi M.","What's decidable about weak memory models?",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,10.1007/978-3-642-28869-2_2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859149159&doi=10.1007%2f978-3-642-28869-2_2&partnerID=40&md5=296b35eb33140e00af6eb322a1599a2d","We investigate the decidability of the state reachability problem in finite-state programs running under weak memory models. In [3], we have shown that this problem is decidable for TSO and its extension with the write-to-write order relaxation, but beyond these models nothing is known to be decidable. Moreover, we have shown that relaxing the program order by allowing reads or writes to overtake reads leads to undecidability. In this paper, we refine these results by sharpening the (un)decidability frontiers on both sides. On the positive side, we introduce a new memory model NSW (for non-speculative writes) that extends TSO with the write-to-write relaxation, the read-to-read relaxation, and support for partial fences. We present a backtrack-free operational model for NSW, and prove that it does not allow causal cycles (thus barring pathological out-of-thin-air effects). On the negative side, we show that adding the read-to-write relaxation to TSO causes undecidability, and that adding non-atomic writes to NSW also causes undecidability. Our results establish that NSW is the first known hardware-centric memory model that is relaxed enough to permit both delayed execution of writes and early execution of reads for which the reachability problem is decidable. © 2012 Springer-Verlag.",,"Finite state program; Memory models; Operational model; Order relaxation; Program order; Reachability problem; State reachability; Undecidability; Backtrack free; Finite state program; Memory modeling; Operational model; Order relaxation; Reachability problem; State reachability; Weak memory models; Problem oriented languages; Artificial intelligence; Computer science; Computers; Computability and decidability; Computability and decidability",Conference Paper,Scopus,2-s2.0-84859149159
"Samadzadegan F., Hasani H., Schenk T.","Simultaneous feature selection and SVM parameter determination in classification of hyperspectral imagery using Ant Colony Optimization",2012,"Canadian Journal of Remote Sensing",16,10.5589/m12-022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865742501&doi=10.5589%2fm12-022&partnerID=40&md5=f038e19e1ad8feab194be91586ad5b27","Hyperspectral remote sensing imagery, due to its rich source of spectral information, provides an efficient tool for land cover classifications in complex geographical areas. However, the high-dimensional space of this imagery poses two important challenges in the classification process: the Hughes phenomena and the existence of relevant and redundant features. The robustness of Support Vector Machines (SVM) in high-dimensional space makes them an efficient tool for classifying hyperspectral imagery. However, optimum SVM parameter determination and optimum feature selection are the two optimization issues that strongly effect SVM performance. Traditional optimization algorithms can discover optimum solutions in a limited search space with one local optimum. Nevertheless, in high-dimensional space traditional optimization algorithms usually get trapped in a local optimum, therefore it is necessary to apply meta-heuristic optimization algorithms to obtain near-global optimum solutions. This study evaluates the potential of Ant Colony Optimization (ACO) for determining SVM parameters and selecting features. Results obtained from AVIRIS and ROSIS hyperspectral datasets demonstrate the superior performance of SVM, achieved by simultaneously optimizing SVM parameters and subsets of the input feature. For comparison, the evaluation is also performed by applying it to other meta-heuristic optimization algorithms such as simulated annealing, tabu search, and genetic algorithm. The results demonstrate a better performance of the ACO-based algorithm in regards to improving the classification accuracy and decreasing the size of selected feature subsets. © 2012 CASI.",,"Ant Colony Optimization (ACO); Classification accuracy; Classification process; Data sets; Feature subset; Geographical area; High dimensional spaces; HyperSpectral; Hyperspectral imagery; Hyperspectral remote sensing; Input features; Land cover classification; Local optima; Metaheuristic; Optimization algorithms; Optimum solution; Parameter determination; Redundant features; Search spaces; Spectral information; Artificial intelligence; Constrained optimization; Heuristic algorithms; Remote sensing; Simulated annealing; Tabu search; Support vector machines",Article,Scopus,2-s2.0-84865742501
"Girish K.P., John S.J.","Multiset topologies induced by multiset relations",2012,"Information Sciences",16,10.1016/j.ins.2011.11.023,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855471098&doi=10.1016%2fj.ins.2011.11.023&partnerID=40&md5=cec7ce8a165a609ba1f63bdad0201be2","Multiset topology is a collection of multisets which satisfies the axioms of topology. In this paper, multiset topologies are obtained using binary multiset relations. The relationship between multiset topologies is investigated and some of the properties of these multiset topologies are proved. The quasi-discrete multiset topology is obtained from a symmetric multiset relation instead of an equivalence multiset relation and several examples are given to indicate counter connections. Finally, the rough multiset is introduced and the multiset topology induced by a multiset relation is used to generalize the rough multiset concept. © 2011 Elsevier Inc. All rights reserved.","Binary multiset relation; Closure multiset space; Interior multiset space; Multiset topologies; Multisets; Rough multisets","Multi-sets; Multiset; Artificial intelligence; Software engineering; Topology",Article,Scopus,2-s2.0-84855471098
"Zhang D., Song H., Xu H., Wu W., Gao S., Hong B.","An N200 speller integrating the spatial profile for the detection of the non-control state",2012,"Journal of Neural Engineering",16,10.1088/1741-2560/9/2/026016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859132942&doi=10.1088%2f1741-2560%2f9%2f2%2f026016&partnerID=40&md5=ed200c7bcc22e78473998ea7772ed423","The N200 speller is a recently developed non-flashing visual brain-computer interface (BCI) paradigm utilizing the overt attention modulation effects on motion-onset visual evoked potentials (mVEP). In this study, a novel algorithm is proposed and applied in an online N200 speller. The proposed algorithm integrates the spatial information of the speller matrix to provide a more precise description of the mVEP response patterns, which is defined as the 'spatial profile'. More importantly, only control state data are used in the algorithm to train a classifier that nonetheless can detect the non-control state effectively. Compared to an algorithm with similar structure but not using the spatial profile information, the proposed algorithm shows significantly higher performance for the recognition of the non-control state while achieving a comparable performance for classifying different control states. Offline and online classification results show that the proposed N200 speller is a promising step toward a practical, online non-flashing BCI system for daily use. © 2012 IOP Publishing Ltd.",,"Brain-computer interfaces (BCI); Control state; Daily use; Modulation effects; Novel algorithm; Offline; On-line classification; Overt attention; Response patterns; Spatial informations; Spatial profiles; Visual evoked potential; Brain computer interface; Electrophysiology; Fermi level; Algorithms; adult; algorithm; article; brain computer interface; evoked visual response; female; human; human experiment; male; normal human; online system; priority journal; validation process; visual stimulation; artificial intelligence; brain; computer interface; electroencephalography; movement (physiology); photostimulation; physiology; receiver operating characteristic; reproducibility; Adult; Algorithms; Artificial Intelligence; Brain; Electroencephalography; Evoked Potentials, Visual; Female; Humans; Male; Movement; Online Systems; Photic Stimulation; Reproducibility of Results; ROC Curve; User-Computer Interface; Young Adult",Article,Scopus,2-s2.0-84859132942
"Batzios A., Mitkas P.A.","WebOWL: A Semantic Web search engine development experiment",2012,"Expert Systems with Applications",16,10.1016/j.eswa.2011.11.034,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855885815&doi=10.1016%2fj.eswa.2011.11.034&partnerID=40&md5=51de7f8e11c98068cca8e369d50c9172","This paper presents WebOWL, an experiment in using the latest technologies to develop a Semantic Web search engine. WebOWL consists of a community of intelligent agents, acting as crawlers, that are able to discover and learn the locations of Semantic Web neighborhoods on the Web, a semantic database to store data from different ontologies, a query mechanism that supports semantic queries in OWL, and a ranking algorithm that determines the order of the returned results based on the semantic relationships of classes and individuals. The system has been implemented using Jade, Jena and the db4o object database engine and has successfully stored over one million OWL classes, individuals and properties. © 2011 Elsevier Ltd. All rights reserved.","OWL database; OWL querying; Semantic querying; Semantic search; Semantic Web","Latest technology; Object database; OWL database; OWL querying; Query mechanisms; Ranking algorithm; Semantic query; Semantic relationships; Semantic search; Semantic web search; Artificial intelligence; Experiments; Information retrieval; Intelligent agents; Ontology; Query languages; Search engines; Semantic Web; Silicate minerals; World Wide Web",Article,Scopus,2-s2.0-84855885815
"Sarabia D., de Prada C., Gómez E., Gutierrez G., Cristea S., Sola J.M., Gonzalez R.","Data reconciliation and optimal management of hydrogen networks in a petrol refinery",2012,"Control Engineering Practice",16,10.1016/j.conengprac.2011.06.009,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857191932&doi=10.1016%2fj.conengprac.2011.06.009&partnerID=40&md5=d984c6a673b0064287b124605235ab4f","This paper describes the main problems associated to the management of hydrogen networks in petrol refineries and presents an approach to deal with them with the aim of operating the installation in the most profitable way. In particular, the problems of data reconciliation, economic optimization and interaction with the underlying basic control system are reviewed. The paper provides also a proposal for the implementation of the system and illustrates the approach with results obtained using real data from an industrial site. © 2011 Elsevier Ltd.","Data reconciliation; Decision support systems; Human interaction; Hydrogen networks; Process optimization","Data reconciliation; Decision supports; Economic optimization; Human interactions; Industrial sites; Optimal management; Artificial intelligence; Decision support systems; Gasoline; Hydrogen; Optimization; Profitability; Network management",Article,Scopus,2-s2.0-84857191932
"Anagnostopoulos K., Vavatsikos A.","Site suitability analysis for natural systems for wastewater treatment with spatial fuzzy analytic hierarchy process",2012,"Journal of Water Resources Planning and Management",16,10.1061/(ASCE)WR.1943-5452.0000155,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859293517&doi=10.1061%2f%28ASCE%29WR.1943-5452.0000155&partnerID=40&md5=806f59dfb47f8ad5edb3492e3bdcb8dc","When decentralized strategies are considered for managing wastewater, natural systems for wastewater treatment (NSWT) (constructed wetlands) seem to be strongly preferred in comparison with the conventional activated sludge wastewater treatment systems. However, because of their high land requirements, land-use suitability analysis should be conducted to specify adequate areas for their accommodation. Multicriteria spatial decision support systems have emerged as the technology that takes into account both decision criteria and constraints in complex land-use planning problems. This study presents a multiattribute decision analysis approach called the spatial fuzzy Analytic Hierarchy Process (SFAHP) to support raster-based land-use suitability studies for the implementation of NSWT. The SFAHP enables decision makers to circumvent vagueness when they perform evaluations using linguistic variables. Its application in a region of Northeastern Greece using a two-level criteria hierarchical model demonstrates its potential use for suitability analyses. © 2012 American Society of Civil Engineers.","Decision support systems; Fuzzy sets; Geographic information systems; Multiple objective analysis; Spatial analysis; Wastewater management","Constructed wetlands; Conventional activated sludges; Decision makers; Fuzzy analytic hierarchy process; Geographic information systems (GIS); Hierarchical model; Land-use planning; Linguistic variable; Multi-attribute decision analysis; Multi-criteria Spatial Decision Support Systems; Multiple-objective analysis; Natural systems; Spatial analysis; Suitability analysis; Wastewater management; Wastewater treatment system; Analytic hierarchy process; Artificial intelligence; Decision making; Decision support systems; Forestry; Fuzzy sets; Geographic information systems; Hierarchical systems; Land use; Reclamation; Activated sludge process; decision support system; fuzzy mathematics; GIS; hierarchical system; implementation process; land use planning; spatial analysis; waste treatment; wastewater",Article,Scopus,2-s2.0-84859293517
"Maurer U., Rüedlinger A., Tackmann B.","Confidentiality and integrity: A constructive perspective",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,10.1007/978-3-642-28914-9_12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858304203&doi=10.1007%2f978-3-642-28914-9_12&partnerID=40&md5=51eeec9b8f913fd2b655a2933e6ddaae","Traditional security definitions in the context of secure communication specify properties of cryptographic schemes. For symmetric encryption schemes, these properties are intended to capture the protection of the confidentiality or the integrity of the encrypted messages. A vast variety of such definitions has emerged in the literature and, despite the efforts of previous work, the relations and interplay of many of these notions (which are a priori not composable) are unexplored. Also, the exact guarantees implied by the properties are hard to understand. In constructive cryptography, notions such as confidentiality and integrity appear as attributes of channels, i.e., the communication itself. This makes the guarantees achieved by cryptographic schemes explicit, and leads to security definitions that are composable. In this work, we follow the approach of constructive cryptography, questioning the justification for the existing (game-based) security definitions. In particular, we compare these definitions with related constructive notions and find that some are too weak, such as INT-PTXT, or artificially strong, such as INT-CTXT. Others appear unsuitable for symmetric encryption, such as IND-CCA. © 2012 Springer-Verlag.","confidentiality; constructive cryptography; integrity","confidentiality; Cryptographic schemes; Encrypted messages; IND-CCA; integrity; Secure communications; Security definitions; Symmetric encryption; Symmetric encryption schemes; Confidentiality; Constructive cryptographies; Cryptographic schemes; Encrypted messages; Integrity; Security definitions; Symmetric encryption; Symmetric encryption schemes; Communication; Artificial intelligence; Computers; Cryptography; Cryptography",Conference Paper,Scopus,2-s2.0-84858304203
"Huang Z., Lu X., Duan H.","A task operation model for resource allocation optimization in business process management",2012,"IEEE Transactions on Systems, Man, and Cybernetics Part A:Systems and Humans",16,10.1109/TSMCA.2012.2187889,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865407033&doi=10.1109%2fTSMCA.2012.2187889&partnerID=40&md5=7ccd66d06401d7e99970731c0b39bcbe","Resource allocation, as an integral part of business process management (BPM), is more widely acknowledged by its importance for process-aware information systems. Despite the industrial need for efficient and effective resource allocation in BPM, few scientifically-grounded approaches exist to support these initiatives. In this paper, a new approach of resource allocation optimization is proposed, built on the concepts that is part of an operation-oriented view on process optimization. Essentially, the proposed approach automatically generates a specific task operation model (TOM) for a particular business process. In addition, in order to support end users in making sensible resource allocations, an ant colony optimization-based algorithm is presented, which makes it possible to search an optimal task operation path on the generated TOM. This allows one to suggest how a business user should efficiently allocate resources to perform the tasks of a particular process case. The feasibility of the presented approach is demonstrated by a simulation experiment. The experimental results show that the proposed approach outperforms reasonable heuristic approaches to satisfy process performance goals, and it is possible to improve the current state of BPM. © 2012 IEEE.","Ant colony optimization (ACO); business process; optimization; resource allocation; task operation model (TOM)","Ant colonies; Ant Colony Optimization (ACO); Business Process; Business process management; Business-users; End users; Heuristic approach; Integral part; Operation model; Optimization-based algorithm; Process performance; Process-aware information systems; Resource allocation optimization; Simulation experiments; Specific tasks; Algorithms; Artificial intelligence; Enterprise resource management; Heuristic methods; Optimization; Resource allocation",Article,Scopus,2-s2.0-84865407033
"Wang C.-S., Yang H.-L.","A recommender mechanism based on case-based reasoning",2012,"Expert Systems with Applications",16,10.1016/j.eswa.2011.09.161,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-82255192263&doi=10.1016%2fj.eswa.2011.09.161&partnerID=40&md5=135496f779f599de10d12b25e41bac2f","Case-based reasoning (CBR) algorithm is particularly suitable for solving ill-defined and unstructured decision-making problems in many different areas. The traditional CBR algorithm, however, is inappropriate to deal with complicated problems and therefore needs to be further revised. This study thus proposes a next-generation CBR (GCBR) model and algorithm. GCBR presents as a new problem-solving paradigm that is a case-based recommender mechanism for assisting decision making. GCBR can resolve decision-making problems by using hierarchical criteria architecture (HCA) problem representation which involves multiple decision objectives on each level of hierarchical, multiple-level decision criteria, thereby enables decision makers to identify problems more precisely. Additionally, the proposed GCBR can also provide decision makers with series of cases in support of these multiple decision-making stages. GCBR furthermore employs a genetic algorithm in its implementation in order to reduce the effort involved in case evaluation. This study found experimentally that using GCBR for making travel-planning recommendations involved approximately 80% effort than traditional CBR, and therefore concluded that GCBR should be the next generation of case-based reasoning algorithms and can be applied to actual case-based recommender mechanism implementation. © 2011 Elsevier Ltd. All rights reserved.","Artificial intelligence application; Case-based reasoning; Genetic algorithm; Multiple stage reasoning; Recommender mechanism","Based on case-based reasoning; CBr; Decision makers; Decision objectives; Decision-making problem; Hierarchical criteria; Multiple stage reasoning; Problem representation; Reasoning algorithms; Artificial intelligence; Decision making; Genetic algorithms; Case based reasoning",Article,Scopus,2-s2.0-82255192263
"Shi Z., Ma X.H., Qin C., Jia J., Jiang Y.Y., Tan C.Y., Chen Y.Z.","Combinatorial support vector machines approach for virtual screening of selective multi-target serotonin reuptake inhibitors from large compound libraries",2012,"Journal of Molecular Graphics and Modelling",16,10.1016/j.jmgm.2011.09.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-82255183052&doi=10.1016%2fj.jmgm.2011.09.002&partnerID=40&md5=dc50d2858ca3985573393891da2918da","Selective multi-target serotonin reuptake inhibitors enhance antidepressant efficacy. Their discovery can be facilitated by multiple methods, including in silico ones. In this study, we developed and tested an in silico method, combinatorial support vector machines (COMBI-SVMs), for virtual screening (VS) multi-target serotonin reuptake inhibitors of seven target pairs (serotonin transporter paired with noradrenaline transporter, H 3 receptor, 5-HT 1A receptor, 5-HT 1B receptor, 5-HT 2C receptor, melanocortin 4 receptor and neurokinin 1 receptor respectively) from large compound libraries. COMBI-SVMs trained with 917-1951 individual target inhibitors correctly identified 22-83.3% (majority &gt;31.1%) of the 6-216 dual inhibitors collected from literature as independent testing sets. COMBI-SVMs showed moderate to good target selectivity in misclassifying as dual inhibitors 2.2-29.8% (majority &lt;15.4%) of the individual target inhibitors of the same target pair and 0.58-7.1% of the other 6 targets outside the target pair. COMBI-SVMs showed low dual inhibitor false hit rates (0.006-0.056%, 0.042-0.21%, 0.2-4%) in screening 17 million PubChem compounds, 168,000 MDDR compounds, and 7-8181 MDDR compounds similar to the dual inhibitors. Compared with similarity searching, k-NN and PNN methods, COMBI-SVM produced comparable dual inhibitor yields, similar target selectivity, and lower false hit rate in screening 168,000 MDDR compounds. The annotated classes of many COMBI-SVMs identified MDDR virtual hits correlate with the reported effects of their predicted targets. COMBI-SVM is potentially useful for searching selective multi-target agents without explicit knowledge of these agents. © 2011 Elsevier Inc. All rights reserved.","Antidepressants; Computer aided drug design; High-throughput screening; Multi-target; Support vector machines; Virtual screening","Antidepressants; Computer aided drug design; High-throughput screening; Multitarget; Support vector; Virtual Screening; Digital libraries; Libraries; Support vector machines; histamine H3 receptor; melanocortin 4 receptor; neurokinin 1 receptor; noradrenalin transporter; serotonin 1A receptor; serotonin 1B receptor; serotonin 2C receptor; serotonin transporter; serotonin uptake inhibitor; article; drug binding; drug screening; priority journal; support vector machine; virtual screening; Antidepressive Agents; Artificial Intelligence; Combinatorial Chemistry Techniques; Humans; Norepinephrine Plasma Membrane Transport Proteins; Receptor, Melanocortin, Type 4; Receptor, Serotonin, 5-HT1A; Receptor, Serotonin, 5-HT1B; Receptor, Serotonin, 5-HT2C; Receptors, Neurokinin-1; Serotonin Plasma Membrane Transport Proteins; Serotonin Uptake Inhibitors; Support Vector Machines",Article,Scopus,2-s2.0-82255183052
"Hadavandi E., Shavandi H., Ghanbari A., Abbasian-Naghneh S.","Developing a hybrid artificial intelligence model for outpatient visits forecasting in hospitals",2012,"Applied Soft Computing Journal",16,10.1016/j.asoc.2011.09.018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84655167886&doi=10.1016%2fj.asoc.2011.09.018&partnerID=40&md5=7df445927256df5709d48ebef9f48336","Accurate forecasting of outpatient visits aids in decision-making and planning for the future and is the foundation for greater and better utilization of resources and increased levels of outpatient care. It provides the ability to better manage the ways in which outpatient's needs and aspirations are planned and delivered. This study presents a hybrid artificial intelligence (AI) model to develop a Mamdani type fuzzy rule based system to forecast outpatient visits with high accuracy. The hybrid model uses genetic algorithm for evolving knowledge base of fuzzy system. Actually it extracts useful patterns of information with a descriptive rule induction approach based on Genetic Fuzzy Systems (GFS). This is the first study on using a GFS to constructing an expert system for outpatient visits forecasting problems. Evaluation of the proposed approach will be carried out by applying it for forecasting outpatient visits of the department of internal medicine in a hospital in Taiwan and four big hospitals in Iran. Results show that the proposed approach has high accuracy in comparison with other related studies in the literature, so it can be considered as a suitable tool for outpatient visits forecasting problems. © 2011 Elsevier B.V. All rights reserved.","Data clustering; Forecasting; Genetic fuzzy system; Number of outpatient visits; Self organizing map","Data clustering; Descriptive rules; Forecasting problems; Genetic fuzzy systems; Hybrid model; Internal medicine; Knowledge base; Mamdani type; Number of outpatient visits; Self organizing; Artificial intelligence; Clustering algorithms; Expert systems; Fuzzy systems; Hospitals; Medicine; Self organizing maps; Forecasting",Article,Scopus,2-s2.0-84655167886
"Lan G.-C., Hong T.-P., Tseng V.S.","A projection-based approach for discovering high average-utility itemsets",2012,"Journal of Information Science and Engineering",16,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873616808&partnerID=40&md5=ce0c4fc11807260678ffd423d678a462","Utility mining has recently been an important issue due to its wide applications. An itemset in traditional utility mining considers individual profits and quantities of items in transactions regardless of its length. The average-utility measure, which is the total utility of an itemset divided by its number of items within it, was then proposed to reveal a better utility effect than the original utility measure. A mining algorithm was also proposed to find high average-utility itemsets from a transaction database. However, the previous mining approach was based on the principle of level-wise processing to find high average-utility itemsets from a database. In this paper, we thus propose an efficient average-utility mining approach which adopts a projection technique and an indexing mechanism to speed up the execution and reduce the memory requirement in the mining process. The proposed approach can project relevant sub-databases for mining, thus avoiding some unnecessary checking. In addition, a pruning strategy is also designed to reduce the number of unpromising itemsets in mining. Finally, the experimental results on synthetic datasets and two real datasets show the superior performance of the proposed approach.","Association rule mining; Average utility; Data mining; Indexing mechanism; Utility mining","Average utilities; Indexing mechanisms; Item sets; Itemset; Memory requirements; Mining algorithms; Mining process; Projection techniques; Pruning strategy; Real data sets; Speed up; Synthetic datasets; Transaction database; Utility measure; Utility mining; Algorithms; Artificial intelligence; Indexing (of information); Profitability; Data mining",Article,Scopus,2-s2.0-84873616808
"Do H., Kalousis A., Wang J., Woznica A.","A metric learning perspective of SVM: On the relation of LMNN and SVM",2012,"Journal of Machine Learning Research",16,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907021520&partnerID=40&md5=1cdb04fecd052bebc7e43089b1af3a4a","Support Vector Machines, SVMs, and the Large Margin Nearest Neighbor algorithm, LMNN, are two very popular learning algorithms with quite different learning biases. In this paper we bring them into a unified view and show that they have a much stronger relation than what is commonly thought. We analyze SVMs from a metric learning perspective and cast them as a metric learning problem, a view which helps us uncover the relations of the two algorithms. We show that LMNN can be seen as learning a set of local SVM-like models in a quadratic space. Along the way and inspired by the metric-based interpretation of SVMs we derive a novel variant of SVMs, ∈-SVM, to which LMNN is even more similar. We give a unified view of LMNN and the different SVM variants. Finally we provide some preliminary experiments on a number of benchmark datasets in which show that ∈-SVM compares favorably both with respect to LMNN and SVM.",,"Algorithms; Artificial intelligence; Support vector machines; Benchmark datasets; Large margin nearest neighbors; Metric learning; Learning algorithms",Conference Paper,Scopus,2-s2.0-84907021520
"Hofer B., Wotawa F.","Spectrum enhanced dynamic slicing for better fault localization",2012,"Frontiers in Artificial Intelligence and Applications",16,10.3233/978-1-61499-098-7-420,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874385172&doi=10.3233%2f978-1-61499-098-7-420&partnerID=40&md5=b3aeba5129de8061adb1b321dc2b42a5","Debugging consumes a considerable amount of time in software engineering, but it is rarely automated. In this paper, we focus on improving existing fault localization techniques. Spectrum-based fault localization (SFL) and slicing-hitting-set-computation (SHSC) are two techniques based on program execution traces. Both techniques come with small computational overhead and aid programmers to faster identify possible locations of faults. However, they have disadvantages: SHSC results in an undesirable high ranking of statements which are executed in many test cases, such as constructors. SFL operates on block level. Therefore, it cannot provide fine-grained results. We combine SHSC with SFL in order to eliminate these disadvantages. Our objective is to improve the ranking of faulty statements so that they allow for better fault localization than when using the previously mentioned methods separately. We show empirically that the resulting approach reduces the number of statements a programmer needs to check manually. In particular, we gain improvements of about 50% percent for SHSC and 25 % for SFL. © 2012 The Author(s).",,"Artificial intelligence; Software engineering; Computational overheads; Dynamic slicing; Fault localization; Fine grained; Gain improvement; Hitting sets; Program execution; Test case; Program debugging",Conference Paper,Scopus,2-s2.0-84874385172
"Singh H.P., Sukavanam N.","Simulation and stability analysis of neural network based control scheme for switched linear systems",2012,"ISA Transactions",16,10.1016/j.isatra.2011.08.004,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-83555162598&doi=10.1016%2fj.isatra.2011.08.004&partnerID=40&md5=c3b672a584568b3a4faffb96f3fae70b","This paper proposes a new adaptive neural network based control scheme for switched linear systems with parametric uncertainty and external disturbance. A key feature of this scheme is that the prior information of the possible upper bound of the uncertainty is not required. A feedforward neural network is employed to learn this upper bound. The adaptive learning algorithm is derived from Lyapunov stability analysis so that the system response under arbitrary switching laws is guaranteed uniformly ultimately bounded. A comparative simulation study with robust controller given in [Zhang L, Lu Y, Chen Y, Mastorakis NE. Robust uniformly ultimate boundedness control for uncertain switched linear systems. Computers and Mathematics with Applications 2008; 56: 170914] is presented. Copyright © 2011 Published by Elsevier Ltd on behalf of ISA. All rights reserved.","Feedforward neural network; Lyapunov function; Stability analysis; Switched systems; Uncertainty; Uniformly ultimate boundedness","Adaptive learning algorithm; Adaptive neural networks; Arbitrary switching laws; Comparative simulation; External disturbances; Key feature; Lyapunov stability analysis; Neural network based control; Parametric uncertainties; Prior information; Robust controllers; Stability analysis; Switched linear system; Switched systems; System response; Uncertain switched linear systems; Uncertainty; Uniformly ultimate boundedness; Uniformly ultimately bounded; Upper Bound; Adaptive algorithms; Computer simulation; Control system stability; Feedforward neural networks; Learning algorithms; Linear systems; Lyapunov functions; Switching circuits; Switching systems; Uncertainty analysis; Adaptive control systems; algorithm; article; artificial intelligence; artificial neural network; computer simulation; industry; instrumentation; statistical model; uncertainty; Algorithms; Artificial Intelligence; Computer Simulation; Industry; Linear Models; Neural Networks (Computer); Uncertainty",Article,Scopus,2-s2.0-83555162598
"Lin K.-H., Chang C.-H., Dopfer A., Wang C.-C.","Mapping and localization in 3D environments using a 2D laser scanner and a stereo camera",2012,"Journal of Information Science and Engineering",16,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862908677&partnerID=40&md5=e177762e7d40c825a48af8713c0c31ad","2D laser scanners have been widely used for accomplishing a number of challenging AI and robotics tasks such as mapping of large environments and localization in highly dynamic environments. However, using only one 2D laser scanner could be insufficient and less reliable for accomplishing tasks in 3D environments. The problem could be solved using multiple 2D laser scanners or a 3D laser scanner for performing 3D perception. Unfortunately, the cost of such 3D sensing systems is still too high for enabling AI and robotics applications. In this paper, we propose to use a 2D laser scanner and a stereo camera for accomplishing simultaneous localization and mapping (SLAM) in 3D indoor environments in which the 2D laser scanner is used for SLAM and the stereo camera is used for 3D mapping. The experimental results demonstrate that the proposed system is lower cost yet effective, and the obstacle detection rate is significant improved compares to using one 2D laser scanner for mapping.","Localization; Mapping; Navigation; Range sensing; Stereo vision","2D laser scanners; 3-D environments; 3-D mapping; 3-D sensing; 3D laser scanners; 3D perception; Dynamic environments; Indoor environment; Localization; Lower cost; Mapping and localization; Obstacle detection; Range sensing; Robotics applications; Simultaneous localization and mapping; Stereo cameras; Artificial intelligence; Cameras; Laser applications; Mapping; Mathematical techniques; Navigation; Obstacle detectors; Robotics; Scanning; Sensors; Stereo vision; Three dimensional",Article,Scopus,2-s2.0-84862908677
"Bimonte S., Bertolotto M., Gensel J., Boussaid O.","Spatial OLAP and map generalization: Model and algebra",2012,"International Journal of Data Warehousing and Mining",16,10.4018/jdwm.2012010102,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860528186&doi=10.4018%2fjdwm.2012010102&partnerID=40&md5=ad8f05ff143cc12b909cccdcd7c55569","Map generalization can be used as a central component of Spatial Decision Support Systems to provide a simplified and more readable cartographic visualization of geographic information. Indeed, it supports the user mental process for discovering important and unknown geospatial relations, trends and patterns. Spatial OLAP (SOLAP) integrates spatial data into OLAP and data warehouse systems. SOLAP models and tools are based on the concepts of spatial dimensions and measures that represent the axes and the subjects of the spatio-multidimensional analysis. Although powerful under some respect, current SOLAP models cannot support map generalization capabilities. This paper provides the first effort to integrate Map Generalization and OLAP. Firstly the authors define all modeling and querying requirements to do this integration, and then present a SOLAP model and algebra that support map generalization concepts. The approach extends SOLAP spatial hierarchies introducing multi-association relationships, supports imprecise measures, and it takes into account spatial dimensions constraints generated by multiple map generalization hierarchies. Copyright © 2012, IGI Global.","Geographic information systems; Map generalization; Multidimensional models; Spatial data warehouses; Spatial OLAP","Cartographic visualization; Central component; Data warehouse systems; Geo-spatial; Geographic information; Map generalization; Mental process; Multi-dimensional model; Spatial data; Spatial data warehouse; Spatial decision support systems; Spatial dimension; Spatial hierarchy; Spatial OLAP; Algebra; Artificial intelligence; Decision support systems; Geographic information systems; Mapping; Tools; Visualization; Data warehouses",Article,Scopus,2-s2.0-84860528186
"Romero E., Alquézar R.","Comparing error minimized extreme learning machines and support vector sequential feed-forward neural networks",2012,"Neural Networks",16,10.1016/j.neunet.2011.08.005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-82355175771&doi=10.1016%2fj.neunet.2011.08.005&partnerID=40&md5=5d525ae9fa7d667b1d7c5146eab3471d","Recently, error minimized extreme learning machines (EM-ELMs) have been proposed as a simple and efficient approach to build single-hidden-layer feed-forward networks (SLFNs) sequentially. They add random hidden nodes one by one (or group by group) and update the output weights incrementally to minimize the sum-of-squares error in the training set. Other very similar methods that also construct SLFNs sequentially had been reported earlier with the main difference that their hidden-layer weights are a subset of the data instead of being random. These approaches are referred to as support vector sequential feed-forward neural networks (SV-SFNNs), and they are a particular case of the sequential approximation with optimal coefficients and interacting frequencies (SAOCIF) method. In this paper, it is firstly shown that EM-ELMs can also be cast as a particular case of SAOCIF. In particular, EM-ELMs can easily be extended to test some number of random candidates at each step and select the best of them, as SAOCIF does. Moreover, it is demonstrated that the cost of the computation of the optimal output-layer weights in the originally proposed EM-ELMs can be improved if it is replaced by the one included in SAOCIF. Secondly, we present the results of an experimental study on 10 benchmark classification and 10 benchmark regression data sets, comparing EM-ELMs and SV-SFNNs, that was carried out under the same conditions for the two models. Although both models have the same (efficient) computational cost, a statistically significant improvement in generalization performance of SV-SFNNs vs. EM-ELMs was found in 12 out of the 20 benchmark problems. © 2011 Elsevier Ltd.","Error minimized extreme learning machines; Sequential approximations; Support vector sequential feed-forward neural networks","Bench-mark problems; Benchmark classification; Computational costs; Data sets; Experimental studies; Extreme learning machine; Feed-forward network; Generalization performance; Optimal coefficient; Random hidden nodes; Sequential approximation; Sum of squares; Support vector; Training sets; Classification (of information); Image quality; Learning systems; Optimization; Neural networks; algorithm; article; calculation; controlled study; error minimized extreme machine learning; experimental study; machine learning; measurement error; methodology; priority journal; sequential approximation with optimal coefficients and interacting frequencies method; single hidden layer feed forward network; support vector machine; support vector sequential feed forward neural network; Artificial Intelligence; Neural Networks (Computer); Random Allocation; Research Design; Support Vector Machines",Article,Scopus,2-s2.0-82355175771
"Briggs F., Lakshminarayanan B., Neal L., Fern X.Z., Raich R., Hadley S.J.K., Hadley A.S., Betts M.G.","Acoustic classification of multiple simultaneous bird species: A multi-instance multi-label approach",2012,"Journal of the Acoustical Society of America",16,10.1121/1.4707424,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84918783217&doi=10.1121%2f1.4707424&partnerID=40&md5=90470bb5a270b021f122d6fd89bf5672","Although field-collected recordings typically contain multiple simultaneously vocalizing birds of different species, acoustic species classification in this setting has received little study so far. This work formulates the problem of classifying the set of species present in an audio recording using the multi-instance multi-label (MIML) framework for machine learning, and proposes a MIML bag generator for audio, i.e., an algorithm which transforms an input audio signal into a bag-of-instances representation suitable for use with MIML classifiers. The proposed representation uses a 2D time-frequency segmentation of the audio signal, which can separate bird sounds that overlap in time. Experiments using audio data containing 13 species collected with unattended omnidirectional microphones in the H. J. Andrews Experimental Forest demonstrate that the proposed methods achieve high accuracy (96.1% true positives/negatives). Automated detection of bird species occurrence using MIML has many potential applications, particularly in long-term monitoring of remote sites, species distribution modeling, and conservation planning. © 2012 Acoustical Society of America.",,"Artificial intelligence; Audio acoustics; Birds; Learning systems; Acoustic classification; Automated detection; Conservation planning; Long term monitoring; Species classification; Species distribution modeling; Time frequency; True positive; Conservation",Article,Scopus,2-s2.0-84918783217
"Nouira K., Trabelsi A.","Intelligent monitoring system for intensive care units",2012,"Journal of Medical Systems",16,10.1007/s10916-011-9698-x,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873028817&doi=10.1007%2fs10916-011-9698-x&partnerID=40&md5=9fbddb36eb49ba3395f6bdd70ccfcfcd","We address in the present paper a medical monitoring system designed as a multi-Agent based approach. Our system includes mainly numerous agents that act as correlated multi-Agent sub-systems at the three layers of the whole monitoring infrastructure, to avoid non informative alarms and send effective alarms at time. The intelligence in the proposed monitoring system is provided by the use of time series technology. In fact, the capability of continuous learning of time series from the physiological variables allows the design of a system that monitors patients in real-time. Such system is a contrast to the classical threshold-based monitoring system actually present in the Intensive Care Units (ICUs) which causes a huge number of irrelevant alarms. © 2011 Springer Science+Business Media, LLC.","ICUs; Multi-Agent system; Online monitoring system; Time series","article; automation; experimental study; genetic algorithm; health care system; human; information system; intelligence; intensive care; intensive care unit; learning; multi agent monitoring system; online monitoring; sample size; sensitivity and sensibility; sensitivity and specificity; statistical model; stochastic model; time series analysis; alarm monitor; artificial intelligence; computer system; evaluation; methodology; organization and management; physiologic monitoring; Artificial Intelligence; Clinical Alarms; Computer Systems; Humans; Intensive Care Units; Monitoring, Physiologic",Article,Scopus,2-s2.0-84873028817
"Hasanpour Kashani M., Dinpashoh Y.","Evaluation of efficiency of different estimation methods for missing climatological data",2012,"Stochastic Environmental Research and Risk Assessment",16,10.1007/s00477-011-0536-y,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-82755161222&doi=10.1007%2fs00477-011-0536-y&partnerID=40&md5=e7103e39125b8486277d349c81f68f51","Reliable estimation of missing data is an important task for meteorologists, hydrologists and environment protection workers all over the world. In recent years, artificial intelligence techniques have gained enormous interest of many researchers in estimating of missing values. In the current study, we evaluated 11 artificial intelligence and classical techniques to determine the most suitable model for estimating of climatological data in three different climate conditions of Iran. In this case, 5 years (2001-2005) of observed data at target and neighborhood stations were used to estimate missing data of monthly minimum temperature, maximum temperature, mean air temperature, relative humidity, wind speed and precipitation variables. The comparison includes both visual and parametric approaches using such statistic as mean absolute errors, coefficient of efficiency and skill score. In general, it was found that although the artificial intelligence techniques are more complex and time-consuming models in identifying their best structures for optimum estimation, but they outperform the classical methods in estimating missing data in three distinct climate conditions. Moreover, the in-filling done by artificial neural network rivals that by genetic programming and sometimes becomes more satisfactory, especially for precipitation data. The results also indicated that multiple regression analysis method is the suitable method among the classical methods. The results of this research proved the high importance of choosing the best and most precise method in estimating different climatological data in Iran and other arid and semi-arid regions. © 2011 Springer-Verlag.","Artificial intelligence and classical techniques; Climatological data; Iran; Missing data","Arid and semi-arid regions; Artificial intelligence techniques; Artificial Neural Network; Classical methods; Classical techniques; Climate condition; Climatological data; Environment protection; Estimation methods; Iran; Maximum temperature; Mean absolute error; Mean air temperatures; Missing data; Missing values; Multiple regression analysis; Observed data; Optimum estimations; Parametric approach; Precipitation data; Precise method; Skill Score; Wind speed; Arid regions; Climate models; Genetic programming; Neural networks; Regression analysis; Structural optimization; Estimation; arid region; artificial intelligence; artificial neural network; climate conditions; climatology; estimation method; multiple regression; numerical model; parameterization; precipitation assessment; relative humidity; temperature profile; visual analysis; wind velocity; Iran",Article,Scopus,2-s2.0-82755161222
"Huang X., Qiao L.","A risk index model for multi-period uncertain portfolio selection",2012,"Information Sciences",15,10.1016/j.ins.2012.06.017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865576727&doi=10.1016%2fj.ins.2012.06.017&partnerID=40&md5=c1417e5126b326f847ef51a6acd24089","This paper discusses a multi-period portfolio selection problem when security returns are given by experts' evaluations. The security return rates are regarded as uncertain variables and an uncertain risk index adjustment model is proposed. Optimal portfolio adjustments are determined with the objective of maximizing the total incremental wealth within the constraints of controlling the cumulative risk index value over the investment horizon and satisfying self-financing at each period. To enable the users to solve the model problem with currently available programming tools, an equivalent of the model is provided. In addition, a method of obtaining the uncertainty distributions of the security returns is given based on experts' evaluations, and a selection example is presented. © 2012 Elsevier Inc. All rights reserved.","Mean-risk index model; Multi-period portfolio selection; Portfolio selection; Risk index; Uncertain programming","Adjustment model; Index models; Investment horizon; Model problems; Multi-period; Optimal portfolios; Portfolio selection; Portfolio selection problems; Programming tools; Risk indices; Self-financing; Uncertain programming; Uncertain variables; Uncertainty distributions; Artificial intelligence; Software engineering",Article,Scopus,2-s2.0-84865576727
"Eklund M., Norinder U., Boyer S., Carlsson L.","Application of conformal prediction in QSAR",2012,"IFIP Advances in Information and Communication Technology",15,10.1007/978-3-642-33412-2_17,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870886685&doi=10.1007%2f978-3-642-33412-2_17&partnerID=40&md5=6473394eb72e6c4ee37ed79074ea2696","QSAR modeling is a method for predicting properties, e.g. the solubility or toxicity, of chemical compounds using statistical learning techniques. QSAR is in widespread use within the pharmaceutical industry to prioritize compounds for experimental testing or to alert for potential toxicity. However, predictions from a QSAR model are difficult to assess if their prediction intervals are unknown. In this paper we introduce conformal prediction into the QSAR field to address this issue. We apply support vector machine regression in combination with two nonconformity measures to five datasets of different sizes to demonstrate the usefulness of conformal prediction in QSAR modeling. One of the nonconformity measures provides prediction intervals with almost the same width as the size of the QSAR models' prediction errors, showing that the prediction intervals obtained by conformal prediction are efficient and useful. © 2012 IFIP International Federation for Information Processing.",,"Data sets; Different sizes; Experimental testing; Pharmaceutical industry; Predicting properties; Prediction errors; Prediction interval; QSAR model; QSAR modeling; Statistical learning techniques; Support vector machine regressions; Artificial intelligence; Chemical compounds; Molecular graphics; Toxicity; Forecasting",Conference Paper,Scopus,2-s2.0-84870886685
"Holzinger A., Stocker C., Bruschi M., Auinger A., Silva H., Gamboa H., Fred A.","On applying approximate entropy to ECG signals for knowledge discovery on the example of big sensor data",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",15,10.1007/978-3-642-35236-2_64,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870367323&doi=10.1007%2f978-3-642-35236-2_64&partnerID=40&md5=e70dec9c643e270154c5530648057e19","Information entropy as a universal and fascinating statistical concept is helpful for numerous problems in the computational sciences. Approximate entropy (ApEn), introduced by Pincus (1991), can classify complex data in diverse settings. The capability to measure complexity from a relatively small amount of data holds promise for applications of ApEn in a variety of contexts. In this work we apply ApEn to ECG data. The data was acquired through an experiment to evaluate human concentration from 26 individuals. The challenge is to gain knowledge with only small ApEn windows while avoiding modeling artifacts. Our central hypothesis is that for intra subject information (e.g. tendencies, fluctuations) the ApEn window size can be significantly smaller than for inter subject classification. For that purpose we propose the term truthfulness to complement the statistical validity of a distribution, and show how truthfulness is able to establish trust in their local properties. © 2012 Springer-Verlag.","ApEn; big data; ECG complexity; Information entropy; knowledge discovery","ApEn; Approximate entropy; Big datum; Complex data; Computational science; ECG data; ECG signals; Information entropy; Local property; Sensor data; Statistical concepts; Statistical validity; Subject classification; Window Size; Artificial intelligence; Data mining; Electrocardiography",Conference Paper,Scopus,2-s2.0-84870367323
"Battaïa O., Dolgui A., Guschinsky N., Levin G.","A decision support system for design of mass production machining lines composed of stations with rotary or mobile table",2012,"Robotics and Computer-Integrated Manufacturing",15,10.1016/j.rcim.2012.04.005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860849485&doi=10.1016%2fj.rcim.2012.04.005&partnerID=40&md5=7b5180f1d692773a2c0247e20818e26d","This paper describes a decision support system (DSS) developed in order to offer to machining line designers a cognitive aid for early design stages. The aim of DSS is to assist the decision makers in finding the configuration of a new line that will meet quality and productivity requirements and minimize the investment costs. The current version of DSS is oriented to design of mass production machining lines composed of machines with rotary or mobile tables. This decision support system is based on mathematical models and methods which were devised to provide the designers with the optimal parameters of new line configuration including the required number of working stations of different types, the number of working positions at each station and spindle heads at each working position. The system is implemented under Autodesk Inventor and includes the modules for part modeling, process planning and machining system configuration. Its modular character and open architecture make upgrading with new mathematical tools suitable for other machining systems easy and fast. Moreover, it can be employed either as a separate software or integrated in a Product Life-cycle Management (PLM) tool. © 2012 Elsevier Ltd. All rights reserved.","Decision support tool; Machining systems; Optimization; Preliminary design; Product Life-cycle Management","Autodesk Inventor; Decision makers; Decision support tools; Early design stages; Investment costs; Line configuration; Machining systems; Mass production; Mathematical tools; Open architecture; Optimal parameter; Preliminary design; Product life cycle management; Spindle head; Working stations; Artificial intelligence; Computer aided manufacturing; Computer architecture; Decision support systems; Machine tools; Mathematical models; Optimization; Production engineering; Design",Article,Scopus,2-s2.0-84860849485
"Lee D., Kim H., Myung H.","GPU-based real-time RGB-D 3D SLAM",2012,"2012 9th International Conference on Ubiquitous Robots and Ambient Intelligence, URAI 2012",15,10.1109/URAI.2012.6462927,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874668585&doi=10.1109%2fURAI.2012.6462927&partnerID=40&md5=538f3553305cb7b11bdc1399880b47ce","This paper proposes a GPU (graphics processing unit)-based real-time RGB-D (red-green-blue depth) 3D SLAM (simultaneous localization and mapping) system. RGB-D data contain 2D image and per-pixel depth information. First, 6-DOF (degree-of-freedom) visual odometry is obtained through the 3D-RANSAC (three-dimensional random sample consensus) algorithm with image features. And a projective ICP (iterative closest point) algorithm gives an accurate odometry estimation result with depth information. For speed up extraction of features and ICP computation, GPU-based parallel computation is performed. After detecting loop closure, a graph-based SLAM algorithm optimizes trajectory of the sensor and 3D map. Copyright © 2012 IEEE.","3D SLAM; 3D-RANSAC; Image features; Projective iterative closest point; RGB-D camera","3D SLAM; 3D-RANSAC; Image features; Iterative Closest Points; Rgb-d cameras; Algorithms; Artificial intelligence; Computer graphics equipment; Program processors; Robotics; Three dimensional computer graphics",Conference Paper,Scopus,2-s2.0-84874668585
"Lee J., Kim H.-J., Park G.-L.","Integration of battery charging to tour schedule generation for an EV-based rent-a-car business",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",15,10.1007/978-3-642-31020-1_47,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870791955&doi=10.1007%2f978-3-642-31020-1_47&partnerID=40&md5=da5975466aaa7af30af527c3e29ebda5","To promote an electric vehicle-based rent-a-car business, this paper designs a tour scheduler capable of minimizing the waiting time induced by frequent and long battery charging during the tour. As charging can be conducted during the stay time in each tourist spot, the waiting time is greatly dependent on the visiting order. After formulating the per-spot waiting time according to the initial battery amount and the earned distance credit, our scheme traverses the search space to find the visiting sequence having the minimum waiting time. The performance measurement results obtained from a prototype implementation reveal that the proposed scheme can add just 40 minutes when the total trip length is about 195 km, which may need about a few hour charging in slow chargers, for the given parameter set including average stay time and inter-spot distance. Moreover, our scheme outperforms the well-known traveling salesman problem solver by up to 14.7 % in terms of tour time. © 2012 Springer-Verlag.","battery charging; electric vehicle; rent-a-car business; tour schedule; trip time efficiency","Parameter set; Performance measurements; Prototype implementations; Schedule generation; Search spaces; tour schedule; Traveling salesman; Trip time; Artificial intelligence; Electric vehicles; Traveling salesman problem; Charging (batteries)",Conference Paper,Scopus,2-s2.0-84870791955
"Nguyen P., Tran D., Huang X., Sharma D.","A proposed feature extraction method for EEG-based person identification",2012,"Proceedings of the 2012 International Conference on Artificial Intelligence, ICAI 2012",15,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875180554&partnerID=40&md5=860f6b3489c6f688c342338f13777ead","We propose in this paper a feature extraction method to extract brain wave features from electroencephalography (EEG) signal. The proposed feature extraction method is based on an assumption that EEG signal could be considered as stationary if the time window is sufficiently short. With this assumption, EEG signal has some similar properties to speech signal and hence a feature extraction method that is currently used to extract speech features can be applied to extract brain wave features from EEG signal. Mel-frequency cepstral coefficients are features extracted and evaluated in EEG-based person identication. Experimental results show that the proposed method could provide very high recognition rate.","Brain computer interface; Eeg; Person identification","Brain wave; EEG signals; Feature extraction methods; Mel-frequency cepstral coefficients; Person identification; Speech features; Speech signals; Time windows; Artificial intelligence; Brain computer interface; Electroencephalography; Electrophysiology; Feature extraction; Neuroimaging",Conference Paper,Scopus,2-s2.0-84875180554
"Yetilmezsoy K., Abdul-Wahab S.A.","A prognostic approach based on fuzzy-logic methodology to forecast PM10 levels in Khaldiya residential area, Kuwait",2012,"Aerosol and Air Quality Research",15,10.4209/aaqr.2012.07.0163,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874990964&doi=10.4209%2faaqr.2012.07.0163&partnerID=40&md5=ed7636d42eb41d9ca0341f22dd3a9152","A prognostic approach is proposed based on a fuzzy-logic model to estimate suspended dust concentrations, related to PM10, in a specific residential area in Kuwait with high traffic and industrial influences. Seven input variables, including four important meteorological parameters (wind speed, wind direction, relative humidity and solar radiation) and the ambient concentrations of three gaseous pollutants (methane, carbon monoxide and ozone) were fuzzified using a sytem with a graphical user interface (GUI) and an artificial intelligence-based approach. Trapezoidal membership functions with ten and fifteen levels were employed for the fuzzy subsets of each model variable. A Mamdani-type fuzzy inference system (FIS) was developed to introduce a total of 146 rules in the IF-THEN format. The product (prod) and the centre of gravity (centroid) methods were performed as the inference operator and defuzzification methods, respectively, for the proposed FIS. The results obtained using uzzy-logic were compared with the outputs of an exponential regression model. The predictive performances of the models were compared based on various descriptive statistical indicators, and the proposed method was tested against additional observed data. The prognostic model presented in this work produced very small deviations from the actual results, and showed better predictive performance than the other model with regard to forecasting PM10 levels, with a very high determination coefficient of over 0.99. © Taiwan Association for Aerosol Research.","Fuzzy-logic model; Kuwait; Multiple regression; Particulate matter; PM10","Determination coefficients; Fuzzy logic model; Graphical user interfaces (GUI); Kuwait; Meteorological parameters; Multiple regressions; Particulate Matter; Trapezoidal membership functions; Artificial intelligence; Carbon monoxide; Fog; Graphical user interfaces; Methane; Regression analysis; Sun; Housing; atmospheric pollution; fuzzy mathematics; particulate matter; regression analysis; residential location; Kuwait [Middle East]",Article,Scopus,2-s2.0-84874990964
"He H., Cao Y.","SSC: A classifier combination method based on signal strength",2012,"IEEE Transactions on Neural Networks and Learning Systems",15,10.1109/TNNLS.2012.2198227,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876137337&doi=10.1109%2fTNNLS.2012.2198227&partnerID=40&md5=6922b929d335b6c7d12154827273ef27","We propose a new classifier combination method, the signal strength-based combining (SSC) approach, to combine the outputs of multiple classifiers to support the decision-making process in classification tasks. As ensemble learning methods have attracted growing attention from both academia and industry recently, it is critical to understand the fundamental issues of the combining rule. Motivated by the signal strength concept, our proposed SSC algorithm can effectively integrate the individual vote from different classifiers in an ensemble learning system. Comparative studies of our method with nine major existing combining rules, namely, geometric average rule, arithmetic average rule, median value rule, majority voting rule, Borda count, max and min rule, weighted average, and weighted majority voting rules, is presented. Furthermore, we also discuss the relationship of the proposed method with respect to margin-based classifiers, including the boosting method (AdaBoost.M1 and AdaBoost.M2) and support vector machines by margin analysis. Detailed analyses of margin distribution graphs are presented to discuss the characteristics of the proposed method. Simulation results for various real-world datasets illustrate the effectiveness of the proposed method. © 2012 IEEE.","Classification; classifier combination; combining rule; ensemble learning; signal strength","Classification tasks; Classifier combination; Combining rules; Decision making process; Ensemble learning; Majority voting rules; Margin-based classifiers; Signal strengths; Artificial intelligence; Classification (of information); Computer networks; Learning systems",Article,Scopus,2-s2.0-84876137337
"Shahid N., Aleem S.A., Naqvi I.H., Zaffar N.","Support Vector Machine based fault detection & classification in smart grids",2012,"2012 IEEE Globecom Workshops, GC Wkshps 2012",15,10.1109/GLOCOMW.2012.6477812,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875672083&doi=10.1109%2fGLOCOMW.2012.6477812&partnerID=40&md5=dd39d21b57ef8d5b27593dd1e3777770","Smart Grids have recently attracted the attention of many profound research groups with their ability to create an automated and distributed energy level delivery. Computational Intelligence (CI) has been incorporated into various aspects of the smart grids, including fault detection and classification, which is a key issue in all the power systems. This paper presents two novel techniques for fault detection and classification in power Transmission Lines (TL). The proposed approaches are based on One-Class Quarter-Sphere Support Vector Machine (QSSVM). The first technique, Temporal-attribute QSSVM (TA-QSSVM), exploits the temporal and attribute correlations of the data measured in a TL for fault detection during the transient stage. The second technique is based on a novel One-Class SVM formulation, named as Attribute-QSSVM (A-QSSVM), that exploits attribute correlations only for automatic fault classification. The results indicate a detection and classification accuracy as high as 99%. Significant reduction (from O(n 4) to O(n2)) in computational complexity is achieved as compared to the state-of-the-art techniques, which use Multi-Class SVM for fault classification. Moreover, unlike state-of-the-art techniques, both of these techniques are unsupervised and online and can be implemented on the existing monitoring infrastructure for online monitoring, fault detection and classification in power sytems. © 2012 IEEE.","fault detection and classification; Smart Grid; Support Vector Machines; transmission systems","Classification accuracy; Distributed energies; Fault classification; Fault detection and classification; Online monitoring; Smart grid; State-of-the-art techniques; Transmission systems; Artificial intelligence; Electric power transmission; Smart power grids; Support vector machines; Fault detection",Conference Paper,Scopus,2-s2.0-84875672083
"Cuong N.V., Dinh V., Ho L.S.T.","Mel-frequency cepstral coefficients for eye movement identification",2012,"Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI",15,10.1109/ICTAI.2012.42,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876864698&doi=10.1109%2fICTAI.2012.42&partnerID=40&md5=74c1dae824b735419b58ad281a6402ef","Human identification is an important task for various activities in society. In this paper, we consider the problem of human identification using eye movement information. This problem, which is usually called the eye movement identification problem, can be solved by training a multiclass classification model to predict a person's identity from his or her eye movements. In this work, we propose using Mel-frequency cepstral coefficients (MFCCs) to encode various features for the classification model. Our experiments show that using MFCCs to represent useful features such as eye position, eye difference, and eye velocity would result in a much better accuracy than using Fourier transform, cepstrum, or raw representations. We also compare various classification models for the task. From our experiments, linear-kernel SVMs achieve the best accuracy with 93.56% and 91.08% accuracy on the small and large datasets respectively. Besides, we conduct experiments to study how the movements of each eye contribute to the final classification accuracy. © 2012 IEEE.","Biometric method; eye movement identification; Mel-frequency cepstral coefficients","Biometric methods; Classification accuracy; Classification models; Human identification; Identification problem; Large datasets; Mel-frequency cepstral coefficients; Multi-class classification; Artificial intelligence; Biometrics; Experiments; Eye movements",Conference Paper,Scopus,2-s2.0-84876864698
"Klein P.N., Marx D.","Solving Planar k-Terminal Cut in O(nc√k) time",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",15,10.1007/978-3-642-31594-7_48,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883166052&doi=10.1007%2f978-3-642-31594-7_48&partnerID=40&md5=fa95accd15c30e7d776a7bc6d9a7f298","The problem Planar k -Terminal Cut is as follows: given an undirected planar graph with edge-costs and with k vertices designated as terminals, find a minimum-cost set of edges whose removal pairwise separates the terminals. It was known that the complexity of this problem is O(n2k-4log n). We show that there is a constant c such that the complexity is O(n c√k). This matches a recent lower bound of Marx showing that the c√k term in the exponent is best possible up to the constant c (assuming the Exponential Time Hypothesis). © 2012 Springer-Verlag.",,"Exponential time hypothesis; Lower bounds; Planar graph; Artificial intelligence; Computer science; Automata theory",Conference Paper,Scopus,2-s2.0-84883166052
"Karargyris A., Karargyris O., Pantelopoulos A.","DERMA/Care: An advanced image-processing mobile application for monitoring skin cancer",2012,"Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI",15,10.1109/ICTAI.2012.180,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876868769&doi=10.1109%2fICTAI.2012.180&partnerID=40&md5=a162ae34f6e395643dbe5e5467ecd6b9","This paper describes a mobile hardware/software system (DERMA/care) to help with screening of skin cancer (melanomas). Our system uses an inexpensive apparatus (microscope) and a smart phone (iPhone). These two components standalone are sufficient to capture highly detailed images for use by experts with medical background. However the novelty of our system lies in the fact that we further improved the efficiency of the system by implementing an advanced image-processing framework to detect suspicious areas and help with skin cancer prevention. Our main goal was to demonstrate how smart phones could turn into powerful and intelligent machines and help large populations without expertise in low-resource settings. © 2012 IEEE.","health; image; iOS; iPhone; machine learning; melanomas; microscope; Mobile; skin cancer prevention","image; iOS; iPhone; melanomas; Mobile; Skin cancers; Artificial intelligence; Computer applications; Dermatology; Diagnosis; Health; Learning systems; Microscopes; Smartphones; Medical imaging",Conference Paper,Scopus,2-s2.0-84876868769
"Wen X., Huang M., Shi J.","Study on resources scheduling based on ACO allgorithm and PSO algorithm in cloud computing",2012,"Proceedings - 11th International Symposium on Distributed Computing and Applications to Business, Engineering and Science, DCABES 2012",15,10.1109/DCABES.2012.63,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872518844&doi=10.1109%2fDCABES.2012.63&partnerID=40&md5=74b6fecf8db8905ca1bb95cd227af7c9","It improves the algorithm because of the shortcoming that the ACO algorithm is easy to fall into local optimal solution in the cloud computing resource scheduling. The improved algorithm makes particle optimization inosculated into ant colony algorithm, which first finds out several groups of solutions using ACO algorithm according to the updated pheromone, and then gets more effective solutions using PSO algorithm to do crossover operation and mutation operation so as to avoid the algorithm prematurely into the local optimal solution. © 2012 IEEE.","ACO algorithm; Cloud Computing; PSO algorithm; Resources Scheduling","ACO algorithms; Ant colony algorithms; Computing resource; Crossover operations; Effective solution; Local optimal solution; Mutation operations; Particle optimization; PSO algorithms; Resources scheduling; Artificial intelligence; Cloud computing; Optimal systems; Particle swarm optimization (PSO); Scheduling; Scheduling algorithms",Conference Paper,Scopus,2-s2.0-84872518844
"Dimitrov S., Haas H.","Optimum signal shaping in OFDM-based optical wireless communication systems",2012,"IEEE Vehicular Technology Conference",15,10.1109/VTCFall.2012.6399084,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878932123&doi=10.1109%2fVTCFall.2012.6399084&partnerID=40&md5=ebf63cb14da1ce366fdbe5ce5aa72637","In this paper, a framework for optimum signal shaping in multi-carrier modulation is presented for optical wireless communications (OWC). The two fundamental multi-carrier transmission schemes based on orthogonal frequency division multiplexing (OFDM), direct-current-biased optical OFDM (DCO-OFDM) and asymmetrically clipped optical OFDM (ACO-OFDM), are studied. The optimum signal shaping is defined as optimum biasing and optimum scaling of the time domain signal within the optical power constraints of the transmitter front-end. These include the boundaries of the limited linear dynamic range, such as minimum and maximum radiated optical power, and the desired average optical power level. As a result, the minimum required electrical signal-to-noise ratio (SNR) to maintain a target bit-error ratio (BER) is obtained for a desired multi-level quadrature amplitude modulation (M-QAM) scheme and a given combination of optical power constraints. The average optical power is varied over dynamic ranges of 10 dB, 20 dB and 30 dB. With the increase of the dynamic range and for a major portion of the average optical power levels, DCO-OFDM demonstrates a lower minimum electrical SNR requirement for a target BER as compared to ACO-OFDM for modulation orders with similar spectral efficiencies. © 2012 IEEE.","ACO-OFDM; DCO-OFDM; Optical wireless communication; Optimization; Signal shaping","ACO-OFDM; Asymmetrically clipped Optical OFDM; DCO-OFDM; Multicarrier transmission schemes; Optical wireless communication systems; Optical wireless communications; Signal shaping; Signaltonoise ratio (SNR); Ant colony optimization; Artificial intelligence; Electric power transmission; Optical fiber communication; Optimization; Quadrature amplitude modulation; Wireless telecommunication systems; Orthogonal frequency division multiplexing",Conference Paper,Scopus,2-s2.0-84878932123
"Anderson G., Rathke J.","Dynamic software update for message passing programs",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",15,10.1007/978-3-642-35182-2_15,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872255613&doi=10.1007%2f978-3-642-35182-2_15&partnerID=40&md5=c95ee98db27fd970d4f54270c5db0b97","Global Session Types are typically used to express communication protocols between a number of participating entities. Analyses on these types can be used to prove that message passing programs have a variety of desirable properties such as communications safety and deadlock freedom. In this paper we provide a Global Session Type analysis for queued channel message passing programs whose code may be updated during runtime (Dynamic Software Update). In particular, we prove safety and liveness properties for well-typed programs by identifying suitable restrictions on the runtime points at which dynamic updates may occur. This includes the possibility of updating several threads without requiring global thread synchronisation. © Springer-Verlag Berlin Heidelberg 2012.",,"Deadlock freedom; Dynamic software update; Dynamic update; Liveness properties; Message passing programs; Runtimes; Session types; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84872255613
"Grosse R.B., Salakhutdinov R., Freeman W.T., Tenenbaum J.B.","Exploiting compositionality to explore a large space of model structures",2012,"Uncertainty in Artificial Intelligence - Proceedings of the 28th Conference, UAI 2012",15,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886001624&partnerID=40&md5=efc35977442a178ac2ad9c6f6537cb11","The recent proliferation of richly structured probabilistic models raises the question of how to automatically determine an appropriate model for a dataset. We investigate this question for a space of matrix decomposition models which can express a variety of widely used models from unsupervised learning. To enable model selection, we organize these models into a context-free grammar which generates a wide variety of structures through the compositional application of a few simple rules. We use our grammar to generically and efficiently infer latent components and estimate predictive likelihood for nearly 2500 structures using a small toolbox of reusable algorithms. Using a greedy search over our grammar, we automatically choose the decomposition structure from raw data by evaluating only a small fraction of all models. The proposed method typically finds the correct structure for synthetic data and backs off gracefully to simpler models under heavy noise. It learns sensible structures for datasets as diverse as image patches, motion capture, 20 Questions, and U.S. Senate votes, all using exactly the same code.",,"Appropriate models; Compositionality; Decomposition structures; Matrix decomposition; Model Selection; Predictive likelihoods; Probabilistic models; Re-usable algorithms; Computer software reusability; Model structures; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84886001624
"Kantchelian A., Ma J., Huang L., Afroz S., Joseph A.D., Tygar J.D.","Robust detection of comment spam using entropy rate",2012,"Proceedings of the ACM Conference on Computer and Communications Security",15,10.1145/2381896.2381907,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869838776&doi=10.1145%2f2381896.2381907&partnerID=40&md5=fceb3953cd179d27a0044fd86d9ad72a","In this work, we design a method for blog comment spam detection using the assumption that spam is any kind of uninformative content. To measure the ""informativeness"" of a set of blog comments, we construct a language and tokenization independent metric which we call content complexity, providing a normalized answer to the informal question ""how much information does this text contain?"" We leverage this metric to create a small set of features well-adjusted to comment spam detection by computing the content complexity over groupings of messages sharing the same author, the same sender IP, the same included links, etc. We evaluate our method against an exact set of tens of millions of comments collected over a four months period and containing a variety of websites, including blogs and news sites. The data was provided to us with an initial spam labeling from an industry competitive source. Nevertheless the initial spam labeling had unknown performance characteristics. To train a logistic regression on this dataset using our features, we derive a simple mislabeling tolerant logistic regression algorithm based on expectationmaximization, which we show generally outperforms the plain version in precision-recall space. By using a parsimonious hand-labeling strategy, we show that our method can operate at an arbitrary high precision level, and that it significantly dominates, both in terms of precision and recall, the original labeling, despite being trained on it alone. The content complexity metric, the use of a noise-tolerant logistic regression and the evaluation methodology are thus the three central contributions with this work.","Comment spam; Content complexity; Logistic regression; Noisy label; Spam filtering","Comment spam; Content complexity; Logistic regressions; Noisy labels; Spam filtering; Artificial intelligence; Logistics; Regression analysis; Internet",Conference Paper,Scopus,2-s2.0-84869838776
"Um-e-Ghazia, Masood R., Shibli M.A.","Comparative analysis of access control systems on cloud",2012,"Proceedings - 13th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing, SNPD 2012",15,10.1109/SNPD.2012.33,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868595941&doi=10.1109%2fSNPD.2012.33&partnerID=40&md5=401af99b8ae9a51c67ce20f6230c88aa","Cloud computing, a relatively new concept and has gained an immense attention of research community in the past few years. R&D organizations and industry are investing a lot in cloud based research and applications. Similarly on theconsumers' side organizations are moving their business on cloud to provide flexibility and conceive ever increasing computational power requirements. In spite of significant advantages, and its demand, different stakeholders are still reluctant to migrate to cloud. A major hindrance is the absence of reliable and comprehensive access control mechanism for cloud resources. We have analyzed existing cloud based access control systems and evaluated those using NIST defined access control systems evaluation criteria. Based on our analysis we have proposed future research direction in the domain of access control systems for cloud based environments, which will eventually pave the way towards cloud adoption. © 2012 IEEE.","attribute-based-encryption; capability-based-access-control; fine-grained; role-based-access-control; task-based-access-control","attribute-based-encryption; capability-based-access-control; fine-grained; role-based-access-control; task-based-access-control; Artificial intelligence; Research; Software engineering; Access control",Conference Paper,Scopus,2-s2.0-84868595941
"Xu W., Zhang H., Jiao S., Wang D., Song F., Liu Z.","Optimizing sparse matrix vector multiplication using cache blocking method on Fermi GPU",2012,"Proceedings - 13th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing, SNPD 2012",15,10.1109/SNPD.2012.20,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868551964&doi=10.1109%2fSNPD.2012.20&partnerID=40&md5=6d3151e08c9397928c77f6234503617e","It is an important task to tune performance for sparse matrix vector multiplication (SpMV), but it is also a difficult task because of its irregularity. In this paper, we propose a cache blocking method to improve the performance of SpMV on the emerging GPU architecture. The sparse matrix is partitioned into many sub-blocks, which are stored in CSR format. With the blocking method, the corresponding part of vector x can be reused in the GPU cache, so the time spent on accessing the global memory for vector x is reduced heavily. Experimental results on GeForce GTX 480 show that SpMV kernel with the cache blocking method is 5x faster than the unblocked CSR kernel in the best case. © 2012 IEEE.","cache blocking; GPU; SpMV","Blocking method; Cache blocking; GPU; Sparse matrices; Sparse matrix-vector multiplication; SpMV; Sub-blocks; Time spent; Artificial intelligence; Vectors; Software engineering",Conference Paper,Scopus,2-s2.0-84868551964
"Huang M., Yu W., Zhu D.","An improved image segmentation algorithm based on the Otsu method",2012,"Proceedings - 13th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing, SNPD 2012",15,10.1109/SNPD.2012.26,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868533419&doi=10.1109%2fSNPD.2012.26&partnerID=40&md5=1de93205c6f6f0fbdc90ad43ad5f5d10","By analyzing the basic principle of Otsu method and its application in image segmentation, and according to the distribution characteristics of the target and background, an improved threshold image segmentation algorithm based on the Otsu method is developed. By narrowing the selection range of threshold and searching the minimum variance ratio, the improved algorithm selects the optimal threshold. Through the compared with the Otsu method and other methods, the results show that the new improved algorithm has these advantages such as high segmentation precision and fast computation speed. © 2012 IEEE.","image segmentation; minimum variance ratio; optimal threshold; Otsu method; selection range","Basic principles; Distribution characteristics; Fast computation speed; Image segmentation algorithm; Minimum variance; Optimal threshold; Otsu method; Segmentation precision; selection range; Target and background; Algorithms; Artificial intelligence; Optimization; Software engineering; Image segmentation",Conference Paper,Scopus,2-s2.0-84868533419
"Gogate V., Jha A., Venugopal D.","Advances in lifted importance sampling",2012,"Proceedings of the National Conference on Artificial Intelligence",15,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868289601&partnerID=40&md5=5477b9c44c527cb2f86391ec2f9aee59","We consider lifted importance sampling (LIS), a previously proposed approximate inference algorithm for statistical relational learning (SRL) models. LIS achieves substantial variance reduction over conventional importance sampling by using various lifting rules that take advantage of the symmetry in the relational representation. However, it suffers from two drawbacks. First, it does not take advantage of some important symmetries in the relational representation and may exhibit needlessly high variance on models having these symmetries. Second, it uses an uninformative proposal distribution which adversely affects its accuracy. We propose two improvements to LIS that address these limitations. First, we identify a new symmetry in SRL models and define a lifting rule for taking advantage of this symmetry. The lifting rule reduces the variance of LIS. Second, we propose a new, structured approach for constructing and dynamically updating the proposal distribution via adaptive sampling. We demonstrate experimentally that our new, improved LIS algorithm is substantially more accurate than the LIS algorithm. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Adaptive sampling; Approximate inference; Proposal distribution; Relational representations; Statistical relational learning; Structured approach; Variance reductions; Algorithms; Artificial intelligence; Importance sampling; Inference engines",Conference Paper,Scopus,2-s2.0-84868289601
"Eaton E., Mansbach R.","A spin-glass model for semi-supervised community detection",2012,"Proceedings of the National Conference on Artificial Intelligence",15,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868279665&partnerID=40&md5=5878877a5dac65492b505768406b2269","Current modularity-based community detection methods show decreased performance as relational networks become increasingly noisy. These methods also yield a large number of diverse community structures as solutions, which is problematic for applications that impose constraints on the acceptable solutions or in cases where the user is focused on specific communities of interest. To address both of these problems, we develop a semi-supervised spin-glass model that enables current community detection methods to incorporate background knowledge in the forms of individual labels and pairwise constraints. Unlike current methods, our approach shows robust performance in the presence of noise in the relational network, and the ability to guide the discovery process toward specific community structures. We evaluate our algorithm on several benchmark networks and a new political sentiment network representing cooperative events between nations that was mined from news articles over six years. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Background knowledge; Benchmark networks; Communities of interest; Community detection; Community structures; Diverse community; News articles; Pairwise constraints; Relational network; Robust performance; Semi-supervised; Spin-glass models; Artificial intelligence; Population dynamics",Conference Paper,Scopus,2-s2.0-84868279665
"Bareinboim E., Pearl J.","Transportability of causal effects: Completeness results",2012,"Proceedings of the National Conference on Artificial Intelligence",15,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868282548&partnerID=40&md5=30e66822f0de9e11a56051f8a5ed3ba9","The study of transportability aims to identify conditions under which causal information learned from experiments can be reused in a different environment where only passive observations can be collected. The theory introduced in [Pearl and Bareinboim, 2011] (henceforth [PB, 2011]) defines formal conditions for such transfer but falls short of providing an effective procedure for deciding, given assumptions about differences between the source and target domains, whether transportability is feasible. This paper provides such procedure. It establishes a necessary and sufficient condition for deciding when causal effects in the target domain are estimable from both the statistical information available and the causal information transferred from the experiments. The paper further provides a complete algorithm for computing the transport formula, that is, a way of fusing experimental and observational information to synthesize an estimate of the desired causal relation. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Causal relations; Statistical information; Sufficient conditions; Target domain; Experiments; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868282548
"Ackerman M., Ben-David S., Brânzei S., Loker D.","Weighted clustering",2012,"Proceedings of the National Conference on Artificial Intelligence",15,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868283370&partnerID=40&md5=5979f17e7df1e8c5c0cf25d243700147","We investigate a natural generalization of the classical clustering problem, considering clustering tasks in which different instances may have different weights. We conduct the first extensive theoretical analysis on the influence of weighted data on standard clustering algorithms in both the partitional and hierarchical settings, characterizing the conditions under which algorithms react to weights. Extending a recent framework for clustering algorithm selection, we propose intuitive properties that would allow users to choose between clustering algorithms in the weighted setting and classify algorithms accordingly. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Clustering problems; Natural generalization; Weighted data; Artificial intelligence; Clustering algorithms",Conference Paper,Scopus,2-s2.0-84868283370
"Pfeiffer T., Gao X.A., Mao A., Chen Y., Rand D.G.","Adaptive polling for information aggregation",2012,"Proceedings of the National Conference on Artificial Intelligence",15,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868274012&partnerID=40&md5=ccca44a64509fecd3f47e3af3f5ae477","The flourishing of online labor markets such as Amazon Mechanical Turk (MTurk) makes it easy to recruit many workers for solving small tasks. We study whether information elicitation and aggregation over a combinatorial space can be achieved by integrating small pieces of potentially imprecise information, gathered from a large number of workers through simple, one-shot interactions in an online labor market. We consider the setting of predicting the ranking of n competing candidates, each having a hidden underlying strength parameter. At each step, our method estimates the strength parameters from the collected pairwise comparison data and adaptively chooses another pairwise comparison question for the next recruited worker. Through an MTurk experiment, we show that the adaptive method effectively elicits and aggregates information, outperforming a naïve method using a random pairwise comparison question at each step. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Adaptive methods; Adaptive polling; Imprecise information; Information aggregation; Labor markets; Mechanical turks; Pair-wise comparison; Strength parameters; Commerce; Employment; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868274012
"Van Den Broeck G., Davis J.","Conditioning in first-order knowledge compilation and lifted probabilistic inference",2012,"Proceedings of the National Conference on Artificial Intelligence",15,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868273908&partnerID=40&md5=38fadd19a36fcf2eecb1e5835412ba73","Knowledge compilation is a powerful technique for compactly representing and efficiently reasoning about logical knowledge bases. It has been successfully applied to numerous problems in artificial intelligence, such as probabilistic inference and conformant planning. Conditioning, which updates a knowledge base with observed truth values for some propositions, is one of the fundamental operations employed for reasoning. In the propositional setting, conditioning can be efficiently applied in all cases. Recently, people have explored compilation for first-order knowledge bases. The majority of this work has centered around using first-order d-DNNF circuits as the target compilation language. However, conditioning has not been studied in this setting. This paper explores how to condition a first-order d-DNNF circuit. We show that it is possible to efficiently condition these circuits on unary relations. However, we prove that conditioning on higher arity relations is #P-hard. We study the implications of these findings on the application of performing lifted inference for first-order probabilistic models. This leads to a better understanding of which types of queries lifted inference can address. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Conformant planning; First-order; Fundamental operations; Knowledge base; Knowledge basis; Knowledge compilation; Probabilistic inference; Probabilistic models; Truth values; Unary relations; Knowledge based systems; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868273908
"Cakmak M., Lopes M.","Algorithmic and human teaching of sequential decision tasks",2012,"Proceedings of the National Conference on Artificial Intelligence",15,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868291108&partnerID=40&md5=e0971887f291bd75c2858d4bd0f75ac9","A helpful teacher can significantly improve the learning rate of a learning agent. Teaching algorithms have been formally studied within the field of Algorithmic Teaching. These give important insights into how a teacher can select the most informative examples while teaching a new concept. However the field has so far focused purely on classification tasks. In this paper we introduce a novel method for optimally teaching sequential decision tasks. We present an algorithm that automatically selects the set of most informative demonstrations and evaluate it on several navigation tasks. Next, we explore the idea of using this algorithm to produce instructions for humans on how to choose examples when teaching sequential decision tasks. We present a user study that demonstrates the utility of such instructions. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Classification tasks; Decision task; Learning agents; Learning rates; Navigation tasks; Teaching algorithms; User study; Artificial intelligence; Algorithms",Conference Paper,Scopus,2-s2.0-84868291108
"Herranz N., Ruiz-Castillo J.","Multiplicative and fractional strategies when journals are assigned to several subfields",2012,"Journal of the American Society for Information Science and Technology",15,10.1002/asi.22629,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868215301&doi=10.1002%2fasi.22629&partnerID=40&md5=4e22c539962ceb6f979ac0cc9213127f","In many data sets, articles are classified into subfields through the journals in which they have been published. The problem is that while many journals are assigned to a single subfield, many others are assigned to several. This article discusses a multiplicative and a fractional strategy to deal with this situation. The empirical part studies different aspects of citation distributions under the two strategies, namely: the number of articles, the mean citation rate, the broad shape of the distribution, their characterization in terms of size- and scale-invariant indicators of high and low impact, and the presence of extreme distributions, that is, distributions that behave very differently from the rest. We found that, despite large differences in the number of articles according to both strategies, the similarity of the citation characteristics of articles published in journals assigned to one or several subfields guarantees that choosing one of the two strategies may not lead to a radically different picture in practical applications. Nevertheless, the characterization of citation excellence through a high-impact indicator may considerably differ depending on that choice. © 2012 ASIS&T.","bibliographic citations","bibliographic citations; Citation distribution; Data sets; Extreme distribution; Scale-invariant; Subfields; Artificial intelligence; Software engineering",Article,Scopus,2-s2.0-84868215301
"Holzer S., Shotton J., Kohli P.","Learning to efficiently detect repeatable interest points in depth data",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",15,10.1007/978-3-642-33718-5_15,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867850980&doi=10.1007%2f978-3-642-33718-5_15&partnerID=40&md5=85966426cedb4e7bad3a7203a0e80e58","Interest point (IP) detection is an important component of many computer vision methods. While there are a number of methods for detecting IPs in RGB images, modalities such as depth images and range scans have seen relatively little work. In this paper, we approach the IP detection problem from a machine learning viewpoint and formulate it as a regression problem. We learn a regression forest (RF) model that, given an image patch, tells us if there is an IP in the center of the patch. Our RF based method for IP detection allows an easy trade-off between speed and repeatability by adapting the depth and number of trees used for approximating the interest point response maps. The data used for training the RF model is obtained by running state-of-the-art IP detection methods on the depth images. We show further how the IP response map used for training the RF can be specifically designed to increase repeatability by employing 3D models of scenes generated by reconstruction systems such as KinectFusion [1]. Our experiments demonstrate that the use of such data leads to considerably improved IP detection. © 2012 Springer-Verlag.",,"3D models; Depth image; Detection methods; Detection problems; Image patches; Number of methods; Point response; Range scans; Reconstruction systems; Regression problem; RGB images; Artificial intelligence; Computer vision",Conference Paper,Scopus,2-s2.0-84867850980
"Vineet V., Warrell J., Torr P.H.S.","Filter-based mean-field inference for random fields with higher-order terms and product label-spaces",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",15,10.1007/978-3-642-33715-4_3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867854022&doi=10.1007%2f978-3-642-33715-4_3&partnerID=40&md5=3665da1baad73cd5986a2d83ac1d2451","Recently, a number of cross bilateral filtering methods have been proposed for solving multi-label problems in computer vision, such as stereo, optical flow and object class segmentation that show an order of magnitude improvement in speed over previous methods. These methods have achieved good results despite using models with only unary and/or pairwise terms. However, previous work has shown the value of using models with higher-order terms e.g. to represent label consistency over large regions, or global co-occurrence relations. We show how these higher-order terms can be formulated such that filter-based inference remains possible. We demonstrate our techniques on joint stereo and object labeling problems, as well as object class segmentation, showing in addition for joint object-stereo labeling how our method provides an efficient approach to inference in product label-spaces. We show that we are able to speed up inference in these models around 10-30 times with respect to competing graph-cut/move-making methods, as well as maintaining or improving accuracy in all cases. We show results on PascalVOC-10 for object class segmentation, and Leuven for joint object-stereo labeling. © 2012 Springer-Verlag.",,"Bilateral filtering; Co-occurrence; Filter-based; Graph-cut; Large regions; Mean-field; Multi-label; Object class; Object labeling; Random fields; Artificial intelligence; Computer vision",Conference Paper,Scopus,2-s2.0-84867854022
"Almagambetov A., Casares M., Velipasalar S.","Autonomous tracking of vehicle rear lights and detection of brakes and turn signals",2012,"2012 IEEE Symposium on Computational Intelligence for Security and Defence Applications, CISDA 2012",15,10.1109/CISDA.2012.6291543,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867696976&doi=10.1109%2fCISDA.2012.6291543&partnerID=40&md5=3888ec18aa242b86baf30c9692c4cc72","Automatic detection of vehicle alert signals is extremely critical in autonomous vehicle applications and collision avoidance systems, as these detection systems can help in the prevention of deadly and costly accidents. In this paper, we present a novel and lightweight algorithm that uses a Kalman filter and a codebook to achieve a high level of robustness. The algorithm is able to detect braking and turning signals of the vehicle in front both during the daytime and at night (daytime detection being a major advantage over current research), as well as correctly track a vehicle despite changing lanes or encountering periods of no or low-visibility of the vehicle in front. We demonstrate that the proposed algorithm is able to detect the signals accurately and reliably under different lighting conditions. © 2012 IEEE.",,"Automatic Detection; Autonomous tracking; Autonomous Vehicles; Changing lanes; Codebooks; Collision avoidance systems; Detection system; Lighting conditions; Over current; Accidents; Algorithms; Artificial intelligence; Friction materials; Vehicles; Signal detection",Conference Paper,Scopus,2-s2.0-84867696976
"Cai Z., Li Y., Gu M.","Real-time recognition system of traffic light in urban environment",2012,"2012 IEEE Symposium on Computational Intelligence for Security and Defence Applications, CISDA 2012",15,10.1109/CISDA.2012.6291516,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867725210&doi=10.1109%2fCISDA.2012.6291516&partnerID=40&md5=949eca51377751395b843a35dc974ffa","Detection of arrow traffic light is a focal point research in autonomous vehicle, and in urban environment it is the basic technique. However, most researches mainly concern the circular traffic lights. A novel algorithm is proposed in this paper to resolve the problems of detection and recognition of arrow traffic lights. Two sub-modules, detection module and recognition module, are introduced in the main framework. In detection submodule, the color space conversion, binarization and morphology features filtering methods are performed to get the regions of candidates of blackboards. For getting the regions of arrow of traffic lights, segmentation based on the YCbCr color space is used in the cropping image, which is cropped from original image by the region of blackboard. In recognition sub-module, Gabor wavelet transform and 2D independent component analysis(2DICA) are used to extract traffic light candidate's features for features of the arrow traffic lights. A library for recognition has been built, and experimental results show that rate of recognition exceeds 91%. © 2012 IEEE.","2DICA; Arrow traffic light; Gabor wavelet; Intelligent vehicle","2DICA; Autonomous Vehicles; Binarizations; Color space conversion; Detection modules; Filtering method; Focal points; Gabor wavelet transforms; Gabor wavelets; Novel algorithm; Original images; Real-time recognition system; Submodules; Traffic light; Urban environments; Ycbcr color spaces; Algorithms; Artificial intelligence; Face recognition; Independent component analysis; Intelligent vehicle highway systems; Urban planning; Image segmentation",Conference Paper,Scopus,2-s2.0-84867725210
"Crouser R.J., Chang R.","An affordance-based framework for human computation and human-computer collaboration",2012,"IEEE Transactions on Visualization and Computer Graphics",15,10.1109/TVCG.2012.195,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867632479&doi=10.1109%2fTVCG.2012.195&partnerID=40&md5=7cefc9b54d5a146dbda08b0edf7cbb09","Visual Analytics is &amp;#8220;the science of analytical reasoning facilitated by visual interactive interfaces&amp;#8221; [70]. The goal of this field is to develop tools and methodologies for approaching problems whose size and complexity render them intractable without the close coupling of both human and machine analysis. Researchers have explored this coupling in many venues: VAST, Vis, InfoVis, CHI, KDD, IUI, and more. While there have been myriad promising examples of human-computer collaboration, there exists no common language for comparing systems or describing the benefits afforded by designing for such collaboration. We argue that this area would benefit significantly from consensus about the design attributes that define and distinguish existing techniques. In this work, we have reviewed 1,271 papers from many of the top-ranking conferences in visual analytics, human-computer interaction, and visualization. From these, we have identified 49 papers that are representative of the study of human-computer collaborative problem-solving, and provide a thorough overview of the current state-of-the-art. Our analysis has uncovered key patterns of design hinging on humanand machine-intelligence affordances, and also indicates unexplored avenues in the study of this area. The results of this analysis provide a common framework for understanding these seemingly disparate branches of inquiry, which we hope will motivate future work in the field. © 1995-2012 IEEE.","framework; human complexity; Human computation; theory","Affordances; Analytical reasoning; Close coupling; Common languages; Design attributes; framework; human complexity; Human computation; Human-computer collaboration; Key Patterns; Machine analysis; theory; Visual analytics; Visual interactive interface; Computation theory; Couplings; Visualization; Human computer interaction; artificial intelligence; computer graphics; computer interface; computer program; human; vision; Artificial Intelligence; Computer Graphics; Humans; Software; User-Computer Interface; Visual Perception",Article,Scopus,2-s2.0-84867632479
"Siew Z.W., Wong C.H., Chin C.S., Kiring A., Teo K.T.K.","Cluster heads distribution of wireless sensor networks via adaptive particle swarm optimization",2012,"Proceedings - 2012 4th International Conference on Computational Intelligence, Communication Systems and Networks, CICSyN 2012",15,10.1109/CICSyN.2012.25,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867352657&doi=10.1109%2fCICSyN.2012.25&partnerID=40&md5=1f3d49a85a36a063b33e43f4485f6bd8","Wireless sensor networks consists of hundreds or thousands of sensor nodes supported by small capacity battery. For environmental monitoring purposes, sensor nodes must have high endurance capabilities. Therefore, selecting suitable cluster heads (CH) location becomes a challenging issue. In this work, cluster heads distribution based on adaptive particle swarm (PSO) is proposed. PSO is one of the swarm intelligence methods designed to find optimum solution by mimicking the behavior of bird flocking and fish schooling. Adaptive cognitive and social learning factor can achieve better convergence speed and particles reselection mechanism can reduce the chances of getting trapped in local maximum. The performance of the proposed method is compared with low energy adaptive cluster hierarchical (LEACH). Simulation result shows that proposed method outperforms LEACH in terms of first node die (FND) round, total data received by base station and energy consume per round. © 2012 IEEE.","cluster head; clustering; LEACH; particle swarm optimization","Adaptive particle swarm optimizations; Cluster head; clustering; Convergence speed; Environmental Monitoring; LEACH; Local maximum; Low energies; Optimum solution; Particle swarm; Social learning; Swarm Intelligence; Artificial intelligence; Communication systems; Leaching; Particle swarm optimization (PSO); Sensor nodes",Conference Paper,Scopus,2-s2.0-84867352657
"Kavut S.","Results on rotation-symmetric S-boxes",2012,"Information Sciences",15,10.1016/j.ins.2012.02.030,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860307239&doi=10.1016%2fj.ins.2012.02.030&partnerID=40&md5=7797184f95b7f2d58a83e1faf322e709","We give an efficient exhaustive search strategy to enumerate 6 × 6 bijective rotation-symmetric S-boxes (RSSBs) having nonlinearity 24, which is found to be the maximum nonlinearity within the class of 6 × 6 bijective RSSBs. It is shown that there are 3072 RSSBs achieving the cryptographic properties of the inverse function over GF(2 6), i.e., nonlinearity 24, differential uniformity 4, and algebraic degree 5, such that among them there are only four which are not affine-equivalent. Among these four RSSBs, we find a non-affine transformation under which the cryptographic properties of the inverse function are invariant. Then, we define the generalized classes of k-RSSBs as the polynomials of GF(2 n) with coefficients in GF(2 k), where k divides n. Moreover, motivated by the fact that RSSBs are symmetric under a special permutation, we classify all possible permutations up to the linear equivalence of S-boxes that are symmetric under them. © 2012 Elsevier Inc. All rights reserved.","Affine equivalence; Finite field; Permutation polynomial; S-box","Affine equivalence; Algebraic degrees; Differential uniformity; Exhaustive search; Finite fields; Inverse functions; Linear equivalence; Non-Linearity; Permutation polynomials; S-box; S-boxes; Artificial intelligence; Software engineering; Cryptography",Article,Scopus,2-s2.0-84860307239
"Guo Y., Xiao M.","Cross language text classification via subspace co-regularized multi-view learning",2012,"Proceedings of the 29th International Conference on Machine Learning, ICML 2012",15,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867122134&partnerID=40&md5=e601eb2e2157758df628ba5979c554f4","In many multilingual text classification problems, the documents in different languages often share the same set of categories. To reduce the labeling cost of training a classification model for each individual language, it is important to transfer the label knowledge gained from one language to another language by conducting cross language classification. In this paper we develop a novel subspace co-regularized multi-view learning method for cross language text classification. This method is built on parallel corpora produced by machine translation. It jointly minimizes the training error of each classifier in each language while penalizing the distance between the subspace representations of parallel documents. Our empirical study on a large set of cross language text classification tasks shows the proposed method consistently outperforms a number of inductive methods, domain adaptation methods, and multi-view learning methods. Copyright 2012 by the author(s)/owner(s).",,"Classification models; Domain adaptation; Empirical studies; Inductive method; Machine translations; Multi-view learning; Multilingual texts; Parallel corpora; Subspace representation; Text classification; Training errors; Artificial intelligence; Software engineering; Learning systems",Conference Paper,Scopus,2-s2.0-84867122134
"Boyd K., Costa V.S., Davis J., Page C.D.","Unachievable region in precision-recall space and its effect on empirical evaluation",2012,"Proceedings of the 29th International Conference on Machine Learning, ICML 2012",15,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867122322&partnerID=40&md5=048982cf8ddddeb272874dfcfc66ce13","Precision-recall (PR) curves and the areas under them are widely used to summarize machine learning results, especially for data sets exhibiting class skew. They are often used analogously to ROC curves and the area under ROC curves. It is known that PR curves vary as class skew changes. What was not recognized before this paper is that there is a region of PR space that is completely unachievable, and the size of this region depends only on the skew. This paper precisely characterizes the size of that region and discusses its implications for empirical evaluation methodology in machine learning. Copyright 2012 by the author(s)/owner(s).",,"Area under roc curve (AUC); Class skew; Data sets; Empirical evaluations; ROC curves; Artificial intelligence; Software engineering; Learning systems",Conference Paper,Scopus,2-s2.0-84867122322
"David B., Yin C., Zhou Y., Xu T., Zhang B., Jin H., Chalon R.","SMART-CITY: Problematics, techniques and case studies",2012,"Proceedings - 2012 8th International Conference on Computing Technology and Information Management, ICCM 2012",15,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867032832&partnerID=40&md5=f6c510557816c9af844964f2d011bdc0","The SMART CITY is an important field for ubiquitous computing (UC) and ambient intelligence (AmI). Data vitalization related to in city data collection and their appropriate diffusion to city users (actors) and their services (applications) is the main problematic. To put it into practice a real augmented environment middleware for data collection and sharing is needed, as well as location based services, mobile and in-environment human computer interactions (HCI) and just in time contextual mobile learning. To illustrate and concretize this problematic we present three case studies: a dynamic lane allocation system, a system for goods urban delivery and a bus shelter based communication. © 2012 AICIT.","ambient intelligence; contextual mobile learning; data vitalization; dynamic lane allocation; in-environment HCI; location-based servoices; middleware; ubiquitious computing; urban goods delivary","Ambient intelligence; data vitalization; Location based; Mobile Learning; Ubiquitious computing; urban goods delivary; Artificial intelligence; Data acquisition; Information management; Location based services; Telecommunication services; Ubiquitous computing; Middleware",Conference Paper,Scopus,2-s2.0-84867032832
"Kumar A., Vembu S., Menon A.K., Elkan C.","Learning and inference in probabilistic classifier chains with beam search",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",15,10.1007/978-3-642-33460-3_48,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866843118&doi=10.1007%2f978-3-642-33460-3_48&partnerID=40&md5=3b5d4b7206cfbd786618dbb95f6beef2","Multilabel learning is an extension of binary classification that is both challenging and practically important. Recently, a method for multilabel learning called probabilistic classifier chains (PCCs) was proposed with numerous appealing properties, such as conceptual simplicity, flexibility, and theoretical justification. However, PCCs suffer from the computational issue of having inference that is exponential in the number of tags, and the practical issue of being sensitive to the suitable ordering of the tags while training. In this paper, we show how the classical technique of beam search may be used to solve both these problems. Specifically, we show how to use beam search to perform tractable test time inference, and how to integrate beam search with training to determine a suitable tag ordering. Experimental results on a range of multilabel datasets show that these proposed changes dramatically extend the practical viability of PCCs. © 2012 Springer-Verlag.",,"Beam search; Binary classification; Classical techniques; Computational issues; Conceptual simplicity; Data sets; Multi-label; Practical issues; Probabilistic classifiers; Test time; Artificial intelligence; Learning systems",Conference Paper,Scopus,2-s2.0-84866843118
"Sarnovsky M., Kacur T.","Cloud-based classification of text documents using the Gridgain platform",2012,"SACI 2012 - 7th IEEE International Symposium on Applied Computational Intelligence and Informatics, Proceedings",15,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866776991&partnerID=40&md5=f4d735e37d6cf8f281c3ac0f86674ed5","Motivation for the research effort presented in this paper is to use the cloud computing storage and computational capabilities for text mining tasks. Cloud computing is nowadays favored approach in area of data-analysis and related fields by providing data storage and computational capabilities as the services. Main aim of our research activities is to design and develop experimental cloud platform for text mining tasks. In this particular paper we describe the design and implementation of a distributed tree-based algorithm for text categorization purposes. We used our own implementation of decision tree classification algorithm and used Gridgain framework for its cloud implementation. Cloud also provides storage services for handling large data collections as well as increases computational effectiveness as the algorithm is implemented in distributed fashion. We describe the experiments we have performed on the private cloud using the two datasets and analyze the results. ©2012 IEEE.",,"Computational capability; Data sets; Data storage; Decision tree classification; Large data; Private clouds; Research activities; Research efforts; Storage services; Text categorization; Text document; Text mining; Tree-based algorithms; Algorithms; Artificial intelligence; Cloud computing; Decision trees; Information retrieval systems; Information science; Text processing; Trees (mathematics); Data mining",Conference Paper,Scopus,2-s2.0-84866776991
"Correnson L., Signoles J.","Combining analyses for C program verification",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",15,10.1007/978-3-642-32469-7_8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866658740&doi=10.1007%2f978-3-642-32469-7_8&partnerID=40&md5=99fbfbc8f660fa4f76c1a6ecff118441","Static analyzers usually return partial results. They can assert that some properties are valid during all possible executions of a program, but generally leave some other properties to be verified by other means. In practice, it is common to combine results from several methods manually to achieve the full verification of a program. In this context, Frama-C is a platform for analyzing C source programs with multiple analyzers. Hence, one analyzer might conclude about properties assumed by another one, in the same environment. We present here the semantical foundations of validity of program properties in such a context. We propose a correct and complete algorithm for combining several partial results into a fully consolidated validity status for each program property. We illustrate how such a framework provides meaningful feedback on partial results. © 2012 Springer-Verlag.",,"C programs; Program properties; Semantical foundations; Static analyzers; Artificial intelligence; C (programming language)",Conference Paper,Scopus,2-s2.0-84866658740
"Bobadilla J., Ortega F., Hernando A., Arroyo Á.","A balanced memory-based collaborative filtering similarity measure",2012,"International Journal of Intelligent Systems",15,10.1002/int.21556,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865484388&doi=10.1002%2fint.21556&partnerID=40&md5=fd221d260c26470045e687bd9757b427","Collaborative filtering recommender systems contribute to alleviating the problem of information overload that exists on the Internet as a result of the mass use of Web 2.0 applications. The use of an adequate similarity measure becomes a determining factor in the quality of the prediction and recommendation results of the recommender system, as well as in its performance. In this paper, we present a memory-based collaborative filtering similarity measure that provides extremely high-quality and balanced results; these results are complemented with a low processing time (high performance), similar to the one required to execute traditional similarity metrics. The experiments have been carried out on the MovieLens and Netflix databases, using a representative set of information retrieval quality measures. © 2012 Wiley Periodicals, Inc.",,"Collaborative filtering; High quality; Information overloads; Processing time; Retrieval quality; Similarity measure; Similarity metrics; Web 2.0 applications; Artificial intelligence; Software engineering; Recommender systems",Article,Scopus,2-s2.0-84865484388
"Yu S., Zhang J., Moran M.S., Lu J.Q., Feng Y., Hu X.-H.","A novel method of diffraction imaging flow cytometry for sizing microspheres",2012,"Optics Express",15,10.1364/OE.20.022245,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866636125&doi=10.1364%2fOE.20.022245&partnerID=40&md5=08282208998c3b02e5678f5c36057e62","We report a novel method of diffraction imaging flow cytometry to measure and analyze size distribution of microspheres. An automated and robust image processing software based on the short-Time-Fourier-Transform algorithm has been developed to analyze the characteristic and spatially varying oscillations of side scatters recorded as a diffraction image. Our results demonstrate that the new method allows accurate and rapid determination of single microspheres' diameters ranging from 1 to 100μm. The capacity for analysis of light scattering by two-sphere aggregates has been demonstrated but analytical tools for characterization of aggregates by multiple microspheres remain to be developed. © 2012 Optical Society of America.",,"Aggregates; Diffraction; Flow cytometry; Image processing; Analytical tool; Diffraction images; Diffraction imaging; Image-processing software; Rapid determination; Microspheres; microsphere; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; flow cytometry; methodology; particle size; photometry; Algorithms; Artificial Intelligence; Flow Cytometry; Image Interpretation, Computer-Assisted; Microspheres; Nephelometry and Turbidimetry; Particle Size; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84866636125
"Seck-Tuoh-Mora J.C., Martínez G.J., Alonso-Sanz R., Hernández-Romero N.","Invertible behavior in elementary cellular automata with memory",2012,"Information Sciences",15,10.1016/j.ins.2012.02.063,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860228099&doi=10.1016%2fj.ins.2012.02.063&partnerID=40&md5=0b1550dd048c8c087fb638baf4fb0c4e","Elementary cellular automata (ECAs) have been studied for their ability to generate complex global behavior, despite their simplicity. One variation of ECAs is obtained by adding memory to each cell in a neighborhood. This process generates a provisional configuration in which the application of an evolution rule establishes the dynamics of the system. This version is known as an ECA with memory (ECAM). Most previous work on ECAMs analyzed the complex behavior taking chaotic ECAs. However, the present paper investigates reversible ECAMs as obtained from reversible and permutative ECAs. These ECAs have at least one ancestor for every configuration; thus, the correct permutation of states may specify the memory function to obtain reversible ECAMs. For permutative ECAs, which are often irreversible, we demonstrate that the use of a quiescent state and the correct manipulation of de Bruijn blocks produce reversible ECAMs. © 2012 Elsevier Inc. All rights reserved.","De Bruijn block; Elementary cellular automata; Memory; Reversibility","Complex behavior; De Bruijn; Evolution rules; Global behaviors; Memory functions; Quiescent state; Reversibility; Artificial intelligence; Data storage equipment; Software engineering; Cellular automata",Article,Scopus,2-s2.0-84860228099
"Sun Y., Bhanu B.","Reflection symmetry-integrated image segmentation",2012,"IEEE Transactions on Pattern Analysis and Machine Intelligence",15,10.1109/TPAMI.2011.259,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865599082&doi=10.1109%2fTPAMI.2011.259&partnerID=40&md5=683be0895ac795c575e947a69b235b83","This paper presents a new symmetry-integrated region-based image segmentation method. The method is developed to obtain improved image segmentation by exploiting image symmetry. It is realized by constructing a symmetry token that can be flexibly embedded into segmentation cues. Interesting points are initially extracted from an image by the SIFT operator and they are further refined for detecting the global bilateral symmetry. A symmetry affinity matrix is then computed using the symmetry axis and it is used explicitly as a constraint in a region growing algorithm in order to refine the symmetry of the segmented regions. A multi-objective genetic search finds the segmentation result with the highest performance for both segmentation and symmetry, which is close to the global optimum. The method has been investigated experimentally in challenging natural images and images containing man-made objects. It is shown that the proposed method outperforms current segmentation methods both with and without exploiting symmetry. A thorough experimental analysis indicates that symmetry plays an important role as a segmentation cue, in conjunction with other attributes like color and texture. © 2012 IEEE.","comparison of segmentation algorithms.; Local and global symmetry; region growing; segmentation and symmetry evaluation; symmetry affinity","Affinity matrix; Bilateral symmetry; Experimental analysis; Genetic search; Global optimum; Global symmetries; Image symmetry; Interesting points; Man made objects; Multi objective; Natural images; Region growing; Region growing algorithm; Region-based; Segmentation algorithms; Segmentation methods; Segmentation results; Segmented regions; Symmetry axis; Artificial intelligence; Computer vision; Image segmentation",Article,Scopus,2-s2.0-84865599082
"Lemme A., Reinhart R.F., Steil J.J.","Online learning and generalization of parts-based image representations by non-negative sparse autoencoders",2012,"Neural Networks",15,10.1016/j.neunet.2012.05.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863863720&doi=10.1016%2fj.neunet.2012.05.003&partnerID=40&md5=57147e2df79720e4f06c834076c96456","We present an efficient online learning scheme for non-negative sparse coding in autoencoder neural networks. It comprises a novel synaptic decay rule that ensures non-negative weights in combination with an intrinsic self-adaptation rule that optimizes sparseness of the non-negative encoding. We show that non-negativity constrains the space of solutions such that overfitting is prevented and very similar encodings are found irrespective of the network initialization and size. We benchmark the novel method on real-world datasets of handwritten digits and faces. The autoencoder yields higher sparseness and lower reconstruction errors than related offline algorithms based on matrix factorization. It generalizes to new inputs both accurately and without costly computations, which is fundamentally different from the classical matrix factorization approaches. © 2012 Elsevier Ltd.","Autoencoder; Non-negativity; Sparse coding","Autoencoders; Encodings; Handwritten digit; Image representations; Matrix factorizations; Network initialization; Non-negative sparse coding; Non-negativity; Off-line algorithm; Online learning; Online learning scheme; Overfitting; Real-world datasets; Reconstruction error; Self adaptation; Sparse coding; Encoding (symbols); Learning systems; accuracy; algorithm; analytical error; article; artificial neural network; autoencoder; data base; image reconstruction; machine learning; mathematical computing; mathematical parameters; offline algorithm; online analysis; online learning; online system; priority journal; synapse; Artificial Intelligence; Databases, Factual; Face; Humans; Image Processing, Computer-Assisted; Neural Networks (Computer); Pattern Recognition, Automated; Photic Stimulation",Article,Scopus,2-s2.0-84863863720
"Pereira C.R., Nakamura R.Y.M., Costa K.A.P., Papa J.P.","An Optimum-Path Forest framework for intrusion detection in computer networks",2012,"Engineering Applications of Artificial Intelligence",15,10.1016/j.engappai.2012.03.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864563699&doi=10.1016%2fj.engappai.2012.03.008&partnerID=40&md5=4ea1ace9a8f13a18a79395887913be82","Intrusion detection systems that make use of artificial intelligence techniques in order to improve effectiveness have been actively pursued in the last decade. However, their complexity to learn new attacks has become very expensive, making them inviable for a real time retraining. In order to overcome such limitations, we have introduced a new pattern recognition technique called optimum-path forest (OPF) to this task. Our proposal is composed of three main contributions: to apply OPF for intrusion detection, to identify redundancy in some public datasets and also to perform feature selection over them. The experiments have been carried out on three datasets aiming to compare OPF against Support Vector Machines, Self Organizing Maps and a Bayesian classifier. We have showed that OPF has been the fastest classifier and the always one with the top results. Thus, it can be a suitable tool to detect intrusions on computer networks, as well as to allow the algorithm to learn new attacks faster than other techniques. © 2012 Elsevier Ltd. All rights reserved.","Computer security; Intrusion detection system; Machine learning; Optimum-Path Forest","Artificial intelligence techniques; Bayesian classifier; Data sets; Intrusion Detection Systems; Optimum-path forests; Pattern recognition techniques; Real time; Artificial intelligence; Computer crime; Forestry; Learning systems; Pattern recognition; Security of data; Self organizing maps; Intrusion detection; Artificial Intelligence; Data Bases; Forests; Pattern Recognition",Article,Scopus,2-s2.0-84864563699
"Di Nuovo A.G., Marocco D., Cangelosi A., De La Cruz V.M., Di Nuovo S.","Mental practice and verbal instructions execution: A cognitive robotics study",2012,"Proceedings of the International Joint Conference on Neural Networks",15,10.1109/IJCNN.2012.6252751,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865104649&doi=10.1109%2fIJCNN.2012.6252751&partnerID=40&md5=88638156a6076e1dc7302442eb3e3bbd","Understanding the tight relationship that exists between mental imagery and motor activities (i.e. how images in the mind can influence movements and motor skills) has become a topic of interest and is of particular importance in domains in which improving those skills is crucial for obtaining better performance, such as in sports and rehabilitation. In this paper, using an embodied cognition approach and a cognitive robotics platform, we introduce initial results of an ongoing study that explores the impact linguistic stimuli could have in processes of mental imagery practice and subsequent motor execution and performance. Results are presented to show that the robot used, is able to ""imagine"" or ""mentally"" recall and accurately execute movements learned in previous training phases, strictly on the basis of the verbal commands issued. Further tests show that data obtained with ""imagination"" could be used to simulate ""mental training"" processes such as those that have been employed with human subjects in sports training, in order to enhance precision in the performance of new tasks, through the association of different verbal commands. © 2012 IEEE.","cognitive robotics; embodied cognition; mental training; motor imagery; recurrent neural network","Cognitive robotics; Embodied cognition; Human subjects; In-process; Mental imagery; Motor activity; Motor imagery; Motor skills; Training phasis; Verbal instructions; Artificial intelligence; Recurrent neural networks; SportS; Robotics",Conference Paper,Scopus,2-s2.0-84865104649
"Mihalák M., Schlegel J.C.","Asymmetric swap-equilibrium: A unifying equilibrium concept for network creation games",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",15,10.1007/978-3-642-32589-2_60,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865029403&doi=10.1007%2f978-3-642-32589-2_60&partnerID=40&md5=7c412a30aa200715175c2120e2ef3c43","We introduce and study the concept of an asymmetric swap-equilibrium for network creation games. A graph where every edge is owned by one of its endpoints is called to be in asymmetric swap-equilibrium, if no vertex v can delete its own edge {v,w} and add a new edge {v,w′} and thereby decrease the sum of distances from v to all other vertices. This equilibrium concept generalizes and unifies some of the previous equilibrium concepts for network creation games. While the structure and the quality of equilibrium networks is still not fully understood, we provide further (partial) insights for this open problem. As the two main results, we show that (1) every asymmetric swap-equilibrium has at most one (non-trivial) 2-edge-connected component, and (2) we show a logarithmic upper bound on the diameter of an asymmetric swap-equilibrium for the case that the minimum degree of the unique 2-edge-connected component is at least n ε , for ε &gt; \frac{4\lg 3}{\lg n} . Due to the generalizing property of asymmetric swap equilibria, these results hold for several equilibrium concepts that were previously studied. Along the way, we introduce a node-weighted version of the network creation games, which is of independent interest for further studies of network creation games. © 2012 Springer-Verlag.",,"Minimum degree; Network creation; Non-trivial; Sum of distances; Upper Bound; Artificial intelligence; Computer science",Conference Paper,Scopus,2-s2.0-84865029403
"Szemis J.M., Maier H.R., Dandy G.C.","A framework for using ant colony optimization to schedule environmental flow management alternatives for rivers, wetlands, and floodplains",2012,"Water Resources Research",15,10.1029/2011WR011276,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864854276&doi=10.1029%2f2011WR011276&partnerID=40&md5=64b95d9a711be6a8f64dcbba05a77228","Rivers, wetlands, and floodplains are in need of management as they have been altered from natural conditions and are at risk of vanishing because of river development. One method to mitigate these impacts involves the scheduling of environmental flow management alternatives (EFMA); however, this is a complex task as there are generally a large number of ecological assets (e.g., wetlands) that need to be considered, each with species with competing flow requirements. Hence, this problem evolves into an optimization problem to maximize an ecological benefit within constraints imposed by human needs and the physical layout of the system. This paper presents a novel optimization framework which uses ant colony optimization to enable optimal scheduling of EFMAs, given constraints on the environmental water that is available. This optimization algorithm is selected because, unlike other currently popular algorithms, it is able to account for all aspects of the problem. The approach is validated by comparing it to a heuristic approach, and its utility is demonstrated using a case study based on the Murray River in South Australia to investigate (1) the trade-off between plant recruitment (i.e., promoting germination) and maintenance (i.e., maintaining habitat) flow requirements, (2) the trade-off between flora and fauna flow requirements, and (3) a hydrograph inversion case. The results demonstrate the usefulness and flexibility of the proposed framework as it is able to determine EFMA schedules that provide optimal or near-optimal trade-offs between the competing needs of species under a range of operating conditions and valuable insight for managers.© 2012. American Geophysical Union. All Rights Reserved.",,"Ant Colony Optimization (ACO); Competing flow; Complex task; Ecological benefits; Environmental flow; Environmental water; Flood-plains; Flow requirements; Heuristic approach; Human needs; Hydrographs; Murray River; Natural conditions; Operating condition; Optimal scheduling; Optimization algorithms; Optimization framework; Optimization problems; Physical layout; South Australia; Algorithms; Artificial intelligence; Ecology; Economic and social effects; Environmental management; Heuristic methods; Optimization; Rivers; Wetlands; Scheduling; algorithm; floodplain; germination; hydrograph; optimization; recruitment (population dynamics); resource development; river management; river water; sustainability; trade-off; water resource; wetland; Australia; Murray River; South Australia",Article,Scopus,2-s2.0-84864854276
"Firoz J.S., Rahman M.S., Saha T.K.","Bee algorithms for solving DNA fragment assembly problem with noisy and noiseless data",2012,"GECCO'12 - Proceedings of the 14th International Conference on Genetic and Evolutionary Computation",15,10.1145/2330163.2330192,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864664744&doi=10.1145%2f2330163.2330192&partnerID=40&md5=0f8f1d719b604d005b9ec89f3adc7be7","DNA fragment assembly problem is one of the crucial challenges faced by computational biologists where, given a set of DNA fragments, we have to construct a complete DNA sequence from them. As it is an NP-hard problem, accurate DNA sequence is hard to find. Moreover, due to experimental limitations, the fragments considered for assembly are exposed to additional errors while reading the fragments. In such scenarios, meta-heuristic based algorithms can come in handy. We analyze the performance of two swarm intelligence based algorithms namely Artificial Bee Colony (ABC) algorithm and Queen Bee Evolution Based on Genetic Algorithm (QEGA) to solve the fragment assembly problem and report quite promising results. Our main focus is to design meta-heuristic based techniques to efficiently handle DNA fragment assembly problem for noisy and noiseless data. © 2012 ACM.","bioinformatics; combinatorial optimization; dna computing; genetic algorithms; meta-heuristics","Artificial bee colonies; Bee Algorithm; DNA fragment; DNA-computing; Fragment assembly; Meta heuristics; Metaheuristic; Queen bee evolution; Swarm Intelligence; Artificial intelligence; Assembly; Bioinformatics; Combinatorial optimization; Computational complexity; DNA sequences; Genetic algorithms; Heuristic algorithms; DNA",Conference Paper,Scopus,2-s2.0-84864664744
"Madhloom H.T., Kareem S.A., Ariffin H.","An image processing application for the localization and segmentation of lymphoblast cell using peripheral blood images",2012,"Journal of Medical Systems",15,10.1007/s10916-011-9679-0,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873046381&doi=10.1007%2fs10916-011-9679-0&partnerID=40&md5=daeb7f00cbb41b6f3b090a37ea13c2e3","An important preliminary step in the diagnosis of leukemia is the visual examination of the patient's peripheral blood smear under the microscope. Morphological changes in the white blood cells can be an indicator of the nature and severity of the disease. Manual techniques are labor intensive, slow, error prone and costly. A computerized system can be used as a supportive tool for the specialist in order to enhance and accelerate the morphological analysis process. This research present a new method that integrates color features with the morphological reconstruction to localize and isolate lymphoblast cells from a microscope image that contains many cells. The localization and segmentation are conducted using a proposed method that consists of an integration of several digital image processing techniques. 180 microscopic blood images were tested, and the proposed framework managed to obtain 100% accuracy for the localization of the lymphoblast cells and separate it from the image scene. The results obtained indicate that the proposed method can be safely used for the purpose of lymphoblast cells localization and segmentation and subsequently, aiding the diagnosis of leukemia. © 2011 Springer Science+Business Media, LLC.","Automatic cell segmentation; Differential blood count; Image analysis; Leukemia diagnosis; Segmentation evaluation","article; artificial intelligence; cell nucleus; cell segmentation; cell shape; cell structure; cellular distribution; cellular parameters; cytoplasm; human; human cell; image analysis; image processing; leukemia; leukemia cell; leukocyte; lymphoblast; microscope image; thrombocyte; computer assisted diagnosis; differential diagnosis; lymphocyte; methodology; pathology; radiography; Diagnosis, Differential; Humans; Image Interpretation, Computer-Assisted; Image Processing, Computer-Assisted; Leukemia; Lymphocytes",Article,Scopus,2-s2.0-84873046381
"Yang Y., Tan W., Li T., Ruan D.","Consensus clustering based on constrained self-organizing map and improved Cop-Kmeans ensemble in intelligent decision support systems",2012,"Knowledge-Based Systems",15,10.1016/j.knosys.2011.08.011,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861586107&doi=10.1016%2fj.knosys.2011.08.011&partnerID=40&md5=584c6245795ddcb62fd447478d285b39","Data mining processes data from different perspectives into useful knowledge, and becomes an important component in designing intelligent decision support systems (IDSS). Clustering is an effective method to discover natural structures of data objects in data mining. Both clustering ensemble and semi-supervised clustering techniques have been emerged to improve the clustering performance of unsupervised clustering algorithms. Cop-Kmeans is a K-means variant that incorporates background knowledge in the form of pairwise constraints. However, there exists a constraint violation in Cop-Kmeans. This paper proposes an improved Cop-Kmeans (ICop-Kmeans) algorithm to solve the constraint violation of Cop-Kmeans. The certainty of objects is computed to obtain a better assignment order of objects by the weighted co-association. The paper proposes a new constrained self-organizing map (SOM) to combine multiple semi-supervised clustering solutions for further enhancing the performance of ICop-Kmeans. The proposed methods effectively improve the clustering results from the validated experiments and the quality of complex decisions in IDSS. © 2011 Elsevier B.V. All rights reserved.","Clustering ensemble; Cop-Kmeans; Decision support systems (DSS); Self-organizing map (SOM); Semi-supervised clustering","Background knowledge; Clustering Ensemble; Clustering results; Complex decision; Consensus clustering; Constraint violation; Cop-Kmeans; Data mining process; Data objects; Intelligent decision support systems; K-means; Natural structures; Pairwise constraints; Semi-supervised Clustering; Unsupervised clustering algorithm; Automobile drivers; Clustering algorithms; Conformal mapping; Decision support systems; Artificial intelligence",Article,Scopus,2-s2.0-84861586107
"Nilufar S., Ray N., Zhang H.","Object detection with DoG scale-space: A multiple kernel learning approach",2012,"IEEE Transactions on Image Processing",15,10.1109/TIP.2012.2192130,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864144291&doi=10.1109%2fTIP.2012.2192130&partnerID=40&md5=f5a45d7c75f2f28032c354f19a81c2fe","Difference of Gaussians (DoG) scale-space for an image is a significant way to generate features for object detection and classification. While applying DoG scale-space features for object detection/classification, we face two inevitable issues: dealing with high-dimensional data and selecting/weighting of proper scales. The scale selection process is mostly ad-hoc to present. In this paper, we propose a multiple kernel learning (MKL) method for both DoG scale selection/weighting and dealing with high-dimensional scale-space data. We design a novel shift invariant kernel function for DoG scale-space. To select only the useful scales in the DoG scale-space, a novel framework of MKL is also proposed. We utilize a 1-norm support vector machine (SVM) in the MKL optimization problem for sparse weighting of scales from DoG scale-space. The optimized data-dependent kernel accommodates only a few scales that are most discriminatory according to the large margin principle. With a 2-norm SVM, this learned kernel is applied to a challenging detection problem in oil sand mining: to detect large lumps in oil sand videos. We tested our method on several challenging oil sand data sets. Our method yields encouraging results on these difficult-to-process images and compares favorably against other popular multiple kernel methods. © 1992-2012 IEEE.","1-norm support vector machine (SVM); Circular convolution; Difference of Gaussian (DoG) scale-space; Multiple kernel learning (MKL)","Circular convolutions; Data sets; Detection problems; Difference of Gaussians; High dimensional data; High-dimensional; Kernel function; Large margin principle; Multiple Kernel Learning; Multiple kernels; Object Detection; Optimization problems; Sand mining; Scale selection; Scale-space; Shift invariant; Object recognition; Oil sands; Optimization; Support vector machines; algorithm; artificial intelligence; automated pattern recognition; computer assisted diagnosis; image enhancement; normal distribution; procedures; reproducibility; sensitivity and specificity; statistical analysis; article; automated pattern recognition; computer assisted diagnosis; methodology; Algorithms; Artificial Intelligence; Data Interpretation, Statistical; Image Enhancement; Image Interpretation, Computer-Assisted; Normal Distribution; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Algorithms; Artificial Intelligence; Data Interpretation, Statistical; Image Enhancement; Image Interpretation, Computer-Assisted; Normal Distribution; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84864144291
"Chalkiadakis G., Elkind E., Wooldridge M.","Cooperative game theory: Basic concepts and computational challenges",2012,"IEEE Intelligent Systems",15,10.1109/MIS.2012.47,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863933652&doi=10.1109%2fMIS.2012.47&partnerID=40&md5=0261f04e0db636d7f18f6e5ac275fb6c","Cooperative game theory studies situations in which agents can benefit by working together. This article outlines the key concepts of cooperative game theory, and discusess the challenges that arise in applying these in AI applications. © 2011 IEEE.","complexity; cooperative game theory; core; Shapley value","AI applications; Basic concepts; complexity; Computational challenges; Cooperative game theory; core; Shapley value; Artificial intelligence; Intelligent systems; Game theory",Article,Scopus,2-s2.0-84863933652
"Shafiei F., Sundaram D., Piramuthu S.","Multi-enterprise collaborative decision support system",2012,"Expert Systems with Applications",15,10.1016/j.eswa.2012.01.029,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858341259&doi=10.1016%2fj.eswa.2012.01.029&partnerID=40&md5=6ab2e5f2c55113df126c4786d67b17ab","The increasing level of collaboration among firms has necessitated the emergence of decision support tools that span firm boundaries. This essentially translates to decision support systems that seamlessly communicate and operate with disparate system characteristics. We propose and develop a multi-enterprise collaborative decision support system for SCM that aids in this process. The multi-enterprise collaborative decision making and support proposed and implemented in this study helps decision makers within and across organizational boundaries to generate more accurate, effective and timely decisions. We contend that this line of research is being well demonstrated in practice, and there is support for organizations requiring such support in their decision making environments. This study contributes to both the fields of ERP, DSS, and the decision making and support aspect of multi-enterprise collaboration. Decision makers in a multi-enterprise collaborative environment need flexible systems that allow for seamless integration among all members of organizational supply networks without being dependent on the knowledge of the users. Within such a system, decision makers from all across supply networks can access, and flexibly use decision making components, explore a range of what-if scenarios and make the best decision for their organization and their customers, partners, etc. based on the results. © 2012 Elsevier Ltd. All rights reserved.","Decision support systems; Multi-enterprise collaboration","Best decision; Collaborative decision making; Collaborative decisions; Collaborative environments; Decision makers; Decision support tools; Disparate systems; Firm boundaries; Flexible system; Multi-enterprise collaboration; Organizational boundaries; Seamless integration; Supply networks; What-if scenarios; Artificial intelligence; Decision making; Decision support systems; Industry",Article,Scopus,2-s2.0-84858341259
"Jung J.J.","Semantic optimization of query transformation in a large-scale peer-to-peer network",2012,"Neurocomputing",15,10.1016/j.neucom.2011.08.028,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860227485&doi=10.1016%2fj.neucom.2011.08.028&partnerID=40&md5=a527dbb15a88a4acd363f5d7aa01e1c5","Ontologies have an important role in supporting efficient interoperability among information systems in distributed environment. In this paper, I propose a query transformation method to efficiently collect as many relevant resources from the distributed information systems (e.g., peer-to-peer network) as possible. More importantly, I consider a composite query which contains multiple semantics. The composite query from the source peer can be decomposed and propagated as maintaining the original contexts. Through the experiments, I have shown that query-activated concept (QAC)-based schemes have fulfilled an efficient query decomposition process on semantic peer-to-peer networks. © 2012 Elsevier B.V.","Collective intelligence; Query transformation; Semantic peer-to-peer networks; Tag matching","Collective intelligences; Distributed environments; Distributed information systems; Query decomposition; Query transformations; Semantic peer-to-peer network; Tag matching; Interoperability; Peer to peer networks; Semantics; Distributed computer systems; article; artificial intelligence; artificial neural network; computer language; information processing; information retrieval; information system; priority journal; process optimization; query transformation; semantic optimization; semantic peer to peer network; semantics; social network",Article,Scopus,2-s2.0-84860227485
"Nuxoll A.M., Laird J.E.","Enhancing intelligent agents with episodic memory",2012,"Cognitive Systems Research",15,10.1016/j.cogsys.2011.10.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858004963&doi=10.1016%2fj.cogsys.2011.10.002&partnerID=40&md5=fc935abada4f2fc2d51c6d6e29087685","For a human, episodic memory is a memory of past experiences that one gains over a lifetime. While episodic memory appears critical to human function, researchers have done little to explore the potential benefits for an artificially intelligent agent. In this research, we have added a task-independent, episodic memory to a cognitive architecture. To frame the research, we propose that episodic memory supports a set of cognitive capabilities that improve an agent's ability to sense its environment, reason, and learn. We demonstrate that episodic memory enables agents created with our architecture to employ these cognitive capabilities. © 2011 Elsevier B.V.","Cognitive architecture; Episodic memory; Soar","Cognitive architectures; Cognitive capability; Episodic memories; Potential benefits; Soar; Artificial intelligence; Intelligent agents; Research; Memory architecture; article; artificial intelligence; autoanalysis; computer model; computer prediction; computer simulation; episodic memory; information retrieval; information storage; learning algorithm; priority journal; research priority; signal processing; system analysis; virtual reality; working memory",Article,Scopus,2-s2.0-84858004963
"Ling S.H., Nguyen H.T.","Natural occurrence of nocturnal hypoglycemia detection using hybrid particle swarm optimized fuzzy reasoning model",2012,"Artificial Intelligence in Medicine",15,10.1016/j.artmed.2012.04.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863776233&doi=10.1016%2fj.artmed.2012.04.003&partnerID=40&md5=7b7ec2c293b635ee1016471eea4c738d","Introduction: Low blood glucose (hypoglycemia) is a common and serious side effect of insulin therapy in patients with diabetes. This paper will make a contribution to knowledge in the modeling and design of a non-invasive hypoglycemia monitor for patients with type 1 diabetes mellitus (T1DM) using a fuzzy-reasoning system. Methods: Based on the heart rate and the corrected QT interval of the electrocardiogram (ECG) signal, we have developed a hybrid particle-swarm-optimization-based fuzzy-reasoning model to recognize the presence of hypoglycemic episodes. To optimize the fuzzy rules and the fuzzy-membership functions, a hybrid particle-swarm-optimization with wavelet mutation operation is investigated. Results: From our clinical study of 16 children with T1DM, natural occurrence of nocturnal-hypoglycemic episodes was associated with increased heart rates and increased corrected QT intervals. All the data sets were collected from the Government of Western Australia's Department of Health. All data were organized randomly into a training set (8 patients with 320 data points) and a testing set (another 8 patients with 269 data points). To prevent the phenomenon of overtraining, we separated the training set into 2 sets (4 patients in each set) and a fitness function was introduced for this training process. The testing performances of the proposed algorithm for detection of advanced hypoglycemic episodes (sensitivity = 85.71% and specificity = 79.84%) and hypoglycemic episodes (sensitivity = 80.00% and specificity = 55.14%) were given. Conclusion: We have investigated the detection for the natural occurrence of nocturnal hypoglycemic episodes in T1DM using a hybrid particle-swarm-optimization-based fuzzy-reasoning model with physiological parameters. In this study, no restricted environment (e.g. patient's dietary requirements) is required. Furthermore, the sampling time is between 5 and 10. min. To conclude, we have shown that the testing performances of the proposed algorithm for detection of advanced hypoglycemic and hypoglycemic episodes for T1DM patients are satisfactory. © 2012 Elsevier B.V.","Fuzzy-reasoning model; Hypoglycemia detection; Particle-swarm optimization","Blood glucose; Clinical study; Data points; Data sets; Department of healths; Electrocardiogram signal; Fitness functions; Fuzzy reasoning; Heart rates; Hybrid particles; Insulin therapy; Mutation operations; Physiological parameters; Sampling time; Side effect; Testing performance; Training process; Training sets; Type 1 diabetes mellitus; Western Australia; Algorithms; Electrocardiography; Glucose; Heart; Insulin; Optimization; Physiological models; Patient treatment; glucose; adolescent; article; blood glucose monitoring; clinical article; controlled study; electrocardiography; fuzzy logic; glucose blood level; heart rate; human; hybrid; insulin dependent diabetes mellitus; mutation; nocturnal hypoglycemia; priority journal; QT interval; sensitivity and specificity; Adolescent; Algorithms; Artificial Intelligence; Blood Glucose; Diabetes Mellitus, Type 1; Electrocardiography; Fuzzy Logic; Heart Rate; Humans; Hypoglycemia; Hypoglycemic Agents; Insulin; Sensitivity and Specificity",Article,Scopus,2-s2.0-84863776233
"Layton R., Watters P., Dazeley R.","Recentred local profiles for authorship attribution",2012,"Natural Language Engineering",15,10.1017/S1351324911000180,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872014623&doi=10.1017%2fS1351324911000180&partnerID=40&md5=aa7b3ba2ddc0e46291e8ee7272ae60e3","Authorship attribution methods aim to determine the author of a document, by using information gathered from a set of documents with known authors. One method of performing this task is to create profiles containing distinctive features known to be used by each author. In this paper, a new method of creating an author or document profile is presented that detects features considered distinctive, compared to normal language usage. This recentreing approach creates more accurate profiles than previous methods, as demonstrated empirically using a known corpus of authorship problems. This method, named recentred local profiles, determines authorship accurately using a simple 'best matching author' approach to classification, compared to other methods in the literature. The proposed method is shown to be more stable than related methods as parameter values change. Using a weighted voting scheme, recentred local profiles is shown to outperform other methods in authorship attribution, with an overall accuracy of 69.9% on the ad-hoc authorship attribution competition corpus, representing a significant improvement over related methods. Copyright © Cambridge University Press 2011.",,"Authorship attribution; Parameter values; Weighted voting; Artificial intelligence; Software engineering",Article,Scopus,2-s2.0-84872014623
"Yang M., Zheng H., Wang H., McClean S., Hall J., Harris N.","A machine learning approach to assessing gait patterns for Complex Regional Pain Syndrome",2012,"Medical Engineering and Physics",15,10.1016/j.medengphy.2011.09.018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863310500&doi=10.1016%2fj.medengphy.2011.09.018&partnerID=40&md5=2117b92eda19a28e07d8cd6e2407559a","Complex Regional Pain Syndrome (CRPS) is a condition that causes a long-term burning pain in a limb or part of a limb and it can cause various degrees of the physical functional performance deterioration. Objective assessment of physical functional performance of patients is one critical component to evaluate the therapy outcome for CRPS. This paper aims to investigate the feasibility of assessing the physical performance of patients with Complex Regional Pain Syndrome based on the analysis of gait data recorded by an accelerometer in short walking distances. Ten subjects with CRPS and ten control subjects were recruited. Thirty three features were extracted from each recording. A machine learning method, Multilayer perceptron neural-networks (MLP), was applied to classify the normal and abnormal gait patterns from data obtained on a 2.4. m performance evaluation test. The best classification accuracy (99.38%) was achieved using 3 features selected by a step-wise-forward method. To further validate its performance, an independent test set including 14 cases extracted from data obtained on a 20. m performance evaluation test was adopted. A prediction accuracy of 85.7% was obtained. © 2011 IPEM.","Accelerometer; Classification; Complex Regional Pain Syndrome; Feature extraction; Gait analysis","Classification accuracy; Complex regional pain syndromes; Control subject; Critical component; Functional performance; Gait pattern; Learning approach; Machine learning methods; Multi layer perceptron; Objective assessment; Performance evaluation; Physical performance; Prediction accuracy; Test sets; Accelerometers; Classification (of information); Feature extraction; Gait analysis; Learning systems; Patient treatment; Health; accelerometer; accuracy; adult; aged; article; clinical article; complex regional pain syndrome; controlled study; disease classification; feasibility study; female; gait; human; information processing; machine learning; male; Multilayer perceptron neural network; physical performance; prediction; priority journal; walking difficulty; Acceleration; Adult; Aged; Artificial Intelligence; Complex Regional Pain Syndromes; Feasibility Studies; Female; Gait; Humans; Male; Middle Aged; Reproducibility of Results; Time Factors",Article,Scopus,2-s2.0-84863310500
"Chang N.-B., Pongsanone N.P., Ernest A.","A rule-based decision support system for sensor deployment in small drinking water networks",2012,"Journal of Cleaner Production",15,10.1016/j.jclepro.2012.02.010,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858718760&doi=10.1016%2fj.jclepro.2012.02.010&partnerID=40&md5=9fc21c97fdae337a1bd4f777c133d551","The physical layout of drinking water utilities makes them inherently vulnerable to contamination incidents caused by routine operations. These contaminations present environmental health concerns including but not limited to total trihalomethanes, lead, and chlorine residual issues. To achieve the goal of cleaner production, sensor placement in municipal drinking water networks in response to possible public health threats has become one of the most significant challenges currently facing drinking water utilities, especially in small-scale communities. Long-term monitoring is needed to develop modern concepts and approaches to risk management for these utilities. We developed a Rule-based Decision Support System (RBDSS), a methodology to generate near-optimal sensor deployment strategies with low computational burden, such as those we often encountered in large-scale optimization analyses. Three rules were derived to address the efficacy and efficiency characteristics of such a sensor deployment process: (1) intensity, (2) accessibility, and (3) complexity rules. Implementation potential of this RBDSS was assessed for a small-scale drinking water network in rural Kentucky, United States. Our case study showed that RBDSS is able to generate the near-optimal sensor deployment strategies for small-scale drinking water distribution networks relatively quickly. The RBDSS is transformative and transferable to drinking water distribution networks elsewhere with any scale. © 2012 Elsevier Ltd. All rights reserved.","Decision support system; Drinking water; EPANET; Graph theory; Rule-based sensor deployment; Systems analysis","Chlorine residuals; Cleaner production; Computational burden; Drinking water distribution networks; Efficiency characteristic; Environmental health concern; EPANET; Kentucky; Large-scale optimization; Long term monitoring; Physical layout; Rule based; Rule-based decision support system; Sensor deployment; Sensor placement; Trihalomethanes; Artificial intelligence; Chlorine; Decision support systems; Electric utilities; Graph theory; Optimization; Pollution control; Potable water; Risk management; Systems analysis; Water analysis; Water supply; Knowledge based systems",Article,Scopus,2-s2.0-84858718760
"Ahn J.J., Byun H.W., Oh K.J., Kim T.Y.","Using ridge regression with genetic algorithm to enhance real estate appraisal forecasting",2012,"Expert Systems with Applications",15,10.1016/j.eswa.2012.01.183,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862783733&doi=10.1016%2fj.eswa.2012.01.183&partnerID=40&md5=bc20e3deb403d3b78adc61e990b322e2","This study considers real estate appraisal forecasting problem. While there is a great deal of literature about use of artificial intelligence and multiple linear regression for the problem, there has been always controversy about which one performs better. Noting that this controversy is due to difficulty finding proper predictor variables in real estate appraisal, we propose a modified version of ridge regression, i.e.; ridge regression coupled with genetic algorithm (GA-Ridge). In order to examine the performance of the proposed method, experimental study is done for Korean real estate market, which verifies that GA-Ridge is effective in forecasting real estate appraisal. This study addresses two critical issues regarding the use of ridge regression, i.e.; when to use it and how to improve it. © 2012 Elsevier Ltd. All rights reserved.","Genetic algorithm; Real estate market; Ridge regression","Critical issues; Experimental studies; Forecasting problems; Multiple linear regressions; Predictor variables; Real estate; Real estate market; Ridge regression; Artificial intelligence; Genetic algorithms; Linear regression; Forecasting",Article,Scopus,2-s2.0-84862783733
"Johnson T.T., Mitra S.","A small model theorem for rectangular hybrid automata networks",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",15,10.1007/978-3-642-30793-5_2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862734046&doi=10.1007%2f978-3-642-30793-5_2&partnerID=40&md5=8afea94638608d9a2e65767df67e8d7b","Rectangular hybrid automata (RHA) are finite state machines with additional skewed clocks that are useful for modeling realtime systems. This paper is concerned with the uniform verification of safety properties of networks with arbitrarily many interacting RHAs. Each automaton is equipped with a finite collection of pointers to other automata that enables it to read their state. This paper presents a small model result for such networks that reduces the verification problem for a system with arbitrarily many processes to a system with finitely many processes. The result is applied to verify and discover counterexamples of inductive invariant properties for distributed protocols like Fischer's mutual exclusion algorithm and the Small Aircraft Transportation System (SATS).We have implemented a prototype tool called Passel relying on the satisfiability modulo theories (SMT) solver Z3 to check inductive invariants automatically. © 2012 IFIP International Federation for Information Processing.","hybrid automata; parameterized verification; small model theorem; uniform verification","Distributed protocols; Hybrid automatons; Invariant properties; Model results; Mutual exclusion algorithms; Parameterized verifications; Prototype tools; Safety property; Satisfiability modulo Theories; Small Aircraft Transportation Systems; Small model theorem; Verification problems; Artificial intelligence; Automata theory",Conference Paper,Scopus,2-s2.0-84862734046
"Coron J.-S., Giraud C., Prouff E., Renner S., Rivain M., Vadnala P.K.","Conversion of security proofs from one leakage model to another: A new issue",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",15,10.1007/978-3-642-29912-4_6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862149254&doi=10.1007%2f978-3-642-29912-4_6&partnerID=40&md5=8e4a71e3f23cafb33cb38627dfd10c1c","To guarantee the security of a cryptographic implementation against Side Channel Attacks, a common approach is to formally prove the security of the corresponding scheme in a model as pertinent as possible. Nowadays, security proofs for masking schemes in the literature are usually conducted for models where only the manipulated data are assumed to leak. However in practice, the leakage is better modeled encompassing the memory transitions as e.g. the Hamming distance model. From this observation, a natural question is to decide at which extent a countermeasure proved to be secure in the first model stays secure in the second. In this paper, we look at this issue and we show that it must definitely be taken into account. Indeed, we show that a countermeasure proved to be secure against second-order side-channel attacks in the first model becomes vulnerable against a first-order side-channel attack in the second model. Our result emphasize the issue of porting an implementation from devices leaking only on the manipulated data to devices leaking on the memory transitions. © 2012 Springer-Verlag.",,"Cryptographic implementation; First-order; Leakage model; Masking schemes; Second orders; Security proofs; Side channel attack; Artificial intelligence; Hamming distance",Conference Paper,Scopus,2-s2.0-84862149254
"Lyon C., Nehaniv C.L., Saunders J.","Interactive language learning by robots: The transition from Babbling to word forms",2012,"PLoS ONE",15,10.1371/journal.pone.0038236,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862199731&doi=10.1371%2fjournal.pone.0038236&partnerID=40&md5=748f2d7cd62e5ffcc27eec05a4bec6ac","The advent of humanoid robots has enabled a new approach to investigating the acquisition of language, and we report on the development of robots able to acquire rudimentary linguistic skills. Our work focuses on early stages analogous to some characteristics of a human child of about 6 to 14 months, the transition from babbling to first word forms. We investigate one mechanism among many that may contribute to this process, a key factor being the sensitivity of learners to the statistical distribution of linguistic elements. As well as being necessary for learning word meanings, the acquisition of anchor word forms facilitates the segmentation of an acoustic stream through other mechanisms. In our experiments some salient one-syllable word forms are learnt by a humanoid robot in real-time interactions with naive participants. Words emerge from random syllabic babble through a learning process based on a dialogue between the robot and the human participant, whose speech is perceived by the robot as a stream of phonemes. Numerous ways of representing the speech as syllabic segments are possible. Furthermore, the pronunciation of many words in spontaneous speech is variable. However, in line with research elsewhere, we observe that salient content words are more likely than function words to have consistent canonical representations; thus their relative frequency increases, as does their influence on the learner. Variable pronunciation may contribute to early word form acquisition. The importance of contingent interaction in real-time between teacher and learner is reflected by a reinforcement process, with variable success. The examination of individual cases may be more informative than group results. Nevertheless, word forms are usually produced by the robot after a few minutes of dialogue, employing a simple, real-time, frequency dependent mechanism. This work shows the potential of human-robot interaction systems in studies of the dynamics of early language acquisition. © 2012 Lyon et al.",,"article; controlled study; interactive voice response system; language; language development; learning; linguistics; reinforcement; robotics; skill; speech; teacher; word recognition; Artificial Intelligence; Language; Robotics",Article,Scopus,2-s2.0-84862199731
"Chen C.-T., Peng H.-P., Jian J.-W., Tsai K.-C., Chang J.-Y., Yang E.-W., Chen J.-B., Ho S.-Y., Hsu W.-L., Yang A.-S.","Protein-protein interaction site predictions with three-dimensional probability distributions of interacting atoms on protein surfaces",2012,"PLoS ONE",15,10.1371/journal.pone.0037706,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862003258&doi=10.1371%2fjournal.pone.0037706&partnerID=40&md5=34482925a1672b17a9f76d69c4327405","Protein-protein interactions are key to many biological processes. Computational methodologies devised to predict protein-protein interaction (PPI) sites on protein surfaces are important tools in providing insights into the biological functions of proteins and in developing therapeutics targeting the protein-protein interaction sites. One of the general features of PPI sites is that the core regions from the two interacting protein surfaces are complementary to each other, similar to the interior of proteins in packing density and in the physicochemical nature of the amino acid composition. In this work, we simulated the physicochemical complementarities by constructing three-dimensional probability density maps of non-covalent interacting atoms on the protein surfaces. The interacting probabilities were derived from the interior of known structures. Machine learning algorithms were applied to learn the characteristic patterns of the probability density maps specific to the PPI sites. The trained predictors for PPI sites were cross-validated with the training cases (consisting of 432 proteins) and were tested on an independent dataset (consisting of 142 proteins). The residue-based Matthews correlation coefficient for the independent test set was 0.423; the accuracy, precision, sensitivity, specificity were 0.753, 0.519, 0.677, and 0.779 respectively. The benchmark results indicate that the optimized machine learning models are among the best predictors in identifying PPI sites on protein surfaces. In particular, the PPI site prediction accuracy increases with increasing size of the PPI site and with increasing hydrophobicity in amino acid composition of the PPI interface; the core interface regions are more likely to be recognized with high prediction confidence. The results indicate that the physicochemical complementarity patterns on protein surfaces are important determinants in PPIs, and a substantial portion of the PPI sites can be predicted correctly with the physicochemical complementarity features based on the non-covalent interaction data derived from protein interiors. © 2012 Chen et al.",,"amino acid; protein; amino acid; accuracy; algorithm; amino acid composition; article; atom; controlled study; covalent bond; hydrophobicity; machine learning; molecular model; physical chemistry; prediction; probability; protein database; protein protein interaction; sensitivity and specificity; simulation; surface property; validation process; artificial intelligence; artificial neural network; biology; chemical model; chemical structure; chemistry; computer simulation; methodology; nonparametric test; probability; protein analysis; statistical distribution; Algorithms; Amino Acids; Artificial Intelligence; Computational Biology; Computer Simulation; Models, Chemical; Models, Molecular; Neural Networks (Computer); Probability; Protein Interaction Mapping; Proteins; Statistical Distributions; Statistics, Nonparametric",Article,Scopus,2-s2.0-84862003258
"Martínez-Prieto M.A., Arias Gallego M., Fernández J.D.","Exchange and consumption of huge RDF data",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",15,10.1007/978-3-642-30284-8_36,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861723983&doi=10.1007%2f978-3-642-30284-8_36&partnerID=40&md5=0bb25aad9fb7cce6e8416b1ed68bf371","Huge RDF datasets are currently exchanged on textual RDF formats, hence consumers need to post-process them using RDF stores for local consumption, such as indexing and SPARQL query. This results in a painful task requiring a great effort in terms of time and computational resources. A first approach to lightweight data exchange is a compact (binary) RDF serialization format called HDT. In this paper, we show how to enhance the exchanged HDT with additional structures to support some basic forms of SPARQL query resolution without the need of ""unpacking"" the data. Experiments show that i) with an exchanging efficiency that outperforms universal compression, ii) post-processing now becomes a fast process which iii) provides competitive query performance at consumption. © 2012 Springer-Verlag.",,"Additional structures; Computational resources; Data sets; Fast process; Post process; Post processing; Query performance; Query resolution; RDF data; Rdf stores; Universal compression; Artificial intelligence; Electronic data interchange",Conference Paper,Scopus,2-s2.0-84861723983
"Kim W., Kim K.S., Lee J.E., Noh D.-Y., Kim S.-W., Jung Y.S., Park M.Y., Park R.W.","Development of novel breast cancer recurrence prediction model using support vector machine",2012,"Journal of Breast Cancer",15,10.4048/jbc.2012.15.2.230,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865143565&doi=10.4048%2fjbc.2012.15.2.230&partnerID=40&md5=8f1be357cbc23f1eb8a2a54c6747d905","Purpose: The prediction of breast cancer recurrence is a crucial factor for successful treatment and follow-up planning. The principal objective of this study was to construct a novel prognostic model based on support vector machine (SVM) for the prediction of breast cancer recurrence within 5 years after breast cancer surgery in the Korean population, and to compare the predictive performance of the model with the previously established models. Methods: Data on 679 patients, who underwent breast cancer surgery between 1994 and 2002, were collected retrospectively from a Korean tertiary teaching hospital. The following variables were selected as independent variables for the prognostic model, by using the established medical knowledge and univariate analysis: histological grade, tumor size, number of metastatic lymph node, estrogen receptor, lymphovascular invasion, local invasion of tumor, and number of tumors. Three prediction algorithms, with each using SVM, artificial neural network and Cox-proportional hazard regression model, were constructed and compared with one another. The resultant and most effective model based on SVM was compared with previously established prognostic models, which included Adjuvant! Online, Nottingham prognostic index (NPI), and St. Gallen guidelines. Results: The SVM-based prediction model, named ''breast cancer recurrence prediction based on SVM (BCRSVM),' proposed herein outperformed other prognostic models (area under the curve=0.85, 0.71, 0.70, respectively for the BCRSVM, Adjuvant! Online, and NPI). The BCRSVM evidenced substantially high sensitivity (0.89), specificity (0.73), positive predictive values (0.75), and negative predictive values (0.89). Conclusion: As the selected prognostic factors can be easily obtained in clinical practice, the proposed model might prove useful in the prediction of breast cancer recurrence. The prediction model is freely available in the website (http://ami.ajou.ac.kr/bcr/). © 2012 Korean Breast Cancer Society. All rights reserved.","Artificial intelligence; Breast neoplasms; Neural networks; Recurrence; Risk factors","estrogen receptor; adult; article; breast cancer; cancer grading; cancer prognosis; cancer recurrence; controlled study; follow up; human; independent variable; Korea; lymph node metastasis; lymph vessel metastasis; major clinical study; postoperative period; practice guideline; prediction; proportional hazards model; support vector machine; tertiary health care; tumor volume; univariate analysis",Article,Scopus,2-s2.0-84865143565
"Jouhet V., Defossez G., Burgun A., le Beux P., Levillain P., Ingrand P., Claveau V.","Automated classification of free-text pathology reports for registration of incident cases of cancer",2012,"Methods of Information in Medicine",15,10.3414/ME11-01-0005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861322936&doi=10.3414%2fME11-01-0005&partnerID=40&md5=d475bc596f0bfc441bf8ebc69dd43513","Objective: Our study aimed to construct and evaluate functions called ""classifiers"", produced by supervised machine learning techniques, in order to categorize automatically pathology reports using solely their content. Methods: Patients from the Poitou-Charen -tes Cancer Registry having at least one pathology report and a single non-metastatic invasive neoplasm were included. A descriptor weighting function accounting for the distribution of terms among targeted classes was developed and compared to classic methods based on inverse document frequencies. The classification was performed with support vector machine (SVM) and Naive Bayes classifiers. Two levels of granularity were tested for both the topographical and the morphological axes of the ICD-O3 code. The ability to correctly attribute a precise ICD-O3 code and the ability to attribute the broad category defined by the International Agency for Research on Cancer (IARC) for the multiple primary cancer registration rules were evaluated using F1-measures. Results: 5121 pathology reports produced by 35 pathologists were selected. The best performance was achieved by our class-weighted descriptor, associated with a SVM classifier. Using this method, the pathology reports were properly classified in the IARC categories with F1-measures of 0.967 for both topography and morphology. The ICD-O3 code attribution had lower performance with a 0.715 F1-measure for topography and 0.854 for morphology. Conclusion: These results suggest that free-text pathology reports could be useful as a data source for automated systems in order to identify and notify new cases of cancer. Future work is needed to evaluate the improvement in performance obtained from the use of nat -ural language processing, including the case of multiple tumor description and possible incorporation of other medical documents such as surgical reports. © Schattauer 2012.","Automated Classification; Free Text; Medical Informatics; Neoplasm; Pathology","article; artificial intelligence; classification; France; human; International Classification of Diseases; medical informatics; neoplasm; organization and management; pathology; register; semantics; Artificial Intelligence; France; Humans; International Classification of Diseases; Medical Informatics; Neoplasms; Pathology; Registries; Semantics",Article,Scopus,2-s2.0-84861322936
"Duda P., Jaworski M., Pietruczuk L.","On pre-processing algorithms for data stream",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",15,10.1007/978-3-642-29350-4-7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861091461&doi=10.1007%2f978-3-642-29350-4-7&partnerID=40&md5=3e96ebceb27be3dbd15a97d8f17b7c04","Clustering is a one of the most important tasks of data mining. Algorithms like the Fuzzy C-Means and Possibilistic C-Means provide good result both for the static data and data streams. All clustering algorithms compute centers from chunk of data, what requires a lot of time. If the rate of incoming data is faster than speed of algorithm, part of data will be lost. To prevent such situation, some pre-processing algorithms should be used. The purpose of this paper is to propose a pre-processing method for clustering algorithms. Experimental results show that proposed method is appropriate to handle noisy data and can accelerate processing time. © 2012 Springer-Verlag Berlin Heidelberg.","data streams; fuzzy clustering; pre-processing","Data stream; Fuzzy C mean; Noisy data; Possibilistic C-means; Pre-processing; Pre-processing algorithms; Pre-processing method; Processing time; Artificial intelligence; Clustering algorithms; Data communication systems; Data processing; Fuzzy clustering; Processing; Soft computing; Data mining",Conference Paper,Scopus,2-s2.0-84861091461
"Jaworski M., Duda P., Pietruczuk L.","On fuzzy clustering of data streams with concept drift",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",15,10.1007/978-3-642-29350-4-10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861073291&doi=10.1007%2f978-3-642-29350-4-10&partnerID=40&md5=9225512351ea377c2e37fb01e10d8ce0","In the paper the clustering algorithms based on fuzzy set theory are considered. Modifications of the Fuzzy C-Means and the Possibilistic C-Means algorithms are presented, which adjust them to deal with data streams. Since data stream is of infinite size, it has to be partitioned into chunks. Simulations show that this partitioning procedure does not affect the quality of clustering results significantly. Moreover, properly chosen weights can be assigned to each data element. This modification allows the presented algorithms to handle concept drift during simulations. © 2012 Springer-Verlag Berlin Heidelberg.",,"Concept drifts; Data elements; Data stream; Fuzzy C mean; Possibilistic C-means; Quality of clustering; Artificial intelligence; Clustering algorithms; Fuzzy set theory; Soft computing; Data communication systems",Conference Paper,Scopus,2-s2.0-84861073291
"Sopyła K., Drozda P., Górecki P.","SVM with CUDA accelerated kernels for big sparse problems",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",15,10.1007/978-3-642-29347-4_51,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861073337&doi=10.1007%2f978-3-642-29347-4_51&partnerID=40&md5=e060f6a62de9700831ccf239d44c6207","The SVM algorithm is one of the most frequently used methods for the classification process. For many domains, where the classification problems have many features as well as numerous instances, classification is a difficult and time-consuming task. For this reason, the following paper presents the CSR-GPU-SVM algorithm which accelerates SVM training for large and sparse problems with the use of the CUDA technology. Implementation is based on the SMO (Sequential Minimal Optimization) algorithm and utilizes the CSR(Compressed Sparse Row) sparse matrix format. The proposed solution allows us to perform efficient classification of big datasets, for example rcv1 and newsgroup20, for which classification with dense representation is not possible. The performed experiments have proven the accelerations in the order of 6 - 35 training times compared to original LibSVM implementation. © 2012 Springer-Verlag Berlin Heidelberg.","Classification; CUDA; GPGPU; Sparse Matrix; SVM","Classification process; Compressed sparse row; CUDA; Data sets; GPGPU; Sequential minimal optimization; Sparse matrices; SVM; SVM algorithm; Time-consuming tasks; Artificial intelligence; Optimization; Program processors; Soft computing; Classification (of information)",Conference Paper,Scopus,2-s2.0-84861073337
"Duda P., Hayashi Y., Jaworski M.","On the strong convergence of the orthogonal series-type kernel regression neural networks in a non-stationary environment",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",15,10.1007/978-3-642-29347-4_6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861028411&doi=10.1007%2f978-3-642-29347-4_6&partnerID=40&md5=5891a06f14eb1a3a8bde227f42f5d640","Strong convergence of general regression neural networks is proved assuming non-stationary noise. The network is based on the orthogonal series-type kernel. Simulation results are discussed in details. © 2012 Springer-Verlag Berlin Heidelberg.",,"General regression neural network; Kernel regression; Non-stationary environment; Nonstationary noise; Series-type; Strong convergence; Artificial intelligence; Soft computing; Fourier analysis",Conference Paper,Scopus,2-s2.0-84861028411
"Salomaa A.","On state sequences defined by reaction systems",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",15,10.1007/978-3-642-29485-3_17,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861040359&doi=10.1007%2f978-3-642-29485-3_17&partnerID=40&md5=b76512a5a9282c7348d2c36749b2174c","The paper investigates sequences generated by reaction systems. Arbitrary sequences can be generated if the cardinalities of the sets of reactants and inhibitors are unbounded. Most of the paper investigates systems where both of these cardinalities equal 1. A general result is obtained concerning sequences generated by systems with interaction. New estimates are obtained for lengths of sequences in the non-interactive case. © 2012 Springer-Verlag Berlin Heidelberg.","inhibitor; interactive process; reactant; reaction system; state sequence","inhibitor; Interactive process; reactant; Reaction system; State sequences; Artificial intelligence; Semantics",Article,Scopus,2-s2.0-84861040359
"Zhang S., McClean S.I., Scotney B.W.","Probabilistic learning from incomplete data for recognition of activities of daily living in smart homes",2012,"IEEE Transactions on Information Technology in Biomedicine",15,10.1109/TITB.2012.2188534,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860685669&doi=10.1109%2fTITB.2012.2188534&partnerID=40&md5=e6e4656edbcc6474dc9ff7d3e9c5cb68","Learning behavioral patterns for activities of daily living in a smart home environment can be challenged by the limited number of training data that may be available. This may be due to the infrequent repetition of routine activities (e.g., once daily), the expense of using observers to label activities, and the intrusion that would be caused by the presence of observers over long time periods. It is important, therefore, to make as much use of any labeled data that are collected, however, incomplete these data may be. In this paper, we propose an algorithm for learning behavioral patterns for multi-inhabitants living in a single smart home environment, by making full use of all limited labeled activities, including incomplete data resulting from unreliable low-level sensors in this environment. Through maximum-likelihood estimation, using Expectation-Maximization, we build a model that captures both environmental uncertainties from sensor readings and user uncertainties, including variations in how individuals carry out activities. Our algorithm outperforms models that cannot handle data incompleteness, with increasing performance gains as incompleteness increases. The approach also enables the impact of particular sensors to be assessed and can thus inform sensor maintenance and deployment. © 2012 IEEE.","activities of daily living (ADLs); Activity recognition; Expectation-Maximization (EM) algorithm; incomplete data; probabilistic learning","Activities of Daily Living; Activity recognition; Expectation-maximization algorithms; Incomplete data; Probabilistic Learning; Automation; Intelligent buildings; Maximum principle; Sensors; Algorithms; algorithm; ambulatory monitoring; article; artificial intelligence; automated pattern recognition; computer simulation; daily life activity; human; methodology; theoretical model; Activities of Daily Living; Algorithms; Artificial Intelligence; Computer Simulation; Humans; Models, Theoretical; Monitoring, Ambulatory; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84860685669
"Ntoutsi I., Stefanidis K., Norvag K., Kriegel H.-P.","gRecs: A group recommendation system based on user clustering",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",15,10.1007/978-3-642-29035-0_25,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860666535&doi=10.1007%2f978-3-642-29035-0_25&partnerID=40&md5=8ebd98dd575a7023f5603cef1946e037","In this demonstration paper, we present gRecs, a system for group recommendations that follows a collaborative strategy. We enhance recommendations with the notion of support to model the confidence of the recommendations. Moreover, we propose partitioning users into clusters of similar ones. This way, recommendations for users are produced with respect to the preferences of their cluster members without extensively searching for similar users in the whole user base. Finally, we leverage the power of a top-k algorithm for locating the top-k group recommendations. © 2012 Springer-Verlag.",,"Collaborative strategies; Group recommendations; Artificial intelligence; Database systems",Conference Paper,Scopus,2-s2.0-84860666535
"Nakov P., Tou Ng H.","Improving statistical machine translation for a resource-poor language using related resource-rich languages",2012,"Journal of Artificial Intelligence Research",15,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866860084&partnerID=40&md5=c5c8206ef3d803bdce55b9444a6b027a","We propose a novel language-independent approach for improving machine translation for resource-poor languages by exploiting their similarity to resource-rich ones. More precisely, we improve the translation from a resource-poor source language X1 into a resourcerich language Y given a bi-text containing a limited number of parallel sentences for X 1-Y and a larger bi-text for X 2-Y for some resource-rich language X 2 that is closely related to X 1. This is achieved by taking advantage of the opportunities that vocabulary overlap and similarities between the languages X 1 and X 2 in spelling, word order, and syntax offer: (1) we improve the word alignments for the resource-poor language, (2) we further augment it with additional translation options, and (3) we take care of potential spelling differences through appropriate transliteration. The evaluation for Indonesian!English using Malay and for Spanish!English using Portuguese and pretending Spanish is resource-poor shows an absolute gain of up to 1.35 and 3.37 BLEU points, respectively, which is an improvement over the best rivaling approaches, while using much less additional data. Overall, our method cuts the amount of necessary ""real"" training data by a factor of 2-5. © 2012 AI Access Foundation. All rights reserved.",,"Absolute gain; Machine translations; Resource-Rich; Source language; Statistical machine translation; Training data; Word alignment; Word orders; Artificial intelligence; Translation (languages)",Article,Scopus,2-s2.0-84866860084
"Gu Y., Wang L.-L., Tai X.-C.","A direct approach toward global minimization for multiphase labeling and segmentation problems",2012,"IEEE Transactions on Image Processing",15,10.1109/TIP.2011.2182522,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860118779&doi=10.1109%2fTIP.2011.2182522&partnerID=40&md5=b84f23db1c95893b4747dc2f37d43e2c","This paper intends to extend the minimization algorithm developed by Bae, Yuan and Tai [IJCV, 2011] in several directions. First, we propose a new primal-dual approach for global minimization of the continuous Potts model with applications to the piecewise constant Mumford-Shah model for multiphase image segmentation. Different from the existing methods, we work directly with the binary setting without using convex relaxation, which is thereby termed as a direct approach. Second, we provide the sufficient and necessary conditions to guarantee a global optimum. Moreover, we provide efficient algorithms based on a reduction in the intermediate unknowns from the augmented Lagrangian formulation. As a result, the underlying algorithms involve significantly fewer parameters and unknowns than the naive use of augmented Lagrangian-based methods; hence, they are fast and easy to implement. Furthermore, they can produce global optimums under mild conditions. © 1992-2012 IEEE.","Augmented Lagrangian method (ALM); Chambolle's algorithm; continuous Potts model; global optimum; multi class labeling; multiphase segmentation; Mumford-Shah model; primal-dual formulation","Augmented Lagrangian methods; Global optimum; Multi-class; Mumford Shah model; Primal-dual; Image segmentation; Lagrange multipliers; Potts model; Relaxation processes; Algorithms; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; image enhancement; methodology; reproducibility; sensitivity and specificity; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84860118779
"Yang J., Tian Y., Duan L.-Y., Huang T., Gao W.","Group-sensitive multiple kernel learning for object recognition",2012,"IEEE Transactions on Image Processing",15,10.1109/TIP.2012.2183139,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860144953&doi=10.1109%2fTIP.2012.2183139&partnerID=40&md5=2f73acc149f978ccb5e1ca401f3023bc","In this paper, a group-sensitive multiple kernel learning (GS-MKL) method is proposed for object recognition to accommodate the intraclass diversity and the interclass correlation. By introducing the group between the object category and individual images as an intermediate representation, GS-MKL attempts to learn group-sensitive multikernel combinations together with the associated classifier. For each object category, the image corpus from the same category is partitioned into groups. Images with similar appearance are partitioned into the same group, which corresponds to the subcategory of the object category. Accordingly, intraclass diversity can be represented by the set of groups from the same category but with diverse appearances; interclass correlation can be represented by the correlation between groups from different categories. GS-MKL provides a tractable solution to adapt multikernel combination to local data distribution and to seek a tradeoff between capturing the diversity and keeping the invariance for each object category. Different from the simple hybrid grouping strategy that solves sample grouping and GS-MKL training independently, two sample grouping strategies are proposed to integrate sample grouping and GS-MKL training. The first one is a looping hybrid grouping method, where a global kernel clustering method and GS-MKL interact with each other by sharing group-sensitive multikernel combination. The second one is a dynamic divisive grouping method, where a hierarchical kernel-based grouping process interacts with GS-MKL. Experimental results show that performance of GS-MKL does not significantly vary with different grouping strategies, but the looping hybrid grouping method produces slightly better results. On four challenging data sets, our proposed method has achieved encouraging performance comparable to the state-of-the-art and outperformed several existing MKL methods. © 1992-2012 IEEE.","Dynamic divisive grouping (DDG); interclass correlation; intraclass diversity; looping hybrid grouping; multiple kernel learning (MKL); object recognition","Data sets; Grouping process; Grouping strategies; Interclass correlation; Intermediate representations; intraclass diversity; Kernel clustering methods; Local data; looping hybrid grouping; Multi-kernel; Multiple Kernel Learning; Object categories; Image processing; Mathematical models; Object recognition; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; image enhancement; methodology; reproducibility; sensitivity and specificity; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84860144953
"Regnier-Coudert O., McCall J., Lothian R., Lam T., McClinton S., N'Dow J.","Machine learning for improved pathological staging of prostate cancer: A performance comparison on a range of classifiers",2012,"Artificial Intelligence in Medicine",15,10.1016/j.artmed.2011.11.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858859611&doi=10.1016%2fj.artmed.2011.11.003&partnerID=40&md5=6461f33c673f10e5f9ac0191d379b8ff","Objectives: Prediction of prostate cancer pathological stage is an essential step in a patient's pathway. It determines the treatment that will be applied further. In current practice, urologists use the pathological stage predictions provided in Partin tables to support their decisions. However, Partin tables are based on logistic regression (LR) and built from US data. Our objective is to investigate a range of both predictive methods and of predictive variables for pathological stage prediction and assess them with respect to their predictive quality based on UK data. Methods and material: The latest version of Partin tables was applied to a large scale British dataset in order to measure their performances by mean of concordance index (c-index). The data was collected by the British Association of Urological Surgeons (BAUS) and gathered records from over 1700 patients treated with prostatectomy in 57 centers across UK. The original methodology was replicated using the BAUS dataset and evaluated using concordance index. In addition, a selection of classifiers, including, among others, LR, artificial neural networks and Bayesian networks (BNs) was applied to the same data and compared with each other using the area under the ROC curve (AUC). Subsets of the data were created in order to observe how classifiers perform with the inclusion of extra variables. Finally a local dataset prepared by the Aberdeen Royal Infirmary was used to study the effect on predictive performance of using different variables. Results: Partin tables have low predictive quality (c-index = 0.602) when applied on UK data for comparison on patients with organ confined and extra prostatic extension conditions, patients at the two most frequently observed pathological stages. The use of replicate lookup tables built from British data shows an improvement in the classification, but the overall predictive quality remains low (c-index = 0.610).Comparing a range of classifiers shows that BNs generally outperform other methods. Using the four variables from Partin tables, naive Bayes is the best classifier for the prediction of each class label (AUC = 0.662 for OC). When two additional variables are added, the results of LR (0.675), artificial neural networks (0.656) and BN methods (0.679) are overall improved. BNs show higher AUCs than the other methods when the number of variables raises. Conclusion: The predictive quality of Partin tables can be described as low to moderate on UK data. This means that following the predictions generated by Partin tables, many patients would received an inappropriate treatment, generally associated with a deterioration of their quality of life. In addition to demographic differences between UK and the original US population, the methodology and in particular LR present limitations. BN represents a promising alternative to LR from which prostate cancer staging can benefit. Heuristic search for structure learning and the inclusion of more variables are elements that further improve BN models quality. © 2011 Elsevier B.V.","Bayesian networks; Logistic regression; Partin tables; Predictive modeling; Prostate cancer staging","Aberdeen; Area under the ROC curve; Bayesian Networks (bns); Class labels; Concordance index; Data sets; Extra variables; Heuristic search; Logistic regression; Logistic regressions; Naive bayes; Partin tables; Performance comparison; Predictive methods; Predictive modeling; Predictive performance; Predictive variables; Prostate cancers; Quality of life; Stage prediction; Structure-learning; Bayesian networks; Diseases; Forecasting; Heuristic methods; Logistics; Neural networks; Patient monitoring; Regression analysis; Urology; Patient treatment; area under the curve; article; artificial neural network; cancer classification; cancer staging; classification; comparative study; disease model; hospital; human; information processing; machine learning; methodology; population; priority journal; prostate cancer; prostatectomy; quality of life; United Kingdom; United States; Algorithms; Area Under Curve; Artificial Intelligence; Bayes Theorem; Humans; Logistic Models; Male; Models, Biological; Neoplasm Staging; Neural Networks (Computer); Nomograms; Predictive Value of Tests; Prostate-Specific Antigen; Prostatic Neoplasms; ROC Curve",Article,Scopus,2-s2.0-84858859611
"Deguchi D., Feuerstein M., Kitasaka T., Suenaga Y., Ide I., Murase H., Imaizumi K., Hasegawa Y., Mori K.","Real-time marker-free patient registration for electromagnetic navigated bronchoscopy: A phantom study",2012,"International Journal of Computer Assisted Radiology and Surgery",15,10.1007/s11548-011-0626-9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862988916&doi=10.1007%2fs11548-011-0626-9&partnerID=40&md5=a9c00560454f343ffc54e87a1b85a202","Purpose To perform bronchoscopy safely and smoothly, it is very important to develop a bronchoscopic guidance system. Transbronchial lung biopsy (TBLB) with a bron-choscopic guidance system especially should permit safe image-guided procedures. Recently, electromagnetic tracking (EMT) is utilized to track the tip of the bronchoscope camera in real time. For most tracking methods using position sensors, registration between tracking data and previously acquired reference image data, such as CT image, is performed using natural landmarks of the patient or fiducial markers attached to the patient, whose positions need to be measured manually by the physician before the actual bronchoscopy. Therefore, this paper proposes a marker-free CT-to-patient registration method utilizing bronchoscope's position and orientation obtained by the EMT. Methods We developed a guidance system that is able to track the tip of the bronchoscope camera in real time. In the case of a guidance system that uses position sensors, natural landmarks of the patient or fiducial markers attached to the patient are needed to obtain the correspondence between EMT outputs and previously acquired reference image data, such as CT image. This paper proposes a registration method without landmarks or fiducials by estimating the transformation matrix between the patient and the CT image taken prior to the bronchoscopic examination. This estimation is performed by computing correspondences between the outputs of the EMT sensor and airways extracted from the CT image. As ambiguities between EMT measurements and their corresponding airway branches may arise at airway bifurcations, we introduce a stable airway branch selection mechanism for improving the robustness of the estimation of the transformation matrix. To evaluate the performance of the proposed method, we applied the method to a rubber bronchial phantom and added virtual breathing motion to the sensor output. Results Experimental results show that the accuracy of our proposed method is within 2.0-3.0 mm (without breathing motion) and 2.5-3.5 mm (with breathing motion). The proposed method could also track a bronchoscope camera in real time. Conclusions We developed a method for CT-to-patient registration using a position sensor without fiducial markers and natural landmarks. Endoscopic guided biopsy of lung lesions is feasible using a marker-free CT-to-patient registration method. © 2011 CARS.","Bronchoscopy; Camera tracking; Marker-free registration; Motion recovery; Position sensor; Tracking; Virtual bronchoscopy","accuracy; airway; anatomy; article; biosensor; breathing mechanics; bronchoscope; bronchoscopy; camera; computer assisted tomography; electromagnetic field; image display; image processing; imaging system; phantom; priority journal; tracheobronchial tree; virtual reality; Algorithms; Artificial Intelligence; Bronchoscopes; Bronchoscopy; Electromagnetic Phenomena; Humans; Imaging, Three-Dimensional; Phantoms, Imaging; Radiographic Image Interpretation, Computer-Assisted; Reproducibility of Results; Subtraction Technique; Systems Integration",Article,Scopus,2-s2.0-84862988916
"Eskevich M., Magdy W., Jones G.J.F.","New metrics for meaningful evaluation of informally structured speech retrieval",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",15,10.1007/978-3-642-28997-2_15,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860186165&doi=10.1007%2f978-3-642-28997-2_15&partnerID=40&md5=6dcae64b9a551e13bf844ffde1871dab","Search effectiveness for tasks where the retrieval units are clearly defined documents is generally evaluated using standard measures such as mean average precision (MAP). However, many practical speech search tasks focus on content within large spoken files lacking defined structure. These data must be segmented into smaller units for search which may only partially overlap with relevant material. We introduce two new metrics for the evaluation of search effectiveness for informally structured speech data: mean average segment precision (MASP) which measures retrieval performance in terms of both content segmentation and ranking with respect to relevance; and mean average segment distance-weighted precision (MASDWP) which takes into account the distance between the start of the relevant segment and the retrieved segment. We demonstrate the effectiveness of these new metrics on a retrieval test collection based on the AMI meeting corpus. © 2012 Springer-Verlag Berlin Heidelberg.","evaluation metrics; informally structured speech; Speech retrieval","Content segmentation; Evaluation metrics; Retrieval performance; Search tasks; Speech data; Speech retrieval; Test Collection; Artificial intelligence; Information retrieval",Conference Paper,Scopus,2-s2.0-84860186165
"Jin T., Wang J., Wen L.","Efficient retrieval of similar workflow models based on behavior",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",15,10.1007/978-3-642-29253-8_64,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859706014&doi=10.1007%2f978-3-642-29253-8_64&partnerID=40&md5=4a57489cb352978fe5b02b44f0f84d10","With the workflow technology being more widely used, there are more and more workflow models. How to retrieve the similar models efficiently from a large model repository is challenging. Since dynamic behavior is the essential characteristic of workflow models, we measure the similarity between models based on their behavior. Since the number of models is large, the efficiency of similarity retrieval is very important. To improve the efficiency of similarity retrieval based on behavior, we propose a more efficient algorithm for similarity calculation and use an index named TARIndex for query processing. To make our approach more applicable, we consider the semantic similarity between labels. Analysis and experiments show that our approach is efficient. © 2012 Springer-Verlag Berlin Heidelberg.",,"Dynamic behaviors; Efficient algorithm; Essential characteristic; Model repositories; Semantic similarity; Similar models; Similarity calculation; Similarity retrieval; Workflow models; Workflow technology; Artificial intelligence; Algorithms",Conference Paper,Scopus,2-s2.0-84859706014
"Kim M., Wu G., Yap P.-T., Shen D.","A general fast registration framework by learning deformation-appearance correlation",2012,"IEEE Transactions on Image Processing",15,10.1109/TIP.2011.2170698,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859010761&doi=10.1109%2fTIP.2011.2170698&partnerID=40&md5=c10f78236bfe817132b0dca960d57bc9","In this paper, we propose a general framework for performance improvement of the current state-of-the-art registration algorithms in terms of both accuracy and computation time. The key concept involves rapid prediction of a deformation field for registration initialization, which is achieved by a statistical correlation model learned between image appearances and deformation fields. This allows us to immediately bring a template image as close as possible to a subject image that we need to register. The task of the registration algorithm is hence reduced to estimating small deformation between the subject image and the initially warped template image, i.e., the intermediate template (IT). Specifically, to obtain a good subject-specific initial deformation, support vector regression is utilized to determine the correlation between image appearances and their respective deformation fields. When registering a new subject onto the template, an initial deformation field is first predicted based on the subject's image appearance for generating an IT. With the IT, only the residual deformation needs to be estimated, presenting much less challenge to the existing registration algorithms. Our learning-based framework affords two important advantages: 1) by requiring only the estimation of the residual deformation between the IT and the subject image, the computation time can be greatly reduced; 2) by leveraging good deformation initialization, local minima giving suboptimal solution could be avoided. Our framework has been extensively evaluated using medical images from different sources, and the results indicate that, on top of accuracy improvement, significant registration speedup can be achieved, as compared with the case where no prediction of initial deformation is performed. © 2011 IEEE.","Deformation prediction; fast image registration; principal component analysis (PCA); support vector regression (SVR)","Accuracy Improvement; Computation time; Deformation field; Deformation prediction; Fast image registration; Image appearance; Local minimums; Medical images; Performance improvements; principal component analysis (PCA); Registration algorithms; Residual deformation; Statistical correlation; Subject-specific; Suboptimal solution; support vector regression (SVR); Template images; Algorithms; Forecasting; Principal component analysis; Deformation; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; image enhancement; image subtraction; methodology; reproducibility; sensitivity and specificity; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-84859010761
"Soroor J., Tarokh M.J., Khoshalhan F., Sajjadi S.","Intelligent evaluation of supplier bids using a hybrid technique in distributed supply chains",2012,"Journal of Manufacturing Systems",15,10.1016/j.jmsy.2011.09.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858340427&doi=10.1016%2fj.jmsy.2011.09.002&partnerID=40&md5=1963924de0771ee62db93324439ad3f3","The main idea of this research is to devise the smart module to pick the best supplier bid(s) automatically. The hybrid model is composed of three useful tools: fuzzy logic, AHP, and QFD. The approach has been carefully implemented and verified via a real-world case study in a medium-to-large industry manufacturing vehicle tires and other rubber products. A collection of 12 assessment criteria classified into two categories have been considered. Eight factors are derived from customer suggestions and the other four are design specifications required to manufacture the product. The main outcomes are: a hybrid autonomous model to evaluate supplier bids without direct human intervention; devising a hybrid three-module method and overcoming complexity of computations in resulting algorithm by means of agents; outlining the best criteria to assess suppliers; evaluating the suppliers based on voice of customer during all stages of the process; and discussing analysis, design, and implementation issues of the evaluation agent. The paper includes implications for development of an integrated total system for supply chain coordination. The most important advantages of this work over earlier researches on supplier selection are: implementation of an autonomous assessment mechanism using intelligent agents for the first time, making the best out of three widely applied methodologies all at once, evaluation process mainly based on features of customer order, coordination of supply job based on a bidding system, and portal-mediated operation and control. © 2011 The Society of Manufacturing Engineers. Published by Elsevier Ltd. All rights reserved.","Agent-based systems; Decentralized supply chain; Distributed coordination; Fuzzy hybrid method; Product supply chain; Supplier evaluation and selection; Ubiquitous control","Agent-based systems; Decentralized supply chains; Distributed coordination; Fuzzy hybrid method; Product supply chains; Supplier Evaluations; Artificial intelligence; Customer satisfaction; Feature extraction; Fuzzy logic; Intelligent agents; Manufacture; Product design; Rating; Research; Sales; Supply chains; Autonomous agents",Article,Scopus,2-s2.0-84858340427
"Janaki Meena M., Chandran K.R., Karthik A., Vijay Samuel A.","An enhanced ACO algorithm to select features for text categorization and its parallelization",2012,"Expert Systems with Applications",15,10.1016/j.eswa.2011.11.081,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855907953&doi=10.1016%2fj.eswa.2011.11.081&partnerID=40&md5=4e45fb47d88de38191e3da2626619889","Feature selection is an indispensable preprocessing step for effective analysis of high dimensional data. It removes irrelevant features, improves the predictive accuracy and increases the comprehensibility of the model constructed by the classifiers sensitive to features. Finding an optimal feature subset for a problem in an outsized domain becomes intractable and many such feature selection problems have been shown to be NP-hard. Optimization algorithms are frequently designed for NP-hard problems to find nearly optimal solutions with a practical time complexity. This paper formulates the text feature selection problem as a combinatorial problem and proposes an Ant Colony Optimization (ACO) algorithm to find the nearly optimal solution for the same. It differs from the earlier algorithm by Aghdam et al. by including a heuristic function based on statistics and a local search. The algorithm aims at determining a solution that includes 'n' distinct features for each category. Optimization algorithms based on wrapper models show better results but the processes involved in them are time intensive. The availability of parallel architectures as a cluster of machines connected through fast Ethernet has increased the interest on parallelization of algorithms. The proposed ACO algorithm was parallelized and demonstrated with a cluster formed with a maximum of six machines. Documents from 20 newsgroup benchmark dataset were used for experimentation. Features selected by the proposed algorithm were evaluated using Naïve bayes classifier and compared with the standard feature selection techniques. It was observed that the performance of the classifier had been improved with the features selected by the enhanced ACO and local search. Error of the classifier decreases over iterations and it was observed that the number of positive features increases with the number of iterations. © 2011 Elsevier Ltd. All rights reserved.","χ 2; Ant Colony Optimization; Bag of Words; CHIR; Distributed environment; Heuristic information; Local search; MapReduce; Metaheuristic algorithms; Parallel algorithm","Ant-colony optimization; Bag of words; CHIR; Distributed environments; Heuristic information; Local search; Map-reduce; Meta heuristic algorithm; Artificial intelligence; Computational complexity; Constrained optimization; Feature extraction; Heuristic algorithms; Optimal systems; Parallel algorithms; Parallel architectures; Text processing; Clustering algorithms",Article,Scopus,2-s2.0-84855907953
"Sugimoto C.R., Cronin B.","Biobibliometric profiling: An examination of multifaceted approaches to scholarship",2012,"Journal of the American Society for Information Science and Technology",15,10.1002/asi.21695,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857649061&doi=10.1002%2fasi.21695&partnerID=40&md5=8800f02761738d2e3a20fe7018d20b98","We conducted a fine-grained prosopography of six distinguished information scientists to explore commonalities and differences in their approaches to scholarly production at different stages of their careers. Specifically, we gathered data on authors' genre preferences, rates and modes of scholarly production, and coauthorship patterns. We also explored the role played by gender and place in determining mentoring and collaboration practices across time. Our biobibliometric profiles of the sextet reveal the different shapes a scholar's career can take. We consider the implications of our findings for new entrants into the academic marketplace. © 2011 ASIS&T.",,"Across time; Coauthorship; Collaboration practices; Multi-faceted approach; Artificial intelligence; Software engineering",Article,Scopus,2-s2.0-84857649061
"Orpen D., Jäger G.","Lattice-valued convergence spaces: Extending the lattice context",2012,"Fuzzy Sets and Systems",15,10.1016/j.fss.2011.05.026,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855317876&doi=10.1016%2fj.fss.2011.05.026&partnerID=40&md5=659a31d1499719d09d75041e13655c53","We define a category of stratified L-generalized convergence spaces for the case where the lattice is an enriched cl-premonoid. We then investigate some of its categorical properties and those of its subcategories, in particular the stratified L-principal convergence spaces and the stratified L-topological convergence spaces. For some results we need to introduce a new condition on the lattice (which is always true in the case where the lattice is a frame, but not always true in the more general case). As examples where we may apply the more general lattice context we examine the stratified L-topological spaces and probabilistic limit spaces. We show that the category of stratified L-topological spaces is a reflective subcategory of our category and that the category of probabilistic limit spaces under a T-norm is both a reflective and a coreflective subcategory of our category if we choose the lattice context appropriately. © 2011 Elsevier B.V.","Enriched cl-premonoid; L-convergence space; L-filter; L-topology; Probabilistic limit space","Enriched cl-premonoid; L-convergence space; L-filter; L-topology; Limit space; Artificial intelligence; Fuzzy sets; Topology",Article,Scopus,2-s2.0-84855317876
"Csapo A., Baranyi P.","The spiral discovery method: An interpretable tuning model for CogInfoCom channels",2012,"Journal of Advanced Computational Intelligence and Intelligent Informatics",15,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859229138&partnerID=40&md5=4382c4ec73ca2253e01cd8eb94cc8ad6","Cognitive Infocommunications (CogInfoCom) messages that are used to carry information on the state of the same high-level concept can be regarded as belonging to a CogInfoCom channel. Such channels can be generated using any kind of parametric model. By changing the values of the parameters, it is possible to arrive at a large variety of CogInfoCom messages, a subset of which can belong to a CogInfoCom channel -provided they are perceptually well-suited to the purpose of conveying information on the same highlevel concept. Thus, for any CogInfoCom channel, we may speak of a parameter space and a perceptual space that is created by the totality of messages in the CogInfoCom channel. In this paper, we argue that in general, the relationship between the parameter space and the perceptual space is highly non-linear. For this reason, it is extremely difficult for the designer of a CogInfoCom channel to tune the parameters in such a way that the resulting CogInfoCom messages are perceptually continuous, and suitable to carry information on a single high-level concept. To address this problem, we propose a cognitive artifact that uses a rank concept available in tensor algebra to provide the designer of CogInfoCom channels with practical tradeoffs between complexity and interpretability. We refer to the artifact as the Spiral Discovery Method (SDM).","CogInfoCom channels; CogInfoCom messages; Cognitive artifacts; Cognitive infocommunications","CogInfoCom channels; CogInfoCom messages; Cognitive artifacts; Infocommunications; Interpretability; Parameter spaces; Parametric models; Tensor algebra; Artificial intelligence; Computational methods; Space division multiple access",Conference Paper,Scopus,2-s2.0-84859229138
"Naranjo-Hernández D., Roa L.M., Reina-Tosina J., Estudillo-Valderrama M.A.","Personalization and adaptation to the medium and context in a fall detection system",2012,"IEEE Transactions on Information Technology in Biomedicine",15,10.1109/TITB.2012.2185851,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858111260&doi=10.1109%2fTITB.2012.2185851&partnerID=40&md5=5188d323160158ca4e572df43e9ef0ea","The main objective of this paper is to present a distributed processing architecture that explicitly integrates capabilities for its continuous adaptation to the medium, the context, and the user. This architecture is applied to a falling detection system through: 1) an optimization module that finds the optimal operation parameters for the detection algorithms of the system devices; 2) a distributed processing architecture that provides capabilities for remote firmware update of the smart sensors. The smart sensor also provides an estimation of activities of daily living (ADL), which results very useful in monitoring of the elderly and patients with chronic diseases. The developed experiments have demonstrated the feasibility of the system and specifically, the accuracy of the proposed algorithms and procedures (100 success for impact detection, 100 sensitivity and 95.68 specificity rates for fall detection, and 100 success for ADL level classification). Although the experiments have been developed with a cohort of young volunteers, the personalization and adaption mechanisms of the proposed architecture related to the concepts of design for all and design space will significantly ease the adaptation of the system for its application to the elderly. © 2012 IEEE.","Daily living activities; distributed processing; dynamic adaptation; fall detection; personalization","Daily living; Distributed processing; dynamic adaptation; Fall detection; Personalizations; Algorithms; Experiments; Firmware; Health care; Smart sensors; Distributed parameter networks; acceleration; adult; algorithm; ambulatory monitoring; article; artificial intelligence; classification; daily life activity; falling; feasibility study; female; human; male; methodology; reproducibility; signal processing; telemetry; wireless communication; Acceleration; Accidental Falls; Activities of Daily Living; Adult; Algorithms; Artificial Intelligence; Feasibility Studies; Female; Humans; Male; Monitoring, Ambulatory; Reproducibility of Results; Signal Processing, Computer-Assisted; Telemetry; Wireless Technology",Article,Scopus,2-s2.0-84858111260
"Stach W., Pedrycz W., Kurgan L.A.","Learning of fuzzy cognitive maps using density estimate",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",15,10.1109/TSMCB.2011.2182646,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861182007&doi=10.1109%2fTSMCB.2011.2182646&partnerID=40&md5=313ca680bdb4b57641799d36c87758a4","Fuzzy cognitive maps (FCMs) are convenient and widely used architectures for modeling dynamic systems, which are characterized by a great deal of flexibility and adaptability. Several recent works in this area concern strategies for the development of FCMs. Although a few fully automated algorithms to learn these models from data have been introduced, the resulting FCMs are structurally considerably different than those developed by human experts. In particular, maps that were learned from data are much denser (with the density over 90% versus about 40% density of maps developed by humans). The sparseness of the maps is associated with their interpretability: the smaller the number of connections is, the higher is the transparency of the map. To this end, a novel learning approach, sparse real-coded genetic algorithms (SRCGAs), to learn FCMs is proposed. The method utilizes a density parameter to guide the learning toward a formation of maps of a certain predefined density. Comparative tests carried out for both synthetic and real-world data demonstrate that, given a suitable density estimate, the SRCGA method significantly outperforms other state-of-the-art learning methods. When the density estimate is unknown, the new method can be used in an automated fashion using a default value, and it is still able to produce models whose performance exceeds or is equal to the performance of the models generated by other methods. © 2012 IEEE.","Fuzzy cognitive maps (FCMs); real-coded genetic algorithms (RCGAs)","Automated algorithms; Comparative tests; Default values; Density estimates; Density parameters; Fuzzy cognitive map; Human expert; Interpretability; Learning approach; Learning methods; Real world data; Real-coded genetic algorithm; Estimation; Fuzzy systems; Genetic algorithms; Fuzzy rules; algorithm; article; artificial intelligence; automated pattern recognition; computer simulation; decision support system; fuzzy logic; methodology; theoretical model; Algorithms; Artificial Intelligence; Computer Simulation; Decision Support Techniques; Fuzzy Logic; Models, Theoretical; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84861182007
"Qian P., Chung F.-L., Wang S., Deng Z.","Fast graph-based relaxed clustering for large data sets using minimal enclosing ball",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",15,10.1109/TSMCB.2011.2172604,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862816874&doi=10.1109%2fTSMCB.2011.2172604&partnerID=40&md5=d81fe919fefb08e9ca28850df2af8b8c","Although graph-based relaxed clustering (GRC) is one of the spectral clustering algorithms with straightforwardness and self-adaptability, it is sensitive to the parameters of the adopted similarity measure and also has high time complexity O(N 3) which severely weakens its usefulness for large data sets. In order to overcome these shortcomings, after introducing certain constraints for GRC, an enhanced version of GRC [constrained GRC (CGRC)] is proposed to increase the robustness of GRC to the parameters of the adopted similarity measure, and accordingly, a novel algorithm called fast GRC (FGRC) based on CGRC is developed in this paper by using the core-set-based minimal enclosing ball approximation. A distinctive advantage of FGRC is that its asymptotic time complexity is linear with the data set size N. At the same time, FGRC also inherits the straightforwardness and self-adaptability from GRC, making the proposed FGRC a fast and effective clustering algorithm for large data sets. The advantages of FGRC are validated by various benchmarking and real data sets. © 2012 IEEE.","Clustering; large data sets; minimal enclosing ball (MEB); time complexity","Asymptotic time complexity; Clustering; Data set size; Graph-based; Large datasets; minimal enclosing ball (MEB); Novel algorithm; Real data sets; Self-adaptability; Similarity measure; Time complexity; Graphic methods; Clustering algorithms; algorithm; article; artificial intelligence; automated pattern recognition; cluster analysis; computer simulation; factual database; information retrieval; methodology; theoretical model; Algorithms; Artificial Intelligence; Cluster Analysis; Computer Simulation; Databases, Factual; Information Storage and Retrieval; Models, Theoretical; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84862816874
"Wu C.-H., Chen L.-C.","3D spatial information for fire-fighting search and rescue route analysis within buildings",2012,"Fire Safety Journal",15,10.1016/j.firesaf.2011.12.006,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855443621&doi=10.1016%2fj.firesaf.2011.12.006&partnerID=40&md5=95d4f31963d59a3d3b16c5d8308d3b0f","This study is motivated by the need for a micro-geographic information system (GIS) to represent and analyze 3D spatial data for the plotting of fire-fighting search and rescue routes within buildings. The GIS uses a 3D geometric network model (GNM) and the Dijkstra algorithm to consider smoke movement during different times of a building fire. Therefore, the route calculation algorithm can avoid routes through heavy smoke within buildings. In addition, when firefighters must search an area to find victims, the GNM and Ant Colony Optimization are applied to find the shortest path that passes through each room of the area. Finally, the GNM is implemented to perform a search and rescue route analysis from an actual underground station. The proposed method can not only provide the shortest safe route within a building but can also minimize the time required to search for potential victims. © 2011 Elsevier Ltd. All rights reserved.","Ant Colony Optimization; Dijkstra algorithm; Emergency response; Geometric network model","Ant-colony optimization; Building fires; Dijkstra algorithms; Emergency response; Fire fighting; Geometric network model; Network models; Search and rescue; Shortest path; Smoke movement; Spatial data; Spatial informations; Underground stations; Algorithms; Artificial intelligence; Buildings; Fire extinguishers; Geographic information systems; Optimization; Subway stations; Three dimensional; Three dimensional computer graphics; Search engines",Article,Scopus,2-s2.0-84855443621
"Coelho L.D.S., Guerra F.A., Leite J.V.","Multiobjective exponential particle swarm optimization approach applied to hysteresis parameters estimation",2012,"IEEE Transactions on Magnetics",15,10.1109/TMAG.2011.2172581,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856391193&doi=10.1109%2fTMAG.2011.2172581&partnerID=40&md5=0179e892c8983fe98b772d617bf72897","The term swarm intelligence is used to describe algorithms and distributed problem solvers inspired by the collective behavior of insect colonies and other animal societies. Particle swarm optimization (PSO) is a kind of swarm intelligence that is based on the social behavior metaphor. Furthermore, PSO is a stochastic search technique with reduced memory requirement, computationally effective and easier to implement compared to other optimization metaheuristics. Unlike the traditional optimization algorithms, PSO is a derivative-free algorithm and thus it is especially effective in dealing with complex and nonlinear problems in electromagnetic optimization applications. In this paper, a multiobjective PSO approach based on exponential distribution probability operator (MOPSO-E) is proposed and evaluated. Numerical comparisons with results using a multiobjective PSO with external archiving and the proposed MOPSO-E demonstrated that the performance of the MOPSO-E is promising in Jiles-Atherton vector hysteresis model parameter identification. The proposed MOPSO-E to find nondominated solutions that represent the good trade-offs among the objectives in the evaluated case study. © 2012 IEEE.","Electromagnetics; evolutionary computation; optimization; swarm intelligence","Animal societies; Collective behavior; Derivative-free algorithm; Distributed problems; Electromagnetic optimization; Electromagnetics; Exponential distributions; Insect colonies; Jiles-Atherton; Meta heuristics; Multi objective; Nondominated solutions; Nonlinear problems; Numerical comparison; Optimization algorithms; Parameters estimation; Particle swarm; Reduced memory requirements; Social behavior; Stochastic search techniques; Swarm Intelligence; Vector hysteresis models; Algorithms; Artificial intelligence; Cellular automata; Electromagnetism; Evolutionary algorithms; Hysteresis; Optimization; Parameter estimation; Particle swarm optimization (PSO); Probability distributions; Multiobjective optimization",Conference Paper,Scopus,2-s2.0-84856391193
"Manwani N., Sastry P.S.","Geometric decision tree",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",15,10.1109/TSMCB.2011.2163392,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856295338&doi=10.1109%2fTSMCB.2011.2163392&partnerID=40&md5=58a4589a9f0ee16ec9dddc5643e32777","In this paper, we present a new algorithm for learning oblique decision trees. Most of the current decision tree algorithms rely on impurity measures to assess the goodness of hyperplanes at each node while learning a decision tree in top-down fashion. These impurity measures do not properly capture the geometric structures in the data. Motivated by this, our algorithm uses a strategy for assessing the hyperplanes in such a way that the geometric structure in the data is taken into account. At each node of the decision tree, we find the clustering hyperplanes for both the classes and use their angle bisectors as the split rule at that node. We show through empirical studies that this idea leads to small decision trees and better performance. We also present some analysis to show that the angle bisectors of clustering hyperplanes that we use as the split rules at each node are solutions of an interesting optimization problem and hence argue that this is a principled method of learning a decision tree. © 2011 IEEE.","Decision trees; generalized eigenvalue problem; multiclass classification; oblique decision tree","Decision-tree algorithm; Empirical studies; generalized eigenvalue problem; Geometric structure; Method of learning; multiclass classification; oblique decision tree; Optimization problems; Show through; Topdown; Decision trees; Eigenvalues and eigenfunctions; Forestry; Geometry; Learning algorithms; Trees (mathematics); Algorithms; Forestry; Geometry; Problem Solving; Trees; algorithm; article; artificial intelligence; automated pattern recognition; computer simulation; decision support system; methodology; theoretical model; Algorithms; Artificial Intelligence; Computer Simulation; Decision Support Techniques; Models, Theoretical; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84856295338
"Deng H., King I., Lyu M.R.","Enhanced models for expertise retrieval using community-aware strategies",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",15,10.1109/TSMCB.2011.2161980,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856254959&doi=10.1109%2fTSMCB.2011.2161980&partnerID=40&md5=f28b6cf3ef2cbf38794fbbfb258d1b6c","Expertise retrieval, whose task is to suggest people with relevant expertise on the topic of interest, has received increasing interest in recent years. One of the issues is that previous algorithms mainly consider the documents associated with the experts while ignoring the community information that is affiliated with the documents and the experts. Motivated by the observation that communities could provide valuable insight and distinctive information, we investigate and develop two community-aware strategies to enhance expertise retrieval. We first propose a new smoothing method using the community context for statistical language modeling, which is employed to identify the most relevant documents so as to boost the performance of expertise retrieval in the document-based model. Furthermore, we propose a query-sensitive AuthorRank to model the authors' authorities based on the community coauthorship networks and develop an adaptive ranking refinement method to enhance expertise retrieval. Experimental results demonstrate the effectiveness and robustness of both community-aware strategies. Moreover, the improvements made in the enhanced models are significant and consistent. © 2011 IEEE.","Community-aware strategy; expertise retrieval; language model; query-sensitive AuthorRank","AuthorRank; Coauthorship networks; Community-aware strategy; expertise retrieval; language model; query-sensitive AuthorRank; Ranking refinement; Relevant documents; Smoothing methods; Statistical language modeling; Computational linguistics; algorithm; article; artificial intelligence; automated pattern recognition; computer simulation; data mining; decision support system; expert system; methodology; patient referral; theoretical model; Algorithms; Artificial Intelligence; Computer Simulation; Data Mining; Decision Support Techniques; Expert Systems; Models, Theoretical; Pattern Recognition, Automated; Referral and Consultation",Article,Scopus,2-s2.0-84856254959
"Wu T., Bae M.H., Zhang M., Pan R., Badea A.","A prior feature svm-mrf based method for mouse brain segmentation",2012,"NeuroImage",15,10.1016/j.neuroimage.2011.09.053,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855439188&doi=10.1016%2fj.neuroimage.2011.09.053&partnerID=40&md5=b106aeaaf34ac1f0898db2751fb27600","We introduce an automated method, called prior feature Support Vector Machine- Markov Random Field (pSVMRF), to segment three-dimensional mouse brain Magnetic Resonance Microscopy (MRM) images. Our earlier work, extended MRF (eMRF) integrated Support Vector Machine (SVM) and Markov Random Field (MRF) approaches, leading to improved segmentation accuracy; however, the computation of eMRF is very expensive, which may limit its performance on segmentation and robustness. In this study pSVMRF reduces training and testing time for SVM, while boosting segmentation performance. Unlike the eMRF approach, where MR intensity information and location priors are linearly combined, pSVMRF combines this information in a nonlinear fashion, and enhances the discriminative ability of the algorithm. We validate the proposed method using MR imaging of unstained and actively stained mouse brain specimens, and compare segmentation accuracy with two existing methods: eMRF and MRF. C57BL/6 mice are used for training and testing, using cross validation. For formalin fixed C57BL/6 specimens, pSVMRF outperforms both eMRF and MRF. The segmentation accuracy for C57BL/6 brains, stained or not, was similar for larger structures like hippocampus and caudate putamen, (~. 87%), but increased substantially for smaller regions like susbtantia nigra (from 78.36% to 91.55%), and anterior commissure (from ~. 50% to ~. 80%). To test segmentation robustness against increased anatomical variability we add two strains, BXD29 and a transgenic mouse model of Alzheimer's disease. Segmentation accuracy for new strains is 80% for hippocampus, and caudate putamen, indicating that pSVMRF is a promising approach for phenotyping mouse models of human brain disorders. © 2011 Elsevier Inc.","Automated segmentation; Magnetic resonance microscopy; Markov Random Field; Mouse brain; Support Vector Machine","accuracy; Alzheimer disease; anterior commissure; article; automation; brain; brain cortex; brain segmentation; brain ventricle; caudate putamen; cerebellar peduncle; classification accuracy; classification algorithm; corpus callosum; disease model; globus pallidus; hippocampus; inbred strain; intermethod comparison; mouse; mouse strain; nerve tract; neuroanatomy; nonhuman; nuclear magnetic resonance imaging; olfactory bulb; optic tract; pons; prior feature support vector machine Markov Random Field; priority journal; probability; putamen; substantia nigra; support vector machine; transgenic mouse; trigeminal tract; validation process; Algorithms; Alzheimer Disease; Amyloid beta-Protein Precursor; Animals; Artificial Intelligence; Brain; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Markov Chains; Mice; Mice, Inbred C57BL; Mice, Inbred DBA; Mice, Transgenic; Reproducibility of Results; Support Vector Machines",Article,Scopus,2-s2.0-84855439188
"Freixas J., Marciniak D., Pons M.","On the ordinal equivalence of the Johnston, Banzhaf and Shapley power indices",2012,"European Journal of Operational Research",15,10.1016/j.ejor.2011.07.028,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862587300&doi=10.1016%2fj.ejor.2011.07.028&partnerID=40&md5=ef96e5d04f5daa06d6b19b9c71ecdc61","In this paper, we characterize the games in which Johnston, Shapley-Shubik and Penrose-Banzhaf-Cole-man indices are ordinally equivalent, meaning that they rank players in the same way. We prove that these three indices are ordinally equivalent in semicomplete simple games, which is a newly defined class that contains complete games and includes most of the real-world examples of binary voting systems. This result constitutes a twofold extension of Diffo Lambo and Moulen's result (Diffo Lambo and Moulen, 2002) in the sense that ordinal equivalence emerges for three power indices (not just for the Shapley-Shubik and Penrose-Banzhaf-Coleman indices), and it holds for a class of games strictly larger than the class of complete games. © 2011 Elsevier B.V. All rights reserved.","Complete simple games; Decision support systems; Game theory; Ordinal equivalence; Power indices; Simple games","Artificial intelligence; Boolean functions; Decision support systems; Decision theory; Game theory; Voting machines; Coleman; Complete simple games; Ordinal equivalence; Power indices; Real-world; Shapley; Simple games; Voting systems; Equivalence classes",Article,Scopus,2-s2.0-84862587300
"Corman F., D'Ariano A.","Assessment of advanced dispatching measures for recovering disrupted railway traffic situations",2012,"Transportation Research Record",15,10.3141/2289-01,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869844347&doi=10.3141%2f2289-01&partnerID=40&md5=c7b52a3de4e061d688d5846ed3daf58d","Railway timetables are developed to make operations robust and resilient to small delays. However, disturbances perturb the daily plan, and dispatchers need to adjust the plan to keep operations feasible and to limit delay propagation. For large infrastructure disruptions, the available railway capacity is reduced, and the timetable could become infeasible. The paper studies how to support dispatchers in the management of traffic flow during disruptions. A set of disruption resolution scenarios to manage seriously disturbed traffic conditions in large networks is investigated. For instance, in the case of track blockage, train services can be canceled, rerouted in the disrupted dispatching area, or rerouted in other areas while still with the same origin and destination. Feasible and efficient operations schedules are found quickly by an advanced decision support system for dispatching known as ROMA (railway traffic optimization by means of alternative graphs), which is based on microscopic detail and can handle large areas by decomposition. Detailed performance indicators can be computed to let dispatchers choose a specific solution, for example, by minimizing train delays and reducing passengers' discomfort. In the computational experiments, an analysis is done of a blockage on a double track line, combined with multiple entrance delays on a large railway network with heavy traffic. Several disruption resolution scenarios involving cancellation of services, rerouting, and shuttle trains are considered, and each feasible plan is evaluated in relation to travel times, frequency of services, and delay propagation.",,"Alternative graphs; Computational experiment; Delay propagation; Dispatching areas; Double track line; Heavy traffics; Large networks; Origin and destinations; Performance indicators; Railway capacity; Railway network; Railway timetables; Railway traffic; Traffic conditions; Traffic flow; Train delay; Train services; Artificial intelligence; Benchmarking; Decision support systems; Railroads; Scheduling; Traffic control; Railroad transportation",Article,Scopus,2-s2.0-84869844347
"Garcia Villalba M.P., Saint-Dizier P.","Some facets of argument mining for opinion analysis",2012,"Frontiers in Artificial Intelligence and Applications",15,10.3233/978-1-61499-111-3-23,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876240040&doi=10.3233%2f978-1-61499-111-3-23&partnerID=40&md5=12cabea47de8654cca2b06f1a654d006","In this paper, we present some foundational elements related to argument extraction in opinion texts with the objective to further analyse and synthetise user preferences and value systems as they emerge in such texts. We show that (1) within the context of opinionated expressions, a number of evaluative expressions with a 'heavy' semantic load receive an argumentative interpretation and (2) that the association of an evaluative expression with a discourse structure such as an elaboration, an illustration, or a reformulation must also be interpreted as an argument. We develop a conceptual semantics of these relations and show how they are analyzed using the Dislog programming language on the <TextCoop> platform, dedicated to discourse analysis. © 2012 The authors and IOS Press. All rights reserved.","Arguments in opinions; Discourse analysis; Opinion analysis","Artificial intelligence; Arguments in opinions; Conceptual semantics; Discourse analysis; Discourse structure; Evaluative expressions; Foundational elements; Opinion analysis; Value systems; Semantics",Conference Paper,Scopus,2-s2.0-84876240040
"Akbani R., Korkmaz T., Raju G.V.","EMLTrust: An enhanced Machine Learning based Reputation System for MANETs",2012,"Ad Hoc Networks",15,10.1016/j.adhoc.2011.08.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856532668&doi=10.1016%2fj.adhoc.2011.08.003&partnerID=40&md5=f51744108b96d8ae0ca6db8cccadb7a0","Many mission critical networks including MANETs for military communications and disaster relief communications rely on node cooperation. If malicious nodes gain access to such networks they can easily launch attacks, such as spreading viruses or spam, or attacking known vulnerabilities. One way to defend against malicious nodes is to use Reputation Systems (RS) that try to predict future behavior of nodes by observing their past behavior. In this paper, we propose a Machine Learning (ML) based RS that defends against many patterns of attacks. We specifically consider the proposed RS in the context of MANETs. After introducing a basic RS, we propose further enhancements to it to improve its performance and to deal with some of the more challenging aspects of MANETs. For instance, we consider digital signature based mechanisms that do not require trusted third parties, or servers that are always online. Another enhancement uses an algorithm called Fading Memories that allows us to look back at longer histories using fewer features. Finally, we introduce a new technique, called Dynamic Thresholds, to improve accuracies even further. We compare the performance of our RS with another RS found in the literature, called TrustGuard, and perform detailed evaluations against a variety of attacks. The results show that our RS significantly outperforms TrustGuard, even when the proportion of malicious nodes in the network is high. We also show that our scheme has very low bandwidth and computation overhead. In contrast to existing RSs designed to detect specific attacks, ML based RSs can be retrained to detect new attack patterns as well. © 2011 Elsevier B.V. All rights reserved.","Digital credentials; Fading Memories; Machine Learning; Trust management","Artificial intelligence; Computer viruses; Disaster prevention; Learning systems; Military communications; Viruses; Computation overheads; Digital credentials; Dynamic threshold; Fading memory; Mission critical networks; Reputation systems; Trust management; Trusted third parties; Mobile ad hoc networks",Article,Scopus,2-s2.0-84856532668
"Azari H., Airoldi E.M.","Graphlet decomposition of a weighted network",2012,"Journal of Machine Learning Research",15,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946906649&partnerID=40&md5=c605244b2c727d995c8d86b3bc6d8fe0","We introduce the graphlet decomposition of a weighted network, which encodes a notion of social information based on social structure. We develop a scalable algorithm, which combines EM with Bron-Kerbosch in a novel fashion, for estimating the parameters of the model underlying graphlets using one network sample. We explore theoretical properties of graphlets, including computational complexity, redundancy and expected accuracy. We test graphlets on synthetic data, and we analyze messaging on Facebook and crime associations in the 19th century.",,"Artificial intelligence; 19th century; Facebook; Graphlets; Scalable algorithms; Social information; Social structure; Synthetic data; Weighted networks; Complex networks",Conference Paper,Scopus,2-s2.0-84946906649
"Biehl M., Schneider P., Smith D.J., Stiekema H., Taylor A.E., Hughes B.A., Shackleton C.H.L., Stewart P.M., Arlt W.","Matrix relevance LVQ in steroid metabolomics based classification of adrenal tumors",2012,"ESANN 2012 proceedings, 20th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning",15,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947792616&partnerID=40&md5=9e737137e778ba5de4d514362d399815","We present a machine learning system for the differential diagnosis of benign adrenocortical adenoma (ACA) vs. malignant adrenocortical carcinoma (ACC). The data employed for the classification are urinary excretion values of 32 steroid metabolites. We apply prototype-based classification techniques to discriminate the classes, in particular, we use modifications of Generalized Learning Vector Quantization including matrix relevance learning. The obtained system achieves high sensitivity and specificity and outperforms previously used approaches for the detection of adrenal malignancy. Moreover, the method identifies a subset of most discriminative markers which facilitates its future use as a noninvasive high-throughput diagnostic tool. © 2012, i6doc.com publication. All rights reserved.",,"Artificial intelligence; Diagnosis; Neural networks; Tumors; Vector quantization; Diagnostic tools; Differential diagnosis; Generalized learning vector quantization; High sensitivity; High throughput; Prototype-based classifications; Relevance learning; Urinary excretion; Learning systems",Conference Paper,Scopus,2-s2.0-84947792616
"Nafar M., Gharehpetian G.B., Niknam T.","Using modified fuzzy particle swarm optimization algorithm for parameter estimation of surge arresters models",2012,"International Journal of Innovative Computing, Information and Control",15,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856976107&partnerID=40&md5=b08637d653b06728087d7593eeb4a023","Accurate modeling and parameters identification of Metal Oxide Surge Arrester (MOSA) are very important for arrester allocation, systems reliability and insulation coordination studies. Several models with acceptable accuracy have been proposed to describe this behavior. It should be mentioned that the estimation of nonlinear elements of MOSAs is very important for all models. In this paper, a new method, which is the combination of Fuzzy Particle Swarm Optimization (FPSO) and Ant Colony Optimization (ACO) methods, is proposed to estimate the parameters of MOSA models. The proposed method is named Modified Fuzzy Particle Swarm Optimization (MFPSO). In the proposed algorithm, to overcome the drawback of the PSO algorithm (convergence to local optima), the inertia weight is tuned by using fuzzy rules. Also, to improve the global search capability and prevent the convergence to local minima, ACO algorithm is combined to proposed FPSO algorithm. The transient models of MOSA have been simulated by using ATP-EMTP. The results of simulations have been applied to the program, which is based on MFPSO method and can determine the fitness and parameters of different models. The validity and the accuracy of the estimated parameters are assessed by comparing the predicted residual voltage with the experimental results. Also, Using proposed algorithm, different surge arrester models and V-I characteristics determination methods have been compared. © 2012 ICIC International.","ACO; Fuzzy rules; Parameter estimation; PSO; Surge arrester models","Accurate modeling; ACO; ACO algorithms; Ant colony optimization methods; ATP-EMTP; Estimated parameter; Fuzzy particle swarm; Global search capability; Inertia weight; Local minimums; Local optima; Metal oxide surge arresters; Nonlinear elements; Parameters identification; PSO; PSO algorithms; Residual voltage; Surge arresters; Transient model; V-I characteristic; Artificial intelligence; Computer simulation; Electric insulation coordination; Electric surges; Fuzzy rules; Offshore oil fields; Parameter estimation; Particle swarm optimization (PSO); Algorithms",Article,Scopus,2-s2.0-84856976107
"Zhou M., Hannah L.A., Dunson D.B., Carin L.","Beta-negative binomial process and poisson factor analysis",2012,"Journal of Machine Learning Research",15,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925405805&partnerID=40&md5=8845b85d5309261c0a8ff5b1d3fa4522","A beta-negative binomial (BNB) process is proposed, leading to a beta-gamma-Poisson process, which may be viewed as a ""multiscoop"" generalization of the beta-Bernoulli process. The BNB process is augmented into a beta-gamma-gamma-Poisson hierarchical structure, and applied as a nonparametric Bayesian prior for an infinite Poisson factor analysis model. A finite approximation for the beta process Lévy random measure is constructed for convenient implementation. Efficient MCMC computations are performed with data augmentation and marginalization techniques. Encouraging results are shown on document count matrix factorization. © Copyright 2012 by the authors.",,"Artificial intelligence; Factorization; Multivariant analysis; Bernoulli process; Data augmentation; Factor analysis model; Hierarchical structures; Marginalization; MCMC computation; Negative binomial; Non-parametric Bayesian; Factor analysis",Conference Paper,Scopus,2-s2.0-84925405805
"Ohashi O., Torgo L.","Wind speed forecasting using spatio-temporal indicators",2012,"Frontiers in Artificial Intelligence and Applications",15,10.3233/978-1-61499-098-7-975,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878809512&doi=10.3233%2f978-1-61499-098-7-975&partnerID=40&md5=e94c2451d02e7890af5ea8b6e09af1d3","From small farms to electricity markets the interest and importance of wind power production is continuously increasing. This interest is mainly caused by the fact that wind is a continuous resource of clean energy. To take full advantage of the potential of wind power production it is crucial to have tools that accurately forecast the expected wind speed. However, forecasting the wind speed is not a trivial task. Wind speed is characterised by a random behaviour as well as several other intermittent characteristics. This paper proposes a new approach to the task of wind speed forecasting. The main distinguishing feature of this proposal is its reliance on both temporal and spatial characteristics to produce a forecast of the future wind speed. We have experimentally tested the proposed method with historical data concerning wind speed on the eastern region of the US. Nevertheless, the methodology that is described in the paper can be seen as a general approach to spatio-temporal prediction. We have compared our proposal to other standard approaches in the task of forecasting 2 hours ahead wind speed. Our extensive experiments show that our proposal has clear advantages in most setups. © 2012 The Author(s).",,"Artificial intelligence; Electric power generation; Forecasting; Speed; Wind effects; Wind power; Eastern regions; Historical data; New approaches; Spatio temporal; Spatio-temporal prediction; Temporal and spatial; Wind power production; Wind speed forecasting; Wind",Conference Paper,Scopus,2-s2.0-84878809512
"Cretu A.-M., Payeur P., Petriu E.M.","Soft object deformation monitoring and learning for model-based robotic hand manipulation",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",15,10.1109/TSMCB.2011.2176115,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861192053&doi=10.1109%2fTSMCB.2011.2176115&partnerID=40&md5=b4ff401fd16f51f5c11900726cc939ce","This paper discusses the design and implementation of a framework that automatically extracts and monitors the shape deformations of soft objects from a video sequence and maps them with force measurements with the goal of providing the necessary information to the controller of a robotic hand to ensure safe model-based deformable object manipulation. Measurements corresponding to the interaction force at the level of the fingertips and to the position of the fingertips of a three-finger robotic hand are associated with the contours of a deformed object tracked in a series of images using neural-network approaches. The resulting model captures the behavior of the object and is able to predict its behavior for previously unseen interactions without any assumption on the object's material. The availability of such models can contribute to the improvement of a robotic hand controller, therefore allowing more accurate and stable grasp while providing more elaborate manipulation capabilities for deformable objects. Experiments performed for different objects, made of various materials, reveal that the method accurately captures and predicts the object's shape deformation while the object is submitted to external forces applied by the robot fingers. The proposed method is also fast and insensitive to severe contour deformations, as well as to smooth changes in lighting, contrast, and background. © 2012 IEEE.","Deformable object; neural networks; object deformation monitoring; object segmentation","Deformable object; External force; Hand manipulation; Interaction forces; Object deformation; Object segmentation; Robot fingers; Shape deformation; Soft objects; Video sequences; Image segmentation; Neural networks; Robotic arms; Deformation; algorithm; article; artificial intelligence; automated pattern recognition; biomimetics; computer assisted diagnosis; computer simulation; decision support system; hand; human; methodology; motion; robotics; theoretical model; videorecording; Young modulus; Algorithms; Artificial Intelligence; Biomimetics; Computer Simulation; Decision Support Techniques; Elastic Modulus; Hand; Humans; Image Interpretation, Computer-Assisted; Models, Theoretical; Motion; Pattern Recognition, Automated; Robotics; Video Recording",Article,Scopus,2-s2.0-84861192053
"Mallapragada G., Ray A., Jin X.","Symbolic dynamic filtering and language measure for behavior identification of mobile robots",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",15,10.1109/TSMCB.2011.2172419,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861188671&doi=10.1109%2fTSMCB.2011.2172419&partnerID=40&md5=a0440d1291145e2ec86a25301d3151cd","This paper presents a procedure for behavior identification of mobile robots, which requires limited or no domain knowledge of the underlying process. While the features of robot behavior are extracted by symbolic dynamic filtering of the observed time series, the behavior patterns are classified based on language measure theory. The behavior identification procedure has been experimentally validated on a networked robotic test bed by comparison with commonly used tools, namely, principal component analysis for feature extraction and Bayesian risk analysis for pattern classification. © 2012 IEEE.","Feature extraction; language measure; pattern classification; robotic signatures; symbolic dynamic filtering (SDF)","Behavior patterns; Domain knowledge; Identification procedure; Language measure; Networked robotics; Robot behavior; Symbolic Dynamic Filtering; Data handling; Equipment testing; Feature extraction; Pattern recognition; Principal component analysis; Robotics; Mobile robots; algorithm; article; artificial intelligence; automated pattern recognition; Bayes theorem; computer simulation; decision support system; methodology; motion; robotics; theoretical model; Algorithms; Artificial Intelligence; Bayes Theorem; Computer Simulation; Decision Support Techniques; Models, Theoretical; Motion; Pattern Recognition, Automated; Robotics",Article,Scopus,2-s2.0-84861188671
"Correa W., Prade H., Richard G.","When intelligence is just a matter of copying",2012,"Frontiers in Artificial Intelligence and Applications",15,10.3233/978-1-61499-098-7-276,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878791737&doi=10.3233%2f978-1-61499-098-7-276&partnerID=40&md5=f9049822880498edba6df636e0a3c365","The paper investigates a general approach that allows us to solve IQ tests based on Raven's progressive matrices automatically. The 3 × 3 Raven matrices exhibit 8 geometric pictures displayed in 8 cells of the matrix, which have to be logically completed with a 9th picture to be put in the remaining empty matrix cell. In these tests, a set of candidate pictures for the solution is also given. The suggested approach is based on a logical view of analogical proportions (i.e., statements of the form ""A is to B as C is to D""). The reading of Raven matrices in terms of such proportions can be applied to a feature-based description of the pictures, but also, in a number of cases, to a very low level representation, i.e., the pixel level. It appears that the analogical proportion reading just amounts here to a recopy of patterns of feature values that already appear in the data (after checking that there is no conflicting patterns). Implementing this principle, the proposed algorithm computes the 9th picture, without the help of any set of candidate solutions, but only on the basis of the 8 known cells of the Raven matrices. A comparison with other approaches is provided, and we emphasize the generality of the approach which is able to provide a simple and uniform mechanism applicable in many situations. © 2012 The Author(s).",,"Artificial intelligence; Analogical proportions; Conflicting pattern; Feature values; Feature-based description; Geometric picture; Low level representation; Matrix cells; Pixel level; Matrix algebra",Conference Paper,Scopus,2-s2.0-84878791737
"Orseau L., Ring M.","Space-time embedded intelligence",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,10.1007/978-3-642-35506-6_22,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871360653&doi=10.1007%2f978-3-642-35506-6_22&partnerID=40&md5=ff7c7e5cd6869432547ff89fb6d2595d","This paper presents the first formal measure of intelligence for agents fully embedded within their environment. Whereas previous measures such as Legg's universal intelligence measure and Russell's bounded optimality provide theoretical insights into agents that interact with an external world, ours describes an intelligence that is computed by, can be modified by, and is subject to the time and space constraints of the environment with which it interacts. Our measure merges and goes beyond Legg's and Russell's, leading to a new, more realistic definition of artificial intelligence that we call Space-Time Embedded Intelligence. © 2012 Springer-Verlag.","AIXI; bounded optimality; Intelligence measure; real-world assumptions","AIXI; Embedded intelligence; Intelligence measure; Optimality; real-world assumptions; Space constraints; Artificial intelligence; Optimization",Conference Paper,Scopus,2-s2.0-84871360653
"Budden D., Fenn S., Walker J., Mendes A.","A novel approach to ball detection for humanoid robot soccer",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,10.1007/978-3-642-35101-3_70,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871373343&doi=10.1007%2f978-3-642-35101-3_70&partnerID=40&md5=c012ff587bc6a0569067707943672418","The ability to accurately track a ball is a critical issue in humanoid robot soccer, made difficult by processor limitations and resultant inability to process all available data from a high-definition image. This paper proposes a computationally efficient method of determining position and size of balls in a RoboCup environment, and compares the performance to two common methods: one utilising Levenberg-Marquardt least squares circle fitting, and the other utilising a circular Hough transform. The proposed method is able to determine the position of a non-occluded tennis ball with less than 10% error at a distance of 5 meters, and a half-occluded ball with less than 20% error, overall outperforming both compared methods whilst executing 300 times faster than the circular Hough transform method. The proposed method is described fully in the context of a colour based vision system, with an explanation of how it may be implemented independent of system paradigm. An extension to allow tracking of multiple balls utilising unsupervised learning and internal cluster validation is described. © 2012 Springer-Verlag.","clustering; computer vision; feature extraction; object recognition; robotic soccer; Robotics","Ball detection; Circle fitting; Cluster validation; clustering; Computationally efficient; Critical issues; High definition; Humanoid robot; Least Square; Levenberg-Marquardt; Processor limitations; RoboCup; Robotic soccer; Tennis balls; Transform methods; Vision systems; Anthropomorphic robots; Artificial intelligence; Computer vision; Feature extraction; Hough transforms; Object recognition; Robotics; Least squares approximations",Conference Paper,Scopus,2-s2.0-84871373343
"Rashid M.A., Hoque M.T., Newton M.A.H., Pham D.N., Sattar A.","A new genetic algorithm for simplified protein structure prediction",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,10.1007/978-3-642-35101-3_10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871376560&doi=10.1007%2f978-3-642-35101-3_10&partnerID=40&md5=a1554a0feb4eeefeaf48eede05191bc7","In this paper, we present a new genetic algorithm for protein structure prediction problem using face-centred cubic lattice and hydrophobic-polar energy model. Our algorithm uses i) an exhaustive generation approach to diversify the search; ii) a novel hydrophobic core-directed macro move to intensify the search; and iii) a random-walk strategy to recover from stagnation. On a set of standard benchmark proteins, our algorithm significantly outperforms the state-of-the-art algorithms for the same models. © 2012 Springer-Verlag.","Energy Models; Genetic Algorithms; Lattice Models; Local Search; Protein Structure Prediction; Random-walk","Energy model; Lattice models; Local search; Protein structure prediction; Random-walk; Artificial intelligence; Genetic algorithms; Hydrophobicity; Mathematical models",Conference Paper,Scopus,2-s2.0-84871376560
"Perols J.L., Murthy U.S.","Information Fusion in Continuous Assurance",2012,"Journal of Information Systems",14,10.2308/isys-50216,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870750991&doi=10.2308%2fisys-50216&partnerID=40&md5=40a808db0f0bd39c553f129a26a0e7e7","We extend continuous assurance research by proposing a novel continuous assurance architecture grounded in information fusion research. Existing continuous assurance architectures focus primarily on methods of monitoring assurance clients' systems to detect anomalous activities and have not addressed the question of how to process the detected anomalies. Consequently, actual implementations of these systems typically detect a large number of anomalies, with the resulting information overload leading to suboptimal decision making due to human information processing limitations. The proposed architecture addresses these issues by performing anomaly detection, aggregation, and evaluation. Within the proposed architecture, artifacts developed in prior continuous assurance, ontology, and artificial intelligence research are used to perform the detection, aggregation, and evaluation information fusion tasks. The architecture contributes to the academic continuous assurance literature and has implications for practitioners involved in the development of more robust and useful continuous assurance systems.","Artificial intelligence; Continuous assurance; Information fusion; Machine learning; REA",,Article,Scopus,2-s2.0-84870750991
"Karsak E.E., Sener Z., Dursun M.","Robot selection using a fuzzy regression-based decision-making approach",2012,"International Journal of Production Research",14,10.1080/00207543.2011.627886,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868545691&doi=10.1080%2f00207543.2011.627886&partnerID=40&md5=6f343c6a4a86836347debdb17b5d3994","Industrial robots, which enable manufacturing firms to produce high-quality products in a cost-effective manner, are important components of advanced manufacturing technologies. The performance of industrial robots is determined by multiple and conflicting criteria that have to be simultaneously considered in a robust selection study. In this study, a decision model based on fuzzy linear regression is presented for industrial robot selection. Fuzzy linear regression provides an alternative approach to statistical regression for modelling situations where the relationships are vague or the data set cannot satisfy the assumptions of statistical regression. The results obtained by employing fuzzy linear regression are compared with those of earlier studies applying different analytical methods to a previously reported robot selection problem. © 2012 Copyright Taylor and Francis Group, LLC.","advanced manufacturing technology; decision support systems; fuzzy regression; robot selection","Advanced manufacturing technologies; Alternative approach; Analytical method; Data sets; Decision models; Fuzzy linear regression; Fuzzy regressions; High-quality products; Manufacturing firms; Robot selection; Statistical regression; Artificial intelligence; Decision support systems; Industrial robots; Linear regression; Manufacture",Article,Scopus,2-s2.0-84868545691
"Christodoulou G., Petrakis E.G.M., Batsakis S.","Qualitative spatial reasoning using topological and directional information in OWL",2012,"Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI",14,10.1109/ICTAI.2012.86,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876865068&doi=10.1109%2fICTAI.2012.86&partnerID=40&md5=893d66efb17ee973e1cf59ad8f911baa","We investigate on potential improvements to reasoning approaches designed for spatial information in OWL. First, we introduce CHOROS, a qualitative spatial reasoning engine for ontologies in OWL. Building upon Pellet Spatial, CHOROS supports consistency checking and query answering for spatial information using Region-Connection Calculus (RCC), but also using the Cone-Shaped Directional (CSD) logic formalism. It works with all RCC and CSD relations in combination with standard RDF/OWL semantic relations in an OWL ontology and can answer SPARQL queries with spatial and non-spatial relations. We also present SOWL, a spatial reasoner for both relation calculi implemented in SWRL and runs under Pellet. We discuss and evaluate possible optimizations of CHOROS and compare its performance with that of SOWL. The experimental results demonstrate that CHOROS runs significantly faster than its respective SWRL implementation in most cases. © 2012 IEEE.","direction relations; Pellet; reasoning; spatial ontology; topologic relations","Direction relations; Pellet; reasoning; Spatial ontologies; topologic relations; Artificial intelligence; Biomineralization; Semantics; Pelletizing",Conference Paper,Scopus,2-s2.0-84876865068
"Zhang A., Fawaz N., Ioannidis S., Montanari A.","Guess who rated this movie: Identifying users through subspace clustering",2012,"Uncertainty in Artificial Intelligence - Proceedings of the 28th Conference, UAI 2012",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886033809&partnerID=40&md5=24a5122031985bc00efd60b14d4e2f58","It is often the case that, within an online recommender system, multiple users share a common account. Can such shared accounts be identified solely on the basis of the userprovided ratings? Once a shared account is identified, can the different users sharing it be identified as well? Whenever such user identification is feasible, it opens the way to possible improvements in personalized recommendations, but also raises privacy concerns. We develop a model for composite accounts based on unions of linear subspaces, and use subspace clustering for carrying out the identification task. We show that a significant fraction of such accounts is identifiable in a reliable manner, and illustrate potential uses for personalized recommendation.",,"Linear subspace; Multiple user; Online recommender systems; Personalized recommendation; Privacy concerns; Sub-Space Clustering; User identification; Artificial intelligence; Clustering algorithms",Conference Paper,Scopus,2-s2.0-84886033809
"Qu B., Liang J., Suganthan P.N., Chen T.","Ensemble of clearing differential evolution for multi-modal optimization",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,10.1007/978-3-642-30976-2_42,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875097846&doi=10.1007%2f978-3-642-30976-2_42&partnerID=40&md5=0c1b83d2a2d6a7727de610aae5d0c559","Multi-modal Optimization refers to finding multiple global and local optima of a function in one single run, so that the user can have a better knowledge about different optimal solutions. Multiple global/local peaks generate extra difficulties for the optimization algorithms. Many niching techniques have been developed in literature to tackle multi-modal optimization problems. Clearing is one of the simplest and most effective methods in solving multi-modal optimization problems. In this work, an Ensemble of Clearing Differential Evolution (ECLDE) algorithm is proposed to handle multi-modal problems. In this algorithm, the population is evenly divided into 3 subpopulations and each of the subpopulations is assigned a set of niching parameters (clearing radius). The algorithms is tested on 12 benchmark multi-modal optimization problems and compared with the Clearing Differential Evolution (CLDE) with single clearing radius as well as a number of commonly used niching algorithms. As shown in the experimental results, the proposed algorithm is able to generate satisfactory performance over the benchmark functions. © 2012 Springer-Verlag.","Differential evolution; evolutionary computation; multi-modal optimization; niching","Benchmark functions; Differential Evolution; Multi-modal optimization; Multimodal problems; niching; Niching algorithms; Niching techniques; Optimization algorithms; Artificial intelligence; Benchmarking; Evolutionary algorithms; Optimization",Conference Paper,Scopus,2-s2.0-84875097846
"Kim Y.-H., Kim J., Lee J.-H.","Iterative approach of dual regression with a sparse prior enhances the performance of independent component analysis for group functional magnetic resonance imaging (fMRI) data",2012,"NeuroImage",14,10.1016/j.neuroimage.2012.08.055,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866991727&doi=10.1016%2fj.neuroimage.2012.08.055&partnerID=40&md5=3abd9e1426879dd7cffe41ece98200de","This study proposes an iterative dual-regression (DR) approach with sparse prior regularization to better estimate an individual's neuronal activation using the results of an independent component analysis (ICA) method applied to a temporally concatenated group of functional magnetic resonance imaging (fMRI) data (i.e., Tc-GICA method). An ordinary DR approach estimates the spatial patterns (SPs) of neuronal activation and corresponding time courses (TCs) specific to each individual's fMRI data with two steps involving least-squares (LS) solutions. Our proposed approach employs iterative LS solutions to refine both the individual SPs and TCs with an additional a priori assumption of sparseness in the SPs (i.e., minimally overlapping SPs) based on L1-norm minimization. To quantitatively evaluate the performance of this approach, semi-artificial fMRI data were created from resting-state fMRI data with the following considerations: (1) an artificially designed spatial layout of neuronal activation patterns with varying overlap sizes across subjects and (2) a BOLD time series (TS) with variable parameters such as onset time, duration, and maximum BOLD levels. To systematically control the spatial layout variability of neuronal activation patterns across the ""subjects"" (n=12), the degree of spatial overlap across all subjects was varied from a minimum of 1voxel (i.e., 0.5-voxel cubic radius) to a maximum of 81voxels (i.e., 2.5-voxel radius) across the task-related SPs with a size of 100voxels for both the block-based and event-related task paradigms. In addition, several levels of maximum percentage BOLD intensity (i.e., 0.5, 1.0, 2.0, and 3.0%) were used for each degree of spatial overlap size. From the results, the estimated individual SPs of neuronal activation obtained from the proposed iterative DR approach with a sparse prior showed an enhanced true positive rate and reduced false positive rate compared to the ordinary DR approach. The estimated TCs of the task-related SPs from our proposed approach showed greater temporal correlation coefficients with a reference hemodynamic response function than those of the ordinary DR approach. Moreover, the efficacy of the proposed DR approach was also successfully demonstrated by the results of real fMRI data acquired from left-/right-hand clenching tasks in both block-based and event-related task paradigms. © 2012 Elsevier Inc.","Alternating least squares; Back reconstruction; Dual regression; General linear model; Group ICA; Independent component analysis; Iterative dual regression; Non-Gaussianity; Sparse prior","algorithm; article; BOLD signal; bootstrapping; functional magnetic resonance imaging; hemodynamics; image analysis; image processing; image quality; independent component analysis; mathematical analysis; mathematical model; neuroimaging; priority journal; receiver operating characteristic; Algorithms; Artificial Intelligence; Brain Mapping; Computer Simulation; Functional Laterality; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Normal Distribution; Oxygen; Principal Component Analysis; Regression Analysis; Reproducibility of Results; ROC Curve",Article,Scopus,2-s2.0-84866991727
"Chi C.-L., Nick Street W., Robinson J.G., Crawford M.A.","Individualized patient-centered lifestyle recommendations: An expert system for communicating patient specific cardiovascular risk information and prioritizing lifestyle options",2012,"Journal of Biomedical Informatics",14,10.1016/j.jbi.2012.07.011,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869868749&doi=10.1016%2fj.jbi.2012.07.011&partnerID=40&md5=7216648064c0ac4a12422c37f74e183d","We propose a proof-of-concept machine-learning expert system that learned knowledge of lifestyle and the associated 10-year cardiovascular disease (CVD) risks from individual-level data (i.e., Atherosclerosis Risk in Communities Study, ARIC). The expert system prioritizes lifestyle options and identifies the one that maximally reduce an individual's 10-year CVD risk by (1) using the knowledge learned from the ARIC data and (2) communicating for patient-specific cardiovascular risk information and personal limitations and preferences (as defined by variables used in this study). As a result, the optimal lifestyle is not only prioritized based on an individual's characteristics but is also relevant to personal circumstances.We also explored probable uses and tested the system in several examples using real-world scenarios and patient preferences. For example, the system identifies the most effective lifestyle activities as the starting point for an individual's behavior change, shows different levels of BMI changes and the associated CVD risk reductions to encourage weight loss, identifies whether weight loss or smoking cessation is the most urgent change for a diabetes patient, etc. Answers to the questions noted above vary based on an individual's characteristics. Our validation results from clinical trial simulations, which compared original with the optimal lifestyle using an independent dataset, show that the optimal individualized patient-centered lifestyle consistently reduced 10-year CVD risks. © 2012.","Decision support systems; Individualized lifestyle recommendation; K; Machine learning; Optimization; Patient centered medicine","Behavior change; Cardiovascular disease; Cardiovascular risk; Clinical trial; Data sets; Diabetes patients; Individualized lifestyle recommendation; Machine-learning; Patient specific; Proof of concept; Real-world scenario; Risk reductions; Validation results; Weight loss; Artificial intelligence; Decision support systems; Diseases; Learning systems; Optimization; Potassium; Expert systems; adult; aged; article; behavior change; body mass; cardiovascular disease; cardiovascular risk; female; human; individualized patient centered lifestyle recommendation; knowledge; lifestyle modification; machine learning; major clinical study; male; patient preference; priority journal; risk reduction; smoking cessation; validation study; weight reduction; Artificial Intelligence; Cardiovascular Diseases; Decision Support Systems, Clinical; Humans; Life Style; Patient Acceptance of Health Care; Patient Education as Topic; Patient-Centered Care; Risk Factors; Risk Reduction Behavior",Article,Scopus,2-s2.0-84869868749
"Jaffe A., Miller A., Andersen E., Liu E.A.Y.-E., Karlin A., Popović Z.","Evaluating competitive game balance with restricted play",2012,"Proceedings of the 8th AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, AIIDE 2012",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883073204&partnerID=40&md5=60f41ac7527e4b3047d96683c139c6fa","Game balancing is the fine-tuning phase in which a functioning game is adjusted to be deep, fair, and interesting. Balancing is difficult and time-consuming, as designers must repeatedly tweak parameters, and run lengthy playtests to evaluate the effects of these changes. If designers could receive immediate feedback on their designs, they could explore a vast space of variations, and select only the most promising games for playtesting. Such automated design feedback has been difficult to achieve, as there is no mathematical formulation of game balance that unifies many of its forms. We argue for a formulation in which carefully restricted agents are played against standard agents. We develop this restricted-play balance framework, and evaluate its utility by building a tool capable of calculating measures of balance for a large family of games. By applying this tool to an educational card game, we demonstrate how the framework and tool allow designers to rapidly evaluate and iterate on the balance of their games. Copyright © 2012, Association for the Advancement of Artificial Intelligence.",,"Automated design; Card games; Competitive games; Game balances; Game balancing; Immediate feedbacks; Mathematical formulation; Artificial intelligence; Human computer interaction; Tools; Design",Conference Paper,Scopus,2-s2.0-84883073204
"Lu C., Zheng Y., Birkbeck N., Zhang J., Kohlberger T., Tietjen C., Boettger T., Duncan J.S., Zhou S.K.","Precise segmentation of multiple organs in CT volumes using learning-based approach and information theory.",2012,"Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872909521&partnerID=40&md5=66125edba7dba1243e46e56033268fc8","In this paper, we present a novel method by incorporating information theory into the learning-based approach for automatic and accurate pelvic organ segmentation (including the prostate, bladder and rectum). We target 3D CT volumes that are generated using different scanning protocols (e.g., contrast and non-contrast, with and without implant in the prostate, various resolution and position), and the volumes come from largely diverse sources (e.g., diseased in different organs). Three key ingredients are combined to solve this challenging segmentation problem. First, marginal space learning (MSL) is applied to efficiently and effectively localize the multiple organs in the largely diverse CT volumes. Second, learning techniques, steerable features, are applied for robust boundary detection. This enables handling of highly heterogeneous texture pattern. Third, a novel information theoretic scheme is incorporated into the boundary inference process. The incorporation of the Jensen-Shannon divergence further drives the mesh to the best fit of the image, thus improves the segmentation performance. The proposed approach is tested on a challenging dataset containing 188 volumes from diverse sources. Our approach not only produces excellent segmentation accuracy, but also runs about eighty times faster than previous state-of-the-art solutions. The proposed method can be applied to CT images to provide visual guidance to physicians during the computer-aided diagnosis, treatment planning and image-guided radiotherapy to treat cancers in pelvic region.",,"algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; computer assisted tomography; human; image quality; methodology; radiography; reproducibility; sensitivity and specificity; three dimensional imaging; viscera; Algorithms; Artificial Intelligence; Humans; Imaging, Three-Dimensional; Pattern Recognition, Automated; Radiographic Image Enhancement; Radiographic Image Interpretation, Computer-Assisted; Reproducibility of Results; Sensitivity and Specificity; Tomography, X-Ray Computed; Viscera",Article,Scopus,2-s2.0-84872909521
"Wipf D.","Non-convex rank minimization via an empirical bayesian approach",2012,"Uncertainty in Artificial Intelligence - Proceedings of the 28th Conference, UAI 2012",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886046670&partnerID=40&md5=647876c71b68d287d9407dc9ff33d1ca","In many applications that require matrix solutions of minimal rank, the underlying cost function is non-convex leading to an intractable, NP-hard optimization problem. Consequently, the convex nuclear norm is frequently used as a surrogate penalty term for matrix rank. The problem is that in many practical scenarios there is no longer any guarantee that we can correctly estimate generative low-rank matrices of interest, theoretical special cases notwithstanding. Consequently, this paper proposes an alternative empirical Bayesian procedure build upon a variational approximation that, unlike the nuclear norm, retains the same globally minimizing point estimate as the rank function under many useful constraints. However, locally minimizing solutions are largely smoothed away via marginalization, allowing the algorithm to succeed when standard convex relaxations completely fail. While the proposed methodology is generally applicable to a wide range of low-rank applications, we focus our attention on the robust principal component analysis problem (RPCA), which involves estimating an unknown low-rank matrix with unknown sparse corruptions. Theoretical and empirical evidence are presented to show that our method is potentially superior to related MAP-based approaches, for which the convex principle component pursuit (PCP) algorithm (Cand'es et al., 2011) can be viewed as a special case.",,"Convex relaxation; Empirical Bayesian; Map-based approach; Optimization problems; Principle component; Rank minimizations; Robust principal component analysis; Variational approximation; Algorithms; Artificial intelligence; Bayesian networks; Principal component analysis; Relaxation processes; Estimation",Conference Paper,Scopus,2-s2.0-84886046670
"Bareinboim E., Pearl J.","Causal inference by surrogate experiments: Z-identifiability",2012,"Uncertainty in Artificial Intelligence - Proceedings of the 28th Conference, UAI 2012",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886026123&partnerID=40&md5=0b2f4f89e7b94becd54f4ea3b1026dd2","We address the problem of estimating the effect of intervening on a set of variables X from experiments on a different set, Z, that is more accessible to manipulation. This problem, which we call z-identifiability, reduces to ordinary identifiability when Z = ; and, like the latter, can be given syntactic characterization using the do-calculus [Pearl, 1995; 2000]. We provide a graphical necessary and sufficient condition for zidentifiability for arbitrary sets X,Z, and Y (the outcomes). We further develop a complete algorithm for computing the causal effect of X on Y using information provided by experiments on Z. Finally, we use our results to prove completeness of do-calculus relative to z-identifiability, a result that does not follow from completeness relative to ordinary identifiability.",,"Arbitrary sets; Causal inferences; Do-calculus; Identifiability; Syntactic characterization; Artificial intelligence; Calculations; Experiments",Conference Paper,Scopus,2-s2.0-84886026123
"Tumkur K., Subbiah S.","Modeling human walking for step detection and stride determination by 3-axis accelerometer readings in pedometer",2012,"Proceedings of International Conference on Computational Intelligence, Modelling and Simulation",14,10.1109/CIMSim.2012.65,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872568948&doi=10.1109%2fCIMSim.2012.65&partnerID=40&md5=c6fd05a50322fa49c8371ddb93a43a7b","Pedometers are increasingly being used for purposes like assessment of physical activity, rehabilitation, disease management and as a utility for the visually impaired. These devices rely on algorithms that consist of measuring distance covered by the number of steps taken. Conventional estimations of distance covered rely on empirically obtained relations to arrive at their data. In this paper, we present a means of estimating the distance covered by the wearer through a more realistic model using sensor readings from a 3-axis accelerometer present in most pedometers as well as in modern phones and tablets. First comes step detection where each individual step taken by the wearer is identified. Subsequently, we determine stride length, based on the inverted pendulum model and the periodicity of accelerations generated by humans. Lastly, we obtain the total distance covered during the walk. Finally, we analyze the results with respect to currently existing models. © 2012 IEEE.","3-axis accelerometer; Human walking; Sinusoidal curve approximation; Step detection; Stride length","3-axis accelerometer; Curve approximation; Human walking; Step detection; Stride length; Accelerometers; Artificial intelligence; Estimation; Walking aids",Conference Paper,Scopus,2-s2.0-84872568948
"Groumpos P.P., Anninou A.P.","A theoretical mathematical modeling of Parkinson's disease using Fuzzy Cognitive Maps",2012,"IEEE 12th International Conference on BioInformatics and BioEngineering, BIBE 2012",14,10.1109/BIBE.2012.6399748,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872851641&doi=10.1109%2fBIBE.2012.6399748&partnerID=40&md5=3fbb9d248c0cf58799fdda817c9e43ac","The mathematical model of medical problems is considered. The aim of this paper is to present a new approach in modeling the disease of Parkinson using Fuzzy Cognitive Maps (FCM). Basic theories of FCMs are reviewed and presented. Decision Support Systems (DSS) for Medical problems are considered. The disease of Parkinson is mathematically modeled using Fuzzy Cognitive Maps and three (3) experts. Linguistic variables are proposed and used to describe the correlation among concepts of the FCM. Simulations are performed and very interesting results are obtained and discussed. © 2012 IEEE.","decision support system; fuzzy cognitive map; mathematical modeling; parkinson disease","Basic theory; Fuzzy cognitive map; Linguistic variable; Parkinson disease; Parkinson's disease; Artificial intelligence; Bioinformatics; Decision support systems; Diseases; Fuzzy rules; Large scale systems; Mathematical models; Fuzzy systems",Conference Paper,Scopus,2-s2.0-84872851641
"Dibangoye J.S., Amato C., Doniec A.","Scaling up decentralized MDPs through heuristic search",2012,"Uncertainty in Artificial Intelligence - Proceedings of the 28th Conference, UAI 2012",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886019307&partnerID=40&md5=7dafa2259c30cd1848e2659e1d58d6a9","Decentralized partially observable Markov decision processes (Dec-POMDPs) are rich models for cooperative decision-making under uncertainty, but are often intractable to solve optimally (NEXP-complete). The transition and observation independent Dec-MDP is a general subclass that has been shown to have complexity in NP, but optimal algorithms for this subclass are still inefficient in practice. In this paper, we first provide an updated proof that an optimal policy does not depend on the histories of the agents, but only the local observations. We then present a new algorithm based on heuristic search that is able to expand search nodes by using constraint optimization. We show experimental results comparing our approach with the state-of-the-art Dec- MDP and Dec-POMDP solvers. These results show a reduction in computation time and an increase in scalability by multiple orders of magnitude in a number of benchmarks.",,"Computation time; Constraint optimizations; Cooperative decision-making; Heuristic search; Local observations; Optimal algorithm; Optimal policies; Partially observable Markov decision process; Artificial intelligence; Heuristic algorithms; Optimization; Modular robots",Conference Paper,Scopus,2-s2.0-84886019307
"Savolainen R.","The structure of argument patterns on a social Q&A site",2012,"Journal of the American Society for Information Science and Technology",14,10.1002/asi.22722,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870519670&doi=10.1002%2fasi.22722&partnerID=40&md5=d8b21b118f80d8d64924a114dac011bc","This study investigates the argument patterns in Yahoo! Answers, a major question and answer (Q&A) site. Mainly drawing on the ideas of Toulmin (), argument pattern is conceptualized as a set of 5 major elements: claim, counterclaim, rebuttal, support, and grounds. The combinations of these elements result in diverse argument patterns. Failed opening consists of an initial claim only, whereas nonoppositional argument pattern also includes indications of support. Oppositional argument pattern contains the elements of counterclaim and rebuttal. Mixed argument pattern entails all 5 elements. The empirical data were gathered by downloading from Yahoo! Answers 100 discussion threads discussing global warming-a controversial topic providing a fertile ground for arguments for and against. Of the argument patterns, failed openings were most frequent, followed by oppositional, nonoppositional, and mixed patterns. In most cases, the participants grounded their arguments by drawing on personal beliefs and facts. The findings suggest that oppositional and mixed argument patterns provide more opportunities for the assessment of the quality and credibility of answers, as compared to failed openings and nonoppositional argument patterns. © 2012 ASIS&T.","computer mediated communications; credibility","Computer mediated communication; Controversial topics; credibility; Empirical data; Major elements; Mixed argument; Toulmin; Artificial intelligence; Software engineering; Global warming",Article,Scopus,2-s2.0-84870519670
"Perera L.P., Rodrigues J.M., Pascoal R., Guedes Soares C.","Development of an onboard decision support system for ship navigation under rough weather conditions",2012,"Sustainable Maritime Transportation and Exploitation of Sea Resources - Proceedings of the 14th International Congress of the International Maritime Association of the Mediterranean, IMAM 2011",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896689524&partnerID=40&md5=d0f518d8e961cb3dc19f42a7cb8510b4","The paper describes the development of an onboard decision support system to support ship operation, in particular on decisions about ship handling in waves, which will contribute to vessel safety. The prototype system monitors several motion related parameters, and, by processing these data, provides the ship master with the information about the consequences of the different ship handling decisions. The paper describes the decision criteria and the approaches adopted for the calculation of the parameters that govern the master's decisions. It describes the software that was developed to perform those calculations and to display in a user interface the advice to the master as well as the data acquisition and processing hardware that has been organized for the on board monitoring of motions and strains in the structure. © 2012 Taylor & Francis Group, London, UK.",,"Decision criterions; On-board monitoring; Processing hardware; Prototype system; Ship handling; Ship navigation; Support ships; Vessel safety; Artificial intelligence; Data handling; Decision support systems; Navigation; User interfaces; Waterway transportation; Ships",Conference Paper,Scopus,2-s2.0-84896689524
"Rahman A., Ng V.","Resolving complex cases of definite pronouns: The winograd schema challenge",2012,"EMNLP-CoNLL 2012 - 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, Proceedings of the Conference",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876801475&partnerID=40&md5=95fead130462e2932e7830cd77a8a71a","We examine the task of resolving complex cases of definite pronouns, specifically those for which traditional linguistic constraints on coreference (e.g., Binding Constraints, gender and number agreement) as well as commonly-used resolution heuristics (e.g., string-matching facilities, syntactic salience) are not useful. Being able to solve this task has broader implications in artificial intelligence: a restricted version of it, sometimes referred to as the Winograd Schema Challenge, has been suggested as a conceptually and practically appealing alternative to the Turing Test. We employ a knowledge-rich approach to this task, which yields a pronoun resolver that outperforms state-of-the-art resolvers by nearly 18 points in accuracy on our dataset. © 2012 Association for Computational Linguistics.",,"Coreference; Linguistic constraints; String matching; Turing tests; Winograd; Artificial intelligence; Natural language processing systems",Conference Paper,Scopus,2-s2.0-84876801475
"Ma F., Liu F., Liu Z.","Multi-objective optimization for initial virtual machine placement in cloud data center",2012,"Journal of Information and Computational Science",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872253089&partnerID=40&md5=f6d74796cd1e471915ed680bad2631e5","Virtual machine (VM) placement in the cloud infrastructure is an important problem that remains to be effectively addressed. Fine-grained virtual machine resource allocation and reallocation are possible in order to meet the performance targets of applications running on virtual machines. On the other hand, these capabilities create demands on system management, especially for cloud data center. In this paper, a management framework for virtual machine placement in an IaaS environment was firstly presented, then the initial VM placement problem was defined as a multi-objective optimization problem and finally multi-objective optimization for initial virtual machine placement based on Ant Colony Optimization (ACO) was proposed to determine VM placement strategy. The proposed algorithm is a distributed optimization method, which is beneficial to parallel computing. It has the positive feedback mechanism, and through the pheromone is constantly updated, it can get the optimal solution by the efficient convergence. Experimental results show that compared to heuristic method and genetic algorithm, the proposed algorithm can achieve the optimal balance in multiple conflict objectives, which effectively reduces the resource wastage and power consumption, and minimize violation of SLA. © 2012 Binary Information Press.","Ant colony optimization; Cloud computing; Multi-objective optimization; Virtual machine placement; Virtualization","Ant Colony Optimization (ACO); Cloud data; Distributed optimization; Feedback mechanisms; Management frameworks; Multi objective optimizations (MOO); Multi-objective optimization problem; Optimal balance; Optimal solutions; Performance targets; Placement problems; Placement strategy; System management; Virtual machines; Virtualizations; Artificial intelligence; Cloud computing; Computer simulation; Heuristic methods; Information management; Multiobjective optimization; Parallel architectures; Algorithms",Article,Scopus,2-s2.0-84872253089
"Liu L., Jia K.","Detecting spam in Chinese microblogs - A study on Sina Weibo",2012,"Proceedings of the 2012 8th International Conference on Computational Intelligence and Security, CIS 2012",14,10.1109/CIS.2012.135,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873539202&doi=10.1109%2fCIS.2012.135&partnerID=40&md5=e7886db4f7e53ee0d6c602070c4d5406","Sina Weibo is the most popular and fast growing microblogging social network in China. However, more and more spam messages are also emerging on Sina Weibo. How to detect these spam is essential for the social network security. While most previous studies attempt to detect the microblogging spam by identifying spammers, in this paper, we want to exam whether we can detect the spam by each single Weibo message, because we notice that more and more spam Weibos are posted by normal users or even popular verified users. We propose a Weibo spam detection method based on machine learning algorithm. In addition, different from most existing microblogging spam detection methods which are based on English microblogs, our method is designed to deal with the features of Chinese microblogs. Our extensive empirical study shows the effectiveness of our approach. © 2012 IEEE.","data mining; machine learning; spam detection; web security","Empirical studies; Microblogging; Microblogs; On-machines; Sina-weibo; Social network securities; Social Networks; Spam detection; Spam messages; Spammers; WEB security; Artificial intelligence; Data mining; Learning algorithms; Learning systems; Network security; Social networking (online); Internet",Conference Paper,Scopus,2-s2.0-84873539202
"Feng S., Deng L., Shu G., Wang F., Deng H., Ji K.","A subpixel registration algorithm for low PSNR images",2012,"2012 IEEE 5th International Conference on Advanced Computational Intelligence, ICACI 2012",14,10.1109/ICACI.2012.6463241,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874619554&doi=10.1109%2fICACI.2012.6463241&partnerID=40&md5=6495a6a63ce3a85cd51bc94acf86d363","This paper presents a fast algorithm for obtaining high-accuracy subpixel translation of low PSNR images. Instead of locating the maximum point on the upsampled images or fitting the peak of correlation surface, the proposed algorithm is based on the measurement of centroid on the cross correlation surface by Modified Moment method. Synthetic images, real solar images and standard testing images with white Gaussian noise added were tested, and the results show that the accuracies of our algorithm are comparable with other subpixel registration techniques and the processing speed is higher. The drawback is also discussed at the end of this paper. © 2012 IEEE.",,"Cross correlations; Fast algorithms; High-accuracy; Processing speed; Solar images; Standard testing; Sub-pixel registrations; Subpixel translation; Synthetic images; White Gaussian Noise; Artificial intelligence; Method of moments; Algorithms",Conference Paper,Scopus,2-s2.0-84874619554
"Charles A.S., Garrigues P., Rozell C.J.","A common network architecture efficiently implements a variety of sparsity-based inference problems",2012,"Neural Computation",14,10.1162/NECO_a_00372,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871006492&doi=10.1162%2fNECO_a_00372&partnerID=40&md5=b7755acb6ffff3469ed9ad7b24d05942","The sparse coding hypothesis has generated significant interest in the computational and theoretical neuroscience communities, but there remain open questions about the exact quantitative form of the sparsity penalty and the implementation of such a coding rule in neurally plausible architectures. The main contribution of this work is to show that a wide variety of sparsity-based probabilistic inference problems proposed in the signal processing and statistics literatures can be implemented exactly in the common network architecture known as the locally competitive algorithm (LCA). Among the cost functions we examine are approximate ℓp norms (0 ≤ P ≤ 2), modified ℓp-norms, block-ℓ1 orms, and reweighted algorithms. Of particular interest is that we show significantly increased performance in reweighted ℓ1 algorithms by inferring all parameters jointly in a dynamical system rather than using an iterative approach native to digital computational architectures. © 2012 Massachusetts Institute of Technology.",,"algorithm; animal; article; artificial intelligence; computer simulation; human; physiology; somatosensory cortex; Algorithms; Animals; Artificial Intelligence; Computer Simulation; Humans; Somatosensory Cortex",Article,Scopus,2-s2.0-84871006492
"Affandi R.H., Kulesza A., Fox E.B.","Markov determinantal point processes",2012,"Uncertainty in Artificial Intelligence - Proceedings of the 28th Conference, UAI 2012",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885977616&partnerID=40&md5=5084b014e02bf90129f2518ebe8df2e7","A determinantal point process (DPP) is a random process useful for modeling the combinatorial problem of subset selection. In particular, DPPs encourage a random subset Y to contain a diverse set of items selected from a base set Y. For example, we might use a DPP to display a set of news headlines that are relevant to a user's interests while covering a variety of topics. Suppose, however, that we are asked to sequentially select multiple diverse sets of items, for example, displaying new headlines day-by-day. We might want these sets to be diverse not just individually but also through time, offering headlines today that are unlike the ones shown yesterday. In this paper, we construct a Markov DPP (M-DPP) that models a sequence of random sets {Y t}. The proposed M-DPP defines a stationary process that maintains DPP margins. Crucially, the induced union process Zt Y t[Y t-1 is also marginally DPP-distributed. Jointly, these properties imply that the sequence of random sets are encouraged to be diverse both at a given time step as well as across time steps. We describe an exact, efficient sampling procedure, and a method for incrementally learning a quality measure over items in the base set Y based on external preferences. We apply the M-DPP to the task of sequentially displaying diverse and relevant news articles to a user with topic preferences.",,"Combinatorial problem; Efficient sampling; Quality measures; Random subsets; Stationary process; Subset selection; Topic preferences; User's interest; Random processes; Set theory; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84885977616
"Robert V., Leroy X.","A formally-verified alias analysis",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,10.1007/978-3-642-35308-6_5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869816501&doi=10.1007%2f978-3-642-35308-6_5&partnerID=40&md5=e1d5d9001de227f47d37ebf129b537e0","This paper reports on the formalization and proof of soundness, using the Coq proof assistant, of an alias analysis: a static analysis that approximates the flow of pointer values. The alias analysis considered is of the points-to kind and is intraprocedural, flow-sensitive, field-sensitive, and untyped. Its soundness proof follows the general style of abstract interpretation. The analysis is designed to fit in the CompCert C verified compiler, supporting future aggressive optimizations over memory accesses. © 2012 Springer-Verlag Berlin Heidelberg.",,"Abstract interpretations; Alias analysis; Coq proof assistant; Memory access; Soundness proofs; Artificial intelligence; Static analysis",Conference Paper,Scopus,2-s2.0-84869816501
"Bouchon-Meunier B., Moyse G.","Fuzzy linguistic summaries: Where are we, where can we go?",2012,"2012 IEEE Conference on Computational Intelligence for Financial Engineering and Economics, CIFEr 2012 - Proceedings",14,10.1109/CIFEr.2012.6327810,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869854244&doi=10.1109%2fCIFEr.2012.6327810&partnerID=40&md5=0f8b8111ce598fa91c7e0acee2d8aa5c","Along with the increase of the amount of data stored and to be analyzed, different techniques of data analysis have been developed over the years. One of them, the linguistic summary, aims at summing up large volume of data into simple sentences. In this paper, we present an overview of two main streams of research, namely fuzzy logic based systems and natural language generation, covering the methods designed to work with numerical data, time series, or simple labels (enumerations). We focus on the former stream and we give some hints to go further on fuzzy quantifiers. © 2012 IEEE.",,"Fuzzy linguistics; Fuzzy quantifiers; Linguistic summaries; Logic based systems; Natural language generation; Numerical data; Artificial intelligence; Fuzzy logic; Numerical methods; Linguistics",Conference Paper,Scopus,2-s2.0-84869854244
"Linda O., Wijayasekara D., Manic M., Rieger C.","Computational intelligence based anomaly detection for Building Energy Management Systems",2012,"Proceedings - 2012 5th International Symposium on Resilient Control Systems, ISRCS 2012",14,10.1109/ISRCS.2012.6309297,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868556243&doi=10.1109%2fISRCS.2012.6309297&partnerID=40&md5=c145a6df3e923e3a79d9b451098086e8","In the past several decades Building Energy Management Systems (BEMSs) have become vital components of most modern buildings. BEMSs utilize advanced microprocessor technology combined with extensive sensor data collection and communication to minimize energy consumption while maintaining high human comfort levels. When properly tuned and operated, BEMSs can provide significant energy savings. However, the complexity of the acquired sensory data and the overwhelming amount of presented information renders them difficult to adjust or even understand by responsible building managers. This inevitably results in suboptimal BEMS operation and performance. To address this issue, this paper reports on a research effort that utilizes Computational Intelligence techniques to fuse multiple heterogeneous sources of BEMS data and to extract relevant actionable information. This actionable information can then be easily understood and acted upon by responsible building managers. In particular, this paper describes the use of anomaly detection algorithms for improving the understandability of BEMS data and for increasing the state-awareness of building managers. The developed system utilizes modified nearest neighbor clustering algorithm and fuzzy logic rule extraction technique to automatically build a model of normal BEMS operations and detect possible anomalous behavior. In addition, linguistic summaries based on fuzzy set representation of the input values are generated for the detected anomalies which increase the understandability of the presented results. © 2012 IEEE.","Anomaly Detection; Building Energy Management Systems; Computational Intelligence","Anomalous behavior; Anomaly detection; Anomaly-detection algorithms; Building energy management systems; Building managers; Computational intelligence techniques; Fuzzy logic rules; Heterogeneous sources; Human comforts; Input values; Linguistic summaries; Modern buildings; Nearest neighbor-clustering algorithm; Research efforts; Sensor data collections; Sensory data; Set representation; Understandability; Clustering algorithms; Control systems; Energy utilization; Fuzzy logic; Fuzzy sets; Management; Managers; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868556243
"Domingos P., Webb W.A.","A tractable first-order probabilistic logic",2012,"Proceedings of the National Conference on Artificial Intelligence",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868266905&partnerID=40&md5=fe0a7e02ca13ec081abdae84b3eba518","Tractable subsets of first-order logic are a central topic in AI research. Several of these formalisms have been used as the basis for first-order probabilistic languages. However, these are intractable, losing the original motivation. Here we propose the first non-trivially tractable first-order probabilistic language. It is a subset of Markov logic, and uses probabilistic class and part hierarchies to control complexity. We call it TML (Tractable Markov Logic). We show that TML knowledge bases allow for efficient inference even when the corresponding graphical models have very high treewidth. We also show how probabilistic inheritance, default reasoning, and other inference patterns can be carried out in TML. TML opens up the prospect of efficient large-scale first-order probabilistic inference. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Control complexity; Default reasoning; First order logic; First-order; GraphicaL model; Inference patterns; Knowledge basis; Markov logic; Probabilistic class; Probabilistic inference; Probabilistic language; Tractable subset; Tree-width; Markov processes; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868266905
"Weber B.G., Mateas M., Jhala A.","Learning from demonstration for goal-driven autonomy",2012,"Proceedings of the National Conference on Artificial Intelligence",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868271383&partnerID=40&md5=d006d4c85028e6e2dbd43d3b546fc89e","Goal-driven autonomy (GDA) is a conceptual model for creating an autonomous agent that monitors a set of expectations during plan execution, detects when discrepancies occur, builds explanations for the cause of failures, and formulates new goals to pursue when planning failures arise. While this framework enables the development of agents that can operate in complex and dynamic environments, implementing the logic for each of the subtasks in the model requires substantial domain engineering. We present a method using case-based reasoning and intent recognition in order to build GDA agents that learn from demonstrations. Our approach reduces the amount of domain engineering necessary to implement GDA agents and learns expectations, explanations, and goals from expert demonstrations. We have applied this approach to build an agent for the real-time strategy game StarCraft. Our results show that integrating the GDA conceptual model into the agent greatly improves its win rate. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Conceptual model; Domain engineering; Dynamic environments; Intent recognition; Learning from demonstration; Plan execution; Real-time strategy games; Subtasks; Autonomous agents; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868271383
"Gupta A., Verma Y., Jawahar C.V.","Choosing linguistics over vision to describe images",2012,"Proceedings of the National Conference on Artificial Intelligence",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868289993&partnerID=40&md5=b2319a8a87ddf6816507a3c360fc2453","In this paper, we address the problem of automatically generating human-like descriptions for unseen images, given a collection of images and their corresponding human-generated descriptions. Previous attempts for this task mostly rely on visual clues and corpus statistics, but do not take much advantage of the semantic information inherent in the available image descriptions. Here, we present a generic method which benefits from all these three sources (i.e. visual clues, corpus statistics and available descriptions) simultaneously, and is capable of constructing novel descriptions. Our approach works on syntactically and linguistically motivated phrases extracted from the human descriptions. Experimental evaluations demonstrate that our formulation mostly generates lucid and semantically correct descriptions, and significantly outperforms the previous methods on automatic evaluation metrics. One of the significant advantages of our approach is that we can generate multiple interesting descriptions for an image. Unlike any previous work, we also test the applicability of our method on a large dataset containing complex images with rich descriptions. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Automatic evaluation; Complex image; Data sets; Experimental evaluation; Generic method; Image descriptions; Semantic information; Visual clues; Linguistics; Statistical tests; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868289993
"Rahwan T., Michalak T., Jennings N.R.","A hybrid algorithm for coalition structure generation",2012,"Proceedings of the National Conference on Artificial Intelligence",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868297680&partnerID=40&md5=cf4858b56f5511b7947bc9e7dd4ad4d8","The current state-of-the-art algorithm for optimal coalition structure generation is IDP-IP - an algorithm that combines IDP (a dynamic programming algorithm due to Rahwan and Jennings, 2008b) with IP (a tree-search algorithm due to Rahwan et al., 2009). In this paper we analyse IDP-IP, highlight its limitations, and then develop a new approach for combining IDP with IP that overcomes these limitations. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Coalition structure; Dynamic programming algorithm; Hybrid algorithms; Optimal coalition; State-of-the-art algorithms; Tree-search; Artificial intelligence; Copyrights; Algorithms",Conference Paper,Scopus,2-s2.0-84868297680
"Bäckström C., Chen Y., Jonsson P., Ordyniak S., Szeider S.","The complexity of planning revisited - A parameterized analysis",2012,"Proceedings of the National Conference on Artificial Intelligence",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868297258&partnerID=40&md5=9677e8d361090f1a003c7ee82857ce72","The early classifications of the computational complexity of planning under various restrictions in STRIPS (Bylander) and SAS + (Bäckström and Nebel) have influenced following research in planning in many ways. We go back and reanalyse their subclasses, but this time using the more modern tool of parameterized complexity analysis. This provides new results that together with the old results give a more detailed picture of the complexity landscape. We demonstrate separation results not possible with standard complexity theory, which contributes to explaining why certain cases of planning have seemed simpler in practice than theory has predicted. In particular, we show that certain restrictions of practical interest are tractable in the parameterized sense of the term, and that a simple heuristic is sufficient to make a well-known partial-order planner exploit this fact. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Complexity theory; Modern tools; New results; Parameterized; Parameterized complexity; Computational complexity; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868297258
"Kong W., Li W.-J.","Double-bit quantization for hashing",2012,"Proceedings of the National Conference on Artificial Intelligence",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868289901&partnerID=40&md5=eb7cbc0f19c05d74d0d30f66a5423f2e","Hashing, which tries to learn similarity-preserving binary codes for data representation, has been widely used for efficient nearest neighbor search in massive databases due to its fast query speed and low storage cost. Because it is NP hard to directly compute the best binary codes for a given data set, mainstream hashing methods typically adopt a two-stage strategy. In the first stage, several projected dimensions of real values are generated. Then in the second stage, the real values will be quantized into binary codes by thresholding. Currently, most existing methods use one single bit to quantize each projected dimension. One problem with this single-bit quantization (SBQ) is that the threshold typically lies in the region of the highest point density and consequently a lot of neighboring points close to the threshold will be hashed to totally different bits, which is unexpected according to the principle of hashing. In this paper, we propose a novel quantization strategy, called double-bit quantization (DBQ), to solve the problem of SBQ. The basic idea of DBQ is to quantize each projected dimension into double bits with adaptively learned thresholds. Extensive experiments on two real data sets show that our DBQ strategy can significantly outperform traditional SBQ strategy for hashing. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Data representations; Data sets; Fast query; Hashing method; Low-storage; Nearest Neighbor search; Neighboring point; NP-hard; Point density; Real data sets; Real values; Single-bit; Thresholding; Binary codes; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868289901
"Perfilieva I., Hodáková P., Hurtík P.","F 1-transform edge detector inspired by Canny's algorithm",2012,"Communications in Computer and Information Science",14,10.1007/978-3-642-31709-5_24,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867766645&doi=10.1007%2f978-3-642-31709-5_24&partnerID=40&md5=f47b40044b72ed53720e370bc5bd3e40","In this paper the edge detection technique based on the F 1-transform is presented. The method F 1-transform is used for preprocessing in the well known Canny detector. A justification of using F 1-transform in edge detection is presented. Finally, a comparative analysis of the classical Canny algorithm and the F 1-transform edge detector together with various examples is given. © 2012 Springer-Verlag Berlin Heidelberg.","Canny detector; edge detection; F 1-transform; F-transform; image processing","Canny algorithm; Canny detector; Comparative analysis; Detection technique; Edge detectors; F-transform; Algorithms; Artificial intelligence; Data processing; Image processing; Information management; Knowledge based systems; Edge detection",Conference Paper,Scopus,2-s2.0-84867766645
"Castillo-Ortega R., Marín N., Sánchez D., Tettamanzi A.G.B.","Quality assessment in linguistic summaries of data",2012,"Communications in Computer and Information Science",14,10.1007/978-3-642-31715-6-31,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868023343&doi=10.1007%2f978-3-642-31715-6-31&partnerID=40&md5=3f2b4585cd0ea109a06387b07aa0b817","We study the problem of ordering linguistic summaries of data in terms of quality, and we propose a model of quality on the basis of four basic criteria. The importance of the user in the definition and the properties of ordering relations are stressed in this paper. We illustrate our approach in the specific setting of linguistic summaries of time series data. © 2012 Springer-Verlag Berlin Heidelberg.","linguistic summaries of data; preorder; Quality","Linguistic summaries; Ordering relations; Preorders; Quality assessment; Time-series data; Artificial intelligence; Data processing; Image quality; Knowledge based systems; Linguistics; Information management",Conference Paper,Scopus,2-s2.0-84868023343
"Brozzi A., Capotorti A., Vantaggi B.","Incoherence correction strategies in statistical matching",2012,"International Journal of Approximate Reasoning",14,10.1016/j.ijar.2012.06.009,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866041303&doi=10.1016%2fj.ijar.2012.06.009&partnerID=40&md5=4309af619ab4d80dafcf954edb3dd900","Several economic applications require to consider different data sources and to integrate the information coming from them. This paper focuses on statistical matching, in particular we deal with incoherences. In fact, when logical constraints among the variables are present incoherencies on the probability evaluations can arise. The aim of this paper is to remove such incoherences by using different methods based on distances minimization or least commitment imprecise probabilities extensions. An illustrative example shows peculiarities of the different correction methods. Finally, limited to pseudo distance minimization, we performed a systematic comparison through a simulation study. © 2012 Elsevier Inc. All rights reserved.","Incoherence; Inference; Specialized discrepancy measure; Statistical matching","Correction method; Data source; Illustrative examples; Imprecise probabilities; Incoherence; Inference; Logical constraints; Probability evaluation; Simulation studies; Specialized discrepancy measure; Statistical matching; Artificial intelligence; Software engineering",Conference Paper,Scopus,2-s2.0-84866041303
"Su Z.-G., Wang P.-H., Shen J., Li Y.-G., Zhang Y.-F., Hu E.-J.","Automatic fuzzy partitioning approach using Variable string length Artificial Bee Colony (VABC) algorithm",2012,"Applied Soft Computing Journal",14,10.1016/j.asoc.2012.06.019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865847207&doi=10.1016%2fj.asoc.2012.06.019&partnerID=40&md5=3f9720c6ef2cbef8162cab3aec6c6fda","Swarm intelligence based automatic fuzzy clustering is recently an important and interesting unsupervised learning problem. In this article, an automatic fuzzy clustering technique is proposed based on a novel version of Artificial Bee Colony (ABC) algorithm. The idea of variable length genotypes is introduced to the ABC, and a novel version of ABC, called Variable string length Artificial Bee Colony (VABC) algorithm, is proposed. The VABC algorithm is derived from the ABC by redefining or modifying some operations in the ABC: the fixed length strings are represented by using variable length strings, the scheme for producing candidate solutions is modified, and some mutation operations are introduced. Use of VABC allows the encoding of variable number of clusters. This makes the VABC based Fuzzy C-Means clustering technique (VABC-FCM) not require a priori specification of the number of clusters. Moreover, the VABC-FCM has powerful global search ability under rational parameter setting. Some artificial data sets and real-life data sets are applied to validate the performance of VABC-FCM. The experimental results show that VABC-FCM can automatically evolve the optimal number of clusters and find proper fuzzy partitioning for these data sets when a rational validity index is adopted. Finally, the performance of VABC-FCM is compared with those of the Variable string length Genetic Algorithm based Fuzzy C-Means clustering (VGA-FCM), Particle Swarm Optimization algorithm based Fuzzy C-Means clustering (PSO-FCM), and Differential Evolutional algorithm based Fuzzy C-Means clustering (DE-FCM). The results show that the VABC-FCM outperforms VGA-FCM, PSO-FCM and DE-FCM in most of the cases. © 2012 Elsevier B.V. All rights reserved.","Artificial Bee Colony; Automatic clustering; Fuzzy C-Means; Genetic Algorithm; Swarm intelligence; Variable string length Artificial Bee Colony","Artificial bee colonies; Artificial data; Automatic clustering; Candidate solution; Data sets; Evolutional algorithm; Fuzzy C mean; Fuzzy C means clustering; Fuzzy clustering techniques; Fuzzy partitioning; Global search ability; Mutation operations; Number of clusters; Optimal number; Parameter setting; Particle swarm optimization algorithm; Real life datasets; Swarm Intelligence; Validity index; Variable length; Variable length genotypes; Variable number of clusters; Variable string length; Artificial intelligence; Differential amplifiers; Fuzzy clustering; Fuzzy systems; Genetic algorithms; Particle swarm optimization (PSO); Clustering algorithms",Article,Scopus,2-s2.0-84865847207
"Da Rosa M.A., Leite Da Silva A.M., Miranda V.","Multi-agent systems applied to reliability assessment of power systems",2012,"International Journal of Electrical Power and Energy Systems",14,10.1016/j.ijepes.2012.03.048,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860994952&doi=10.1016%2fj.ijepes.2012.03.048&partnerID=40&md5=4737e8dab7f23d68b92ca215cc9d070f","This paper discusses the development of a Multi-Agent Systems (MAS) technology-based platform with potential applications in management and simulation processes in power systems. In order to explore some of the features of MAS, a new methodology is proposed to assess power systems reliability based on Monte Carlo simulation (MCS), exploiting the benefits of the distributed artificial intelligence area and, mainly, the use of the distributed capacity in two ways: building autonomous behaviors to the applications and mitigating computational effort. Through the use of this technology, it was possible to divide the MCS algorithm into distinct tasks and submit them to the agents' processing. Two different approaches to solve generating capacity reliability problems based on chronological MCS illustrate the potential of MAS in power systems reliability assessment. © 2012 Elsevier Ltd. All rights reserved.","Distributed systems; Monte Carlo simulation; Multi-agent systems; Power system reliability; Reliability","Autonomous behaviors; Computational effort; Distributed Artificial Intelligence; Distributed systems; Generating capacity; Monte Carlo Simulation; Multi agent system (MAS); Potential applications; Power system reliability; Power systems reliability; Reliability assessments; Simulation process; Technology-based; Multi agent systems; Reliability; Reliability analysis; Monte Carlo methods",Article,Scopus,2-s2.0-84860994952
"Bergmann K., Eyssel F., Kopp S.","A second chance to make a first impression? how appearance and nonverbal behavior affect perceived warmth and competence of virtual agents over time",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,10.1007/978-3-642-33197-8-13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867544244&doi=10.1007%2f978-3-642-33197-8-13&partnerID=40&md5=568ba03469e73bc085881c51570b201a","First impressions of others are fundamental for the further development of a relationship and are thus of major importance for the design of virtual agents, too. We addressed the question whether there is a second chance for first impressions with regard to the major dimensions of social cognition-warmth and competence. We employed a novel experimental set-up that combined agent appearance (robot-like vs. human-like) and agent behavior (gestures present vs. absent) of virtual agents as between-subject factors with a repeated measures design. Results indicate that ratings of warmth depend on interaction effects of time and agent appearance, while evaluations of competence seem to depend on the interaction of time and nonverbal behavior. Implications of these results for basic and applied research on intelligent virtual agents will be discussed. © 2012 Springer-Verlag Berlin Heidelberg.","agent appearance; competence; Evaluation; nonverbal behavior; warmth","Agent appearance; competence; Evaluation; Nonverbal behavior; warmth; Artificial intelligence; Intelligent virtual agents",Conference Paper,Scopus,2-s2.0-84867544244
"Casanova R., Hsu F.-C., Espeland M.A.","Classification of Structural MRI Images in Alzheimer's Disease from the Perspective of Ill-Posed Problems",2012,"PLoS ONE",14,10.1371/journal.pone.0044877,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867379844&doi=10.1371%2fjournal.pone.0044877&partnerID=40&md5=f039d6172a22ddbc4d3501fc634911cd","Background: Machine learning neuroimaging researchers have often relied on regularization techniques when classifying MRI images. Although these were originally introduced to deal with ""ill-posed"" problems it is rare to find studies that evaluate the ill-posedness of MRI image classification problems. In addition, to avoid the effects of the ""curse of dimensionality"" very often dimension reduction is applied to the data. Methodology: Baseline structural MRI data from cognitively normal and Alzheimer's disease (AD) patients from the AD Neuroimaging Initiative database were used in this study. We evaluated here the ill-posedness of this classification problem across different dimensions and sample sizes and its relationship to the performance of regularized logistic regression (RLR), linear support vector machine (SVM) and linear regression classifier (LRC). In addition, these methods were compared with their principal components space counterparts. Principal Findings: In voxel space the prediction performance of all methods increased as sample sizes increased. They were not only relatively robust to the increase of dimension, but they often showed improvements in accuracy. We linked this behavior to improvements in conditioning of the linear kernels matrices. In general the RLR and SVM performed similarly. Surprisingly, the LRC was often very competitive when the linear kernel matrices were best conditioned. Finally, when comparing these methods in voxel and principal component spaces, we did not find large differences in prediction performance. Conclusions and Significance: We analyzed the problem of classifying AD MRI images from the perspective of linear ill-posed problems. We demonstrate empirically the impact of the linear kernel matrix conditioning on different classifiers' performance. This dependence is characterized across sample sizes and dimensions. In this context we also show that increased dimensionality does not necessarily degrade performance of machine learning methods. In general, this depends on the nature of the problem and the type of machine learning method. © 2012 Casanova et al.",,"aged; Alzheimer disease; article; cognition; controlled study; female; human; intermethod comparison; linear regression analysis; logistic regression analysis; major clinical study; male; mathematical computing; mild cognitive impairment; nuclear magnetic resonance imaging; principal component analysis; sample size; support vector machine; Aged; Aged, 80 and over; Algorithms; Alzheimer Disease; Artificial Intelligence; Databases, Factual; Female; Humans; Linear Models; Logistic Models; Magnetic Resonance Imaging; Male; Neuroimaging; Principal Component Analysis; Support Vector Machines",Article,Scopus,2-s2.0-84867379844
"Ben-David S., Loker D., Srebro N., Sridharan K.","Minimizing the misclassification error rate using a surrogate convex loss",2012,"Proceedings of the 29th International Conference on Machine Learning, ICML 2012",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867129520&partnerID=40&md5=afd0a0c7f4c6819b65584bafe765772c","We carefully study how well minimizing convex surrogate loss functions corresponds to minimizing the misclassification error rate for the problem of binary classification with linear predictors. We consider the agnostic setting, and investigate guarantees on the misclassification error of the loss-minimizer in terms of the margin error rate of the best predictor. We show that, aiming for such a guarantee, the hinge loss is essentially optimal among all convex losses. Copyright 2012 by the author(s)/owner(s).",,"Agnostic setting; Binary classification; Error rate; Linear predictors; Loss functions; Misclassification error; Artificial intelligence; Software engineering; Learning systems",Conference Paper,Scopus,2-s2.0-84867129520
"Giordano L., Gliozzi V., Olivetti N., Pozzato G.L.","A minimal model semantics for nonmonotonic reasoning",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,10.1007/978-3-642-33353-8_18,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866938356&doi=10.1007%2f978-3-642-33353-8_18&partnerID=40&md5=b1b70d17f9787560040dc5dd5f8e41c7","This paper provides a general semantic framework for nonmonotonic reasoning, based on a minimal models semantics on the top of KLM systems for nonmonotonic reasoning. This general framework can be instantiated in order to provide a semantic reconstruction within modal logic of the notion of rational closure, introduced by Lehmann and Magidor. We give two characterizations of rational closure: the first one in terms of minimal models where propositional interpretations associated to worlds are fixed along minimization, the second one where they are allowed to vary. In both cases a knowledge base must be expanded with a suitable set of consistency assumptions, represented by negated conditionals. The correspondence between rational closure and minimal model semantics suggests the possibility of defining variants of rational closure by changing either the underlying modal logic or the comparison relation on models. © 2012 Springer-Verlag.",,"Knowledge base; Minimal model; Minimal model semantics; Modal logic; Non-monotonic reasoning; Semantic framework; Artificial intelligence; Formal logic; Knowledge based systems; Semantics",Conference Paper,Scopus,2-s2.0-84866938356
"Koch-Ciobotaru C., Mihet-Popa L., Isleifsson F., Bindner H.","Simulation model developed for a small-scale PV system in distribution networks",2012,"SACI 2012 - 7th IEEE International Symposium on Applied Computational Intelligence and Informatics, Proceedings",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866783902&partnerID=40&md5=136f1a27b22a63875652b8c9d2958157","This paper presents a PV panel simulation model using the single-diode four-parameter model based on data sheet values. The model was implemented first in MATLAB/Simulink, and the results have been compared with the data sheet values and characteristics of the PV panels in standard test conditions. Moreover to point out the strong dependency on ambient conditions and its influence on array operation and to validate simulation results with measured data a complex model has also been developed. A PV inverter model, using the same equations and parameters as in MATLAB/Simulink has also been developed and implemented in PowerFactory to study load flow, steady-state voltage stability and dynamic behavior of a distributed power system. ©2012 IEEE.",,"Ambient conditions; Complex model; Data-sheet values; Distributed power systems; Dynamic behaviors; Four-parameter model; Load flow; MATLAB /simulink; PV inverter; PV panel; PV system; Simulation model; Standard test condition (STC); Steady state voltage; Artificial intelligence; Information science; Photovoltaic cells; Computer simulation",Conference Paper,Scopus,2-s2.0-84866783902
"Várkonyi T.A., Tar J.K., Rudas I.J., Krómer I.","VS-type stabilization of MRAC controllers using robust fixed point transformations",2012,"SACI 2012 - 7th IEEE International Symposium on Applied Computational Intelligence and Informatics, Proceedings",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866784609&partnerID=40&md5=0f30edcd636c8e394a9746d68f3ecc84","Nowadays, control of dynamical systems with uncertainties is a common problem. Many sulutions can be found in the literature, one of these methods is the family of Robust Fixed. Point Transformations (RFPT) with local basin of attraction. The method is based on the idea that if someone has to use an approximate model in a control task, there is a function which, locally converging to the right solution, can reduce the disadvantages of the approximation. In this paper, authors show that though RFPT can loose its local convergensity, it can still improve a simple controller's results and this improvement makes the controller's behavior very similar to that of a sliding mode controller. The similarity includes the so called chattering effect, but a simple smoothing algorithm is also introuced to minimize the fluctuation of the control signal. ©2012 IEEE.",,"Approximate model; Basin of attraction; Chattering effects; Control signal; Control task; Fixed points; IMPROVE-A; Point transformations; Sliding mode controller; Smoothing algorithms; Artificial intelligence; Dynamical systems; Information science; Controllers",Conference Paper,Scopus,2-s2.0-84866784609
"Wild S., Nebel M.E.","Average case analysis of Java 7's dual pivot quicksort",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,10.1007/978-3-642-33090-2_71,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866674034&doi=10.1007%2f978-3-642-33090-2_71&partnerID=40&md5=90bb3662bbbe7d945d9a04a67e066bb6","Recently, a new Quicksort variant due to Yaroslavskiy was chosen as standard sorting method for Oracle's Java 7 runtime library. The decision for the change was based on empirical studies showing that on average, the new algorithm is faster than the formerly used classic Quicksort. Surprisingly, the improvement was achieved by using a dual pivot approach, an idea that was considered not promising by several theoretical studies in the past. In this paper, we identify the reason for this unexpected success. Moreover, we present the first precise average case analysis of the new algorithm showing e.g. that a random permutation of length n is sorted using 1.9n ln n - 2.46n + O(ln n) key comparisons and 0.6n ln n + 0.08n + O(ln n) swaps. © 2012 Springer-Verlag.",,"Average-case analysis; Empirical studies; Key comparison; Quicksort; Random permutations; Run-time library; Sorting method; Theoretical study; Artificial intelligence; Algorithms",Conference Paper,Scopus,2-s2.0-84866674034
"Trillas E.","A model for ""crisp reasoning"" with fuzzy sets",2012,"International Journal of Intelligent Systems",14,10.1002/int.21551,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865459842&doi=10.1002%2fint.21551&partnerID=40&md5=7d5a11505c0b2d834e27c39c6deff0f1","In the setting of a general type of fuzzy algebras, this paper deals with a new theoretic view on the commonsense reasoning, consisting of a kind of Popper's search for conjectures and refutations. It is supposed that the reasoning is done in natural language, but only with nonambiguous precise and imprecise terms, respectively, represented by crisp and fuzzy sets. © 2012 Wiley Periodicals, Inc.",,"Commonsense reasoning; Fuzzy algebra; Natural languages; Artificial intelligence; Software engineering; Fuzzy sets",Article,Scopus,2-s2.0-84865459842
"Karasuyama M., Sugiyama M.","Canonical dependency analysis based on squared-loss mutual information",2012,"Neural Networks",14,10.1016/j.neunet.2012.06.009,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865419682&doi=10.1016%2fj.neunet.2012.06.009&partnerID=40&md5=9a80e43865b71a89b9aa118fcdf5eda1","Canonical correlation analysis (CCA) is a classical dimensionality reduction technique for two sets of variables that iteratively finds projection directions with maximum correlation. Although CCA is still in vital use in many practical application areas, recent real-world data often contain more complicated nonlinear correlations that cannot be properly captured by classical CCA. In this paper, we thus propose an extension of CCA that can effectively capture such complicated nonlinear correlations through statistical dependency maximization. The proposed method, which we call least-squares canonical dependency analysis (LSCDA), is based on a squared-loss variant of mutual information, and it has various useful properties besides its ability to capture higher-order correlations: for example, it can simultaneously find multiple projection directions (i.e., subspaces), it does not involve density estimation, and it is equipped with a model selection strategy. We demonstrate the usefulness of LSCDA through various experiments on artificial and real-world datasets. © 2012 Elsevier Ltd.","Canonical correlation analysis; Direct density-ratio estimation; Squared-loss mutual information","Application area; Canonical correlation analysis; Density estimation; Dependency analysis; Dimensionality reduction techniques; Higher order correlation; Least Square; Maximum correlations; Model Selection; Multiple projections; Mutual informations; Non-linear correlations; Projection direction; Real world data; Real-world datasets; Statistical dependencies; Useful properties; Artificial intelligence; Cognitive systems; Least squares approximations; article; artificial neural network; controlled study; correlation analysis; information; information processing; intermethod comparison; least squares canonical dependency analysis; priority journal; squared loss mutual information; statistical analysis; Algorithms; Artificial Intelligence; Databases, Factual; Least-Squares Analysis",Article,Scopus,2-s2.0-84865419682
"Rasheed Z., Rangwala H.","Metagenomic taxonomic classification using extreme learning machines",2012,"Journal of Bioinformatics and Computational Biology",14,10.1142/S0219720012500151,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864440647&doi=10.1142%2fS0219720012500151&partnerID=40&md5=79861f54e2e14bb6704a916f272b8b4c","Next-generation sequencing technologies have allowed researchers to determine the collective genomes of microbial communities co-existing within diverse ecological environments. Varying species abundance, length and complexities within different communities, coupled with discovery of new species makes the problem of taxonomic assignment to short DNA sequence reads extremely challenging. We have developed a new sequence composition-based taxonomic classifier using extreme learning machines referred to as TAC-ELM for metagenomic analysis. TAC-ELM uses the framework of extreme learning machines to quickly and accurately learn the weights for a neural network model. The input features consist of GC content and oligonucleotides. TAC-ELM is evaluated on two metagenomic benchmarks with sequence read lengths reflecting the traditional and current sequencing technologies. Our empirical results indicate the strength of the developed approach, which outperforms state-of-the-art taxonomic classifiers in terms of accuracy and implementation complexity. We also perform experiments that evaluate the pervasive case within metagenome analysis, where a species may not have been previously sequenced or discovered and will not exist in the reference genome databases. TAC-ELM was also combined with BLAST to show improved classification results. Code and Supplementary Results: http://www.cs.gmu.edu/mlbio/TAC-ELM (BSD License). © 2012 Imperial College Press.","ELM; metagenomics; phylogenetic classification","algorithm; article; artificial intelligence; artificial neural network; DNA sequence; metagenome; metagenomics; methodology; nucleotide sequence; phylogeny; Algorithms; Artificial Intelligence; Base Sequence; Metagenome; Metagenomics; Neural Networks (Computer); Phylogeny; Sequence Analysis, DNA",Article,Scopus,2-s2.0-84864440647
"Yongchareon S., Liu C., Zhao X.","A framework for behavior-consistent specialization of artifact-centric business processes",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,10.1007/978-3-642-32885-5_23,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866377181&doi=10.1007%2f978-3-642-32885-5_23&partnerID=40&md5=edff39cc43e0050693166b95d8e34e4c","Driven by complex and dynamic business process requirements, there has been an increasing demand for business process reuse to improve modeling efficiency. Process specialization is an effective reuse method that can be used to customize and extend base process models to specialized models. In the recent years, artifact-centric business process modeling has emerged as it supports a more flexible process structure compared with traditional activity-centric process models. Although, process specialization has been studied for the traditional models by treating a process as a single object, the specialization of artifact-centric processes that consist of multiple interacting artifacts has not been studied. Inheriting interactions among artifacts for specialized processes and ensuring the consistency of the processes are challenging. To address these issues, we propose a novel framework for process specialization comprising artifact-centric process models, methods to define a specialized process model based on an existing process model, and the behavior consistency between the specialized model and its base model. © 2012 Springer-Verlag.",,"Artifact-centric; Base models; Behavior consistency; Business Process; Business process modeling; Dynamic business; Flexible process; Process model; Single object; Artificial intelligence; Enterprise resource management",Conference Paper,Scopus,2-s2.0-84866377181
"Santaniello S., Sherman D.L., Thakor N.V., Eskandar E.N., Sarma S.V.","Optimal control-based bayesian detection of clinical and behavioral state transitions",2012,"IEEE Transactions on Neural Systems and Rehabilitation Engineering",14,10.1109/TNSRE.2012.2210246,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866079024&doi=10.1109%2fTNSRE.2012.2210246&partnerID=40&md5=3bb31f2b78b732a8b2080ae03f5ad0d9","Accurately detecting hidden clinical or behavioral states from sequential measurements is an emerging topic in neuroscience and medicine, which may dramatically impact neural prosthetics, brain-computer interface and drug delivery. For example, early detection of an epileptic seizure from sequential electroencephalographic (EEG) measurements would allow timely administration of anticonvulsant drugs or neurostimulation, thus reducing physical impairment and risks of overtreatment. We develop a Bayesian paradigm for state transition detection that combines optimal control and Markov processes. We define a hidden Markov model of the state evolution and develop a detection policy that minimizes a loss function of both probability of false positives and accuracy (i.e., lag between estimated and actual transition time). Our strategy automatically adapts to each newly acquired measurement based on the state evolution model and the relative loss for false positives and accuracy, thus resulting in a time varying threshold policy. The paradigm was used in two applications: 1) detection of movement onset (behavioral state) from subthalamic single unit recordings in Parkinson's disease patients performing a motor task; 2) early detection of an approaching seizure (clinical state) from multichannel intracranial EEG recordings in rodents treated with pentylenetetrazol chemoconvulsant. Our paradigm performs significantly better than chance and improves over widely used detection algorithms. © 2011 IEEE.","Bayesian estimation; neural systems; optimal control; quickest detection (QD)","Bayesian detection; Bayesian estimations; Bayesian paradigm; Behavioral state; Clinical state; Detection algorithm; EEG recording; Emerging topics; Epileptic seizures; False positive; Loss functions; Measurement-based; Motor tasks; Multi-channel; Neural prosthetic; Neural systems; Neurostimulation; Optimal controls; Parkinson's disease; Physical impairments; Probability of false positives; Quickest detection; Single unit recordings; State evolutions; State transitions; Threshold policies; Time varying; Transition time; Bayesian networks; Drug delivery; Hidden Markov models; Mammals; Neurodegenerative diseases; Patient treatment; Control; aged; algorithm; article; artificial intelligence; automated pattern recognition; Bayes theorem; behavior; brain; evoked muscle response; female; human; male; methodology; middle aged; physiology; reproducibility; sensitivity and specificity; Aged; Algorithms; Artificial Intelligence; Bayes Theorem; Behavior; Brain; Evoked Potentials, Motor; Female; Humans; Male; Middle Aged; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84866079024
"Kao Y., Chen M.-H., Huang Y.-T.","A hybrid algorithm based on ACO and PSO for capacitated vehicle routing problems",2012,"Mathematical Problems in Engineering",14,10.1155/2012/726564,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866095393&doi=10.1155%2f2012%2f726564&partnerID=40&md5=1f2a8c8741dbce7eb69fb3a3d6c7462d","The vehicle routing problem (VRP) is a well-known combinatorial optimization problem. It has been studied for several decades because finding effective vehicle routes is an important issue of logistic management. This paper proposes a new hybrid algorithm based on two main swarm intelligence (SI) approaches, ant colony optimization (ACO) and particle swarm optimization (PSO), for solving capacitated vehicle routing problems (CVRPs). In the proposed algorithm, each artificial ant, like a particle in PSO, is allowed to memorize the best solution ever found. After solution construction, only elite ants can update pheromone according to their own best-so-far solutions. Moreover, a pheromone disturbance method is embedded into the ACO framework to overcome the problem of pheromone stagnation. Two sets of benchmark problems were selected to test the performance of the proposed algorithm. The computational results show that the proposed algorithm performs well in comparison with existing swarm intelligence approaches. © 2012 Yucheng Kao et al.",,"Ant Colony Optimization (ACO); Artificial ant; Bench-mark problems; Capacitated vehicle routing problem; Combinatorial optimization problems; Computational results; Hybrid algorithms; Logistic management; Swarm Intelligence; Vehicle routing problem; Artificial intelligence; Benchmarking; Combinatorial optimization; Network routing; Particle swarm optimization (PSO); Routing algorithms; Algorithms",Article,Scopus,2-s2.0-84866095393
"Lin H., Pass R.","Black-box constructions of composable protocols without set-up",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,10.1007/978-3-642-32009-5_27,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865525618&doi=10.1007%2f978-3-642-32009-5_27&partnerID=40&md5=3f0a29247a75d65972f989ef5d7f6b2b","We present the first black-box construction of a secure multi-party computation protocol that satisfies a meaningful notion of concurrent security in the plain model (without any set-up, and without assuming an honest majority). Moreover, our protocol relies on the minimal assumption of the existence of a semi-honest OT protocol, and our security notion ""UC with super-polynomial helpers"" (Canetti et al, STOC'10) is closed under universal composition, and implies super-polynomial-time simulation security. © 2012 International Association for Cryptologic Research.",,"Black boxes; Secure multi-party computation; Security notion; Artificial intelligence; Cryptography",Conference Paper,Scopus,2-s2.0-84865525618
"Xu Y., Tsujii J., Chang E.I.-C.","Named entity recognition of follow-up and time information in 20 000 radiology reports",2012,"Journal of the American Medical Informatics Association",14,10.1136/amiajnl-2012-000812,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872259831&doi=10.1136%2famiajnl-2012-000812&partnerID=40&md5=3623fb39ec84edeb3162bf9726b24c29","Objective To develop a system to extract follow-up information from radiology reports. The method may be used as a component in a system which automatically generates follow-up information in a timely fashion. Methods A novel method of combining an LSP (labeled sequential pattern) classifier with a CRF (conditional random field) recognizer was devised. The LSP classifier filters out irrelevant sentences, while the CRF recognizer extracts follow-up and time phrases from candidate sentences presented by the LSP classifier. Measurements The standard performance metrics of precision (P), recall (R), and F measure (F) in the exact and inexact matching settings were used for evaluation. Results Four experiments conducted using 20 000 radiology reports showed that the CRF recognizer achieved high performance without time-consuming feature engineering and that the LSP classifier further improved the performance of the CRF recognizer. The performance of the current system is P=0.90, R=0.86, F=0.88 in the exact matching setting and P=0.98, R=0.93, F=0.95 in the inexact matching setting. Conclusion The experiments demonstrate that the system performs far better than a baseline rule-based system and is worth considering for deployment trials in an alert generation system. The LSP classifier successfully compensated for the inherent weakness of CRF, that is, its inability to use global information.",,"article; effect size; follow up; human; machine learning; medical information; medical record; radiology; artificial intelligence; classification; data mining; electronic medical record; feasibility study; hospital information system; methodology; natural language processing; Artificial Intelligence; Data Mining; Electronic Health Records; Feasibility Studies; Humans; Natural Language Processing; Radiology Information Systems",Article,Scopus,2-s2.0-84872259831
"Baldi P., Lu Z.","Complex-valued autoencoders",2012,"Neural Networks",14,10.1016/j.neunet.2012.04.011,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863883286&doi=10.1016%2fj.neunet.2012.04.011&partnerID=40&md5=32d32126653849621c1f8b65a2931043","Autoencoders are unsupervised machine learning circuits, with typically one hidden layer, whose learning goal is to minimize an average distortion measure between inputs and outputs. Linear autoencoders correspond to the special case where only linear transformations between visible and hidden variables are used. While linear autoencoders can be defined over any field, only real-valued linear autoencoders have been studied so far. Here we study complex-valued linear autoencoders where the components of the training vectors and adjustable matrices are defined over the complex field with the L2 norm. We provide simpler and more general proofs that unify the real-valued and complex-valued cases, showing that in both cases the landscape of the error function is invariant under certain groups of transformations. The landscape has no local minima, a family of global minima associated with Principal Component Analysis, and many families of saddle points associated with orthogonal projections onto sub-space spanned by sub-optimal subsets of eigenvectors of the covariance matrix. The theory yields several iterative, convergent, learning algorithms, a clear understanding of the generalization properties of the trained autoencoders, and can equally be applied to the hetero-associative case when external targets are provided. Partial results on deep architecture as well as the differential geometry of autoencoders are also presented. The general framework described here is useful to classify autoencoders and identify general properties that ought to be investigated for each class, illuminating some of the connections between autoencoders, unsupervised learning, clustering, Hebbian learning, and information theory. © 2012 Elsevier Ltd.","Autoencoders; Complex neural networks; Complex numbers; Critical points; Deep architectures; Differential geometry; EM algorithm; Linear networks; Principal component analysis; Unsupervised learning","Autoencoders; Complex neural networks; Complex number; Critical points; Differential geometry; EM algorithms; Covariance matrix; Information theory; Linear networks; Neural networks; Principal component analysis; Unsupervised learning; Learning algorithms; analytical error; article; autoencoder; automation; classification; cluster analysis; covariance; geometry; information; learning algorithm; linear system; machine learning; mathematical computing; mathematical parameters; principal component analysis; priority journal; sampling; theory; Artificial Intelligence; Neural Networks (Computer); Principal Component Analysis",Article,Scopus,2-s2.0-84863883286
"Ruffalo M., Koyutürk M., Ray S., LaFramboise T.","Accurate estimation of short read mapping quality for next-generation genome sequencing",2012,"Bioinformatics",14,10.1093/bioinformatics/bts408,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866461485&doi=10.1093%2fbioinformatics%2fbts408&partnerID=40&md5=a9d26a8f38638ffd21975010911029e2","Motivation: Several software tools specialize in the alignment of short next-generation sequencing reads to a reference sequence. Some of these tools report a mapping quality score for each alignment-in principle, this quality score tells researchers the likelihood that the alignment is correct. However, the reported mapping quality often correlates weakly with actual accuracy and the qualities of many mappings are underestimated, encouraging the researchers to discard correct mappings. Further, these low-quality mappings tend to correlate with variations in the genome (both single nucleotide and structural), and such mappings are important in accurately identifying genomic variants. Approach: We develop a machine learning tool, LoQuM (LOgistic regression tool for calibrating the Quality of short read mappings, to assign reliable mapping quality scores to mappings of Illumina reads returned by any alignment tool. LoQuM uses statistics on the read (base quality scores reported by the sequencer) and the alignment (number of matches, mismatches and deletions, mapping quality score returned by the alignment tool, if available, and number of mappings) as features for classification and uses simulated reads to learn a logistic regression model that relates these features to actual mapping quality. Results: We test the predictions of LoQuM on an independent dataset generated by the ART short read simulation software and observe that LoQuM can 'resurrect' many mappings that are assigned zero quality scores by the alignment tools and are therefore likely to be discarded by researchers. We also observe that the recalibration of mapping quality scores greatly enhances the precision of called single nucleotide polymorphisms. © The Author(s) 2012. Published by Oxford University Press.",,"article; artificial intelligence; chromosome map; computer program; DNA sequence; genomics; high throughput sequencing; human; human genome; methodology; sequence alignment; single nucleotide polymorphism; statistical model; Artificial Intelligence; Chromosome Mapping; Genome, Human; Genomics; High-Throughput Nucleotide Sequencing; Humans; Logistic Models; Polymorphism, Single Nucleotide; Sequence Alignment; Sequence Analysis, DNA; Software",Article,Scopus,2-s2.0-84866461485
"Larrodé E., Moreno-Jiménez J.M., Muerza M.V.","An AHP-multicriteria suitability evaluation of technological diversification in the automotive industry",2012,"International Journal of Production Research",14,10.1080/00207543.2012.657975,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865682883&doi=10.1080%2f00207543.2012.657975&partnerID=40&md5=c051789ace7eb5ec8945e35944063933","This paper advances a methodological framework for analysing the process of technological diversification in the automotive industry and presents a multicriteria procedure, based on the analytic hierarchy process (AHP), to evaluate, in a simple and user-friendly manner, the suitability of technological diversification for any company in the automotive sector. The AHP-multicriteria procedure associated with the initial stage of the methodology and the decision support systems developed to evaluate suitability have been applied to 22 Spanish automotive companies. The multicriteria procedure classifies the companies into three different groups: (i) those that are suitable for diversification; (ii) those for which product development (innovation) or market development (internationalisation) is more appropriate; (iii) those that are not suitable for any diversification process. The true value added in this initial stage of the technological diversification process is not, despite its importance, the final classification of the firms, but the learning procedure derived from the multicriteria analysis and the evaluation of diversification suitability. From this learning procedure, a set of recommendations related to the diversification of the automotive companies is extracted. © 2012 Copyright Taylor and Francis Group, LLC.","AHP; automotive industry; knowledge extraction; multicriteria suitability evaluation; technological diversification process","AHP; Automotive companies; Automotive sector; Initial stages; Internationalisation; Knowledge extraction; Learning procedures; Market development; Methodological frameworks; Multi Criteria Analysis; Multi-criteria; Suitability evaluation; Technological diversification; Artificial intelligence; Decision support systems; Hierarchical systems; Product development; Automotive industry",Article,Scopus,2-s2.0-84865682883
"Khare V.R., Chougule R.","Decision support for improved service effectiveness using domain aware text mining",2012,"Knowledge-Based Systems",14,10.1016/j.knosys.2012.03.005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861650931&doi=10.1016%2fj.knosys.2012.03.005&partnerID=40&md5=0c3cf22ffc5dfacc21d055d4a6cc1c6b","This paper presents a decision support system 'Domain Aware Text & Association Mining (DATAM)' which has been developed to improve after-sales service and repairs for the automotive domain. A novel approach that compares textual and non-textual data for anomaly detection is proposed. It combines association and ontology based text mining. Association mining has been employed to identify the repairs performed in the field for a given symptom, whereas, text mining is used to infer repairs from the textual instructions mentioned in service documents for the same symptom. These in turn are compared and contrasted to identify the anomalous cases. The developed approach has been applied to automotive field data. Using the top 20 most frequent symptoms, observed in a mid-sized sedan built and sold in North America, it is demonstrated that DATAM can identify all the anomalous symptom - repair code combinations (with a false positive rate of 0.04). This knowledge, in the form of anomalies, can subsequently be used to improve the service/trouble-shooting procedure and identify technician training needs. © 2012 Elsevier B.V. All rights reserved.","Anomaly detection; Association mining; Decision support systems; Semantic text analysis; Text mining","After-sales services; Anomaly detection; Association mining; Automotive domains; Code combination; Decision supports; False positive rates; Field data; Ontology-based; Text analysis; Text mining; Artificial intelligence; Decision support systems; Repair; Semantics; Data mining",Article,Scopus,2-s2.0-84861650931
"Er M.J., Duda P.","On the weak convergence of the orthogonal series-type kernel regresion neural networks in a non-stationary environment",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,10.1007/978-3-642-31464-3_45,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865268269&doi=10.1007%2f978-3-642-31464-3_45&partnerID=40&md5=7a0d0b57af3a0471d6d1e441558491c5","In the paper general regression neural networks, based on the orthogonal series-type kernel, is studied. Convergence in probability is proved assuming non-stationary noise. The performance is investigated using syntetic data. © 2012 Springer-Verlag.",,"Convergence in probability; General regression neural network; Non-stationary environment; Nonstationary noise; Series-type; Weak convergence; Artificial intelligence; Fourier analysis",Conference Paper,Scopus,2-s2.0-84865268269
"Chen W., Deelman E.","Partitioning and scheduling workflows across multiple sites with storage constraints",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,10.1007/978-3-642-31500-8_2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865218020&doi=10.1007%2f978-3-642-31500-8_2&partnerID=40&md5=deb1e811df2fcfbd17f160c58deaa0cb","This paper aims to address the problem of scheduling large workflows onto multiple execution sites with storage constraints. Three heuristics are proposed to first partition the workflow into sub-workflows. Three estimators and two schedulers are then used to schedule sub-workflows to the execution sites. Performance with three real-world workflows shows that this approach is able to satisfy storage constraints and improve the overall runtime by up to 48% over a default whole-workflow scheduling. © 2012 Springer-Verlag.","partitioning; storage constraints; workflow scheduling","partitioning; Runtimes; Storage constraints; Work-flows; Workflow scheduling; Artificial intelligence; Scheduling",Conference Paper,Scopus,2-s2.0-84865218020
"Picone M., Amoretti M., Zanichelli F.","A decentralized smartphone based traffic information system",2012,"IEEE Intelligent Vehicles Symposium, Proceedings",14,10.1109/IVS.2012.6232209,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865016401&doi=10.1109%2fIVS.2012.6232209&partnerID=40&md5=a548348b70f7e4518db0ed10ba0672cd","Location-Based Services (LBSs) are information or entertainment services where the request, the response and served contents depend on the physical position of the requesting device. LBS are frequently used to implement Traffic Information Systems (TIS), which are increasingly based on user-contributed information. In this paper we present the first prototype of our solution for a decentralized, smartphone-based TIS, called D4V, that allows each participant vehicle to efficiently discover data or services located near any chosen geographic position. The experimental evaluation has shown that D4V could be effectively used on the road to reduce the number of drivers involved in traffic jams, as well as to disseminate alert messages about potentially dangerous road stretches, thus allowing drivers to reduce risks and nuisances along their paths. © 2012 IEEE.",,"Entertainment services; Experimental evaluation; Traffic information systems; Traffic jams; Artificial intelligence; Emergency traffic control; Highway traffic control; Intelligent vehicle highway systems; Location based services; Roads and streets; Smartphones; Traffic congestion; Information systems",Conference Paper,Scopus,2-s2.0-84865016401
"Michalewicz Z.","Quo Vadis, evolutionary computation? On a growing gap between theory and practice",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,10.1007/978-3-642-30687-7_6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864995777&doi=10.1007%2f978-3-642-30687-7_6&partnerID=40&md5=7d6e31cd63c56fc6604a4ba794decb16","At the Workshop on Evolutionary Algorithms, organized by the Institute for Mathematics and Its Applications, University of Minnesota, Minneapolis, Minnesota, October 21 - 25, 1996, one of the invited speakers, Dave Davis made an interesting claim. As the most recognised practitioner of Evolutionary Algorithms at that time he said that all theoretical results in the area of Evolutionary Algorithms were of no use to him - actually, his claim was a bit stronger. He said that if a theoretical result indicated that, say, the best value of some parameter was such-and-such, he would never use the recommended value in any real-world implementation of an evolutionary algorithm! Clearly, there was - in his opinion - a significant gap between theory and practice of Evolutionary Algorithms. Fifteen years later, it is worthwhile revisiting this claim and to answer some questions; these include: What are the practical contributions coming from the theory of Evolutionary Algorithms? Did we manage to close the gap between the theory and practice? How do Evolutionary Algorithms compare with Operation Research methods in real-world applications? Why do so few papers on Evolutionary Algorithms describe real-world applications? For what type of problems are Evolutionary Algorithms ""the best"" method? In this article, I'll attempt to answer these questions - or at least to provide my personal perspective on these issues. © 2012 Springer-Verlag.",,"Best value; Minneapolis; Minnesota; Operation research; Personal perspective; Real-world application; Real-world implementation; Recommended values; Theoretical result; Theory and practice; University of Minnesota; Artificial intelligence; Evolutionary algorithms",Conference Paper,Scopus,2-s2.0-84864995777
"Mohammed N.F., Omar N.","Arabic named entity recognition using artificial neural network",2012,"Journal of Computer Science",14,10.3844/jcssp.2012.1285.1293,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864913972&doi=10.3844%2fjcssp.2012.1285.1293&partnerID=40&md5=9789c5f53588ca28ff9019b50dd29cc3","Problem statement: Named Entity Recognition (NER) is a task to identify proper names as well as temporal and numeric expressions, in an open-domain text. The NER task can help to improve the performance of various Natural Language Processing (NLP) applications such as Information Extraction (IE), Information Retrieval (IR) and Question Answering (QA) tasks. This study discusses on the Named Entity Recognition of Arabic (NERA). The motivation is due to the lack of resources for Arabic named entities and to enhance the accuracy that has been reached in previous NERA systems. Approach: This system is designed based on neural network approach. The main task of neural network approach is to automatically learn to recognize component patterns and make intelligent decisions based on available data and it can also be applied to classify new information within large databases. The use of machine learning approach to classify NER from Arabic text based on neural network technique is proposed. Neural network approach has performed successfully in many areas of artificial intelligence. The system involves three stages: the first stage is pre-processing that cleans the collected data, the second involves converting Arabic letters to Roman alphabets and the final stage applies neural network to classify the collected data. Results: The accuracy of the system is 92 %. The system is compared with decision tree using the same data. The results showed that the neural network approach achieved better than decision tree. Conclusion: These results prove that our technique is capable to recognize named entities of Arabic texts. © 2012 Science Publications.","Arabic; Arabic script; Artificial intelligence; Information extraction (IE); Named entity recognition; Natural language processing; Neural network approach; Question answering (QA)",,Article,Scopus,2-s2.0-84864913972
"Koh Y.W., Celik T., Lee H.K., Petznick A., Tong L.","Detection of meibomian glands and classification of meibography images",2012,"Journal of Biomedical Optics",14,10.1117/1.JBO.17.8.086008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874980876&doi=10.1117%2f1.JBO.17.8.086008&partnerID=40&md5=35641cadb37c72379a87a3bac0d388a2","Computational methods are presented that can automatically detect the length and width of meibomian glands imaged by infrared meibography without requiring any input from the user. The images are then automatically classified. The length of the glands are detected by first normalizing the pixel intensity, extracting stationary points, and then applying morphological operations. Gland widths are detected using scale invariant feature transform and analyzed using Shannon entropy. Features based on the gland lengths and widths are then used to train a linear classifier to accurately differentiate between healthy (specificity 96.1%) and unhealthy (sensitivity 97.9%) meibography images. The user-free computational method is fast, does not suffer from inter-observer variability, and can be useful in clinical studies where large number of images needs to be analyzed efficiently. © 2012 Society of Photo-Optical Instrumentation Engineers (SPIE).","Computer vision; Diagnosis; Dry-eye; Image processing; Machine learning; Meibography","adult; aged; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; dry eye; female; human; image enhancement; male; meibomian gland; methodology; middle aged; ophthalmoscopy; pathology; reproducibility; sensitivity and specificity; Adult; Aged; Algorithms; Artificial Intelligence; Dry Eye Syndromes; Female; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Male; Meibomian Glands; Middle Aged; Ophthalmoscopy; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Young Adult",Article,Scopus,2-s2.0-84874980876
"Pedrycz W., Russo B., Succi G.","Knowledge transfer in system modeling and its realization through an optimal allocation of information granularity",2012,"Applied Soft Computing Journal",14,10.1016/j.asoc.2012.02.004,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861858486&doi=10.1016%2fj.asoc.2012.02.004&partnerID=40&md5=507bea2ddb360000cb3d8a4bfaa21415","In this study, we introduce and discuss a concept of knowledge transfer in system modeling. In a nutshell, knowledge transfer is about forming ways on how a source of knowledge (namely, an existing model) can be used in presence of new, very limited experimental evidence. In virtue of the nature of the problem at hand (a situation encountered quite commonly, e.g. in project cost estimation), new data could be very limited and this scarcity of data makes it insufficient to construct a new model. At the same time, the new data originate from a similar (but not the same) phenomenon (process) for which the original model has been constructed so the existing model, even though it could applied, has to be treated with a certain level of reservation. Such situations can be encountered, e.g. in software engineering where in spite existing similarities, each project, process, or product exhibits its own unique characteristics. Taking this into consideration, the existing model is generalized (abstracted) by forming its granular counterpart - granular model where its parameters are regarded as information granules rather than numeric entities, viz. their non-numeric (granular) version is formed based on the values of the numeric parameters present in the original model. The results produced by the granular model are also granular and in this manner they become reflective of the differences existing between the current phenomenon and the process for which the previous model has been formed. In the study on knowledge transfer and reusability, information granularity is viewed as an important design asset and as such it is subject to optimization. We formulate an optimal information granularity allocation problem: assuming a certain level of granularity, distribute it optimally among the parameters of the model (making them granular) so that a certain data coverage criterion is maximized. While the underlying concept is general and applicable to a variety of models, in this study, we discuss its use to fuzzy neural networks with intent to clearly visualize the advantages of the approach and emphasize various ways of forming granular versions of the weights (parameters) of the connections of the network. Several granularity allocation protocols (ranging from a uniform distribution of granularity, symmetric and asymmetric schemes of allocation) are discussed and the effectiveness of each of them is quantified. The use of Particle Swarm Optimization (PSO) as the underlying optimization tool to realize optimal granularity allocation is discussed. © 2012 Elsevier B.V.","Computational intelligence; Granular model; Information granularity; Knowledge transfer and knowledge reusability; Optimal granularity allocation; Software cost estimation","Granular models; Information granularity; Knowledge reusability; Optimal granularity allocation; Software cost estimations; Artificial intelligence; Cost estimating; Fuzzy neural networks; Information granules; Knowledge management; Particle swarm optimization (PSO); Reusability; Software engineering; Particles (particulate matter)",Article,Scopus,2-s2.0-84861858486
"Biesdorf A., Rohr K., Feng D., von Tengg-Kobligk H., Rengier F., Böckler D., Kauczor H.-U., Wörz S.","Segmentation and quantification of the aortic arch using joint 3D model-based segmentation and elastic image registration",2012,"Medical Image Analysis",14,10.1016/j.media.2012.05.010,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866084589&doi=10.1016%2fj.media.2012.05.010&partnerID=40&md5=ca3963229eee532d90ddadee47959a78","Accurate quantification of the morphology of vessels is important for diagnosis and treatment of cardiovascular diseases. We introduce a new joint segmentation and registration approach for the quantification of the aortic arch morphology that combines 3D model-based segmentation with elastic image registration. With this combination, the approach benefits from the robustness of model-based segmentation and the accuracy of elastic registration. The approach can cope with a large spectrum of vessel shapes and particularly with pathological shapes that deviate significantly from the underlying model used for segmentation. The performance of the approach has been evaluated on the basis of 3D synthetic images, 3D phantom data, and clinical 3D CTA images including pathologies. We also performed a quantitative comparison with previous approaches. © 2012 Elsevier B.V.","Aortic arch; Elastic image registration; Model-based segmentation; Parametric intensity model; Vessel segmentation","Aortic arch; Elastic image registration; Intensity models; Model-based segmentation; Vessel segmentation; Arches; Blood vessels; Diseases; Image registration; Morphology; Three dimensional; accuracy; aorta arch; article; computed tomographic angiography; human; image analysis; morphology; phantom; priority journal; three dimensional imaging; Algorithms; Aorta, Thoracic; Aortic Diseases; Artificial Intelligence; Coronary Angiography; Humans; Imaging, Three-Dimensional; Pattern Recognition, Automated; Radiographic Image Enhancement; Radiographic Image Interpretation, Computer-Assisted; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique; Tomography, X-Ray Computed",Article,Scopus,2-s2.0-84866084589
"Tenne Y.","A computational intelligence algorithm for expensive engineering optimization problems",2012,"Engineering Applications of Artificial Intelligence",14,10.1016/j.engappai.2012.03.009,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862131579&doi=10.1016%2fj.engappai.2012.03.009&partnerID=40&md5=b71fe24fb99cbfa3ca82605baa0e0e2a","The modern engineering design optimization process often replaces laboratory experiments with computer simulations, which leads to expensive black-box optimization problems. Such problems often contain candidate solutions which cause the simulation to fail, and therefore they will have no objective value assigned to them, a scenario which degrades the search effectiveness. To address this, this paper proposes a new computational intelligence optimization algorithm which incorporates a classifier into the optimization search. The classifier predicts which solutions are expected to cause a simulation failure, and its prediction is used to bias the search towards solutions for which the simulation is expected to succeed. To further enhance the search effectiveness, the proposed algorithm continuously adapts during the search the type of model and classifier being used. A rigorous performance analysis using a representative application of airfoil shape optimization shows that the proposed algorithm outperformed existing approaches in terms of the final result obtained, and performed a search with a competitively low number of failed evaluations. Analysis also highlights the contribution of incorporating the classifier into the search, and of the model and classifier selection steps. © 2012 Elsevier Ltd. All rights reserved.","Classification; Evolutionary algorithms; Expensive optimization problems; Model-selection modeling","Airfoil shape optimization; Black-box optimization; Candidate solution; Classifier selection; Computational intelligence optimizations; Engineering optimization problems; Laboratory experiments; Model-selection; Modern engineering; Optimization problems; Performance analysis; Algorithms; Artificial intelligence; Classification (of information); Computer simulation; Evolutionary algorithms; Failure analysis; Optimization",Article,Scopus,2-s2.0-84862131579
"Davies S., Galluppi F., Rast A.D., Furber S.B.","A forecast-based STDP rule suitable for neuromorphic implementation",2012,"Neural Networks",14,10.1016/j.neunet.2012.02.018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861770528&doi=10.1016%2fj.neunet.2012.02.018&partnerID=40&md5=64a2f978c02b2bedf1cfee4e15923b24","Artificial neural networks increasingly involve spiking dynamics to permit greater computational efficiency. This becomes especially attractive for on-chip implementation using dedicated neuromorphic hardware. However, both spiking neural networks and neuromorphic hardware have historically found difficulties in implementing efficient, effective learning rules. The best-known spiking neural network learning paradigm is Spike Timing Dependent Plasticity (STDP) which adjusts the strength of a connection in response to the time difference between the pre- and post-synaptic spikes. Approaches that relate learning features to the membrane potential of the post-synaptic neuron have emerged as possible alternatives to the more common STDP rule, with various implementations and approximations. Here we use a new type of neuromorphic hardware, SpiNNaker, which represents the flexible ""neuromimetic"" architecture, to demonstrate a new approach to this problem. Based on the standard STDP algorithm with modifications and approximations, a new rule, called STDP TTS (Time-To-Spike) relates the membrane potential with the Long Term Potentiation (LTP) part of the basic STDP rule. Meanwhile, we use the standard STDP rule for the Long Term Depression (LTD) part of the algorithm. We show that on the basis of the membrane potential it is possible to make a statistical prediction of the time needed by the neuron to reach the threshold, and therefore the LTP part of the STDP algorithm can be triggered when the neuron receives a spike. In our system these approximations allow efficient memory access, reducing the overall computational time and the memory bandwidth required. The improvements here presented are significant for real-time applications such as the ones for which the SpiNNaker system has been designed. We present simulation results that show the efficacy of this algorithm using one or more input patterns repeated over the whole time of the simulation. On-chip results show that the STDP TTS algorithm allows the neural network to adapt and detect the incoming pattern with improvements both in the reliability of, and the time required for, consistent output. Through the approximations we suggest in this paper, we introduce a learning rule that is easy to implement both in event-driven simulators and in dedicated hardware, reducing computational complexity relative to the standard STDP rule. Such a rule offers a promising solution, complementary to standard STDP evaluation algorithms, for real-time learning using spiking neural networks in time-critical applications. © 2012 Elsevier Ltd.","Forecast; Izhikevich; Neuromorphic; Spiking; SpiNNaker; STDP; Time To Spike; Unsupervised learning","Forecast; Izhikevich; Neuromorphic; Spiking; SpiNNaker; STDP; Time To Spike; Forecasting; Hardware; Intelligent agents; Neural networks; Unsupervised learning; Approximation algorithms; accuracy; article; artificial neural network; automated pattern recognition; computer program; forecasting; learning algorithm; mathematical computing; mathematical model; nerve cell plasticity; Poisson distribution; postsynaptic potential; prediction; priority journal; problem solving; process development; sensitivity and specificity; signal noise ratio; signal processing; simulation; Spike Timing Dependent Plasticity algorithm; statistical analysis; Algorithms; Artificial Intelligence; Computer Simulation; Computer Systems; Computers; Long-Term Potentiation; Membrane Potentials; Models, Neurological; Neural Networks (Computer); Neuronal Plasticity; Neurons; Software; Synaptic Transmission",Article,Scopus,2-s2.0-84861770528
"Calavia L., Baladrón C., Aguiar J.M., Carro B., Sánchez-Esguevillas A.","A semantic autonomous video surveillance system for dense camera networks in Smart Cities",2012,"Sensors (Switzerland)",14,10.3390/s120810407,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865357823&doi=10.3390%2fs120810407&partnerID=40&md5=7b3c72d2b4afdf966231be3d016daebf","This paper presents a proposal of an intelligent video surveillance system able to detect and identify abnormal and alarming situations by analyzing object movement. The system is designed to minimize video processing and transmission, thus allowing a large number of cameras to be deployed on the system, and therefore making it suitable for its usage as an integrated safety and security solution in Smart Cities. Alarm detection is performed on the basis of parameters of the moving objects and their trajectories, and is performed using semantic reasoning and ontologies. This means that the system employs a high-level conceptual language easy to understand for human operators, capable of raising enriched alarms with descriptions of what is happening on the image, and to automate reactions to them such as alerting the appropriate emergency services using the Smart City safety network. © 2012 by the authors; licensee MDPI, Basel, Switzerland.","Safety and security; Semantics; Smart sensors; Surveillance","article; artificial intelligence; automated pattern recognition; city; classification; human; human activities; image processing; methodology; movement (physiology); safety and security; semantics; smart sensors; Surveillance; videorecording; safety and security; semantics; smart sensors; surveillance; Artificial Intelligence; Cities; Human Activities; Humans; Image Processing, Computer-Assisted; Movement; Pattern Recognition, Automated; Semantics; Video Recording",Article,Scopus,2-s2.0-84865357823
"Ekwevugbe T., Brown N., Fan D.","A design model for building occupancy detection using sensor fusion",2012,"IEEE International Conference on Digital Ecosystems and Technologies",14,10.1109/DEST.2012.6227924,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864198573&doi=10.1109%2fDEST.2012.6227924&partnerID=40&md5=ae81fafc76c35266898963288fb4a9f8","Building occupancy sensing is useful for control of building services such as lighting and ventilation, enabling energy savings, whilst maintaining a comfortable environment. However, a precise and reliable measurement of occupancy still remains difficult. Existing technologies are plagued with a number of issues ranging from unreliable data, maintaining privacy, sensor drift, change of use, and short-term financial pressures, including low quality parts and insufficient commissioning. A major performance barrier is currently the fitness to purpose, or otherwise of sensing technologies used. Sensor fusion techniques offer a way to make up for this, aiming to more reliably determine occupancy using a range of different indoor climatic variables. Over the last decade, artificial intelligence (AI) techniques have found some application for building controls, and can also be applied to occupancy estimation. We describe a novel methodology for building occupancy detection using a sensor fusion model based on the Adaptive Neuro-Fuzzy Inference System (ANFIS) algorithm. The system monitors indoor climatic variables, indoor events and energy data obtained from a non-domestic building to infer occupancy patterns. © 2012 IEEE.","ANFIS; Occupancy; Sensor fusion; Sensors","Adaptive neuro-fuzzy inference system; ANFIS; Building controls; Building occupancy; Building services; Climatic variables; Design models; Energy data; Financial pressure; Low qualities; Novel methodology; Occupancy; Reliable measurement; Sensing technology; Sensor drift; Sensor fusion; System monitors; Artificial intelligence; Ecosystems; Technology; Sensors",Article,Scopus,2-s2.0-84864198573
"Mentré D., Marché C., Filliâtre J.-C., Asuka M.","Discharging proof obligations from Atelier B using multiple automated provers",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,10.1007/978-3-642-30885-7_17,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863917985&doi=10.1007%2f978-3-642-30885-7_17&partnerID=40&md5=731500fed841f32fa489c9eeef2bc5c4","We present a method to discharge proof obligations from Atelier B using multiple SMT solvers. It is based on a faithful modeling of B's set theory into polymorphic first-order logic. We report on two case studies demonstrating a significant improvement in the ratio of obligations that are automatically discharged. © 2012 Springer-Verlag.",,"First order logic; Proof obligations; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84863917985
"Cerreta M., Mele R.","A landscape complex values map: Integration among soft values and hard values in a spatial decision support system",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,10.1007/978-3-642-31075-1_49,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863966852&doi=10.1007%2f978-3-642-31075-1_49&partnerID=40&md5=129cb9f235eeb55e0fe05f1d8b354e3b","The paper develops a Spatial Decision Support System (SDSS) for the identification and evaluation of the landscape complexity for the Massa Lubrense territory, in the South of Italy. Through the elaboration of a selection of spatial indicators and the combination of GIS and Analytic Hierarchy Process (AHP) method, it has been defined a decision-making process for the construction of a map of complex values, soft and hard values, that characterize the landscape of Massa Lubrense. The paper explores the potential of a Spatial Decision Support System (SDSS) in the field of land-use planning, recognizing different weights and priorities according to a complex definition of the landscape and its values. © 2012 Springer-Verlag.","Complex values; Knowledge generation; Multi-Criteria Analysis; Spatial Decision Support System; Spatial indicators","Complex values; Knowledge generation; Multi Criteria Analysis; Spatial decision support systems; Spatial indicators; Artificial intelligence; Decision support systems",Conference Paper,Scopus,2-s2.0-84863966852
"Gay R., Mantel H., Sprick B.","Service automata",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,10.1007/978-3-642-29420-4_10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863974134&doi=10.1007%2f978-3-642-29420-4_10&partnerID=40&md5=923f7566ed940acbbe2b1c26722c44fa","We propose a novel framework for reliably enforcing security in distributed systems. Service automata monitor the execution of a distributed program and enforce countermeasures before a violation of a security policy can occur. A key novelty of our proposal is that security is enforced in a decentralized though coordinated fashion. This provides the basis for reliably enforcing global security requirements without introducing unnecessary latencies or communication overhead. The novel contributions of this article include the concept of service automata and a generic formalization of service automata in CSP. We also illustrate how the generic model can be tailored to given security requirements by instantiating its parameters in a stepwise and modular manner. © 2012 Springer-Verlag.",,"Communication overheads; Distributed program; Distributed systems; Generic models; Global Security; Security policy; Security requirements; Service automata; Artificial intelligence; Network security",Conference Paper,Scopus,2-s2.0-84863974134
"Escalante H.J., Montes-y-Gómez M., González J.A., Gómez-Gil P., Altamirano L., Reyes C.A., Reta C., Rosales A.","Acute leukemia classification by ensemble particle swarm model selection",2012,"Artificial Intelligence in Medicine",14,10.1016/j.artmed.2012.03.005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863785984&doi=10.1016%2fj.artmed.2012.03.005&partnerID=40&md5=ce331bf42ffe820f5558c1044c9d4e12","Objective: Acute leukemia is a malignant disease that affects a large proportion of the world population. Different types and subtypes of acute leukemia require different treatments. In order to assign the correct treatment, a physician must identify the leukemia type or subtype. Advanced and precise methods are available for identifying leukemia types, but they are very expensive and not available in most hospitals in developing countries. Thus, alternative methods have been proposed. An option explored in this paper is based on the morphological properties of bone marrow images, where features are extracted from medical images and standard machine learning techniques are used to build leukemia type classifiers. Methods and materials: This paper studies the use of ensemble particle swarm model selection (EPSMS), which is an automated tool for the selection of classification models, in the context of acute leukemia classification. EPSMS is the application of particle swarm optimization to the exploration of the search space of ensembles that can be formed by heterogeneous classification models in a machine learning toolbox. EPSMS does not require prior domain knowledge and it is able to select highly accurate classification models without user intervention. Furthermore, specific models can be used for different classification tasks. Results: We report experimental results for acute leukemia classification with real data and show that EPSMS outperformed the best results obtained using manually designed classifiers with the same data. The highest performance using EPSMS was of 97.68% for two-type classification problems and of 94.21% for more than two types problems. To the best of our knowledge, these are the best results reported for this data set. Compared with previous studies, these improvements were consistent among different type/subtype classification tasks, different features extracted from images, and different feature extraction regions. The performance improvements were statistically significant. We improved previous results by an average of 6% and there are improvements of more than 20% with some settings. In addition to the performance improvements, we demonstrated that no manual effort was required during acute leukemia type/subtype classification. Conclusions: Morphological classification of acute leukemia using EPSMS provides an alternative to expensive diagnostic methods in developing countries. EPSMS is a highly effective method for the automated construction of ensemble classifiers for acute leukemia classification, which requires no significant user intervention. EPSMS could also be used to address other medical classification tasks. © 2012 Elsevier B.V.","Acute leukemia classification; Analysis of bone marrow cell images; Ensemble learning; Full model selection; Morphological classification; Swarm optimization","Acute leukemia; Bone marrow cells; Ensemble learning; Full model; Swarm optimization; Developing countries; Diagnosis; Feature extraction; Learning systems; Diseases; accuracy; acute leukemia; area under the curve; article; automation; bone marrow; cancer classification; classification algorithm; clinical effectiveness; cytoplasm; data base; ensemble particle swarm model selection; experimental study; human; image analysis; leukemia cell; leukocyte; microarray analysis; molecular imaging; morphology; nucleus accumbens; priority journal; process optimization; Algorithms; Artificial Intelligence; Bone Marrow; Bone Marrow Cells; Hematologic Neoplasms; Humans; Image Interpretation, Computer-Assisted; Leukemia; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84863785984
"Kim H.-Y., Park H.-A.","Development and evaluation of data entry templates based on the entity-attribute-value model for clinical decision support of pressure ulcer wound management",2012,"International Journal of Medical Informatics",14,10.1016/j.ijmedinf.2011.10.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862118225&doi=10.1016%2fj.ijmedinf.2011.10.008&partnerID=40&md5=a9bae96b9fdc2305cc7bca596cf641e9","Purposes: The purpose of this study was to develop and evaluate the functionality of structured data entry templates using the entity-attribute-value (EAV) model for clinical decision support of pressure ulcer wound management. Methods: A data set for wound assessment of pressure ulcers that has commonly been recommended by clinical practice guidelines was identified, and then the EAV models on each data were developed. Structured data entry templates and a database were developed based on these EAV models. These were integrated with a knowledge engine into the clinical decision support system (CDSS) to provide patient-specific recommendations on pressure ulcer wound management. The functionality of the EAV model and structured data entry templates for the CDSS was evaluated heuristically by five nurse experts using clinical scenarios. Results: The data set containing 13 entities was identified and EAV models of these entities were created. Cardinalities and data types of attributes were defined to represent the models in more detail. Terms used in the EAV models were mapped to SNOMED CT concepts. Six data entry templates and the relational database with ten tables were developed. Five nurses successfully entered all data in the scenarios except one data element and retrieved expected recommendations successfully from the clinical decision support system when all data were entered correctly. Conclusions: The clinical data models and structured data entry templates developed in this study were useful in supporting clinical decision making on pressure ulcer wound management. © 2011 Elsevier Ireland Ltd.","Clinical decision support system; Models; Pressure ulcer; Reference standards","Cardinalities; Clinical data; Clinical decision making; Clinical decision support; Clinical decision support systems; Clinical practice guidelines; Data elements; Data sets; Data type; Knowledge Engines; Pressure ulcers; Reference standard; Relational Database; Structured data; Wound assessment; Wound management; Artificial intelligence; Decision making; Decision support systems; Models; Nursing; Rating; Diseases; article; clinical decision making; clinical study; data analysis; data base; decision support system; decubitus; entity attribute value model; evaluation; human; information processing; practice guideline; priority journal; process model; wound assessment; wound care; Computer Simulation; Database Management Systems; Decision Support Systems, Clinical; Humans; Practice Guidelines as Topic; Pressure Ulcer",Article,Scopus,2-s2.0-84862118225
"Medhat M.E.","Artificial intelligence methods applied for quantitative analysis of natural radioactive sources",2012,"Annals of Nuclear Energy",14,10.1016/j.anucene.2012.02.013,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860487376&doi=10.1016%2fj.anucene.2012.02.013&partnerID=40&md5=618fabc8c9f24210919acabb4e788ec4","Artificial neural network (ANN) represents one of artificial intelligence methods in the field of modeling and uncertainty in different applications. The objective of the proposed work was focused to apply ANN to identify isotopes and to predict uncertainties of their activities of some natural radioactive sources. The method was tested for analyzing gamma-ray spectra emitted from natural radionuclides in soil samples detected by a high-resolution gamma-ray spectrometry based on HPGe (high purity germanium). The principle of the suggested method is described, including, relevant input parameters definition, input data scaling and networks training. It is clear that there is satisfactory agreement between obtained and predicted results using neural network. © 2012 Elsevier Ltd. All rights reserved.","Gamma-ray spectrometry; Natural radioactivity; Neural networks","Artificial intelligence methods; Gamma ray spectra; Gamma ray spectrometry; High purity germaniums; High resolution; Input datas; Input parameter; Natural radioactivity; Natural radionuclides; Radioactive sources; Soil sample; Gamma ray spectrometers; Germanium; Isotopes; Neural networks; Radioactive prospecting; Ultraviolet spectroscopy; Uncertainty analysis",Article,Scopus,2-s2.0-84860487376
"Tennis J.T.","The strange case of eugenics: A subject's ontogeny in a long-lived classification scheme and the question of collocative integrity",2012,"Journal of the American Society for Information Science and Technology",14,10.1002/asi.22686,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862279108&doi=10.1002%2fasi.22686&partnerID=40&md5=4bb226e77a39114daed00876ce3c9667","This article introduces the problem of collocative integrity present in long-lived classification schemes that undergo several changes. A case study of the subject ""eugenics"" in the Dewey Decimal Classification is presented to illustrate this phenomenon. Eugenics is strange because of the kinds of changes it undergoes. The article closes with a discussion of subject ontogeny as the name for this phenomenon and describes implications for information searching and browsing. © 2012 ASIS&T.","classification; maintainability; vocabulary control","Classification scheme; Dewey decimal classifications; Information searching; Artificial intelligence; Classification (of information); Software engineering; Vocabulary control; Maintainability",Article,Scopus,2-s2.0-84862279108
"Sápi J., Drexler D.A., Harmati I., Sápi Z., Kovács L.","Linear state-feedback control synthesis of tumor growth control in antiangiogenic therapy",2012,"IEEE 10th Jubilee International Symposium on Applied Machine Intelligence and Informatics, SAMI 2012 - Proceedings",14,10.1109/SAMI.2012.6208945,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862750291&doi=10.1109%2fSAMI.2012.6208945&partnerID=40&md5=75aed3735f6cb1f62b79b282c7c51f52","Cancer diseases are one of the most lethal, incurable diseases today, thus fighting cancer is an actual and urgent problem in clinical practice. Beside classical therapies, a new approach is represented by model-based therapies, where human body works as a complex system. These therapies are called targeted molecular therapies (TMTs). TMTs are fighting specifically against different cancer mechanisms and usually don't eliminate the whole tumor, but control the tumor into a given state and keep it there. The aim of antiangiogenic cancer therapy is to prevent tumors from forming new blood vessels, because without angiogenesis tumor growth is inhibited. In this paper we analyze a nonlinear tumor growth model and design linear controllers based on a linear model acquired from working point linearization. Realized controllers are state feedback with pole placement, LQ control method and both controllers with state observer. Simulations are carried out and the controllers are analyzed in many aspects, including the working points used at linearization. © 2012 IEEE.",,"Angiogenesis; Anti-angiogenic therapy; Antiangiogenic; Cancer disease; Cancer therapy; Clinical practices; Human bodies; Incurable disease; Linear controllers; LQ control; Molecular therapy; Pole placement; State observer; Tumor growth; Tumor growth models; Urgent problems; Working point; Artificial intelligence; Blood vessels; Controllers; Diseases; Information science; Linear control systems; Linearization; State feedback; Tumors",Conference Paper,Scopus,2-s2.0-84862750291
"Nikolov A., D'Aquin M., Motta E.","What should i link to? Identifying relevant sources and classes for data linking",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,10.1007/978-3-642-29923-0_19,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862231188&doi=10.1007%2f978-3-642-29923-0_19&partnerID=40&md5=8f1672234d81d74506991d2a9c344b52","With more data repositories constantly being published on the Web, choosing appropriate data sources to interlink with newly published datasets becomes a non-trivial problem. It is necessary to choose both the repositories to link to and the relevant subsets of these repositories, which contain potentially matching individuals. In order to do this, detailed information about the content and structure of semantic repositories is often required. However, retrieving and processing such information for a potentially large number of datasets is practically unfeasible. In this paper, we propose an approach which utilises an existing semantic web index in order to identify potentially relevant datasets for interlinking and rank them. Furthermore, we adapt instance-based ontology schema matching to extract relevant subsets of selected data source and, in this way, pre-configure data linking tools. © 2012 Springer-Verlag.",,"Content and structure; Data repositories; Data sets; Data source; Datalinking; Non-trivial; Schema matching; Semantic repository; Artificial intelligence; Semantic Web",Conference Paper,Scopus,2-s2.0-84862231188
"Rahman S., Odeyinka H., Perera S., Bi Y.","Product-cost modelling approach for the development of a decision support system for optimal roofing material selection",2012,"Expert Systems with Applications",14,10.1016/j.eswa.2012.01.010,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857655506&doi=10.1016%2fj.eswa.2012.01.010&partnerID=40&md5=8228f672b41db14f534ac4d56b7a12b5","Selection of optimal roofing materials is very important but it is a complex and onerous task as varieties of materials are available for housing roof construction. In order to select suitable materials, an extensive range of criteria would need to be considered. This paper presents the framework and the development of a knowledge-based decision support system for material selection implemented in roofing material selection domain, called 'Knowledge-based Decision Support system for roofing Material Selection and cost estimating' (KDSMS). It was developed to facilitate the selection of optimal materials for different roof sub elements. The system consists of a database and knowledge base that is equipped with an inference engine. The former is used to store different types of roofing materials with assigned attribute values. The later is used to hold qualitative and quantitative knowledge which were collected from domain experts and other technical literatures such as building regulations, price guide book and product catalogues. The proposed system employs the TOPSIS (Technique of ranking Preferences by Similarity to the Ideal Solution) multiple criteria decision making method to solve materials selection and optimisation problem. This study utilised the available roofing materials in the UK housing market in developing the system reported. The main contribution of the developed system is that it provides a tool for the architects, quantity surveyors or self house builder to select optimal materials from a wide array of possibilities for different roof sub elements and also to estimate the conceptual cost for the roof element in the early stage of building design. © 2011 Elsevier Ltd. All rights reserved.","Decision support system; Energy efficient building; Knowledge-based system; Product-cost modelling; Roofing material selection; TOPSIS","Attribute values; Building design; Building regulations; Domain experts; Energy efficient building; House builders; Housing markets; Ideal solutions; Knowledge base; Knowledge based decision support systems; Material selection; Materials selection; Multiple criteria decision making; Optimal materials; Optimisations; Product-cost modelling; Quantitative knowledge; Roof constructions; Roofing materials; Sub-element; Technical literature; TOPSIS; Artificial intelligence; Building codes; Cost benefit analysis; Cost estimating; Decision support systems; Energy efficiency; Housing; Knowledge based systems; Materials; Optimization; Roofs",Article,Scopus,2-s2.0-84857655506
"Pernestål A., Nyberg M., Warnquist H.","Modeling and inference for troubleshooting with interventions applied to a heavy truck auxiliary braking system",2012,"Engineering Applications of Artificial Intelligence",14,10.1016/j.engappai.2011.02.018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859422220&doi=10.1016%2fj.engappai.2011.02.018&partnerID=40&md5=c2467695ea0d32110bdfeb580af7be27","Computer assisted troubleshooting with external interventions is considered. The work is motivated by the task of repairing an automotive vehicle at lowest possible expected cost. The main contribution is a decision theoretic troubleshooting system that is developed to handle external interventions. In particular, practical issues in modeling for troubleshooting are discussed, the troubleshooting system is described, and a method for the efficient probability computations is developed. The troubleshooting systems consists of two parts; a planner that relies on AO search and a diagnoser that utilizes Bayesian networks (BN). The work is based on a case study of an auxiliary braking system of a modern truck. Two main challenges in troubleshooting automotive vehicles are the need for disassembling the vehicle during troubleshooting to access parts to repair, and the difficulty to verify that the vehicle is fault free. These facts lead to that probabilities for faults and for future observations must be computed for a system that has been subject to external interventions that cause changes in the dependency structure. The probability computations are further complicated due to the mixture of instantaneous and non-instantaneous dependencies. To compute the probabilities, we develop a method based on an algorithm, updateBN, that updates a static BN to account for the external interventions. © 2011 Elsevier Ltd. All rights reserved.","Automobile industry; Bayesian network; Decision support systems; Fault diagnosis; Probabilistic models","Access parts; Automotive vehicle; Braking system; Computer assisted; Decision supports; Decision-theoretic; Dependency structures; Expected costs; Future observations; Heavy truck; Practical issues; Probabilistic models; Probability computations; Troubleshooting systems; Artificial intelligence; Automotive industry; Bayesian networks; Brakes; Decision support systems; Failure analysis; Probability; Vehicles; Trucks",Article,Scopus,2-s2.0-84859422220
"Lu R., Xiang L., Liu M.-R., Yang Q.","Discovering news topics from microblogs based on hidden topics analysis and text clustering",2012,"Moshi Shibie yu Rengong Zhineng/Pattern Recognition and Artificial Intelligence",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866277062&partnerID=40&md5=b6b597922aa600dc1b0808a333ff01cf","A method of news topics extraction from large-scale short posts of microblogging-service is proposed. Through the hidden topic analysis, the similarity measurement of short texts is solved well. In every time window, the short posts which are most likely to talk about news events are selected according to the characteristics of the news. Then, a two-level K-means-hierarchical hybrid clustering method is used to cluster all the selected data into different news topics. The experimental results show the proposed method works well on large-scale microblog dataset.","Hidden topic model; Hybrid clustering; Microblog; Short text; Topics extraction","Data sets; Hybrid clustering; Micro-blog; Short text; Similarity measurements; Text Clustering; Time windows; Topic analysis; Topic model; Artificial intelligence; Computer vision",Article,Scopus,2-s2.0-84866277062
"Garibaldi J.M., Zhou S.-M., Wang X.-Y., John R.I., Ellis I.O.","Incorporation of expert variability into breast cancer treatment recommendation in designing clinical protocol guided fuzzy rule system models",2012,"Journal of Biomedical Informatics",14,10.1016/j.jbi.2011.12.007,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862829004&doi=10.1016%2fj.jbi.2011.12.007&partnerID=40&md5=91d4dac291f9ab774d6c3bf70a2bd737","It has been often demonstrated that clinicians exhibit both inter-expert and intra-expert variability when making difficult decisions. In contrast, the vast majority of computerized models that aim to provide automated support for such decisions do not explicitly recognize or replicate this variability. Furthermore, the perfect consistency of computerized models is often presented as a de facto benefit. In this paper, we describe a novel approach to incorporate variability within a fuzzy inference system using non-stationary fuzzy sets in order to replicate human variability. We apply our approach to a decision problem concerning the recommendation of post-operative breast cancer treatment; specifically, whether or not to administer chemotherapy based on assessment of five clinical variables: NPI (the Nottingham Prognostic Index), estrogen receptor status, vascular invasion, age and lymph node status. In doing so, we explore whether such explicit modeling of variability provides any performance advantage over a more conventional fuzzy approach, when tested on a set of 1310 unselected cases collected over a fourteen year period at the Nottingham University Hospitals NHS Trust, UK. The experimental results show that the standard fuzzy inference system (that does not model variability) achieves overall agreement to clinical practice around 84.6% (95% CI: 84.1-84.9%), while the non-stationary fuzzy model can significantly increase performance to around 88.1% (95% CI: 88.0-88.2%), p<0.001. We conclude that non-stationary fuzzy models provide a valuable new approach that may be applied to clinical decision support systems in any application domain. © 2012 Elsevier Inc..","Breast cancer; Decision support; Expert systems; Fuzzy logic; Variability","Application domains; Automated support; Breast Cancer; Breast cancer treatment; Clinical decision support systems; Clinical practices; Decision problems; Decision supports; Estrogen receptor; Explicit modeling; Fuzzy approach; Fuzzy inference systems; Fuzzy models; Fuzzy rule systems; Human variability; Lymph node; Non-stationary fuzzy sets; Nonstationary; Variability; Vascular invasion; Artificial intelligence; Chemotherapy; Decision support systems; Expert systems; Fuzzy logic; Fuzzy sets; Fuzzy systems; Diseases; antineoplastic agent; epidermal growth factor receptor 2; estrogen receptor; article; breast cancer; cancer chemotherapy; cancer hormone therapy; cancer invasion; cancer therapy; decision support system; fuzzy system; human; medical decision making; practice guideline; priority journal; Breast Neoplasms; Clinical Protocols; Decision Support Systems, Clinical; Fuzzy Logic; Great Britain; Humans; Models, Biological",Article,Scopus,2-s2.0-84862829004
"Shinkareva S.V., Malave V.L., Just M.A., Mitchell T.M.","Exploring commonalities across participants in the neural representation of objects",2012,"Human Brain Mapping",14,10.1002/hbm.21296,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860836540&doi=10.1002%2fhbm.21296&partnerID=40&md5=8f00c2394b6f24fe4e1a98024417853f","The question of whether the neural encodings of objects are similar across different people is one of the key questions in cognitive neuroscience. This article examines the commonalities in the internal representation of objects, as measured with fMRI, across individuals in two complementary ways. First, we examine the commonalities in the internal representation of objects across people at the level of interobject distances, derived from whole brain fMRI data, and second, at the level of spatially localized anatomical brain regions that contain sufficient information for identification of object categories, without making the assumption that their voxel patterns are spatially matched in a common space. We examine the commonalities in internal representation of objects on 3T fMRI data collected while participants viewed line drawings depicting various tools and dwellings. This exploratory study revealed the extent to which the representation of individual concepts, and their mutual similarity, is shared across participants. © 2011 Wiley Periodicals, Inc.","Eigen decomposition; fMRI; Logistic regression; Machine learning; Multivariate data analysis; RV-coefficient; STATIS","article; brain region; cognition; data analysis; functional magnetic resonance imaging; human; image analysis; logistic regression analysis; nerve cell network; priority journal; voxel based morphometry; Adult; Artificial Intelligence; Brain; Brain Mapping; Humans; Image Processing, Computer-Assisted; Individuality; Magnetic Resonance Imaging; Pattern Recognition, Visual; Photic Stimulation",Article,Scopus,2-s2.0-84860836540
"Yu X.","A stock model with jumps for uncertain markets",2012,"International Journal of Uncertainty, Fuzziness and Knowlege-Based Systems",14,10.1142/S0218488512500213,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862085939&doi=10.1142%2fS0218488512500213&partnerID=40&md5=bc3d4673377048fbe584fcbaa5453899","Uncertain differential equation with jumps is a type of differential equation driven by two classes of uncertain processes, namely canonical process and renewal process. Based on uncertain differential equation with jumps, this paper proposes a stock model with jumps for uncertain financial markets. Furthermore, the European call and put option pricing formulas for the stock model are formulated and some mathematical properties of them are studied. Finally, some generalized uncertain stock models with jumps are discussed. © 2012 World Scientific Publishing Company.","European option; Stock model; Uncertain differential equation with jumps; Uncertain finance","Canonical process; European option; Financial market; Mathematical properties; Put options; Renewal process; Uncertain differential equation with jumps; Uncertain markets; Uncertain process; Artificial intelligence; Software engineering; Differential equations",Article,Scopus,2-s2.0-84862085939
"Jaworski M., Pietruczuk L., Duda P.","On resources optimization in fuzzy clustering of data streams",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,10.1007/978-3-642-29350-4-11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861074207&doi=10.1007%2f978-3-642-29350-4-11&partnerID=40&md5=383ce0b0dec84e43a2dd6689a0b2a724","In this paper the resource consumption of the fuzzy clustering algorithms for data streams is studied. As the examples, the wFCM and the wPCM algorithms are examined. It is shown that partitioning a data stream into chunks reduces the processing time of considered algorithms significantly. The partitioning procedure is accompanied with the reduction of results accuracy, however the change is acceptable. The problems arised due to the high speed data streams are presented as well. The uncontrolable growth of subsequent data chunk sizes, which leads to the overflow of the available memory, is demonstrated for both the wFCM and wPCM algorithms. The maximum chunk size limit modification, as a solution to this problem, is introduced. This modification ensures that the available memory is never exceeded, what is shown in the simulations. The considered modification decreases the quality of clustering results only slightly. © 2012 Springer-Verlag Berlin Heidelberg.",,"Data chunks; Data stream; High-speed data; Processing time; Quality of clustering; Resource consumption; Resources optimization; Artificial intelligence; Clustering algorithms; Fuzzy clustering; Soft computing; Data communication systems",Conference Paper,Scopus,2-s2.0-84861074207
"Szarek A., Korytkowski M., Rutkowski L., Scherer R., Szyprowski J.","Forecasting wear of head and acetabulum in hip joint implant",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,10.1007/978-3-642-29350-4-41,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861072069&doi=10.1007%2f978-3-642-29350-4-41&partnerID=40&md5=b4ed9a97d5a0dcb0b2b56968bc6487c2","Total hip joint replacement is a multi-aspect issue, where life span of the implant system in human body depends on numerous factors. One of the main reasons for having a hip replacement is loosening or wear of the associated components in artificial joint. The rate of wear depends mainly on the type of materials working together in the artificial joint, the burden resulting from the patient's body weight, intensity of use, limb functionality, age of the patient's and individual factors. The analysis of all factors leading to the joint wear and articulation expensiveness will allow for the appropriate selection of an head-acetabulum system which provide long-lasting and trouble-free operation. We use neuro-fuzzy systems to machine-learn the data to predict automatically the wear of elements in the artificial hip joint. © 2012 Springer-Verlag Berlin Heidelberg.",,"Artificial joint; Body weight; Hip replacements; Hip-joint implants; Human bodies; Individual factors; Life span; Long lasting; Neurofuzzy system; Total hip joints; Trouble-free operations; Artificial intelligence; Hip prostheses; Soft computing; Factor analysis",Conference Paper,Scopus,2-s2.0-84861072069
"Doan S., Collier N., Xu H., Duy P.H., Phuong T.M.","Recognition of medication information from discharge summaries using ensembles of classifiers",2012,"BMC Medical Informatics and Decision Making",14,10.1186/1472-6947-12-36,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860595742&doi=10.1186%2f1472-6947-12-36&partnerID=40&md5=e728ca6135d9756e67e9663d99389732","Background: Extraction of clinical information such as medications or problems from clinical text is an important task of clinical natural language processing (NLP). Rule-based methods are often used in clinical NLP systems because they are easy to adapt and customize. Recently, supervised machine learning methods have proven to be effective in clinical NLP as well. However, combining different classifiers to further improve the performance of clinical entity recognition systems has not been investigated extensively. Combining classifiers into an ensemble classifier presents both challenges and opportunities to improve performance in such NLP tasks. Methods. We investigated ensemble classifiers that used different voting strategies to combine outputs from three individual classifiers: a rule-based system, a support vector machine (SVM) based system, and a conditional random field (CRF) based system. Three voting methods were proposed and evaluated using the annotated data sets from the 2009 i2b2 NLP challenge: simple majority, local SVM-based voting, and local CRF-based voting. Results: Evaluation on 268 manually annotated discharge summaries from the i2b2 challenge showed that the local CRF-based voting method achieved the best F-score of 90.84% (94.11% Precision, 87.81% Recall) for 10-fold cross-validation. We then compared our systems with the first-ranked system in the challenge by using the same training and test sets. Our system based on majority voting achieved a better F-score of 89.65% (93.91% Precision, 85.76% Recall) than the previously reported F-score of 89.19% (93.78% Precision, 85.03% Recall) by the first-ranked system in the challenge. Conclusions: Our experimental results using the 2009 i2b2 challenge datasets showed that ensemble classifiers that combine individual classifiers into a voting system could achieve better performance than a single classifier in recognizing medication information from clinical text. It suggests that simple strategies that can be easily implemented such as majority voting could have the potential to significantly improve clinical entity recognition. © 2012 Doan et al; licensee BioMed Central Ltd.",,"drug; algorithm; article; artificial intelligence; automated pattern recognition; computer program; decision support system; female; hospital discharge; hospital organization; human; information retrieval; male; management; methodology; natural language processing; reproducibility; semantics; support vector machine; Algorithms; Artificial Intelligence; Decision Support Techniques; Female; Humans; Information Storage and Retrieval; Institutional Management Teams; Male; Medication Systems; Natural Language Processing; Patient Discharge; Pattern Recognition, Automated; Pharmaceutical Preparations; Reproducibility of Results; Semantics; Software Design; Support Vector Machines",Article,Scopus,2-s2.0-84860595742
"Miller-Coleman R.L., Dodsworth J.A., Ross C.A., Shock E.L., Williams A.J., Hartnett H.E., McDonald A.I., Havig J.R., Hedlund B.P.","Korarchaeota diversity, biogeography, and abundance in Yellowstone and Great Basin hot springs and ecological niche modeling based on machine learning",2012,"PLoS ONE",14,10.1371/journal.pone.0035964,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860507088&doi=10.1371%2fjournal.pone.0035964&partnerID=40&md5=088279973ca50de6119c33995efbff86","Over 100 hot spring sediment samples were collected from 28 sites in 12 areas/regions, while recording as many coincident geochemical properties as feasible (&gt;60 analytes). PCR was used to screen samples for Korarchaeota 16S rRNA genes. Over 500 Korarchaeota 16S rRNA genes were screened by RFLP analysis and 90 were sequenced, resulting in identification of novel Korarchaeota phylotypes and exclusive geographical variants. Korarchaeota diversity was low, as in other terrestrial geothermal systems, suggesting a marine origin for Korarchaeota with subsequent niche-invasion into terrestrial systems. Korarchaeota endemism is consistent with endemism of other terrestrial thermophiles and supports the existence of dispersal barriers. Korarchaeota were found predominantly in &gt;55°C springs at pH 4.7-8.5 at concentrations up to 6.6×106 16S rRNA gene copies g-1 wet sediment. In Yellowstone National Park (YNP), Korarchaeota were most abundant in springs with a pH range of 5.7 to 7.0. High sulfate concentrations suggest these fluids are influenced by contributions from hydrothermal vapors that may be neutralized to some extent by mixing with water from deep geothermal sources or meteoric water. In the Great Basin (GB), Korarchaeota were most abundant at spring sources of pH&lt;7.2 with high particulate C content and high alkalinity, which are likely to be buffered by the carbonic acid system. It is therefore likely that at least two different geological mechanisms in YNP and GB springs create the neutral to mildly acidic pH that is optimal for Korarchaeota. A classification support vector machine (C-SVM) trained on single analytes, two analyte combinations, or vectors from non-metric multidimensional scaling models was able to predict springs as Korarchaeota-optimal or sub-optimal habitats with accuracies up to 95%. To our knowledge, this is the most extensive analysis of the geochemical habitat of any high-level microbial taxon and the first application of a C-SVM to microbial ecology. © 2012 Miller-Coleman et al.",,"article; Basin Groups; biogeography; controlled study; ecological niche; ecosystem; genetic variability; geochemistry; geographic distribution; ion pair chromatography; Korarchaeota; machine learning; microbial diversity; nonhuman; nucleotide sequence; phylogeny; physical chemistry; population structure; species endemicity; support vector machine; artificial intelligence; biodiversity; chemistry; classification; environmental aspects and related phenomena; genetics; pH; phylogeography; temperature; thermal spring; Korarchaeota; water; Artificial Intelligence; Biodiversity; Ecological and Environmental Processes; Ecosystem; Hot Springs; Hydrogen-Ion Concentration; Korarchaeota; Phylogeography; Temperature; Water",Article,Scopus,2-s2.0-84860507088
"Arican Z., Frossard P.","Scale-invariant features and polar descriptors in omnidirectional imaging",2012,"IEEE Transactions on Image Processing",14,10.1109/TIP.2012.2185937,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860147598&doi=10.1109%2fTIP.2012.2185937&partnerID=40&md5=69378ea3a1a51fe9511ac3566080fcd5","We propose a method to compute scale-invariant features in omnidirectional images. We present a formulation based on the Riemannian geometry for the definition of differential operators on non-Euclidian manifolds that adapt to the mirror and lens structures in omnidirectional imaging. These operators lead to a scale-space analysis that preserves the geometry of the visual information in omnidirectional images. We then build a novel scale-invariant feature detection framework for omnidirectional images that can be mapped on the sphere. We further present a new descriptor and feature matching solution for these omnidirectional images. The descriptor builds on the log-polar planar descriptors and adapts the descriptor computation to the specific geometry and the nonuniform sampling density of omnidirectional images. We also propose a rotation-invariant matching method that eliminates the orientation computation during the feature detection phase and thus decreases the computational complexity. Experimental results demonstrate that the new feature computation method combined with the adapted descriptors offers promising detection and matching performance, i.e., it improves on the common scale-invariant feature transform (SIFT) features computed on the unwrapped omnidirectional images, as well as spherical SIFT features. Finally, we show that the proposed framework also permits to match features between images with different native geometry. © 1992-2012 IEEE.",,"Descriptors; Differential operators; Feature computation; Feature detection; Feature matching; Log-polar; Matching methods; Matching performance; Nonuniform sampling; Omnidirectional image; Omnidirectional imaging; Riemannian geometry; Rotation invariant; Scale invariant feature transforms; Scale-invariant; Scale-space analysis; SIFT Feature; Visual information; Mathematical operators; Spheres; Geometry; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; image enhancement; methodology; reproducibility; sensitivity and specificity; three dimensional imaging; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84860147598
"Durand B., Romashchenko A., Shen A.","Fixed-point tile sets and their applications",2012,"Journal of Computer and System Sciences",14,10.1016/j.jcss.2011.11.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857787225&doi=10.1016%2fj.jcss.2011.11.001&partnerID=40&md5=47c164cd98e61ec7827847ef4124908e","An aperiodic tile set was first constructed by R. Berger while proving the undecidability of the domino problem. It turned out that aperiodic tile sets appear in many fields, ranging from logic (the Entscheidungsproblem) to physics (quasicrystals). We present a new construction of an aperiodic tile set that is based on Kleene's fixed-point construction instead of geometric arguments. This construction is similar to J. von Neumann's self-reproducing automata; similar ideas were also used by P. Gács in the context of error-correcting computations. This construction is rather flexible, so it can be used in many ways. We show how it can be used to implement substitution rules, to construct strongly aperiodic tile sets (in which any tiling is far from any periodic tiling), to give a new proof for the undecidability of the domino problem and related results, to characterize effectively closed one-dimensional subshifts in terms of two-dimensional subshifts of finite type (an improvement of a result by M. Hochman), to construct a tile set that has only complex tilings, and to construct a ""robust"" aperiodic tile set that does not have periodic (or close to periodic) tilings even if we allow some (sparse enough) tiling errors. For the latter, we develop a hierarchical classification of points in random sets into islands of different ranks. Finally, we combine and modify our tools to prove our main result: There exists a tile set such that all tilings have high Kolmogorov complexity even if (sparse enough) tiling errors are allowed. Some of these results were included in the DLT extended abstract (Durand et al., 2008 [9]) and in the ICALP extended abstract (Durand et al., 2009 [10]). © 2011 Elsevier Inc. All rights reserved.","Aperiodic tilings; Kleene's fixed-point theorem; Kolmogorov complexity","Aperiodic tilings; Extended abstracts; Fixed point theorems; Geometric arguments; Hierarchical classification; Kolmogorov complexity; Random set; Subshifts; Subshifts-of-finite type; Undecidability; Artificial intelligence; Errors; Gallium; Self reproducing automata; Abstracting",Article,Scopus,2-s2.0-84857787225
"Kutilek P., Viteckova S.","Prediction of lower extremity movement by cyclograms",2012,"Acta Polytechnica",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859630081&partnerID=40&md5=27e5c9af48e99730316382f9393cd896","Human gait is nowadays undergoing extensive analysis. Predictions of leg movements can be used for orthosis and prosthesis programming, and also for rehabilitation. Our work focuses on predicting human gait with the use of angleangle diagrams, also called cyclograms. In conjunction with artificial intelligence, cyclograms offer a wide area of medical applications. We have identified cyclogram characteristics such as the slope and the area of the cyclogram for a neural network learning algorithm. Neural networks learned by cyclograms offer wide applications in prosthesis control systems.","Artificial intelligence; Artificial neural networks; Cyclogram; Gait",,Article,Scopus,2-s2.0-84859630081
"Cook M., Colton S., Gow J.","Initial results from co-operative co-evolution for automated platformer design",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,10.1007/978-3-642-29178-4_20,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859294983&doi=10.1007%2f978-3-642-29178-4_20&partnerID=40&md5=b5ffbae6f08c9c988dc00526d72675b3","We present initial results from ACCME, A Co-operative Co-evolutionary Metroidvania Engine, which uses co-operative co-evolution to automatically evolve simple platform games. We describe the system in detail and justify the use of co-operative co-evolution. We then address two fundamental questions about the use of this method in automated game design, both in terms of its ability to maximise fitness functions, and whether our choice of fitness function produces scores which correlate with player preference in the resulting games. © 2012 Springer-Verlag.","automated game design; co-operative co-evolution; procedural generation","Co-evolution; Co-evolutionary; Fitness functions; Game design; procedural generation; Artificial intelligence; Automation",Conference Paper,Scopus,2-s2.0-84859294983
"Prat P., Benedetti L., Corominas L., Comas J., Poch M.","Model-based knowledge acquisition in environmental decision support system for wastewater integrated management",2012,"Water Science and Technology",14,10.2166/wst.2012.759,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859305672&doi=10.2166%2fwst.2012.759&partnerID=40&md5=725ecef5f023ed9c9cbc7f5298ec4db9","The main goal of the Water Framework Directive is to achieve good chemical and ecological status of water bodies by 2015. The implementation of integrated river basin management, including sewer systems, wastewater treatment plants and receiving water bodies, is essential to accomplishing this objective. Integrated management is complex and therefore the implementation of control systems and the development of decision support systems are needed to facilitate the work of urban wastewater system (UWS) managers. Within this context, the objective of this paper is to apply integrated modelling of an UWS to simulate and analyse the behaviour of the 'Congost' UWS in Spain, and to optimize its performance against different types of perturbations. This analysis results in optimal operating set-points for each perturbation, improves river water quality, minimizes combined sewer overflows and optimizes flow lamination from storm water tanks. This is achieved by running Monte Carlo simulations and applying global sensitivity analysis. The set-points will become part of the knowledge base composed of a set of IF-THEN rules of the environmental decision support system being developed for this case study. © IWA Publishing 2012.","EDSS; Global sensitivity analysis; Integrated modelling; Knowledge acquisition; Monte Carlo","Analysis results; Combined sewer overflows; Ecological status; EDSS; Environmental decision support systems; Global sensitivity analysis; If-then rules; Integrated management; Integrated modelling; Knowledge base; MONTE CARLO; Monte Carlo Simulation; Receiving waters; River basin management; River water quality; Setpoints; Sewer system; Stormwaters; Urban wastewater system; Wastewater treatment plants; Water Framework Directives; Waterbodies; Artificial intelligence; Combined sewers; Decision support systems; Knowledge acquisition; Knowledge based systems; Monte Carlo methods; Optimization; Sensitivity analysis; Water conservation; Water management; Water quality; Water tanks; Integration; storm water; data acquisition; decision support system; integrated approach; knowledge based system; Monte Carlo analysis; numerical model; optimization; perturbation; river basin; river water; sensitivity analysis; sewer network; urban drainage; wastewater; water management; water quality; water treatment; article; computer aided design; ecology; environmental decision support system; knowledge base; qualitative analysis; river basin; Spain; tank; waste water management; waste water treatment plant; water flow; Computer Simulation; Decision Support Systems, Management; Environmental Monitoring; Models, Theoretical; Rivers; Spain; Waste Disposal, Fluid; Water Pollutants, Chemical; Weather; Spain",Article,Scopus,2-s2.0-84859305672
"Vašíček Z., Slaný K.","Efficient phenotype evaluation in Cartesian genetic programming",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,10.1007/978-3-642-29139-5_23,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859135595&doi=10.1007%2f978-3-642-29139-5_23&partnerID=40&md5=d0e4b350d5ff2c9c14bcfe8729c70d13","This paper describes an efficient acceleration technique designed to speedup the evaluation of candidate solutions in Cartesian Genetic Programming (CGP). The method is based on translation of the CGP phenotype to a binary machine code that is consequently executed. The key feature of the presented approach is that the introduction of the translation mechanism into common fitness evaluation procedure requires only marginal knowledge of target CPU instruction set. The proposed acceleration technique is evaluated using a symbolic regression problem in floating point domain. It is shown that for a cost of small changes in a common CGP implementation, a significant speedup can be obtained even on a common desktop CPU. The accelerated version of CGP implementation accompanied with performance analysis is available for free download from http://www.fit.vutbr.cz/~vasicek/cgp © 2012 Springer-Verlag.","Acceleration; Cartesian genetic programming; Fitness evaluation; Symbolic regression","Acceleration technique; Binary machines; Candidate solution; Cartesian genetic programming; Fitness evaluations; Floating points; Instruction set; Key feature; Performance analysis; Symbolic regression; Symbolic regression problems; Acceleration; Artificial intelligence; Genetic programming",Conference Paper,Scopus,2-s2.0-84859135595
"Anokwa Y., Ribeka N., Parikh T., Borriello G., Were M.C.","Design of a phone-based clinical decision support system for resource-limited settings",2012,"ACM International Conference Proceeding Series",14,10.1145/2160673.2160676,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859072806&doi=10.1145%2f2160673.2160676&partnerID=40&md5=5db53c5b41fec66e9133ed9ed6fc5377","While previous work has shown that clinical decision support systems (CDSS) improve patient care in resource-limited settings, there is little access to such systems at the point of care. Moreover, even when CDSS are available, compliance with care suggestions remain low. In this paper, we use a multi-method approach to document four failure modes that can affect CDSS implementations. Building from six iteratively derived design principles, we describe a phone-based system designed to address these failure modes. Through a formal usability evaluation, we discover six core findings that are important for implementers of mobile systems for health care providers in resource-limited settings. Copyright 2012 ACM.","Clinical decision support; Electronic medical health record; MHealth; Mobile phones; Reminders; Summaries","Clinical decision support; Medical health; mHealth; Reminders; Summaries; Artificial intelligence; Decision support systems; Information technology; Mobile phones; Systems analysis; Telephone sets; Telephone systems",Conference Paper,Scopus,2-s2.0-84859072806
"Lee S., Lee K.C.","Context-prediction performance by a dynamic Bayesian network: Emphasis on location prediction in ubiquitous decision support environment",2012,"Expert Systems with Applications",14,10.1016/j.eswa.2011.10.026,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855870782&doi=10.1016%2fj.eswa.2011.10.026&partnerID=40&md5=7356f566c82fff98dc164ecdd2f2be9e","Ubiquitous decision support systems require more intelligent mechanism in which more timely and accurate decision support is available. However, conventional context-aware systems, which have been popular in the ubiquitous decision support systems field, cannot provide such agile and proactive decision support. To fill this research void, this paper proposes a new concept of context prediction mechanism by which the ubiquitous decision support devices are able to predict users' future contexts in advance, and provide more timely and proactive decision support that users would be satisfied much more. Especially, location prediction is useful because ubiquitous decision support systems could dynamically adapt their decision support contents for a user based on a user's future location. In this sense, as an alternative for the inference engine mechanism to be used in the ubiquitous decision support systems capable of context-prediction, we propose an inductive approach to recognizing a user's location by learning a dynamic Bayesian network model. The dynamic Bayesian network model has been evaluated with a set of contextual data from undergraduate students. The evaluation result suggests that a dynamic Bayesian network model offers significant predictive power in the location prediction. Besides, we found that the dynamic Bayesian network model has a great potential for the future types of ubiquitous decision support systems. © 2011 Elsevier Ltd. All rights reserved.","Bayesian network; Context prediction; Dynamic Bayesian networks; Naïve Bayesian network; Tree augmented naïve Bayesian network; Ubiquitous computing; Ubiquitous decision support system","Context prediction; Context predictions; Context-aware systems; Decision supports; Dynamic Bayesian network; Dynamic Bayesian network models; Dynamic Bayesian networks; Evaluation results; Intelligent mechanisms; Location prediction; Predictive power; Undergraduate students; Artificial intelligence; Decision support systems; Dynamics; Forecasting; Students; Trees (mathematics); Ubiquitous computing; Bayesian networks",Article,Scopus,2-s2.0-84855870782
"Turau V.","Efficient transformation of distance-2 self-stabilizing algorithms",2012,"Journal of Parallel and Distributed Computing",14,10.1016/j.jpdc.2011.12.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857791479&doi=10.1016%2fj.jpdc.2011.12.008&partnerID=40&md5=b0cb832669abb7b487bae1fb0479e783","Self-stabilizing algorithms for optimization problems can often be solved more easily using the distance-two model in which each vertex can instantly see the state information of all vertices up to distance two. This paper presents a new technique to emulate algorithms for the distance-two model on the distance-one model using the distributed scheduler with a slowdown factor of O(m) moves. Up until now the best transformer had a slowdown factor of O( n2m) moves. The technique is used to derive improved self-stabilizing algorithms for several graph domination problems. The paper also introduces a generalization of the distance-two model allowing a more space efficient transformer. © 2012 Elsevier Inc. All rights reserved.","Distributed algorithms; Fault tolerant algorithms; Self-stabilization","Domination problem; Fault tolerant algorithms; Optimization problems; Self-stabilization; Self-stabilizing algorithm; Space efficient; State information; Artificial intelligence; Computer programming; Parallel algorithms; Algorithms",Article,Scopus,2-s2.0-84857791479
"Fürnstahl P., Székely G., Gerber C., Hodler J., Snedeker J.G., Harders M.","Computer assisted reconstruction of complex proximal humerus fractures for preoperative planning",2012,"Medical Image Analysis",14,10.1016/j.media.2010.07.012,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858072662&doi=10.1016%2fj.media.2010.07.012&partnerID=40&md5=3dc30e631362792f12d2626376c56d50","Operative treatment of displaced fractures of the proximal humerus is among the most difficult problems in orthopedic shoulder surgery. An accurate preoperative assessment of fragment displacement is crucial for a successful joint restoration. We present a computer assisted approach to precisely quantify these displacements. The bone is virtually reconstructed by multi-fragment alignment. In case of largely displaced pieces, a reconstruction template based on the contralateral humerus is incorporated in the algorithm to determine the optimal assembly. Cadaver experiments were carried out to evaluate our approach. All cases could be successfully reconstructed with little user interaction, and only requiring a few minutes of processing time. On average, the reassembled bone geometries resulted in a translational displacement error of 1.3 ± 0.4. mm and a rotational error of 3.4 ± 2.2°, respectively. © 2010 Elsevier B.V.","Contralateral; Fracture reduction; Fracture surfaces; Proximal humerus","Bone geometry; Computer assisted; Contralateral; Fracture reduction; Fracture surfaces; Pre-operative planning; Processing time; Proximal humerus; Rotational errors; Template-based; Translational displacements; User interaction; Bone; Fracture; accuracy; algorithm; article; bone; cadaver; case report; computed tomography scanner; computer assisted tomography; fracture; geometry; human; humerus; humerus fracture; image processing; image reconstruction; joint; orthopedic surgery; preoperative evaluation; priority journal; shoulder surgery; Algorithms; Artificial Intelligence; Humans; Imaging, Three-Dimensional; Pattern Recognition, Automated; Preoperative Care; Radiographic Image Enhancement; Radiographic Image Interpretation, Computer-Assisted; Reproducibility of Results; Sensitivity and Specificity; Shoulder Fractures; Tomography, X-Ray Computed",Article,Scopus,2-s2.0-84858072662
"Shi W., Huang J., Hou Y.","Fast DOA estimation algorithm for MIMO sonar based on ant colony optimization",2012,"Journal of Systems Engineering and Electronics",14,10.1109/JSEE.2012.00022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863663513&doi=10.1109%2fJSEE.2012.00022&partnerID=40&md5=c66a454066b83a830b29fe41ec205350","The maximum likelihood (ML) estimator demonstrates remarkable performance in direction of arrival (DOA) estimation for the multiple input multiple output (MIMO) sonar. However, this advantage comes with prohibitive computational complexity. In order to solve this problem, an ant colony optimization (ACO) is incorporated into the MIMO ML DOA estimator. Based on the ACO, a novel MIMO ML DOA estimator named the MIMO ACO ML (ML DOA estimator based on ACO for MIMO sonar) with even lower computational complexity is proposed. By extending the pheromone remaining process to the pheromone Gaussian kernel probability distribution function in the continuous space, the proposed algorithm achieves the global optimum value of the MIMO ML DOA estimator. Simulations and experimental results show that the computational cost of MIMO ACO ML is only 1/6 of the MIMO ML algorithm, while maintaining similar performance with the MIMO ML method.","Ant colony optimization (ACO); Computational complexity; Direction of arrival (DOA); Maximum likelihood (ML); Multiple input multiple output (MIMO) sonar","Ant Colony Optimization (ACO); Computational costs; Continuous spaces; Direction-of-arrival estimation; DOA estimation algorithms; Gaussian kernels; Global optimum; Maximum likelihood estimator; Multiple input multiple output; Pheromone remaining process; Artificial intelligence; Computational complexity; Direction of arrival; Distribution functions; Feedback control; Maximum likelihood; MIMO systems; Sonar; Algorithms",Article,Scopus,2-s2.0-84863663513
"De Cooman G., Quaeghebeur E.","Exchangeability and sets of desirable gambles",2012,"International Journal of Approximate Reasoning",14,10.1016/j.ijar.2010.12.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858448873&doi=10.1016%2fj.ijar.2010.12.002&partnerID=40&md5=8ba285a35ac032777685fdd76aa4f8fd","Sets of desirable gambles constitute a quite general type of uncertainty model with an interesting geometrical interpretation. We give a general discussion of such models and their rationality criteria. We study exchangeability assessments for them, and prove counterparts of de Finetti's Finite and Infinite Representation Theorems. We show that the finite representation in terms of count vectors has a very nice geometrical interpretation, and that the representation in terms of frequency vectors is tied up with multivariate Bernstein (basis) polynomials. We also lay bare the relationships between the representations of updated exchangeable models, and discuss conservative inference (natural extension) under exchangeability and the extension of exchangeable sequences. © 2010 Elsevier Inc. All right reserved.","Exchangeability; Extending an exchangeable sequence; Natural extension; Representation; Sets of desirable gambles; Updating","Exchangeability; Extending an exchangeable sequence; Natural extension; Representation; Sets of desirable gambles; Updating; Artificial intelligence; Software engineering",Article,Scopus,2-s2.0-84858448873
"Álvarez del Castillo A., Santoyo E., García-Valladares O.","A new void fraction correlation inferred from artificial neural networks for modeling two-phase flows in geothermal wells",2012,"Computers and Geosciences",14,10.1016/j.cageo.2011.08.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857915080&doi=10.1016%2fj.cageo.2011.08.001&partnerID=40&md5=b6e8f9edbd91e006e620e1df4087cd3a","A new empirical void fraction correlation was developed using artificial neural network (ANN) techniques. The artificial networks were trained using the backpropagation algorithm and production data obtained from a worldwide database of geothermal wells. Wellhead pressure, steam quality, wellbore diameter, the fluid density and viscosity, and the dimensionless numbers Reynolds, Weber, and Froude were used as main input parameters. The target ANN output was defined by the optimized void fraction values (α opt), which were calculated from the numerical modeling of two-phase flow using GEOWELLS (a wellbore simulator). The Levenberg-Marquardt algorithm, the hyperbolic tangent sigmoid, and the linear activation functions were used for the development of the ANN model. The best ANN learning was achieved with an architecture of six neurons in the hidden layer, which made it possible to obtain a set of void fractions (α ANN) with a good accuracy (R 2=0.9722). These void fraction estimates were used to obtain the new correlation, which was later coupled into the simulator GEOWELLS for the prediction of pressure gradients in two-phase geothermal wells. The accuracy of the new correlation (α ANN) was evaluated by a statistical comparison between simulated pressure gradients and measured field data. These simulation results were also compared with those data calculated by using Duns-Ros and Dix correlations, which were also programmed into GEOWELLS. Pressure gradients predicted with the new α ANN correlation showed a better agreement with measured field data, which was also confirmed by the lower values of some statistical parameters (MPE, RMSE, and Theil's U). The statistical evaluation demonstrated the efficiency of the new correlation to predict void fractions and pressure gradients with a better accuracy, in comparison to the other existing correlations. These successful results suggest the use of the new correlation (α ANN) for the analysis of two-phase flow mechanisms of geothermal wells. © 2011 Elsevier Ltd.","Artificial intelligence; Geothermal energy; Liquid holdup; Pressure gradients; Simulation; Statistics","Artificial networks; Dimensionless number; Fluid densities; Hidden layers; Hyperbolic tangent; Input parameter; Levenberg-Marquardt algorithm; Linear activation function; Liquid holdup; Measured field; New correlations; Numerical modeling; Production data; Reynolds; Simulation; Statistical comparisons; Statistical evaluation; Statistical parameters; Steam quality; Wellbore; Wellhead pressures; Artificial intelligence; Computer simulation; Geothermal energy; Geothermal wells; Neural networks; Oil field equipment; Pressure gradient; Statistics; Void fraction; Two phase flow; artificial intelligence; artificial neural network; back propagation; computer simulation; correlation; flow modeling; Froude number; geothermal energy; numerical model; pressure gradient; Reynolds number; two phase flow; void ratio; wellhead",Article,Scopus,2-s2.0-84857915080
"Simões A., Sanromán A.I., Almeida J.J.","Dicionário-Aberto: A source of resources for the Portuguese language processing",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,10.1007/978-3-642-28885-2_14,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858378631&doi=10.1007%2f978-3-642-28885-2_14&partnerID=40&md5=89f506673c81d53932aaf883bc0b42e3","In this paper we describe how Dicionáio-Aberto, an online dictionary for the Portuguese language, is being used as the base to construct diverse resources that are relevant in the processing of the Portuguese language. We will briefly present its history, explaining how we got here. Then, we will describe the resources already available to download and use, followed by the discussion on the resources that are being currently developed. © 2012 Springer-Verlag.","dictionary; lexicography; open resource; thesauri","lexicography; Online dictionaries; open resource; Portuguese language processing; Portuguese languages; Glossaries; Thesauri; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84858378631
"Libkin L., Vrgoč D.","Regular expressions for data words",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,10.1007/978-3-642-28717-6_22,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858305564&doi=10.1007%2f978-3-642-28717-6_22&partnerID=40&md5=e5e424115037f22b024616d299fff4f2","In data words, each position carries not only a letter form a finite alphabet, as the usual words do, but also a data value coming from an infinite domain. There has been a renewed interest in them due to applications in querying and reasoning about data models with complex structural properties, notably XML, and more recently, graph databases. Logical formalisms designed for querying such data often require concise and easily understandable presentations of regular languages over data words. Our goal, therefore, is to define and study regular expressions for data words. As the automaton model, we take register automata, which are a natural analog of NFAs for data words. We first equip standard regular expressions with limited memory, and show that they capture the class of data words defined by register automata. The complexity of the main decision problems for these expressions (nonemptiness, membership) also turns out to be the same as for register automata. We then look at a subclass of these regular expressions that can define many properties of interest in applications of data words, and show that the main decision problems can be solved efficiently for it. © 2012 Springer-Verlag.",,"Automaton model; Data values; Decision problems; Finite alphabet; Graph database; Infinite domains; Limited memory; Logical formalism; Natural analog; Regular expressions; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84858305564
"Zhong J., Fung Y.-F.","Case study and proofs of ant colony optimisation improved particle filter algorithm",2012,"IET Control Theory and Applications",14,10.1049/iet-cta.2010.0405,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860541416&doi=10.1049%2fiet-cta.2010.0405&partnerID=40&md5=ab6c44a240cecd27b3c82eb5d785d9ab","Particle filters (PF), as a kind of non-linear/non-Gaussian estimation method, are suffering from two problems in large-dimensional cases, namely particle impoverishment and sample size dependency. Previous studies from the authors have proposed a novel PF algorithm that incorporates ant colony optimisation (PF ACO), to alleviate these problems. In this paper the authors will provide a theoretical foundation of this new algorithm; two theorems are introduced to validate that the PF ACO introduces smaller Kullback-Leibler divergence (K-L divergence) between the proposal distribution and the optimal one compared to those produced by the generic PF. In addition, with the same threshold level, the PF ACO has a higher probability than the generic PF to achieve a certain K-L divergence. A mobile robot localisation experiment is applied to examine the performance between various PF schemes. © 2012 The Institution of Engineering and Technology.",,"Ant colony optimisation; Estimation methods; Improved particle filter; Kullback Leibler divergence; Particle filter; Proposal distribution; Robot localisation; Sample sizes; Theoretical foundations; Threshold levels; Artificial intelligence; Distributed computer systems; Nonlinear filtering; Optimization; Target tracking; Algorithms",Article,Scopus,2-s2.0-84860541416
"Hetzl S.","Applying tree languages in proof theory",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,10.1007/978-3-642-28332-1_26,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857818707&doi=10.1007%2f978-3-642-28332-1_26&partnerID=40&md5=de6bee84be4f006c2ab243bdb26bb019","We introduce a new connection between formal language theory and proof theory. One of the most fundamental proof transformations in a class of formal proofs is shown to correspond exactly to the computation of the language of a certain class of tree grammars. Translations in both directions, from proofs to grammars and from grammars to proofs, are provided. This correspondence allows theoretical as well as practical applications. © 2012 Springer-Verlag.",,"Formal language theory; Formal proofs; Proof theory; Proof transformation; Tree grammars; Tree languages; Formal languages; Forestry; Algorithms; Artificial Intelligence; Information Retrieval; Languages; Translation",Conference Paper,Scopus,2-s2.0-84857818707
"Leurent G., Roy A.","Boomerang attacks on hash function using auxiliary differentials",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,10.1007/978-3-642-27954-6_14,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857737973&doi=10.1007%2f978-3-642-27954-6_14&partnerID=40&md5=1bae0a493aaa0007fbddb137d9961448","In this paper we study boomerang attacks in the chosen-key setting. This is particularly relevant to hash function analysis, since many boomerang attacks have been described against ARX-based designs. We present a new way to combine message modifications, or auxiliary differentials, with the boomerang attack. We show that under some conditions, we can combine three independent paths instead of two for the classical boomerang attack. Our main result is obtained by applying this technique to round-reduced Skein-256, for which we show a distinguisher on the keyed permutation with complexity only 2 57, and a distinguisher on the compression function with complexity 2 114. We also discuss application of the technique to Skein-512 and show some problems with the paths used in previous boomerang analysis of Skein-512. © 2012 Springer-Verlag.","boomerang attack; chosen-key; Hash function; higher order differential; SHA-3 competition; Skein; Threefish; zero-sum","Boomerang attack; chosen-key; higher order differential; Sha-3 competitions; Skein; Threefish; zero-sum; Artificial intelligence; Hash functions",Conference Paper,Scopus,2-s2.0-84857737973
"Chang W.-Y., Lantz V.A., Hennigar C.R., Maclean D.A.","Economic impacts of forest pests: A case study of spruce budworm outbreaks and control in New Brunswick, Canada",2012,"Canadian Journal of Forest Research",14,10.1139/X11-190,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857533630&doi=10.1139%2fX11-190&partnerID=40&md5=74171dad56fc3d523dda4082214c6f6c","We investigated the potential economic impacts of future spruce budworm (SBW) (Choristoneura fumiferana (Clemens)) outbreaks on 2.8 million hectares of Crown forest land in New Brunswick by coupling an advanced Spruce Budworm Decision Support System (SBW DSS) model with a dynamic computable general equilibrium model. A total of 16 alternative scenarios were evaluated, including two SBW outbreak severities (moderate versus severe), four SBW control program levels (protecting 0%, 10%, 20%, and 40% of susceptible Crown land forest area), and two pest management strategies (""without"" versus ""with"" replanning harvest scheduling and salvage). The ""without"" replanning harvest scheduling and salvage strategy findings indicated that, under uncontrolled moderate and severe SBW outbreaks, total output in the New Brunswick economy over the 2012-2041 period would decline in present-value terms by CDN$3.3 billion and $4.7 billion, respectively. SBW control via aerial spraying was shown to reduce the negative impacts on output by up to 66% when protecting 40% of susceptible area. Combining SBW control with replanning harvest scheduling and salvage strategy under moderate and severe outbreaks would reduce the negative impacts on output by a further 1%-18% depending on the level of control implemented. These findings can help forest managers assess the direct and indirect economic effects of forest pest disturbances on regional economies and can also be used together with other sustainable forest management indicators to help broaden the scope of SBW and other forest pest management decision-making.",,"Aerial spraying; Choristoneura fumiferana; Computable general equilibrium model; Control program; Economic impacts; Forest area; Forest land; Forest managers; Forest pest; Forest pest management; Harvest scheduling; Negative impacts; Pest management; Re-planning; Regional economy; Spruce budworm; Sustainable forest management; Artificial intelligence; Civil aviation; Decision support systems; Economic and social effects; Harvesting; Scheduling; Forestry; decision making; decision support system; economic impact; forest management; future prospect; insect; integrated pest management; pest control; pest outbreak; regional economy; sustainable forestry; Forestry; Harvesting; Scheduling; Canada; New Brunswick; Choristoneura fumiferana; Picea",Article,Scopus,2-s2.0-84857533630
"Both F., Hoogendoorn M., Van Der Mee A., Treur J., De Vos M.","An intelligent agent model with awareness of workflow progress",2012,"Applied Intelligence",14,10.1007/s10489-010-0273-9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862136980&doi=10.1007%2fs10489-010-0273-9&partnerID=40&md5=2a20f22ae67441327864aaf29556f934","To support human functioning, ambient intelligent agents require knowledge about the tasks executed by the human. This knowledge includes design-time information like: (i) the goal of a task and (ii) the alternative ways for a human to achieve that goal, as well as run-time information such as the choices made by a human during task execution. In order to provide effective support, the agent must know exactly what steps the human is following. However, if not all steps along the path can be observed, it is possible that the agent cannot uniquely derive which path the human is following. Furthermore, in order to provide timely support, the agent must observe, reason, conclude and support within a limited period of time. To deal with these problems, this paper presents a generic focused reasoning mechanism to enable a guided selection of the path which is most likely followed by the human. This mechanism is based upon knowledge about the human and the workflow to perform the task. In order to come to such an approach, a reasoning mechanism is adopted in combination with the introduction of a new workflow representation, which is utilized to focus the reasoning process in an appropriate manner. The approach is evaluated by means of an extensive case study. © Springer Science+Business Media, LLC 2010.","Ambient agent; Awareness; Model-based reasoning; Task progress; Workflow model","Ambient intelligent; Awareness; Intelligent agent model; Model-based Reasoning; Reasoning mechanism; Reasoning process; Run-time information; Task executions; Task progress; Workflow models; Artificial intelligence; Intelligent agents",Article,Scopus,2-s2.0-84862136980
"Cococcioni M., Corucci L., Masini A., Nardelli F.","SVME: An ensemble of support vector machines for detecting oil spills from full resolution MODIS images",2012,"Ocean Dynamics",14,10.1007/s10236-011-0510-8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861100993&doi=10.1007%2fs10236-011-0510-8&partnerID=40&md5=6f4b4bf766d21a5789f68ece2f8b170a","This paper addresses oil spill detection from remotely sensed optical images. In particular, it focuses on the automatic classification of regions of interest (ROIs) in two classes, namely oil spills or look-alikes. Candidate regions and the corresponding boundaries have been manually identified from full resolution Moderate Resolution Imaging Spectroradiometer images, related to the Mediterranean Sea over the years 2008 and 2009. Then, a set of features has been extracted from each ROI, allowing to formulate the oil spill detection problem as a two-class classification task on the provided regions (i.e. using a supervised learning strategy). Since ROI classification is challenging, some desired characteristics for the classification algorithm are first identified, such as accuracy, robustness, etc. Then, a solution (called SVME) is provided: it is based on an ensemble of incremental/decremental cost-oriented Support Vector Machines, aggregated with the Receiving Operating Characteristic (ROC) convex hull method in the ROC space. Such a solution addresses all the desired characteristics. Finally, the results obtained on the collected dataset are shown. The importance of this study is the devising of a powerful classification technique that may have an impact on optical oil spill detection from space, especially if fused with satellite synthetic aperture radar data. Moreover, it is shown how the proposed system can be used as a decision support tool, to help a junior operator in making more reliable detections. © 2011 Springer Science+Business Media, LLC.","Classifier ensemble; Decision support systems; Incremental and decremental learning; Multi-spectral optical remotely sensed images; Oil spill detection; Receiving Operating Characteristic (ROC); ROC convex hull; Support Vector Machines (SVMs); Time-varying two-class classification","Classifier ensembles; Convex hull; Incremental and decremental learning; Oil spill detection; Receiving operating characteristics; Remotely sensed images; Time varying; Artificial intelligence; Computational geometry; Decision support systems; Geometrical optics; Radiometers; Remote sensing; Satellite imagery; Support vector machines; Synthetic aperture radar; Oil spills; accuracy assessment; algorithm; data set; image classification; machinery; MODIS; oil spill; optical method; radar; remote sensing; resolution; satellite imagery; Mediterranean Sea",Article,Scopus,2-s2.0-84861100993
"Boniol F., Cassé H., Noulard E., Pagetti C.","Deterministic execution model on COTS hardware",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,10.1007/978-3-642-28293-5_9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857392612&doi=10.1007%2f978-3-642-28293-5_9&partnerID=40&md5=9262c32121b7d9a3a87f8176e926a961","In order to be able to use multicore COTS hardware in critical systems, we put forward a time-oriented execution model and provide a general framework for programming and analysing a multicore compliant with the execution model. © 2012 Springer-Verlag.",,"Critical systems; Deterministic execution; Execution model; Multi core; Artificial intelligence; Multicore programming",Conference Paper,Scopus,2-s2.0-84857392612
"Cmorik R., Jirásková G.","Basic operations on binary suffix-free languages",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,10.1007/978-3-642-25929-6_9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856894608&doi=10.1007%2f978-3-642-25929-6_9&partnerID=40&md5=b77bc67e220ff2c3575d485942b2f1e0","We give a characterization of nondeterministic automata accepting suffix-free languages, and a sufficient condition on deterministic automata to accept suffix-free languages. Then we investigate the state complexity of basic operations on binary suffix-free regular languages. In particular, we show that the upper bounds on the state complexity of all the boolean operations as well as of Kleene star are tight in the binary case. On the other hand, we prove that the bound for reversal cannot be met by binary languages. This solves several open questions stated by Han and Salomaa (Theoret. Comput. Sci. 410, 2537-2548, 2009). © 2012 Springer-Verlag Berlin Heidelberg.",,"Basic operation; Binary languages; Boolean operations; Deterministic automata; Kleene star; Nondeterministic automata; State complexity; Sufficient conditions; Suffix-free; Upper Bound; Artificial intelligence; Computer science",Conference Paper,Scopus,2-s2.0-84856894608
"Hameed I.A., Bochtis D.D., Sørensen C.G., Vougioukas S.","An object-oriented model for simulating agricultural in-field machinery activities",2012,"Computers and Electronics in Agriculture",14,10.1016/j.compag.2011.11.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-82255195422&doi=10.1016%2fj.compag.2011.11.003&partnerID=40&md5=6fe6d370d82e9b44780bf5bc5375e3cf","Field operations planning is essential for the operational efficiency in terms of time and cost, especially in complex operations involving capacity constraints and cooperating units. In order to deal with such type of planning problems an object-oriented simulation model which simulates in detail in-field machine activities during the execution phase, was developed and applied. The developed simulation model regards the material input operations with capacity constraints, where a quantity of a ""commodity"" is transported by the machine and is distributed in the field area. The case of the organic fertilizing application was used as the basis for the description of the object oriented model involving the description of all related programming entities (i.e., classes, attributes, and methods). Combined state and activity diagrams were provided as the basic elements of the simulation model. It was shown that the simulation model provides all the key operational parameters necessary for the evaluation of a selected scenario. This makes it feasible the use of the simulation model as an integral part of a decision support system for advising farmers about in-field operational decisions (e.g., traffic system, driving direction, refilling position) and machinery dimensioning (e.g., tank capacity, operating width, etc.). © 2011 Elsevier B.V.","DSS; Field efficiency; Machinery management","Activity diagram; Basic elements; Capacity constraints; Combined state; Complex operations; DSS; Execution phase; Field efficiency; Field operation; In-field; Integral part; Machinery management; Object oriented model; Object oriented simulation; Operational decisions; Operational efficiencies; Operational parameters; Planning problem; Simulation model; Tank capacity; Traffic systems; Agricultural machinery; Artificial intelligence; Decision support systems; Object oriented programming; Computer simulation; agricultural technology; agricultural worker; computer simulation; decision support system; efficiency measurement; fertilizer application; machinery; organic farming; parameterization; performance assessment; planning practice; software",Article,Scopus,2-s2.0-82255195422
"García-Garrido M.A., Ocaña M., Llorca D.F., Arroyo E., Pozuelo J., Gavilán M.","Complete vision-based traffic sign recognition supported by an I2V communication system",2012,"Sensors",14,10.3390/s120201148,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857566972&doi=10.3390%2fs120201148&partnerID=40&md5=6daa2a3b11957bfc6439ddd0a0fb4f68","This paper presents a complete traffic sign recognition system based on vision sensor onboard a moving vehicle which detects and recognizes up to one hundred of the most important road signs, including circular and triangular signs. A restricted Hough transform is used as detection method from the information extracted in contour images, while the proposed recognition system is based on Support Vector Machines (SVM). A novel solution to the problem of discarding detected signs that do not pertain to the host road is proposed. For that purpose infrastructure-to-vehicle (I2V) communication and a stereo vision sensor are used. Furthermore, the outputs provided by the vision sensor and the data supplied by the CAN Bus and a GPS sensor are combined to obtain the global position of the detected traffic signs, which is used to identify a traffic sign in the I2V communication. This paper presents plenty of tests in real driving conditions, both day and night, in which an average detection rate over 95% and an average recognition rate around 93% were obtained with an average runtime of 35 ms that allows real-time performance. © 2012 by the authors; licensee MDPI, Basel, Switzerland.","Advanced driver assistance systems; Computer vision; I2V; Traffic sign recognition","article; artificial intelligence; automated pattern recognition; car driving; computer assisted diagnosis; computer interface; construction work and architectural phenomena; equipment; equipment design; instrumentation; methodology; travel; wireless communication; Artificial Intelligence; Automobile Driving; Equipment Design; Equipment Failure Analysis; Image Interpretation, Computer-Assisted; Location Directories and Signs; Pattern Recognition, Automated; Travel; User-Computer Interface; Wireless Technology",Article,Scopus,2-s2.0-84857566972
"Aristondo O., García-Lapresta J.L., Lasso De La Vega C., Pereira R.A.M.","The gini index, the dual decomposition of aggregation functions, and the consistent measurement of inequality",2012,"International Journal of Intelligent Systems",14,10.1002/int.21517,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855165483&doi=10.1002%2fint.21517&partnerID=40&md5=e20bffeea4aeb434a3ca2af3cf07b395","In several economic fields, such as those related to health, education, or poverty, the individuals' characteristics are measured by bounded variables. Accordingly, these characteristics may be indistinctly represented by achievements or shortfalls. A difficulty arises when inequality needs to be assessed. One may focus either on achievements or on shortfalls, but the respective inequality rankings may lead to contradictory results. Specifically, this paper concentrates on the poverty measure proposed by Sen. According to this measure, inequality among the poor is captured by the Gini index. However, the rankings obtained by the Gini index applied to either the achievements or the shortfalls do not coincide in general. To overcome this drawback, we show that an ordered weighted averaging (OWA) operator is underlying in the definition of the Sen measure. The dual decomposition of the OWA operators into a self-dual core and anti-self-dual remainder allows us to propose an inequality component which measures consistently the achievement and shortfall inequality among the poor. Copyright © 2011 Wiley Periodicals, Inc.",,"Aggregation functions; Bounded variables; Dual decomposition; Economic fields; Gini Index; Ordered weighted averaging operator; OWA operators; Self-dual; Artificial intelligence; Software engineering; Codes (symbols)",Conference Paper,Scopus,2-s2.0-84855165483
"Lashkari D., Sridharan R., Vul E., Hsieh P.-J., Kanwisher N., Golland P.","Search for patterns of functional specificity in the brain: A nonparametric hierarchical Bayesian model for group fMRI data",2012,"NeuroImage",14,10.1016/j.neuroimage.2011.08.031,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-83055184786&doi=10.1016%2fj.neuroimage.2011.08.031&partnerID=40&md5=8c200f397139ddbbff017de3a46798ec","Functional MRI studies have uncovered a number of brain areas that demonstrate highly specific functional patterns. In the case of visual object recognition, small, focal regions have been characterized with selectivity for visual categories such as human faces. In this paper, we develop an algorithm that automatically learns patterns of functional specificity from fMRI data in a group of subjects. The method does not require spatial alignment of functional images from different subjects. The algorithm is based on a generative model that comprises two main layers. At the lower level, we express the functional brain response to each stimulus as a binary activation variable. At the next level, we define a prior over sets of activation variables in all subjects. We use a Hierarchical Dirichlet Process as the prior in order to learn the patterns of functional specificity shared across the group, which we call functional systems, and estimate the number of these systems. Inference based on our model enables automatic discovery and characterization of dominant and consistent functional systems. We apply the method to data from a visual fMRI study comprised of 69 distinct stimulus images. The discovered system activation profiles correspond to selectivity for a number of image categories such as faces, bodies, and scenes. Among systems found by our method, we identify new areas that are deactivated by face stimuli. In empirical comparisons with previously proposed exploratory methods, our results appear superior in capturing the structure in the space of visual categories of stimuli. © 2011 Elsevier Inc.","Category selectivity; Clustering; FMRI; High level vision","algorithm; article; automation; Bayes theorem; brain depth stimulation; brain function; controlled study; functional magnetic resonance imaging; image analysis; image processing; image reconstruction; mathematical model; nonparametric test; optical resolution; priority journal; reproducibility; signal detection; stimulus response; voxel based morphometry; Algorithms; Artificial Intelligence; Bayes Theorem; Evoked Potentials, Visual; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Visual Cortex; Visual Perception",Article,Scopus,2-s2.0-83055184786
"Hossain M.S., Gresock J., Edmonds Y., Helm R., Potts M., Ramakrishnan N.","Connecting the dots between PubMed abstracts",2012,"PLoS ONE",14,10.1371/journal.pone.0029509,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855336732&doi=10.1371%2fjournal.pone.0029509&partnerID=40&md5=e8c12ecaafc59ad90e6ce0fdcedeb826","Background: There are now a multitude of articles published in a diversity of journals providing information about genes, proteins, pathways, and diseases. Each article investigates subsets of a biological process, but to gain insight into the functioning of a system as a whole, we must integrate information from multiple publications. Particularly, unraveling relationships between extra-cellular inputs and downstream molecular response mechanisms requires integrating conclusions from diverse publications. Methodology: We present an automated approach to biological knowledge discovery from PubMed abstracts, suitable for ""connecting the dots"" across the literature. We describe a storytelling algorithm that, given a start and end publication, typically with little or no overlap in content, identifies a chain of intermediate publications from one to the other, such that neighboring publications have significant content similarity. The quality of discovered stories is measured using local criteria such as the size of supporting neighborhoods for each link and the strength of individual links connecting publications, as well as global metrics of dispersion. To ensure that the story stays coherent as it meanders from one publication to another, we demonstrate the design of novel coherence and overlap filters for use as post-processing steps. Conclusions: We demonstrate the application of our storytelling algorithm to three case studies: i) a many-one study exploring relationships between multiple cellular inputs and a molecule responsible for cell-fate decisions, ii) a many-many study exploring the relationships between multiple cytokines and multiple downstream transcription factors, and iii) a one-to-one study to showcase the ability to recover a cancer related association, viz. the Warburg effect, from past literature. The storytelling pipeline helps narrow down a scientist's focus from several hundreds of thousands of relevant documents to only around a hundred stories. We argue that our approach can serve as a valuable discovery aid for hypothesis generation and connection exploration in large unstructured biological knowledge bases. © 2012 Hossain et al.",,"cytokine; transcription factor; abstract report; aging; algorithm; article; automation; hypothesis; knowledge base; knowledge discovery; medical literature; Medline; publication; algorithm; animal; artificial intelligence; biology; cluster analysis; data mining; glycolysis; human; metabolism; methodology; neoplasm; pathology; publication; Aging; Algorithms; Animals; Artificial Intelligence; Cluster Analysis; Computational Biology; Cytokines; Data Mining; Glycolysis; Humans; Neoplasms; Publications; PubMed; Transcription Factors",Article,Scopus,2-s2.0-84855336732
"Pendharkar P.C.","Game theoretical applications for multi-agent systems",2012,"Expert Systems with Applications",14,10.1016/j.eswa.2011.07.017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-81855176009&doi=10.1016%2fj.eswa.2011.07.017&partnerID=40&md5=dda8e1f7e2162c926153bee9719be97b","We consider game-theoretic principles for design of cooperative and competitive (non-cooperative self-interested) multi-agent systems. Using economic concepts of tâtonnement and economic core, we show that cooperative multi-agent systems should be designed in games with dominant strategies that may lead to social dilemmas. Non-cooperative multi-agent systems, on the other hand, should be designed for games with no clear dominant strategies and high degree of problem complexity. Further, for non-cooperative multi-agent systems, the number of learning agents should be carefully managed so that solutions in the economic core can be obtained. We provide experimental results for the design of cooperative and non-cooperative MAS from telecommunication and manufacturing industries. © 2011 Elsevier Ltd. All rights reserved.","Distributed artificial intelligence; Game theory; Multi-agent systems","Distributed Artificial Intelligence; Dominant strategy; Learning agents; Manufacturing industries; Non-cooperative; Problem complexity; Social dilemmas; Game theory; Intelligent agents; Multi agent systems",Article,Scopus,2-s2.0-81855176009
"Schultz T.","Learning a reliable estimate of the number of fiber directions in diffusion MRI",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872896823&partnerID=40&md5=35c34bbe4758ab8ff5349f58f75e74e6","Having to determine an adequate number of fiber directions is a fundamental limitation of multi-compartment models in diffusion MRI. This paper proposes a novel strategy to approach this problem, based on simulating data that closely follows the characteristics of the measured data. This provides the ground truth required to determine the number of directions that optimizes a formal measure of accuracy, while allowing us to transfer the result to real data by support vector regression. The method is shown to result in plausible and reproducible decisions on three repeated scans of the same subject. When combined with the ball-and-stick model, it produces directional estimates comparable to constrained spherical deconvolution, but with significantly smaller variance between re-scans, and at a reduced computational cost. © Springer-Verlag Berlin Heidelberg 2012.",,"Magnetic resonance imaging; Medical imaging; Computational costs; Fiber direction; Fundamental limitations; Multi-compartment models; Novel strategies; Reliable estimates; Spherical deconvolution; Support vector regression (SVR); Medical computing; algorithm; article; artificial intelligence; automated pattern recognition; brain; computer assisted diagnosis; connectome; cytology; diffusion tensor imaging; human; image enhancement; methodology; myelinated nerve; reproducibility; sensitivity and specificity; ultrastructure; Algorithms; Artificial Intelligence; Brain; Connectome; Diffusion Tensor Imaging; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Nerve Fibers, Myelinated; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Conference Paper,Scopus,2-s2.0-84872896823
"Chahuara P., Fleury A., Portet F., Vacher M.","Using markov logic network for on-line activity recognition from non-visual home automation sensors",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017105935&partnerID=40&md5=c747007c71c459f7c6390083a1ac49d2","This paper presents the application of Markov Logic Networks(MLN) for the the recognition of Activities of Daily Living (ADL) in a smart home. We describe a procedure that uses raw data from non visual and non wearable sensors in order to create a classification model leveraging logic formal representation and probabilistic inference. SVM and Naive Bayes methods were used as baselines to compare the performance of our implementation, as they have proved to be highly efficient in classification tasks. The evaluation was carried out on a real smart home where 21 participants performed ADLs. Results show not only the appreciable capacities of MLN as a classifier, but also its potential to be easily integrable into a formal knowledge representation framework. © Springer-Verlag Berlin Heidelberg 2012.","Activity recognition; Ambient assisted living; Markov logic network; Smart home; Support vector machine","Artificial intelligence; Assisted living; Automation; Computer circuits; Intelligent buildings; Knowledge representation; Markov processes; Pattern recognition; Probabilistic logics; Support vector machines; Wearable sensors; Activities of Daily Living; Activity recognition; Ambient assisted living; Formal knowledge representations; Formal representations; Markov logic networks; Probabilistic inference; Smart homes; Ambient intelligence",Conference Paper,Scopus,2-s2.0-85017105935
"Sehili M.A., Lecouteux B., Vacher M., Portet F., Istrate D., Dorizzi B., Boudy J.","Sound environment analysis in smart home",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012239799&partnerID=40&md5=2c518d483cb41764e4c8067de746ae7e","This study aims at providing audio-based interaction technology that lets the users have full control over their home environment, at detecting distress situations and at easing the social inclusion of the elderly and frail population. The paper presents the sound and speech analysis system evaluated thanks to a corpus of data acquired in a real smart home environment. The 4 steps of analysis are signal detection, speech/sound discrimination, sound classification and speech recognition. The results are presented for each step and globally. The very first experiments show promising results be it for the modules evaluated independently or for the whole system. © Springer-Verlag Berlin Heidelberg 2012.","Smart home; Sound analysis; Sound detection; Sound recognition; Speech distant recognition","Ambient intelligence; Artificial intelligence; Automation; Intelligent buildings; Interaction technology; Smart homes; Social inclusion; Sound analysis; Sound classification; Sound detection; Sound environment; Sound recognition; Speech recognition",Conference Paper,Scopus,2-s2.0-85012239799
"Mukkamala P., Pálvölgyi D.","Drawing cubic graphs with the four basic slopes",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,10.1007/978-3-642-25878-7_25,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84455182339&doi=10.1007%2f978-3-642-25878-7_25&partnerID=40&md5=8ddc72a2a371b7b3eed1686037818036","We show that every cubic graph can be drawn in the plane with straight-line edges using only the four basic slopes, {0,π/4,π/2,3π/4}. We also prove that four slopes have this property if and only if we can draw K 4 with them. © 2012 Springer-Verlag Berlin Heidelberg.",,"Cubic graph; Cubic graph; Artificial intelligence; Artificial intelligence; Computer science; Computers; Drawing (graphics); Drawing (graphics)",Conference Paper,Scopus,2-s2.0-84455182339
"Gjoreski H., Luštrek M., Gams M.","Context-based fall detection using inertial and location sensors",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017141264&partnerID=40&md5=3557d56e883ed00d1886f4d474913e4b","Falls are some of the most common sources of injury among the elderly. A fall is particularly critical when the elderly person is injured and cannot call for help. This problem is addressed by many fall-detection systems, but they often focus on isolated falls under restricted conditions, neglecting complex, real-life situations. In this paper a combination of body-worn inertial and location sensors for fall detection is studied. A novel context-based method that exploits the information from both types of sensors is designed. The evaluation is performed on a real-life scenario, including fast falls, slow falls and fall-like situations that are difficult to distinguish from falls. All the possible combinations of six inertial and four location sensors are tested. The results show that: (i) context-based reasoning significantly improves the performance; (ii) a combination of two types of sensors in a single physical sensor enclosure seems to be the best practical solution. © Springer-Verlag Berlin Heidelberg 2012.","Activity recognition; Context-based reasoning; Fall detection; Inertial sensors; Location sensors","Ambient intelligence; Artificial intelligence; Location; Activity recognition; Context-based reasoning; Fall detection; Inertial sensor; Location sensors; Wearable sensors",Conference Paper,Scopus,2-s2.0-85017141264
"Hoover A.K., Szerlip P.A., Norton M.E., Brindle T.A., Merritt Z., Stanley K.O.","Generating a complete multipart musical composition from a single monophonic melody with functional scaffolding",2012,"Proceedings of the 3rd International Conference on Computational Creativity, ICCC 2012",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984849093&partnerID=40&md5=a75d68eb1660c6fe8fb679edaa57332d","This paper advances the state of the art for a computer-assisted approach to music generation called functional scaffolding for musical composition (FSMC), whose representation facilitates creative combination, exploration, and transformation of musical concepts. Music in FSMC is represented as a functional relationship between an existing human composition, or scaffold, and a generated accompaniment. This relationship is encoded by a type of artificial neural network called a compositional pattern producing network (CPPN). A human user without any musical expertise can then explore how accompaniment should relate to the scaffold through an interactive evolutionary process akin to animal breeding. While the power of such a functional representation has previously been shown to constrain the search to plausible accompaniments, this study goes further by showing that the user can tailor complete multipart arrangements from only a single original monophonic track provided by the user, thus enabling creativity without the need for musical expertise.",,"Artificial intelligence; Neural networks; Compositional pattern-producing networks; Computer assisted; Evolutionary process; Functional relationship; Functional representation; Musical composition; Musical concepts; State of the art; Scaffolds",Conference Paper,Scopus,2-s2.0-84984849093
"Spielman D.A., Wang H., Wright J.","Exact recovery of sparsely-used dictionaries",2012,"Journal of Machine Learning Research",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885851970&partnerID=40&md5=2d38f71a75b9e12d233b5b51a86d35fb","We consider the problem of learning sparsely used dictionaries with an arbitrary square dictionary and a random, sparse coefficient matrix. We prove that O(n log n) samples are sufficient to uniquely determine the coefficient matrix. Based on this proof, we design a polynomial-time algorithm, called Exact Recovery of Sparsely-Used Dictionaries (ER-SpUD), and prove that it probably recovers the dictionary and coefficient matrix when the coefficient matrix is sufficiently sparse. Simulation results show that ER-SpUD reveals the true dictionary as well as the coefficients with probability higher than many state-of-the-art algorithms. © 2012 D.A. Spielman, H. Wang & J. Wright.","Dictionary learning; Matrix decomposition; Matrix sparsification","Artificial intelligence; Software engineering; Coefficient matrix; Dictionary learning; Exact recoveries; Matrix decomposition; Polynomial-time algorithms; Sparsification; State-of-the-art algorithms; Algorithms",Conference Paper,Scopus,2-s2.0-84885851970
"Salvi G., Montesano L., Bernardino A., Santos-Victor J.","Language bootstrapping: Learning word meanings from perception-action association",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",14,10.1109/TSMCB.2011.2172420,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861192014&doi=10.1109%2fTSMCB.2011.2172420&partnerID=40&md5=b930a8ccc0caabb52b3615c842335346","We address the problem of bootstrapping language acquisition for an artificial system similarly to what is observed in experiments with human infants. Our method works by associating meanings to words in manipulation tasks, as a robot interacts with objects and listens to verbal descriptions of the interactions. The model is based on an affordance network, i.e., a mapping between robot actions, robot perceptions, and the perceived effects of these actions upon objects. We extend the affordance model to incorporate spoken words, which allows us to ground the verbal symbols to the execution of actions and the perception of the environment. The model takes verbal descriptions of a task as the input and uses temporal co-occurrence to create links between speech utterances and the involved objects, actions, and effects. We show that the robot is able form useful word-to-meaning associations, even without considering grammatical structure in the learning process and in the presence of recognition errors. These word-to-meaning associations are embedded in the robot's own understanding of its actions. Thus, they can be directly used to instruct the robot to perform tasks and also allow to incorporate context in the speech recognition task. We believe that the encouraging results with our approach may afford robots with a capacity to acquire language descriptors in their operation's environment as well as to shed some light as to how this challenging process develops with human infants. © 2012 IEEE.","Affordances; automatic speech recognition; Bayesian networks; cognitive robotics; grasping; humanoid robots; language; unsupervised learning","Affordances; Automatic speech recognition; Cognitive robotics; grasping; Humanoid robot; language; Anthropomorphic robots; Bayesian networks; Unsupervised learning; Speech recognition; algorithm; article; artificial intelligence; automated pattern recognition; biomimetics; computer simulation; decision support system; human; infant; methodology; natural language processing; robotics; theoretical model; Algorithms; Artificial Intelligence; Biomimetics; Computer Simulation; Decision Support Techniques; Humans; Infant; Models, Theoretical; Natural Language Processing; Pattern Recognition, Automated; Robotics",Article,Scopus,2-s2.0-84861192014
"Westerveld A.J., Schouten A.C., Veltink P.H., Van Der Kooij H.","Selectivity and resolution of surface electrical stimulation for grasp and release",2012,"IEEE Transactions on Neural Systems and Rehabilitation Engineering",14,10.1109/TNSRE.2011.2178749,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856414552&doi=10.1109%2fTNSRE.2011.2178749&partnerID=40&md5=fb9ab1725f264c8cd933980ed97c567b","Electrical stimulation of arm and hand muscles can be a functional tool for patients with motor dysfunction. Sufficient stimulation of finger and thumb musculature can support natural grasping function. Yet it remains unclear how different grasping movements can be selectively supported by electrical stimulation. The goal of this study is to determine to what extent activation of individual fingers is possible with surface electrical stimulation for the purpose of rehabilitation following stroke. The extensor digitorum communis (EDC) muscle, flexor pollicis longus (FPL) muscle, and the thenar muscle group, all involved in grasp and release, were selected for stimulation. The evoked forces in individual fingers were measured. Stimulation thresholds and selective ranges were determined for each subject. Electrode locations where the highest selective range occurred were compared between subjects and influences of different isometric wrist positions were assessed. In all subjects selective stimulation of middle finger extension and thumb flexion was possible. In addition, selective stimulation of index and ring finger extension was possible in most cases. In nine out of the ten EDC subjects we were able to stimulate three or all four fingers selectively. However, large variability in electrode locations for high selectivity was observed between the subjects. Within the designs of grasping prostheses and grasping rehabilitation devices, the variability of electrode locations should be taken into account. The results of our study facilitate the optimization of such designs and favour a design which allows individualized stimulation locations. © 2006 IEEE.","Electrophysiology; muscle contraction; noninvasive therapeutic techniques; peripheral nerve","Electrical stimulations; Grasping movement; Hand muscles; High selectivity; muscle contraction; Peripheral nerves; Rehabilitation devices; Ring finger; Therapeutic techniques; Design; Electrophysiology; Noninvasive medical procedures; Muscle; adult; algorithm; arm; article; artificial intelligence; daily life activity; elbow; electrode; electrostimulation; female; finger; hand; hand strength; human; individuality; instrumentation; male; muscle contraction; muscle isometric contraction; physiology; prostheses and orthoses; prosthesis; rehabilitation; skeletal muscle; thumb; wrist; Activities of Daily Living; Adult; Algorithms; Arm; Artificial Intelligence; Elbow Joint; Electric Stimulation; Electrodes; Female; Fingers; Hand; Hand Strength; Humans; Individuality; Isometric Contraction; Male; Muscle Contraction; Muscle, Skeletal; Prostheses and Implants; Prosthesis Design; Rehabilitation; Thumb; Wrist Joint; Young Adult",Article,Scopus,2-s2.0-84856414552
"Wang N., Melchior J., Wiskott L.","An analysis of gaussian-binary restricted boltzmann machines for natural images",2012,"ESANN 2012 proceedings, 20th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865581203&partnerID=40&md5=cf575564472093be16e2a39d731751e3","A Gaussian-binary restricted Boltzmann machine is a widely used energy-based model for continuous data distributions, although many authors reported difficulties in training on natural images. To clarify the model’s capabilities and limitations we derive a rewritten formula of the probability density function as a linear superposition of Gaussians. Based on this formula we show how Gaussian-binary RBMs learn natural image statistics. However the probability density function of the model is not a good representation of the data distribution. © 2012, i6doc.com publication. All rights reserved.",,"Artificial intelligence; Discrete cosine transforms; Gaussian distribution; Learning systems; Neural networks; Probability distributions; Continuous data; Data distribution; Energy-based models; Gaussians; Linear superpositions; Natural image statistics; Natural images; Restricted boltzmann machine; Probability density function",Conference Paper,Scopus,2-s2.0-84865581203
"Spüler M., Rosenstiel W., Bogdan M.","One class SVM and canonical correlation analysis increase performance in a c-VEP based brain-computer interface (BCI)",2012,"ESANN 2012 proceedings, 20th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870862844&partnerID=40&md5=d8d8ff4f5e448f94aeff61c3e5837c3f","The goal of a Brain-Computer Interface (BCI) is to enable communication by pure brain activity without the need for muscle control. Recently BCIs based on code-modulated visual evoked potentials (c-VEPs) have shown great potential to establish high-performance communication. In this paper we present two new methods to improve classification in a c-VEP BCI. Canonical correlation analysis can be used to build an optimal spatial filter for detection of c-VEPs, while the use of a one class support vector machine (OCSVM) makes the BCI more robust in terms of artefacts and thus increases performance. We show both methods to increase performance in an offine analysis on data from 8 subjects. As a proof of concept both methods are tested online with one subject, who achieved an average performance of 133 bit/min, which is higher than any other bitrate reported so far for a non-invasive BCI. © 2012, i6doc.com publication. All rights reserved.",,"Artificial intelligence; Brain; C (programming language); Correlation methods; Electrophysiology; Interfaces (computer); Learning systems; Neural networks; Support vector machines; Brain activity; Canonical correlation analysis; High performance communication; Muscle controls; One-class support vector machines (OCSVM); Optimal spatial filter; Proof of concept; Visual evoked potential; Brain computer interface",Conference Paper,Scopus,2-s2.0-84870862844
"Lin H.-Y., Ann Chen Y., Tsai Y.-Y., Qu X., Tseng T.-S., Park J.Y.","TRM: A Powerful Two-Stage Machine Learning Approach for Identifying SNP-SNP Interactions",2012,"Annals of Human Genetics",14,10.1111/j.1469-1809.2011.00692.x,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84155163036&doi=10.1111%2fj.1469-1809.2011.00692.x&partnerID=40&md5=da568965597c0753af135d4de5a90dda","Studies have shown that interactions of single nucleotide polymorphisms (SNPs) may play an important role in understanding the causes of complex disease. We have proposed an integrated machine learning method that combines two machine-learning methods-Random Forests (RF) and Multivariate Adaptive Regression Splines (MARS)-to identify a subset of important SNPs and detect interaction patterns more effectively and efficiently. In this two-stage RF-MARS (TRM) approach, RF is first applied to detect a predictive subset of SNPs, and then MARS is used to identify the interaction patterns. We evaluated the TRM performances in four models. RF variable selection was based on out-of-bag classification error rate (OOB) and variable important spectrum (IS). Our results support that RF OOB had better performance than MARS and RF IS in detecting important variables. This study demonstrates that TRM OOB, which is RF OOB plus MARS, has combined the strengths of RF and MARS in identifying SNP-SNP interactions in a scenario of 100 candidate SNPs. TRM OOB had greater true positive rate and lower false positive rate compared with MARS, particularly for searching interactions with a strong association with the outcome. Therefore, the use of TRM OOB is favored for exploring SNP-SNP interactions in a large-scale genetic variation study. © 2011 The Authors Annals of Human Genetics © 2011 Blackwell Publishing Ltd/University College London.","Interaction; Machine learning; Polymorphism","article; controlled study; genetic variability; human; machine learning; major clinical study; male; molecular genetics; multivariate adaptive regression spline; priority journal; random forest; single nucleotide polymorphism; Artificial Intelligence; Decision Trees; Genotype; Humans; Male; Models, Genetic; Models, Statistical; Polymorphism, Single Nucleotide; Prostatic Neoplasms; Receptors, Estrogen",Article,Scopus,2-s2.0-84155163036
"Kota R., Chalkiadakis G., Robu V., Rogers A., Jennings N.R.","Cooperatives for demand side management",2012,"Frontiers in Artificial Intelligence and Applications",14,10.3233/978-1-61499-098-7-969,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878798280&doi=10.3233%2f978-1-61499-098-7-969&partnerID=40&md5=0c3796f6cd6818e8100eb34d2d8e9fdb","We propose a new scheme for efficient demand side management for the Smart Grid. Specifically, we envisage and promote the formation of cooperatives of medium-large consumers and equip them (via our proposed mechanisms) with the capability of regularly participating in the existing electricity markets by providing electricity demand reduction services to the Grid. Based on mechanism design principles, we develop a model for such cooperatives by designing methods for estimating suitable reduction amounts, placing bids in the market and redistributing the obtained revenue amongst the member agents. Our mechanism is such that the member agents have no incentive to show artificial reductions with the aim of increasing their revenues. © 2012 The Author(s).",,"Artificial intelligence; Commerce; Electric utilities; Machine design; Demand side managements; Designing methods; Electricity demands; Large consumers; Mechanism design; Smart grid; Electric power transmission networks",Conference Paper,Scopus,2-s2.0-84878798280
"Özek C., Caydaş U., Ünal E.","A fuzzy model for predicting surface roughness in plasma arc cutting of AISI 4140 steel",2012,"Materials and Manufacturing Processes",14,10.1080/10426914.2011.551952,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856287550&doi=10.1080%2f10426914.2011.551952&partnerID=40&md5=4e930752831bc013f6fc02e21690bc27","In the present study, fuzzy logic (a tool in artificial intelligence) was used for the prediction of cutting parameters in plasma arc cutting (PAC) process of AISI 4140 steel. The parameters considered in this study were plasma arc current, cutting speed, and thickness of cut material. Fuzzy rule-based modeling was employed for prediction of surface roughness. These models can be effectively used to estimate the surface roughness. The experimental results were compared with fuzzy logic, and good agreement between them was observed. Analysis of the influence of the individual important machining parameters on the surface roughness have been carried out and presented. Statistically, cutting speed was found the most important factor on surface roughness, while the plasma arc current had the least. The characteristics of machined surfaces and microstructural changes are also studied. Copyright © 2012 Taylor and Francis Group, LLC.","AISI 4140; ANOVA; Cutting; Fuzzy; Plasma; Surface","AISI 4140; AISI 4140 steel; Cutting parameters; Cutting speed; Fuzzy; Fuzzy models; Machined surface; Machining parameters; Microstructural changes; Plasma arc; Rule based; Analysis of variance (ANOVA); Artificial intelligence; Cutting; Electric arc welding; Forecasting; Fuzzy logic; Plasma arc cutting; Plasmas; Surfaces; Surface roughness",Article,Scopus,2-s2.0-84856287550
"Zhang C., Lee H., Shin K.G.","Efficient distributed linear classification algorithms via the alternating direction method of multipliers",2012,"Journal of Machine Learning Research",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899031306&partnerID=40&md5=36fbb86d95633b519b63fd38e617dc7d","Linear classification has demonstrated success in many areas of applications. Modern algorithms for linear classification can train reasonably good models while going through the data in only tens of rounds. However, large data often does not fit in the memory of a single machine, which makes the bottleneck in large-scale learning the disk I/O, not the CPU. Following this observation, Yu et al. (2010) made significant progress in reducing disk usage, and their algorithms now outperform LIBLINEAR. In this paper, rather than optimizing algorithms on a single machine, we propose and implement distributed algorithms that achieve parallel disk loading and access the disk only once. Our large-scale learning algorithms are based on the framework of alternating direction methods of multipliers. The framework derives a subproblem that remains to be solved efficiently for which we propose using dual coordinate descent and trust region Newton method. Our experimental evaluations on large datasets demonstrate that the proposed algorithms achieve significant speedup over the classifier proposed by Yu et al. running on a single machine. Our algorithms are faster than existing distributed solvers, such as Zinkevich et al. (2010)'s parallel stochastic gradient descent and Vowpal Wabbit. © Copyright 2012 by the authors.",,"Aluminum; Artificial intelligence; Classification (of information); Learning algorithms; Newton-Raphson method; Stochastic systems; Alternating direction method of multipliers; Alternating direction methods; Coordinate descent; Experimental evaluation; Large-scale learning; Linear classification; Optimizing algorithm; Stochastic gradient descent; Algorithms",Conference Paper,Scopus,2-s2.0-84899031306
"Kivinen J.J., Williams C.K.I.","Multiple texture boltzmann machines",2012,"Journal of Machine Learning Research",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954504330&partnerID=40&md5=73d2159d2886352d9bb391f38f77838f","We assess the generative power of the mPoTmodel of [10] with tiled-convolutional weight sharing as a model for visual textures by specifically training on this task, evaluating model performance on texture synthesis and inpainting tasks using quantitative metrics. We also analyze the relative importance of the mean and covariance parts of the mPoT model by comparing its performance to those of its subcomponents, tiled-convolutional versions of the PoT/FoE and Gaussian-Bernoulli restricted Boltzmann machine (GB-RBM). Our results suggest that while state-of-the-art or better performance can be achieved using the mPoT, similar performance can be achieved with the mean-only model. We then develop a model for multiple textures based on the GB-RBM, using a shared set of weights but texturespecific hidden unit biases. We show comparable performance of the multiple texture model to individually trained texture models.",,"Artificial intelligence; Boltzmann machines; Evaluating model performance; Multiple texture; Quantitative metrics; Restricted boltzmann machine; State of the art; Texture synthesis; Visual texture; Convolution",Conference Paper,Scopus,2-s2.0-84954504330
"Wan L., Zhu L., Fergus R.","A hybrid neural network-latent topic model",2012,"Journal of Machine Learning Research",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954233874&partnerID=40&md5=a5465a23ebcc78bda77e6dea5a679595","This paper introduces a hybrid model that combines a neural network with a latent topic model. The neural network provides a lowdimensional embedding for the input data, whose subsequent distribution is captured by the topic model. The neural network thus acts as a trainable feature extractor while the topic model captures the group structure of the data. Following an initial pretraining phase to separately initialize each part of the model, a unified training scheme is introduced that allows for discriminative training of the entire model. The approach is evaluated on visual data in scene classification task, where the hybrid model is shown to outperform models based solely on neural networks or topic models, as well as other baseline methods. © Copyright 2012 by the authors.",,"Artificial intelligence; Baseline methods; Discriminative training; Feature extractor; Group structure; Hybrid neural networks; Latent topic model; Scene classification; Training schemes; Data mining",Conference Paper,Scopus,2-s2.0-84954233874
"Zhang Z.-F., Gao Z., Liu Y.-Y., Jiang F.-C., Yang Y.-L., Ren Y.-F., Yang H.-J., Yang K., Zhang X.-D.","Computer vision based method and system for online measurement of geometric parameters of train wheel sets",2012,"Sensors",14,10.3390/s120100334,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863070608&doi=10.3390%2fs120100334&partnerID=40&md5=c89c75b091d7579aca0156e6759c1ed7","Train wheel sets must be periodically inspected for possible or actual premature failures and it is very significant to record the wear history for the full life of utilization of wheel sets. This means that an online measuring system could be of great benefit to overall process control. An online non-contact method for measuring a wheel set's geometric parameters based on the opto-electronic measuring technique is presented in this paper. A charge coupled device (CCD) camera with a selected optical lens and a frame grabber was used to capture the image of the light profile of the wheel set illuminated by a linear laser. The analogue signals of the image were transformed into corresponding digital grey level values. The 'mapping function method' is used to transform an image pixel coordinate to a space coordinate. The images of wheel sets were captured when the train passed through the measuring system. The rim inside thickness and flange thickness were measured and analyzed. The spatial resolution of the whole image capturing system is about 0.33 mm. Theoretic and experimental results show that the online measurement system based on computer vision can meet wheel set measurement requirements. © 2012 by the authors.","Calibration; Computer vision; Mapping function method; Optoelectronic technique; Wheel set","article; artificial intelligence; calibration; equipment; image processing; instrumentation; methodology; online system; photography; traffic and transport; Artificial Intelligence; Calibration; Equipment Failure Analysis; Image Processing, Computer-Assisted; Online Systems; Photography; Transportation",Article,Scopus,2-s2.0-84863070608
"Wang Q., Ramírez G., Marx M., Theobald M., Kamps J.","Overview of the INEX 2011 data-centric track",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-642-35734-3_10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871536098&doi=10.1007%2f978-3-642-35734-3_10&partnerID=40&md5=d231f3fdc2f0f57bf96be66b0a9cace2","This paper presents an overview of the INEX 2011 Data-Centric Track. Having the ad hoc search task running its second year, we introduced a new task, faceted search task, whose goal is to provide the infrastructure to investigate and evaluate different techniques and strategies of recommending facet-values to aid the user to navigate through a large set of query results and quickly identify the results of interest. The same IMDB collection as last year was used for both tasks. A total of 9 active participants contributed a total of 60 topics for both tasks and submitted 35 ad hoc search runs and 13 faceted search runs. A total of 38 ad hoc search topics were assessed, which included 18 subtopics for 13 faceted search topics. We discuss the setup for both tasks and the results obtained by their participants. © Springer-Verlag 2012.",,"Data centric; Faceted search; Query results; Search tasks; Artificial intelligence; Query languages",Conference Paper,Scopus,2-s2.0-84871536098
"Chan P.P.K., Yeung D.S., Ng W.W.Y., Lin C.M., Liu J.N.K.","Dynamic fusion method using Localized Generalization Error Model",2012,"Information Sciences",13,10.1016/j.ins.2012.06.026,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865575666&doi=10.1016%2fj.ins.2012.06.026&partnerID=40&md5=d543d21e6f15965efcce9453ae0b819b","Multiple Classifier Systems (MCSs), which combine the outputs of a set of base classifiers, were proposed as a method to develop a more accurate classification system. One fundamental issue is how to combine the base classifiers. In this paper, a new dynamic fusion method named Localized Generalization Error Model Fusion Method (LFM) for MCSs is proposed. The Localized Generalization Error Model (L-GEM) has been used to estimate the local competence of base classifiers in MCSs. L-GEM provides a generalization error bound for unseen samples located within neighborhoods of testing samples. Base classifiers with lower generalization error bounds are assigned higher weights. In contrast to the current dynamic fusion methods, LFM estimates the local competence of base classifiers not only using the information of training error but also the sensitivity of classifier outputs. The additional effect of the sensitivity on the performance of model and the time complexity of the LFM are discussed and analyzed. Experimental results show that the MCSs using the LFM as a combination method outperform those using the other 21 dynamic fusion methods in terms of testing accuracy and time. © 2012 Elsevier Inc. All rights reserved.","Dynamic fusion method; Localized Generalization Error Model (L-GEM); Multiple Classifier Systems (MCSs); Sensitivity","Base classifiers; Classification system; Combination method; Current dynamics; Dynamic fusion; Generalization error bounds; Higher weight; Localized generalization error models; Multiple classifier systems; Sensitivity; Testing accuracy; Testing samples; Time complexity; Training errors; Artificial intelligence; Software engineering; Error analysis",Article,Scopus,2-s2.0-84865575666
"Yu D., Wu X., Shen H., Yang J., Tang Z., Qi Y., Yang J.","Enhancing membrane protein subcellular localization prediction by parallel fusion of multi-view features",2012,"IEEE Transactions on Nanobioscience",13,10.1109/TNB.2012.2208473,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870860627&doi=10.1109%2fTNB.2012.2208473&partnerID=40&md5=9ab5e9f75641ed8503460a1970ceac91","Membrane proteins are encoded by ∼ 30% in the genome and function importantly in the living organisms. Previous studies have revealed that membrane proteins' structures and functions show obvious cell organelle-specific properties. Hence, it is highly desired to predict membrane protein's subcellular location from the primary sequence considering the extreme difficulties of membrane protein wet-lab studies. Although many models have been developed for predicting protein subcellular locations, only a few are specific to membrane proteins. Existing prediction approaches were constructed based on statistical machine learning algorithms with serial combination of multi-view features, i.e., different feature vectors are simply serially combined to form a super feature vector. However, such simple combination of features will simultaneously increase the information redundancy that could, in turn, deteriorate the final prediction accuracy. That's why it was often found that prediction success rates in the serial super space were even lower than those in a single-view space. The purpose of this paper is investigation of a proper method for fusing multiple multi-view protein sequential features for subcellular location predictions. Instead of serial strategy, we propose a novel parallel framework for fusing multiple membrane protein multi-view attributes that will represent protein samples in complex spaces. We also proposed generalized principle component analysis (GPCA) for feature reduction purpose in the complex geometry. All the experimental results through different machine learning algorithms on benchmark membrane protein subcellular localization datasets demonstrate that the newly proposed parallel strategy outperforms the traditional serial approach. We also demonstrate the efficacy of the parallel strategy on a soluble protein subcellular localization dataset indicating the parallel technique is flexible to suite for other computational biology problems. The software and datasets are available at: http://www.csbio.sjtu.edu. cn/bioinf/mpsp. © 2011 IEEE.","Feature extraction; GPCA; membrane protein; parallel feature fusion; serial feature fusion; subcellular localization prediction","Complex geometries; Complex space; Computational biology; Data sets; Feature fusion; Feature reduction; Feature vectors; Generalized principle component analysis; GPCA; Information redundancies; Living organisms; Membrane proteins; Multi-views; Parallel framework; Parallel strategies; Parallel techniques; Prediction accuracy; Primary sequences; Protein samples; Protein subcellular location; Soluble proteins; Subcellular localizations; Subcellular location; Bioinformatics; Biology; Feature extraction; Forecasting; Learning algorithms; Learning systems; Parallel architectures; Principal component analysis; Proteins; Biological membranes; membrane protein; Saccharomyces cerevisiae protein; algorithm; article; artificial intelligence; biological model; metabolism; protein database; protein transport; Algorithms; Artificial Intelligence; Databases, Protein; Membrane Proteins; Models, Biological; Protein Transport; Saccharomyces cerevisiae Proteins",Article,Scopus,2-s2.0-84870860627
"Ismail M.M., Othman M.A., Sulaiman H.A., Misran M.H., Ramlee R.H., Abidin A.F.Z., Nordin N.A., Zakaria M.I., Ayob M.N., Yakop F.","Firefly algorithm for path optimization in PCB holes drilling process",2012,"Proceedings of the 2012 International Conference in Green and Ubiquitous Technology, GUT 2012",13,10.1109/GUT.2012.6344160,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870658718&doi=10.1109%2fGUT.2012.6344160&partnerID=40&md5=1ba4322fb0618a8a360d5535614b2798","In PCB holes drilling process, the time taken for a task completion heavily rely on the distance travels by the drill bit of the CNC machine. In order to minimize the distance traveled by the drill bit, Firefly Algorithm can be used. The proposed model applies Firefly Algorithm to search for the optimized path in PCB holes drilling process. Each agent's position represented a possible path that can be taken by the drill bit. The fitness of the agent is inversely proportional to the distance of the path where the shorter the distance, the better the fitness of an agent. Then, the agents compare their fitness with each other. Agent will try to improve its fitness by moving closer towards other agent with better fitness. The process repeated until maximum iteration achieved. Performance of the proposed model is compared with other literatures using a standard case study. © 2012 IEEE.","Computational Intelligence; Firefly Algorithm; Path Optimization; Printed Circuit Board","CNC machine; Drilling process; Firefly algorithms; Path optimizations; Algorithms; Artificial intelligence; Bioluminescence; Bits; Computer control systems; Health; Optimization; Organic pollutants; Polychlorinated biphenyls; Printed circuit boards; Iterative methods",Conference Paper,Scopus,2-s2.0-84870658718
"Supakkul S., Chung L.","The RE-Tools: A multi-notational requirements modeling toolkit",2012,"2012 20th IEEE International Requirements Engineering Conference, RE 2012 - Proceedings",13,10.1109/RE.2012.6345831,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870715034&doi=10.1109%2fRE.2012.6345831&partnerID=40&md5=1f51b396daa9319fc27f9e2df22fea39","Requirements engineers need to understand and model different aspects of organizations and systems under construction, and may need to use different modeling notations. However, most modeling tools support only one (or at most a few notations), hindering requirements engineers from using the most appropriate notations for the particular modeling task. The RE-Tools is an open-source toolkit implemented using a UML Profile for StarUML, an open-source UML modeling tool. The toolkit supports many leading requirements modeling notations, including the NFR Framework, the i Framework, KAOS, Problem Frames, and UML. Each of these notations may be used for modeling independent corresponding diagrams or together with non-functional requirements (NFRs). The toolkit also supports the original qualitative reasoning of the NFR Framework and augments with a quantitative one. © 2012 IEEE.",,"Modeling notation; Modeling task; Modeling tool; Non-functional requirements; Open-source; Problem Frames; Qualitative reasoning; Requirements engineers; Requirements modeling; Toolkit support; UML modeling tools; Uml profiles; Artificial intelligence; Requirements engineering",Conference Paper,Scopus,2-s2.0-84870715034
"Cerreta M., De Toro P.","Urbanization suitability maps: A dynamic spatial decision support system for sustainable land use",2012,"Earth System Dynamics",13,10.5194/esd-3-157-2012,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870623783&doi=10.5194%2fesd-3-157-2012&partnerID=40&md5=94b27160d79ac55a6c43097997207e1c","Recent developments in land consumption assessment identify the need to implement integrated evaluation approaches, with particular attention to the development of multidimensional tools for guiding and managing sustainable land use. Land use policy decisions are implemented mostly through spatial planning and its related zoning. This involves trade-offs between many sectorial interests and conflicting challenges seeking win-win solutions. In order to identify a decision-making process for land use allocation, this paper proposes a methodological approach for developing a Dynamic Spatial Decision Support System (DSDSS), denominated Integrated Spatial Assessment (ISA), supported by Geographical Information Systems (GIS) combined with the Analytic Hierarchy Process (AHP). Through empirical investigation in an operative case study, an integrated evaluation approach implemented in a DSDSS helps produce ""urbanization suitability maps"" in which spatial analysis combined with multi-criteria evaluation methods proved to be useful for both facing the main issues relating to land consumption as well as minimizing environmental impacts of spatial planning. ©Author(s)2012.",,"Decision making process; Empirical investigation; Geographical information systems; Integrated evaluation; Land use allocation; Land use policy; Methodological approach; Multi-criteria evaluation; Spatial analysis; Spatial assessment; Spatial decision support systems; Spatial planning; Win-win; Analytic hierarchy process (ahp); Sustainable land use; Artificial intelligence; Decision support systems; Economic and social effects; Environmental impact; Geographic information systems; Analytic hierarchy process; Decision making; Land use; Land use; Geographic information systems; analytical hierarchy process; decision making; decision support system; dynamic analysis; environmental impact; GIS; land use planning; mapping; resource allocation; spatial planning; sustainable development; urbanization; multicriteria analysis; spatial analysis",Article,Scopus,2-s2.0-84870623783
"Perick P., St-Pierre D.L., Maes F., Ernst D.","Comparison of different selection strategies in Monte-Carlo Tree Search for the game of Tron",2012,"2012 IEEE Conference on Computational Intelligence and Games, CIG 2012",13,10.1109/CIG.2012.6374162,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871948685&doi=10.1109%2fCIG.2012.6374162&partnerID=40&md5=51b63267599d8c1e3838388624edba97","Monte-Carlo Tree Search (MCTS) techniques are essentially known for their performance on turn-based games, such as Go, for which players have considerable time for choosing their moves. In this paper, we apply MCTS to the game of Tron, a simultaneous real-time two-player game. The fact that players have to react fast and that moves occur simultaneously creates an unusual setting for MCTS, in which classical selection policies such as UCB1 may be suboptimal. In this paper, we perform an empirical comparison of a wide range of selection policies for MCTS applied to Tron, with both deterministic policies (UCB1, UCBl-Tuned, UCB-V, UCB-Minimal, OMC-Deterministic, MOSS) and stochastic policies (εn-greedy, EXP3, Thompson Sampling, OMC-Stochastic, PBBM). From the experiments, we observe that UCBl-Tuned has the best behavior shortly followed by UCB1. Even if UCB-Minimal is ranked fourth, this is a remarkable result for this recently introduced selection policy found through automatic discovery of good policies on generic multi-armed bandit problems. We also show that deterministic policies perform better than stochastic ones for this problem. © 2012 IEEE.",,"Automatic discovery; Empirical comparison; MONTE CARLO; Multi-armed bandit problem; Stochastic policy; Thompson; Tree search; Two-player games; Artificial intelligence; Computational methods; Stochastic systems",Conference Paper,Scopus,2-s2.0-84871948685
"Borkowski P.","Data fusion in a navigational decision support system on a sea-going vessel",2012,"Polish Maritime Research",13,10.2478/v10012-012-0043-1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878295767&doi=10.2478%2fv10012-012-0043-1&partnerID=40&md5=2e2bfe49b538120c97841b2768314a77","The problem of data fusion in a navigational decision support system on a sea-going vessel has been analyzed. The computing algorithm herein applied for solving the formulated problem is based on a multi-sensor Kalman filter. On the practical side, results of the tests done in real conditions are shown. The tests conducted onboard m/s Nawigator XXI, have been aimed at the verification of the proposed computing algorithm implemented in a prototype navigational decision support system.","Data Fusion; Multi-Sensor Kalman Filter; Navigational Decision Support System","Computing algorithms; Formulated problems; Multi sensor; Sea-going vessels; Algorithms; Artificial intelligence; Data fusion; Decision support systems; Kalman filters; Sensors; Navigation",Article,Scopus,2-s2.0-84878295767
"Xiao Y., Liu Y., Leung C.-S., Sum J.P.-F., Ho K.","Analysis on the convergence time of dual neural network-based kWTA",2012,"IEEE Transactions on Neural Networks and Learning Systems",13,10.1109/TNNLS.2012.2186315,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867928460&doi=10.1109%2fTNNLS.2012.2186315&partnerID=40&md5=a0e5eb3e572f017eb57b6fe628faf3c7","A k-winner-take-all (kWTA) network is able to find out the k largest numbers from n inputs. Recently, a dual neural network (DNN) approach was proposed to implement the kWTA process. Compared to the conventional approach, the DNN approach has much less number of interconnections. A rough upper bound on the convergence time of the DNN-kWTA model, which is expressed in terms of input variables, was given. This brief derives the exact convergence time of the DNN-kWTA model. With our result, we can study the convergence time without spending excessive time to simulate the network dynamics. We also theoretically study the statistical properties of the convergence time when the inputs are uniformly distributed. Since a nonuniform distribution can be converted into a uniform one and the conversion preserves the ordering of the inputs, our theoretical result is also valid for nonuniformly distributed inputs. © 2012 IEEE.","$k$-winner-take-all (kWTA); convergence; dual neural networks; WTA process","Conventional approach; convergence; Dual neural networks; Non-uniform distribution; Statistical properties; Theoretical result; Winner take alls; WTA process; Artificial intelligence; Computer networks; Neural networks; algorithm; artificial neural network; computer simulation; decision support system; statistical model; Algorithms; Computer Simulation; Decision Support Techniques; Models, Statistical; Neural Networks (Computer)",Article,Scopus,2-s2.0-84867928460
"Bhuvaneswari P.T.V., Karthikeyan S., Jeeva B., Prasath M.A.","An efficient mobility based localization in underwater sensor networks",2012,"Proceedings - 4th International Conference on Computational Intelligence and Communication Networks, CICN 2012",13,10.1109/CICN.2012.43,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872036192&doi=10.1109%2fCICN.2012.43&partnerID=40&md5=512a82415aefc2e123e2d3b3a8470117","Underwater Wireless Sensor Network (UWSN) has attracted significant attention recently from both academia and industry, as it provides a promising solution to aquatic environmental monitoring and exploration. Due to the limitation of electromagnetic waves, underwater sensor networks utilize acoustic signals that have high error rate, propagation delay and limited bandwidth for data communication. By adopting predictable mobility patterns of underwater objects, we propose a scheme, called Efficient Mobility Based Localization (EMBL) to determine the location information of sensor node in UWSN. The proposed EMBL scheme is divided into two parts, anchor node localization and ordinary node localization. During the localization process, every node predicts its future mobility pattern according to its past known location information, and estimate its future location. The proposed EMBL scheme is simulated in MATLAB and compared with the existing scheme in terms of localization coverage, localization error, average communication cost. It is found that proposed EMBL scheme outperforms the existing schemes. © 2012 IEEE.","EMBL scheme; localization; mobility prediction and localization error; underwater sensor network","Acoustic signals; Anchor nodes; Communication cost; Data-communication; EMBL scheme; Environmental Monitoring; Error rate; Limited bandwidth; localization; Localization errors; Location information; Mobility pattern; Node localization; Predictable mobility; Propagation delays; Underwater objects; Underwater sensor networks; Underwater wireless sensor networks; Artificial intelligence; Telecommunication networks; Sensor nodes",Conference Paper,Scopus,2-s2.0-84872036192
"Kawaler E., Cobian A., Peissig P., Cross D., Yale S., Craven M.","Learning to predict post-hospitalization VTE risk from EHR data.",2012,"AMIA ... Annual Symposium proceedings / AMIA Symposium. AMIA Symposium",13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880787212&partnerID=40&md5=a099f9706cf1af124643aa5e4b7a71e2","We consider the task of predicting which patients are most at risk for post-hospitalization venothromboembolism (VTE) using information automatically elicited from an EHR. Given a set of cases and controls, we use machine-learning methods to induce models for making these predictions. Our empirical evaluation of this approach offers a number of interesting and important conclusions. We identify several risk factors for VTE that were not previously recognized. We show that machine-learning methods are able to induce models that identify high-risk patients with accuracy that exceeds previously developed scoring models for VTE. Additionally, we show that, even without having prior knowledge about relevant risk factors, we are able to learn accurate models for this task.",,"adult; algorithm; article; artificial intelligence; Bayes theorem; classification; electronic medical record; hospitalization; human; methodology; middle aged; predictive value; risk assessment; risk factor; single nucleotide polymorphism; survival; venous thromboembolism; Adult; Algorithms; Artificial Intelligence; Bayes Theorem; Electronic Health Records; Hospitalization; Humans; Middle Aged; Polymorphism, Single Nucleotide; Predictive Value of Tests; Risk Assessment; Risk Factors; Survival Analysis; Venous Thromboembolism",Article,Scopus,2-s2.0-84880787212
"Deufemia V., Paolino L., De Lumley H.","Petroglyph recognition using self-organizing maps and fuzzy visual language parsing",2012,"Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI",13,10.1109/ICTAI.2012.119,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876849389&doi=10.1109%2fICTAI.2012.119&partnerID=40&md5=a9136a695da52cb47228efe8c1b7fb1a","Petroglyphs are images carved into a rock surface by prehistoric people using a symbolic or ritual language. Although they constitute an historical patrimony of inestimable value, little efforts have been devoted to the development of automated tools for their classification and interpretation. In this work we present a new algorithm for recognizing petroglyphs within scenes composed of several engraved figures. The proposal combines an unsupervised recognizer, Self-Organizing Maps (SOM), with a fuzzy visual language parser. The first classifies the petroglyph symbols extracted from a scene by using Radon transform as shape descriptor. The latter exploits the archeological knowledge about recurring patterns within scenes to solve ambiguous interpretations. The algorithm has been evaluated on a set of 50 petroglyph scenes, containing about 500 carved symbols from Mount Bego rock art site, and achieved very promising results. © 2012 IEEE.","fuzzy spatial relationships; Image processing; petroglyph recognition; self-organizing maps; visual languages","Automated tools; Classification and interpretation; petroglyph recognition; Radon Transform; Self-organizing map (SOM); Shape descriptors; Spatial relationships; Visual language parsing; Algorithms; Artificial intelligence; Conformal mapping; Image processing; Pinch effect; Visual languages",Conference Paper,Scopus,2-s2.0-84876849389
"Akhtar S., Ahmad A.R., Abdel-Rahman E.M.","A metaheuristic bat-inspired algorithm for full body human pose estimation",2012,"Proceedings of the 2012 9th Conference on Computer and Robot Vision, CRV 2012",13,10.1109/CRV.2012.55,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878346635&doi=10.1109%2fCRV.2012.55&partnerID=40&md5=50e9fe99633db5bee5655cc7a145f457","This paper addresses the problem of full body articulated human motion tracking from multi-view video data recorded in a laboratory environment. The problem is formulated as a high dimensional (31-dimensional) non-linear optimization problem. In recent years, metaheuristics such as Particle Swarm Optimization (PSO), Ant Colony Optimization (ACO), Artificial Immune System (AIS), Firefly Algorithm (FA) are applied to complex non-linear optimization problems. These population based evolutionary algorithms have diversified search capabilities and are computationally robust and efficient. One such recently proposed metaheuristic, Bat Algorithm (BA), is employed in this work for full human body pose estimation. The performance of BA is compared with Particle Filter (PF), Annealed Particle Filter (APF) and PSO using a standard data set. The qualitative and the quantitative evaluation of the performance of full body human tracking demonstrates that BA performs better then PF, APF and PSO. © 2012 IEEE.","articulated human tracking; bat optimization; Human pose estimation; soft computing; swarm optimization","Annealed particle filters; Ant Colony Optimization (ACO); Artificial Immune System; Human pose estimations; Human Tracking; Non-linear optimization problems; Quantitative evaluation; Swarm optimization; Ant colony optimization; Artificial intelligence; Computer vision; Gesture recognition; Monte Carlo methods; Motion estimation; Particle swarm optimization (PSO); Soft computing; Target tracking; Algorithms",Conference Paper,Scopus,2-s2.0-84878346635
"Batra D.","An efficient message-passing algorithm for the m-best MAP problem",2012,"Uncertainty in Artificial Intelligence - Proceedings of the 28th Conference, UAI 2012",13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885965403&partnerID=40&md5=ef0db332e8ef9f8dce2f75f74c05cb32","Much effort has been directed at algorithms for obtaining the highest probability configuration in a probabilistic random field model - known as the maximum a posteriori (MAP) inference problem. In many situations, one could benefit from having not just a single solution, but the top M most probable solutions - known as the M-Best MAP problem. In this paper, we propose an efficient message-passing based algorithm for solving the M-Best MAP problem. Specifically, our algorithm solves the recently proposed Linear Programming (LP) formulation of MBest MAP [7], while being orders of magnitude faster than a generic LP-solver. Our approach relies on studying a particular partial Lagrangian relaxation of the M-Best MAP LP which exposes a natural combinatorial structure of the problem that we exploit.",,"Algorithm for solving; Combinatorial structures; Inference problem; LaGrangian relaxation; Maximum a posteriori; Message passing algorithm; Orders of magnitude; Random field model; Algorithms; Artificial intelligence; Message passing; Inference engines",Conference Paper,Scopus,2-s2.0-84885965403
"Novak M.","Computer aided decision support in product design engineering [Računalno potpomognuto podupiranje odluka u konstruiranju proizvoda]",2012,"Tehnicki Vjesnik",13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871357499&partnerID=40&md5=7419debb8cb9c9b91ed0cb684d4a34e9","Product design engineering is a complex discipline, which is undergoing a transformation from informal and largely experience-based domain to scientific oriented domain. Computational intelligence can contribute greatly to product design process, as it is becoming more and more evident that adding intelligence to existing computer aids, such as computer aided design systems, can lead to significant improvements in terms of effectiveness and reliability of various tasks within product design engineering. Providing computer aided decision support is one of the computational intelligence methods that proved to be effective in enabling more intelligent and less experience-dependent design performance. In this paper, some of the most crucial areas of product design engineering process that require additional computational intelligence in terms of computer aided decision support are presented together with some examples of intelligent knowledge-based modules applied to this areas.","Computational intelligence; Decision support; Design engineering; Design for X; Knowledge-based modules; Product development","Computational intelligence methods; Computer aided; Computer aided design systems; Decision supports; Design Engineering; Design for X; Design performance; Knowledge-based modules; Product design engineering; Product design process; Artificial intelligence; Computer aided design; Knowledge based systems; Product development; Decision support systems",Article,Scopus,2-s2.0-84871357499
"Montavon G., Müller K.-R.","Deep Boltzmann machines and the centering trick",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-642-35289-8-33,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872571941&doi=10.1007%2f978-3-642-35289-8-33&partnerID=40&md5=45df83ddd6dd379a7f098079511a75b0","Deep Boltzmann machines are in theory capable of learning efficient representations of seemingly complex data. Designing an algorithm that effectively learns the data representation can be subject to multiple difficulties. In this chapter, we present the ""centering trick"" that consists of rewriting the energy of the system as a function of centered states. The centering trick improves the conditioning of the underlying optimization problem and makes learning more stable, leading to models with better generative and discriminative properties. © Springer-Verlag Berlin Heidelberg 2012.","centering; Deep Boltzmann machine; optimization; reparameterization; representations; unsupervised learning","Boltzmann machines; centering; Complex data; Data representations; Optimization problems; Reparameterization; representations; Artificial intelligence; Unsupervised learning; Optimization",Article,Scopus,2-s2.0-84872571941
"Shamsudin H.C., Irawan A., Ibrahim Z., Abidin A.F.Z., Wahyudi S., Rahim M.A.A., Khalil K.","A fast discrete gravitational search algorithm",2012,"Proceedings of International Conference on Computational Intelligence, Modelling and Simulation",13,10.1109/CIMSim.2012.28,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872579659&doi=10.1109%2fCIMSim.2012.28&partnerID=40&md5=01e04f6c91e1b8a981767897f25cc565","This study introduces a variant of Gravitational Search Algorithm (GSA) for discrete optimization problems, namely, Fast Discrete Gravitational Search Algorithm (FDGSA). The main difference between the proposed FDGSA and the existing Binary Gravitational Search Algorithm (BGSA) is that an agent's position is updated based on its direction and velocity. Both the direction and velocity determine the candidates of integer values for the position update of an agent and then the selection is done randomly. Unimodal test functions, such as De Jong's function, Scwefel's function and Rosenbrock's valley are used to evaluate the performance of the proposed FDGSA. Comparison with BGSA is done to benchmark the proposed FDGSA in terms of speed of convergence and quality of solution. The experimental result shows that the proposed FDGSA converges faster compared to the BGSA. © 2012 IEEE.","Discrete optimization; FGSA; GSA","Discrete optimization; Discrete optimization problems; FGSA; Gravitational search algorithms; GSA; Integer values; Quality of solution; Speed of convergence; Test functions; Unimodal; Artificial intelligence; Benchmarking; Optimization; Learning algorithms",Conference Paper,Scopus,2-s2.0-84872579659
"Esmaili S., Kalantari-Dahaghi A., Mohaghegh S.D.","Modeling and history matching of hydrocarbon production from Marcellus shale using data mining and pattern recognition technologies",2012,"SPE Eastern Regional Meeting",13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873882885&partnerID=40&md5=0d6026259e0f726d3598c1500fff2a76","The Marcellus Shale play has attracted much attention in recent years. Our fall understanding of the complexities of the flow mechanism in matrix, sorption process and flow behavior in complex fracture system (natural and hydraulic) still has a long way to go in this prolific and hydrocarbon rich formation. In this paper, we present and discuss a novel approach to modeling and history matching of hydrocarbon production from a Marcellus shale asset in southwestern Pennsylvania using advanced data mining & pattern recognition technologies. In this new approach instead of imposing our understanding of the flow mechanism, the impact of multi-stage hydraulic fractures, and the production process on the reservoir model, we allow the production history, well log, and hydraulic fracturing data to force their will on our model and determine its behavior. The uniqueness of this technique is that it incorporates the so-called ""hard data"" directly into the reservoir model, such that the model can be used to optimize the hydraulic fracture process. The ""hard data"" refers to field measurements during the hydraulic fracturing process such as fluid and proppant type and amount, injection pressure and rate as well as proppant concentration. The study focuses on part of Marcellus shale including 135 wells with multiple pads, different landing targets, well length and reservoir properties. The full-field history matching process was completed successfully. Artificial Intelligence (AI)-based model proved its capability in capturing the production behavior with acceptable accuracy for individual wells and for the entire field. Copyright 2012, Society of Petroleum Engineers.",,"Field measurement; Flow behaviors; Flow mechanisms; Fracture systems; Full-field; Hard data; History matching; Hydraulic fracture; Hydraulic fracturing process; Hydrocarbon production; Injection pressures; Multi-stage; Pattern recognition technologies; Pennsylvania; Production process; Proppant concentrations; Proppants; Reservoir models; Reservoir property; Sorption process; Artificial intelligence; Fluid dynamics; Hydraulic fracturing; Hydrocarbons; Shale; Sorption; Well logging; Petroleum reservoirs",Conference Paper,Scopus,2-s2.0-84873882885
"Hay N., Russell S., Tolpin D., Shimony S.E.","Selecting computations: Theory and applications",2012,"Uncertainty in Artificial Intelligence - Proceedings of the 28th Conference, UAI 2012",13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886054445&partnerID=40&md5=75552d71e5b1223dfb09ecbd37c40fd9","Sequential decision problems are often approximately solvable by simulating possible future action sequences. Metalevel decision procedures have been developed for selecting which action sequences to simulate, based on estimating the expected improvement in decision quality that would result from any particular simulation; an example is the recent work on using bandit algorithms to control Monte Carlo tree search in the game of Go. In this paper we develop a theoretical basis for metalevel decisions in the statistical framework of Bayesian selection problems, arguing (as others have done) that this is more appropriate than the bandit framework. We derive a number of basic results applicable to Monte Carlo selection problems, including the first finite sampling bounds for optimal policies in certain cases; we also provide a simple counterexample to the intuitive conjecture that an optimal policy will necessarily reach a decision in all cases. We then derive heuristic approximations in both Bayesian and distribution-free settings and demonstrate their superiority to bandit-based heuristics in one-shot decision problems and in Go.",,"Decision procedure; Distribution-free; Expected improvements; Heuristic approximations; Monte-Carlo tree searches; Selection problems; Sequential decisions; Statistical framework; Optimization; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84886054445
"Kopelowitz T.","On-line indexing for general alphabets via predecessor queries on subsets of an ordered list",2012,"Proceedings - Annual IEEE Symposium on Foundations of Computer Science, FOCS",13,10.1109/FOCS.2012.79,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871978069&doi=10.1109%2fFOCS.2012.79&partnerID=40&md5=1d55d74bff16ddbb3486fa9ebb6b599c","The problem of Text Indexing is a fundamental algorithmic problem in which one wishes to preprocess a text in order to quickly locate pattern queries within the text. In the ever evolving world of dynamic and on-line data, there is also a need for developing solutions to index texts which arrive on-line, i.e. a character at a time, and still be able to quickly locate said patterns. In this paper, a new solution for on-line indexing is presented by providing an on-line suffix tree construction in O(log log n + log log |Σ|) worst-case expected time per character, where n is the size of the string, and Σ is the alphabet. This improves upon all previously known on-line suffix tree constructions for general alphabets, at the cost of having the run time in expectation. The main idea is to reduce the problem of constructing a suffix tree on-line to an interesting variant of the order maintenance problem, which may be of independent interest. In the famous order maintenance problem, one wishes to maintain a dynamic list L of size n under insertions, deletions, and order queries. In an order query, one is given two nodes from L and must determine which node precedes the other in L. In an extension to this problem, named the Predecessor search on Dynamic Subsets of an Ordered Dynamic List problem (POLP for short), it is also necessary to maintain dynamic subsets S1,..., Sk ⊆ L, such that given some u ∈ L it will be possible to quickly locate the predecessor of u in Si, for any integer 1 ≤ i ≤ k. This paper provides an efficient data structure capable of locating the predecessor of u in Si in O(log log n) worst-case time and answering order queries on L in O(1) worst-case time, while allowing updates to L in O(1) worst-case expected time and updates to the subsets in O(log log n) worst-case expected time. This improves over a previous data structure which may be implicitly obtained from Dietz [8], in which the updates to the sets and L are done in O(log log n) amortized expected time. In addition, the bounds shown here match the currently best known bounds for predecessor search in the RAM model. Furthermore, this paper improves or simplifies bounds for several additional applications, including fully-persistent arrays, the monotonic list labeling problem, and the Order-Maintenance Problem. © 2012 IEEE.","data structures; order-maintenance; pattern matching; predecessor; suffix tree","Algorithmic problems; Developing solutions; Efficient data structures; Expected time; Monotonic list labeling; ON dynamics; Online data; order-maintenance; Pattern query; predecessor; Predecessor queries; Preprocess; RAM model; Runtimes; Suffix-trees; Text-indexing; Computer science; Data structures; Forestry; Indexing (of information); Maintenance; Pattern matching; Set theory; Artificial Intelligence; Data Bases; Forestry; Indexing; Maintenance; Pattern Recognition",Conference Paper,Scopus,2-s2.0-84871978069
"Cesarelli M., Romano M., Bifulco P., Improta G., D'Addio G.","An application of symbolic dynamics for FHRV assessment",2012,"Studies in Health Technology and Informatics",13,10.3233/978-1-61499-101-4-123,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872580804&doi=10.3233%2f978-1-61499-101-4-123&partnerID=40&md5=e6e7e767ed4f78d17d24269532234a95","Fetal heart rate variability is surely one of the most important parameters to monitor fetal wellbeing. Linear studies, widely employed to study fetal heart variability and its correlations with the development of the autonomous nervous system, have shown some limitations in highlight dynamics potentially relevant. During the last decades, therefore, nonlinear analysis methods have gained a growing interest to analyze the chaotic nature of cardiac activity. Techniques investigating nonlinear dynamics have been already successfully employed in adults, to analyze different physiological and pathological states. Concerning fetal monitoring, instead, a smaller number of papers is available in the literature; even if symbolic dynamics was recently employed to quantify fetal heart rate regularity, demonstrating that the use of this technique may lead to a better and more differentiated understanding of normal fetal physiological development. In this work, we applied the symbolic dynamics to analyze fetal heart rate variability in healthy fetuses at the end of a physiological pregnancy. Our results confirmed the potentiality of the technique to highlight differences between signals characterized by more or less variability. © 2012 European Federation for Medical Informatics and IOS Press. All rights reserved.","Cardiotocography; Fetal heart rate variability; Nonlinear dynamics; Symbolic dynamics","algorithm; article; artificial intelligence; automated pattern recognition; cardiotocography; computer assisted diagnosis; fetus heart rate; human; methodology; physiology; reproducibility; sensitivity and specificity; symbolism; Algorithms; Artificial Intelligence; Cardiotocography; Diagnosis, Computer-Assisted; Heart Rate, Fetal; Humans; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Symbolism",Conference Paper,Scopus,2-s2.0-84872580804
"Fan Y., Shen J., Xu K.","A general model and thresholds for random constraint satisfaction problems",2012,"Artificial Intelligence",13,10.1016/j.artint.2012.08.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866518361&doi=10.1016%2fj.artint.2012.08.003&partnerID=40&md5=ca07ac027c058e61d7569cb8882ded8a","In this paper, we study the relation among the parameters in their most general setting that define a large class of random CSP models d-k-CSP where d is the domain size and k is the length of the constraint scopes. The model d-k-CSP unifies several related models such as the model RB and the model k-CSP. We prove that the model d-k-CSP exhibits exact phase transitions if klnd increases no slower than the logarithm of the number of variables. A series of experimental studies with interesting observations are carried out to illustrate the solubility phase transition and the hardness of instances around phase transitions. © 2012 Elsevier B.V.","Constraint satisfaction problem; Phase transition","Constraint satisfaction problems; CSP model; Domain size; Experimental studies; General model; Random constraint satisfaction problems; Artificial intelligence; Phase transitions; Constraint theory",Article,Scopus,2-s2.0-84866518361
"Fernández-Ares A.J., García-Sánchez P., Mora A.M., Merelo J.J.","Adaptive bots for real-time strategy games via map characterization",2012,"2012 IEEE Conference on Computational Intelligence and Games, CIG 2012",13,10.1109/CIG.2012.6374185,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872003396&doi=10.1109%2fCIG.2012.6374185&partnerID=40&md5=49881bf2bac2112c503b69a1765d30d4","This paper presents a proposal for a fast on-line map analysis for the RTS game Planet Wars in order to define specialized strategies for an autonomous bot. This analysis is used to tackle two constraints of the game, as featured in the Google AI Challenge 2010: the players cannot store any information from turn to turn, and there is a limited action time of just one second. They imply that the bot must analyze the game map quickly, to adapt its strategy during the game. Based in our previous work, in this paper we have evolved bots for different types of maps. Then, all bots are combined in one, to choose the evolved strategy depending on the geographical configuration of the game in each turn. Several experiments have been conducted to test the new approach, which outperforms our previous version, based on an off-line general training. © 2012 IEEE.",,"Map analysis; Real-time strategy games; Artificial intelligence; Computational methods",Conference Paper,Scopus,2-s2.0-84872003396
"Fraigniaud P., Halldórsson M.M., Korman A.","On the impact of identifiers on local decision",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-642-35476-2_16,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871645676&doi=10.1007%2f978-3-642-35476-2_16&partnerID=40&md5=772b5b6d5177ffa238b303194fb3b40d","The issue of identifiers is crucial in distributed computing. Informally, identities are used for tackling two of the fundamental difficulties that are inherent to deterministic distributed computing, namely: (1) symmetry breaking, and (2) topological information gathering. In the context of local computation, i.e., when nodes can gather information only from nodes at bounded distances, some insight regarding the role of identities has been established. For instance, it was shown that, for large classes of construction problems, the role of the identities can be rather small. However, for the identities to play no role, some other kinds of mechanisms for breaking symmetry must be employed, such as edge-labeling or sense of direction. When it comes to local distributed decision problems, the specification of the decision task does not seem to involve symmetry breaking. Therefore, it is expected that, assuming nodes can gather sufficient information about their neighborhood, one could get rid of the identities, without employing extra mechanisms for breaking symmetry. We tackle this question in the framework of the LOCAL model. Let LD be the class of all problems that can be decided in a constant number of rounds in the LOCAL model. Similarly, let LD* be the class of all problems that can be decided at constant cost in the anonymous variant of the LOCAL model, in which nodes have no identities, but each node can get access to the (anonymous) ball of radius t around it, for any t, at a cost of t. It is clear that LD* ⊆ LD. We conjecture that LD*=LD. In this paper, we give several evidences supporting this conjecture. In particular, we show that it holds for hereditary problems, as well as when the nodes know an arbitrary upper bound on the total number of nodes. Moreover, we prove that the conjecture holds in the context of non-deterministic local decision, where nodes are given certificates (independent of the identities, if they exist), and the decision consists in verifying these certificates. In short, we prove that NLD*=NLD. © 2012 Springer-Verlag.","decision problems; Distributed complexity; identities; locality; non-determinism; symmetry breaking","Decision problems; Distributed complexity; identities; locality; Non-determinism; Symmetry-breaking; Artificial intelligence; Model checking",Conference Paper,Scopus,2-s2.0-84871645676
"Hu Y.J., Ku T.H., Jan R.H., Wang K., Tseng Y.C., Yang S.F.","Decision tree-based learning to predict patient controlled analgesia consumption and readjustment.",2012,"BMC medical informatics and decision making",13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868702962&partnerID=40&md5=5f6bce1ad6d5e193e4b361101be06363","Appropriate postoperative pain management contributes to earlier mobilization, shorter hospitalization, and reduced cost. The under treatment of pain may impede short-term recovery and have a detrimental long-term effect on health. This study focuses on Patient Controlled Analgesia (PCA), which is a delivery system for pain medication. This study proposes and demonstrates how to use machine learning and data mining techniques to predict analgesic requirements and PCA readjustment. The sample in this study included 1099 patients. Every patient was described by 280 attributes, including the class attribute. In addition to commonly studied demographic and physiological factors, this study emphasizes attributes related to PCA. We used decision tree-based learning algorithms to predict analgesic consumption and PCA control readjustment based on the first few hours of PCA medications. We also developed a nearest neighbor-based data cleaning method to alleviate the class-imbalance problem in PCA setting readjustment prediction. The prediction accuracies of total analgesic consumption (continuous dose and PCA dose) and PCA analgesic requirement (PCA dose only) by an ensemble of decision trees were 80.9% and 73.1%, respectively. Decision tree-based learning outperformed Artificial Neural Network, Support Vector Machine, Random Forest, Rotation Forest, and Naïve Bayesian classifiers in analgesic consumption prediction. The proposed data cleaning method improved the performance of every learning method in this study of PCA setting readjustment prediction. Comparative analysis identified the informative attributes from the data mining models and compared them with the correlates of analgesic requirement reported in previous works. This study presents a real-world application of data mining to anesthesiology. Unlike previous research, this study considers a wider variety of predictive factors, including PCA demands over time. We analyzed PCA patient data and conducted several experiments to evaluate the potential of applying machine-learning algorithms to assist anesthesiologists in PCA administration. Results demonstrate the feasibility of the proposed ensemble approach to postoperative pain management.",,"age; algorithm; analgesia; analysis of variance; article; artificial intelligence; artificial neural network; blood pressure; classification; comparative study; decision tree; drug administration; female; heart rate; human; instrumentation; male; patient controlled analgesia; physiology; postoperative pain; predictive value; retrospective study; risk factor; socioeconomics; standard; treatment outcome; United States; Age Factors; Algorithms; Analgesia, Patient-Controlled; Analysis of Variance; Artificial Intelligence; Blood Pressure; Chicago; Decision Trees; Drug Administration Schedule; Female; Heart Rate; Humans; Male; Neural Networks (Computer); Outcome and Process Assessment (Health Care); Pain Management; Pain, Postoperative; Predictive Value of Tests; Retrospective Studies; Risk Factors; Socioeconomic Factors",Article,Scopus,2-s2.0-84868702962
"Li M., Gent K., Hsiao M.S.","Design validation of RTL circuits using evolutionary swarm intelligence",2012,"Proceedings - International Test Conference",13,10.1109/TEST.2012.6401556,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873206259&doi=10.1109%2fTEST.2012.6401556&partnerID=40&md5=ad7743af2a44e189e96445726394037d","In this paper, we present BEACON, a Branch-oriented Evolutionary Ant Colony OptimizatioN method which is a bio-inspired meta-heuristic for design validation and functional test generation. BEACON combines an evolutionary search technique with Ant Colony Optimization (ACO) for improved search capability. BEACON first cross-compiles the Verilog circuit source to a C++ base for fast simulation. Then, it profiles the code, keeping track of each branch and the number of times it has been visited in a database. Branch coverage provides a very useful metric for exploring the design, especially visiting the most critical states, including corner states, in the design. At 100% branch coverage, we can conclude that every control state described in the RTL has been visited. Thus, during execution, BEACON trims highly visited branches from the search and focuses the search on rarely occurring branches and paths. This approach gives a significant performance boost while maintaining a high level of coverage. Experimental results show that BEACON is able to achieve very high branch coverages with a fraction of computational cost. In addition, previous hard-to-reach corner states in the ITC99 benchmarks have now been reached by BEACON. New states can also be discovered from the RTL descriptions. For many circuits, one to two orders of magnitude speedups over existing methods have been achieved. © 2012 IEEE.",,"Ant Colony Optimization (ACO); Ant colony optimization methods; Bio-inspired; Branch coverage; Computational costs; Control state; Critical state; Design validation; Evolutionary search; Fast simulation; Functional test generation; Metaheuristic; Orders of magnitude; Search capabilities; Swarm Intelligence; Verilog; Evolutionary algorithms; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84873206259
"Sonderegger M., Keshet J.","Automatic measurement of voice onset time using discriminative structured prediction",2012,"Journal of the Acoustical Society of America",13,10.1121/1.4763995,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870893505&doi=10.1121%2f1.4763995&partnerID=40&md5=aea4f2827ba946230a208a55b99861db","A discriminative large-margin algorithm for automatic measurement of voice onset time (VOT) is described, considered as a case of predicting structured output from speech. Manually labeled data are used to train a function that takes as input a speech segment of an arbitrary length containing a voiceless stop, and outputs its VOT. The function is explicitly trained to minimize the difference between predicted and manually measured VOT; it operates on a set of acoustic feature functions designed based on spectral and temporal cues used by human VOT annotators. The algorithm is applied to initial voiceless stops from four corpora, representing different types of speech. Using several evaluation methods, the algorithms performance is near human intertranscriber reliability, and compares favorably with previous work. Furthermore, the algorithms performance is minimally affected by training and testing on different corpora, and remains essentially constant as the amount of training data is reduced to 50-250 manually labeled examples, demonstrating the methods practical applicability to new datasets. © 2012 Acoustical Society of America.",,"Acoustic features; Automatic measurements; Data sets; Evaluation methods; Labeled data; Speech segments; Structured prediction; Temporal cues; Training and testing; Training data; Voice onset time; Algorithms; Speech recognition; Measurements; algorithm; article; artificial intelligence; automated pattern recognition; automation; comparative study; discriminant analysis; human; methodology; periodicity; phonetics; reproducibility; signal processing; sound detection; speech; speech analysis; statistical model; time; voice; Algorithms; Artificial Intelligence; Automation; Discriminant Analysis; Humans; Linear Models; Pattern Recognition, Automated; Periodicity; Phonetics; Reproducibility of Results; Signal Processing, Computer-Assisted; Sound Spectrography; Speech Acoustics; Speech Production Measurement; Time Factors; Voice Quality",Article,Scopus,2-s2.0-84870893505
"Li J., Newberg J.Y., Uhlén M., Lundberg E., Murphy R.F.","Automated Analysis and Reannotation of Subcellular Locations in Confocal Images from the Human Protein Atlas",2012,"PLoS ONE",13,10.1371/journal.pone.0050514,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870595917&doi=10.1371%2fjournal.pone.0050514&partnerID=40&md5=3380a179bae8560facc3c3b11ec5b30f","The Human Protein Atlas contains immunofluorescence images showing subcellular locations for thousands of proteins. These are currently annotated by visual inspection. In this paper, we describe automated approaches to analyze the images and their use to improve annotation. We began by training classifiers to recognize the annotated patterns. By ranking proteins according to the confidence of the classifier, we generated a list of proteins that were strong candidates for reexamination. In parallel, we applied hierarchical clustering to group proteins and identified proteins whose annotations were inconsistent with the remainder of the proteins in their cluster. These proteins were reexamined by the original annotators, and a significant fraction had their annotations changed. The results demonstrate that automated approaches can provide an important complement to visual annotation. © 2012 Li et al.",,"algorithm; article; automation; cellular distribution; classifier; cluster analysis; image analysis; immunofluorescence; information processing; protein database; protein localization; Artificial Intelligence; Automation; Cell Line, Tumor; Humans; Intracellular Space; Microscopy, Confocal; Molecular Sequence Annotation; Protein Transport",Article,Scopus,2-s2.0-84870595917
"Menon P.P., Ghose D.","Simultaneous source localization and boundary mapping for contaminants",2012,"Proceedings of the American Control Conference",13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869422015&partnerID=40&md5=6ffae6a69f6f0aa73197ed974e795342","This paper addresses the problem of localizing the sources of contaminants spread in the environment, and mapping the boundary of the affected region using an innovative swarm intelligence based technique. Unlike most work in this area the algorithm is capable of localizing multiple sources simultaneously while also mapping the boundary of the contaminant spread. At the same time the algorithm is suitable for implementation using a mobile robotic sensor network. Two types of agents, called the source localization agents (or S-agents) and boundary mapping agents (or B-agents) are used for this purpose. The paper uses the basic glowworm swarm optimization (GSO) algorithm, which has been used only for multiple signal source localization, and modifies it considerably to make it suitable for both these tasks. This requires the definition of new behaviour patterns for the agents based on their terminal performance as well as interactions between them that helps the swarm to split into subgroups easily and identify contaminant sources as well as spread along the boundary to map its full length. Simulations results are given to demonstrate the efficacy of the algorithm. © 2012 AACC American Automatic Control Council).",,"Boundary mapping; Contaminant sources; Mobile robotic; Multiple source; Signal source; Source localization; Swarm Intelligence; Swarm optimization; Algorithms; Artificial intelligence; Impurities; Mapping; Sensor networks; Contamination",Conference Paper,Scopus,2-s2.0-84869422015
"Shahdoosti H.R., Ghassemian H.","Spatial PCA as a new method for image fusion",2012,"AISP 2012 - 16th CSI International Symposium on Artificial Intelligence and Signal Processing",13,10.1109/AISP.2012.6313724,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869193590&doi=10.1109%2fAISP.2012.6313724&partnerID=40&md5=a0365a8020d9a0c4ef41b970f7029158","An ideal fusion method preserves the Spectral information in fused image and adds spatial information to it with no spatial distortion. The PCA is a well-known pan-sharpening approach widely used for its efficiency and high spatial resolution. However, it can distort the spectral characteristics of the multispectral images. In this paper, we present a new fusion method based on the same concept. In conventional standard PCA method, PCA transform is applied to the spectral bands of multispectral images, but we applied PCA transform to the pixel blocks instead. Visual and statistical analyzes show that the proposed algorithm clearly improves the merging quality in terms of: RASE, ERGAS, SAM, correlation coefficient and UIQI; compared to fusion methods including, IHS, Brovey, PCA , HPF, HPM. © 2012 IEEE.","image-fusion; multi-resolution analysis; Principal component analysis (PCA) transform","Correlation coefficient; Fused images; Fusion methods; High spatial resolution; Its efficiencies; Multi-resolutions; Multispectral images; Pan-sharpening; PCA method; Principal component analysis transforms; Spatial distortion; Spatial informations; Spectral band; Spectral characteristics; Spectral information; Artificial intelligence; Principal component analysis; Signal processing; Image fusion",Conference Paper,Scopus,2-s2.0-84869193590
"Ansari-Ram F., Hosseini-Khayat S.","ECG signal compression using compressed sensing with nonuniform binary matrices",2012,"AISP 2012 - 16th CSI International Symposium on Artificial Intelligence and Signal Processing",13,10.1109/AISP.2012.6313763,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869171937&doi=10.1109%2fAISP.2012.6313763&partnerID=40&md5=944264af00fa433507c33b0221f3049e","Wearable ECG sensors can assist in prolonged monitoring of cardiac patients. Compression of ECG signals is pursued as a means to minimize the energy consumed during transmission of information from a portable ECG sensor to a server. In this paper, compressed sensing is employed in ECG compression. To increase compression ratio and reduce distortion of the ECG signal, a non-uniform binary sensing matrix is proposed and evaluated. © 2012 IEEE.","Compressed sensing; Electrocardigram; Signal compression","Binary matrix; Cardiac patients; Compressive sensing; ECG compression; ECG sensors; ECG signals; Electrocardigram; Signal compression; Wearable ecg sensors; Artificial intelligence; Body sensor networks; Compression ratio (machinery); Electrocardiography; Signal reconstruction",Conference Paper,Scopus,2-s2.0-84869171937
"Mao C., Yu X., Chen J., Chen J.","Generating test data for structural testing based on ant colony optimization",2012,"Proceedings - International Conference on Quality Software",13,10.1109/QSIC.2012.12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869105897&doi=10.1109%2fQSIC.2012.12&partnerID=40&md5=42a8f6844a3508dab534d996df123db6","Software testing has been always viewed as an effective way to ensure software quality both in academic and industry. In fact, the quality of test data set plays a critical role in the success of software testing activity. According to the basic line of search-based software testing, we introduced ant colony optimization (ACO) to settle this problem and proposed a framework of ACO-based test data generation. In our algorithm TDG-ACO, the local transfer rule, global transfer rule and pheromone update rule are re-defined to handle the continuous input domain searching. Meanwhile, the most widely-used coverage criterion, i.e., branch coverage, is adopted to construct fitness function. In order to validate the feasibility and effectiveness of our method, five real-world programs are utilized to perform experimental analysis. The results show that our algorithm outperforms the existing simulated annealing and genetic algorithm in most cases. © 2012 IEEE.","Ant colony optimization; Branch coverage; Fitness function; Meta-heuristic search; Test data generation","Ant Colony Optimization (ACO); Branch coverage; Fitness functions; Meta-heuristic search; Test data generation; Artificial intelligence; Computer software selection and evaluation; Data communication systems; Heuristic algorithms; Simulated annealing; Test facilities; Software testing",Conference Paper,Scopus,2-s2.0-84869105897
"Gneo M., Schmid M., Conforto S., D'Alessio T.","A free geometry model-independent neural eye-gaze tracking system",2012,"Journal of NeuroEngineering and Rehabilitation",13,10.1186/1743-0003-9-82,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869063145&doi=10.1186%2f1743-0003-9-82&partnerID=40&md5=43953697128312844711ee706f9234fa","Background: Eye Gaze Tracking Systems (EGTSs) estimate the Point Of Gaze (POG) of a user. In diagnostic applications EGTSs are used to study oculomotor characteristics and abnormalities, whereas in interactive applications EGTSs are proposed as input devices for human computer interfaces (HCI), e.g. to move a cursor on the screen when mouse control is not possible, such as in the case of assistive devices for people suffering from locked-in syndrome. If the user's head remains still and the cornea rotates around its fixed centre, the pupil follows the eye in the images captured from one or more cameras, whereas the outer corneal reflection generated by an IR light source, i.e. glint, can be assumed as a fixed reference point. According to the so-called pupil centre corneal reflection method (PCCR), the POG can be thus estimated from the pupil-glint vector. Methods. A new model-independent EGTS based on the PCCR is proposed. The mapping function based on artificial neural networks allows to avoid any specific model assumption and approximation either for the user's eye physiology or for the system initial setup admitting a free geometry positioning for the user and the system components. The robustness of the proposed EGTS is proven by assessing its accuracy when tested on real data coming from: i) different healthy users; ii) different geometric settings of the camera and the light sources; iii) different protocols based on the observation of points on a calibration grid and halfway points of a test grid. Results: The achieved accuracy is approximately 0.49°, 0.41°, and 0.62° for respectively the horizontal, vertical and radial error of the POG. Conclusions: The results prove the validity of the proposed approach as the proposed system performs better than EGTSs designed for HCI which, even if equipped with superior hardware, show accuracy values in the range 0.6°-1°. © 2012 Gneo et al.; licensee BioMed Central Ltd.","Artificial neural networks; Eye-gaze tracking; Human computer interaction; Pupil center corneal reflection","algorithm; article; artificial intelligence; artificial neural network; biological model; brain computer interface; calibration; eye fixation; eye movement; hemispheric dominance; human; nerve cell; physiology; pupil; reproducibility; Algorithms; Artificial Intelligence; Brain-Computer Interfaces; Calibration; Eye Movements; Fixation, Ocular; Functional Laterality; Humans; Models, Neurological; Neural Networks (Computer); Neurons; Pupil; Reproducibility of Results",Article,Scopus,2-s2.0-84869063145
"Celino I., Contessa S., Corubolo M., Dell'Aglio D., Della Valle E., Fumeo S., Krüger T.","Linking smart cities datasets with human computation - The case of UrbanMatch",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-642-35173-0-3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868523981&doi=10.1007%2f978-3-642-35173-0-3&partnerID=40&md5=67d3c2c83e537a844732c4833d1864b1","To realize the Smart Cities vision, applications can leverage the large availability of open datasets related to urban environments. Those datasets need to be integrated, but it is often hard to automatically achieve a high-quality interlinkage. Human Computation approaches can be employed to solve such a task where machines are ineffective. We argue that in this case not only people's background knowledge is useful to solve the task, but also people's physical presence and direct experience can be successfully exploited. In this paper we present UrbanMatch, a Game with a Purpose for players in mobility aimed at validating links between points of interest and their photos; we discuss the design choices and we show the high throughput and accuracy achieved in the interlinking task. © 2012 Springer-Verlag Berlin Heidelberg.",,"Background knowledge; Data sets; Direct experience; High quality; High throughput; Human computation; Points of interest; Urban environments; Artificial intelligence; Electronic commerce",Conference Paper,Scopus,2-s2.0-84868523981
"Felner A., Goldenberg M., Sharon G., Stern R., Beja T., Sturtevant N., Schaeffer J., Holte R.C.","Partial-expansion A* with selective node generation",2012,"Proceedings of the National Conference on Artificial Intelligence",13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868289652&partnerID=40&md5=6adfc3774ffb6b7217cffccb24eb2c55","A* is often described as being 'optimal', in that it expands the minimum number of unique nodes. But, A* may generate many extra nodes which are never expanded. This is a performance loss, especially when the branching factor is large. Partial Expansion A* (PEA*) (Yoshizumi, Miura, and Ishida 2000) addresses this problem when expanding a node, n, by generating all the children of n but only storing children with the same f-cost as n. n is re-inserted into the OPEN list, but with the f-cost of the next best child. This paper introduces an enhanced version of PEA* (EPEA*). Given a priori domain knowledge, EPEA* generates only the children with the same f-cost as the parent. EPEA* is generalized to its iterative-deepening variant, EPE-IDA*. For some domains, these algorithms yield substantial performance improvements. State-of-the-art results were obtained for the pancake puzzle and for some multi-agent pathfinding instances. Drawbacks of EPEA* are also discussed. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Branching factors; Domain knowledge; Node generation; Pathfinding; Performance improvements; Performance loss; Costs; Iterative methods; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868289652
"Li S., Wang R., Zhou G.","Opinion target extraction using a shallow semantic parsing framework",2012,"Proceedings of the National Conference on Artificial Intelligence",13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868293481&partnerID=40&md5=4b57632f667bbe096bd72405cd12ffdd","In this paper, we present a simplified shallow semantic parsing approach to extracting opinion targets. This is done by formulating opinion target extraction (OTE) as a shallow semantic parsing problem with the opinion expression as the predicate and the corresponding targets as its arguments. In principle, our parsing approach to OTE differs from the state of the art sequence labeling one in two aspects. First, we model OTE from parse tree level, where abundant structured syntactic information is available for use, instead of word sequence level, where only lexical information is available. Second, we focus on determining whether a constituent, rather than a word, is an opinion target or not, via a simplified shallow semantic parsing framework. Evaluation on two datasets shows that structured syntactic information plays a critical role in capturing the domination relationship between an opinion expression and its targets. It also shows that our parsing approach much outperforms the state of the art sequence labeling one. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Data sets; Lexical information; Parse trees; Sequence Labeling; Shallow semantic parsing; State of the art; Syntactic information; Target extraction; Artificial intelligence; Semantics",Conference Paper,Scopus,2-s2.0-84868293481
"Hawkin J., Holte R.C., Szafron D.","Using sliding windows to generate action abstractions in extensive-form games",2012,"Proceedings of the National Conference on Artificial Intelligence",13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868280881&partnerID=40&md5=b4f1e65dd1e658b0bf2897b3af951382","In extensive-form games with a large number of actions, careful abstraction of the action space is critically important to performance. In this paper we extend previous work on action abstraction using no-limit poker games as our test domains. We show that in such games it is no longer necessary to choose, a priori, one specific range of possible bet sizes. We introduce an algorithm that adjusts the range of bet sizes considered for each bet individually in an iterative fashion. This flexibility results in a substantially improved game value in no-limit Leduc poker. When applied to no-limit Texas Hold'em our algorithm produces an action abstraction that is about one third the size of a state of the art hand-crafted action abstraction, yet has a better overall game value. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Action spaces; Extensive-form games; Iterative fashion; Poker game; Sliding Window; State of the art; Texas Hold'em; Algorithms; Artificial intelligence; Iterative methods; Abstracting",Conference Paper,Scopus,2-s2.0-84868280881
"Huang R., Riloff E.","Modeling textual cohesion for event extraction",2012,"Proceedings of the National Conference on Artificial Intelligence",13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868266866&partnerID=40&md5=d78fe1361c697dea1726f21601533c80","Event extraction systems typically locate the role fillers for an event by analyzing sentences in isolation and identifying each role filler independently of the others. We argue that more accurate event extraction requires a view of the larger context to decide whether an entity is related to a relevant event. We propose a bottom-up approach to event extraction that initially identifies candidate role fillers independently and then uses that information as well as discourse properties to model textual cohesion. The novel component of the architecture is a sequentially structured sentence classifier that identifies event-related story contexts. The sentence classifier uses lexical associations and discourse relations across sentences, as well as domain-specific distributions of candidate role fillers within and across sentences. This approach yields state-of-the-art performance on the MUC-4 data set, achieving substantially higher precision than previous systems. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Bottom up approach; Data sets; Domain specific; Event extraction; Novel component; Sentence classifiers; State-of-the-art performance; Adhesion; Artificial intelligence; Data mining; Fillers",Conference Paper,Scopus,2-s2.0-84868266866
"Bowers S., McPhillips T., Ludäscher B.","Declarative rules for inferring fine-grained data provenance from scientific workflow execution traces",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-642-34222-6_7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868267011&doi=10.1007%2f978-3-642-34222-6_7&partnerID=40&md5=ae853dd2e53ec077edb1669f2fffd434","Fine-grained dependencies within scientific workflow provenance specify lineage relationships between a workflow result and the input data, intermediate data, and computation steps used in the result's derivation. This information is often needed to determine the quality and validity of scientific data, and as such, plays a key role in both provenance standardization efforts and provenance query frameworks. While most scientific workflow systems can record basic information concerning the execution of a workflow, they typically fall into one of three categories with respect to recording dependencies: (1) they rely on workflow computation steps to declare dependency relationships at runtime; (2) they impose implicit assumptions concerning dependency patterns from which dependencies are automatically inferred; or (3) they do not assert any dependency information at all. We present an alternative approach that decouples dependency inference from workflow systems and underlying execution traces. In particular, we present a high-level declarative language for expressing explicit dependency rules that can be applied (at any time) to workflow trace events to generate fine-grained dependency information. This approach not only makes provenance dependency rules explicit, but allows rules to be specified and refined by different users as needed. We present our dependency rule language and implementation that rewrites dependency rules into relational queries over underlying workflow traces. We also demonstrate the language using common types of dependency patterns found within scientific workflows. © 2012 Springer-Verlag.",,"Alternative approach; Computation steps; Data provenance; Declarative Languages; Dependency informations; Dependency relationship; Dependency rules; Execution trace; Information concerning; Input datas; Query framework; Relational queries; Runtimes; Scientific data; Scientific workflows; Work-flow systems; Artificial intelligence; Data flow analysis",Conference Paper,Scopus,2-s2.0-84868267011
"Khoa N.L.D., Chawla S.","Large scale spectral clustering using resistance distance and spielman-teng solvers",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-642-33492-4_4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868113132&doi=10.1007%2f978-3-642-33492-4_4&partnerID=40&md5=6a808dabca84ee507a34d15dd686e81f","The promise of spectral clustering is that it can help detect complex shapes and intrinsic manifold structure in large and high dimensional spaces. The price for this promise is the computational cost O(n 3) for computing the eigen-decomposition of the graph Laplacian matrix-so far a necessary subroutine for spectral clustering. In this paper we bypass the eigen-decomposition of the original Laplacian matrix by leveraging the recently introduced Spielman and Teng near-linear time solver for systems of linear equations and random projection. Experiments on several synthetic and real datasets show that the proposed approach has better clustering quality and is faster than the state-of-the-art approximate spectral clustering methods. © 2012 Springer-Verlag Berlin Heidelberg.","random projection; resistance distance; spectral clustering; Spielman-Teng Solver","Clustering quality; Complex shapes; Computational costs; Eigen decomposition; Graph Laplacian; High dimensional spaces; Laplacian matrices; Manifold structures; Near-linear time; Random projections; Real data sets; Resistance distance; Spectral clustering; Spectral clustering methods; Spielman-Teng Solver; Systems of linear equations; Artificial intelligence; Matrix algebra",Conference Paper,Scopus,2-s2.0-84868113132
"De Potter P., Cools H., Depraetere K., Mels G., Debevere P., De Roo J., Huszka C., Colaert D., Mannens E., Van de Walle R.","Semantic patient information aggregation and medicinal decision support",2012,"Computer Methods and Programs in Biomedicine",13,10.1016/j.cmpb.2012.04.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867402641&doi=10.1016%2fj.cmpb.2012.04.002&partnerID=40&md5=f8820cb31f0cd73191ea500555853db4","Although the health care sector has already been subjected to a major computerization effort, this effort is often limited to the implementation of standalone systems which do not communicate with each other. Interoperability problems limit health care applications from achieving their full potential. In this paper, we propose the use of Semantic Web technologies to solve interoperability problems between data providers. Through the development of unifying health care ontologies, data from multiple health care providers can be aggregated, which can then be used as input for a decision support system. This way, more data is taken into account than a single health care provider possesses in his local setting. The feasibility of our approach is demonstrated by the creation of an end-to-end proof of concept, focusing on Belgian health care providers and medicinal decision support. © 2012 Elsevier Ireland Ltd.","Data aggregation; Decision support; Electronic health care; Semantic web","Data aggregation; Decision supports; Electronic healthcare; Health care application; Health care providers; Healthcare sectors; Patient information; Proof of concept; Semantic Web technology; Standalone systems; Artificial intelligence; Decision support systems; Semantic Web; Health care; article; Belgium; clinical pathway; decision support system; electronic medical record; health care; health care delivery; health care personnel; knowledge base; medical decision making; medical information; medical information system; patient information; practice guideline; prescription; privacy; Belgium; Decision Support Techniques; Feasibility Studies; Internet",Article,Scopus,2-s2.0-84867402641
"Smolarz A., Kotyra A., Wójcik W., Ballester J.","Advanced diagnostics of industrial pulverized coal burner using optical methods and artificial intelligence",2012,"Experimental Thermal and Fluid Science",13,10.1016/j.expthermflusci.2012.04.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865547743&doi=10.1016%2fj.expthermflusci.2012.04.001&partnerID=40&md5=42b1ead01b70784be8d084339d5cb3ac","Controlling the combustion process is a very complex issue. The difficulty of operation of such process consists in the mutual interference effects of chemical, physical (mainly energy) and mechanical nature on one hand and risks existing if its course becomes unpredictable. In addition, there are restrictions on the control due to the unavailability of certain process signals (input or output) and incomplete knowledge about them. Current availability of high-speed measuring and computing devices allows to extract the hidden relationships between the elements of such complex process and the use them in control. The paper presents the technologies being developed in the Department of Electronics Lublin University of Technology. They use optical diagnostic methods and artificial intelligence methods. Research is aimed to develop a system allowing a parametric evaluation of the quality of pulverized coal burner operation. It is based on an analysis of local variability of the brightness of the flame. Due to the highly nonlinear nature of dependency and lack of an analytical model, fuzzy-neural methods were used to estimate the selected parameter. The studies, described in the article, confirm that in order to obtain NO. x emissions from pulverized coal burner the estimate calculated on the basis of immediate optical signals can be used instead of the delayed signals from the gas analyzers. The use of neuro-fuzzy models allows to determine emissions of nitrogen oxides with satisfactory accuracy and time, what allows application in control systems. © 2012 Elsevier Inc.","Combustion control; Flame diagnostics; Fuzzy modelling; NOx emission; Optical sensors","Advanced diagnostics; Artificial intelligence methods; Combustion control; Combustion pro-cess; Complex Processes; Computing devices; Fuzzy modelling; Fuzzy-neural; Gas analyzers; High-speed; Highly nonlinear; In-control; Incomplete knowledge; Mutual interference; Neuro-Fuzzy model; NOx emissions; Optical diagnostic methods; Optical methods; Optical signals; Process signals; Pulverized coals; University of Technology; Artificial intelligence; Coal fueled furnaces; Combustion; Fuel burners; Gas emissions; Nitrogen oxides; Optical sensors; Process control",Article,Scopus,2-s2.0-84865547743
"Armstrong S., Sandberg A., Bostrom N.","Thinking inside the box: Controlling and using an oracle AI",2012,"Minds and Machines",13,10.1007/s11023-012-9282-2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869880266&doi=10.1007%2fs11023-012-9282-2&partnerID=40&md5=5c4191d2db8a5c301cbdfcb097257bfd","There is no strong reason to believe that human-level intelligence represents an upper limit of the capacity of artificial intelligence, should it be realized. This poses serious safety issues, since a superintelligent system would have great power to direct the future according to its possibly flawed motivation system. Solving this issue in general has proven to be considerably harder than expected. This paper looks at one particular approach, Oracle AI. An Oracle AI is an AI that does not act in the world except by answering questions. Even this narrow approach presents considerable challenges. In this paper, we analyse and critique various methods of controlling the AI. In general an Oracle AI might be safer than unrestricted AI, but still remains potentially dangerous. © Springer Science+Business Media B.V. 2012.","Artificial intelligence; Capability control; Motivational control; Risks; Security; Superintelligence","Safety issues; Security; Superintelligence; Upper limits; Philosophical aspects; Risks; Artificial intelligence",Article,Scopus,2-s2.0-84869880266
"Kerautret B., Lachaud J.-O.","Meaningful scales detection along digital contours for unsupervised local noise estimation",2012,"IEEE Transactions on Pattern Analysis and Machine Intelligence",13,10.1109/TPAMI.2012.38,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865504682&doi=10.1109%2fTPAMI.2012.38&partnerID=40&md5=da9550bd29d0d582f46ecce012ba6ac9","The automatic detection of noisy or damaged parts along digital contours is a difficult problem since it is hard to distinguish between information and perturbation without further a priori hypotheses. However, solving this issue has a great impact on numerous applications, including image segmentation, geometric estimators, contour reconstruction, shape matching, or image edition. We propose an original strategy to detect what the relevant scales are at which each point of the digital contours should be considered. It relies on theoretical results of asymptotic discrete geometry. A direct consequence is the automatic detection of the noisy or damaged parts of the contour, together with its quantitative evaluation (or noise level). Apart from a given maximal observation scale, the proposed approach does not require any parameter tuning and is easy to implement. We demonstrate its effectiveness on several datasets. We present different direct applications of this local measure to contour smoothing and geometric estimators whose algorithms initially required a noise/scale parameter to tune: They show the pertinence of the proposed measure for digital shape analysis and reconstruction. © 2012 IEEE.","Discrete geometry; Local noise detection; Maximal segments; Shape analysis","Automatic Detection; Contour reconstruction; Data sets; Digital shapes; Discrete geometry; Geometric estimator; Maximal segments; Noise detection; Noise estimation; Noise levels; Parameter-tuning; Quantitative evaluation; Shape analysis; Shape matching; Theoretical result; Artificial intelligence; Computer vision; Geometry",Article,Scopus,2-s2.0-84865504682
"Leong L.C., Basri S., Alfred R.","Enhancing Malay stemming algorithm with background knowledge",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-642-32695-0_68,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867668498&doi=10.1007%2f978-3-642-32695-0_68&partnerID=40&md5=aaed1d16ceb4b3902ca24699f1c57846","Stemming is a process of reducing the inflected words to their root form. Stemming algorithm for Malay language is very important especially in building an effective information retrieval system. Although there are many existing Malay stemmers such as Othman's and Fatimah's algorithms, they are not complete stemmers because their algorithms fail to stem all the Malay words as there is still a room for improvement. It is difficult to implement a perfect stemmer for Malay language due to the complexity of words morphology in Malay language. This paper presents a new approach to stem Malay word with higher percentage of correctly stemmed words. In the proposed approach, additional background knowledge is provided in order to increase the accuracy of stemming words in Malay language. This new approach is called a Malay stemmer with background knowledge. Besides having reference to a dictionary that contains all root words, a second reference to a dictionary is added that contains all affixed words. These two files are considered as the background knowledge that will serve as references for the stemming process. A Rule Frequency Order (RFO) is applied as the basis stemming algorithm due to its high accuracy of correctly stemming Malay words. Based on the results obtained, it is proven that the proposed stemmer with background knowledge produces less error in comparison to previously published stemmers that do not apply any background knowledge in stemming Malay words. © 2012 Springer-Verlag.","affixes; Background Knowledge; Malay stemming; Rule Frequency Order; Rule-based Affix Elimination","affixes; Background knowledge; Frequency orders; Malay stemming; Rule based; Algorithms; Artificial intelligence; Natural language processing systems",Conference Paper,Scopus,2-s2.0-84867668498
"Mader C., Haslhofer B., Isaac A.","Finding quality issues in SKOS vocabularies",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-642-33290-6_25,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867668444&doi=10.1007%2f978-3-642-33290-6_25&partnerID=40&md5=229301a06c6ad15a47329920989f9214","The Simple Knowledge Organization System (SKOS) is a standard model for controlled vocabularies on the Web. However, SKOS vocabularies often differ in terms of quality, which reduces their applicability across system boundaries. Here we investigate how we can support taxonomists in improving SKOS vocabularies by pointing out quality issues that go beyond the integrity constraints defined in the SKOS specification. We identified potential quantifiable quality issues and formalized them into computable quality checking functions that can find affected resources in a given SKOS vocabulary. We implemented these functions in the qSKOS quality assessment tool, analyzed 15 existing vocabularies, and found possible quality issues in all of them. © 2012 Springer-Verlag.",,"Integrity constraints; Knowledge organization systems; Quality assessment; Standard model; System boundary; Artificial intelligence; Digital libraries",Conference Paper,Scopus,2-s2.0-84867668444
"Soltani S., Asadi M., Gašević D., Hatala M., Bagheri E.","Automated planning for feature model configuration based on functional and non-functional requirements",2012,"ACM International Conference Proceeding Series",13,10.1145/2362536.2362548,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867455869&doi=10.1145%2f2362536.2362548&partnerID=40&md5=ade54d7d0b7683cf6a460e0f12e1b3d0","Feature modeling is one of the main techniques used in Software Product Line Engineering to manage the variability within the products of a family. Concrete products of the family can be generated through a configuration process. The configuration process selects and/or removes features from the feature model according to the stakeholders' requirements. Selecting the right set of features for one product from amongst all of the available features in the feature model is a complex task because: 1) the multiplicity of stakeholders' functional requirements; 2) the positive or negative impact of features on non-functional properties; and 3) the stakeholders' preferences w.r.t. the desirable non-functional properties of the final product. Many configurations techniques have already been proposed to facilitate automated product derivation. However, most of the current proposals are not designed to consider stakeholders' preferences and constraints especially with regard to non-functional properties. We address the software product line configuration problem and propose a framework, which employs an artificial intelligence planning technique to automatically select suitable features that satisfy both the stakeholders' functional and non-functional preferences and constraints. We also provide tooling support to facilitate the use of our framework. Our experiments show that despite the complexity involved with the simultaneous consideration of both functional and non-functional properties our configuration technique is scalable. Copyright © 2012 ACM.","Artificial intelligence; Configuration; Feature model; Planning techniques; Software product line engineering","Artificial intelligence planning; Automated planning; Complex task; Configuration; Configuration process; Feature modeling; Feature models; Functional requirement; Non functional properties; Non-functional; Non-functional requirements; Planning techniques; Product derivation; Software Product Line; Software product line engineerings; Techniques used; Artificial intelligence; Concrete products; Software design",Conference Paper,Scopus,2-s2.0-84867455869
"Choong M.Y., Khong W.L., Kow W.Y., Angeline L., Teo K.T.K.","Graph-based image segmentation using k-means clustering and normalised cuts",2012,"Proceedings - 2012 4th International Conference on Computational Intelligence, Communication Systems and Networks, CICSyN 2012",13,10.1109/CICSyN.2012.64,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867365418&doi=10.1109%2fCICSyN.2012.64&partnerID=40&md5=7125a852d477cd22b2fae7ae8ccd0ff5","Image segmentation with low computational burden has been highly regarded as important goal for researchers. Various image segmentation methods are widely discussed and more noble segmentation methods are expected to be developed when there is rapid demand from the emerging machine vision field. One of the popular image segmentation methods is by using normalised cuts algorithm. It is unfavourable for a high resolution image to have its resolution reduced as high detail information is not fully made used when critical objects with weak edges is coarsened undesirably after its resolution reduced. Thus, a graph-based image segmentation method done in multistage manner is proposed here. In this paper, an experimental study based on the method is conducted. This study shows an alternative approach on the segmentation method using k-means clustering and normalised cuts in multistage manner. © 2012 IEEE.","image segmentation; k-means clustering; normalised cuts","Alternative approach; Computational burden; Experimental studies; Graph-based image segmentations; High resolution image; K-means clustering; normalised cuts; Segmentation methods; Weak edge; Artificial intelligence; Communication systems; Computer vision; Graphic methods; Image segmentation",Conference Paper,Scopus,2-s2.0-84867365418
"Gali R., Dewal M.L., Anand R.S.","Genetic algorithm for content based image retrieval",2012,"Proceedings - 2012 4th International Conference on Computational Intelligence, Communication Systems and Networks, CICSyN 2012",13,10.1109/CICSyN.2012.52,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867386555&doi=10.1109%2fCICSyN.2012.52&partnerID=40&md5=5c9acc8d7861c597944ddc5b232191de","In this work for CBIR system, all the image feature descriptors including color descriptors, texture descriptors and shape descriptors are used to represent low-level image features. Implementation of one feature descriptor doesn't give sufficient retrieval accuracy. For combining of different types of features, there is a need to train these features with different weights to achieve good results. A real coded chromosome genetic algorithm (GA) and anyone performance evaluation parameter of CBIR like precision or recall are used as fitness function to optimize feature weights. Meanwhile, a real coded chromosome corresponding to higher precision as fitness function is used to select optimum weights of features. The optimal weights of features computed by GA have improved significantly all the evaluation measures including average precision and average recall for the combined features method. © 2012 IEEE.","CBIR; Features; GA","CBIR; CBIR system; Color descriptors; Combined features; Content based image retrieval; Evaluation measures; Feature descriptors; Feature weight; Features; Fitness functions; Image features; Low-level image features; Optimal weight; Performance evaluation; Real-coded; Retrieval accuracy; Shape descriptors; Texture descriptors; Artificial intelligence; Chromosomes; Communication systems; Content based retrieval; Gallium; Optimization; Genetic algorithms",Conference Paper,Scopus,2-s2.0-84867386555
"Tian J.","Reversed version of a generalized sharp Hölder's inequality and its applications",2012,"Information Sciences",13,10.1016/j.ins.2012.03.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860288850&doi=10.1016%2fj.ins.2012.03.002&partnerID=40&md5=d88158e67327e22812fd3aa32586c873","In this paper, we give a reversed version of a generalized sharp Hölder's inequality which is due to Wu. The results are then used to improve Beckenbach's inequality and Minkowski's inequality. Moreover, as an application in information theory, we present a refinement of Singh's inequality which is one generalized Shannon's inequality. © 2012 Elsevier Inc. All rights reserved.","Beckenbach's inequality; Hölder's inequality; Information theory; Minkowski's inequality; Shannon's inequality; Singh's inequality","Beckenbach's inequality; Minkowski's inequality; Shannon's inequality; Singh's inequality; Artificial intelligence; Software engineering; Information theory",Article,Scopus,2-s2.0-84860288850
"Tishkovsky D., Schmidt R.A., Khodadadi M.","The tableau prover generator MetTeL2",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-642-33353-8_41,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866896079&doi=10.1007%2f978-3-642-33353-8_41&partnerID=40&md5=6a4c9ecef4e5e1de1ddcb9319ff4cd90","This paper introduces METTEL2, a tableau prover generator producing Java code from the specification of a tableau calculus for a logical language. METTEL2 is intended to provide an easy to use system for non-technical users and allow technical users to extend the generated implementations. © 2012 Springer-Verlag.",,"Easy-to-use systems; Java codes; Logical language; Non-technical users; Artificial intelligence; Java programming language",Conference Paper,Scopus,2-s2.0-84866896079
"Andersen M.B., Bolander T., Jensen M.H.","Conditional epistemic planning",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-642-33353-8_8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866886018&doi=10.1007%2f978-3-642-33353-8_8&partnerID=40&md5=453334f559a58555fafe5340b82c2b97","Recent work has shown that Dynamic Epistemic Logic (DEL) offers a solid foundation for automated planning under partial observability and non-determinism. Under such circumstances, a plan must branch if it is to guarantee achieving the goal under all contingencies (strong planning). Without branching, plans can offer only the possibility of achieving the goal (weak planning). We show how to formulate planning in uncertain domains using DEL and give a language of conditional plans. Translating this language to standard DEL gives verification of both strong and weak plans via model checking. In addition to plan verification, we provide a tableau-inspired algorithm for synthesising plans, and show this algorithm to be terminating, sound and complete. © 2012 Springer-Verlag.",,"Automated planning; Conditional plans; Dynamic epistemic logic; Epistemic planning; Non-determinism; Partial observability; Algorithms; Model checking; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84866886018
"Kamran M., Farooq M.","An information-preserving watermarking scheme for right protection of EMR systems",2012,"IEEE Transactions on Knowledge and Data Engineering",13,10.1109/TKDE.2011.223,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866918037&doi=10.1109%2fTKDE.2011.223&partnerID=40&md5=847e4cf1f1653b477292b565c4c84be4","Recently, a significant amount of interest has been developed in motivating physicians to use e-health technology (especially Electronic Medical Records (EMR) systems). An important utility of such EMR systems is: a next generation of Clinical Decision Support Systems (CDSS) will extract knowledge from these electronic medical records to enable physicians to do accurate and effective diagnosis. It is anticipated that in future such medical records will be shared through cloud among different physicians to improve the quality of health care. Therefore, right protection of medical records is important to protect their ownership once they are shared with third parties. Watermarking is a proven well-known technique to achieve this objective. The challenges associated with watermarking of EMR systems are: 1) some fields in EMR are more relevant in the diagnosis process; as a result, small variations in them could change the diagnosis, and 2) a misdiagnosis might not only result in a life threatening scenario but also might lead to significant costs of the treatment for the patients. The major contribution of this paper is an information-preserving watermarking scheme to address the above-mentioned challenges. We model the watermarking process as a constrained optimization problem. We demonstrate, through experiments, that our scheme not only preserves the diagnosis accuracy but is also resilient to well-known attacks for corrupting the watermark. Last but not least, we also compare our scheme with a well-known threshold-based scheme to evaluate relative merits of a classifier. Our pilot studies reveal that-using proposed information-preserving scheme-the overall classification accuracy is never degraded by more than 1 percent. In comparison, the diagnosis accuracy, using the threshold-based technique, is degraded by more than 18 percent in a worst case scenario. © 2012 IEEE.","decision support systems; EMR; medical data watermarking; optimization problems; particle swarm optimization; Right protection; watermarking","Classification accuracy; Clinical decision support systems; Constrained optimization problems; Ehealth; Electronic medical record; EMR; EMR systems; Medical data; Medical record; Optimization problems; Pilot studies; Quality of health care; Right protection; Small variations; Third parties; Watermarking schemes; Worst case scenario; Artificial intelligence; Constrained optimization; Decision support systems; Diagnosis; Digital watermarking; Medical computing; Medical problems; Particle swarm optimization (PSO); Patient treatment; Watermarking",Article,Scopus,2-s2.0-84866918037
"Li J., Isobe T., Shibutani K.","Converting meet-in-the-middle preimage attack into pseudo collision attack: Application to SHA-2",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-642-34047-5_16,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866702460&doi=10.1007%2f978-3-642-34047-5_16&partnerID=40&md5=2144e80e3b330538093c923cf7f9dfb5","In this paper, we present a new technique to construct a collision attack from a particular preimage attack which is called a partial target preimage attack. Since most of the recent meet-in-the-middle preimage attacks can be regarded as the partial target preimage attack, a collision attack is derived from the meet-in-the-middle preimage attack. By using our technique, pseudo collisions of the 43-step reduced SHA-256 and the 46-step reduced SHA-512 can be obtained with complexities of 2 126 and 2 254.5, respectively. As far as we know, our results are the best pseudo collision attacks on both SHA-256 and SHA-512 in literature. Moreover, we show that our pseudo collision attacks can be extended to 52 and 57 steps of SHA-256 and SHA-512, respectively, by combined with the recent preimage attacks on SHA-2 by bicliques. Furthermore, since the proposed technique is quite simple, it can be directly applied to other hash functions. We apply our algorithm to several hash functions including Skein and BLAKE, which are the SHA-3 finalists. We present not only the best pseudo collision attacks on SHA-2 family, but also a new insight of relation between a meet-in-the-middle preimage attack and a pseudo collision attack. © 2012 Springer-Verlag.","BLAKE; hash function; meet-in-the-middle attack; narrow-pipe; preimage attack; pseudo collision attack; SHA-2; Skein","BLAKE; Collision attack; Meet-in-the-middle; narrow-pipe; Preimage attack; SHA-2; Skein; Artificial intelligence; Hash functions",Conference Paper,Scopus,2-s2.0-84866702460
"Kim H., Mansi T., Bernasconi N., Bernasconi A.","Surface-based multi-template automated hippocampal segmentation: Application to temporal lobe epilepsy",2012,"Medical Image Analysis",13,10.1016/j.media.2012.04.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866447191&doi=10.1016%2fj.media.2012.04.008&partnerID=40&md5=0b32dc515a729c35dd40dd118351e98f","In drug-resistant temporal lobe epilepsy (TLE), detecting hippocampal atrophy on MRI is crucial as it allows defining the surgical target. In addition to atrophy, about 40% of patients present with malrotation, a developmental anomaly characterized by atypical morphologies of the hippocampus and collateral sulcus. We have recently shown that both atrophy and malrotation impact negatively the performance of volume-based techniques. Here, we propose a novel hippocampal segmentation algorithm (SurfMulti) that integrates deformable parametric surfaces, vertex-wise modeling of locoregional texture and shape, and multiple templates in a unified framework. To account for inter-subject variability, including shape variants, we used a library derived from a large database of healthy (n= 80) and diseased (n= 288) hippocampi. To quantify malrotation, we generated 3D models from manual hippocampal labels and automatically extracted collateral sulci. The accuracy of SurfMulti was evaluated relative to manual labeling and segmentation obtained through a single atlas-based algorithm (FreeSurfer) and a volume-based multi-template approach (Vol-multi) using the Dice similarity index and surface-based shape mapping, for which we computed vertex-wise displacement vectors between automated and manual segmentations. We then correlated segmentation accuracy with malrotation features and atrophy. SurfMulti outperformed FreeSurfer and Vol-multi, and achieved a level of accuracy in TLE patients (Dice = 86.9%) virtually identical to healthy controls (Dice = 87.5%). Vertex-wise shape mapping showed that SurfMulti had an excellent overlap with manual labels, with sub-millimeter precision. Its performance was not influenced by atrophy or malrotation (|r|<0.20, p> 0.2), while FreeSurfer (|r|>0.35, p< 0.0001) and Vol-multi (|r|>0.28, p< 0.05) were hampered by both anomalies. The magnitude of atrophy detected using SurfMulti was the closest to manual volumetry (Cohen's d: manual = 1.71, t= 7.6; SurfMulti = 1.60, t= 7.0; Vol-multi = 1.38, t= 6.1; FreeSurfer = 0.91, t= 3.9). The high performance of SurfMulti regardless of cohort, atrophy and shape variants identifies this algorithm as a robust segmentation tool for hippocampal volumetry. © 2012 Elsevier B.V.","Automatic segmentation; Hippocampus; Multiple templates; Surface parametrization; Texture","3D models; Automatic segmentations; Displacement vectors; Healthy controls; Hippocampus; Large database; Manual segmentation; Multiple templates; Parametric surfaces; Parametrizations; Robust segmentation; Segmentation accuracy; Segmentation algorithms; Similarity indices; Submillimeters; Surface-based; Temporal lobe epilepsy; Unified framework; Volumetry; Algorithms; Image segmentation; Neurology; Textures; Brain; adult; algorithm; article; brain atrophy; brain function; clinical assessment; controlled study; female; hippocampus; human; major clinical study; male; priority journal; surfmulti; temporal lobe epilepsy; Algorithms; Artificial Intelligence; Epilepsy, Temporal Lobe; Hippocampus; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-84866447191
"Beyer O., Cimiano P.","Online semi-supervised growing neural gas",2012,"International Journal of Neural Systems",13,10.1142/S0129065712500232,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867080102&doi=10.1142%2fS0129065712500232&partnerID=40&md5=0903790aeda1a6840d262acdc35b5b6d","In this paper we introduce online semi-supervised growing neural gas (OSSGNG), a novel online semi-supervised classification approach based on growing neural gas (GNG). Existing semi-supervised classification approaches based on GNG require that the training data is explicitly stored as the labeling is performed a posteriori after the training phase. As main contribution, we present an approach that relies on online labeling and prediction functions to process labeled and unlabeled data uniformly and in an online fashion, without the need to store any of the training examples explicitly. We show that using on-the-fly labeling strategies does not significantly deteriorate the performance of classifiers based on GNG, while circumventing the need to explicitly store training examples. Armed with this result, we then present a semi-supervised extension of GNG (OSSGNG) that relies on the above mentioned online labeling functions to label unlabeled examples and incorporate them into the model on-the-fly. As an important result, we show that OSSGNG performs as good as previous semi-supervised extensions of GNG which rely on offline labeling strategies. We also show that OSSGNG compares favorably to other state-of-the-art semi-supervised learning approaches on standard benchmarking datasets. © 2012 World Scientific Publishing Company.","Classification; Neural networks; Online labeling; Semi-supervised learning; Topological maps","Data sets; Growing neural gas; Labeled and unlabeled data; Labeling functions; Labeling strategy; Offline; On-line fashion; On-the-fly; Performance of classifier; Posteriori; Prediction function; Semi-supervised; Semi-supervised classification; Semi-supervised learning; Topological map; Training data; Training example; Training phase; Classification (of information); Computer networks; Neural networks; Supervised learning; algorithm; article; artificial intelligence; artificial neural network; classification; computer simulation; factual database; methodology; online system; Algorithms; Artificial Intelligence; Classification; Computer Simulation; Databases, Factual; Neural Networks (Computer); Online Systems",Conference Paper,Scopus,2-s2.0-84867080102
"Kwitt R., Vasconcelos N., Rasiwasia N., Uhl A., Davis B., Häfner M., Wrba F.","Endoscopic image analysis in semantic space",2012,"Medical Image Analysis",13,10.1016/j.media.2012.04.010,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866445738&doi=10.1016%2fj.media.2012.04.010&partnerID=40&md5=c0395dc87b178e08e70486b9522e076e","A novel approach to the design of a semantic, low-dimensional, encoding for endoscopic imagery is proposed. This encoding is based on recent advances in scene recognition, where semantic modeling of image content has gained considerable attention over the last decade. While the semantics of scenes are mainly comprised of environmental concepts such as vegetation, mountains or sky, the semantics of endoscopic imagery are medically relevant visual elements, such as polyps, special surface patterns, or vascular structures. The proposed semantic encoding differs from the representations commonly used in endoscopic image analysis (for medical decision support) in that it establishes a semantic space, where each coordinate axis has a clear human interpretation. It is also shown to establish a connection to Riemannian geometry, which enables principled solutions to a number of problems that arise in both physician training and clinical practice. This connection is exploited by leveraging results from information geometry to solve problems such as (1) recognition of important semantic concepts, (2) semantically-focused image browsing, and (3) estimation of the average-case semantic encoding for a collection of images that share a medically relevant visual detail. The approach can provide physicians with an easily interpretable, semantic encoding of visual content, upon which further decisions, or operations, can be naturally carried out. This is contrary to the prevalent practice in endoscopic image analysis for medical decision support, where image content is primarily captured by discriminative, high-dimensional, appearance features, which possess discriminative power but lack human interpretability. © 2012 Elsevier B.V.","Image understanding; Pit pattern analysis; Semantic modeling","Average-case; Clinical practices; Endoscopic image; High-dimensional; Image browsing; Image content; Information geometry; Interpretability; Medical decision making; Pit patterns; Riemannian geometry; Scene recognition; Semantic concept; Semantic modeling; Semantic Space; Special surfaces; Vascular structures; Visual content; Visual elements; Encoding (symbols); Endoscopy; Geometry; Image analysis; Image coding; Image understanding; Semantics; article; controlled study; endoscopy; geometry; human; image analysis; medical decision making; polyp; priority journal; vegetation; Algorithms; Artificial Intelligence; Endoscopy; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84866445738
"Eichner M., Ferrari V.","Human pose co-estimation and applications",2012,"IEEE Transactions on Pattern Analysis and Machine Intelligence",13,10.1109/TPAMI.2012.85,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866644964&doi=10.1109%2fTPAMI.2012.85&partnerID=40&md5=5e40f400d7643cbccd8c669c8f03fbc3","Most existing techniques for articulated Human Pose Estimation (HPE) consider each person independently. Here we tackle the problem in a new setting, coined Human Pose Coestimation (PCE), where multiple people are in a common, but unknown pose. The task of PCE is to estimate their poses jointly and to produce prototypes characterizing the shared pose. Since the poses of the individual people should be similar to the prototype, PCE has less freedom compared to estimating each pose independently, which simplifies the problem. We demonstrate our PCE technique on two applications. The first is estimating the pose of people performing the same activity synchronously, such as during aerobics, cheerleading, and dancing in a group. We show that PCE improves pose estimation accuracy over estimating each person independently. The second application is learning prototype poses characterizing a pose class directly from an image search engine queried by the class name (e.g., ""lotus pose""). We show that PCE leads to better pose estimation in such images, and it learns meaningful prototypes which can be used as priors for pose estimation in novel images. © 2012 IEEE.","articulated objects; Human pose estimation; multiple image correspondence; object detection","Articulated object; Cheerleading; Human pose; Human pose estimations; Image search engine; Multiple image; Multiple people; Object Detection; Pose estimation; Artificial intelligence; Computer vision; Estimation; algorithm; article; artificial intelligence; automated pattern recognition; body posture; computer assisted diagnosis; human; image enhancement; image subtraction; methodology; physiology; reproducibility; sensitivity and specificity; whole body imaging; Algorithms; Artificial Intelligence; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Posture; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique; Whole Body Imaging",Article,Scopus,2-s2.0-84866644964
"Khan M., Azamathulla H.M., Tufail M., Ab Ghani A.","Bridge pier scour prediction by gene expression programming",2012,"Proceedings of the Institution of Civil Engineers: Water Management",13,10.1680/wama.11.00008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871043160&doi=10.1680%2fwama.11.00008&partnerID=40&md5=bd0eb77ac707e91399b66fcc98ad19c4","Extensive research has been carried out to predict bridge pier scour, with laboratory and field data, using different modelling techniques. This study introduces a new soft computing technique called gene expression programming (GEP) for pier scour depth prediction using field data. A functional relationship has been established using GEP and its performance is compared with other inductive modelling techniques such as artificial neural networks (ANNs) and conventional regression-based techniques. Field data comprising 370 data sets were collected from the published literature and divided into calibration and validation (testing) data sets. The performance of GEP was found to be satisfactory and encouraging when compared with regression and ANN models in predicting bridge pier scour depth. GEP has the unique capability of providing a compact and explicit mathematical expression for computing bridge scour. This advantage of GEP over ANN is one of the main motivations for this work. The resulting GEP models add to the existing literature on artificial intelligence based inductive models that can be used effectively for bridge scour modelling.","Bridges; Design methods and aids; Mathematical modelling","Bridge scour; Calibration and validations; Data sets; Design method; Field data; Functional relationship; Gene expression programming; Inductive models; Mathematical expressions; Modelling techniques; Pier scour; Softcomputing techniques; Bridge piers; Bridges; Forecasting; Mathematical models; Neural networks; Soft computing; Scour; artificial intelligence; artificial neural network; bridge; calibration; mathematical analysis; model test; model validation; numerical model; pier; prediction; regression analysis; scour",Article,Scopus,2-s2.0-84871043160
"Pillai R.R., Divekar R., Brasier A., Bhavnani S., Calhoun W.J.","Strategies for molecular classification of asthma using bipartite network analysis of cytokine expression",2012,"Current Allergy and Asthma Reports",13,10.1007/s11882-012-0279-y,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870567124&doi=10.1007%2fs11882-012-0279-y&partnerID=40&md5=935a3f2932496fd9d8d2fb8dc56eb0d7","Asthma is a chronic inflammatory disease of the airways that leads to various degrees of recurrent respiratory symptoms affecting patients globally. Specific subgroups of asthma patients have severe disease leading to increased healthcare costs and socioeconomic burden. Despite the overwhelming prevalence of the asthma, there are limitations in predicting response to therapy and identifying patients who are at increased risk of morbidity. This syndrome presents with common clinical signs and symptoms; however, awareness of subgroups of asthma patients with distinct characteristics has surfaced in recent years. Investigators attempt to describe the phenotypes of asthma to ultimately assist with diagnostic and therapeutic applications.Approaches to asthma phenotyping are multifold; however, it can be partitioned into 2 essential groups, clinical phenotyping and molecular phenotyping. Innovative techniques such as bipartite network analysis and visual analytics introduce a new dimension of data analysis to identify underlying mechanistic pathways. © Springer Science+Business Media, LLC 2012.","Airway; Allergic; Asthma; Bipartite; Exercise induced; Heterogeneous; Inflammation; Logistic regression; Molecular; Multivariate adaptive regression splines (MARS) Cytokines Cluster analysis Classification regression trees (CART); Network; Phenotype","corticosteroid; cytokine; immunoglobulin E; interleukin 13; interleukin 4; interleukin 5; salbutamol; steroid; transcription factor RUNX2; allergic asthma; article; asthma; atopy; bronchospasm; clinical feature; disease classification; dyspnea; exercise induced asthma; forced expiratory volume; forced vital capacity; gene expression profiling; health care cost; human; lung function test; lung lavage; lymphocyte differentiation; molecular dynamics; morbidity; mortality; network learning; pathophysiology; phenotype; prevalence; protein expression; proteomics; signal transduction; single nucleotide polymorphism; socioeconomics; Artificial Intelligence; Asthma; Asthma, Exercise-Induced; Cluster Analysis; Cytokines; Gene Expression Regulation; Humans; Logistic Models; Models, Biological; Neural Networks (Computer); Phenotype; Respiratory Hypersensitivity; Systems Biology",Article,Scopus,2-s2.0-84870567124
"van den Berg B.A., Reinders M.J.T., Hulsman M., Wu L., Pel H.J., Roubos J.A., de Ridder D.","Exploring Sequence Characteristics Related to High-Level Production of Secreted Proteins in Aspergillus niger",2012,"PLoS ONE",13,10.1371/journal.pone.0045869,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866972942&doi=10.1371%2fjournal.pone.0045869&partnerID=40&md5=ee1d4ce414738117b4b6ee003b1aac7f","Protein sequence features are explored in relation to the production of over-expressed extracellular proteins by fungi. Knowledge on features influencing protein production and secretion could be employed to improve enzyme production levels in industrial bioprocesses via protein engineering. A large set, over 600 homologous and nearly 2,000 heterologous fungal genes, were overexpressed in Aspergillus niger using a standardized expression cassette and scored for high versus no production. Subsequently, sequence-based machine learning techniques were applied for identifying relevant DNA and protein sequence features. The amino-acid composition of the protein sequence was found to be most predictive and interpretation revealed that, for both homologous and heterologous gene expression, the same features are important: tyrosine and asparagine composition was found to have a positive correlation with high-level production, whereas for unsuccessful production, contributions were found for methionine and lysine composition. The predictor is available online at http://bioinformatics.tudelft.nl/hipsec. Subsequent work aims at validating these findings by protein engineering as a method for increasing expression levels per gene copy. © 2012 van den Berg et al.",,"asparagine; fungal DNA; fungal protein; lysine; methionine; tyrosine; amino acid sequence; article; Aspergillus niger; bioprocess; classification; classifier; enzyme synthesis; fungal gene; fungal strain; gene overexpression; machine learning; molecular biology; nonhuman; polyacrylamide gel electrophoresis; prediction; protein engineering; protein secretion; support vector machine; Amino Acid Sequence; Artificial Intelligence; Aspergillus niger; Computational Biology; Electrophoresis, Polyacrylamide Gel; Enzymes; Fungal Proteins; Gene Expression Profiling; Genes, Fungal; Genetic Engineering; Industrial Microbiology; Molecular Sequence Data; Aspergillus niger; Fungi",Article,Scopus,2-s2.0-84866972942
"Babayigit B., Ozdemir R.","A modified artificial bee colony algorithm for numerical function optimization",2012,"Proceedings - IEEE Symposium on Computers and Communications",13,10.1109/ISCC.2012.6249302,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866612685&doi=10.1109%2fISCC.2012.6249302&partnerID=40&md5=7c377c22af23c9e34d1c9cc86289d4d4","Artificial bee colony (ABC) algorithm, explored in recent literature, is an efficient optimization technique which simulates the foraging behavior of honeybees. ABC algorithm is good at exploration but poor at exploitation. This paper presents a new modified ABC algorithm for numerical optimization problems to improve the exploitation capability of the ABC algorithm. A different probability function and a new searching mechanism are proposed. The modified ABC algorithm is tested on seven numerical optimization problems. The results demonstrate that the modified ABC algorithm outperforms the ABC algorithm on solution quality and faster convergence. © 2012 IEEE.","modified artificial bee colony algorithm; numerical function optimization; Swarm intelligence","Abc algorithms; Artificial bee colonies; Artificial bee colony algorithms; Faster convergence; Foraging behaviors; Numerical function optimization; Numerical optimizations; Optimization techniques; Probability functions; Searching mechanism; Solution quality; Swarm Intelligence; Artificial intelligence; Evolutionary algorithms; Optimization",Conference Paper,Scopus,2-s2.0-84866612685
"Morgan R., Gallagher M.","Length scale for characterising continuous optimization problems",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-642-32937-1_41,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866388304&doi=10.1007%2f978-3-642-32937-1_41&partnerID=40&md5=1c742e699ed1b2243a83bd365cd9a8a9","In metaheuristic optimization, understanding the relationship between problems and algorithms is important but non-trivial. There has been a growing interest in the literature on techniques for analysing problems, however previous work has mainly been developed for discrete problems. In this paper, we develop a novel framework for characterising continuous optimization problems based on the concept of length scale. We argue that length scale is an important property for the characterisation of continuous problems that is not captured by existing techniques. Intuitively, length scale measures the ratio of changes in the objective function value to steps between points in the search space. The concept is simple, makes few assumptions and can be calculated or estimated based only on the information available in black-box optimization (objective function values and search points). Some fundamental properties of length scale and its distribution are described. Experimental results show the potential use of length scale and directions to develop the framework further are discussed. © 2012 Springer-Verlag.","Continuous optimization; Fitness landscape analysis; Problem characterisation; Problem properties","Black-box optimization; Continuous optimization; Continuous optimization problems; Continuous problems; Discrete problems; Fitness landscape analysis; Fundamental properties; Length scale; Metaheuristic optimization; Non-trivial; Objective function values; Problem characterisation; Problem properties; Search spaces; Artificial intelligence; Optimization",Conference Paper,Scopus,2-s2.0-84866388304
"Hecker J.P., Letendre K., Stolleis K., Washington D., Moses M.E.","Formica ex machina: Ant swarm foraging from physical to virtual and back again",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-642-32650-9_25,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866419793&doi=10.1007%2f978-3-642-32650-9_25&partnerID=40&md5=c8c2cb8fe38bf6b618e4516f21151067","Ants use individual memory and pheromone communication to forage efficiently. We implement these strategies as distributed search algorithms in robotic swarms. Swarms of simple robots are robust, scalable and capable of exploring for resources in unmapped environments. We test the ability of individual robots and teams of three robots to collect tags distributed in random and clustered distributions in simulated and real environments. Teams of three real robots that forage based on individual memory without communication collect RFID tags approximately twice as fast as a single robot using the same strategy. Our simulation system mimics the foraging behaviors of the robots and replicates our results. Simulated swarms of 30 and 100 robots collect tags 8 and 22 times faster than teams of three robots. This work demonstrates the feasibility of programming large robot teams for collective tasks such as retrieval of dispersed resources, mapping, and environmental monitoring. It also lays a foundation for evolving collective search algorithms in silico and then implementing those algorithms in machina in robust and scalable robotic swarms. © 2012 Springer-Verlag.",,"Distributed search; Environmental Monitoring; Foraging behaviors; Formica; In-silico; Pheromone communication; Real environments; Real robot; RF-ID tags; Robot teams; Robotic swarms; Search Algorithms; Simulation systems; Single robots; Artificial intelligence; Communication; Learning algorithms; Melamine formaldehyde resins; Robotics; Robots; Robot programming",Conference Paper,Scopus,2-s2.0-84866419793
"Belle A., Hargraves R.H., Najarian K.","An automated optimal engagement and attention detection system using electrocardiogram",2012,"Computational and Mathematical Methods in Medicine",13,10.1155/2012/528781,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866167954&doi=10.1155%2f2012%2f528781&partnerID=40&md5=9ec8dbe3cd7bd4835f7f21a521468020","This research proposes to develop a monitoring system which uses Electrocardiograph (ECG) as a fundamental physiological signal, to analyze and predict the presence or lack of cognitive attention in individuals during a task execution. The primary focus of this study is to identify the correlation between fluctuating level of attention and its implications on the cardiac rhythm recorded in the ECG. Furthermore, Electroencephalograph (EEG) signals are also analyzed and classified for use as a benchmark for comparison with ECG analysis. Several advanced signal processing techniques have been implemented and investigated to derive multiple clandestine and informative features from both these physiological signals. Decomposition and feature extraction are done using Stockwell-transform for the ECG signal, while Discrete Wavelet Transform (DWT) is used for EEG. These features are then applied to various machine-learning algorithms to produce classification models that are capable of differentiating between the cases of a person being attentive and a person not being attentive. The presented results show that detection and classification of cognitive attention using ECG are fairly comparable to EEG. © 2012 Ashwin Belle et al.",,"algorithm; article; attention; DiscreteWavelet Transform; electrical parameters; electrocardiography; electroencephalography; human; task performance; artificial intelligence; automation; biology; cognition; computer program; computer simulation; electroencephalography; electrophysiology; methodology; signal processing; statistical model; wavelet analysis; Algorithms; Artificial Intelligence; Automation; Cognition; Computational Biology; Computer Simulation; Electrocardiography; Electroencephalography; Electrophysiology; Humans; Models, Statistical; Signal Processing, Computer-Assisted; Software; Wavelet Analysis",Article,Scopus,2-s2.0-84866167954
"Cousineau D., Doligez D., Lamport L., Merz S., Ricketts D., Vanzetto H.","TLA + proofs",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-642-32759-9_14,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865982164&doi=10.1007%2f978-3-642-32759-9_14&partnerID=40&md5=627f40a66a13b1bd02b11d671bc83567","TLA + is a specification language based on standard set theory and temporal logic that has constructs for hierarchical proofs. We describe how to write TLA + proofs and check them with TLAPS, the TLA + Proof System. We use Peterson's mutual exclusion algorithm as a simple example and show how TLAPS and the Toolbox (an IDE for TLA +) help users to manage large, complex proofs. © 2012 Springer-Verlag.",,"Peterson's mutual exclusion algorithm; Proof system; Artificial intelligence; Specification languages",Conference Paper,Scopus,2-s2.0-84865982164
"Jaatun M.G., Bernsmed K., Undheim A.","Security SLAs - An idea whose time has come?",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-642-32498-7_10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865660894&doi=10.1007%2f978-3-642-32498-7_10&partnerID=40&md5=1320725603e3bb98b27ae907f96e64af","Service Level Agreements (SLAs) have been used for decades to regulate aspects such as throughput, delay and response times of services in various outsourcing scenarios. However, security aspects have typically been neglected in SLAs. In this paper we argue that security SLAs will be necessary for future Internet services, and provide examples of how this will work in practice. © 2012 IFIP International Federation for Information Processing.",,"Future internet; Service Level Agreements; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84865660894
"Kartakis S., Sakkalis V., Tourlakis P., Zacharioudakis G., Stephanidis C.","Enhancing health care delivery through ambient intelligence applications",2012,"Sensors (Switzerland)",13,10.3390/s120911435,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866990932&doi=10.3390%2fs120911435&partnerID=40&md5=c6474345a6d1a5322a7728c072b6bd71","This paper presents the implementation of a smart environment that employs Ambient Intelligence technologies in order to augment a typical hospital room with smart features that assist both patients and medical staff. In this environment various wireless and wired sensor technologies have been integrated, allowing the patient to control the environment and interact with the hospital facilities, while a clinically oriented interface allows for vital sign monitoring. The developed applications are presented both from a patient's and a doctor's perspective, offering different services depending on the user's role. The results of the evaluation process illustrate the need for such a service, leading to important conclusions about the usefulness and crucial role of AmI in health care. © 2012 by the authors; licensee MDPI, Basel, Switzerland.","Ambient intelligence (AmI); E-health; Health care; Smart hospital; Smart patient rooms; User interface development","Ambient intelligence; Ehealth; Smart hospital; Smart patient rooms; User interface development; Health care; Hospitals; User interfaces; Artificial intelligence; Ambient Intelligence (AmI); ambulatory monitoring; article; health care; health care delivery; human; medical staff; methodology; smart hospital; smart patient rooms; telehealth; telemedicine; user interface development; Ambient Intelligence (AmI); e-health; health care; smart hospital; smart patient rooms; user interface development; Delivery of Health Care; Humans; Medical Staff; Monitoring, Ambulatory; Telemedicine",Article,Scopus,2-s2.0-84866990932
"Jonnalagadda S.R., Li D., Sohn S., Wu S.T.-I., Wagholikar K., Torii M., Liu H.","Coreference analysis in clinical notes: A multi-pass sieve with alternate anaphora resolution modules",2012,"Journal of the American Medical Informatics Association",13,10.1136/amiajnl-2011-000766,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872240730&doi=10.1136%2famiajnl-2011-000766&partnerID=40&md5=fb91b479ec4f2eae542668d364f94554","Objective This paper describes the coreference resolution system submitted by Mayo Clinic for the 2011 i2b2/VA/Cincinnati shared task Track 1C. The goal of the task was to construct a system that links the markables corresponding to the same entity. Materials and methods The task organizers provided progress notes and discharge summaries that were annotated with the markables of treatment, problem, test, person, and pronoun. We used a multi-pass sieve algorithm that applies deterministic rules in the order of preciseness and simultaneously gathers information about the entities in the documents. Our system, MedCoref, also uses a state-of-the-art machine learning framework as an alternative to the final, rule-based pronoun resolution sieve. Results The best system that uses a multi-pass sieve has an overall score of 0.836 (average of B3, MUC, Blanc, and CEAF F score) for the training set and 0.843 for the test set. Discussion A supervised machine learning system that typically uses a single function to find coreferents cannot accommodate irregularities encountered in data especially given the insufficient number of examples. On the other hand, a completely deterministic system could lead to a decrease in recall (sensitivity) when the rules are not exhaustive. The sieve-based framework allows one to combine reliable machine learning components with rules designed by experts. Conclusion Using relatively simple rules, part-of-speech information, and semantic type properties, an effective coreference resolution system could be designed. The source code of the system described is available at https:// sourceforge.net/projects/ohnlp/files/MedCoref.",,"algorithm; article; factorial hidden Markov model; gold standard; hospital discharge; human; information retrieval; language processing; machine learning; reliability; semantics; sensitivity analysis; algorithm; artificial intelligence; data mining; decision support system; electronic medical record; evaluation; methodology; multicenter study; natural language processing; probability; sensitivity and specificity; United States; Algorithms; Artificial Intelligence; Data Mining; Decision Support Systems, Clinical; Electronic Health Records; Humans; Markov Chains; Natural Language Processing; Semantics; Sensitivity and Specificity; United States",Article,Scopus,2-s2.0-84872240730
"Alkim E., Gürbüz E., Kiliç E.","A fast and adaptive automated disease diagnosis method with an innovative neural network model",2012,"Neural Networks",13,10.1016/j.neunet.2012.04.010,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863874926&doi=10.1016%2fj.neunet.2012.04.010&partnerID=40&md5=76ac7bf06c921a1ac37cbf3693c1b4bd","Automatic disease diagnosis systems have been used for many years. While these systems are constructed, the data used needs to be classified appropriately. For this purpose, a variety of methods have been proposed in the literature so far. As distinct from the ones in the literature, in this study, a general-purpose, fast and adaptive disease diagnosis system is developed. This newly proposed method is based on Learning Vector Quantization (LVQ) artificial neural networks which are powerful classification algorithms. In this study, the classification ability of LVQ networks is developed by embedding a reinforcement mechanism into the LVQ network in order to increase the success rate of the disease diagnosis method and reduce the decision time. The parameters of the reinforcement learning mechanism are updated in an adaptive way in the network. Thus, the loss of time due to incorrect selection of the parameters and decrement in the success rate are avoided. After the development process mentioned, the newly proposed classification technique is named ""Adaptive LVQ with Reinforcement Mechanism (ALVQ-RM)"". The method proposed handles data with missing values. To prove that this method did not offer a special solution for a particular disease, because of its adaptive structure, it is used both for diagnosis of breast cancer, and for diagnosis of thyroid disorders, and a correct diagnosis rate after replacing missing values using median method over 99.5% is acquired in average for both diseases. In addition, the success rate of determination of the parameters of the proposed ""LVQ with Reinforcement Mechanism (LVQ-RM)"" classifier, and how this determination affected the required number of iterations for acquiring that success rate are discussed with comparison to the other studies. © 2012 Elsevier Ltd.","Adaptive learning vector quantization with reinforcement mechanism; Artificial intelligence; Diagnosis; Diseases","Adaptive structure; Breast Cancer; Classification ability; Classification algorithm; Classification technique; Decision time; Development process; Disease diagnosis; Learning Vector Quantization; LVQ networks; Missing values; Neural network model; Number of iterations; Reinforcement mechanisms; Thyroid disorders; Artificial intelligence; Diseases; Neural networks; Reinforcement learning; Vector quantization; Diagnosis; adaptive learning vector quantization with reinforcement mechanism; article; artificial intelligence; artificial neural network; breast cancer; classification; classification algorithm; classifier; computer assisted diagnosis; priority journal; thyroid disease; Breast Neoplasms; Databases, Factual; Female; Humans; Neural Networks (Computer); Pattern Recognition, Automated; Thyroid Diseases; Time Factors",Article,Scopus,2-s2.0-84863874926
"Du Jardin P., Séverin E.","Forecasting financial failure using a Kohonen map: A comparative study to improve model stability over time",2012,"European Journal of Operational Research",13,10.1016/j.ejor.2012.04.006,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861094050&doi=10.1016%2fj.ejor.2012.04.006&partnerID=40&md5=cd42621d63194b86ca1c4838f64d3d0c","This study attempts to show how a Kohonen map can be used to improve the temporal stability of the accuracy of a financial failure model. Most models lose a significant part of their ability to generalize when data used for estimation and prediction purposes are collected over different time periods. As their lifespan is fairly short, it becomes a real problem if a model is still in use when re-estimation appears to be necessary. To overcome this drawback, we introduce a new way of using a Kohonen map as a prediction model. The results of our experiments show that the generalization error achieved with a map remains more stable over time than that achieved with conventional methods used to design failure models (discriminant analysis, logistic regression, Cox's method, and neural networks). They also show that type-I error, the economically costliest error, is the greatest beneficiary of this gain in stability. © 2012 Elsevier B.V. All rights reserved.","Bankruptcy prediction; Decision support systems; Finance; Self-organizing map","Bankruptcy prediction; Comparative studies; Conventional methods; Design failures; Financial failure; Generalization Error; Kohonen map; Life span; Logistic regressions; Model stability; Prediction model; Real problems; Temporal stability; Time-periods; Type-I error; Artificial intelligence; Conformal mapping; Decision support systems; Discriminant analysis; Finance; Logistics; Mathematical models; Forecasting",Article,Scopus,2-s2.0-84861094050
"Saberian M.J., Vasconcelos N.","Learning optimal embedded cascades",2012,"IEEE Transactions on Pattern Analysis and Machine Intelligence",13,10.1109/TPAMI.2011.281,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865335489&doi=10.1109%2fTPAMI.2011.281&partnerID=40&md5=e20c0866af47c7dd92c0c98e6579df10","The problem of automatic and optimal design of embedded object detector cascades is considered. Two main challenges are identified: optimization of the cascade configuration and optimization of individual cascade stages, so as to achieve the best tradeoff between classification accuracy and speed, under a detection rate constraint. Two novel boosting algorithms are proposed to address these problems. The first, RCBoost, formulates boosting as a constrained optimization problem which is solved with a barrier penalty method. The constraint is the target detection rate, which is met at all iterations of the boosting process. This enables the design of embedded cascades of known configuration without extensive cross validation or heuristics. The second, ECBoost, searches over cascade configurations to achieve the optimal tradeoff between classification risk and speed. The two algorithms are combined into an overall boosting procedure, RCECBoost, which optimizes both the cascade configuration and its stages under a detection rate constraint, in a fully automated manner. Extensive experiments in face, car, pedestrian, and panda detection show that the resulting detectors achieve an accuracy versus speed tradeoff superior to those of previous methods. © 1979-2012 IEEE.","boosting; Computer vision; embedded detector cascades; real-time object detection","boosting; Boosting algorithm; Cascade configuration; Classification accuracy; Constrained optimization problems; Cross validation; Detection rates; Embedded object; Object Detection; Optimal design; Optimal tradeoffs; Penalty methods; Algorithms; Commerce; Computer vision; Constrained optimization; Object recognition; Detectors; algorithm; animal; article; artificial intelligence; automated pattern recognition; bear; car; face; histology; human; image processing; methodology; Algorithms; Animals; Artificial Intelligence; Automobiles; Face; Humans; Image Processing, Computer-Assisted; Pattern Recognition, Automated; Ursidae",Article,Scopus,2-s2.0-84865335489
"Heinrich M., Grüneberger F.J., Springer T., Gaedke M.","Reusable awareness widgets for collaborative web applications - A non-invasive approach",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-642-31753-8_1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865149215&doi=10.1007%2f978-3-642-31753-8_1&partnerID=40&md5=26e8a4854739d94c9dee3ee3563f6ad6","Creating awareness about other users' activities in a shared workspace is crucial to support efficient collaborative work. Even though the development of awareness widgets such as participant lists, telepointers or radar views is a costly and complex endeavor, awareness widget reuse is largely neglected. Collaborative applications either integrate specific awareness widgets or leverage existing awareness toolkits which require major source code adaptations and thus, are not suited to rapidly enrich existing web applications. Therefore, we propose a generic awareness infrastructure promoting an accelerated, cost-efficient development of awareness widgets as well as a non-invasive integration of awareness support into existing web applications. To validate our approach, we demonstrate the integration of three developed awareness widgets in four collaborative web editors. Furthermore, we expose insights about the development of reusable awareness widgets and discuss the limitations of the devised awareness infrastructure. © 2012 Springer-Verlag.",,"Collaborative application; Collaborative web applications; Collaborative Work; Cost-efficient; Shared-workspace; Source codes; Telepointers; WEB application; Artificial intelligence; World Wide Web",Conference Paper,Scopus,2-s2.0-84865149215
"Rooney N., Wang H., Browne F.","Applying kernel methods to argumentation mining",2012,"Proceedings of the 25th International Florida Artificial Intelligence Research Society Conference, FLAIRS-25",13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865044343&partnerID=40&md5=fc270392f357decf5f989dc989d09c12","The area of argumentation theory is an increasingly important area of artificial intelligence and mechanisms that are able to automatically detect the argument structure provide a novel area of research. This paper considers the use of kernel methods for argumentation detection and classification. It shows that a classification accuracy of 65%, can be attained using Natural Language Processing based kernel approaches, which do not require any heuristic choice of features. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Argument structures; Argumentation theory; Classification accuracy; Kernel approaches; Kernel methods; NAtural language processing; Computational linguistics; Natural language processing systems; Research; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84865044343
"Khan M., Tufail M., Md. Azamathulla H.","Gene-expression programming to predict pier scour depth using laboratory data",2012,"Journal of Hydroinformatics",13,10.2166/hydro.2011.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865045727&doi=10.2166%2fhydro.2011.008&partnerID=40&md5=f0ee5519b5054c854ac73e6f9897b3ad","Prediction of bridge pier scour depth is essential for safe and economical bridge design. Keeping in mind the complex nature of bridge scour phenomenon, there is a need to properly address the methods and techniques used to predict bridge pier scour. Up to the present, extensive research has been carried out for pier scour depth prediction. Different modeling techniques have been applied to achieve better prediction. This paper presents a new soft computing technique called geneexpression programming (GEP) for pier scour depth prediction using laboratory data. A functional relationship has been established using GEP and its performance is compared with other artificial intelligence (AI)-based techniques such as artificial neural networks (ANNs) and conventional regression-based techniques. Laboratory data containing 529 datasets was divided into calibration and validation sets. The performance of GEP was found to be highly satisfactory and encouraging when compared to regression equations but was slightly inferior to ANN. This slightly inferior performance of GEP compared to ANN is offset by its capability to provide compact and explicit mathematical expression for bridge scour. This advantage of GEP over ANN is the main motivation for this work. The resulting GEP models will add to the existing literature of AI-based inductive models for bridge scour modeling. © IWA Publishing 2012.","Artificial intelligence; Artificial neural networks; Data-driven models; Gene-expression programming; Pier scour; Regression models",,Article,Scopus,2-s2.0-84865045727
"Bormer T., Brockschmidt M., Distefano D., Ernst G., Filliâtre J.-C., Grigore R., Huisman M., Klebanov V., Marché C., Monahan R., Mostowski W., Polikarpova N., Scheben C., Schellhorn G., Tofan B., Tschannen J., Ulbrich M.","The COST IC0701 verification competition 2011",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-642-31762-0_2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864849260&doi=10.1007%2f978-3-642-31762-0_2&partnerID=40&md5=dde3d015f1274a30b0bd4c37356782a5","This paper reports on the experiences with the program verification competition held during the FoVeOOS conference in October 2011. There were 6 teams participating in this competition. We discuss the three different challenges that were posed and the solutions developed by the teams. We conclude with a discussion about the value of such competitions and lessons learned from them. © 2012 Springer-Verlag Berlin Heidelberg.",,"Program Verification; Artificial intelligence; Verification",Conference Paper,Scopus,2-s2.0-84864849260
"Gordon G., Ahissar E.","Hierarchical curiosity loops and active sensing",2012,"Neural Networks",13,10.1016/j.neunet.2012.02.024,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861775137&doi=10.1016%2fj.neunet.2012.02.024&partnerID=40&md5=80ad836f06c8e9e2f79728576664318a","A curious agent acts so as to optimize its learning about itself and its environment, without external supervision. We present a model of hierarchical curiosity loops for such an autonomous active learning agent, whereby each loop selects the optimal action that maximizes the agent's learning of sensory-motor correlations. The model is based on rewarding the learner's prediction errors in an actor-critic reinforcement learning (RL) paradigm. Hierarchy is achieved by utilizing previously learned motor-sensory mapping, which enables the learning of other mappings, thus increasing the extent and diversity of knowledge and skills. We demonstrate the relevance of this architecture to active sensing using the well-studied vibrissae (whiskers) system, where rodents acquire sensory information by virtue of repeated whisker movements. We show that hierarchical curiosity loops starting from optimally learning the internal models of whisker motion and then extending to object localization result in free-air whisking and object palpation, respectively. © 2012 Elsevier Ltd.","Active sensing; Internal models; Intrinsic reward; Object localization; Reinforcement learning; Touch; Vibrissa; Whisker","Active Sensing; Internal models; Intrinsic rewards; Object localization; Touch; Vibrissa; Whisker; Mammals; Object recognition; Optimization; Reinforcement learning; Autonomous agents; active sensing; article; artificial neural network; concept mapping; developmental stage; feedback system; information processing; learning algorithm; mathematical computing; mathematical model; prediction; priority journal; probability; process development; process optimization; reinforcement; reward; sensorimotor function; signal detection; vibrissa; Algorithms; Animals; Artificial Intelligence; Computer Simulation; Exploratory Behavior; Neural Networks (Computer); Physical Stimulation; Rats; Rats, Wistar; Reinforcement (Psychology); Space Perception; Touch; Vibrissae",Article,Scopus,2-s2.0-84861775137
"Luxen D., Schieferdecker D.","Candidate sets for alternative routes in road networks",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-642-30850-5_23,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864365278&doi=10.1007%2f978-3-642-30850-5_23&partnerID=40&md5=df696b321eadae975f4d482840c4bba2","We present a fast algorithm with preprocessing for computing multiple good alternative routes in road networks. Our approach is based on single via node routing on top of Contraction Hierarchies and achieves superior quality and efficiency compared to previous methods. The algorithm has neglectable memory overhead. © 2012 Springer-Verlag.",,"Alternative routes; Candidate sets; Fast algorithms; Memory overheads; Road network; Artificial intelligence; Algorithms",Conference Paper,Scopus,2-s2.0-84864365278
"Cabanes G., Bennani Y., Fresneau D.","Enriched topological learning for cluster detection and visualization",2012,"Neural Networks",13,10.1016/j.neunet.2012.02.019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861774156&doi=10.1016%2fj.neunet.2012.02.019&partnerID=40&md5=776dd1466f47c4346cf67d67d697fd8a","The exponential growth of data generates terabytes of very large databases. The growing number of data dimensions and data objects presents tremendous challenges for effective data analysis and data exploration methods and tools. Thus, it becomes crucial to have methods able to construct a condensed description of the properties and structure of data, as well as visualization tools capable of representing the data structure from these condensed descriptions. The purpose of our work described in this paper is to develop a method of describing data from enriched and segmented prototypes using a topological clustering algorithm. We then introduce a visualization tool that can enhance the structure within and between groups in data. We show, using some artificial and real databases, the relevance of the proposed approach. © 2012 Elsevier Ltd.","Coclustering; Prototype enrichment; Self-Organizing Map; Two-level clustering; Visualization","Cluster detection; Co-clustering; Data exploration; Data objects; Exponential growth; Number of datum; Real database; Topological clustering; Two-level clustering; Very large database; Visualization tools; Clustering algorithms; Conformal mapping; Data structures; Flow visualization; Topology; Visualization; Data visualization; accuracy; animal behavior; article; artificial neural network; classification algorithm; cluster analysis; controlled study; descriptive research; female; intermethod comparison; kernel method; learning algorithm; male; mathematical computing; nonhuman; priority journal; process development; social structure; stochastic model; Algorithms; Animals; Ants; Artificial Intelligence; Child, Preschool; Cluster Analysis; Databases, Factual; Humans; Plants",Article,Scopus,2-s2.0-84861774156
"Hélie S., Paul E.J., Ashby F.G.","Simulating the effects of dopamine imbalance on cognition: From positive affect to Parkinson's disease",2012,"Neural Networks",13,10.1016/j.neunet.2012.02.033,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861775110&doi=10.1016%2fj.neunet.2012.02.033&partnerID=40&md5=92a7d4aacf70f3c5d8150a980ef6b668","Cools (2006) suggested that prefrontal dopamine levels are related to cognitive stability whereas striatal dopamine levels are related to cognitive plasticity. With such a wide ranging role, almost all cognitive activities should be affected by dopamine levels in the brain. Not surprisingly, factors influencing brain dopamine levels have been shown to improve/worsen performance in many behavioral experiments. On the one hand, . Nadler, Rabi, and Minda (2010) showed that positive affect (which is thought to increase cortical dopamine levels) improves a type of categorization that depends on explicit reasoning (rule-based) but not another type that depends on procedural learning (information-integration). On the other hand, Parkinson's disease (which is known to decrease dopamine levels in both the striatum and cortex) produces proactive interference in the odd-man-out task (. Flowers & Robertson, 1985) and renders subjects insensitive to negative feedback during reversal learning (. Cools, Altamirano, & D'Esposito, 2006). This article uses the COVIS model of categorization to simulate the effects of different dopamine levels in categorization, reversal learning, and the odd-man-out task. The results show a good match between the simulated and human data, which suggests that the role of dopamine in COVIS can account for several cognitive enhancements and deficits related to dopamine levels in healthy and patient populations. © 2012 Elsevier Ltd.","Computational modeling; COVIS; Dopamine; Parkinson's disease; Positive affect","Computational modeling; COVIS; Dopamine; Parkinson's disease; Positive affects; Interference suppression; Neurodegenerative diseases; Population statistics; Neurophysiology; dopamine; accuracy; affect; algorithm; article; brain depth stimulation; dopamine blood level; dopamine release; dopamine uptake; dopaminergic transmission; human; learning style; learning theory; linear system; mathematical computing; mathematical model; mental performance; negative feedback; Parkinson disease; positive feedback; prediction; prefrontal cortex; priority journal; sensitivity and specificity; sensory cortex; simulation; synaptic transmission; Adult; Affect; Aged; Aging; Algorithms; Artificial Intelligence; Cognition; Computer Simulation; Dopamine; Feedback, Physiological; Humans; Learning; Neural Pathways; Parkinson Disease; Reward; Somatosensory Cortex; Young Adult",Article,Scopus,2-s2.0-84861775110
"Zhang X., Li Y., Zhu L.","Color code identification in coded structured light",2012,"Applied Optics",13,10.1364/AO.51.005340,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864541145&doi=10.1364%2fAO.51.005340&partnerID=40&md5=b9a48d3af569001bc84f2c08f2fa8d98","Color code is widely employed in coded structured light to reconstruct the three-dimensional shape of objects. Before determining the correspondence, a very important step is to identify the color code. Until now, the lack of an effective evaluation standard has hindered the progress in this unsupervised classification. In this paper, we propose a framework based on the benchmark to explore the new frontier. Two basic facets of the color code identification are discussed, including color feature selection and clustering algorithm design. First, we adopt analysis methods to evaluate the performance of different color features, and the order of these color features in the discriminating power is concluded after a large number of experiments. Second, in order to overcome the drawback of K-means, a decision-directed method is introduced to find the initial centroids. Quantitative comparisons affirm that our method is robust with high accuracy, and it can find or closely approach the global peak. © 2012 Optical Society of America.",,"Clustering algorithms; Analysis method; Coded structured light; Color features; Decision-directed; Discriminating power; Evaluation standard; K-means; Quantitative comparison; Three-dimensional shape; Unsupervised classification; Color codes; algorithm; article; artificial intelligence; automated pattern recognition; color; computer simulation; face; female; hand; human; image processing; information retrieval; methodology; statistical model; three dimensional imaging; Algorithms; Artificial Intelligence; Color; Computer Simulation; Face; Female; Hand; Humans; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Information Storage and Retrieval; Models, Statistical; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84864541145
"García-Rodríguez J., Angelopoulou A., García-Chamizo J.M., Psarrou A., Orts Escolano S., Morell Giménez V.","Autonomous Growing Neural Gas for applications with time constraint: Optimal parameter estimation",2012,"Neural Networks",13,10.1016/j.neunet.2012.02.032,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861761321&doi=10.1016%2fj.neunet.2012.02.032&partnerID=40&md5=cb9549183a812573949913a7961fd13f","This paper aims to address the ability of self-organizing neural network models to manage real-time applications. Specifically, we introduce fAGNG (fast Autonomous Growing Neural Gas), a modified learning algorithm for the incremental model Growing Neural Gas (GNG) network. The Growing Neural Gas network with its attributes of growth, flexibility, rapid adaptation, and excellent quality of representation of the input space makes it a suitable model for real time applications. However, under time constraints GNG fails to produce the optimal topological map for any input data set. In contrast to existing algorithms, the proposed fAGNG algorithm introduces multiple neurons per iteration. The number of neurons inserted and input data generated is controlled autonomous and dynamically based on a priory or online learnt model. A detailed study of the topological preservation and quality of representation depending on the neural network parameter selection has been developed to find the best alternatives to represent different linear and non-linear input spaces under time restrictions or specific quality of representation requirements. © 2012 Elsevier Ltd.","Delaunay triangulation; Growing Neural Gas; Self-organizing models; Temporal constraint; Topology preservation","Delaunay triangulation; Growing neural gas; Self-organizing model; Temporal constraints; Topology preservation; Gases; Input output programs; Learning algorithms; Optimization; Parameter estimation; Topology; Neural networks; accuracy; analytical error; article; artificial neural network; classification algorithm; controlled study; growing neural gas; intermethod comparison; learning algorithm; mathematical computing; mathematical model; nonlinear system; priority journal; process development; process optimization; quality control; Algorithms; Artificial Intelligence; Computer Systems; Databases, Factual; Gestures; Humans; Image Processing, Computer-Assisted; Linear Models; Models, Neurological; Neural Networks (Computer); Neurons; Nonlinear Dynamics; Regression Analysis; Software",Article,Scopus,2-s2.0-84861761321
"Chan T.-H.H., Li M., Shi E., Xu W.","Differentially private continual monitoring of heavy hitters from distributed streams",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-642-31680-7_8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864265207&doi=10.1007%2f978-3-642-31680-7_8&partnerID=40&md5=f899651cc330f2cd3c816577fd22af08","We consider applications scenarios where an untrusted aggregator wishes to continually monitor the heavy-hitters across a set of distributed streams. Since each stream can contain sensitive data, such as the purchase history of customers, we wish to guarantee the privacy of each stream, while allowing the untrusted aggregator to accurately detect the heavy hitters and their approximate frequencies. Our protocols are scalable in settings where the volume of streaming data is large, since we guarantee low memory usage and processing overhead by each data source, and low communication overhead between the data sources and the aggregator. © Springer-Verlag Berlin Heidelberg 2012.",,"Communication overheads; Data source; Heavy-hitter; Low memory; Processing overhead; Sensitive datas; Streaming data; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84864265207
"Coronato A., De Pietro G.","Tools for the rapid prototyping of provably correct ambient intelligence applications",2012,"IEEE Transactions on Software Engineering",13,10.1109/TSE.2011.67,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864610604&doi=10.1109%2fTSE.2011.67&partnerID=40&md5=0e8971b7e8b69ed66cdb819fa054dafb","Ambient Intelligence technologies have not yet been widely adopted in safety critical scenarios. This principally has been due to fact that acceptable degrees of dependability have not been reached for the applications that rely on such technologies. However, the new critical application domains, like Ambient Assisted Living and Smart Hospitals, which are currently emerging, are increasing the need for methodologies and tools that can improve the reliability of the final systems. This paper presents a middleware architecture for safety critical Ambient Intelligence applications which provides the developer with services for runtime verification. It is now possible to continuously monitor and check the running system against correctness properties defined at design time. Moreover, a visual tool which allows the formal design of several of the characteristics of an Ambient Intelligence application and the automatic generation of setting up parameters and code for the middleware infrastructure is also presented. © 2012 IEEE.","designing tools; middleware infrastructures; Safety critical ambient intelligence systems","Ambient assisted living; Ambient intelligence; Ambient intelligence systems; Automatic Generation; Correctness properties; Critical applications; Design time; Designing tools; Formal design; Middleware architecture; Middleware infrastructure; Run-time verification; Running systems; Safety-critical; Smart hospital; Visual tools; Automatic programming; Middleware; Rapid prototyping; Artificial intelligence",Article,Scopus,2-s2.0-84864610604
"McShane M., Beale S., Nirenburg S., Jarrell B., Fantry G.","Inconsistency as a diagnostic tool in a society of intelligent agents",2012,"Artificial Intelligence in Medicine",13,10.1016/j.artmed.2012.04.005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863780883&doi=10.1016%2fj.artmed.2012.04.005&partnerID=40&md5=7453edf8cbeae83b846f7ed77522b852","Objective: To use the detection of clinically relevant inconsistencies to support the reasoning capabilities of intelligent agents acting as physicians and tutors in the realm of clinical medicine. Methods: We are developing a cognitive architecture, OntoAgent, that supports the creation and deployment of intelligent agents capable of simulating human-like abilities. The agents, which have a simulated mind and, if applicable, a simulated body, are intended to operate as members of multi-agent teams featuring both artificial and human agents. The agent architecture and its underlying knowledge resources and processors are being developed in a sufficiently generic way to support a variety of applications. Results: We show how several types of inconsistency can be detected and leveraged by intelligent agents in the setting of clinical medicine. The types of inconsistencies discussed include: test results not supporting the doctor's hypothesis; the results of a treatment trial not supporting a clinical diagnosis; and information reported by the patient not being consistent with observations. We show the opportunities afforded by detecting each inconsistency, such as rethinking a hypothesis, reevaluating evidence, and motivating or teaching a patient. Conclusions: Inconsistency is not always the absence of the goal of consistency; rather, it can be a valuable trigger for further exploration in the realm of clinical medicine. The OntoAgent cognitive architecture, along with its extensive suite of knowledge resources an processors, is sufficient to support sophisticated agent functioning such as detecting clinically relevant inconsistencies and using them to benefit patient-centered medical training and practice. © 2012 Elsevier B.V.","Clinical decision making support; Cognitive modeling; Intelligent agents; Virtual tutor","Agent architectures; Clinical decision making; Clinical diagnosis; Clinical medicine; Cognitive architectures; Cognitive modeling; Diagnostic tools; Human agent; Knowledge resource; Medical training; Multiagent teams; Reasoning capabilities; Virtual tutors; Cognitive systems; Computer architecture; Diagnosis; Distance education; Patient treatment; Intelligent agents; article; artificial intelligence; clinical medicine; computer program; diagnostic reasoning; human; inconsistency; information storage; learning; physician; priority journal; simulation; Artificial Intelligence; Decision Support Systems, Clinical; Humans; Reproducibility of Results",Article,Scopus,2-s2.0-84863780883
"Hammer P.L., Kogan A., Lejeune M.A.","A logical analysis of banks' financial strength ratings",2012,"Expert Systems with Applications",13,10.1016/j.eswa.2012.01.087,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858334434&doi=10.1016%2fj.eswa.2012.01.087&partnerID=40&md5=5a3cce462a6e21a66a6f873614dcd65c","We evaluate the creditworthiness of banks using statistical, as well as combinatorics-, optimization-, and logic-based methodologies. We reverse-engineer the Fitch risk ratings of banks using ordered logistic regression, support vector machine, and Logical Analysis of Data (LAD). The LAD ratings are shown to be the most accurate and most successfully cross-validated. The study shows that the LAD rating approach is (i) objective, (ii) transparent, and (iii) generalizable. It can be used to build internal rating systems that (iv) have varying levels of granularity, and (v) are Basel compliant, allowing for their use in the decisions pertaining to the determination of the amount of regulatory capital. © 2012 Elsevier Ltd. All rights reserved.","Credit risk rating; Data mining; Decision support systems; Logical Analysis of Data","Combinatorics; Credit risk rating; Financial strength; Logic-based methodology; Logical analysis; Logical Analysis of Data; Logistic regressions; Rating system; Risk ratings; Artificial intelligence; Data mining; Decision support systems; Logistics; Risk assessment; Rating",Article,Scopus,2-s2.0-84858334434
"Csaba G., Somlyai L., Vámossy Z.","Differences between Kinect and structured lighting sensor in robot navigation",2012,"IEEE 10th Jubilee International Symposium on Applied Machine Intelligence and Informatics, SAMI 2012 - Proceedings",13,10.1109/SAMI.2012.6208934,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862728250&doi=10.1109%2fSAMI.2012.6208934&partnerID=40&md5=85e2c2ae6663fb0e487ebf9e5de96b56","Nowadays, autonomous navigation is getting more and more attention. The mobile robots need to have map of the close environment. The paper presents how can collect information about the environment for robot navigation. The sensors and the navigation system are installed on a mobile equipment platform. After this the article shows the different measurement systems, the generating virtual depth map and the navigation algorithm. At the end the Kinect sensor is compared with a sensor based on structured light. © 2012 IEEE.",,"Autonomous navigation; Depth Map; Measurement system; Mobile equipments; Navigation algorithms; Robot navigation; Structured Light; Structured lighting; Artificial intelligence; Information science; Sensors; Navigation systems",Conference Paper,Scopus,2-s2.0-84862728250
"Namasivayam V., Bajorath J.","Searching for coordinated activity cliffs using particle swarm optimization",2012,"Journal of Chemical Information and Modeling",13,10.1021/ci3000503,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862021088&doi=10.1021%2fci3000503&partnerID=40&md5=b8e297dd16769e2238cebfd2a3ee8447","Activity cliffs are formed by structurally similar compounds having large potency differences. Coordinated activity cliffs evolve when compounds within groups of structural neighbors form multiple cliffs with different partners, giving rise to local networks of cliffs in a data set. Using particle swarm optimization, a machine learning approach, we systematically searched for coordinated activity cliffs in different compound sets. Regardless of the global SAR characteristics of these data sets, coordinated activity cliffs introducing strong local SAR discontinuity were identified in most cases. Compound subsets forming coordinated activity cliffs represent centers of SAR discontinuity and have high SAR information content. Through particle swarm optimization guided by subset discontinuity scoring, compounds forming the largest coordinated activity cliffs can automatically be extracted from large compound data © 2012 American Chemical Society.",,"Coordinated activity; Data sets; Information contents; Learning approach; Local networks; Particle swarm optimization (PSO); Landforms; dipeptidyl peptidase IV; dipeptidyl peptidase IV inhibitor; DPP4 protein, human; dipeptidyl peptidase IV; dipeptidyl peptidase IV inhibitor; molecular library; algorithm; article; artificial intelligence; chemical database; chemistry; cluster analysis; drug development; human; methodology; molecular library; structure activity relation; chemistry; molecular library; Algorithms; Artificial Intelligence; Cluster Analysis; Databases, Chemical; Dipeptidyl Peptidase 4; Dipeptidyl-Peptidase IV Inhibitors; Drug Discovery; Humans; Research Design; Small Molecule Libraries; Structure-Activity Relationship; Algorithms; Artificial Intelligence; Cluster Analysis; Databases, Chemical; Dipeptidyl Peptidase 4; Dipeptidyl-Peptidase IV Inhibitors; Drug Discovery; Humans; Research Design; Small Molecule Libraries; Structure-Activity Relationship",Article,Scopus,2-s2.0-84862021088
"Islam A., Milios E., Kešelj V.","Text similarity using Google tri-grams",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-642-30353-1_29,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861736087&doi=10.1007%2f978-3-642-30353-1_29&partnerID=40&md5=cd1457e71f946f9d5d5d64ab2d02c158","The purpose of this paper is to propose an unsupervised approach for measuring the similarity of texts that can compete with supervised approaches. Finding the inherent properties of similarity between texts using a corpus in the form of a word n-gram data set is competitive with other text similarity techniques in terms of performance and practicality. Experimental results on a standard data set show that the proposed unsupervised method outperforms the state-of-the-art supervised method and the improvement achieved is statistically significant at 0.05 level. The approach is language-independent; it can be applied to other languages as long as n-grams are available. © 2012 Springer-Verlag.","Google n-gram; Text Similarity; Tri-gram; Unsupervised; Word Similarity","Google n-gram; Text similarity; Tri grams; Unsupervised; Word similarity; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84861736087
"Mohaghegh S.D., Liu J., Gaskari R., Maysami M., Olukoko O.","Application of well-based Surrogate Reservoir Models (SRMs) to two offshore fields in Saudi Arabia, case study",2012,"Society of Petroleum Engineers Western Regional Meeting 2012",13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861645039&partnerID=40&md5=c80e806ca356007a455b83fcc62c5589","Well-based Surrogate Reservoir Model (SRM) may be classified as a new technology for building proxy models that represent large, complex numerical reservoir simulation models. The well-based SRM has several advantages over traditional proxy models, such as response surfaces or reduced models. These advantages include (1) to develop an SRM one does not need to approximate the existing simulation model, (2) the number of simulation runs required for the development of an SRM is at least an order of magnitude less than traditional proxy models, and (3) above and beyond representing the pressure and production profiles at each well individually, SRM can replicate, with high accuracy, the pressure and saturation changes at each grid block. Well-based SRM is based on the pattern recognition capabilities of artificial intelligence and data mining (AI&DM) that is also referred to as predictive analytics. During the development process the SRM is trained to leam the principles of fluid flow through porous media as applied to the complexities of the reservoir being modeled. The numerical reservoir simulation model is used for two purposes: (1) to teach the SRM the physics of fluid flow through porous media as applied to the specific reservoir that is being modeled, and (2) to teach the SRM the complexities of the heterogeneous reservoir represented by the geological model and its impact on the fluid production and pressure changes in the reservoir. Application of well-based SRM to two offshore fields in Saudi Arabia is demonstrated. The simulation model of these fields includes millions of grid blocks and tens of producing and injection wells. There are four producing layers in these assets that are contributing to production. In this paper we provide the details that is involved in development of the SRM and show the result of matching the production from the all the wells. We also present the validation of the SRM through matching the results of blind simulation runs. The steps in the development of the SRM includes design of the required simulation runs (usually less than 20 simulation runs are sufficient), identifying the key performance indicators that control the pressure and production in the model, identification of input parameters for the SRM, training and calibration of the SRM and finally validation of the SRM using blind simulation runs. Copyright 2012, Society of Petroleum Engineers.",,"Development process; Flowthrough; Fluid production; Geological models; Grid blocks; Heterogeneous reservoirs; Injection wells; Input parameter; Key performance indicators; Offshore fields; Predictive analytics; Pressure change; Production profiles; Proxy model; Reduced model; Reservoir models; Reservoir simulation model; Response surface; Saudi Arabia; Simulation model; Artificial intelligence; Benchmarking; Flow of fluids; Geologic models; Pattern recognition; Petroleum reservoir evaluation; Petroleum reservoirs; Well stimulation; Wells; Computer simulation",Conference Paper,Scopus,2-s2.0-84861645039
"Codish M., Giesl J., Schneider-Kamp P., Thiemann R.","SAT solving for termination proofs with recursive path orders and dependency pairs",2012,"Journal of Automated Reasoning",13,10.1007/s10817-010-9211-0,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860885514&doi=10.1007%2fs10817-010-9211-0&partnerID=40&md5=4ed6cd18d5e3528ebd71b9f688fd036a","This paper introduces a propositional encoding for recursive path orders (RPO), in connection with dependency pairs. Hence, we capture in a uniform setting all common instances of RPO, i.e.; lexicographic path orders (LPO), multiset path orders (MPO), and lexicographic path orders with status (LPOS). This facilitates the application of SAT solvers for termination analysis of term rewrite systems (TRSs). We address four main inter-related issues and show how to encode them as satisfiability problems of propositional formulas that can be efficiently handled by SAT solving: (A) the lexicographic comparison w.r.t. a permutation of the arguments; (B) the multiset extension of a base order; (C) the combined search for a path order together with an argument filter to orient a set of inequalities; and (D) how the choice of the argument filter influences the set of inequalities that have to be oriented (so-called usable rules). We have implemented our contributions in the termination prover AProVE. Extensive experiments show that by our encoding and the application of SAT solvers one obtains speedups in orders of magnitude as well as increased termination proving power. © 2010 Springer Science+Business Media B.V.","Dependency pairs; Recursive path order; SAT solving; Term rewriting; Termination","Dependency pairs; Path orders; SAT-solving; Term rewriting; Termination; Artificial intelligence; Automata theory; Software engineering; Encoding (symbols)",Article,Scopus,2-s2.0-84860885514
"Ong M.-S., Magrabi F., Coiera E.","Automated identification of extreme-risk events in clinical incident reports",2012,"Journal of the American Medical Informatics Association",13,10.1136/amiajnl-2011-000562,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863550886&doi=10.1136%2famiajnl-2011-000562&partnerID=40&md5=21e96d2c12593ad08e4fc03705216da9","Objectives: To explore the feasibility of using statistical text classification to automatically detect extreme-risk events in clinical incident reports. Methods: Statistical text classifiers based on Naïve Bayes and Support Vector Machine (SVM) algorithms were trained and tested on clinical incident reports to automatically detect extreme-risk events, defined by incidents that satisfy the criteria of Severity Assessment Code (SAC) level 1. For this purpose, incident reports submitted to the Advanced Incident Management System by public hospitals from one Australian region were used. The classifiers were evaluated on two datasets: (1) a set of reports with diverse incident types (n1/4 120); (2) a set of reports associated with patient misidentification (n1/4 166). Results were assessed using accuracy, precision, recall, F-measure, and area under the curve (AUC) of receiver operating characteristic curves. Results: The classifiers performed well on both datasets. In the multi-type dataset, SVM with a linear kernel performed best, identifying 85.8% of SAC level 1 incidents (precision1/4 0.88, recall1/4 0.83, F-measure1/4 0.86, AUC=0.92). In the patient misidentification dataset, 96.4% of SAC level 1 incidents were detected when SVM with linear, polynomial or radial-basis function kernel was used (precision1/4 0.99, recall1/4 0.94, F-measure1/4 0.96, AUC1/4 0.98). Naïve Bayes showed reasonable performance, detecting 80.8% of SAC level 1 incidents in the multi-type dataset and 89.8% of SAC level 1 patient misidentification incidents. Overall, higher prediction accuracy was attained on the specialized dataset, compared with the multi-type dataset. Conclusion: Text classification techniques can be applied effectively to automate the detection of extremerisk events in clinical incident reports.",,"article; automation; Bayesian learning; data base; incident report; information retrieval; medical informatics; medical information system; patient safety; statistical analysis; support vector machine; Artificial Intelligence; Bayes Theorem; Humans; Medical Errors; Risk Management; Support Vector Machines",Article,Scopus,2-s2.0-84863550886
"Rivero J., Cuadra D., Calle J., Isasi P.","Using the ACO algorithm for path searches in social networks",2012,"Applied Intelligence",13,10.1007/s10489-011-0304-1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862143177&doi=10.1007%2fs10489-011-0304-1&partnerID=40&md5=c116c87d6aee97fd87eb85adeff28cab","One of the most important types of applications currently being used to share knowledge across the Internet are social networks. In addition to their use in social, professional and organizational spheres, social networks are also frequently utilized by researchers in the social sciences, particularly in anthropology and social psychology. In order to obtain information related to a particular social network, analytical techniques are employed to represent the network as a graph, where each node is a distinct member of the network and each edge is a particular type of relationship between members including, for example, kinship or friendship. This article presents a proposal for the efficient solution to one of the most frequently requested services on social networks; namely, taking different types of relationships into account in order to locate a particular member of the network. The solution is based on a biologically-inspired modification of the ant colony optimization algorithm. © 2011 Springer Science+Business Media, LLC.","ACO; Dijkstra; Large graphs; Path search; Social networks","ACO; Dijkstra; Large graphs; Path search; Social Networks; Algorithms; Artificial intelligence; Social networking (online)",Article,Scopus,2-s2.0-84862143177
"Petrović J., Ibrić S., Betz G., Urić Z.","Optimization of matrix tablets controlled drug release using Elman dynamic neural networks and decision trees",2012,"International Journal of Pharmaceutics",13,10.1016/j.ijpharm.2012.02.031,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859428478&doi=10.1016%2fj.ijpharm.2012.02.031&partnerID=40&md5=e08fe1601a95ff7af441c3bbc5b456a8","The main objective of the study was to develop artificial intelligence methods for optimization of drug release from matrix tablets regardless of the matrix type. Static and dynamic artificial neural networks of the same topology were developed to model dissolution profiles of different matrix tablets types (hydrophilic/lipid) using formulation composition, compression force used for tableting and tablets porosity and tensile strength as input data. Potential application of decision trees in discovering knowledge from experimental data was also investigated. Polyethylene oxide polymer and glyceryl palmitostearate were used as matrix forming materials for hydrophilic and lipid matrix tablets, respectively whereas selected model drugs were diclofenac sodium and caffeine. Matrix tablets were prepared by direct compression method and tested for in vitro dissolution profiles. Optimization of static and dynamic neural networks used for modeling of drug release was performed using Monte Carlo simulations or genetic algorithms optimizer. Decision trees were constructed following discretization of data. Calculated difference (f 1) and similarity (f 2) factors for predicted and experimentally obtained dissolution profiles of test matrix tablets formulations indicate that Elman dynamic neural networks as well as decision trees are capable of accurate predictions of both hydrophilic and lipid matrix tablets dissolution profiles. Elman neural networks were compared to most frequently used static network, Multi-layered perceptron, and superiority of Elman networks have been demonstrated. Developed methods allow simple, yet very precise way of drug release predictions for both hydrophilic and lipid matrix tablets having controlled drug release. © 2012 Elsevier B.V. All rights reserved.","Controlled release; Decision trees; Drug release modeling; Matrix tablets; Neural networks","caffeine; diclofenac; glycerol palmitostearate; macrogol; article; artificial neural network; compression; computer prediction; computer program; controlled drug release; decision tree; drug formulation; drug solubility; elman dynamic neural network; genetic algorithm; hydrophilicity; lipid composition; matrix tablet; Monte Carlo method; multilayered perceptron static neural network; priority journal; tablet porosity; tensile strength; Artificial Intelligence; Caffeine; Chemistry, Pharmaceutical; Decision Trees; Delayed-Action Preparations; Diclofenac; Diglycerides; Drug Compounding; Hydrophobic and Hydrophilic Interactions; Neural Networks (Computer); Polyethylene Glycols; Porosity; Solubility; Tablets; Tensile Strength",Article,Scopus,2-s2.0-84859428478
"Bergman D., Cire A.A., Van Hoeve W.-J., Hooker J.N.","Variable ordering for the application of BDDs to the maximum independent set problem",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-642-29828-8_3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861451594&doi=10.1007%2f978-3-642-29828-8_3&partnerID=40&md5=77d5a5819e67cc3ea522c4f0399a205c","The ordering of variables can have a significant effect on the size of the reduced binary decision diagram (BDD) that represents the set of solutions to a combinatorial optimization problem. It also influences the quality of the objective function bound provided by a limited-width relaxation of the BDD. We investigate these effects for the maximum independent set problem. By identifying variable orderings for the BDD, we show that the width of an exact BDD can be given a theoretical upper bound for certain classes of graphs. In addition, we draw an interesting connection between the Fibonacci numbers and the width of exact BDDs for general graphs. We propose variable ordering heuristics inspired by these results, as well as a k-layer look-ahead heuristic applicable to any problem domain. We find experimentally that orderings that result in smaller exact BDDs have a strong tendency to produce tighter bounds in relaxation BDDs. © 2012 Springer-Verlag.",,"Combinatorial optimization problems; Fibonacci numbers; General graph; Look-ahead; Maximum independent sets; Objective functions; Problem domain; Upper Bound; Variable ordering; Variable ordering heuristics; Artificial intelligence; Binary decision diagrams; Combinatorial optimization; Computer programming; Constraint theory; Number theory; Set theory",Conference Paper,Scopus,2-s2.0-84861451594
"Schutt A., Chu G., Stuckey P.J., Wallace M.G.","Maximising the net present value for resource-constrained project scheduling",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-642-29828-8_24,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861450033&doi=10.1007%2f978-3-642-29828-8_24&partnerID=40&md5=6792cb2191e0fddcf0ff869d06eed9b0","The Resource-constrained Project Scheduling Problem (Rcpsp), in which a schedule must obey the resource constraints and the precedence constraints between pairs of activities, is one of the most studied scheduling problems. An important variation of the problem (RcpspDc) is to find a schedule which maximises the net present value (discounted cash flow), when every activity has a given cash flow associated with it. Given the success of lazy clause generation (Lcg) approaches to solve Rcpsp with and without generalised precedence relations it seems worthwhile investigating Lcg's use on Rcpspdc. To do so, we must construct propagators for the net-present-value constraint that explain their propagation to the Lcg solver. In this paper we construct three different propagators for net-present-value constraints, and show how they can be used to rapidly solve RcpspDc. © 2012 Springer-Verlag.",,"Cash flow; Discounted cash flow; Net present value; Precedence constraints; Precedence relations; Resource constrained project scheduling; Resource Constraint; Resource-constrained project scheduling problem; Scheduling problem; Artificial intelligence; Combinatorial optimization; Computer programming; Constraint theory; Scheduling",Conference Paper,Scopus,2-s2.0-84861450033
"Prakken H.","Some reflections on two current trends in formal argumentation",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-642-29414-3_14,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861038935&doi=10.1007%2f978-3-642-29414-3_14&partnerID=40&md5=1b58d1f55488324c23829407ae53803c","This paper discusses two recent developments in the formal study of argumentation-based inference: work on preference-based abstract argumentation and on classical (deductive) argumentation. It is first argued that general models of the use of preferences in argumentation cannot leave the structure of arguments and the nature of attack and defeat unspecified. Then it is claimed that classical argumentation cannot model some common forms of defeasible reasoning in a natural way. In both cases it will be argued that the recently proposed ASPIC + framework for structured argumentation does not suffer from these limitations. In the final part of the paper the work of Marek Sergot on argumentation-based inference will be discussed in light of the preceding discussion. © 2012 Springer-Verlag Berlin Heidelberg.",,"Abstract argumentation; Current trends; Defeasible reasoning; Formal argumentation; General model; Artificial intelligence; Logic programming",Article,Scopus,2-s2.0-84861038935
"Ye Z., Hu Z., Lai X., Chen H.","Image segmentation using thresholding and swarm intelligence",2012,"Journal of Software",13,10.4304/jsw.7.5.1074-1082,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860385430&doi=10.4304%2fjsw.7.5.1074-1082&partnerID=40&md5=5bc33202b6444922610eaab117f7b58c","Image segmentation is a significant technology for image process. Many segmentation methods have been brought forward to deal with image segmentation, among these methods thresholding is the simple and important one. To overcome shortcoming without using space information many thresholding methods based on 2-D histogram are often used in practical work. These methods segment images by using the gray value of the pixel and the local average gray value of it, and thus provide better results than the methods based on 1-D histogram. However, its time-consuming computation is often an obstacle in real time application systems. In this paper, fast image segmentation methods based on swarm intelligence and 2-D Fisher criteria thresholding are presented. The proposed approaches have been implemented and tested on several real images. Experiments results indicate that the proposed methods provides improved search performance which are efficient methods to help select optimum 2D thresholds with much less computation cost and suitable for real time applications. © 2012 Academy Publisher.","2-D Fisher criteria; Swarm intelligence; Terms image segmentation; Thresholding","2-D Fisher criteria; 2-D histograms; Computation costs; Gray value; Image process; Local average; Real images; Real-time application; Search performance; Segmentation methods; Swarm Intelligence; Thresholding; Thresholding methods; Artificial intelligence; Graphic methods; Image segmentation",Article,Scopus,2-s2.0-84860385430
"Zhang L., Chen C., Bu J., He X.","A unified feature and instance selection framework using optimum experimental design",2012,"IEEE Transactions on Image Processing",13,10.1109/TIP.2012.2183879,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860150282&doi=10.1109%2fTIP.2012.2183879&partnerID=40&md5=465ec7a67d67385803ed34db8a70338c","The goal of feature selection is to identify the most informative features for compact representation, whereas the goal of active learning is to select the most informative instances for prediction. Previous studies separately address these two problems, despite of the fact that selecting features and instances are dual operations over a data matrix. In this paper, we consider the novel problem of simultaneously selecting the most informative features and instances and develop a solution from the perspective of optimum experimental design. That is, by using the selected features as the new representation and the selected instances as training data, the variance of the parameter estimate of a learning function can be minimized. Specifically, we propose a novel approach, which is called Unified criterion for Feature and Instance selection (UFI), to simultaneously identify the most informative features and instances that minimize the trace of the parameter covariance matrix. A greedy algorithm is introduced to efficiently solve the optimization problem. Experimental results on two benchmark data sets demonstrate the effectiveness of our proposed method. © 1992-2012 IEEE.","Active learning; experimental design; feature selection; instance selection","Active Learning; Benchmark data; Compact representation; Data matrices; Greedy algorithms; Instance selection; Learning functions; Optimization problems; Optimum experimental design; Parameter covariance; Parameter estimate; Training data; Covariance matrix; Design of experiments; Parameter estimation; Problem solving; Statistics; Feature extraction; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; image enhancement; information retrieval; methodology; reproducibility; sensitivity and specificity; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Information Storage and Retrieval; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84860150282
"Hannagan T., Grainger J.","Protein Analysis Meets Visual Word Recognition: A Case for String Kernels in the Brain",2012,"Cognitive Science",13,10.1111/j.1551-6709.2012.01236.x,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860580513&doi=10.1111%2fj.1551-6709.2012.01236.x&partnerID=40&md5=6b5c5e9556ff1a19a59f7ddc3c6483ff","It has been recently argued that some machine learning techniques known as Kernel methods could be relevant for capturing cognitive and neural mechanisms (Jäkel, Schölkopf, & Wichmann, 2009). We point out that ''String kernels,'' initially designed for protein function prediction and spam detection, are virtually identical to one contending proposal for how the brain encodes orthographic information during reading. We suggest some reasons for this connection and we derive new ideas for visual word recognition that are successfully put to the test. We argue that the versatility and performance of String kernels makes a compelling case for their implementation in the brain. © 2012 Cognitive Science Society, Inc.","Open-bigrams; Orthographic coding; String kernels; Visual Word Form Area; Visual word recognition","article; artificial intelligence; automated pattern recognition; computer program; human; methodology; pattern recognition; reading; sequence analysis; Artificial Intelligence; Humans; Pattern Recognition, Automated; Pattern Recognition, Visual; Reading; Sequence Analysis, Protein; Software",Article,Scopus,2-s2.0-84860580513
"Sugimoto N., Morimoto J., Hyon S.-H., Kawato M.","The eMOSAIC model for humanoid robot control",2012,"Neural Networks",13,10.1016/j.neunet.2012.01.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860327685&doi=10.1016%2fj.neunet.2012.01.002&partnerID=40&md5=b3efa6169fcba2a66050ebfac3017296","In this study, we propose an extension of the MOSAIC architecture to control real humanoid robots. MOSAIC was originally proposed by neuroscientists to understand the human ability of adaptive control. The modular architecture of the MOSAIC model can be useful for solving nonlinear and non-stationary control problems. Both humans and humanoid robots have nonlinear body dynamics and many degrees of freedom. Since they can interact with environments (e.g., carrying objects), control strategies need to deal with non-stationary dynamics. Therefore, MOSAIC has strong potential as a human motor-control model and a control framework for humanoid robots. Yet application of the MOSAIC model has been limited to simple simulated dynamics since it is susceptive to observation noise and also cannot be applied to partially observable systems. Our approach introduces state estimators into MOSAIC architecture to cope with real environments. By using an extended MOSAIC model, we are able to successfully generate squatting and object-carrying behaviors on a real humanoid robot. © 2012 Elsevier Ltd.","Computational neuroscience; Humanoid robot; Modular architecture; Nonlinear and non-stationary control problem","Adaptive Control; Body dynamics; Carrying objects; Computational neuroscience; Control framework; Control problems; Control strategies; Human abilities; Humanoid robot; Modular architectures; Motor control; Non-stationary dynamics; Nonstationary; Observation noise; Partially observable systems; Real environments; State Estimators; Computer simulation; Anthropomorphic robots; accuracy; article; artificial neural network; computer simulation; experimental design; learning algorithm; linear system; mathematical computing; mathematical model; nonlinear system; prediction; priority journal; probability; process model; robotics; signal detection; stimulus response; strategic planning; systematic error; task performance; Artificial Intelligence; Humans; Neural Networks (Computer); Psychomotor Performance; Robotics",Article,Scopus,2-s2.0-84860327685
"Liu D., Guo S., Chen X., Shao Q., Ran Q., Song X., Wang Z.","A macro-evolutionary multi-objective immune algorithm with application to optimal allocation of water resources in Dongjiang River basins, South China",2012,"Stochastic Environmental Research and Risk Assessment",13,10.1007/s00477-011-0505-5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859441950&doi=10.1007%2fs00477-011-0505-5&partnerID=40&md5=dff53ffa7b7f836b0632498c36d84166","Macro-evolution is a new kind of high-level species evolution inspired by the dynamics of species extinction and diversification at large time scales. Immune algorithms are a set of computational systems inspired by the defense process of the biological immune system. By taking advantage of the macro-evolutionary algorithm and immune learning of artificial immune systems, this article proposes a macro-evolutionary multi-objective immune algorithm (MEMOIA) for optimizing multi-objective allocation of water resources in river basins. A benchmark test problem, namely the Viennet problem, is utilized to evaluate the performance of the proposed new algorithm. The study indicates that the proposed algorithm yields a much better spread of solutions and converges closer to the true Pareto frontier compared with The Non-dominated Sorting Genetic Algorithm and Improving the Strength Pareto Evolutionary Algorithm. MEMOIA is applied to a water allocation problem in the Dongjiang River basin in southern China, with three objectives named economic interests (OF 1), water shortages (OF 2) and the amount of organic pollutants in water (OF 3). The results demonstrate the capabilities of MEMOIA as well as its suitability as a viable alternative for enhanced water allocation and management in a river basin. © 2011 Her Majesty the Queen in Right of Australia.","Artificial immune systems; Macro-evolutionary; Multi-objective optimization; Water allocation","Artificial Immune System; Benchmark test problem; Biological immune system; Computational system; Economic interests; Immune algorithms; Macro-evolutionary; Multi objective; Multi objective optimizations (MOO); Multi-objective allocation; Non-dominated sorting genetic algorithms; Optimal allocation; Pareto frontiers; River basins; South China; Southern China; Species evolution; Species extinction; Strength pareto evolutionary algorithm; Time-scales; Water allocation; Water allocations; Water shortages; Benchmarking; Immunology; Macros; Multiobjective optimization; Water resources; Water supply; Watersheds; Evolutionary algorithms; artificial intelligence; basin management; genetic algorithm; optimization; organic pollutant; resource allocation; river basin; water resource; China; Dongjiang Basin; Guangdong",Article,Scopus,2-s2.0-84859441950
"Lee W.-I., Shih B.-Y., Chen C.-Y.","A hybrid artificial intelligence sales-forecasting system in the convenience store industry",2012,"Human Factors and Ergonomics In Manufacturing",13,10.1002/hfm.20272,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859017599&doi=10.1002%2fhfm.20272&partnerID=40&md5=aa447d1850fb078e409873df0fe7455d","Recently, there has been increasing interest in computer-aided ergonomics and its applications, such as in the fields of intelligent robots, intelligent mobiles, intelligent stores, and so on. The operation of convenience stores (CVS) in Taiwan is facing a crossover revolution by providing multiple services, including daily fresh foods, a café, ticketing, and a grocery. Therefore, forecasting the daily sales of fresh foods is getting more and more complex due to the influence of both internal and external factors. Eventually, a reliable sales-forecasting system will play an important role in improving business strategies and increasing competitive advantages. The purpose of this study is the development of an enhanced hybrid sales-forecasting model of fresh foods, called ECFM (Enhanced Cluster and Forecast Model), for CVSs by combining a self-organization map (SOM) neural network and radial basis function (RBF) neural networks. The model is evaluated for a six-month sales data set of daily fresh foods at a chained CVS in Taiwan. Meanwhile, the performance of the proposed model is compared with that of fuzzy neural network (FNN) and cluster and forecast model (CFM). The result reveals that the proposed model is not only amenable but can also promise the fresh food sales forecasting for CVSs. Copyright © 2011 Wiley Periodicals, Inc., A Wiley Company.","Artificial intelligence; Convenience stores; Intelligent robot; Radial basis function neural network; Sales forecasting; Self-organizing map","Business strategy; Competitive advantage; Convenience stores; Data sets; FORECAST model; Fresh food; Internal and external factors; Multiple services; Radial basis function neural networks; Sales forecasting; Artificial intelligence; Competition; Conformal mapping; Ergonomics; Fuzzy neural networks; Intelligent robots; Radial basis function networks; Sales; Forecasting",Article,Scopus,2-s2.0-84859017599
"Wrzodek C., Büchel F., Hinselmann G., Eichner J., Mittag F., Zell A.","Linking the epigenome to the genome: Correlation of different features to DNA methylation of CpG islands",2012,"PLoS ONE",13,10.1371/journal.pone.0035327,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860496011&doi=10.1371%2fjournal.pone.0035327&partnerID=40&md5=202def82398e8e9eaacd6cf244e5c814","DNA methylation of CpG islands plays a crucial role in the regulation of gene expression. More than half of all human promoters contain CpG islands with a tissue-specific methylation pattern in differentiated cells. Still today, the whole process of how DNA methyltransferases determine which region should be methylated is not completely revealed. There are many hypotheses of which genomic features are correlated to the epigenome that have not yet been evaluated. Furthermore, many explorative approaches of measuring DNA methylation are limited to a subset of the genome and thus, cannot be employed, e.g., for genome-wide biomarker prediction methods. In this study, we evaluated the correlation of genetic, epigenetic and hypothesis-driven features to DNA methylation of CpG islands. To this end, various binary classifiers were trained and evaluated by cross-validation on a dataset comprising DNA methylation data for 190 CpG islands in HEPG2, HEK293, fibroblasts and leukocytes. We achieved an accuracy of up to 91% with an MCC of 0.8 using ten-fold cross-validation and ten repetitions. With these models, we extended the existing dataset to the whole genome and thus, predicted the methylation landscape for the given cell types. The method used for these predictions is also validated on another external whole-genome dataset. Our results reveal features correlated to DNA methylation and confirm or disprove various hypotheses of DNA methylation related features. This study confirms correlations between DNA methylation and histone modifications, DNA structure, DNA sequence, genomic attributes and CpG island properties. Furthermore, the method has been validated on a genome-wide dataset from the ENCODE consortium. The developed software, as well as the predicted datasets and a web-service to compare methylation states of CpG islands are available at http://www.cogsys.cs.uni-tuebingen.de/software/dna-methylation/. © 2012 Wrzodek et al.",,"accuracy; algorithm; article; cell strain HEK293; cell strain HepG2; chromosome 21; classifier; computer program; CpG island; DNA methylation; DNA sequence; DNA structure; epigenetics; fibroblast; genome; histone modification; information service; leukocyte; machine learning; prediction; radial based function; support vector machine; algorithm; artificial intelligence; biological model; cell line; chemistry; computer program; gene expression regulation; genetics; human; Internet; metabolism; promoter region; validation study; DNA; histone; Algorithms; Artificial Intelligence; Cell Line; CpG Islands; DNA; DNA Methylation; Gene Expression Regulation; Histones; Humans; Internet; Models, Genetic; Promoter Regions, Genetic; Software",Article,Scopus,2-s2.0-84860496011
"Chia C.-C., Rubinfeld I., Scirica B.M., McMillan S., Gurm H.S., Syed Z.","Looking beyond historical patient outcomes to improve clinical models",2012,"Science Translational Medicine",13,10.1126/scitranslmed.3003561,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860366215&doi=10.1126%2fscitranslmed.3003561&partnerID=40&md5=0644f951d96dc57ffe9956ba86e65083","Conventional algorithms for modeling clinical events focus on characterizing the differences between patients with varying outcomes in historical data sets used for the model derivation. For many clinical conditions with low prevalence and where small data sets are available, this approach to developing models is challenging due to the limited number of positive (that is, event) examples available for model training. Here, we investigate how the approach of developing clinical models might be improved across three distinct patient populations (patients with acute coronary syndrome enrolled in the DISPERSE2-TIMI33 and MERLIN-TIMI36 trials, patients undergoing inpatient surgery in the National Surgical Quality Improvement Program registry, and patients undergoing percutaneous coronary intervention in the Blue Cross Blue Shield of Michigan Cardiovascular Consortium registry). For each of these cases, we supplement an incomplete characterization of patient outcomes in the derivation data set (uncensored view of the data) with an additional characterization of the extent to which patients differ from the statistical support of their clinical characteristics (censored view of the data). Our approach exploits the same training data within the derivation cohort in multiple ways to improve the accuracy of prediction. We position this approach within the context of traditional supervised (2-class) and unsupervised (1-class) learning methods and present a 1.5-class approach for clinical decision-making. We describe a 1.5-class support vector machine (SVM) classification algorithm that implements this approach, and report on its performance relative to logistic regression and 2-class SVM classification with cost-sensitive weighting and oversampling. The 1.5-class SVM algorithm improved prediction accuracy relative to other approaches and may have value in predicting clinical events both at the bedside and for risk-adjusted quality of care assessment.",,"accuracy; acute coronary syndrome; area under the curve; article; bleeding; calibration; cardiovascular disease; cerebrovascular accident; classification algorithm; clinical decision making; cohort analysis; coma; controlled study; cost effectiveness analysis; graft failure; heart arrest; heart infarction; human; integrated discrimination improvement; intermethod comparison; kidney failure; learning curve; logistic regression analysis; lung embolism; morbidity; nonbiological model; outcome assessment; parameters; percutaneous coronary intervention; peripheral nerve injury; predictive value; priority journal; receiver operating characteristic; reliability; risk assessment; support vector machine; validation process; acute coronary syndrome; algorithm; artificial intelligence; comparative study; data mining; decision support system; discriminant analysis; health care quality; heart infarction; mortality; postoperative complication; prevalence; register; reproducibility; risk factor; standard; statistical model; statistics; surgery; total quality management; transluminal coronary angioplasty; treatment outcome; United States; Acute Coronary Syndrome; Algorithms; Angioplasty, Balloon, Coronary; Artificial Intelligence; Data Mining; Decision Support Techniques; Discriminant Analysis; Humans; Logistic Models; Models, Statistical; Myocardial Infarction; Postoperative Complications; Prevalence; Quality Improvement; Quality Indicators, Health Care; Registries; Reproducibility of Results; Risk Assessment; Risk Factors; Surgical Procedures, Operative; Treatment Outcome; United States",Article,Scopus,2-s2.0-84860366215
"Perez L., Dragicevic S.","Landscape-level simulation of forest insect disturbance: Coupling swarm intelligent agents with GIS-based cellular automata model",2012,"Ecological Modelling",13,10.1016/j.ecolmodel.2012.01.020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857403503&doi=10.1016%2fj.ecolmodel.2012.01.020&partnerID=40&md5=3e6a0fe23d93d5d31247569581d0a4d9","Forest insect disturbances have an ecological impact and are the cause of partial or complete stands mortality; hence they influence the forest cover change. The modeling of ecological processes such as insect disturbance is challenging due to the complexity of insect outbreaks in forest ecological systems, thus diverse spatial scales need to be considered in order to effectively represent these dynamic spatial phenomena. The objective of the study is to develop a hybrid model that combines swarm intelligence (SI), agent-based modeling (ABM) and cellular automata (CA) with geographic information systems (GIS) for simulating tree mortality patterns introduced by insect infestations at a landscape spatial scale. The focus is on lodgepole pine, Pinus contorta, tree mortality patterns caused by infestations of mountain pine beetle (MPB), Dendroctonus ponderosae Hopkins. The complexity of the insects' behavior during forest disturbances can be captured and simulated by an intelligent ABM. Agents represent insects that have the ability to behave and adapt according to their interactions within the forest environment at a very fine spatial scale at tree-level, and with the use of swarming intelligence approach. However, due to computational complexity such model is not operational at landscape and regional spatial scales where the consequences of infestation phenomenon are most obvious. Therefore, the integration of the ABM with CA approach is proposed to handle modeling at both fine and large spatial scales. The discrete nature of CA enables integration with raster-based geospatial datasets in GIS, and can also be beneficial when modeling complex ecological processes that evolve over time. The developed model includes factors such as wind directions and elevation to demonstrate their influence in the spread patterns of the outbreaks at a landscape spatial scale. The model outcomes provide ""what if"" scenarios that can assist studying and controlling MPB forest disturbance. © 2012 Elsevier B.V.","Agent-based model; Cellular automata; Forest disturbances; Mountain pine beetle (MPB); Spatiotemporal modeling; Swarm intelligence","Agent-based model; Forest disturbances; Mountain pine beetle; Spatiotemporal modeling; Swarm Intelligence; Artificial intelligence; Cellular automata; Computational methods; Ecology; Geographic information systems; Intelligent agents; Forestry; adaptation; artificial intelligence; cellular automaton; complexity; data set; disturbance; ecological impact; forest cover; GIS; insect; landscape ecology; mortality; numerical model; pest species; raster; spatial analysis; wind direction; Ecology; Forestry; Insects; Pinus Mugo; Coleoptera; Dendroctonus ponderosae; Hexapoda; Pinus contorta; Pinus mugo",Article,Scopus,2-s2.0-84857403503
"Den Heijer E., Eiben A.E.","Evolving pop art using scalable vector graphics",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-642-29142-5_5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859144377&doi=10.1007%2f978-3-642-29142-5_5&partnerID=40&md5=ca6f62566198da85d0b5ff6a370b56f2","In this paper we present our findings of our continued investigation into the use of Scalable Vector Graphics as a genotype representation in evolutionary art. In previous work we investigated the feasibility of SVG as a genetic representation for evolutionary art, and found that the representation was very flexible, but that the potential visual output was somewhat limited by the simplicity of our genetic operators. In this paper we extend on this work, and introduce various new, more expressive genetic operators for SVG. We show that SVG is a flexible and powerful representation for evolutionary art, and that the potential visual output is only limited by the design of the genetic operators. With the genetic operators that we describe in this paper, we are able to evolve art that is visually similar to screen printing art and pop art. © 2012 Springer-Verlag.",,"Genetic operators; Genetic representations; Scalable vector graphics; Visual outputs; Artificial intelligence; Design",Conference Paper,Scopus,2-s2.0-84859144377
"Marks M., Jóźwiak-NiedzWiedzka D., Glinicki M.A.","Automatic categorization of chloride migration into concrete modified with CFBC ash",2012,"Computers and Concrete",13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860324885&partnerID=40&md5=32ef27b110056356b4ea4ff110b4886e","The objective of this investigation was to develop rules for automatic categorization of concrete quality using selected artificial intelligence methods based on machine learning. The range of tested materials included concrete containing a new waste material - solid residue from coal combustion in fluidized bed boilers (CFBC fly ash) used as additive. The rapid chloride permeability test - Nordtest Method BUILD 492 method was used for determining chloride ions penetration in concrete. Performed experimental tests on obtained chloride migration provided data for learning and testing of rules discovered by machine learning techniques. It has been found that machine learning is a tool which can be applied to determine concrete durability. The rules generated by computer programs AQ21 and WEKA using J48 algorithm provided means for adequate categorization of plain concrete and concrete modified with CFBC fly ash as materials of good and acceptable resistance to chloride penetration.","Chloride ions migration; Circulated fluidized bed combustion fly ash (cfbc fly ash); Classification rules; Concrete durability; Database; Machine learning","Artificial intelligence methods; Automatic categorization; Chloride ions; Chloride penetration; Circulated fluidized bed combustion; Classification rules; Concrete durability; Concrete quality; Experimental test; Fluidized bed boilers; Machine learning techniques; Plain concrete; Rapid Chloride Permeability Test (RCPT); Solid residues; Artificial intelligence; Coal combustion; Database systems; Durability; Fluidized bed combustors; Fly ash; Learning systems; Chlorine compounds",Article,Scopus,2-s2.0-84860324885
"Wang G., Wang F., Chen T., Yeung D.-Y., Lochovsky F.H.","Solution path for manifold regularized semisupervised classification",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",13,10.1109/TSMCB.2011.2168205,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859009842&doi=10.1109%2fTSMCB.2011.2168205&partnerID=40&md5=30c859c503ef295c7e6ebe5eb8c8e541","Traditional learning algorithms use only labeled data for training. However, labeled examples are often difficult or time consuming to obtain since they require substantial human labeling efforts. On the other hand, unlabeled data are often relatively easy to collect. Semisupervised learning addresses this problem by using large quantities of unlabeled data with labeled data to build better learning algorithms. In this paper, we use the manifold regularization approach to formulate the semisupervised learning problem where a regularization framework which balances a tradeoff between loss and penalty is established. We investigate different implementations of the loss function and identify the methods which have the least computational expense. The regularization hyperparameter, which determines the balance between loss and penalty, is crucial to model selection. Accordingly, we derive an algorithm that can fit the entire path of solutions for every value of the hyperparameter. Its computational complexity after preprocessing is quadratic only in the number of labeled examples rather than the total number of labeled and unlabeled examples. © 2006 IEEE.","manifold regularization; semi-supervised classification; solution path","Computational expense; Hyper-parameter; Labeled data; Loss functions; manifold regularization; Model Selection; Regularization approach; Regularization framework; semi-supervised classification; Semi-supervised learning; solution path; Traditional learning; Unlabeled data; Learning algorithms; Supervised learning; Classification (of information); algorithm; article; artificial intelligence; automated pattern recognition; biometry; factual database; human; methodology; theoretical model; Algorithms; Artificial Intelligence; Biometric Identification; Databases, Factual; Humans; Models, Theoretical; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84859009842
"Ding L., Yilmaz A., Yan R.","Interactive image segmentation using dirichlet process multiple-view learning",2012,"IEEE Transactions on Image Processing",13,10.1109/TIP.2011.2181398,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859081900&doi=10.1109%2fTIP.2011.2181398&partnerID=40&md5=9bcaa9a27c4d04c088d923da43bf573c","Segmenting semantically meaningful whole objects from images is a challenging problem, and it becomes especially so without higher level common sense reasoning. In this paper, we present an interactive segmentation framework that integrates image appearance and boundary constraints in a principled way to address this problem. In particular, we assume that small sets of pixels, which are referred to as seed pixels, are labeled as the object and background. The seed pixels are used to estimate the labels of the unlabeled pixels using Dirichlet process multiple-view learning, which leverages 1) multiple-view learning that integrates appearance and boundary constraints and 2) Dirichlet process mixture-based nonlinear classification that simultaneously models image features and discriminates between the object and background classes. With the proposed learning and inference algorithms, our segmentation framework is experimentally shown to produce both quantitatively and qualitatively promising results on a standard dataset of images. In particular, our proposed framework is able to segment whole objects from images given insufficient seeds. © 2011 IEEE.","Dirichlet processes; image segmentation; probabilistic models","Boundary constraints; Commonsense reasoning; Data sets; Dirichlet process; Image appearance; Image features; Inference algorithm; Interactive image segmentation; Interactive segmentation; Multiple-view; Nonlinear classification; Probabilistic models; Seed pixel; Inference engines; Pixels; Variational techniques; Image segmentation; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; computer interface; image enhancement; methodology; reproducibility; sensitivity and specificity; three dimensional imaging; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; User-Computer Interface",Article,Scopus,2-s2.0-84859081900
"Saa J.F.D., Çetin M.","A latent discriminative model-based approach for classification of imaginary motor tasks from EEG data",2012,"Journal of Neural Engineering",13,10.1088/1741-2560/9/2/026020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859143920&doi=10.1088%2f1741-2560%2f9%2f2%2f026020&partnerID=40&md5=e5392ac63d08182372809cfa5074e5e6","We consider the problem of classification of imaginary motor tasks from electroencephalography (EEG) data for brain-computer interfaces (BCIs) and propose a new approach based on hidden conditional random fields (HCRFs). HCRFs are discriminative graphical models that are attractive for this problem because they (1) exploit the temporal structure of EEG; (2) include latent variables that can be used to model different brain states in the signal; and (3) involve learned statistical models matched to the classification task, avoiding some of the limitations of generative models. Our approach involves spatial filtering of the EEG signals and estimation of power spectra based on autoregressive modeling of temporal segments of the EEG signals. Given this time-frequency representation, we select certain frequency bands that are known to be associated with execution of motor tasks. These selected features constitute the data that are fed to the HCRF, parameters of which are learned from training data. Inference algorithms on the HCRFs are used for the classification of motor tasks. We experimentally compare this approach to the best performing methods in BCI competition IV as well as a number of more recent methods and observe that our proposed method yields better classification accuracy. © 2012 IOP Publishing Ltd.",,"Autoregressive modeling; Brain state; Classification accuracy; Classification tasks; EEG signals; Electroencephalographies (EEG); Generative model; GraphicaL model; Hidden conditional random fields; Inference algorithm; Latent variable; Model based approach; Motor tasks; Power-spectra; Spatial filterings; Statistical models; Temporal segments; Temporal structures; Time-frequency representations; Training data; Brain computer interface; Brain models; Electrophysiology; Frequency bands; Inference engines; Electroencephalography; accuracy; algorithm; article; artifact reduction; brain computer interface; brain electrophysiology; controlled study; electroencephalogram; hidden conditional random field; imagination; motor performance; nervous system parameters; priority journal; artifact; artificial intelligence; biological model; brain mapping; classification; computer interface; electroencephalography; human; methodology; normal distribution; physiology; psychomotor performance; reproducibility; statistical model; statistics; Algorithms; Artifacts; Artificial Intelligence; Brain Mapping; Electroencephalography; Humans; Imagination; Models, Neurological; Models, Statistical; Normal Distribution; Psychomotor Performance; Reproducibility of Results; User-Computer Interface",Article,Scopus,2-s2.0-84859143920
"Lee W.-P., Hsiao Y.-T.","Inferring gene regulatory networks using a hybrid GA-PSO approach with numerical constraints and network decomposition",2012,"Information Sciences",13,10.1016/j.ins.2011.11.020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855463068&doi=10.1016%2fj.ins.2011.11.020&partnerID=40&md5=78a3c428ba5c4fc93fb555a755088ef4","Gene regulatory networks (GRNs) are essential for cellular metabolism during the development of living organisms. Reconstructing gene networks from expression profiling data can help biologists generate and test hypotheses to investigate the complex phenomena of nature systems. However, building regulatory models is a tedious task, especially when the number of genes and the complexity of regulation increase. To automate the procedure of network reconstruction, we establish a methodology to infer the computational network model and to deal with the problem of scalability from two directions. The first is to develop an enhanced GA-PSO hybrid method to search promising solutions, and the second is to develop a network decomposition procedure to reduce the task complexity. Meanwhile, our work includes a quantitative method to consider prior knowledge in the inference process to ensure validity of the obtained results. Experiments have been conducted to evaluate the proposed approach. The results indicate that it can be used to infer GRNs successfully and can achieve better performance. © 2011 Elsevier Inc. All rights reserved.","Constraint-based search; Gene regulatory network; Network decomposition; Reverse engineering; Swarm intelligence; Systems biology","Constraint-based; Gene regulatory network; Network decomposition; Swarm Intelligence; Systems biology; Artificial intelligence; Biology; Cellular automata; Particle swarm optimization (PSO); Reverse engineering; Gene expression",Article,Scopus,2-s2.0-84855463068
"Alama J., Kühlwein D., Urban J.","Automated and human proofs in general mathematics: An initial comparison",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-642-28717-6_6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858328940&doi=10.1007%2f978-3-642-28717-6_6&partnerID=40&md5=a662758ac6c5dcbefed9e032a0b779a4","First-order translations of large mathematical repositories allow discovery of new proofs by automated reasoning systems. Large amounts of available mathematical knowledge can be re-used by combined AI/ATP systems, possibly in unexpected ways. But automated systems can be also more easily misled by irrelevant knowledge in this setting, and finding deeper proofs is typically more difficult. Both large-theory AI/ATP methods, and translation and data-mining techniques of large formal corpora, have significantly developed recently, providing enough data for an initial comparison of the proofs written by mathematicians and the proofs found automatically. This paper describes such an initial experiment and comparison conducted over the 50000 mathematical theorems from the Mizar Mathematical Library. © 2012 Springer-Verlag.",,"Automated reasoning; Automated systems; First-order; Irrelevant knowledge; Mathematical knowledge; Mizar mathematical libraries; Artificial intelligence; Automata theory; Automation; Mathematical programming",Conference Paper,Scopus,2-s2.0-84858328940
"Mandal A., Patarin J., Seurin Y.","On the public indifferentiability and correlation intractability of the 6-round feistel construction",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-642-28914-9_16,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858325568&doi=10.1007%2f978-3-642-28914-9_16&partnerID=40&md5=0daa7917c3fdd3afa49351dbb0ae43ff","We show that the Feistel construction with six rounds and random round functions is publicly indifferentiable from a random invertible permutation (a result that is not known to hold for full indifferentiability). Public indifferentiability (pub-indifferentiability for short) is a variant of indifferentiability introduced by Yoneyama et al. [29] and Dodis et al. [12] where the simulator knows all queries made by the distinguisher to the primitive it tries to simulate, and is useful to argue the security of cryptosystems where all the queries to the ideal primitive are public (as e.g. in many digital signature schemes). To prove the result, we introduce a new and simpler variant of indifferentiability, that we call sequential indifferentiability (seq-indifferentiability for short) and show that this notion is in fact equivalent to pub-indifferentiability for stateless ideal primitives. We then prove that the 6-round Feistel construction is seq-indifferentiable from a random invertible permutation. We also observe that sequential indifferentiability implies correlation intractability, so that the Feistel construction with six rounds and random round functions yields a correlation intractable invertible permutation, a notion we define analogously to correlation intractable functions introduced by Canetti et al. [4]. © 2012 Springer-Verlag.","correlation intractability; Feistel construction; indifferentiability","Digital signature schemes; Distinguishers; Indifferentiability; Round functions; Correlation intractabilities; Digital signature schemes; Distinguishers; Feistel constructions; Indifferentiability; Round functions; Artificial intelligence; Aluminum; Authentication; Electronic document identification systems; Cryptography; Cryptography",Conference Paper,Scopus,2-s2.0-84858325568
"Parkinson J., Muthas D., Clark M., Boyer S., Valentin J., Ewart L.","Application of data mining and visualization techniques for the prediction of drug-induced nausea in man",2012,"Toxicological Sciences",13,10.1093/toxsci/kfr334,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857979437&doi=10.1093%2ftoxsci%2fkfr334&partnerID=40&md5=abde518d6513d5b2f522867b3b1db781","The therapeutic value of many drugs can be limited by gastrointestinal (GI) adverse effects such as nausea and vomiting. Nausea is a subjective human sensation, hence little is known about preclinical biomarkers that may accurately and effectively predict its presence in man. The aim of this analysis was to use informatics and data-mining tools to identify plausible preclinical GI effects that may be associated with nausea and that could be of potential use in its prediction. A total of 86 marketed drugs were used in this analysis, and the main outcome was a confirmation that nausogenic and non-nausogenic drugs can be clearly separated based on their preclinical GI observations. Specifically, combinations of common preclinical GI effects (vomiting, diarrhea, and salivary hypersecretion) proved to be strong predictors. The model was subsequently validated with a subset of 20 blinded proprietary small molecules and successfully predicted clinical outcome in 90% of cases. This investigation demonstrated the feasibility of data-mining approaches to facilitate discovery of novel, plausible associations that can be used to understand drug-induced adverse effects. © The Author 2011. Published by Oxford University Press on behalf of the Society of Toxicology. All rights reserved.","Data mining; Drug safety; Drug-induced gastrointestinal adverse effects; Nausea; Predictive value; Risk assessment; Vomiting","article; constipation; data mining; diarrhea; digestive system inflammation; digestive system ulcer; drug induced nausea; feasibility study; gastritis; human; hypersalivation; nausea; nonhuman; pancreas disease; salivary gland disease; umbilical hernia; vomiting; Animals; Artificial Intelligence; Cluster Analysis; Computer Graphics; Data Mining; Diarrhea; Drug Evaluation, Preclinical; Drugs, Investigational; Gastrointestinal Tract; Humans; Medical Informatics; Models, Biological; Nausea; Prescription Drugs; Risk Assessment; Sialorrhea; Vomiting",Article,Scopus,2-s2.0-84857979437
"Fuentes-Castañeda L., Knapp E., Rodríguez-Henríquez F.","Faster Hashing to double-struck G 2",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-642-28496-0_25,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857757840&doi=10.1007%2f978-3-642-28496-0_25&partnerID=40&md5=b5ab0e65a2742c11ffc347c8a447508e","An asymmetric pairing e: double-struck G 2 x double-struck G 1 → double-struck G T is considered such that double-struck G 1 = E(double-struck F p)[r] and double-struck G 2 = Ẽ (double-struck F pk/d)[r], where k is the embedding degree of the elliptic curve E/double-struck F p, r is a large prime divisor of #E(double-struck F p), and Ẽ is the degree-d twist of E over double-struck F pk/d with r | Ẽ(double-struck F pk/d). Hashing to double-struck G 1 is considered easy, while hashing to double-struck G 2 is done by selecting a random point Q in Ẽ (double-struck F pk/d) and computing the hash value cQ, where c · r is the order of Ẽ(double-struck F pk/d). We show that for a large class of curves, one can hash to double-struck G 2 in O(1/φ(k) log c) time, as compared with the previously fastestknown O(log p). In the case of BN curves, we are able to double the speed of hashing to double-struck G 2. For higher-embedding-degree curves, the results can be more dramatic. We also show how to reduce the cost of the finalexponentiation step in a pairing calculation by a fixed number of field multiplications. © 2012 Springer-Verlag.","fast hashing; final exponentiation; Pairing-based cryptography","Elliptic curve; Exponentiations; fast hashing; Fixed numbers; Hash value; Large class; Pairing-based cryptography; Random points; Artificial intelligence; Cryptography",Conference Paper,Scopus,2-s2.0-84857757840
"Adams S.S., Arel I., Bach J., Coop R., Furlan R., Goertzel B., Hall J.S., Samsonovich A., Scheutz M., Schlesinger M., Shapiro S.C., Sowa J.F.","Mapping the landscape of human-level artificial general intelligence",2012,"AI Magazine",13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861407513&partnerID=40&md5=29c20aa18bbc675da7283b62d784d914","We present the broad outlines of a roadmap toward human-level artificial general intelligence (henceforth, AGI). We begin by discussing AGI in general, adopting a pragmatic goal for its attainment and a necessary foundation of characteristics and requirements. An initial capability landscape will be presented, drawing on major themes from developmental psychology and illuminated by mathematical, physiological, and information-processing perspectives. The challenge of identifying appropriate tasks and environments for measuring AGI will be addressed, and seven scenarios will be presented as milestones suggesting a roadmap across the AGI landscape along with directions for future research and collaboration. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Developmental psychology; General Intelligence; Roadmap; Artificial intelligence",Article,Scopus,2-s2.0-84861407513
"Huang H., Andrews J., Tang J.","Citation characterization and impact normalization in bioinformatics journals",2012,"Journal of the American Society for Information Science and Technology",13,10.1002/asi.21707,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857646709&doi=10.1002%2fasi.21707&partnerID=40&md5=2db5dbb3e9c83a919ac33f1b3d87ce44","Bioinformatics journals publish research findings of intellectual synergies among subfields such as biology, mathematics, and computer science. The objective of this study is to characterize the citation patterns in bioinformatics journals and their correspondent knowledge subfields. Our study analyzed bibliometric data (impact factor, cited-half-life, and references-per-article) of bioinformatics journals and their related subfields collected from the Journal Citation Reports (JCR). The findings showed that bioinformatics journals' citations are field-dependent, with scattered patterns in article life span and citing propensity. Bioinformatics journals originally derived from biology-related subfields have shorter article life spans, more citing on average, and higher impact factors. Those journals, derived from mathematics and statistics, demonstrate converse citation patterns. Journal impact factors were normalized, taking into account the impacts of article life spans and citing propensity. A comparison of these normalized factors to JCR journal impact factors showed rearrangements in the ranking orders of a number of individual journals, but a high overall correlation with JCR impact factors. © 2011 ASIS&T.",,"Impact factor; Journal citation reports; Life span; Subfields; Artificial intelligence; Software engineering; Bioinformatics",Article,Scopus,2-s2.0-84857646709
"Celino I., Dell'Aglio D., Della Valle E., Huang Y., Lee T., Kim S.-H., Tresp V.","Towards BOTTARI: Using stream reasoning to make sense of location-based micro-posts",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-642-25953-1_7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863132081&doi=10.1007%2f978-3-642-25953-1_7&partnerID=40&md5=0050d35d0f25b266329f367337e2903f","Consider an urban environment and its semi-public realms (e.g., shops, bars, visitors attractions, means of transportation). Who is the maven of a district? How fast and how broad can such maven influence the opinions of others? These are just few of the questions BOTTARI (our Location-based Social Media Analysis mobile app) is getting ready to answer. In this position paper, we recap our investigation on deductive and inductive stream reasoning for social media analysis, and we show how the results of this research form the underpinning of BOTTARI. © 2012 Springer-Verlag.",,"Location based; Position papers; Social media analysis; Urban environments; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84863132081
"Burstein D., Gould S.B., Zimorski V., Kloesges T., Kiosse F., Major P., Martin W.F., Pupko T., Dagan T.","A machine learning approach to identify hydrogenosomal proteins in trichomonas vaginalis",2012,"Eukaryotic Cell",13,10.1128/EC.05225-11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856669080&doi=10.1128%2fEC.05225-11&partnerID=40&md5=7433deaecc2e2bdbe173bd40276c6d95","The protozoan parasite Trichomonas vaginalis is the causative agent of trichomoniasis, the most widespread nonviral sexually transmitted disease in humans. It possesses hydrogenosomes-anaerobic mitochondria that generate H 2, CO 2, and acetate from pyruvate while converting ADP to ATP via substrate-level phosphorylation. T. vaginalis hydrogenosomes lack a genome and translation machinery; hence, they import all their proteins from the cytosol. To date, however, only 30 imported proteins have been shown to localize to the organelle. A total of 226 nuclear-encoded proteins inferred from the genome sequence harbor a characteristic short N-terminal presequence, reminiscent of mitochondrial targeting peptides, which is thought to mediate hydrogenosomal targeting. Recent studies suggest, however, that the presequences might be less important than previously thought. We sought to identify new hydrogenosomal proteins within the 59,672 annotated open reading frames (ORFs) of T. vaginalis, independent of the N-terminal targeting signal, using a machine learning approach. Our training set included 57 gene and protein features determined for all 30 known hydrogenosomal proteins and 576 nonhydrogenosomal proteins. Several classifiers were trained on this set to yield an import score for all proteins encoded by T. vaginalis ORFs, predicting the likelihood of hydrogenosomal localization. The machine learning results were tested through immunofluorescence assay and immunodetection in isolated cell fractions of 14 protein predictions using hemagglutinin constructs expressed under the homologous SCSa promoter in transiently transformed T. vaginalis cells. Localization of 6 of the 10 top predicted hydrogenosome-localized proteins was confirmed, and two of these were found to lack an obvious N-terminal targeting signal. © 2012, American Society for Microbiology. All Rights Reserved.",,"protozoal protein; amino acid sequence; article; artificial intelligence; chemistry; gene; genetics; metabolism; molecular genetics; open reading frame; phylogeny; sequence alignment; Trichomonas vaginalis; Amino Acid Sequence; Artificial Intelligence; Genes, Protozoan; Molecular Sequence Data; Open Reading Frames; Phylogeny; Protozoan Proteins; Sequence Alignment; Trichomonas vaginalis; Protozoa; Trichomonas vaginalis",Article,Scopus,2-s2.0-84856669080
"Ensmenger N.","Is chess the drosophila of artificial intelligence? a social history of an algorithm",2012,"Social Studies of Science",13,10.1177/0306312711424596,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855952251&doi=10.1177%2f0306312711424596&partnerID=40&md5=c9f7d47109c597cea5d30abab5c0a822","Since the mid 1960s, researchers in computer science have famously referred to chess as the 'drosophila' of artificial intelligence (AI). What they seem to mean by this is that chess, like the common fruit fly, is an accessible, familiar, and relatively simple experimental technology that nonetheless can be used productively to produce valid knowledge about other, more complex systems. But for historians of science and technology, the analogy between chess and drosophila assumes a larger significance. As Robert Kohler has ably described, the decision to adopt drosophila as the organism of choice for genetics research had far-reaching implications for the development of 20th century biology. In a similar manner, the decision to focus on chess as the measure of both human and computer intelligence had important and unintended consequences for AI research. This paper explores the emergence of chess as an experimental technology, its significance in the developing research practices of the AI community, and the unique ways in which the decision to focus on chess shaped the program of AI research in the decade of the 1970s. More broadly, it attempts to open up the virtual black box of computer software - and of computer games in particular - to the scrutiny of historical and sociological analysis. © SAGE Publications 2011.","artificial intelligence; computing; drosophila; experimental technology","algorithm; animal; article; artificial intelligence; computer; computer program; Drosophila; history; psychology; recreation; research; sport; Algorithms; Animals; Artificial Intelligence; Cognitive Science; Computers; Drosophila; History, 20th Century; History, 21st Century; Play and Playthings; Research; Software; Sports",Article,Scopus,2-s2.0-84855952251
"Zhao T., Hachiya H., Niu G., Sugiyama M.","Analysis and improvement of policy gradient estimation",2012,"Neural Networks",13,10.1016/j.neunet.2011.09.005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855919433&doi=10.1016%2fj.neunet.2011.09.005&partnerID=40&md5=9559c9df0bdbb2880c5950d8bfaa9542","Policy gradient is a useful model-free reinforcement learning approach, but it tends to suffer from instability of gradient estimates. In this paper, we analyze and improve the stability of policy gradient methods. We first prove that the variance of gradient estimates in the PGPE (policy gradients with parameter-based exploration) method is smaller than that of the classical REINFORCE method under a mild assumption. We then derive the optimal baseline for PGPE, which contributes to further reducing the variance. We also theoretically show that PGPE with the optimal baseline is more preferable than REINFORCE with the optimal baseline in terms of the variance of gradient estimates. Finally, we demonstrate the usefulness of the improved PGPE method through experiments. © 2011 Elsevier Ltd.","Policy gradients; Policy gradients with parameter-based exploration; Reinforcement learning; Variance reduction","Gradient estimates; Model free; Policy gradient; Policy gradient methods; Reinforcement learning approach; Variance reduction; Estimation; Gradient methods; Optimization; Reinforcement; Reinforcement learning; algorithm; analysis of variance; analytic method; article; dynamics; mathematical analysis; mathematical model; methodology; policy gradient; priority journal; probability; process optimization; reinforcement; statistics; theory; Artificial Intelligence; Computer Simulation; Humans; Models, Statistical; Neural Networks (Computer); Reinforcement (Psychology)",Article,Scopus,2-s2.0-84855919433
"Combi C., Oliboni B.","Visually defining and querying consistent multi-granular clinical temporal abstractions",2012,"Artificial Intelligence in Medicine",13,10.1016/j.artmed.2011.10.004,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855948244&doi=10.1016%2fj.artmed.2011.10.004&partnerID=40&md5=e58288aa4e814fc649f5b191e8727ebf","Objective: The main goal of this work is to propose a framework for the visual specification and query of consistent multi-granular clinical temporal abstractions. We focus on the issue of querying patient clinical information by visually defining and composing temporal abstractions, i.e., high level patterns derived from several time-stamped raw data. In particular, we focus on the visual specification of consistent temporal abstractions with different granularities and on the visual composition of different temporal abstractions for querying clinical databases. Background: Temporal abstractions on clinical data provide a concise and high-level description of temporal raw data, and a suitable way to support decision making. Granularities define partitions on the time line and allow one to represent time and, thus, temporal clinical information at different levels of detail, according to the requirements coming from the represented clinical domain. The visual representation of temporal information has been considered since several years in clinical domains. Proposed visualization techniques must be easy and quick to understand, and could benefit from visual metaphors that do not lead to ambiguous interpretations. Recently, physical metaphors such as strips, springs, weights, and wires have been proposed and evaluated on clinical users for the specification of temporal clinical abstractions. Visual approaches to boolean queries have been considered in the last years and confirmed that the visual support to the specification of complex boolean queries is both an important and difficult research topic. Methodology: We propose and describe a visual language for the definition of temporal abstractions based on a set of intuitive metaphors (striped wall, plastered wall, brick wall), allowing the clinician to use different granularities. A new algorithm, underlying the visual language, allows the physician to specify only consistent abstractions, i.e., abstractions not containing contradictory conditions on the component abstractions. Moreover, we propose a visual query language where different temporal abstractions can be composed to build complex queries: temporal abstractions are visually connected through the usual logical connectives AND, OR, and NOT. Results: The proposed visual language allows one to simply define temporal abstractions by using intuitive metaphors, and to specify temporal intervals related to abstractions by using different temporal granularities. The physician can interact with the designed and implemented tool by point-and-click selections, and can visually compose queries involving several temporal abstractions. The evaluation of the proposed granularity-related metaphors consisted in two parts: (i) solving 30 interpretation exercises by choosing the correct interpretation of a given screenshot representing a possible scenario, and (ii) solving a complex exercise, by visually specifying through the interface a scenario described only in natural language. The exercises were done by 13 subjects. The percentage of correct answers to the interpretation exercises were slightly different with respect to the considered metaphors (54.4 - striped wall, 73.3 - plastered wall, 61 - brick wall, and 61 - no wall), but post hoc statistical analysis on means confirmed that differences were not statistically significant. The result of the user's satisfaction questionnaire related to the evaluation of the proposed granularity-related metaphors ratified that there are no preferences for one of them. The evaluation of the proposed logical notation consisted in two parts: (i) solving five interpretation exercises provided by a screenshot representing a possible scenario and by three different possible interpretations, of which only one was correct, and (ii) solving five exercises, by visually defining through the interface a scenario described only in natural language. Exercises had an increasing difficulty. The evaluation involved a total of 31 subjects. Results related to this evaluation phase confirmed us about the soundness of the proposed solution even in comparison with a well known proposal based on a tabular query form (the only significant difference is that our proposal requires more time for the training phase: 21. min versus 14. min). Discussion and conclusions: In this work we have considered the issue of visually composing and querying temporal clinical patient data. In this context we have proposed a visual framework for the specification of consistent temporal abstractions with different granularities and for the visual composition of different temporal abstractions to build (possibly) complex queries on clinical databases. A new algorithm has been proposed to check the consistency of the specified granular abstraction. From the evaluation of the proposed metaphors and interfaces and from the comparison of the visual query language with a well known visual method for boolean queries, the soundness of the overall system has been confirmed; moreover, pros and cons and possible improvements emerged from the comparison of different visual metaphors and solutions. © 2011 Elsevier B.V.","Hemodialysis data; Metaphors; Temporal abstractions; Temporal clinical data; Temporal granularities; Visual query languages","Hemodialysis data; Metaphors; Temporal abstraction; Temporal clinical data; Temporal granularity; Visual query languages; Algorithms; Brick; Data visualization; Dialysis; Hospital data processing; Query languages; Specifications; Visualization; Abstracting; algorithm; article; artificial intelligence; comparative study; conceptual framework; decision making; human; human computer interaction; intuition; natural language processing; priority journal; questionnaire; satisfaction; temporal abstraction; temporal granularity; visual information; visual query language; visual system; Abstracting and Indexing as Topic; Algorithms; Artificial Intelligence; Computer Graphics; Data Mining; Decision Support Systems, Clinical; Decision Support Techniques; Humans; Medical Records Systems, Computerized; Natural Language Processing; Pattern Recognition, Visual; Programming Languages; Renal Dialysis; Reproducibility of Results; Time Factors; User-Computer Interface; Visual Perception",Article,Scopus,2-s2.0-84855948244
"De Marco G., Romaniello M.","Beliefs correspondences and equilibria in ambiguous games",2012,"International Journal of Intelligent Systems",13,10.1002/int.21515,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855179617&doi=10.1002%2fint.21515&partnerID=40&md5=5c7d083940ca5e0fd1273ddc79d7e569","The Nash equilibrium concept combines two fundamental ideas. First, rational players choose the most preferred strategy given their beliefs about what other players will do. Second, it imposes the consistency condition that all players' beliefs are correct. This consistency condition has often been considered too strong, and different solution concepts have been introduced in the literature to take into account ambiguous beliefs. In this paper, we show, by means of examples, that in some situation beliefs might be dependent on the strategy profile and that this kind of contingent ambiguity affects equilibrium behavior differently with respect to the existing models of ambiguous games. Hence, we consider a multiple prior approach and subjective beliefs correspondences, which represent an exogenous ability of each player to put restrictions on beliefs over outcomes consistently with the strategy profile; we investigate existence of the equilibrium concepts corresponding to different attitudes toward ambiguity (namely, optimism and pessimism). Finally, we analyze particular beliefs correspondences: beliefs given by correlated equilibria and by ambiguity levels on events. Copyright © 2011 Wiley Periodicals, Inc.",,"Consistency conditions; Correlated equilibria; Equilibrium behavior; Nash Equilibrium; Solution concepts; Artificial intelligence; Software engineering",Conference Paper,Scopus,2-s2.0-84855179617
"Zhang G., Ferrari S., Cai C.","A comparison of information functions and search strategies for sensor planning in target classification",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",13,10.1109/TSMCB.2011.2165336,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856304820&doi=10.1109%2fTSMCB.2011.2165336&partnerID=40&md5=3a57155680448bd1baec403d983324a3","This paper investigates the comparative performance of several information-driven search strategies and decision rules using a canonical target classification problem. Five sensor models are considered: one obtained from classical estimation theory and four obtained from Bernoulli, Poisson, binomial, and mixture-of-binomial distributions. A systematic approach is presented for deriving information functions that represent the expected utility of future sensor measurements from mutual information, Rnyi divergence, Kullback-Leibler divergence, information potential, quadratic entropy, and the Cauchy-Schwarz distance. The resulting information-driven strategies are compared to direct-search, alert-confirm, task-driven (TS), and log-likelihood-ratio (LLR) search strategies. Extensive numerical simulations show that quadratic entropy typically leads to the most effective search strategy with respect to correct-classification rates. In the presence of prior information, the quadratic-entropy-driven strategy also displays the lowest rate of false alarms. However, when prior information is absent or very noisy, TS and LLR strategies achieve the lowest false-alarm rates for the Bernoulli, mixture-of-binomial, and classical sensor models. © 2011 IEEE.","Classification; detection; information driven; information theory; management; optimal; planning; search; sensor; strategy; target","Bernoulli; Decision rules; Estimation theory; Expected utility; False alarms; False-alarm rates; information driven; Information function; Information potential; Kullback Leibler divergence; Mutual informations; optimal; Prior information; Quadratic entropy; search; Search strategies; Sensor measurements; Sensor model; Sensor planning; strategy; Target Classification; Task-driven; Classification (of information); Entropy; Error detection; Information theory; Lunar surface analysis; Management; Planning; Poisson distribution; Targets; Sensors; algorithm; article; artificial intelligence; automated pattern recognition; comparative study; computer simulation; data mining; decision support system; methodology; theoretical model; Algorithms; Artificial Intelligence; Computer Simulation; Data Mining; Decision Support Techniques; Models, Theoretical; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84856304820
"Hu W., Almansoori A., Kannan P.K., Azarm S., Wang Z.","Corporate dashboards for integrated business and engineering decisions in oil refineries: An agent-based approach",2012,"Decision Support Systems",13,10.1016/j.dss.2011.11.019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856022279&doi=10.1016%2fj.dss.2011.11.019&partnerID=40&md5=7452ebbe852c0ccb7c5ff3f8e96f7814","It is generally very challenging for an oil refinery to make integrated decisions encompassing multiple functions based on a traditional Decision Support System (DSS), given the complexity and interactions of various decisions. To overcome this limitation, we propose an integrated DSS framework by combining both business and engineering systems with a dashboard. The dashboard serves as a human-computer interface and allows a decision maker to adjust decision variables and exchange information with the DSS. The proposed framework provides a two-stage decision making mechanism based on optimization and agent-based models. Under the proposed DSS, the decision maker decides on the values of a subset of decision variables. These values, or the first-stage decision, are forwarded through the dashboard to the DSS. For the given set of first-stage decision variables, a multi-objective robust optimization problem, based on an integrated business and engineering simulation model, is solved to obtain the values for a set of second-stage decision variables. The two-stage decision making process iterates until a convergence is achieved. A simple oil refinery case study with an example dashboard demonstrates the applicability of the integrated DSS. © 2011 Elsevier B.V. All rights reserved.","Agent-based modeling; Decision Support System (DSS); Knowledge management; Non-linear optimization; Oil markets; Refinery; Simulation","Agent-based modeling; Decision supports; Non-linear optimization; Oil market; Refinery; Simulation; Artificial intelligence; Computational methods; Computer simulation; Decision support systems; Integration; Knowledge management; Multiobjective optimization; Petroleum refineries; Refining; Decision making",Article,Scopus,2-s2.0-84856022279
"El-Yaniv R., Wiener Y.","Active learning via perfect selective classification",2012,"Journal of Machine Learning Research",13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857868393&partnerID=40&md5=0618ad877009acb6171983ac46b0f95a","We discover a strong relation between two known learning models: stream-based active learning and perfect selective classification (an extreme case of 'classification with a reject option'). For these models, restricted to the realizable case, we show a reduction of active learning to selective classification that preserves fast rates. Applying this reduction to recent results for selective classification, we derive exponential target-independent label complexity speedup for actively learning general (non-homogeneous) linear classifiers when the data distribution is an arbitrary high dimensional mixture of Gaussians. Finally, we study the relation between the proposed technique and existing label complexity measures, including teaching dimension and disagreement coefficient. © 2012 Ran El-Yaniv and Yair Wiener.","Active learning; Classification with a reject option; Disagreement coefficient; Exploration vs. exploitation; Perfect classification; Selective classification; Selective sampling; Teaching dimension","Active Learning; Complexity measures; Data distribution; Disagreement coefficient; Exploration vs. exploitation; Fast rate; High-dimensional; Learning models; Linear classifiers; Mixture of Gaussians; Non-homogeneous; Selective sampling; Artificial intelligence; Software engineering",Article,Scopus,2-s2.0-84857868393
"Sánchez J., Benet G., Simó J.E.","Video sensor architecture for surveillance applications",2012,"Sensors",13,10.3390/s120201509,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857573687&doi=10.3390%2fs120201509&partnerID=40&md5=85c67a8d996e7a62b2af6ca9b90473b3","This paper introduces a flexible hardware and software architecture for a smart video sensor. This sensor has been applied in a video surveillance application where some of these video sensors are deployed, constituting the sensory nodes of a distributed surveillance system. In this system, a video sensor node processes images locally in order to extract objects of interest, and classify them. The sensor node reports the processing results to other nodes in the cloud (a user or higher level software) in the form of an XML description. The hardware architecture of each sensor node has been developed using two DSP processors and an FPGA that controls, in a flexible way, the interconnection among processors and the image data flow. The developed node software is based on pluggable components and runs on a provided execution run-time. Some basic and application-specific software components have been developed, in particular: acquisition, segmentation, labeling, tracking, classification and feature extraction. Preliminary results demonstrate that the system can achieve up to 7.5 frames per second in the worst case, and the true positive rates in the classification of objects are better than 80%. © 2012 by the authors; licensee MDPI, Basel, Switzerland.","Computer vision; Distributed smart cameras; Domain-specific architectures; Image processing; Real-time systems; Surveillance systems","article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; computer network; computer program; equipment; equipment design; instrumentation; methodology; organization and management; photography; transducer; videorecording; Artificial Intelligence; Computer Communication Networks; Equipment Design; Equipment Failure Analysis; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Photography; Security Measures; Software; Transducers; Video Recording",Article,Scopus,2-s2.0-84857573687
"Wu J., Shen J., Krug M., Nguang S.K., Li Y.","GA-based nonlinear predictive switching control for a boiler-turbine system",2012,"Journal of Control Theory and Applications",13,10.1007/s11768-012-0050-x,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84555188572&doi=10.1007%2fs11768-012-0050-x&partnerID=40&md5=fcf2d75b904b1341d33982a1c2d228e7","In this paper, the problem of designing a controller for a highly coupled constrained nonlinear boiler-turbine system is addressed with a predictive controller. First, a nonlinear predictive control is implemented by genetic algorithm. Second, to guarantee fast output stabilization, an H-infinity fuzzy state-feedback tracking control is applied with a designed switching principle. The success of such a control structure is based on taking advantage of the optimal input sequence derived from the nonlinear predictive control based on artificial intelligent while ensuring a fast decay of the steady state error. Simulation results of the proposed design are given to illustrate its effectiveness and compared to other control schemes. © 2012 South China University of Technology, Academy of Mathematics and Systems Science, Chinese Academy of Sciences and Springer-Verlag Berlin Heidelberg.","Adaptive control; Boiler-turbine; Genetic algorithm (GA); Nonlinear receding horizon control; Switching control","Adaptive Control; Artificial intelligent; Boiler turbine system; Boiler-turbine; Control schemes; Control structure; H-infinity; Input sequence; Nonlinear predictive control; Nonlinear receding horizon control; Output stabilization; Predictive controller; Predictive switching; Steady state errors; Switching control; Tracking controls; Artificial intelligence; Boiler control; Boilers; Controllers; Genetic algorithms; Predictive control systems; State feedback; Switching systems; Turbine rings; Adaptive control systems",Article,Scopus,2-s2.0-84555188572
"Yorke-Smith N., Saadati S., Myers K.L., Morley D.N.","The design of a proactive personal agent for task management",2012,"International Journal on Artificial Intelligence Tools",13,10.1142/S0218213012500042,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858384935&doi=10.1142%2fS0218213012500042&partnerID=40&md5=f54d905408e84fc20138ee707afd7dec","Personal assistant agents capable of proactively offering assistance can be more helpful to their users through their ability to perform tasks that otherwise would require user involvement. This article characterizes the properties desired of proactive behavior by a personal assistant agent in the realm of task management and develops an operational framework to implement such capabilities. We present an extended agent architectural model that features a meta-level layer charged with identifying potentially helpful actions and determining when it is appropriate to perform them. The reasoning that answers these questions draws on a theory of proactivity that describes user desires and a model of helpfulness. Operationally, assistance patterns represent a compiled form of this knowledge, instantiating meta-reasoning over the agent's beliefs about its user's activities as well as over world state. The resulting generic framework for proactive goal generation and deliberation has been implemented as part of a personal assistant agent in the computer desktop domain. © 2012 World Scientific Publishing Company.","Assistive agents; proactivity; task management","Architectural models; Assistive agents; Generic frameworks; Metareasoning; Personal Agent; Personal assistant agents; Proactive behavior; Proactivity; task management; User involvement; Artificial intelligence",Article,Scopus,2-s2.0-84858384935
"Ignatov D.I., Poelmans J., Dedene G., Viaene S.","A new cross-validation technique to evaluate quality of recommender systems",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-642-27387-2_25,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856052146&doi=10.1007%2f978-3-642-27387-2_25&partnerID=40&md5=6aa88076415ff7c9924ab438b3c83fec","The topic of recommender systems is rapidly gaining interest in the user-behaviour modeling research domain. Over the years, various recommender algorithms based on different mathematical models have been introduced in the literature. Researchers interested in proposing a new recommender model or modifying an existing algorithm should take into account a variety of key performance indicators, such as execution time, recall and precision. Till date and to the best of our knowledge, no general cross-validation scheme to evaluate the performance of recommender algorithms has been developed. To fill this gap we propose an extension of conventional cross-validation. Besides splitting the initial data into training and test subsets, we also split the attribute description of the dataset into a hidden and visible part. We then discuss how such a splitting scheme can be applied in practice. Empirical validation is performed on traditional user-based and item-based recommender algorithms which were applied to the MovieLens dataset. © 2012 Springer-Verlag.","applied combinatorics; quality of recommendations; recommender systems; user-behavior modeling","Combinatorics; Cross validation; Cross-validation technique; Data sets; Empirical validation; Execution time; Key performance indicators; Recall and precision; Recommender algorithms; Research domains; Algorithms; Artificial intelligence; Behavioral research; Benchmarking; Quality control; Recommender systems; Statistical tests; Mathematical models",Conference Paper,Scopus,2-s2.0-84856052146
"Lu H., Fang G., Shao X., Li X.","Segmenting human from photo images based on a coarse-to-fine scheme",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",13,10.1109/TSMCB.2011.2182048,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862825758&doi=10.1109%2fTSMCB.2011.2182048&partnerID=40&md5=6e070d252975d4cd2b4ddd8c56f5ca21","Human segmentation in photo images is a challenging and important problem that finds numerous applications ranging from album making and photo classification to image retrieval. Previous works on human segmentation usually demand a time-consuming training phase for complex shape-matching processes. In this paper, we propose a straightforward framework to automatically recover human bodies from color photos. Employing a coarse-to-fine strategy, we first detect a coarse torso (CT) using the multicue CT detection algorithm and then extract the accurate region of the upper body. Then, an iterative multiple oblique histogram algorithm is presented to accurately recover the lower body based on human kinematics. The performance of our algorithm is evaluated on our own data set (contains 197 images with human body region ground truth data), VOC 2006, and the 2010 data set. Experimental results demonstrate the merits of the proposed method in segmenting a person with various poses. © 2012 IEEE.","Graph cuts; human segmentation; multicue coarse torso detection algorithm (MCTD); multiple oblique histogram (MOH)","Coarse-to-fine; Coarse-to-fine strategy; Color photos; Data sets; Detection algorithm; Graph cut; Ground truth data; Human bodies; Human kinematics; Human segmentation; Lower body; multiple oblique histogram (MOH); Photo classification; Photo images; Training phase; Algorithms; Graphic methods; Image matching; Image retrieval; Signal detection; Image segmentation; algorithm; article; artificial intelligence; automated pattern recognition; biometry; computer assisted diagnosis; human; methodology; photography; whole body imaging; Algorithms; Artificial Intelligence; Biometry; Humans; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Photography; Whole Body Imaging",Article,Scopus,2-s2.0-84862825758
"Ha E.Y., Grafsgaard J.F., Mitchell C.M., Boyer K.E., Lester J.C.","Combining verbal and nonverbal features to overcome the'information gap' in task-oriented dialogue",2012,"SIGDIAL 2012 - 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue, Proceedings of the Conference",13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988005800&partnerID=40&md5=0bf4acce6dbd1dcf2fd7e065110463d7","Dialogue act modeling in task-orienteddialogue poses significant challenges. It isparticularly challenging for corporaconsisting of two interleavedcommunication streams: a dialogue streamand a task stream. In such corpora,information can be conveyed implicitly bythe task stream, yielding a dialogue streamwith seemingly missing information. Apromising approach leverages richresources from both the dialog and the taskstreams, combining verbal and non-verbalfeatures. This paper presents work ondialogue act modeling that leverages bodyposture, which may be indicative ofparticular dialogue acts. Combining threeinformation sources (dialogue exchanges,task context, and users' posture), threetypes of machine learning frameworkswere compared. The results indicate thatsome models better preserve the structureof task-oriented dialogue than others, andthat automatically recognized posturalfeatures may help to disambiguate userdialogue moves. ©2012 Association for Computational Linguistics.",,"Artificial intelligence; Dialogue acts; Information gap; Missing information; Non-verbal features; Task-oriented; Learning systems",Conference Paper,Scopus,2-s2.0-84988005800
"Kersting K.","Lifted probabilistic inference",2012,"Frontiers in Artificial Intelligence and Applications",13,10.3233/978-1-61499-098-7-33,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878778336&doi=10.3233%2f978-1-61499-098-7-33&partnerID=40&md5=253b78bb65aff93beb53f912a538beca","Many AI problems arising in a wide variety of fields such as machine learning, semantic web, network communication, computer vision, and robotics can elegantly be encoded and solved using probabilistic graphical models. Often, however, we are facing inference problems with symmetries and redundancies only implicitly captured in the graph structure and, hence, not exploitable by efficient inference approaches. A prominent example are probabilistic logical models that tackle a long standing goal of AI, namely unifying first-order logic - capturing regularities and symmetries - and probability - capturing uncertainty. Although they often encode large, complex models using few rules only and, hence, symmetries and redundancies abound, inference in them was originally still at the propositional representation level and did not exploit symmetries. This paper is intended to give a (not necessarily complete) overview and invitation to the emerging field of lifted probabilistic inference, inference techniques that exploit these symmetries in graphical models in order to speed up inference, ultimately orders of magnitude. © 2012 The Author(s).",,"Artificial intelligence; Complex networks; Formal logic; Graphic methods; Learning systems; Redundancy; Semantic Web; First order logic; Inference problem; Inference techniques; Network communications; Orders of magnitude; Probabilistic graphical models; Probabilistic inference; Probabilistic-logical models; Computer vision",Conference Paper,Scopus,2-s2.0-84878778336
"Ebrahimzadeh A., Addeh J., Rahmani Z.","Control chart pattern recognition using K-MICA clustering and neural networks",2012,"ISA Transactions",13,10.1016/j.isatra.2011.08.005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-83555166101&doi=10.1016%2fj.isatra.2011.08.005&partnerID=40&md5=e15e697f3f87822706fed0a43a066edb","Automatic recognition of abnormal patterns in control charts has seen increasing demands nowadays in manufacturing processes. This paper presents a novel hybrid intelligent method (HIM) for recognition of the common types of control chart pattern (CCP). The proposed method includes two main modules: a clustering module and a classifier module. In the clustering module, the input data is first clustered by a new technique. This technique is a suitable combination of the modified imperialist competitive algorithm (MICA) and the K-means algorithm. Then the Euclidean distance of each pattern is computed from the determined clusters. The classifier module determines the membership of the patterns using the computed distance. In this module, several neural networks, such as the multilayer perceptron, probabilistic neural networks, and the radial basis function neural networks, are investigated. Using the experimental study, we choose the best classifier in order to recognize the CCPs. Simulation results show that a high recognition accuracy, about 99.65%, is achieved. © 2011 ISA. Published by Elsevier Ltd. All rights reserved.","Clustering; Control chart patterns; K-means algorithm; Modified imperialist competitive algorithm; Neural networks","Abnormal patterns; Automatic recognition; Clustering; Competitive algorithms; Control chart pattern; Control chart patterns; Euclidean distance; Experimental studies; Hybrid intelligent method; In-control; Input datas; k-Means algorithm; Main module; Manufacturing process; Multi layer perceptron; Probabilistic neural networks; Radial basis function neural networks; Recognition accuracy; Face recognition; Flowcharting; Mica; Multilayer neural networks; Neural networks; Radial basis function networks; Clustering algorithms; algorithm; article; artificial intelligence; artificial neural network; automated pattern recognition; automation; classification; cluster analysis; computer simulation; computer system; industry; instrumentation; randomization; statistical model; statistics; Algorithms; Artificial Intelligence; Automation; Classification; Cluster Analysis; Computer Simulation; Computer Systems; Industry; Models, Statistical; Neural Networks (Computer); Pattern Recognition, Automated; Random Allocation; Stochastic Processes",Article,Scopus,2-s2.0-83555166101
"Ong M.E., Lee Ng C.H., Goh K., Liu N., Koh Z.X., Shahidah N., Zhang T.T., Fook-Chong S., Lin Z.","Prediction of cardiac arrest in critically ill patients presenting to the emergency department using a machine learning score incorporating heart rate variability compared with the modified early warning score",2012,"Critical care (London, England)",13,10.1186/cc11396,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933037928&doi=10.1186%2fcc11396&partnerID=40&md5=0341414282297b1168fe9485ca9c2f44","INTRODUCTION: A key aim of triage is to identify those with high risk of cardiac arrest, as they require intensive monitoring, resuscitation facilities, and early intervention. We aim to validate a novel machine learning (ML) score incorporating heart rate variability (HRV) for triage of critically ill patients presenting to the emergency department by comparing the area under the curve, sensitivity and specificity with the modified early warning score (MEWS).METHODS: We conducted a prospective observational study of critically ill patients (Patient Acuity Category Scale 1 and 2) in an emergency department of a tertiary hospital. At presentation, HRV parameters generated from a 5-minute electrocardiogram recording are incorporated with age and vital signs to generate the ML score for each patient. The patients are then followed up for outcomes of cardiac arrest or death.RESULTS: From June 2006 to June 2008 we enrolled 925 patients. The area under the receiver operating characteristic curve (AUROC) for ML scores in predicting cardiac arrest within 72 hours is 0.781, compared with 0.680 for MEWS (difference in AUROC: 0.101, 95% confidence interval: 0.006 to 0.197). As for in-hospital death, the area under the curve for ML score is 0.741, compared with 0.693 for MEWS (difference in AUROC: 0.048, 95% confidence interval: -0.023 to 0.119). A cutoff ML score ≥ 60 predicted cardiac arrest with a sensitivity of 84.1%, specificity of 72.3% and negative predictive value of 98.8%. A cutoff MEWS ≥ 3 predicted cardiac arrest with a sensitivity of 74.4%, specificity of 54.2% and negative predictive value of 97.8%.CONCLUSION: We found ML scores to be more accurate than the MEWS in predicting cardiac arrest within 72 hours. There is potential to develop bedside devices for risk stratification based on cardiac arrest prediction.",,"aged; artificial intelligence; cohort analysis; comparative study; critical illness; emergency health service; female; heart arrest; heart rate; human; male; middle aged; pathophysiology; physiology; predictive value; prospective study; severity of illness index; standards; Aged; Artificial Intelligence; Cohort Studies; Critical Illness; Emergency Service, Hospital; Female; Heart Arrest; Heart Rate; Humans; Male; Middle Aged; Predictive Value of Tests; Prospective Studies; Severity of Illness Index",Article,Scopus,2-s2.0-84933037928
"Stulp F., Fedrizzi A., Mösenlechner L., Beetz M.","Learning and reasoning with action-related places for robust mobile manipulation",2012,"Journal of Artificial Intelligence Research",13,10.1613/jair.3451,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862673736&doi=10.1613%2fjair.3451&partnerID=40&md5=4d6d26006edc662bd26ac51ed8185bcc","We propose the concept of Action-Related Place (ARPlace) as a powerful and exible representation of task-related place in the context of mobile manipulation. ARPlace represents robot base locations not as a single position, but rather as a collection of positions, each with an associated probability that the manipulation action will succeed when located there. ARPlaces are generated using a predictive model that is acquired through experience-based learning, and take into account the uncertainty the robot has about its own location and the location of the object to be manipulated. When executing the task, rather than choosing one specic goal position based only on the initial knowledge about the task context, the robot instantiates an ARPlace, and bases its decisions on this ARPlace, which is updated as new information about the task becomes available. To show the advantages of this least-commitment approach, we present a transformational planner that reasons about ARPlaces in order to optimize symbolic plans. Our empirical evaluation demonstrates that using ARPlaces leads to more robust and ecient mobile manipulation in the face of state estimation uncertainty on our simulated robot. © 2012 AI Access Foundation.",,"Empirical evaluations; Estimation uncertainties; Experience-based learning; Mobile manipulation; Predictive models; Simulated robot; Artificial intelligence; Robots",Article,Scopus,2-s2.0-84862673736
"Gao Y., Liao S., Shen D.","Prostate segmentation by sparse representation based classification",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872929584&partnerID=40&md5=eedbf611661136644aa62987e6e80078","Accurate segmentation of prostate in CT images is important in image-guided radiotherapy. However, it is difficult to localize the prostate in CT images due to low image contrast, unpredicted motion and large appearance variations across different treatment days. To address these issues, we propose a sparse representation based classification method to accurately segment the prostate. The main contributions of this paper are: (1) A discriminant dictionary learning technique is proposed to overcome the limitation of the traditional Sparse Representation based Classifier (SRC). (2) Context features are incorporated into SRC to refine the prostate boundary in an iterative scheme. (3) A residue-based linear regression model is trained to increase the classification performance of SRC and extend it from hard classification to soft classification. To segment the prostate, the new treatment image is first rigidly aligned to the planning image space based on the pelvic bones. Then two sets of location-adaptive SRCs along two coordinate directions are applied on the aligned treatment image to produce a probability map, based on which all previously segmented images of the same patient are rigidly aligned onto the new treatment image and majority voting strategy is further adopted to finally segment the prostate in the new treatment image. The proposed method has been evaluated on a CT dataset consisting of 15 patients and 230 CT images. Promising results have been achieved. © Springer-Verlag Berlin Heidelberg 2012.",,"Image segmentation; Iterative methods; Linear regression; Medical computing; Medical imaging; Patient treatment; Regression analysis; Rigidity; Urology; Classification performance; Different treatments; Image guided radiotherapy; Linear regression models; Prostate segmentation; Soft classification; Sparse representation; Sparse representation based classifications; Computerized tomography; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; computer assisted tomography; human; image quality; male; methodology; prostate tumor; radiography; Algorithms; Artificial Intelligence; Humans; Male; Pattern Recognition, Automated; Prostatic Neoplasms; Radiographic Image Enhancement; Radiographic Image Interpretation, Computer-Assisted; Tomography, X-Ray Computed",Conference Paper,Scopus,2-s2.0-84872929584
"Barbieri G., Pachet F., Roy P., Esposti M.D.","Markov constraints for generating lyrics with style",2012,"Frontiers in Artificial Intelligence and Applications",13,10.3233/978-1-61499-098-7-115,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878770751&doi=10.3233%2f978-1-61499-098-7-115&partnerID=40&md5=9111b657fd32bd33c7f614b61579738d","We address the issue of generating texts in the style of an existing author, that also satisfy structural constraints imposed by the genre of the text. We focus on song lyrics, for which structural constraints are well-defined: rhyme and meter. Although Markov processes are known to be suitable for representing style, they are difficult to control in order to satisfy non-local properties, such as structural constraints, that require long distance modeling. We show that the framework of Constrained Markov Processes allows us to precisely generate texts that are consistent with a corpus, while being controllable in terms of rhymes and meter, a result that no other technique, to our knowledge, could achieve to date. Controlled Markov processes consist in reformulating Markov processes in the context of constraint satisfaction. We describe how to represent stylistic and structural properties in terms of constraints in this framework and we provide an evaluation of our method by comparing it to both pure Markov and pure constraint-based approaches. We show how this approach can be used for the semi-automatic generation of lyrics in the style of a popular author that has the same structure as an existing song. © 2012 The Author(s).",,"Artificial intelligence; Markov processes; Constraint Satisfaction; Constraint-based; Controlled Markov process; Nonlocal; Semi-automatic generation; Structural constraints; Process control",Conference Paper,Scopus,2-s2.0-84878770751
"Cruz-Roa A., González F., Galaro J., Judkins A.R., Ellison D., Baccon J., Madabhushi A., Romero E.","A visual latent semantic approach for automatic analysis and interpretation of anaplastic medulloblastoma virtual slides",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872539491&partnerID=40&md5=4f4eb2287eb3ea5fea426af4b50af0f4","A method for automatic analysis and interpretation of histopathology images is presented. The method uses a representation of the image data set based on bag of features histograms built from visual dictionary of Haar-based patches and a novel visual latent semantic strategy for characterizing the visual content of a set of images. One important contribution of the method is the provision of an interpretability layer, which is able to explain a particular classification by visually mapping the most important visual patterns associated with such classification. The method was evaluated on a challenging problem involving automated discrimination of medulloblastoma tumors based on image derived attributes from whole slide images as anaplastic or non-anaplastic. The data set comprised 10 labeled histopathological patient studies, 5 for anaplastic and 5 for non-anaplastic, where 750 square images cropped randomly from cancerous region from whole slide per study. The experimental results show that the new method is competitive in terms of classification accuracy achieving 0.87 in average. © Springer-Verlag Berlin Heidelberg 2012.",,"Brain; Medical computing; Semantics; Automatic analysis; Bag of features; Classification accuracy; Interpretability; Latent semantics; Medulloblastoma; Visual dictionaries; Whole slide images; Medical imaging; algorithm; article; artificial intelligence; automated pattern recognition; automation; cerebellum tumor; computer assisted diagnosis; computer program; factual database; human; image processing; medulloblastoma; methodology; pathology; probability; reproducibility; statistical model; Algorithms; Artificial Intelligence; Automation; Cerebellar Neoplasms; Databases, Factual; Humans; Image Interpretation, Computer-Assisted; Image Processing, Computer-Assisted; Medulloblastoma; Models, Statistical; Pattern Recognition, Automated; Probability; Reproducibility of Results; Software",Conference Paper,Scopus,2-s2.0-84872539491
"Guttman J.D.","State and progress in strand spaces: Proving fair exchange",2012,"Journal of Automated Reasoning",13,10.1007/s10817-010-9202-1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893028963&doi=10.1007%2fs10817-010-9202-1&partnerID=40&md5=9995373564cdf4d3d1931752ae654aae","Many cryptographic protocols are intended to coordinate state changes among principals. Exchange protocols, for instance, coordinate delivery of new values to the participants, i.e. additions to the set of values they possess. An exchange protocol is fair if it ensures that delivery of new values is balanced: If one participant obtains a new possession via the protocol, then all other participants will, too. Understanding this balanced coordination of different principals in a distributed system re'uires relating (long-term) state to (short-term) protocol activities. Fair exchange also re'uires progress assumptions. In this paper we adapt the strand space framework to protocols, such as fair exchange, that coordinate state changes. We regard the state as a multiset of facts, and we allow protocol actions to cause local changes in this state via multiset rewriting. Second, progress assumptions stipulate that some channels are resilient-and guaranteed to deliver messages-and some principals will not stop at critical steps. Our proofs of correctness cleanly separate protocol properties, such as authentication and confidentiality, from properties about progress and state evolution. G. Wang's recent fair exchange protocol illustrates the approach. © Springer Science+Business Media B.V. 2010.","Cryptographic protocol analysis; Fair exchange; Strand spaces; Verification","Automata theory; Software engineering; Verification; Cryptographic protocols; Distributed systems; Exchange protocols; Fair exchange; Fair-exchange protocols; Multiset rewriting; State evolutions; Strand space; Artificial intelligence",Article,Scopus,2-s2.0-84893028963
"Artale A., Ryzhikov V., Kontchakov R.","DL-lite with attributes and datatypes",2012,"Frontiers in Artificial Intelligence and Applications",13,10.3233/978-1-61499-098-7-61,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878800444&doi=10.3233%2f978-1-61499-098-7-61&partnerID=40&md5=189e0149c5ab7db78a047159b1005d17","We extend the DL-Lite languages by means of attributes and datatypes. Attributes-a notion borrowed from data models-associate concrete values from datatypes to abstract objects and in this way complement roles, which describe relationships between abstract objects. The extended languages remain tractable (with a notable exception) even though they contain both existential and (a limited form of) universal quantification. We present complexity results for two most important reasoning problems in DL-Lite: combined complexity of knowledge base satisfiability and data complexity of positive existential query answering. © 2012 The Author(s).",,"Artificial intelligence; Computational linguistics; Knowledge based systems; Abstract object; Combined complexity; Complexity results; Data complexity; Extended languages; Knowledge base; Query answering; Reasoning problems; Formal logic",Conference Paper,Scopus,2-s2.0-84878800444
"Chandrasekaran S., Stojcevski A., Littlefair G., Joordens M.","Learning through projects in engineering education",2012,"Proceedings of the 40th SEFI Annual Conference 2012 - Engineering Education 2020: Meet the Future",13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939545361&partnerID=40&md5=bb16b24c620597f1a3e5c286db846a01","This paper explores different styles of project work carried out in engineering curriculum and presents some novel frameworks and ideas used for learning through projects in engineering education. Projects can influence an engineering curriculum in various ways. This can be done at a course level and/or program level. Educators, students and industry clients are approaching project work as a key for social, academic and industrial partnership. In most Australian universities, in the final year, the students complete an individual project involving the application of skills and knowledge attained during their earlier years of their degree program. Through these projects students develop new abilities for application to a real-world problem, learn the art of modeling and simulation, design, development and management of an industry based or research based projects. Through project based learning teachers, students, and employers undertake different activities with varying purposes. This paper firstly presents data collected from final year undergraduate engineering units in all different engineering programs across the country. The findings from this research illustrate how students in various disciplines perform projects and as important shows the alignment or non-alignment of delivery, learning outcomes, and assessment activities. The types of projects available to students in the universities are also presented. Based on the findings, the research suggests that styles of teaching and learning environments can be adapted to the student's learning mode that could meet the requirements of our society at present. Based on this, the paper presents some novel frameworks in curriculum development to assist with constructive alignment when an engineering curriculum is based on learning through projects. This is then mapped to the local context in engineering at Deakin University Australia, which looks at the learning through projects theme in specially designed engineering learning spaces. Learning through projects has a positive effect on student content knowledge and the development of skills such as collaboration, critical thinking, and problem solving which increase their motivation and engagement. It is challenging task of teaching and teachers finding hard to implement the system, to integrate technology into projects in meaningful ways. When we look at the method of learning through Projects, it is a benefit for all the stakeholders such as students, industry, community, university involved in it. It provides us a framework for embedding experiential and rich learning activities, integrated with discipline-based curriculum that improves employment and career outcomes. The benefits of Project based learning include enhanced students' participation in the learning process (active learning and self-learning), enhanced communication skills, addressing of a wider set of learning styles, and promotion of critical and proactive thinking.","Engineering education; Engineering projects; Project based learning","Application programs; Artificial intelligence; Computer aided instruction; Curricula; Education; Problem solving; Project management; Research and development management; Students; Teaching; Constructive alignments; Curriculum development; Engineering project; Industrial partnerships; Motivation and engagements; Project based learning; Teaching and learning environments; Undergraduate engineering; Engineering education",Conference Paper,Scopus,2-s2.0-84939545361
"Caballero J., Rueckert D., Hajnal J.V.","Dictionary learning and time sparsity in dynamic MRI",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872529331&partnerID=40&md5=eebb2ddf28ad8edf255601eda6e8a760","Sparse representation methods have been shown to tackle adequately the inherent speed limits of magnetic resonance imaging (MRI) acquisition. Recently, learning-based techniques have been used to further accelerate the acquisition of 2D MRI. The extension of such algorithms to dynamic MRI (dMRI) requires careful examination of the signal sparsity distribution among the different dimensions of the data. Notably, the potential of temporal gradient (TG) sparsity in dMRI has not yet been explored. In this paper, a novel method for the acceleration of cardiac dMRI is presented which investigates the potential benefits of enforcing sparsity constraints on patch-based learned dictionaries and TG at the same time. We show that an algorithm exploiting sparsity on these two domains can outperform previous sparse reconstruction techniques. © Springer-Verlag Berlin Heidelberg 2012.",,"Education; Medical computing; Medical imaging; Dictionary learning; Learned dictionaries; Potential benefits; Sparse reconstruction; Sparse representation; Sparsity constraints; Speed limit; Temporal gradients; Magnetic resonance imaging; algorithm; article; artificial intelligence; automated pattern recognition; computer simulation; human; image processing; image quality; methodology; nuclear magnetic resonance imaging; signal processing; statistical model; theoretical model; time; Algorithms; Artificial Intelligence; Computer Simulation; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Models, Statistical; Models, Theoretical; Pattern Recognition, Automated; Phantoms, Imaging; Signal Processing, Computer-Assisted; Time Factors",Conference Paper,Scopus,2-s2.0-84872529331
"Azadeh A., Ziaei B., Moghaddam M.","A hybrid fuzzy regression-fuzzy cognitive map algorithm for forecasting and optimization of housing market fluctuations",2012,"Expert Systems with Applications",13,10.1016/j.eswa.2011.07.020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-81855218665&doi=10.1016%2fj.eswa.2011.07.020&partnerID=40&md5=3edf683331e0ef2d712fc5c5e9b4552c","This paper presents a hybrid algorithm based on fuzzy linear regression (FLR) and fuzzy cognitive map (FCM) to deal with the problem of forecasting and optimization of housing market fluctuations. Due to the uncertainty and severe noise associated with the housing market, the application of crisp data for forecasting and optimization purposes is insufficient. Hence, in order to enable the decision-makers to make decisions with respect to imprecise/fuzzy data, FLR is used in the proposed hybrid algorithm. The best-fitted FLR model is then selected with respect to two indicators including Index of Confidence (IC) and Mean Absolute Percentage Error (MAPE). To achieve this objective, analysis of variance (ANOVA) for a randomized complete block design (RCBD) is employed. The primary objective of this study is to utilize imprecise/fuzzy data in order to improve the analysis of housing price fluctuations, in accordance with the factors obtained through the best-fitted FLR model. The secondary objective of this study is the exhibition of the resulted values in a schematic way via FCM. Hybridization of FLR and FCM provides a decision support system (DSS) for utilization of historical data to predict housing market fluctuation in the future and identify the influence of the other parameters. The proposed hybrid FLR-FCM algorithm enables the decision-makers to utilize imprecise and ambiguous data and represent the resulted values of the model more clearly. This is the first study that utilizes a hybrid intelligent approach for housing price and market forecasting and optimization. © 2011 Elsevier Ltd. All rights reserved.","Forecasting; Fuzzy cognitive map; Fuzzy linear regression; Housing market fluctuations; Housing price; Optimization","Ambiguous Data; Block designs; Cognitive maps; Crisp data; Decision makers; Fuzzy cognitive map; Fuzzy linear regression; Historical data; Housing market fluctuations; Housing markets; Housing price; Housing prices; Hybrid algorithms; Hybrid intelligent approach; Market forecasting; Mean absolute percentage error; Primary objective; Algorithms; Artificial intelligence; Commerce; Decision support systems; Forecasting; Fuzzy rules; Fuzzy systems; Housing; Optimization; Regression analysis; Analysis of variance (ANOVA)",Article,Scopus,2-s2.0-81855218665
"Arcangeli J.-P., Bouzeghoub A., Camps V., Canut M.-F., Chabridon S., Conan D., Desprats T., Laborde R., Lavinal E., Leriche S., Maurel H., Péninou A., Taconet C., Zaraté P.","INCOME - Multi-scale context management for the internet of things",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84895064956&partnerID=40&md5=994e9efaab7473e04919adf43ffe62a6","Nowadays, context management solutions in ambient networks are well-known. However, with the IoT paradigm, ambient information is not anymore the only source of context. Context management solutions able to address multiple network scales ranging from ambient networks to the Internet of Things (IoT) are required. We present the INCOME project whose goal is to provide generic software and middleware components to ease the design and development of mass market context-aware applications built above the Internet of Things. By revisiting ambient intelligence (AmI) context management solutions for extending them to the IoT, INCOME allows to bridge the gap between these two very active research domains. In this landscape paper, we identify how INCOME plans to advance the state of the art and we briefly describe its scientific program which consists of three main tasks: (i) multi-scale context management, (ii) management of extrafunctional concerns (quality of context and privacy), and (iii) autonomous deployment of context management entities. © Springer-Verlag Berlin Heidelberg 2012.",,"Application programs; Artificial intelligence; Internet of things; Middleware; Ambient information; Context aware applications; Context management; Design and Development; Internet of thing (IOT); Middleware components; Quality of contexts; Scientific programs; Ambient intelligence",Conference Paper,Scopus,2-s2.0-84895064956
"Gomes J., Urbano P., Christensen A.L.","Progressive minimal criteria novelty search",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-642-34654-5_29,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906653157&doi=10.1007%2f978-3-642-34654-5_29&partnerID=40&md5=38d58b48618fce7510c26bd8f65745b3","We propose progressive minimal criteria novelty search (PMCNS), which is an extension of minimal criteria novelty search. In PMCNS, we combine the respective benefits of novelty search and fitness-based evolution by letting novelty search freely explore new regions of behaviour space as long as the solutions meet a progressively stricter fitness criterion. We evaluate the performance of our approach in the evolution of neurocontrollers for a swarm of robots in a coordination task where robots must share a single charging station. The robots can only survive by periodically recharging their batteries. We compare the performance of PMCNS with (i) minimal criteria novelty search, (ii) pure novelty search, (iii) pure fitness-based evolution, and (iv) with evolutionary search based on a linear blend of novelty and fitness. Our results show that PMCNS outperforms all four approaches. Finally, we analyse how different parameter setting in PMCNS influence the exploration of the behaviour space. © Springer-Verlag Berlin Heidelberg 2012.","Deception; Evolutionary swarm robotics; Novelty search","Artificial intelligence; Charging (batteries); Evolutionary algorithms; Robots; Charging station; Coordination tasks; Deception; Evolutionary search; Neuro controllers; Novelty search; Parameter setting; Swarm robotics; Health",Conference Paper,Scopus,2-s2.0-84906653157
"Lin Y.-L., Chen W.-L.","Fast search strategies for fractal image compression",2012,"Journal of Information Science and Engineering",13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862934589&partnerID=40&md5=e04c84afc446c4f8150ea1ae238aaa0b","In traditional fractal image compression, the encoding procedure is time-consuming due to the full search mechanism. In order to speedup the encoder, we adopt particle swarm optimization method performed under classification and Dihedral transformation to further decrease the amount of MSE computations. The classifier partitions all of the blocks in domain pool and range pool into three classes according to the third level wavelet coefficients. Each range block searches the most similar block only from the blocks of the same class. Furthermore, according to the property of Dihedral transformation, only four transformations for each domain block are considered so as to reduce the encoding time. Experimental results show that, the encoding time of the proposed method is faster than that of the full search method. Experimental results show that the proposed method is about 178 times faster with only 1.46 dB decay in image quality.","Classification; Dihedral transformation; Discrete wavelet transform; Fractal image compression; Particle swarm optimization","Dihedral transformation; Discrete wavelets; Domain block; Encoding time; Fast search; Fractal image compression; Full search; Full search methods; Particle swarm; Particle swarm optimization method; Range block; Third level; Wavelet coefficients; Artificial intelligence; Classification (of information); Discrete wavelet transforms; Encoding (symbols); Fractals; Image quality; Lakes; Particle swarm optimization (PSO); Image compression",Article,Scopus,2-s2.0-84862934589
"Yang Y., Cheng N., Zhang M.","Research on activity recognition method based on human motion trajectory features",2012,"Journal of Convergence Information Technology",13,10.4156/jcit.vol7.issue1.10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863080166&doi=10.4156%2fjcit.vol7.issue1.10&partnerID=40&md5=602abf8be17cb04436f1dd9133801fe5","Human activity recognition identified has a large number of applications in the field of intelligent control, human-computer interaction, virtual reality etc. In recent years, it has attracted a wide interest in the study. The information expressed by body posture and movement is the common means for human interaction. Human behavior can also clearly reflect the human's intentions. Machine vision-based recognition method is a manifestation of advanced machine intelligence. It is able to capture and analysis video and image, tracking the movement of various parts of the human body, in order to understand human action and behavior. A vision-based activity recognition technology is described in this thesis. By using static color space model, skin color is extracted and human head and hands' motion trajectories are tracked. After calculating the discrete direction vector information, uses HMM to train dynamic trajectory model, and identifies the activities through matching the database.","Activity recognition; HMM; Trajectory tracking","Activity recognition; Body postures; Color space; Direction vector; HMM; Human actions; Human activity recognition; Human behaviors; Human bodies; Human head; Human interactions; Human motions; Machine intelligence; Motion trajectories; Recognition methods; Skin color; Train dynamics; Trajectory tracking; Vision based; Artificial intelligence; Computer vision; Motion estimation; Trajectories; Virtual reality; Behavioral research",Article,Scopus,2-s2.0-84863080166
"Cervante L., Xue B., Shang L., Zhang M.","A dimension reduction approach to classification based on particle swarm optimisation and rough set theory",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-35101-3_27,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871389617&doi=10.1007%2f978-3-642-35101-3_27&partnerID=40&md5=5407e436fb62e22b8df8364212b6e3bc","Dimension reduction aims to remove unnecessary attributes from datasets to overcome the problem of ""the curse of dimensionality"", which is an obstacle in classification. Based on the analysis of the limitations of the standard rough set theory, we propose a new dimension reduction approach based on binary particle swarm optimisation (BPSO) and probabilistic rough set theory. The new approach includes two new specific algorithms, which are PSOPRS using only the probabilistic rough set in the fitness function and PSOPRSN adding the number of attributes in the fitness function. Decision trees, naive Bayes and nearest neighbour algorithms are employed to evaluate the classification accuracy of the reduct achieved by the proposed algorithms on five datasets. Experimental results show that the two new algorithms outperform the algorithm using BPSO with standard rough set and two traditional dimension reduction algorithms. PSOPRSN obtains a smaller number of attributes than PSOPRS with the same or slightly worse classification performance. This work represents the first study on probabilistic rough set for for filter dimension reduction in classification problems. © 2012 Springer-Verlag.","Classification; Dimension reduction; Filter Approaches; Particle Swarm Optimisation","Binary particle swarm; Classification accuracy; Classification performance; Curse of dimensionality; Data sets; Dimension reduction; Dimension reduction algorithm; Filter approach; Filter dimensions; Fitness functions; Naive bayes; Nearest neighbour; New dimensions; Particle swarm optimisation; Rough set; Algorithms; Artificial intelligence; Data processing; Decision trees; Particle swarm optimization (PSO); Rough set theory; Classification (of information)",Conference Paper,Scopus,2-s2.0-84871389617
"Xiong W., Luo X., Ma W.","Games with ambiguous payoffs and played by ambiguity and regret minimising players",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-35101-3_35,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871370800&doi=10.1007%2f978-3-642-35101-3_35&partnerID=40&md5=3784b5c239d1e4b6f6c9e85c9280653e","In real life games, a player's belief about the consequence of a strategy is often ambiguous due to out-of-control factors in the environment where the games are played. However, existing work cannot handle this situation. To address the issue, we introduce a new kind of games, called ambiguous games, and incorporate human cognitive factors of ambiguity aversion and minimising regret to propose a concept of solution to such a game. Moreover, we also study how ambiguity degrees of belief about payoffs impact the outcomes of a game, and find the condition under which a player should release more or less ambiguous information to his opponents. © 2012 Springer-Verlag.",,"Ambiguity aversion; Cognitive factors; Out-of-control; Real-life games; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84871370800
"Boril J., Jalovecky R.","Experimental identification of pilot response using measured data from a flight simulator",2012,"IFIP Advances in Information and Communication Technology",12,10.1007/978-3-642-33409-2_14,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870788954&doi=10.1007%2f978-3-642-33409-2_14&partnerID=40&md5=820649511ed16cf53715c247e84c22fa","This paper describes the measuring of pilot response time to a sudden change in a controlled parameter whilst flying an aircraft. The authors of this paper created an analytical model of human behavior from the basic data of an automated regulation. The measurements have been done on a Cessna 152 simulator at the University of Hertfordshire, Hatfield. The tested pilots were pilot students with several tens of flight hours in real planes. The pilot's response to a sudden aircraft altitude change was measured. For analysis of the measured results a mathematical identification model in MATLAB® environment was used. The results obtained from MATLAB® confirm that the experimental measurements were successful. © 2012 IFIP International Federation for Information Processing.","Aircraft Control; Human Behavior Model; MATLAB®; Parameter Identification; Pilot Response; X-Plane","Controlled parameter; Experimental identification; Experimental measurements; Flight hour; Hertfordshire; Human behavior models; Human behaviors; Mathematical identifications; Measured results; Pilot Response; Sudden change; X-Plane; Aircraft control; Artificial intelligence; Flight simulators; Identification (control systems); Social sciences",Conference Paper,Scopus,2-s2.0-84870788954
"Abdelaziz A.Y., Mekhamer S.F., Ezzat M., El-Saadany E.F.","Line outage detection using support Vector Machine (SVM) based on the Phasor Measurement Units (PMUs) technology",2012,"IEEE Power and Energy Society General Meeting",12,10.1109/PESGM.2012.6345116,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870591556&doi=10.1109%2fPESGM.2012.6345116&partnerID=40&md5=e698a6847ce7b7b3f5106b7b64be9667","Phasor Measurement Units (PMUs) have been increasingly widespread throughout the power network. As a result, several researches have been made to locate the PMUs for complete system observability. Many protection applications are based upon the PMUs locations. This paper introduces an important application in power system protection which is the detection of single line outage. In addition, a detection of the outaged line is achieved depending on the variations of phase angles measured at the system buses where the PMUs are located. Hence, a protection scheme from unexpected overloading in the network that may lead to system collapse can be achieved. Such detections are based upon an artificial intelligence technique which is the support Vector Machine (SVM) classification tool. To demonstrate the effectiveness of the proposed approach, the algorithm is tested using offline simulation for the 14-bus IEEE system. © 2012 IEEE.","Phasor measurement units; PSCAD; Support vector machines; Transmission line measurements","Artificial intelligence techniques; Classification tool; Complete system; Line outage; Off-line simulations; Phase angles; Phasor measurement unit; Phasor measurement unit (PMUs); Power networks; Power system protection; Protection application; Protection schemes; PSCAD; System collapse; Transmission-line measurements; Electric power system interconnection; Support vector machines",Conference Paper,Scopus,2-s2.0-84870591556
"Vyas V.K., Ghate M.","CoMFA and CoMSIA studies on aryl carboxylic acid amide derivatives as dihydroorotate dehydrogenase (DHODH) inhibitors vivek k. vyas",2012,"Current Computer-Aided Drug Design",12,10.2174/157340912803519598,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870210536&doi=10.2174%2f157340912803519598&partnerID=40&md5=39e7605df5380473691aec02dcc3dfd5","DHODH is a flavoenzyme that catalyzes the oxidation of dihydroorotate (DHO) to orotate (ORO) as part of the fourth and rate limiting step of the de novo pyrimidine biosynthetic pathway. Inhibitors of DHODHs have proven efficacy for the treatment of cancer, malaria and immunological disorders. 3D QSAR studies on some aryl carboxylic acid amide derivatives as hDHODH inhibitors were performed by comparative molecular field analysis (CoMFA) and comparative molecular similarity indices (CoMSIA) methods to rationalize the structural requirements responsible for the inhibitory activity of these compounds. The alignment strategy was used for these compounds by means of Distill function defined in SYBYL X 1.2. The best CoMFA and CoMSIA models obtained for the training set were statistically significant with cross-validated coefficients (q2) of 0.636 and 0.604 and conventional coefficients (r2) of 0.993 and 0.950, respectively. Both the models were validated by an external test set of five compounds giving satisfactory prediction (r2 pred) of 0.563 and 0.523 for CoMFA and CoMSIA models, respectively. Further the robustness of the model was verified by bootstrapping analysis. Generated CoMFA and CoMSIA models provide useful information for the design of novel inhibitors with good hDHODH inhibitory. © 2012 Bentham Science Publishers.","3D QSAR; Aryl carboxylic acid amide derivatives; CoMFA; COMSIA; HDHODH inhibitors","dihydroorotate dehydrogenase inhibitor; article; bootstrapping; chemical reaction; comparative molecular field analysis; comparative molecular similarity indices analysis; drug conformation; drug design; drug structure; hydrogen bond; hydrophobicity; IC 50; interferometry; lipophilicity; partial least squares regression; predictive value; quantitative structure activity relation; static electricity; stereospecificity; Amides; Antimalarials; Antineoplastic Agents; Artificial Intelligence; Carboxylic Acids; Computational Biology; Drug Design; Enzyme Inhibitors; Heterocyclic Compounds; Humans; Kinetics; Ligands; Models, Biological; Models, Molecular; Molecular Conformation; Oxidoreductases; Quantitative Structure-Activity Relationship; Recombinant Proteins; Statistics as Topic; Stereoisomerism",Article,Scopus,2-s2.0-84870210536
"Zhang S., Li C., Zhang S., Zhang H., Pang L., Lam K., Hui C.","Using the K-nearest neighbor algorithm for the classification of lymph node metastasis in gastric cancer",2012,"Computational and Mathematical Methods in Medicine",12,10.1155/2012/876545,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870199899&doi=10.1155%2f2012%2f876545&partnerID=40&md5=8190cd0dcca0f077a5e0f1f7f0a1ae80","Accurate tumor, node, and metastasis (TNM) staging, especially N staging in gastric cancer or the metastasis on lymph node diagnosis, is a popular issue in clinical medical image analysis in which gemstone spectral imaging (GSI) can provide more information to doctors than conventional computed tomography (CT) does. In this paper, we apply machine learning methods on the GSI analysis of lymph node metastasis in gastric cancer. First, we use some feature selection or metric learning methods to reduce data dimension and feature space. We then employ the K-nearest neighbor classifier to distinguish lymph node metastasis from nonlymph node metastasis. The experiment involved 38 lymph node samples in gastric cancer, showing an overall accuracy of 96.33. Compared with that of traditional diagnostic methods, such as helical CT (sensitivity 75.2 and specificity 41.8) and multidetector computed tomography (82.09), the diagnostic accuracy of lymph node metastasis is high. GSI-CT can then be the optimal choice for the preoperative diagnosis of patients with gastric cancer in the N staging. © 2012 Chao Li et al.",,"nonionic contrast medium; article; cancer classification; classification algorithm; clinical article; computer assisted tomography; controlled study; diagnostic accuracy; diagnostic test accuracy study; differential diagnosis; gemstone spectral imaging; human; image analysis; image processing; lymph node metastasis; machine learning; multidetector computed tomography; sensitivity and specificity; spiral computer assisted tomography; stomach cancer; algorithm; artificial intelligence; computer assisted diagnosis; computer assisted tomography; diagnostic imaging; lymph node; lymph node metastasis; methodology; pathology; photon; radiography; reproducibility; statistical model; stomach tumor; X ray; Algorithms; Artificial Intelligence; Diagnostic Imaging; Humans; Lymph Nodes; Lymphatic Metastasis; Models, Statistical; Photons; Radiographic Image Interpretation, Computer-Assisted; Reproducibility of Results; Sensitivity and Specificity; Stomach Neoplasms; Tomography, X-Ray Computed; X-Rays",Article,Scopus,2-s2.0-84870199899
"Wang C.-J., Luo J.-F.","A key-policy attribute-based encryption scheme with constant size ciphertext",2012,"Proceedings of the 2012 8th International Conference on Computational Intelligence and Security, CIS 2012",12,10.1109/CIS.2012.106,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873560180&doi=10.1109%2fCIS.2012.106&partnerID=40&md5=53b01f9987d31dec9672750b15936d08","Attribute-based encryption (ABE) is a new cryptographic primitive which provides a promising tool for addressing the problem of secure and fine-grained data sharing and decentralized access control. Key-policy attribute-based encryption (KP-ABE) is an important class of ABE, where cipher texts are labeled with sets of attributes and private keys are associated with access structures that control which cipher texts a user is able to decrypt. KP-ABE has important applications in data sharing on untrusted cloud storage. However, the cipher text size grows linearly with the number of attributes embedded in cipher text in most existing KP-ABE schemes. In this paper, we describe our work on designing a KP-ABE scheme with constant size cipher text for monotonic access structures. The downside of the proposed KP-ABE scheme is that private keys have multiple size growth in the number of attributes in the access structure. The proposed KP-ABE scheme is proved to be secure under the general Diffie-Hellman exponent assumption. © 2012 IEEE.","cloud computing; constant size ciphertexts; general Diffie-Hellman exponent assumption; identity-based broadcast encryption; key-policy attribute-based encryption","Access structure; Attribute-based encryption schemes; Attribute-based encryptions; Ciphertexts; Cloud storages; Constant size ciphertext; Constant sizes; Cryptographic primitives; Data Sharing; Decentralized access control; Diffie Hellman; Identity-based broadcast encryptions; Key policies; Private key; Size growths; Access control; Artificial intelligence; Cloud computing; Cryptography",Conference Paper,Scopus,2-s2.0-84873560180
"Llera A., Gómez V., Kappen H.J.","Adaptive classification on brain-computer interfaces using reinforcement signals",2012,"Neural Computation",12,10.1162/NECO_a_00348,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874207140&doi=10.1162%2fNECO_a_00348&partnerID=40&md5=f0469d6e8a9bc5b51409675f032fae20","We introduce a probabilistic model that combines a classifier with an extra reinforcement signal (RS) encoding the probability of an erroneous feedback being delivered by the classifier. This representation computes the class probabilities given the task related features and the reinforcement signal. Using expectation maximization (EM) to estimate the parameter values under such a model shows that some existing adaptive classifiers are particular cases of such an EM algorithm. Further, we present a new algorithm for adaptive classification, which we call constrained means adaptive classifier, and show using EEG data and simulated RS that this classifier is able to significantly outperform state-ofthe-art adaptive classifiers. © 2012 Massachusetts Institute of Technology.",,"algorithm; article; artificial intelligence; automated pattern recognition; biological model; brain computer interface; human; reinforcement; signal processing; statistical model; Algorithms; Artificial Intelligence; Brain-Computer Interfaces; Humans; Models, Neurological; Models, Statistical; Pattern Recognition, Automated; Reinforcement (Psychology); Signal Processing, Computer-Assisted",Article,Scopus,2-s2.0-84874207140
"Szymczak D., Magnusson C., Rassmus-Gröhn K.","Guiding tourists through haptic interaction: Vibration feedback in the Lund time machine",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-31404-9_27,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868027964&doi=10.1007%2f978-3-642-31404-9_27&partnerID=40&md5=2d0ba5582bff568ce7c68ec77f239056","This paper describes the vibrationnal feedback that was chosen for the guiding interaction part of the Lund Time Machine application. This tourist guide provides information on points of interests along a trail, and guides the user along it. The interface uses audio and tactile modalities to be accessible in situations where the visual channel is not available. To navigate to the next goal, the user scans around and feels the phone vibrating in the correct direction. The distance coding was embedded in the directional feedback by making the bursts more frequent when getting closer to the goal. The design was first evaluated in a controlled study and then validated as usable and non-obtrusive within an evaluation in the real context of use. © 2012 Springer-Verlag.",,"Context of use; Haptic interactions; Points of interest; Tactile modality; Time machine; Tourist guides; Vibration feedback; Visual channels; Artificial intelligence; Computer science; Communication",Conference Paper,Scopus,2-s2.0-84868027964
"Park H., Lee K., Cho H.-C., Kim K.-J.","Prediction of early stage opponents strategy for StarCraft AI using scouting and machine learning",2012,"Proceedings - WASA 2012: Workshop at SIGGRAPH Asia 2012",12,10.1145/2425296.2425298,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872846497&doi=10.1145%2f2425296.2425298&partnerID=40&md5=330ebfa475e46e1fef4a5a4baa8d7d9a","StarCraft is one of the most famous Real-Time Strategy Games and there have been several competitions on AI bots. In order to win StarCraft, players have to predict their opponents strategy and respond properly. Human players used to scout their opponent territory using a unit and gathering information through direct observation to predict their opponents strategy. The accurate prediction of an opponents strategy gives players a big advantage in the early stage of a game. Usually, strategies of StarCraft can be divided into two parts: fast and slow attack strategies. Initial attack timing is an important factor of game strategies. In this paper, we apply a scouting algorithm and various machine learning algorithms to predict an opponents attack timing (strategy). Training data are collected from the games between our Xelnaga bot with the scouting algorithm and various online human players. Experimental results show that the machine learning approach based on realistic scouting data can be beneficial in predicting the opponents early-stage strategy. © 2012 ACM.","artificial intelligence; game AI; machine learning; scouting; StarCraft; strategy prediction","Accurate prediction; Attack strategies; Game strategies; Human players; Learning approach; Real-time strategy games; scouting; StarCraft; Training data; Artificial intelligence; Forecasting; Interactive computer graphics; Learning systems; Learning algorithms",Conference Paper,Scopus,2-s2.0-84872846497
"Apsel U., Brafman R.I.","Exploiting uniform assignments in first-order MPE",2012,"Uncertainty in Artificial Intelligence - Proceedings of the 28th Conference, UAI 2012",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885879081&partnerID=40&md5=f4876bc009d6b82d490216c2edd96f45","The MPE (Most Probable Explanation) query plays an important role in probabilistic inference. MPE solution algorithms for probabilistic relational models essentially adapt existing belief assessment method, replacing summation with maximization. But the rich structure and symmetries captured by relational models together with the properties of the maximization operator offer an opportunity for additional simplification with potentially significant computational ramifications. Specifically, these models often have groups of variables that define symmetric distributions over some population of formulas. The maximizing choice for different elements of this group is the same. If we can realize this ahead of time, we can significantly reduce the size of the model by eliminating a potentially significant portion of random variables. This paper defines the notion of uniformly assigned and partially uniformly assigned sets of variables, shows how one can recognize these sets efficiently, and how the model can be greatly simplified once we recognize them, with little computational effort. We demonstrate the effectiveness of these ideas empirically on a number of models.",,"Computational effort; Most probable explanation; Probabilistic inference; Probabilistic relational models; Relational Model; Rich structure; Solution algorithms; Symmetric distributions; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84885879081
"Zhao E.-D., Qi Y.-Q., Xiang X.-X., Chen Y.","A data placement strategy based on genetic algorithm for scientific workflows",2012,"Proceedings of the 2012 8th International Conference on Computational Intelligence and Security, CIS 2012",12,10.1109/CIS.2012.40,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873535959&doi=10.1109%2fCIS.2012.40&partnerID=40&md5=5f45edcd0a74310216dfc27ea7da0238","The data placement strategy is an important issue in the scientific workflows which is devoted to reducing the data movements while placing datasets in a few data centers according to the data centers' storage capacity and the data dependency. The data placement is proved to be a NP hard problem, and several methods for this problem like K-means clustering algorithm are presented in the literatures. K-means clustering algorithm can reduce the number of data movements very well, but it may result that the datasets will be concentrated to few data centers, and so the loads of data centers greatly deviate from each other. The paper proposes a data placement strategy based on heuristic genetic algorithm to reduce data movements among the data centers while balancing the loads of data centers. The simulation results show that the proposed algorithm can effectively reduce data movements and balance the load of data centers. © 2012 IEEE.","data dependency; data placement; heuristic genetic algorithm; load balancing","Data centers; Data dependencies; Data movements; Data placement; K-Means clustering algorithm; Number of datum; Scientific workflows; Storage capacity; Artificial intelligence; Resource allocation; Clustering algorithms",Conference Paper,Scopus,2-s2.0-84873535959
"Tar J.K., Rudas I.J., Kosi K., Csapó Á., Baranyi P.","Cognitive Control initiative",2012,"3rd IEEE International Conference on Cognitive Infocommunications, CogInfoCom 2012 - Proceedings",12,10.1109/CogInfoCom.2012.6422046,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874443850&doi=10.1109%2fCogInfoCom.2012.6422046&partnerID=40&md5=1010ba739dccc8b12e64bea843a2568e","In this paper, we make an initial effort to define the scope and goals of 'Cognitive Control (CoCo)', which intends to be a research initiative for bringing about a novel generation of control systems which - in a certain sense - reflect the behavior of humans while solving everyday tasks. Very briefly stated, through the use of a rich inventory of available modeling techniques, control theory has achieved profound results in model-based control approaches. However, these solutions do not seem to begin to approach the true level of human intelligence. We are convinced that by the synthesis of these 'tools' from a particular point of view the great efficiency of human intelligence can be integrated/fused with the complementary virtues of our machines (e.g. high computing power, fast operation, etc.), and our CoCo initiative can be realized and applied in practice. © 2012 IEEE.","Adaptive Control; Artificial Intelligence; Artificial Neural Networks; Cognitive Control; Fuzzy Systems; Iterative Control; Model-based Control; Neuro-fuzzy Systems; Robust Control; Universal Approximators","Adaptive Control; Cognitive control; Iterative control; Model-based control; Neurofuzzy system; Universal approximators; Artificial intelligence; Fuzzy systems; Neural networks; Robust control; Iterative methods",Conference Paper,Scopus,2-s2.0-84874443850
"Kerssemakers M., Tuxen J., Togelius J., Yannakakis G.N.","A procedural procedural level generator generator",2012,"2012 IEEE Conference on Computational Intelligence and Games, CIG 2012",12,10.1109/CIG.2012.6374174,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871970712&doi=10.1109%2fCIG.2012.6374174&partnerID=40&md5=1567998ab448b36b4d0c236ce3ec6ffd","Procedural content generation (PCG) is concerned with automatically generating game content, such as levels, rules, textures and items. But could the content generator itself be seen as content, and thus generated automatically? This would be very useful if one wanted to avoid writing a content generator for a new game, or if one wanted to create a content generator that generates an arbitrary amount of content with a particular style or theme. In this paper, we present a procedural procedural level generator generator for Super Mario Bros. It is an interactive evolutionary algorithm that evolves agent-based level generators. The human user makes the aesthetic judgment on what generators to prefer, based on several views of the generated levels including a possibility to play them, and a simulation-based estimate of the playability of the levels. We investigate the characteristics of the generated levels, and to what extent there is similarity or dissimilarity between levels and between generators. © 2012 IEEE.",,"Agent based; Human users; Interactive evolutionary algorithms; Playability; Artificial intelligence; Computational methods",Conference Paper,Scopus,2-s2.0-84871970712
"Macindoe O., Kaelbling L.P., Lozano-Pérez T.","POMCoP: Belief space planning for sidekicks in cooperative games",2012,"Proceedings of the 8th AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, AIIDE 2012",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883114920&partnerID=40&md5=e98d94c59bac4586481aadc94a09a728","We present POMCoP, a system for online planning in collaborative domains that reasons about how its actions will affect its understanding of human intentions, and demonstrate its use in building sidekicks for cooperative games. POMCoP plans in belief space. It explicitly represents its uncertainty about the intentions of its human ally, and plans actions which reveal those intentions or hedge against its uncertainty. This allows POMCoP to reason about the usefulness of incorporating information gathering actions into its plans, such as asking questions, or simply waiting to let humans reveal their intentions. We demonstrate POMCoP by constructing a sidekick for a cooperative pursuit game, and evaluate its effectiveness relative to MDP-based techniques that plan in state space, rather than belief space. Copyright © 2012, Association for the Advancement of Artificial Intelligence.",,"Belief space; Cooperative game; Human intentions; In-buildings; Information gathering; On-line planning; Artificial intelligence; Human computer interaction; Game theory",Conference Paper,Scopus,2-s2.0-84883114920
"Wee C.Y., Yap P.T., Zhang D., Wang L., Shen D.","Constrained sparse functional connectivity networks for MCI classification.",2012,"Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872932372&partnerID=40&md5=f183854743ebab479f7178380e3286b1","Mild cognitive impairment (MCI) is difficult to diagnose due to its subtlety. Recent emergence of advanced network analysis techniques utilizing resting-state functional Magnetic Resonance Imaging (rs-fMRI) has made the understanding of neurological disorders more comprehensively at a whole-brain connectivity level. However, inferring effective brain connectivity from fMRI data is a challenging task, particularly when the ultimate goal is to obtain good control-patient classification performance. Incorporating sparsity into connectivity modeling can potentially produce results that are biologically more meaningful since most biologically networks are formed by a relatively few number of connections. However, this constraint, when applied at an individual level, will degrade classification performance due to inter-subject variability. To address this problem, we consider a constrained sparse linear regression model associated with the least absolute shrinkage and selection operator (LASSO). Specifically, we introduced sparsity into brain connectivity via l1-norm penalization, and ensured consistent non-zero connections across subjects via l2-norm penalization. Our results demonstrate that the constrained sparse network gives better classification performance than the conventional correlation-based network, indicating its greater sensitivity to early stage brain pathologies.",,"algorithm; article; artificial intelligence; automated pattern recognition; brain; computer assisted diagnosis; connectome; human; image enhancement; methodology; mild cognitive impairment; nerve cell network; nuclear magnetic resonance imaging; pathophysiology; reproducibility; sensitivity and specificity; Algorithms; Artificial Intelligence; Brain; Connectome; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Mild Cognitive Impairment; Nerve Net; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84872932372
"Mahadevan S., Liu B.","Sparse Q-learning with mirror descent",2012,"Uncertainty in Artificial Intelligence - Proceedings of the 28th Conference, UAI 2012",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886008156&partnerID=40&md5=bd935374870c911c1d78257ac409d2ba","This paper explores a new framework for reinforcement learning based on online convex optimization, in particular mirror descent and related algorithms. Mirror descent can be viewed as an enhanced gradient method, particularly suited to minimization of convex functions in highdimensional spaces. Unlike traditional gradient methods, mirror descent undertakes gradient updates of weights in both the dual space and primal space, which are linked together using a Legendre transform. Mirror descent can be viewed as a proximal algorithm where the distance generating function used is a Bregman divergence. A new class of proximal-gradient based temporaldifference (TD) methods are presented based on different Bregman divergences, which are more powerful than regular TD learning. Examples of Bregman divergences that are studied include pnorm functions, and Mahalanobis distance based on the covariance of sample gradients. A new family of sparse mirror-descent reinforcement learning methods are proposed, which are able to find sparse fixed points of an l1-regularized Bellman equation at significantly less computational cost than previous methods based on secondorder matrix methods. An experimental study of mirror-descent reinforcement learning is presented using discrete and continuous Markov decision processes.",,"Generating functions; High dimensional spaces; Legendre transforms; Mahalanobis distances; Markov Decision Processes; Online convex optimizations; Reinforcement learning method; Temporal differences; Artificial intelligence; Convex optimization; Functions; Gradient methods; Learning algorithms; Markov processes; Mirrors; Reinforcement learning",Conference Paper,Scopus,2-s2.0-84886008156
"Ibrahim Z., Muhammad B., Ghazali K.H., Lim K.S., Nawawi S.W., Yusof Z.M.","Vector Evaluated Gravitational Search Algorithm (VEGSA) for multi-objective optimization problems",2012,"Proceedings of International Conference on Computational Intelligence, Modelling and Simulation",12,10.1109/CIMSim.2012.29,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872530418&doi=10.1109%2fCIMSim.2012.29&partnerID=40&md5=c1e3cce7cba0043e835fda6091ea7e8b","This paper presents a novel algorithm, which is based on Gravitational Search Algorithm (GSA), for multiobjective optimization problems. The proposed algorithm, which is called Vector Evaluated Gravitational Search Algorithm (VEGSA), uses a number of populations of particles. In particular, a population of particles corresponds to one objective function to be minimized or maximized. Simultaneous minimization or maximization of every objective function is realized by exchanging a variable between populations. Two versions of VEGSA algorithm are presented in this study. Convex and non-convex test functions on biobjective optimization problems are used to evaluate the effectiveness of the proposed VEGSA. © 2012 IEEE.","GSA; Multi-objective optimization; VEGSA","Bi-objective optimization; Gravitational search algorithms; GSA; Multi objective optimizations (MOO); Multi-objective optimization problem; Novel algorithm; Objective functions; Test functions; Vector evaluated; VEGSA; Artificial intelligence; Learning algorithms; Multiobjective optimization",Conference Paper,Scopus,2-s2.0-84872530418
"Retnasamy V., Sauli Z., Shapri A.H.M., Taniselass S., Vairavan R., Ramli N.","Interaction of surface roughness and copper ball adhesion using shearing simulation",2012,"Proceedings of International Conference on Computational Intelligence, Modelling and Simulation",12,10.1109/CIMSim.2012.87,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872507183&doi=10.1109%2fCIMSim.2012.87&partnerID=40&md5=452d4f8ce40f9c06c7bbf23d23a64ae7","Wire bonding is one of the commanding interconnection techniques used at the back end of line due to its vast adaptability to the advancing trend of circuit designs. Wire bond shear test method is utilized to examine adhesion strength of the bonded wires. In this paper, the stress response of copper ball bond during wire bond shear test is investigated The influences of three types of bond pad surface; flat surface, hemisphere surface and sharp groove surface on the stress response of copper ball bond during wire bond shear test were evaluated. The simulation was done using Ansys version 11.The simulation results showed the bond pad surface had a significant influence on the stress response of copper ball bond. At the shear ram displacement distance of 35μm, the peak stress responses for all three bond pad surface were obtained. © 2012 IEEE.","Wire bond shear test; copper wire; bond pad surface","Back end of lines; Bond pad; Bonded wires; Circuit designs; Flat surfaces; Interconnection technique; Peak stress; Shear tests; Stress response; Wire bonding; Wire bonds; Adhesion; Artificial intelligence; Copper; Surface roughness; Electronics packaging",Conference Paper,Scopus,2-s2.0-84872507183
"Kothgassner O.D., Felnhofer A., Beutl L., Hlavacs H., Lehenbauer M., Stetina B.","A virtual training tool for giving talks",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-33542-6_5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875652601&doi=10.1007%2f978-3-642-33542-6_5&partnerID=40&md5=6f0f94c90500995730ded860d390e24f","In this paper we present two studies concerning the application of a virtual environment for public speaking anxiety. We have created a program simulating a virtual lecture room, which can be filled with a large number of listeners behaving in different ways. The purpose of the scene is to train people who are anxious to give talks in front of a large audience. We present the results of two studies, showing the impact of this kind of virtual exposure. Results indicate that people do experience such a situation as realistic, as well as report social insecurity and show heightened psychophysiological arousal (HR). Furthermore, we show that especially curious people, and people with high social insecurity rate the system as useful. © 2012 Springer-Verlag Berlin Heidelberg.",,"Public speaking; Virtual training; Artificial intelligence; Virtual reality",Conference Paper,Scopus,2-s2.0-84875652601
"Xu Y., Liu J., Lin S., Xu D., Cheung C.Y., Aung T., Wong T.Y.","Efficient optic cup detection from intra-image learning with retinal structure priors.",2012,"Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872554827&partnerID=40&md5=d7b1fb766a8d87defd9b2e5363097917","We present a superpixel based learning framework based on retinal structure priors for glaucoma diagnosis. In digital fundus photographs, our method automatically localizes the optic cup, which is the primary image component clinically used for identifying glaucoma. This method provides three major contributions. First, it proposes processing of the fundus images at the superpixel level, which leads to features more descriptive and effective than those employed by pixel-based techniques, while yielding significant computational savings over methods based on sliding windows. Second, the classifier learning process does not rely on pre-labeled training samples, but rather the training samples are extracted from the test image itself using structural priors on relative cup and disc positions. Third, we present a classification refinement scheme that utilizes both structural priors and local context. Tested on the ORIGA(-light) clinical dataset comprised of 650 images, the proposed method achieves a 26.7% non-overlap ratio with manually-labeled ground-truth and a 0.081 absolute cup-to-disc ratio (CDR) error, a simple yet widely used diagnostic measure. This level of accuracy is comparable to or higher than the state-of-the-art technique, with a speedup factor of tens or hundreds.",,"algorithm; article; artificial intelligence; blood vessel; computer program; diagnostic imaging; eye fundus; glaucoma; histology; human; image processing; methodology; optic disk; optic nerve disease; pathology; reproducibility; retina; statistical model; visual system examination; Algorithms; Artificial Intelligence; Blood Vessels; Diagnostic Imaging; Diagnostic Techniques, Ophthalmological; Fundus Oculi; Glaucoma; Humans; Image Processing, Computer-Assisted; Models, Statistical; Optic Disk; Optic Nerve Diseases; Reproducibility of Results; Retina; Software",Article,Scopus,2-s2.0-84872554827
"Yang Q., Yang Z., Sun Y.","Universal neural network control of MIMO uncertain nonlinear systems",2012,"IEEE Transactions on Neural Networks and Learning Systems",12,10.1109/TNNLS.2012.2197219,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876896853&doi=10.1109%2fTNNLS.2012.2197219&partnerID=40&md5=49332b9d223a892cd876dc5e8f61df4a","In this brief, a continuous tracking control law is proposed for a class of high-order multi-input-multi-output uncertain nonlinear dynamic systems with external disturbance and unknown varying control direction matrix. The proposed controller consists of high-gain feedback, Nussbaum gain matrix selector, online approximator (OLA) model and a robust term. The OLA model is represented by a two-layer neural network. The continuousness of the control signal is guaranteed to relax the requirement for the actuator bandwidth and avoid the incurred chattering effect. Asymptotic tracking performance is achieved theoretically by standard Lyapunov analysis. The control feasibility is also verified in simulation environment. © 2012 IEEE.","Asymptotic convergence; neural networks; Nussbaum gain; online approximators","Approximators; Asymptotic convergence; External disturbances; Multi-input multi-output; Neural network control; Nussbaum gain; Simulation environment; Uncertain nonlinear systems; Nonlinear dynamical systems; Neural networks; algorithm; artificial intelligence; artificial neural network; computer simulation; environment; feedback system; nonlinear system; standards; theoretical model; uncertainty; Algorithms; Artificial Intelligence; Computer Simulation; Environment; Feedback; Models, Theoretical; Neural Networks (Computer); Nonlinear Dynamics; Uncertainty",Article,Scopus,2-s2.0-84876896853
"Fu G., Nan X., Liu H., Patel R.Y., Daga P.R., Chen Y., Wilkins D.E., Doerksen R.J.","Implementation of multiple-instance learning in drug activity prediction.",2012,"BMC bioinformatics",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877021042&partnerID=40&md5=e876a9d2c8c92c54f435d78db09d39ad","In the context of drug discovery and development, much effort has been exerted to determine which conformers of a given molecule are responsible for the observed biological activity. In this work we aimed to predict bioactive conformers using a variant of supervised learning, named multiple-instance learning. A single molecule, treated as a bag of conformers, is biologically active if and only if at least one of its conformers, treated as an instance, is responsible for the observed bioactivity; and a molecule is inactive if none of its conformers is responsible for the observed bioactivity. The implementation requires instance-based embedding, and joint feature selection and classification. The goal of the present project is to implement multiple-instance learning in drug activity prediction, and subsequently to identify the bioactive conformers for each molecule. We encoded the 3-dimensional structures using pharmacophore fingerprints which are binary strings, and accomplished instance-based embedding using calculated dissimilarity distances. Four dissimilarity measures were employed and their performances were compared. 1-norm SVM was used for joint feature selection and classification. The approach was applied to four data sets, and the best proposed model for each data set was determined by using the dissimilarity measure yielding the smallest number of selected features. The predictive abilities of the proposed approach were compared with three classical predictive models without instance-based embedding. The proposed approach produced the best predictive models for one data set and second best predictive models for the rest of the data sets, based on the external validations. To validate the ability of the proposed approach to find bioactive conformers, 12 small molecules with co-crystallized structures were seeded in one data set. 10 out of 12 co-crystallized structures were indeed identified as significant conformers using the proposed approach. The proposed approach was proven not to suffer from overfitting and to be highly competitive with classical predictive models, so it is very powerful for drug activity prediction. The approach was also validated as a useful method for pursuit of bioactive conformers.",,"article; artificial intelligence; biology; comparative study; conformation; drug development; methodology; quantitative structure activity relation; theoretical model; validation study; Artificial Intelligence; Computational Biology; Drug Discovery; Models, Theoretical; Molecular Conformation; Quantitative Structure-Activity Relationship",Article,Scopus,2-s2.0-84877021042
"Goel A., Rousseau L.-M.","Truck driver scheduling in Canada",2012,"Journal of Scheduling",12,10.1007/s10951-011-0249-6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870553564&doi=10.1007%2fs10951-011-0249-6&partnerID=40&md5=8981add92c6a2cae6be9a46686685c95","This paper presents and studies the Canadian Truck Driver Scheduling Problem (CAN-TDSP), which is the problem of determining whether a sequence of locations can be visited within given time windows in such a way that driving and working activities of truck drivers comply with Canadian Commercial Vehicle Drivers Hours of Service Regulations. Canadian regulations comprise the provisions found in US hours of service regulations as well as additional constraints on the maximum amount of driving and the minimum amount of off-duty time on each day. We present two heuristics and an exact approach for solving the CAN-TDSP. Computational experiments demonstrate the effectiveness of our approaches and indicate that Canadian regulations are significantly more permissive than US hours of service regulations. © Springer Science+Business Media, LLC 2012.","Hours of service regulations; Vehicle scheduling","Computational experiment; Driver scheduling; Exact approach; Hours of services; Time windows; Vehicle scheduling; Artificial intelligence; Scheduling; Software engineering; Truck drivers",Article,Scopus,2-s2.0-84870553564
"Walker S.I., Cisneros L., Davies P.C.W.","Evolutionary transitions and top-down causation",2012,"Artificial Life 13: Proceedings of the 13th International Conference on the Simulation and Synthesis of Living Systems, ALIFE 2012",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872231323&partnerID=40&md5=cf49241dda86c3e3cfaf2da58e97630b","Top-down causation has been suggested to occur at all scales of biological organization as a mechanism for explaining the hierarchy of structure and causation in living systems (Campbell, 1974; Auletta et al., 2008; Davies, 2006b, 2012; Ellis, 2012). Here we propose that a transition from bottom-up to top-down causation - mediated by a reversal in the flow of information from lower to higher levels of organization, to that from higher to lower levels of organization - is a driving force for most major evolutionary transitions. We suggest that many major evolutionary transitions might therefore be marked by a transition in causal structure. We use logistic growth as a toy model for demonstrating how such a transition can drive the emergence of collective behavior in replicative systems. We then outline how this scenario may have played out in those major evolutionary transitions in which new, higher levels of organization emerged, and propose possible methods via which our hypothesis might be tested. © 2012 Massachusetts Institute of Technology.",,"A transitions; Biological organization; Campbell; Collective behavior; Driving forces; Evolutionary transitions; Living systems; Logistic growth; Top-down causation; Toy models; Artificial intelligence; Chemistry; Circuit simulation; Genetic engineering; Software engineering; Speech synthesis; Biological systems",Conference Paper,Scopus,2-s2.0-84872231323
"Bierman G., Russo C., Mainland G., Meijer E., Torgersen M.","Pause 'n' Play: Formalizing asynchronous C#",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-31057-7-12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879697611&doi=10.1007%2f978-3-642-31057-7-12&partnerID=40&md5=b0e84b0b24bfeda3ffda603be349fbfb","Writing applications that connect to external services and yet remain responsive and resource conscious is a difficult task. With the rise of web programming this has become a common problem. The solution lies in using asynchronous operations that separate issuing a request from waiting for its completion. However, doing so in common object-oriented languages is difficult and error prone. Asynchronous operations rely on callbacks, forcing the programmer to cede control. This inversion of control-flow impedes the use of structured control constructs, the staple of sequential code. In this paper, we describe the language support for asynchronous programming in the upcoming version of C. The feature enables asynchronous programming using structured control constructs. Our main contribution is a precise mathematical description that is abstract (avoiding descriptions of compiler-generated state machines) and yet sufficiently concrete to allow important implementation properties to be identified and proved correct. © 2012 Springer-Verlag Berlin Heidelberg.",,"Asynchronous operation; Asynchronous programming; Control constructs; Error prones; Mathematical descriptions; Object-oriented languages; State machine; Web programming; Artificial intelligence; Computer science; Object oriented programming",Conference Paper,Scopus,2-s2.0-84879697611
"Bouker S., Saidi R., Yahia S.B., Nguifo E.M.","Ranking and selecting association rules based on dominance relationship",2012,"Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI",12,10.1109/ICTAI.2012.94,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876863802&doi=10.1109%2fICTAI.2012.94&partnerID=40&md5=e546c762352099d73b024eca713e7e29","The huge number of association rules represent the main hamper that a decision maker faces. In order to bypass this hamper, an efficient selection of rules has to be performed. Since selection is necessarily based on evaluation, many interestingness measures have been proposed. However, the abundance of these measures gave rise to a new problem, namely the heterogeneity of the evaluation results and this created confusion to the decision. In this respect, we propose a novel approach to discover interesting association rules without favoring or excluding any measure by adopting the notion of dominance between association rules. Our approach bypasses the problem of measure heterogeneity and unveils a compromise between their evaluations. Interestingly enough, the proposed approach also avoids another non-trivial problem which is the threshold value specification. © 2012 IEEE.","Association rules selection; Dominance relationship; Interestingness measures","Decision makers; Dominance relationships; Evaluation results; Interestingness measures; Non-trivial; Artificial intelligence; Association rules",Conference Paper,Scopus,2-s2.0-84876863802
"Brandstetter M.F., Ahmadi S.","Reactive control of Ms. Pac Man using information retrieval based on Genetic Programming",2012,"2012 IEEE Conference on Computational Intelligence and Games, CIG 2012",12,10.1109/CIG.2012.6374163,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871985821&doi=10.1109%2fCIG.2012.6374163&partnerID=40&md5=df2d31d69f9e7802721053d838bd5d54","During the last years the well-known Ms. Pac Man video game has been - and still is - an interesting test bed for the research on various concepts from the broad area of Artificial Intelligence (AI). Among these concepts is the use of Genetic Programming (GP) to control the game from a human player's perspective. Several GP-based approaches have been examined so far, where traditionally they define two types of GP terminals: one type for information retrieval, the second type for issuing actions (commands) to the game world. However, by using these action terminals the controller has to manage actions issued to the game during their runtime and to monitor their outcome. In order to avoid the need for active task management this paper presents a novel approach for the design of a GP-based Ms. Pac Man controller: the proposed approach solely relies on information retrieval terminals in order to rate all possible directions of movement at every time step during a running game. Based on these rating values the controller can move the agent through the mazes of the the game world of Ms. Pac Man. With this design, which forms the main contribution of our work, we decrease the overall GP solution complexity by removing all action control management tasks from the system. It is demonstrated that by following the proposed approach such a system can successfully control an autonomous agent in a computer game environment on the level of an amateur human player. © 2012 IEEE.",,"Active tasks; Computer game; Control management; Human players; Reactive control; Runtimes; Time step; Video game; Artificial intelligence; Autonomous agents; Equipment testing; Genetic programming; Human computer interaction; Information retrieval; Interactive computer graphics; Controllers",Conference Paper,Scopus,2-s2.0-84871985821
"Ben-Zvi T.","Measuring the perceived effectiveness of decision support systems and their impact on performance",2012,"Decision Support Systems",12,10.1016/j.dss.2012.05.033,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868640683&doi=10.1016%2fj.dss.2012.05.033&partnerID=40&md5=90d893b8cd31a9aed2228232fa9be39d","This study investigates decision support systems (DSS) by assessing the factors that enhance their perceived effectiveness and their impact on performance. This was achieved by using a simulation exercise with 652 senior graduate students who developed DSS and reported on the systems created. Our analysis shows that DSS users who perceive the system as effective correlate to improved company performance. However, investing significant human resources in developing a system does not necessarily guarantee enhanced performance. In addition, the study exemplifies how user traits can impact perceived effectiveness. © 2012 Elsevier B.V. All rights reserved.","Decision support systems; Effectiveness; Performance; Simulation","Company performance; Effectiveness; Graduate students; Performance; Simulation; Simulation exercise; Decision support systems; Students; Artificial intelligence",Article,Scopus,2-s2.0-84868640683
"Mahdiani H.R., Fakhraie S.M., Lucas C.","Relaxed fault-tolerant hardware implementation of neural networks in the presence of multiple transient errors",2012,"IEEE Transactions on Neural Networks and Learning Systems",12,10.1109/TNNLS.2012.2199517,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876895429&doi=10.1109%2fTNNLS.2012.2199517&partnerID=40&md5=23a2ecdc5d16ba8c36677be88ec7cf8e","Reliability should be identified as the most important challenge in future nano-scale very large scale integration (VLSI) implementation technologies for the development of complex integrated systems. Normally, fault tolerance (FT) in a conventional system is achieved by increasing its redundancy, which also implies higher implementation costs and lower performance that sometimes makes it even infeasible. In contrast to custom approaches, a new class of applications is categorized in this paper, which is inherently capable of absorbing some degrees of vulnerability and providing FT based on their natural properties. Neural networks are good indicators of imprecision-tolerant applications. We have also proposed a new class of FT techniques called relaxed fault-tolerant (RFT) techniques which are developed for VLSI implementation of imprecision-tolerant applications. The main advantage of RFT techniques with respect to traditional FT solutions is that they exploit inherent FT of different applications to reduce their implementation costs while improving their performance. To show the applicability as well as the efficiency of the RFT method, the experimental results for implementation of a face-recognition computationally intensive neural network and its corresponding RFT realization are presented in this paper. The results demonstrate promising higher performance of artificial neural network VLSI solutions for complex applications in faulty nano-scale implementation environments. © 2012 IEEE.","Artificial neural networks; digital hardware implementation; face recognition hardware; fault tolerant techniques; soft error; very large scale integration (VLSI)","Complex applications; Conventional systems; Digital hardware implementation; Fault tolerant technique; Hardware implementations; Implementation cost; Soft error; Very large-scale integration; Face recognition; Fault tolerance; Nanotechnology; Neural networks; Hardware; artificial intelligence; artificial neural network; computer; human; pattern recognition; photostimulation; procedures; standards; Artificial Intelligence; Computers; Humans; Neural Networks (Computer); Pattern Recognition, Visual; Photic Stimulation",Article,Scopus,2-s2.0-84876895429
"Darouich H., Gonçalves J.M., Muga A., Pereira L.S.","Water saving vs. farm economics in cotton surface irrigation: An application of multicriteria analysis",2012,"Agricultural Water Management",12,10.1016/j.agwat.2012.09.006,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869798342&doi=10.1016%2fj.agwat.2012.09.006&partnerID=40&md5=f71afc99fe22f9d82b4aeb41a6ba49c3","Improving surface irrigation systems for cotton in Ras-El-Ain district, Northeast Syria, needs finding alternative solutions that provide for both water saving and farm economic benefits in a context of small and family farms. Multicriteria analysis was used to evaluate and rank a set of furrow and border irrigation alternatives, with and without precise land leveling, that were created with the decision support system SADREG. This approach allowed to consider various criteria, mainly water saving and farm economics. Results show that both graded furrow and border alternatives are acceptable, with a slight advantage for graded furrows. Alternatives without land leveling have shown to be more appropriate when focusing farm economic results, while alternatives including land leveling were selected when priorities were assigned to water saving. These results relate with higher costs of alternatives that consider land leveling. Equipment for appropriate control of inflow rates was considered for all cases. The improved alternatives may lead to savings of 20-28% of irrigation water and increasing the irrigation water productivity from present 0.31 to 0.44kgm-3. When the same alternatives were ranked for a 20% deficit irrigation their rankings changed, with reduced ranks of alternatives requiring land leveling. This is due to the fact that yields and yield values are reduced with deficit irrigation, thus making it less favorable to select alternatives that imply higher costs. The study shows that adopting more advanced but more costly irrigation technologies aimed at water saving requires appropriate economic incentives, training of farmers and an institutional framework able to support the sustainable use of water in irrigation. © 2012 Elsevier B.V.","Decision support systems; Deficit irrigation; Economic water productivity; Furrow and border irrigation; Khabour basin; Land leveling; Model SADREG; Northeast Syria","Border irrigation; Deficit irrigation; Khabour basin; Land leveling; Northeast Syria; Water productivity; Artificial intelligence; Cotton; Decision support systems; Economic analysis; Irrigation; Water conservation; Water supply; Water management; cotton; decision support system; farm; furrow irrigation; multicriteria analysis; sustainable development; water storage; Khabur River; Syrian Arab Republic; Gossypium hirsutum",Article,Scopus,2-s2.0-84869798342
"Moon S., Pakhomov S., Melton G.B.","Automated disambiguation of acronyms and abbreviations in clinical texts: window and training size considerations.",2012,"AMIA ... Annual Symposium proceedings / AMIA Symposium. AMIA Symposium",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880809454&partnerID=40&md5=24ef9ef20c5762a0b345ce0653b310e6","Acronyms and abbreviations within electronic clinical texts are widespread and often associated with multiple senses. Automated acronym sense disambiguation (WSD), a task of assigning the context-appropriate sense to ambiguous clinical acronyms and abbreviations, represents an active problem for medical natural language processing (NLP) systems. In this paper, fifty clinical acronyms and abbreviations with 500 samples each were studied using supervised machine-learning techniques (Support Vector Machines (SVM), Naïve Bayes (NB), and Decision Trees (DT)) to optimize the window size and orientation and determine the minimum training sample size needed for optimal performance. Our analysis of window size and orientation showed best performance using a larger left-sided and smaller right-sided window. To achieve an accuracy of over 90%, the minimum required training sample size was approximately 125 samples for SVM classifiers with inverted cross-validation. These findings support future work in clinical acronym and abbreviation WSD and require validation with other clinical texts.",,"article; artificial intelligence; Bayes theorem; decision tree; electronic medical record; natural language processing; nomenclature; support vector machine; Abbreviations as Topic; Artificial Intelligence; Bayes Theorem; Decision Trees; Electronic Health Records; Natural Language Processing; Support Vector Machines",Article,Scopus,2-s2.0-84880809454
"Ghazali K., Xiao R., Ma J.","Road lane detection using h-maxima and improved hough transform",2012,"Proceedings of International Conference on Computational Intelligence, Modelling and Simulation",12,10.1109/CIMSim.2012.31,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872583457&doi=10.1109%2fCIMSim.2012.31&partnerID=40&md5=a0c71d46646889b9a1404346d7efbf1d","A fast and improved algorithm with the ability to detect unexpected lane changes is aimed in this paper. A short segment of a long curve has relative low curvature which is approximated as a straight line. Based on the characteristics of physical road lane, this paper presents a lane detection technique based on H-MAXIMA transformation and improved Hough Transform algorithm which first defines the region of interest from input image for reducing searching space, divided the image into near field of view and far field of view. In near field of view, Hough transform has been applied to detect lane markers after image noise filtering. The proposed method has been developed using image processing programming language platform and was tested on collected video data. Promising result was obtained with high efficiency of detection. © 2012 IEEE.","H-MAXIMA; Hough Transform; Lane markers","After-images; Far field; H-MAXIMA; Hough transform algorithms; Input image; Lane change; Lane detection; Lane marker; Near fields; Region of interest; Searching spaces; Short segments; Video data; Algorithms; Artificial intelligence; Hough transforms; Image segmentation; Roads and streets; Feature extraction",Conference Paper,Scopus,2-s2.0-84872583457
"Mohan N.R.R., Baburaj E.","Resource allocation techniques in cloud computing - Research challenges for applications",2012,"Proceedings - 4th International Conference on Computational Intelligence and Communication Networks, CICN 2012",12,10.1109/CICN.2012.177,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872033387&doi=10.1109%2fCICN.2012.177&partnerID=40&md5=9aae843e74cb0ba5824b775268536ee6","The computational humanity is flattering extremely bulky and multifaceted. Cloud computing is becoming one of the most expanding methodologies in the computing industry. It is a novel approach for the deliverance of IT services on the World Wide Web. This model provides computing resources in the puddle for consumers, all the way through Internet. In cloud computing, resource allocation and scheduling of numerous aggregate web services is an imperative and demanding quandary. This paper estimates the various network resource allocation strategies and their applications in Cloud Computing Environment. A brief description for network resource allocation in Cloud Computing, based on differentially adapted dynamic proportions, has also been done. This paper addresses and categorizes the foremost challenges normal to the resource allocation progress of cloud computing in terms of diverse types of resource allocation techniques. © 2012 IEEE.","cloud computing; distributed computing; resource allocation","Computing environments; Computing industry; Computing resource; IT services; Network resource allocations; Research challenges; Resource allocation techniques; Artificial intelligence; Cloud computing; Distributed computer systems; Resource allocation; Web services; Computer systems",Conference Paper,Scopus,2-s2.0-84872033387
"McArthur S.D.J., Taylor P.C., Ault G.W., King J.E., Athanasiadis D., Alimisis V.D., Czaplewski M.","The Autonomic Power System - Network operation and control beyond smart grids",2012,"IEEE PES Innovative Smart Grid Technologies Conference Europe",12,10.1109/ISGTEurope.2012.6465807,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874741156&doi=10.1109%2fISGTEurope.2012.6465807&partnerID=40&md5=ec774a1204b65ef79d13932621f859b7","A wide range of applications are being researched and developed within the Smart Grid community, such as voltage control, thermal constraint management, dynamic line ratings and automated reconfiguration. Typically, the current approach is to develop piecemeal automation applied to small sections of legacy networks under current market, commercial and regulatory regimes. The challenges of future energy networks are the anticipated uncertainty and complexity within them. This includes uncertainty in the equipment, configurations and control functionality required married with uncertainty in the participation of consumers through demand side technologies and the uptake of electric vehicles and microgeneration technologies; while complexity is engendered in managing the vast number of interactions within such a system. The authors are developing the concept of the Autonomic Power System which provides flexible and adaptable control through fully distributed intelligence and control. Fundamental research in intelligent systems and network control will deliver a truly integrated self-controlling, self-optimising, self-healing and self-protecting electricity network. This paper outlines the vision, architecture and initial control techniques which will deliver the Autonomic Power System. © 2012 IEEE.","artificial intelligence; power grids; smart grids","Control functionality; Control techniques; Demand-side; Distributed intelligence; Dynamic line ratings; Electricity networks; Fundamental research; Future energies; Legacy networks; Microgeneration; Network control; Network operations; Power grids; Regulatory regime; Self protecting; Self-healing; Smart grid; Thermal constraints; Uncertainty and complexity; Artificial intelligence; Electric vehicles; Intelligent systems; Smart power grids; Complex networks",Conference Paper,Scopus,2-s2.0-84874741156
"Song W., Cho K., Um K., Won C.S., Sim S.","Intuitive terrain reconstruction using height observation-based ground segmentation and 3D object boundary estimation",2012,"Sensors (Switzerland)",12,10.3390/s121217186,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871687845&doi=10.3390%2fs121217186&partnerID=40&md5=dc1d57a2c386abb2fac3180f2445e613","Mobile robot operators must make rapid decisions based on information about the robot's surrounding environment. This means that terrain modeling and photorealistic visualization are required for the remote operation of mobile robots. We have produced a voxel map and textured mesh from the 2D and 3D datasets collected by a robot's array of sensors, but some upper parts of objects are beyond the sensors' measurements and these parts are missing in the terrain reconstruction result. This result is an incomplete terrain model. To solve this problem, we present a new ground segmentation method to detect non-ground data in the reconstructed voxel map. Our method uses height histograms to estimate the ground height range, and a Gibbs-Markov random field model to refine the segmentation results. To reconstruct a complete terrain model of the 3D environment, we develop a 3D boundary estimation method for non-ground objects. We apply a boundary detection technique to the 2D image, before estimating and refining the actual height values of the non-ground vertices in the reconstructed textured mesh. Our proposed methods were tested in an outdoor environment in which trees and buildings were not completely sensed. Our results show that the time required for ground segmentation is faster than that for data sensing, which is necessary for a real-time approach. In addition, those parts of objects that were not sensed are accurately recovered to retrieve their real-world appearances. © 2012 by the authors; licensee MDPI, Basel, Switzerland.","3D boundary estimation; 3D ground segmentation; Gibbs-Markov Random Field; Height histogram; Terrain reconstruction","Boundary estimation; Height histogram; Outdoor environment; Random field model; Random fields; Segmentation methods; Segmentation results; Surrounding environment; Estimation; Graphic methods; Image segmentation; Landforms; Markov processes; Mobile robots; Sensors; Statistical methods; Three dimensional computer graphics; Three dimensional; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; human; image processing; robotics; three dimensional imaging; Algorithms; Artificial Intelligence; Humans; Image Interpretation, Computer-Assisted; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Pattern Recognition, Automated; Robotics",Article,Scopus,2-s2.0-84871687845
"Mattei N., Pini M.S., Rossi F., Venable K.B.","Bribery in voting over combinatorial domains is easy",2012,"International Symposium on Artificial Intelligence and Mathematics, ISAIM 2012",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885717131&partnerID=40&md5=2c0d862450ade8fde8a70b9876b1f513","We investigate the computational complexity of finding optimal bribery and manipulation schemes in voting domains where the candidate set is the Cartesian product of a set of variables and agents' preferences are represented as compact CP-nets. We find that this change in the domain structure, which may lead to an exponential number of candidates in the size of the input, causes many existing computational results for bribery to break down. We provide new algorithms and complexity results which show that, in most cases, bribery in combinatorial domains is easy. This also holds for some cases of k-approval, where bribery is difficult in traditional domains.",,"Algorithms and complexity; Break down; Candidate sets; Cartesian Products; Computational results; CP-nets; Domain structure; Exponential numbers; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84885717131
"Abro A.G., Mohamad-Saleh J.","Enhanced global-best artificial bee colony optimization algorithm",2012,"Proceedings - UKSim-AMSS 6th European Modelling Symposium, EMS 2012",12,10.1109/EMS.2012.65,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874563845&doi=10.1109%2fEMS.2012.65&partnerID=40&md5=01cf1e47641d5346e41059947a74795e","Artificial Bee Colony (ABC) optimization algorithm has captured much attention of researchers from various fields, in recent times. Moreover, various comparative studies clearly reports robust convergence of ABC algorithm than other bio-inspired optimization algorithms. Nevertheless, like other optimization algorithms, ABC suffers from slower convergence and tendency towards local optima trappings. Therefore, various amendments have been proposed to avertthe flaws of ABC algorithm. Nonetheless, the variants are either computationally intensive or could not avert the flaws of the algorithms. Hence, this research work proposes an efficient variant of ABC algorithm. The proposed variant capitalizes on the global-best food-source. The proposed variant has been compared with various existing variants of ABC algorithm on a few benchmark functions. Significance of the proposed variant has also been analyzed statistically. Results show the best convergence of the proposed variant among all the compared optimization algorithms on all benchmark functions. © 2012 IEEE.","ABC variant; computational intelligence; metaheuristic algorithms; swarm intelligence","Abc algorithms; ABC variant; Artificial bee colonies (ABC); Artificial bee colony optimization algorithms; Benchmark functions; Bio-inspired optimizations; Comparative studies; Food sources; Local optima; Meta heuristic algorithm; Optimization algorithms; Swarm Intelligence; Artificial intelligence; Algorithms",Conference Paper,Scopus,2-s2.0-84874563845
"Symons C.T., Beaver J.M.","Nonparametric semi-supervised learning for network intrusion detection: Combining performance improvements with realistic in-situ training",2012,"Proceedings of the ACM Conference on Computer and Communications Security",12,10.1145/2381896.2381905,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869830681&doi=10.1145%2f2381896.2381905&partnerID=40&md5=556c170c1f2ece58b17a925be8cbb6d5","A barrier to the widespread adoption of learning-based net- work intrusion detection tools is the in-situ training require- ments for effctive discrimination of malicious trafic. Su- pervised learning techniques necessitate a quantity of la- beled examples that is often intractable, and at best cost- prohibitive. Recent advances in semi-supervised techniques have demonstrated the ability to generalize well based on a significantly smaller set of labeled samples. In network intru- sion detection, placing reasonable requirements on the num- ber of training examples provides realistic expectations that a learning-based system can be trained in the environment where it will be deployed. This in-situ training is necessary to ensure that the assumptions associated with the learning process hold, and thereby support a reasonable belief in the generalization ability of the resulting model. In this paper, we describe the application of a carefully selected nonpara- metric, semi-supervised learning algorithm to the network intrusion problem, and compare the performance to other model types using feature-based data derived from an oper- ational network. We demonstrate dramatic performance im- provements over supervised learning and anomaly detection in discriminating real, previously unseen, malicious network trafic while generating an order of magnitude fewer false alerts than any alternative, including a signature IDS tool deployed on the same network.","Machine learning; Network intrusion detection; Nonparamet- ric; Semi-supervised","Anomaly detection; Feature-based; Generalization ability; Learning process; Learning techniques; Network intrusion detection; Network intrusions; Nonparamet- ric; Performance improvements; Semi-supervised; Semi-supervised learning; Training example; Artificial intelligence; Intrusion detection; Learning algorithms; Learning systems; Supervised learning",Conference Paper,Scopus,2-s2.0-84869830681
"Bond-Lamberty B., Bunn A.G., Thomson A.M.","Multi-Year Lags between Forest Browning and Soil Respiration at High Northern Latitudes",2012,"PLoS ONE",12,10.1371/journal.pone.0050441,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870013885&doi=10.1371%2fjournal.pone.0050441&partnerID=40&md5=58ff24a7d7196783aba93676d7d9c1f0","High-latitude northern ecosystems are experiencing rapid climate changes, and represent a large potential climate feedback because of their high soil carbon densities and shifting disturbance regimes. A significant carbon flow from these ecosystems is soil respiration (RS, the flow of carbon dioxide, generated by plant roots and soil fauna, from the soil surface to atmosphere), and any change in the high-latitude carbon cycle might thus be reflected in RS observed in the field. This study used two variants of a machine-learning algorithm and least squares regression to examine how remotely-sensed canopy greenness (NDVI), climate, and other variables are coupled to annual RS based on 105 observations from 64 circumpolar sites in a global database. The addition of NDVI roughly doubled model performance, with the best-performing models explaining ~62% of observed RS variability. We show that early-summer NDVI from previous years is generally the best single predictor of RS, and is better than current-year temperature or moisture. This implies significant temporal lags between these variables, with multi-year carbon pools exerting large-scale effects. Areas of decreasing RS are spatially correlated with browning boreal forests and warmer temperatures, particularly in western North America. We suggest that total circumpolar RS may have slowed by ~5% over the last decade, depressed by forest stress and mortality, which in turn decrease RS. Arctic tundra may exhibit a significantly different response, but few data are available with which to test this. Combining large-scale remote observations and small-scale field measurements, as done here, has the potential to allow inferences about the temporal and spatial complexity of the large-scale response of northern ecosystems to changing climate. © 2012 Bond-Lamberty et al.",,"carbon; algorithm; analytical parameters; article; canopy; climate change; ecosystem; environmental temperature; forest; latitude; machine learning; mortality; North America; plant browning; plant stress; prediction; regression analysis; soil moisture; soil respiration; statistical model; summer; Arctic Regions; Artificial Intelligence; Climate; Geography; Models, Theoretical; Soil; Trees",Article,Scopus,2-s2.0-84870013885
"Liao T.W., Kuo R.J., Hu J.T.L.","Hybrid ant colony optimization algorithms for mixed discrete-continuous optimization problems",2012,"Applied Mathematics and Computation",12,10.1016/j.amc.2012.09.064,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868468233&doi=10.1016%2fj.amc.2012.09.064&partnerID=40&md5=424cfd4b2a57a75868d83932cddab781","This paper presents three new hybrid ant colony optimization algorithms that are extended from the ACO R developed by Socha and Dorigo for solving mixed discrete-continuous constrained optimization problems. The first two hybrids, labeled ACO R-HJ and ACO R-DE, differs in philosophy with the former integrating ACO R with the effective Hooke and Jeeves local search method and the latter a cooperative hybrid between ACO R and differentia evolution. The third hybrid, labeled ACO R-DE-HJ, is the second cooperative hybrid enhanced with the Hooke and Jeeves local search. All three algorithms incorporate a method to handle mixed discrete-continuous variables and the Deb's parameterless penalty method for handling constraints. Fourteen problems selected from various domains were used for testing the performance of both algorithms. It was showed that all three algorithms greatly outperform the original ACO R in finding the exact or near global optima. An investigation was also carried out to determine the relative performance of applying local search with a fixed probability or varying probability. © 2012 Elsevier Inc. All rights reserved.","Ant colony optimization; Constrained optimization; Hybrid metaheuristic; Metaheuristic; Mixed discrete-continuous","Ant Colony Optimization (ACO); Constrained optimization problems; Globaloptimum; Handling constraints; Hybrid ant colony optimization; Local search; Local search method; Metaheuristic; Mixed discrete-continuous; Optimization problems; Penalty methods; Relative performance; Artificial intelligence; Constrained optimization; Algorithms",Article,Scopus,2-s2.0-84868468233
"Salehi Z., Ghiasi M., Sami A.","A miner for malware detection based on API function calls and their arguments",2012,"AISP 2012 - 16th CSI International Symposium on Artificial Intelligence and Signal Processing",12,10.1109/AISP.2012.6313810,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869184774&doi=10.1109%2fAISP.2012.6313810&partnerID=40&md5=b895dac47de0fc50d64ff3f651e4f73b","Since signature based methods cannot identify sophisticated malware quickly and effectively, research is moving toward using samples' runtime behavior. But these methods are often slow and have lower detection rate and are not usually used in antivirus software. In this article we introduce a scalable method that relies on utilizing features other than traditional API calls to obtain higher accuracies. Two feature categories including API names and a combination of API names and their input arguments were extracted to investigate their effect in identifying and distinguishing malware and benign applications. Feature selection techniques are then applied to reduce the number of features and enhance the analysis time. Various classifiers were then utilized along with 10-fold cross validation approach to achieve an accuracy of 98.4% with a false positive rate less than two percent in best case. The small number of extracted features in the proposed technique and the high accuracy achieved makes it an appropriate approach to be used in industrial applications. © 2012 IEEE.","API calls arguments; Behavior-based detection; Dynamic analysis; Machine learning algorithms; Malware detection; System calls","Analysis time; Antivirus softwares; API calls; API function calls; Behavior-based detection; Cross validation; Detection rates; False positive rates; Malware detection; Malwares; Runtime behaviors; Scalable methods; Selection techniques; System calls; Artificial intelligence; Dynamic analysis; Industrial applications; Intrusion detection; Learning algorithms; Learning systems; Scalability; Signal processing; Computer crime",Conference Paper,Scopus,2-s2.0-84869184774
"Kacalak W., Majewski M.","New intelligent interactive automated systems for design of machine elements and assemblies",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-34478-7_15,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869009127&doi=10.1007%2f978-3-642-34478-7_15&partnerID=40&md5=ebff2d6fbb0f83a37a15876de510b0cc","This paper presents a new concept of intelligent interactive automated systems for design of machine elements and assemblies on the basis of its features described in a natural language. In the proposed system, computational intelligence methods allow for communication by speech and handwriting, meaning analyses of design engineer's messages, analyses of constructions, encoding and assessments of constructions, CAD system controlling and visualizations. The system uses an intelligent subsystem for assessment of engineer's ability for efficient designing. It is capable of control, supervision and optimization of the designing process. The system consists of spoken natural language and handwriting interfaces between the designing system and design engineers. They are equipped with several adaptive intelligent layers for human biometric identification, recognition of speech and handwriting, recognition of words, analyses and recognition of messages, meaning analyses of messages, and assessments of human reactions. The paper also makes a comparison of the proposed new automated designing system with the present system of realization of designing tasks. In the system also proposed are new concepts of a system of symbolic notation of construction features and language for notation, archiving and processing of construction description data (object oriented language for construction). © 2012 Springer-Verlag.","Artificial Intelligence; Intelligent Designing System; Intelligent Interface; Natural Language Processing; User-Computer Interaction","Automated systems; Biometric identifications; CAD system; Computational intelligence methods; Design engineers; Designing process; Designing systems; Human reaction; Intelligent interface; Machine element; NAtural language processing; Natural languages; System use; User-Computer Interaction; Artificial intelligence; Automation; Character recognition; Computer aided design; Engineers; Natural language processing systems; Speech communication; Speech recognition",Conference Paper,Scopus,2-s2.0-84869009127
"Amelin K., Amelina N., Granichin O., Granichina O.","Multi-agent stochastic systems with switched topology and noise",2012,"Proceedings - 13th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing, SNPD 2012",12,10.1109/SNPD.2012.12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868584600&doi=10.1109%2fSNPD.2012.12&partnerID=40&md5=1540277747d3c49696be54093fb140e4","In this paper the multi-agent stochastic systems are considered under randomly switched agents communications topology and noisy information about the current states of neighbouring agents. We study the consensus problem for such systems when controls actions are formed by the local voting protocol (stochastic approximation type algorithm). This protocol is applied to the load balancing distributed computer system and to the system of Unnamed Aerial Vehicles (the UAVs system). Obtained results are important for the control properties analysis of production or logistic networks, multiprocessor or multicomputer networks, etc. © 2012 IEEE.","approximate consensus; consensus problem; multi-agent stochastic systems","Agents communication; approximate consensus; Consensus problems; Control properties; Logistic networks; Multicomputer network; Stochastic approximations; Unnamed aerial vehicles; Voting protocols; Approximation theory; Artificial intelligence; Software engineering; Topology; Stochastic control systems",Conference Paper,Scopus,2-s2.0-84868584600
"Kotoulas S., Urbani J., Boncz P., Mika P.","Robust runtime optimization and skew-resistant execution of analytical SPARQL queries on pig",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-35176-1-16,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868587217&doi=10.1007%2f978-3-642-35176-1-16&partnerID=40&md5=d5ba5f8df44847fd26abfc1121bdbf82","We describe a system that incrementally translates SPARQL queries to Pig Latin and executes them on a Hadoop cluster. This system is designed to work efficiently on complex queries with many self-joins over huge datasets, avoiding job failures even in the case of joins with unexpected high-value skew. To be robust against cost estimation errors, our system interleaves query optimization with query execution, determining the next steps to take based on data samples and statistics gathered during the previous step. Furthermore, we have developed a novel skew-resistant join algorithm that replicates tuples corresponding to popular keys. We evaluate the effectiveness of our approach both on a synthetic benchmark known to generate complex queries (BSBM-BI) as well as on a Yahoo! case of data analysis using RDF data crawled from the web. Our results indicate that our system is indeed capable of processing huge datasets without pre-computed statistics while exhibiting good load-balancing properties. © 2012 Springer-Verlag Berlin Heidelberg.",,"Complex queries; Cost estimations; Data sample; Data sets; Join algorithm; Load-Balancing; Query execution; Query optimization; RDF data; Runtime optimization; Synthetic benchmark; Artificial intelligence; Search engines",Conference Paper,Scopus,2-s2.0-84868587217
"Popescu I., Jonoski A., Bhattacharya B.","Experiences from online and classroom education in hydroinformatics",2012,"Hydrology and Earth System Sciences",12,10.5194/hess-16-3935-2012,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868514670&doi=10.5194%2fhess-16-3935-2012&partnerID=40&md5=7a6b0606d30ed3853720bc0caf7d260f","Universities and other higher education institutions involved in water-related engineering education are facing new challenges in offering lifelong learning services and online educational support. Both the curricula and the form of delivery are changing, as contemporary water problems require interdisciplinary approaches involving diverse and up to date expertise maintained via continuous professional development. Hydroinformatics education faces similar challenges in developing relevant curricula and finding appropriate combinations of course delivery to its target group. This article presents experiences from delivering two hydroinformatics courses in the fields of flood modelling for management (FMM) and decision support systems (DSS) in river basin management that in recent years have been delivered both online and in classroom settings. Comparisons between the two modes of delivery are provided, with the conclusion that online education in this field, although still faced with many challenges, has a promising potential for meeting future educational needs. © 2012 Author(s).",,"Classroom education; Classroom settings; Continuous professional development; Course delivery; Educational needs; Flood modelling; Higher education institutions; Hydroinformatics; Life long learning; On-line education; River basin management; Target group; Water problems; Artificial intelligence; Curricula; Decision support systems; Teaching; Water management; E-learning; curriculum; decision support system; education; flooding; hydrological modeling; Internet; mathematical analysis; river basin; river management; water economics; water planning",Article,Scopus,2-s2.0-84868514670
"Long M., Wang J., Ding G., Shen D., Yang Q.","Transfer learning with graph co-regularization",2012,"Proceedings of the National Conference on Artificial Intelligence",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868286853&partnerID=40&md5=770df57073ed522edb031cbb026a1dcb","Transfer learning proves to be effective for leveraging labeled data in the source domain to build an accurate classifier in the target domain. The basic assumption behind transfer learning is that the involved domains share some common latent factors. Previous methods usually explore these latent factors by optimizing two separate objective functions, i.e., either maximizing the empirical likelihood, or preserving the geometric structure. Actually, these two objective functions are complementary to each other and optimizing them simultaneously can make the solution smoother and further improve the accuracy of the final model. In this paper, we propose a novel approach called Graph co-regularized Transfer Learning (GTL) for this purpose, which integrates the two objective functions seamlessly into one unified optimization problem. Thereafter, we present an iterative algorithm for the optimization problem with rigorous analysis on convergence and complexity. Our empirical study on two open data sets validates that GTL can consistently improve the classification accuracy compared to the state-of-the-art transfer learning methods. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Classification accuracy; Data sets; Empirical likelihood; Empirical studies; Geometric structure; Iterative algorithm; Labeled data; Latent factor; Objective functions; Optimization problems; Rigorous analysis; Target domain; Transfer learning; Algorithms; Classification (of information); Optimization; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868286853
"Banerjee B., Lyle J., Kraemer L., Yellamraju R.","Sample bounded distributed reinforcement learning for decentralized POMDPs",2012,"Proceedings of the National Conference on Artificial Intelligence",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868275593&partnerID=40&md5=578fcbbb4e3b43a71ae25437e3b48047","Decentralized partially observable Markov decision processes (Dec-POMDPs) offer a powerful modeling technique for realistic multi-agent coordination problems under uncertainty. Prevalent solution techniques are centralized and assume prior knowledge of the model. We propose a distributed reinforcement learning approach, where agents take turns to learn best responses to each other's policies. This promotes decentralization of the policy computation problem, and relaxes reliance on the full knowledge of the problem parameters. We derive the relation between the sample complexity of best response learning and error tolerance. Our key contribution is to show that sample complexity could grow exponentially with the problem horizon. We show empirically that even if the sample requirement is set lower than what theory demands, our learning approach can produce (near) optimal policies in some benchmark Dec-POMDP problems. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Best response; Computation problems; Distributed reinforcement learning; Error tolerance; Learning approach; Modeling technique; Multi-agent coordinations; Optimal policies; Partially observable Markov decision process; Prior knowledge; Problem parameters; Sample complexity; Solution techniques; Uncertainty analysis; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868275593
"Zech P., Felderer M., Kalb P., Breu R.","A generic platform for model-based regression testing",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-34026-0_9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868277587&doi=10.1007%2f978-3-642-34026-0_9&partnerID=40&md5=33c651ab41a99c215c4e8259ec3b2c8e","Model-based testing has gained widespread acceptance in the last few years. Models enable the platform independent analysis and design of tests in an early phase of software development resulting in effort reduction in terms of time and money. Furthermore, test models are easier to maintain than test code when software systems evolve due to their platform independence and traceability support. Nevertheless, most regression testing approaches, which ensure that system evolution does not introduce unintended effects, are solely code-based. Additionally, many model-based testing approaches do not consider regression testing when applied in practice, mainly due to the lack of appropriate tool support. Therefore, in this paper we present a generic tool platform for model-based regression testing based on the model versioning and evolution framework MoVE. Our approach enhances existing model-based testing approaches with regression testing capabilities aiming at better tool support for model-based regression testing. In a case study, we apply our platform to the model-based testing approaches UML Testing Profile and Telling TestStories. © 2012 Springer-Verlag.",,"Generic platforms; Model based testing; Platform independence; Platform independent; Regression testing; Software systems; System evolution; Test code; Test models; Testing profile; Tool support; Versioning; Artificial intelligence; Software testing",Conference Paper,Scopus,2-s2.0-84868277587
"Elkawkagy M., Bercher P., Schattenberg B., Biundo S.","Improving hierarchical planning performance by the use of landmarks",2012,"Proceedings of the National Conference on Artificial Intelligence",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868268231&partnerID=40&md5=2d28dea16d033de4f0d81a3aa63d0aff","In hierarchical planning, landmarks are tasks that occur on every search path leading from the initial plan to a solution. In this work, we present novel domain-independent planning strategies based on such hierarchical landmarks. Our empirical evaluation on four benchmark domains shows that these landmark-aware strategies outperform established search strategies in many cases. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Benchmark domains; Domain-independent planning; Empirical evaluations; Hierarchical planning; Search strategies; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868268231
"Scheffler T., Schirru R., Lehmann P.","Matching points of interest from different social networking sites",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-33347-7_24,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868223172&doi=10.1007%2f978-3-642-33347-7_24&partnerID=40&md5=fe0324ef973873657a5792640395a52e","Valuable user-generated information about locations (points of interest, POIs) is stored in various online social media platforms. Merging the data associated with one POI is hard because the platforms lack common identifiers. In addition, user-generated data is commonly faulty or contradictory. Here we present an approach matching POIs from Qype and Facebook Places to their counterparts in OpenStreetMap. The algorithm uses different similarity measures taking the geographic distance of POIs into account as well as the string similarity of selected metadata fields, showing good results. © 2012 Springer-Verlag.","Data Integration; Points of Interest; Social Networks; User-Generated Content","Data integration; Facebook; Matching points; Points of interest; Similarity measure; Social media; Social networking sites; Social Networks; String similarity; User-generated content; Artificial intelligence; Metadata; Potassium iodide; Social networking (online)",Conference Paper,Scopus,2-s2.0-84868223172
"Hampson C., Agosti M., Orio N., Bailey E., Lawless S., Conlan O., Wade V.","The CULTURA project: Supporting next generation interaction with digital cultural heritage collections",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-34234-9_70,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868013361&doi=10.1007%2f978-3-642-34234-9_70&partnerID=40&md5=7fe0bf8622a73aca4ce7f3be0754d1cb","In recent years there has been a marked uptake in the digitisation of cultural heritage collections. Though this has enabled more sources to be made available to experts and the wider public, curators still struggle to instigate and enhance engagement with cultural archives. This is largely due to the monolithic nature of many digital archives; the challenge of understanding large collections, especially if the language is inconsistent; and because users vary in expertise and have different tasks and goals that they are trying to accomplish. This paper describes CULTURA, an FP7 funded project that is addressing these specific issues. The various technologies and approaches being used by CULTURA are discussed, along with the lessons learnt thus far. © 2012 Springer-Verlag Berlin Heidelberg.","Adaptation; CULTURA; Digital Humanities; Influence Network Analysis; Personalisation; Social Network Analysis; Text Normalisation","Adaptation; CULTURA; Digital Humanities; Influence networks; Normalisation; Personalisation; Social Network Analysis; Artificial intelligence; Social networking (online)",Conference Paper,Scopus,2-s2.0-84868013361
"Jiroušek R.","Local computations in Dempster-Shafer theory of evidence",2012,"International Journal of Approximate Reasoning",12,10.1016/j.ijar.2012.06.012,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866041107&doi=10.1016%2fj.ijar.2012.06.012&partnerID=40&md5=a65ebdb3a63580fc0aa5d058a6145d87","When applying any technique of multidimensional models to problems of practice, one always has to cope with two problems: the necessity to represent the models with a ""reasonable"" number of parameters and to have sufficiently efficient computational procedures at one's disposal. When considering graphical Markov models in probability theory, both of these conditions are fulfilled; various computational procedures for decomposable models are based on the ideas of local computations, whose theoretical foundations were laid by Lauritzen and Spiegelhalter. The presented contribution studies a possibility of transferring these ideas from probability theory into Dempster-Shafer theory of evidence. The paper recalls decomposable models, discusses connection of the model structure with the corresponding system of conditional independence relations, and shows that under special additional conditions, one can locally compute specific basic assignments which can be considered to be conditional. © 2012 Elsevier Inc. All rights reserved.","Belief network; Composition operator; Computational complexity; Conditional independence; Factorisation; Graphical model","Composition operators; Computational procedures; Conditional independences; Dempster-shafer theory of evidence; GraphicaL model; Local computation; Markov model; Multi-dimensional model; Probability theory; Theoretical foundations; Artificial intelligence; Bayesian networks; Computational complexity; Factorization; Software engineering; Markov processes",Conference Paper,Scopus,2-s2.0-84866041107
"Mermillod M., Bonin P., Méot A., Ferrand L., Paindavoine M.","Computational evidence that frequency trajectory theory does not oppose but emerges from age-of-acquisition theory",2012,"Cognitive Science",12,10.1111/j.1551-6709.2012.01266.x,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868318316&doi=10.1111%2fj.1551-6709.2012.01266.x&partnerID=40&md5=80e685c5f4f14aee0f0cbee1fc434643","According to the age-of-acquisition hypothesis, words acquired early in life are processed faster and more accurately than words acquired later. Connectionist models have begun to explore the influence of the age/order of acquisition of items (and also their frequency of encounter). This study attempts to reconcile two different methodological and theoretical approaches (proposed by Lambon Ralph & Ehsan, 2006 and Zevin & Seidenberg, 2002) to age-limited learning effects. The current simulations extend the findings reported by Zevin and Seidenberg (2002) that have shown that frequency trajectories (FTs) have limited and specific effects on word-reading tasks. Using the methodological framework proposed by Lambon Ralph and Ehsan (2006), which makes it possible to compare word-reading and picture-naming tasks in connectionist networks, we were able to show that FT has a considerable influence on age-limited learning effects in a picture naming task. The findings show that when the input-output mappings are arbitrary (simulating picture naming tasks), the links formed by the network become entrenched as a result of early experience and that subsequent variations in frequency of exposure of the items have only a minor impact. In contrast, when the mappings between input-output are quasi-systematic or systematic (simulating word-reading tasks), the training of new items was generalized and resulted in the suppression of age-limited learning effects. At a theoretical level, we suggest that FT, which simultaneously takes account of time and the level of exposure across time, represents a more precise and modulated measure compared with the order of introduction of the items and may lead to innovative hypotheses in the field of age-limited learning effects. © 2012 Cognitive Science Society, Inc.","Age of acquisition; Arbitrary mappings; Frequency trajectory; Quasi-systematic/systematic mappings","age; article; artificial intelligence; artificial neural network; human; language development; linguistics; psychological model; reading; time; Age Factors; Artificial Intelligence; Humans; Language Development; Models, Psychological; Neural Networks (Computer); Psycholinguistics; Reading; Time Factors",Article,Scopus,2-s2.0-84868318316
"De Castro M.F., Ribeiro L.B., Oliveira C.H.S.","An autonomic bio-inspired algorithm for wireless sensor network self-organization and efficient routing",2012,"Journal of Network and Computer Applications",12,10.1016/j.jnca.2012.07.023,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867545901&doi=10.1016%2fj.jnca.2012.07.023&partnerID=40&md5=2460e92a7a3fa5d2db27dfa1720411d5","Self-configuration and autonomy are key features required for the next generation of gadgets and networks, since regular users are willing to have computationally enabled devices pervasively spread into their environment. Sensors are among the most promising devices into this new scenario. However, their low battery and processing power raise several issues to these autonomy requirements. This paper presents BiO4SeL (Bio-Inspired Optimization for Sensor Network Lifetime), a swarm intelligence-based algorithm to perform self-organization and optimization of lifetime by means of routing into a Wireless Sensor Network. Results show that BiO4SeL achieves its objectives when compared to similar approaches: ARAMA (Ant-based Routing Algorithm for MANETs), EAR (Energy-Aware Routing) and AODV (Ad-hoc On-demand Distance Vector). © 2012 Elsevier Ltd. All rights reserved.","Ant system optimization; Autonomic computing; Bio-inspired systems; Swarm intelligence; Wireless sensor networks","Ad hoc on demand distance vector; Ant based routing; Ant systems; Autonomic Computing; Bio-inspired; Bio-inspired algorithms; Bioinspired systems; Efficient routing; Energy-aware routing; Key feature; Network lifetime; Processing power; Self configuration; Swarm Intelligence; Algorithms; Artificial intelligence; Mobile ad hoc networks; Optimization; Routers; Wireless sensor networks; Next generation networks",Article,Scopus,2-s2.0-84867545901
"Gottlob G.","On minimal constraint networks",2012,"Artificial Intelligence",12,10.1016/j.artint.2012.07.006,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867553621&doi=10.1016%2fj.artint.2012.07.006&partnerID=40&md5=fcdaa1e6fba04997bea45a8587887fe1","In a minimal binary constraint network, every tuple of a constraint relation can be extended to a solution. The tractability or intractability of computing a solution to such a minimal network was a long standing open question. Dechter conjectured this computation problem to be NP-hard. We prove this conjecture. We also prove a conjecture by Dechter and Pearl stating that for k≥2 it is NP-hard to decide whether a single constraint can be decomposed into an equivalent k-ary constraint network. We show that this holds even in case of bi-valued constraints where k≥3, which proves another conjecture of Dechter and Pearl. Finally, we establish the tractability frontier for this problem with respect to the domain cardinality and the parameter k. © 2012 Elsevier B.V. All Rights Reserved.","Complexity; Constraints; Database theory; Join decomposition; Knowledge compilation; Minimal network; Structure identification","Complexity; Constraints; Data base theory; Knowledge compilation; Structure identification; Artificial intelligence; Computational complexity",Article,Scopus,2-s2.0-84867553621
"Ganapathy S., Yogesh P., Kannan A.","Intelligent agent-based intrusion detection system using enhanced multiclass SVM",2012,"Computational Intelligence and Neuroscience",12,10.1155/2012/850259,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867755215&doi=10.1155%2f2012%2f850259&partnerID=40&md5=281b380acce70e6078e17a140a7b8360","Intrusion detection systems were used in the past along with various techniques to detect intrusions in networks effectively. However, most of these systems are able to detect the intruders only with high false alarm rate. In this paper, we propose a new intelligent agent-based intrusion detection model for mobile ad hoc networks using a combination of attribute selection, outlier detection, and enhanced multiclass SVM classification methods. For this purpose, an effective preprocessing technique is proposed that improves the detection accuracy and reduces the processing time. Moreover, two new algorithms, namely, an Intelligent Agent Weighted Distance Outlier Detection algorithm and an Intelligent Agent-based Enhanced Multiclass Support Vector Machine algorithm are proposed for detecting the intruders in a distributed database environment that uses intelligent agents for trust management and coordination in transaction processing. The experimental results of the proposed model show that this system detects anomalies with low false alarm rate and high-detection rate when tested with KDD Cup 99 data set. © 2012 S. Ganapathy et al.",,"Agent based; Attribute selection; Data sets; Detection accuracy; Distributed database; False alarm rate; Intrusion detection models; Intrusion Detection Systems; Multi-class support vector machines; Multiclass SVM; Outlier Detection; Outlier detection algorithm; Preprocessing techniques; Processing time; Transaction processing; Trust management; Weighted distance; Algorithms; Computer crime; Errors; Financial data processing; Intrusion detection; Mobile ad hoc networks; Statistics; Support vector machines; Telecommunication networks; Intelligent agents; algorithm; article; artificial intelligence; automated pattern recognition; computer network; methodology; support vector machine; theoretical model; Algorithms; Artificial Intelligence; Computer Communication Networks; Models, Theoretical; Pattern Recognition, Automated; Support Vector Machines",Article,Scopus,2-s2.0-84867755215
"Kokkinos I.","Bounding part scores for rapid detection with deformable part models",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-33885-4_5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867727461&doi=10.1007%2f978-3-642-33885-4_5&partnerID=40&md5=fc06fe915787e63fdd8cd7dc8cc0db07","Computing part scores is the main computational bottleneck in object detection with Deformable Part Models. In this work we introduce an efficient method to obtain bounds on part scores, which we then integrate with deformable model detection. As in [1] we rapidly approximate the inner product between a weight vector and HOG-based features by quantizing the HOG cells onto a codebook and replace their inner product with the lookup of a precomputed score. The novelty in our work consists in combining this lookup-based estimate with the codebook quantization error so as to construct probabilistic bounds to the exact inner product. In particular we use Chebyshev's inequality to obtain probably correct bounds for the inner product at each image location. We integrate these bounds with both the Dual-Tree Branch-and-Bound work of [2,3] and the Cascade-DPMs of [4]; in both cases the bounds are used in a first phase to conservatively construct a short-list of locations, for which the exact inner products are subsequently evaluated. We quantitatively evaluate our method and demonstrate that it allows for approximately a twofold speedup over both [2] and [4] with negligible loss in accuracy. © 2012 Springer-Verlag.",,"Branch and bounds; Chebyshev's inequality; Codebooks; Computational bottlenecks; Deformable models; Dual-tree; Image locations; Inner product; Lookups; Object Detection; Probabilistic bounds; Quantization errors; Rapid detection; Weight vector; Artificial intelligence; Computer vision",Conference Paper,Scopus,2-s2.0-84867727461
"Menelas B.-A.J., Otis M.J.D.","A serious game for training balance control over different types of soil",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-33687-4_3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867613511&doi=10.1007%2f978-3-642-33687-4_3&partnerID=40&md5=e7d19fb35cb1c548176507ba8961a933","It is known that the type of the soil can affect balance. Here we report a serious game designed for training users at maintaining balance over five types of soil (broken stone, stone dust, sand, concrete and wood). By using an augmented shoe and proposed navigation metaphor, in this game, the user is invited to browse a maze while standing balance over the physical grounds. During the exploration, exercises targeting assessment of balance control are suggested. To insure the effectiveness of this training program, four exercises based on the Berg Balance Scale and the Tinetti Balance Assessment Tool are incorporated in the game. © 2012 Springer-Verlag.",,"Assessment tool; Balance control; Berg balance scale; Serious games; Training program; Artificial intelligence; Soils",Conference Paper,Scopus,2-s2.0-84867613511
"Van Welbergen H., Reidsma D., Kopp S.","An incremental multimodal realizer for behavior co-articulation and coordination",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-33197-8-18,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867567890&doi=10.1007%2f978-3-642-33197-8-18&partnerID=40&md5=c7f790c78cb944724fc746fad3333b8c","Human conversations are highly dynamic, responsive interactions. To enter into flexible interactions with humans, a conversational agent must be capable of fluent incremental behavior generation. New utterance content must be integrated seamlessly with ongoing behavior, requiring dynamic application of co-articulation. The timing and shape of the agent's behavior must be adapted on-the-fly to the interlocutor, resulting in natural interpersonal coordination. We present AsapRealizer, a BML 1.0 behavior realizer that achieves these capabilities by building upon, and extending, two state of the art existing realizers, as the result of a collaboration between two research groups. © 2012 Springer-Verlag Berlin Heidelberg.",,"Behavior generation; Co-articulation; Conversational agents; Dynamic applications; Multi-modal; On-the-fly; Research groups; Two-state; Artificial intelligence; Intelligent virtual agents",Conference Paper,Scopus,2-s2.0-84867567890
"Wong C.H., Siew Z.W., Tan M.K., Chin R.K.Y., Teo K.T.K.","Optimization of distributed and collaborative beamforming in wireless sensor networks",2012,"Proceedings - 2012 4th International Conference on Computational Intelligence, Communication Systems and Networks, CICSyN 2012",12,10.1109/CICSyN.2012.26,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867363318&doi=10.1109%2fCICSyN.2012.26&partnerID=40&md5=116a68617c4ca263071503e127755516","Collaborate beamforming in wireless sensor networks (WSNs) is a concept of using beamforming technology to establish link in the networks. It can effectively increase the transmission distance and improve the energy efficiency of the networks. Due to random deployment of the sensor nodes in the networks, proper assignment for the sensor nodes in wireless sensor networks is vital to achieve better array pattern synthesis. In this paper, a node selection method based on uniform space linear array synthesis is presented. A virtual line was constructed in the network topology as a reference guide to optimize the selection of nodes to mimic the uniform space linear array. The node selection for collaborate beamforming is further optimized using genetic algorithm. Using the method, simulation results show an improvement of radiation beam pattern. © 2012 IEEE.","collaborate beamforming; component; genetic algorithm; wireless sensor networks","Array pattern synthesis; Collaborative beamforming; component; Linear arrays; Network topology; Node selection; Radiation beams; Random deployment; Reference guides; Transmission distances; Uniform spaces; Wireless sensor network (WSNs); Artificial intelligence; Beamforming; Communication systems; Directional patterns (antenna); Electric network topology; Energy efficiency; Genetic algorithms; Optimization; Wireless sensor networks; Sensor nodes",Conference Paper,Scopus,2-s2.0-84867363318
"Dong H., Huang M., Ip W.H., Wang X.","Improved variable neighbourhood search for integrated tundish planning in primary steelmaking processes",2012,"International Journal of Production Research",12,10.1080/00207543.2011.624563,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-83455263997&doi=10.1080%2f00207543.2011.624563&partnerID=40&md5=863edda9da672ec8a7f3dd057b4004ac","Production planning (or product design) in the steel industry needs specific, sophisticated procedures in order to guarantee competitive plant performance. This paper describes an integrated tundish planning problem, considering the steelmaking-continuous casting-hot rolling and other downstream integrated technical constraints, and a multi-objective optimisation model is proposed with the objective to optimise the number of tundish, the additional cost of technical operations and the throughput balance to each flow. Also, instead of using traditional metaheuristic algorithm or artificial intelligence (AI)-based heuristic approaches, this paper develops two new approaches, the improved variable neighbourhood descent (IVND) search method and improved reduced variable neighbourhood search (IRVNS) method, by introducing the iterated local search into local search to the problem described above. The performance of IVND and IRVNS are analysed based on changing the number of local iteration and weights of objective function, these two algorithms are also compared with tabu search(TS) and heuristic method based on numeral analysis of the actual data, and the results show that the model and algorithm are feasible and efficient. © 2012 Copyright Taylor and Francis Group, LLC.","integrated tundish planning; multi-objective optimisation; reduced variable neighbourhood search; variable neighbourhood descend search","Additional costs; Heuristic approach; Iterated local search; Local search; Meta heuristic algorithm; Neighbourhood; Objective functions; Planning problem; Plant performance; Production Planning; Search method; Steelmaking process; Technical constraints; Technical operations; Tundish; Variable neighbourhood descent; Variable neighbourhood search; Algorithms; Artificial intelligence; Continuous casting; Heuristic methods; Hot rolling; Multiobjective optimization; Product design; Production control; Steelmaking; Tabu search; Iterative methods",Article,Scopus,2-s2.0-83455263997
"Zhai Y., Tan M., Tsang I.W., Ong Y.-S.","Discovering support and affiliated features from very high dimensions",2012,"Proceedings of the 29th International Conference on Machine Learning, ICML 2012",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867122131&partnerID=40&md5=91c37cc78a3804356431e5fa9ebc8d6c","In this paper, a novel learning paradigm is presented to automatically identify groups of informative and correlated features from very high dimensions. Specifically, we explicitly incorporate correlation measures as constraints and then propose an efficient embedded feature selection method using recently developed cutting plane strategy. The benefits of the proposed algorithm are two-folds. First, it can identify the optimal discriminative and uncorrected feature subset to the output labels, denoted here as Support Features, which brings about significant improvements in prediction performance over other state of the art feature selection methods considered in the paper. Second, during the learning process, the underlying group structures of correlated features associated with each support feature, denoted as Affiliated Features, can also be discovered without any additional cost. These affiliated features serve to improve the interpretations on the learning tasks. Extensive empirical studies on both synthetic and very high dimensional real-world datasets verify the validity and efficiency of the proposed method. Copyright 2012 by the author(s)/owner(s).",,"Additional costs; Correlation measures; Cutting planes; Empirical studies; Feature selection methods; Feature subset; Group structure; High dimensions; High-dimensional; Learning paradigms; Learning process; Learning tasks; Prediction performance; Real-world datasets; State of the art; Artificial intelligence; Software engineering; Learning systems",Conference Paper,Scopus,2-s2.0-84867122131
"Bischof J.M., Airoldi E.M.","Summarizing topical content with word frequency and exclusivity",2012,"Proceedings of the 29th International Conference on Machine Learning, ICML 2012",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867113614&partnerID=40&md5=10188a55af81454c4562e396e1b614e6","Recent work in text analysis commonly describes topics in terms of their most frequent words, but the exclusivity of words to topics is equally important for communicating content. We introduce Hierarchical Poisson Convolution (HPC), a model which infers regularized estimates of the differential use of words across topics as well as their frequency within topics. HPC uses known hierarchical structure on human-labeled topics to make focused comparisons of differential usage within each branch of the hierarchy of labels. We then infer a summary for each topic in terms of words that are both frequent and exclusive. We develop a parallelized Hamiltonian Monte Carlo sampler that allows for fast and scalable computation. Copyright 2012 by the author(s)/owner(s).",,"Hierarchical structures; MONTE CARLO; Text analysis; Word frequencies; Artificial intelligence; Software engineering; Learning systems",Conference Paper,Scopus,2-s2.0-84867113614
"Clark P.G., Grzymala-Busse J.W.","Rule induction using probabilistic approximations and data with missing attribute values",2012,"Proceedings of the IASTED International Conference on Artificial Intelligence and Soft Computing, ASC 2012",12,10.2316/P.2012.777-028,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867035031&doi=10.2316%2fP.2012.777-028&partnerID=40&md5=42d4ce384e792f15ef1a409939a6f7c9","This paper presents results of experiments on rule induction from incomplete data (data with missing attribute values) using probabilistic approximations. Such approximations, broadly studied for many years, are fundamental concepts of variable precision rough set theory and similar models to deal with inconsistent data sets. Our main objective was to study how useful are probabilistic approximations that are different from ordinary lower and upper approximations. Our results are rather pessimistic: for eight data sets with two types of missing attribute values, in only one case out of 16 some of such probabilistic approximations were better than ordinary approximations. On the other hand, in another case, some probabilistic approximations were worse than ordinary approximations. Additionally, we studied how many different probabilistic approximations may exist for a given concept of a data set.","Data mining; Incomplete data; Parameterized approximations; Probabilistic approximations; Rough set theory; Rule induction","Attribute values; Data sets; Fundamental concepts; Incomplete data; Inconsistent data; Lower and upper approximations; Parameterized; Probabilistic approximation; Rule induction; Similar models; Variable precision rough sets; Artificial intelligence; Data mining; Soft computing; Rough set theory",Conference Paper,Scopus,2-s2.0-84867035031
"Dissanayake S.D., Armstrong J., Hranilovic S.","Performance analysis of noise cancellation in a diversity combined ACO-OFDM system",2012,"International Conference on Transparent Optical Networks",12,10.1109/ICTON.2012.6253910,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867026797&doi=10.1109%2fICTON.2012.6253910&partnerID=40&md5=245058cf6196bbc5fe294f7a4d57a7f5","In this paper we analyze the combined performance of noise-cancellation and diversity-combining in an ACO-OFDM system. Noise cancellation and diversity combining are two techniques which have each been shown to improve the performance of ACO-OFDM, each giving a maximum improvement of 3dB. As noise cancellation reduces the noise at the detection point, while diversity combining adds two different signal components, it might be expected that combining the two techniques could give further performance gains. In this paper we show analytically that no additional performance gain can be achieved by combining the techniques. Simulation results are presented which are consistent with the analysis. It is also observed that the noise cancellation is more computationally efficient than diversity combining. © 2012 IEEE.","ACO-OFDM; anti-symmetry; diversity combined ACO-OFDM; noise cancellation ACO-OFDM","ACO-OFDM; anti-symmetry; Computationally efficient; Detection point; diversity combined ACO-OFDM; Diversity combining; Noise cancellation; Performance analysis; Performance Gain; Signal components; Artificial intelligence; Orthogonal frequency division multiplexing; Spurious signal noise; Telecommunication systems; Transparent optical networks; Signal denoising",Conference Paper,Scopus,2-s2.0-84867026797
"Stocker R., Dennis L., Dixon C., Fisher M.","Verifying Brahms human-robot teamwork models",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-33353-8_30,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866909744&doi=10.1007%2f978-3-642-33353-8_30&partnerID=40&md5=cb74110e917bb590bec38449a92ff79f","Collaboration between robots and humans is an increasingly important aspect of industrial and scientific settings. In addition, significant effort is being put into the development of robot helpers for more general use in the workplace, at home, and in health-care environments. However, before such robots can be fully utilised, a comprehensive analysis of their safety is necessary. Formal verification techniques are regularly used to exhaustively assess system behaviour. Our aim is to apply such techniques to Brahms, a human-agent-robot modelling language. We show how to translate from Brahms scenarios, using a formal semantics for Brahms, into the input language of a model checker. We illustrate the approach by defining, translating, and verifying a domestic robot helper example. © 2012 Springer-Verlag.",,"Comprehensive analysis; Domestic robots; Formal Semantics; Formal verifications; Model checker; Modelling language; Artificial intelligence; Home health care; Model checking; Robots",Conference Paper,Scopus,2-s2.0-84866909744
"Dao-Tran M., Eiter T., Fink M., Weidinger G., Weinzierl A.","OMiGA: An open minded grounding on-the-fly answer set solver",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-33353-8_38,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866906200&doi=10.1007%2f978-3-642-33353-8_38&partnerID=40&md5=b6cdab70feca065ae7dfc4928a3e4cba","We present a new solver for Answer-Set Programs whose main features include grounding on-the-fly and readiness for use in solving distributed answer-set programs. The solver is implemented in Java and uses an underlying Rete network for propagation. Initial experimental results show the benefit of using Rete for this purpose, but also exhibit the need for learning in the presence of grounding on-the-fly. © 2012 Springer-Verlag.",,"Answer set; On-the-fly; Rete network; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84866906200
"Slota M., Leite J.","A unifying perspective on knowledge updates",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-33353-8_29,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866890486&doi=10.1007%2f978-3-642-33353-8_29&partnerID=40&md5=b0d0fc64192c6d34e9a49135a52bcfcd","We introduce an abstract update framework based on viewing a knowledge base as the set of sets of models of its elements and performing updates by introducing additional interpretations - exceptions - to the sets of models of elements of the original knowledge base. In [36], an instantiation of this framework for performing rule updates has been shown to semantically characterise one of the syntax-based rule update semantics. In this paper we show that the framework can also capture a wide range of both model- and formula-based belief update operators which constitute the formal underpinning of existing approaches to ontology updates. Exception-driven operators thus form a unifying perspective on both ontology and rule updates, opening new possibilities for addressing updates of hybrid knowledge bases consisting of both an ontology and a rule component. © 2012 Springer-Verlag.",,"Knowledge base; Knowledge basis; Knowledge update; Update operators; Update semantics; Knowledge based systems; Semantics; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84866890486
"Erenel Z., Altnçay H.","Nonlinear transformation of term frequencies for term weighting in text categorization",2012,"Engineering Applications of Artificial Intelligence",12,10.1016/j.engappai.2012.06.013,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866732584&doi=10.1016%2fj.engappai.2012.06.013&partnerID=40&md5=00a93c3aa15c49f33f8da5dfbff9d549","In automatic text categorization, the influence of features on the decision is set by the term weights which are conventionally computed as the product of term frequency and collection frequency factors. The raw form of term frequencies or their logarithmic forms are generally used as the term frequency factor whereas the leading collection frequency factors take into account the document frequency of each term. In this study, it is firstly shown that the best-fitting form of the term frequency factor depends on the distribution of term frequency values in the dataset under concern. Taking this observation into account, a novel collection frequency factor is proposed which considers term frequencies. Five datasets are firstly tested to show that the distribution of term frequency values is task dependent. The proposed method is then proven to provide better F 1 scores compared to two recent approaches on majority of the datasets considered. It is confirmed that the use of term frequencies in the collection frequency factor is beneficial on tasks which does not involve highly repeated terms. It is also shown that the best F 1 scores are achieved on majority of the datasets when smaller number of features are considered. © 2012 Elsevier Ltd. All rights reserved.","Collection frequency factor; Document length normalization; Term frequency; Term weighting; Text categorization","Collection frequency; Document length normalization; Term Frequency; Term weighting; Text categorization; Applications; Artificial intelligence; Text processing",Article,Scopus,2-s2.0-84866732584
"Schillings C., Wanderer T., Cameron L., van der Wal J.T., Jacquemin J., Veum K.","A decision support system for assessing offshore wind energy potential in the North Sea",2012,"Energy Policy",12,10.1016/j.enpol.2012.06.056,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865590669&doi=10.1016%2fj.enpol.2012.06.056&partnerID=40&md5=c3b4db6ef79c006ba8b18376b7aaea74","Offshore wind energy (OWE) in the North Sea has the potential to meet large share of Europe's future electricity demand. To deploy offshore wind parks in a rational way, the overall OWE potential has to be realistically determined. This has to be done on an international, cross-border level and by taking into account the existing man-made and nature-related uses of the North Sea. As spatial conflicts will arise between existing uses and the new OWE uses, a Decision Support System (DSS) based on a Geographic Information System (GIS) was developed. Based on data of existing sea uses and calculation rules for spatial prioritisation analysis, the DSS helps in identifying areas that are (1) generally suitable for offshore wind power, (2) strictly excluded or (3) negotiable with respect to other existing sea uses. The combination of this conflict analysis together with cost assumptions for offshore wind farms and their expected electricity yield leads to identification of favourable areas for OWE deployment in the North Sea. This approach helps to reduce the conflict between offshore wind deployment and existing sea uses in the North Sea for future planning. The results can assist decision makers in developing transnational roadmaps for OWE. © 2012 Elsevier Ltd.","Decision support system; North Sea; Offshore wind energy","Conflict analysis; Cross-border; Decision makers; Electricity demands; Future planning; North Sea; Off-shore wind energy; Off-shore wind parks; Offshore wind farms; Offshore wind power; Offshore winds; Road-maps; Artificial intelligence; Decision support systems; Geographic information systems; Wind power; decision making; decision support system; demand analysis; electricity supply; energy planning; GIS; offshore engineering; spatial analysis; wind farm; wind power; Atlantic Ocean; Europe; North Sea",Article,Scopus,2-s2.0-84865590669
"Kotarski W., Gdawiec K., Lisowska A.","Polynomiography via Ishikawa and Mann iterations",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-33179-4_30,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866658054&doi=10.1007%2f978-3-642-33179-4_30&partnerID=40&md5=439a76839f1c1a808c91931e631edea7","The aim of this paper is to present some modifications of the complex polynomial roots finding visualization process. In this paper Ishikawa and Mann iterations are used instead of the standard Picard iteration. The name polynomiography was introduced by Kalantari for that visualization process and the obtained images are called polynomiographs. Polynomiographs are interesting both from educational and artistic points of view. By the use of different iterations we obtain quite new polynomiographs that look aestheatically pleasing comparing to the ones from standard Picard iteration. As examples we present some polynomiographs for complex polynomial equation z 3 - 1 = 0, permutation and doubly stochastic matrices. We believe that the results of this paper can inspire those who may be interested in created automatically aesthetic patterns. They also can be used to increase functionality of the existing polynomiography software. © 2012 Springer-Verlag.",,"Doubly stochastic; Mann iteration; Picard iteration; Polynomial equation; Polynomial roots; Visualization process; Artificial intelligence; Polynomials",Conference Paper,Scopus,2-s2.0-84866658054
"Carmody M., Yulius A., Edwall D., Lee D., Piquette E., Jacobs R., Benson D., Stoltz A., Markunas J., Almeida A., Arias J.","Recent progress in MBE growth of CdTe and HgCdTe on (211)B GaAs substrates",2012,"Journal of Electronic Materials",12,10.1007/s11664-012-2129-z,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867333198&doi=10.1007%2fs11664-012-2129-z&partnerID=40&md5=d54da925c9f0b97add08d4e513d5c638","Alternate substrates for molecular beam epitaxy growth of HgCdTe including Si, Ge, and GaAs have been under development for more than a decade. MBE growth of HgCdTe on GaAs substrates was pioneered by Teledyne Imaging Sensors (TIS) in the 1980s. However, recent improvements in the layer crystal quality including improvements in both the CdTe buffer layer and the HgCdTe layer growth have resulted in GaAs emerging as a strong candidate for replacement of bulk CdZnTe substrates for certain infrared imaging applications. In this paper the current state of the art in CdTe and HgCdTe MBE growth on (211)B GaAs and (211) Si at TIS is reviewed. Recent improvements in the CdTe buffer layer quality (double crystal rocking curve full-width at half-maximum ≈ 30 arcsec) with HgCdTe dislocation densities of ≤10 6 cm -2 are discussed and comparisons are made with historical HgCdTe on bulk CdZnTe and alternate substrate data at TIS. Material properties including the HgCdTe majority carrier mobility and dislocation density are presented as a function of the CdTe buffer layer quality. © 2012 TMS.","Alternate substrate; GaAs; HgCdTe; MBE","CdTe; CdZnTe substrate; Dislocation densities; Double crystal rocking curves; GaAs; GaAs substrates; HgCdTe; Imaging applications; Imaging sensors; Layer crystals; Layer growth; Majority carriers; Material property; MBE growth; Recent progress; State of the art; Teledyne; Artificial intelligence; Buffer layers; Cadmium alloys; Cadmium compounds; Cadmium telluride; Gallium arsenide; Germanium; Infrared imaging; Mercury compounds; Molecular beam epitaxy; Semiconducting gallium; Silicon; Substrates",Conference Paper,Scopus,2-s2.0-84867333198
"Kowalski R., Sadri F.","A logic-based framework for reactive systems",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-32689-9_1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866722009&doi=10.1007%2f978-3-642-32689-9_1&partnerID=40&md5=2a0850aaff6eb634eca1c93e34a02d55","We sketch a logic-based framework in which computation consists of performing actions to generate a sequence of states, with the purpose of making a set of reactive rules in the logical form antecedents ( consequents all true. The antecedents of the rules are conjunctions of past or present conditions and events, and the consequents of the rules are disjunctions of conjunctions of future conditions and actions. The antecedents can be viewed as complex/composite events, and the consequents as complex/composite/macro actions or processes. States are represented by sets of atomic sentences, and can be viewed as global variables, relational databases, Herbrand models, or mental representations of the real world. Events, including actions, transform one state into another. The operational semantics maintains only a single, destructively updated current state, whereas the model-theoretic semantics treats the entire sequence of states, events and actions as a single model. The model-theoretic semantics can be viewed as the problem of generating a model that makes all the reactive rules true. © 2012 Springer-Verlag.","complex actions; complex events; KELPS; LPS; model generation; reactive systems","Complex actions; Complex events; KELPS; LPS; Model generation; Reactive system; Artificial intelligence; Semantics; World Wide Web; Computer programming languages",Conference Paper,Scopus,2-s2.0-84866722009
"Nepomuceno F.V., Engelbrecht A.P.","A self-adaptive heterogeneous PSO inspired by ants",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-32650-9_17,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866362667&doi=10.1007%2f978-3-642-32650-9_17&partnerID=40&md5=70a28e3cfdc427344b567b57eaa9d61f","Heterogeneous particle swarm optimizers have been proposed where particles are allowed to implement different behaviors. A selected behavior may not be optimal for the duration of the search process. Since the optimality of a behavior depends on the fitness landscape it is necessary that particles be able to dynamically adapt their behaviors. This paper introduces two new self-adaptive heterogeneous particle swarm optimizers which are influenced by the ant colony optimization meta-heuristic. These self-adaptive strategies are compared with three other heterogeneous particle swarm optimizers. The results show that the proposed models outrank the existing models overall. © 2012 Springer-Verlag.",,"Ant Colony Optimization (ACO); Fitness landscape; Heterogeneous particles; Metaheuristic; Optimality; Search process; Self-adaptive; Self-adaptive strategy; Artificial intelligence; Particle swarm optimization (PSO)",Conference Paper,Scopus,2-s2.0-84866362667
"Reguieg H., Toumani F., Motahari-Nezhad H.R., Benatallah B.","Using Mapreduce to scale events correlation discovery for business processes mining",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-32885-5_22,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866366083&doi=10.1007%2f978-3-642-32885-5_22&partnerID=40&md5=7853304116168d2bd429f0d804e110ed","In this paper, we present a scalable data analysis technique to support efficient event correlation for mining business processes. We propose a two-stages approach to compute correlation conditions and their entailed process instances from event logs using MapReduce framework. The experimental results show that the algorithm scales well to large datasets. © 2012 Springer-Verlag.",,"Business Process; Data analysis techniques; Event correlation; Large datasets; Map-reduce; Process instances; Artificial intelligence; Enterprise resource management",Conference Paper,Scopus,2-s2.0-84866366083
"Wu B., Wu G., Yang M.","A MapReduce based Ant Colony Optimization approach to combinatorial optimization problems",2012,"Proceedings - International Conference on Natural Computation",12,10.1109/ICNC.2012.6234645,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866182621&doi=10.1109%2fICNC.2012.6234645&partnerID=40&md5=da98607170c1595d2bba07088ee6504b","Ant Colony Optimization (ACO) is a kind of meta-heuristics algorithm, which simulates the social behavior of ants and could be a good alternative to existing algorithms for NP hard combinatorial optimization problems, like the 0-1 knapsack problem and the Traveling Salesman Problem (TSP). Although ACO can get solutions that are quite near to the optimal solution, it still has its own problems. Premature bogs the system down in a locally optimal solution rather than the global optimal one. To get better solutions, it requires a larger number of ants and iterations which consume more time. Parallelization is an effective way to solve large-scale ant colony optimization algorithms over large dataset. We propose a MapReduce based ACO approach. We show how ACO algorithms can be modeled into the MapReduce framework. We describe the algorithm design and implementation of ACO on Hadoop. © 2012 IEEE.","0-1 Knapsack Problem; Ant Colony; MapReduce; Meta-heuristics; Traveling Salesman Problem","0-1 knapsack problem; ACO algorithms; Algorithm design; Ant colonies; Ant Colony Optimization (ACO); Ant Colony Optimization algorithms; Combinatorial optimization problems; Data sets; Map-reduce; Meta heuristics; Meta-heuristics algorithms; NP-hard; Optimal solutions; Parallelizations; Social behavior; Artificial intelligence; Combinatorial optimization; Constrained optimization; Integer programming; Optimal systems; Traveling salesman problem; Algorithms",Conference Paper,Scopus,2-s2.0-84866182621
"Kutsuna N., Higaki T., Matsunaga S., Otsuki T., Yamaguchi M., Fujii H., Hasezawa S.","Active learning framework with iterative clustering for bioimage classification",2012,"Nature Communications",12,10.1038/ncomms2030,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866100677&doi=10.1038%2fncomms2030&partnerID=40&md5=02d7e8e03df0a59e7d359c9aaf61e31f","Advances in imaging systems have yielded a flood of images into the research field. A semi-automated facility can reduce the laborious task of classifying this large number of images. Here we report the development of a novel framework, CARTA (Clustering-Aided Rapid Training Agent), applicable to bioimage classification that facilitates annotation and selection of features. CARTA comprises an active learning algorithm combined with a genetic algorithm and self-organizing map. The framework provides an easy and interactive annotation method and accurate classification. The CARTA framework enables classification of subcellular localization, mitotic phases and discrimination of apoptosis in images of plant and human cells with an accuracy level greater than or equal to annotators. CARTA can be applied to classification of magnetic resonance imaging of cancer cells or multicolour time-course images after surgery. Furthermore, CARTA can support development of customized features for classification, high-throughput phenotyping and application of various classification schemes dependent on the user's purpose. © 2012 Macmillan Publishers Limited. All rights reserved.",,"apoptosis; article; cancer cell; cell nucleus; cellular distribution; cytoplasm; image analysis; in vitro study; learning; mitosis; nuclear magnetic resonance imaging; phenotype; plant cell; protein localization; Algorithms; Artificial Intelligence; Cell Tracking; Cells; Cellular Structures; HeLa Cells; Humans; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84866100677
"Malapert A., Guéret C., Rousseau L.-M.","A constraint programming approach for a batch processing problem with non-identical job sizes",2012,"European Journal of Operational Research",12,10.1016/j.ejor.2012.04.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861595002&doi=10.1016%2fj.ejor.2012.04.008&partnerID=40&md5=2bff503ad19bd8c46981584216de521f","This paper presents a constraint programming approach for a batch processing machine on which a finite number of jobs of non-identical sizes must be scheduled. A parallel batch processing machine can process several jobs simultaneously and the objective is to minimize the maximal lateness. The constraint programming formulation proposed relies on the decomposition of the problem into finding an assignment of the jobs to the batches, and then minimizing the lateness of the batches on a single machine. This formulation is enhanced by a new optimization constraint which is based on a relaxed problem and applies cost-based domain filtering techniques. Experimental results demonstrate the efficiency of cost-based domain filtering techniques. Comparisons to other exact approaches clearly show the benefits of the proposed approach: it can optimally solve problems that are one order of magnitude greater than those solved by a mathematical formulation or by a branch-and-price. © 2012 Elsevier B.V. All rights reserved.","Artificial intelligence; Combinatorial optimization; Constraint programming; Packing; Scheduling","Assignment of; Batch processing machine; Branch and price; Constraint programming; Domain filtering; Exact approach; Finite number; Job size; Mathematical formulation; Parallel batch processing; Processing problems; Relaxed problem; Artificial intelligence; Batch data processing; Combinatorial optimization; Computer programming; Constraint theory; Costs; Packing; Scheduling; Problem solving",Article,Scopus,2-s2.0-84861595002
"Plagge D., Leuschel M.","Validating B,Z and TLA + using ProB and Kodkod",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-32759-9_31,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865973863&doi=10.1007%2f978-3-642-32759-9_31&partnerID=40&md5=da0cd0a6e95fe7307c61a15b72ae75d4","We present the integration of the Kodkod high-level interface to SAT-solvers into the kernel of ProB. As such, predicates from B, Event-B, Z and TLA + can be solved using a mixture of SAT-solving and ProB's own constraint-solving capabilities developed using constraint logic programming: the first-order parts which can be dealt with by Kodkod and the remaining parts solved by the existing ProB kernel. We also present an empirical evaluation and analyze the respective merits of SAT-solving and classical constraint solving. We also compare to using SMT solvers via recently available translators for Event-B. © 2012 Springer-Verlag.","B-Method; Constraints; SAT; SMT; TLA; Tool Support; Z","B-method; Constraints; SAT; TLA; Tool support; Z; Artificial intelligence; Surface mount technology; Logic programming",Conference Paper,Scopus,2-s2.0-84865973863
"Skulimowski A.M.J.","Discovering complex system dynamics with intelligent data retrieval tools",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-31919-8_78,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865808451&doi=10.1007%2f978-3-642-31919-8_78&partnerID=40&md5=465f3e5c314ea9e592a7068dc6d71169","This paper presents the theoretical foundations of an intelligent on-line modelling tool capable of processing heterogeneous information on complex techno-economical systems. Its main functionality is to investigate, elicit, and apply rules and principles that govern the development processes of technologies and related markets. Specifically, we will focus on applications of the tool to model the evolution of information technology (IT). We will distinguish several relevant subsystems of the system under study, which describe the demographic, education, global economic trends, as well as specific market factors that determine the demand for and use of IT. The group modelling techniques are implemented in the new tool to enable the collaborative and distributed model building with intelligent verification of entries called 'model wiki'. Based on the information elicited from experts, gathered from the web and professional databases, a discrete-time control model of technological evolution emerges, coupled with a controlled discrete-event system. The latter processes qualitative information and models the influence of external events and trends on the discrete-time control system parameters. We propose novel uncertainty handling techniques capable of processing and combining different types of uncertain information, coming i.a. from Delphi research and forecasts. The quantitative information is dynamically updated by autonomous webcrawlers, following an adaptive intelligent strategy. The resulting model can be used to simulate long-term future trends and scenarios. Its ultimate goal is to perform an optimization process and derive recommendations for decision makers, for example when selecting IT investment strategies in an innovative enterprise. © 2012 Springer-Verlag.","Complex Systems; Decision Support Systems; Discrete-Time Control; Foresight; Group Modelling Tool; Hybrid Models; Model Discovery","Complex system dynamics; Decision makers; Development process; Discrete-time control; Distributed models; Foresight; Global economics; Heterogeneous information; Hybrid model; Innovative enterprise; Intelligent data; IT investments; Long-term future; Market factors; Modelling techniques; Modelling tools; On-line modelling; Optimization process; Qualitative information; Quantitative information; Technological evolution; Theoretical foundations; Uncertain informations; Uncertainty handling; Artificial intelligence; Decision support systems; Discrete time control systems; Investments; Large scale systems; Information technology",Conference Paper,Scopus,2-s2.0-84865808451
"Mohaghegh S.D., Liu J., Gaskari R., Maysami M., Olukoko O.A.","Application of surrogate reservoir model (SRM) to an onshore green field in Saudi Arabia; case study",2012,"Society of Petroleum Engineers - North Africa Technical Conference and Exhibition 2012, NATC 2012: Managing Hydrocarbon Resources in a Changing Environment",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865785734&partnerID=40&md5=aa7c58f89c4634a812b27fd122aa321e","Application of the Surrogate Reservoir Model (SRM) to an onshore green field in Saudi Arabia is the subject of this paper. SRM is a recently introduced technology that is used to tap into the unrealized potential of the reservoir simulation models. High computational cost and long processing time of reservoir simulation models limit our ability to perform comprehensive sensitivity analysis, quantify uncertainties and risks associated with the geologic and operational parameters or to evaluate a large set of scenarios for development of green fields. SRM accurately replicates the results of a numerical simulation model with very low computational cost and low turnaround period and allows for extended study of reservoir behavior and potentials. SRM represents the application of artificial intelligence and data mining to reservoir simulation and modeling. In this paper, development and the results of the SRM for an onshore green field in Saudi Arabia is presented. A reservoir simulation model has been developed for this green field using Saudi Aramco's in-house POWERS™ simulator. The geological model that serves as the foundation of the simulation model is developed using an analogy that incorporates limited measured data augmented with information from similar fields producing from the same formations. The reservoir simulation model consists of 1.4 million active grid blocks, including 40 vertical production wells and 22 vertical water injection wells. Steps involved in developing the SRM are identifying the number of runs that are required for the development of the SRM, making the runs, extracting static and dynamic data from the simulation runs to develop the necessary spatio-temporal dataset, identifying the key performance indicators (KPIs) that rank the influence of different reservoir characteristics on the oil and gas production in the field, training and matching the results of the simulation model, and finally validating the performance of the SRM using a blind simulation run. SRM for this reservoir is then used to perform sensitivity analysis as well as quantification of uncertainties associated with the geological model. These analyses that require thousands of simulation runs were performed using the SRM in minutes. Copyright 2012, Society of Petroleum Engineers.",,"Computational costs; Geological models; Green fields; Grid blocks; Key performance indicators; Oil and gas production; Operational parameters; Processing time; Production wells; Reservoir characteristic; Reservoir models; Reservoir simulation; Reservoir simulation model; Saudi Arabia; Saudi aramco; Simulation model; Spatio-temporal dataset; Static and dynamic; Water injection wells; Artificial intelligence; Benchmarking; Data mining; Exhibitions; Geologic models; Hydrocarbons; Petroleum reservoir evaluation; Petroleum reservoirs; Risk perception; Sensitivity analysis; Wells; Computer simulation",Conference Paper,Scopus,2-s2.0-84865785734
"Coco S., Laudani A., Pollicino G., Pulcini G., Fulginei F.R., Salvini A.","TWT magnetic focusing structure optimization by parallel evolutionary algorithm",2012,"COMPEL - The International Journal for Computation and Mathematics in Electrical and Electronic Engineering",12,10.1108/03321641211246347,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865746395&doi=10.1108%2f03321641211246347&partnerID=40&md5=ef7c55967c826293a183600c39e79756","Purpose - The purpose of this paper is to present the application of a novel hybrid algorithm, called MeTEO (Metric-Topological-Evolutionary- Optimization), based on the combination of three heuristics inspired by artificial life to the solution of optimization problems of a real electronic vacuum device. Design/methodology/approach - The Particle Swarm Optimization (PSO), the Flock-of-Starlings Optimization (FSO) and the Bacterial Chemotaxis Algorithm (BCA) were adapted to implement a novel meta-heuristic MeTEO the FSO has been powerfully employed for exploring the whole space of solutions, whereas the PSO is used to explore local regions where FSO had found solutions, and BCA to refine the solutions found by PSO, thanks its better performances in local search. Findings - The optimization of the focusing magnetic field of a Travelling Wave Tubes (TWT) collector is presented in order to show the effectiveness of MeTEO, in combination with COLLGUN FE simulator and equivalent source representation. The optimization of the focusing magnetic structure is obtained by using a maximum of 100 steps for each heuristic. Practical implications - The paper describes the development of a novel efficient parallel method for the solution of electromagnetic device optimization problems. Originality/value - The paper shows the capabilities of a novel combination of optimization methods inspired by ""artificial life"" which allows us to achieve effective solutions of multimodal optimization problems, typical of the electromagnetic device optimization, with an acceptable computational cost, thanks also to its natural parallel implementation. © Emerald Group Publishing Limited.","Artificial intelligence; Artificial life; Finite element method; Focusing magnetic field; Hybrid optimization algorithm; Magnetic fields; Particle-in-cell method; Programming and algorithm theory; Swarm intelligence; Traveling wave tube","Artificial Life; Hybrid optimization algorithm; Particle in cell method; Programming and algorithm theory; Swarm Intelligence; Algorithms; Artificial intelligence; Biochemistry; Electromagnets; Finite element method; Focusing; Magnetic devices; Magnetic fields; Structural optimization; Traveling wave tubes; Particle swarm optimization (PSO)",Article,Scopus,2-s2.0-84865746395
"Vytiniotis D., Coquand T., Wahlstedt D.","Stop when you are almost-full: Adventures in constructive termination",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-32347-8_17,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865590359&doi=10.1007%2f978-3-642-32347-8_17&partnerID=40&md5=af33d92832481e26608203aa04e37b14","Disjunctive well-foundedness, size-change termination, and well-quasi-orders are examples of techniques that have been successfully applied to program termination. Although these works originate in different communities, they rely on closely related principles and both employ similar arguments from Ramsey theory. At the same time there is a notable absence of these techniques in programming systems based on constructive type theory. In this paper we'd like to highlight the aforementioned connection and make the core ideas widely accessible to theoreticians and programmers, by offering a development in type theory which culminates in some novel tools for induction. Inevitably, we have to present some Ramsey-like arguments: Though similar proofs are typically classical, we offer an entirely constructive development based on the work of Bezem and Veldman, and Richman and Stolzenberg. © 2012 Springer-Verlag.",,"Constructive type theory; Program termination; Programming system; Ramsey theory; Type theory; Well-foundedness; Artificial intelligence; Theorem proving",Conference Paper,Scopus,2-s2.0-84865590359
"Beiki A.H., Saboor S., Ebrahimi M.","A New Avenue for Classification and Prediction of Olive Cultivars Using Supervised and Unsupervised Algorithms",2012,"PLoS ONE",12,10.1371/journal.pone.0044164,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866052094&doi=10.1371%2fjournal.pone.0044164&partnerID=40&md5=b949d005f11f365e843187ef3e4aed79","Various methods have been used to identify cultivares of olive trees; herein we used different bioinformatics algorithms to propose new tools to classify 10 cultivares of olive based on RAPD and ISSR genetic markers datasets generated from PCR reactions. Five RAPD markers (OPA0a21, OPD16a, OP01a1, OPD16a1 and OPA0a8) and five ISSR markers (UBC841a4, UBC868a7, UBC841a14, U12BC807a and UBC810a13) selected as the most important markers by all attribute weighting models. K-Medoids unsupervised clustering run on SVM dataset was fully able to cluster each olive cultivar to the right classes. All trees (176) induced by decision tree models generated meaningful trees and UBC841a4 attribute clearly distinguished between foreign and domestic olive cultivars with 100% accuracy. Predictive machine learning algorithms (SVM and Naïve Bayes) were also able to predict the right class of olive cultivares with 100% accuracy. For the first time, our results showed data mining techniques can be effectively used to distinguish between plant cultivares and proposed machine learning based systems in this study can predict new olive cultivars with the best possible accuracy. © 2012 Ebrahimi et al.",,"genomic DNA; accuracy; article; bioinformatics; classification algorithm; cluster analysis; controlled study; cultivar identification; data mining; decision tree; domestic species; genetic marker; genetic selection; inter simple sequence repeat; nonhuman; olive tree; polymerase chain reaction; prediction; random amplified polymorphic DNA; support vector machine; Agriculture; Algorithms; Artificial Intelligence; Cluster Analysis; Computational Biology; Data Mining; Decision Trees; Genes, Plant; Genetic Markers; Olea; Reproducibility of Results",Article,Scopus,2-s2.0-84866052094
"Hilletofth P., Lättilä L.","Agent based decision support in the supply chain context",2012,"Industrial Management and Data Systems",12,10.1108/02635571211264636,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865460255&doi=10.1108%2f02635571211264636&partnerID=40&md5=525aef3bf913d05c92f1f2d08597652d","Purpose - The purpose of this paper is to investigate the benefits and the barriers of agent based decision support (ABDS) systems in the supply chain context. Design/methodology/approach - Two ABDS systems have been developed and evaluated. The first system concerns a manufacturing supply chain while the second concerns a service supply chain. The systems are based on actual case companies. Findings - This research shows that the benefits of ABDS systems in the supply chain context include the possibility to increase versatility of system architecture, to improve supply chain visibility, to conduct experiments and what-if analyses, to improve the understanding of the real system, and the possibility to improve communication within and between organizations in the supply chain. The barriers of ABDS systems in the supply chain context include the difficulty to access data from partners in the supply chain, the difficulty to access data on a higher level of granularity, and the difficulty to retrieve data from other information systems. Research limitations/implications - The research is explorative in nature therefore empirical data from similar and other research settings should be gathered to reinforce the validity of the findings. Practical implications - This research provides knowledge and insights on how ABDS systems may be developed and used in the supply chain context and demonstrates its main benefits and barriers. Originality/value - This research expands the current research of benefits of ABDS systems to the supply chain domain and also addresses the barriers of ABDS systems to a larger extent than previous research. Comparisons to other simulation based decision support systems are also given. © Emerald Group Publishing Limited.","Agent based modeling and simulation; Decision support system; Information systems; Supply chain management","Agent-based decision support; Agent-based modeling and simulation; Design/methodology/approach; Empirical data; First systems; Real systems; Service supply chains; Supply chain visibility; System architectures; Artificial intelligence; Computer simulation; Decision support systems; Information systems; Supply chain management; Research",Article,Scopus,2-s2.0-84865460255
"Kilgarriff A.","Getting to know your corpus",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-32790-2_1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865464876&doi=10.1007%2f978-3-642-32790-2_1&partnerID=40&md5=5af670b6a0442171f2576a9621172ee7","Corpora are not easy to get a handle on. The usual way of getting to grips with text is to read it, but corpora are mostly too big to read (and not designed to be read). We show, with examples, how keyword lists (of one corpus vs. another) are a direct, practical and fascinating way to explore the characteristics of corpora, and of text types. Our method is to classify the top one hundred keywords of corpus1 vs. corpus2, and corpus2 vs. corpus1. This promptly reveals a range of contrasts between all the pairs of corpora we apply it to. We also present improved maths for keywords, and briefly discuss quantitative comparisons between corpora. All the methods discussed (and almost all of the corpora) are available in the Sketch Engine, a leading corpus query tool. © 2012 Springer-Verlag.","corpora; corpus similarity; keyword lists; keywords; Sketch Engine","corpora; corpus similarity; keyword lists; keywords; Quantitative comparison; Query tools; Artificial intelligence; Software agents",Conference Paper,Scopus,2-s2.0-84865464876
"Nam W.-H., Choi J.-Y., Yoo S.-H., Jang M.-W.","A decision support system for agricultural drought management using risk assessment",2012,"Paddy and Water Environment",12,10.1007/s10333-012-0329-z,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865438259&doi=10.1007%2fs10333-012-0329-z&partnerID=40&md5=fa7cf1d93d164e2f8c266ca0e54e0bc4","Since predicted changes in climate will modify temperature and rainfall patterns, there are concerns about the potential impacts of these changes on agricultural drought and agricultural water resources management. An agricultural drought is influenced by several factors such as rainfall, soil characteristics, crops, and reservoir water supply and may be defined as the imbalance of water circulation in paddy and water environments. In particular, soil moisture and water supply for reservoir demand are the most direct and important indicators of agricultural drought events. In the past, conventional drought management approaches based on climatic and meteorological observations have been the primary tools used for measuring drought severity. Because of the spatial and temporal variability and multiple impacts of drought, it is necessary to improve tools to determine the onset, severity, spatial extent, and end of the drought conditions. Improved and available data for mapping and monitoring of this phenomenon are also needed. Effective and efficient drought management can be achieved through drought monitoring based on the ability to assess current conditions and provide improved tools to adapt and mitigate the impacts of future changes. In this article, a methodology is developed to support the risk-based decision-making process involved in agricultural drought management using the following four strategies: drought assessment and monitoring, drought forecast and outlook, drought countermeasures, and drought records management. © 2012 Springer-Verlag.","Agricultural drought; Decision support system; Drought management; Reservoir drought index; Risk assessment; Soil moisture index","Agricultural drought; Agricultural water; Decision making process; Drought conditions; Drought indices; Drought management; Drought monitoring; Drought severity; Meteorological observation; Multiple impact; Potential impacts; Rainfall patterns; Reservoir water; Risk-based; Soil characteristics; Soil moisture index; Spatial and temporal variability; Spatial extent; Water circulation; Water environments; Agriculture; Artificial intelligence; Decision support systems; Rain; Records management; Reservoir management; Reservoirs (water); Risk assessment; Soil moisture; Water resources; Water supply; Weather forecasting; Drought",Article,Scopus,2-s2.0-84865438259
"Ali A., Siddharth S., Syed Z., El-Sheimy N.","Swarm optimization-based magnetometer calibration for personal handheld devices",2012,"Sensors (Switzerland)",12,10.3390/s120912455,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867032765&doi=10.3390%2fs120912455&partnerID=40&md5=051e1303377bc8d312deb7640f50983e","Inertial Navigation Systems (INS) consist of accelerometers, gyroscopes and a processor that generates position and orientation solutions by integrating the specific forces and rotation rates. In addition to the accelerometers and gyroscopes, magnetometers can be used to derive the user heading based on Earth's magnetic field. Unfortunately, the measurements of the magnetic field obtained with low cost sensors are usually corrupted by several errors, including manufacturing defects and external electro-magnetic fields. Consequently, proper calibration of the magnetometer is required to achieve high accuracy heading measurements. In this paper, a Particle Swarm Optimization (PSO)-based calibration algorithm is presented to estimate the values of the bias and scale factor of low cost magnetometers. The main advantage of this technique is the use of the artificial intelligence which does not need any error modeling or awareness of the nonlinearity. Furthermore, the proposed algorithm can help in the development of Pedestrian Navigation Devices (PNDs) when combined with inertial sensors and GPS/Wi-Fi for indoor navigation and Location Based Services (LBS) applications. © 2012 by the authors; licensee MDPI, Basel, Switzerland.","Algorithms; Artificial intelligence; Measurement; Navigation; Sensor; Systems","Calibration algorithm; Earth's magnetic field; In-door navigations; Inertial navigation systems (INS); Magnetometer calibration; Manufacturing defects; Pedestrian navigation; Position and orientations; Accelerometers; Algorithms; Artificial intelligence; Computer systems; Cost benefit analysis; Gyroscopes; Inertial navigation systems; Location based services; Magnetic fields; Magnetometers; Measurements; Navigation; Particle swarm optimization (PSO); Sensors; Calibration",Article,Scopus,2-s2.0-84867032765
"Escario J.B., Jimenez J.F., Giron-Sierra J.M.","Optimisation of autonomous ship manoeuvres applying Ant Colony Optimisation metaheuristic",2012,"Expert Systems with Applications",12,10.1016/j.eswa.2012.02.069,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859419343&doi=10.1016%2fj.eswa.2012.02.069&partnerID=40&md5=ed0dd99412c7c48e1e238248614deb46","This paper introduces the use of a swarm algorithm, derived from Ant Colony Optimisation, to solve path planning problems for autonomous vehicles. The purpose is to obtain optimal trajectories for manoeuvres of Autonomous Surface Vessels. The algorithm works with a model of the vehicle, and the solutions achieved are always feasible. With enough time, it can also obtain trajectories very close to the optimal. Provided the appropriate modifications the algorithm can be applied to solve other combinatorial optimisations problems with a non restricted number of feasible solutions. The methodology is tested through simulations on open sea manoeuvres and scenarios with the presence of obstacles. © 2012 Elsevier Ltd. All rights reserved.","Ant Colony Optimisation; Combinatorial optimisation; Marine robotics; Motion planning; Swarm intelligence","Ant colony optimisation; Autonomous surface vessels; Autonomous Vehicles; Feasible solution; Marine robotics; Metaheuristic; Optimal trajectories; Optimisations; Path planning problems; Swarm algorithms; Swarm Intelligence; Algorithms; Artificial intelligence; Combinatorial optimization; Motion planning; Optimization",Article,Scopus,2-s2.0-84859419343
"Maye A., Engel A.K.","Time scales of sensorimotor contingencies",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-31561-9_27,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865273627&doi=10.1007%2f978-3-642-31561-9_27&partnerID=40&md5=4642e82191ed2b2c445d450841c3621d","In Sensorimotor Contingency Theory (SMCT) differences between the perceptual qualities of sensory modalities are explained by the different structure of dependencies between a human's actions and the ensuing changes in sensory stimulation. It distinguishes modality-related Sensory-Motor Contingencies (SMCs), that describe the structure of changes for individual sensory modalities, and object-related SMCs, that capture the multisensory patterns caused by actions directed towards objects. These properties suggest a division of time scales in that modality-related SMCs describe the immediate effect of actions on characteristics of the sensory signal, and object-related SMCs account for sequences of actions and sensory observations. We present a computational model of SMCs that implements this distinction and allows to analyze the properties of the different SMC types. The emergence of perceptual capabilities is demonstrated in a locomotive robot controlled by this model that develops an action-based understanding for the size of its confinement without using any distance sensors. © 2012 Springer-Verlag.",,"Computational model; Different structure; Distance sensors; Multisensory; Perceptual quality; Sensorimotor contingencies; Sensory modality; Sensory signals; Sensory stimulation; Time-scales; Artificial intelligence; Cognitive systems",Conference Paper,Scopus,2-s2.0-84865273627
"Blahuta J., Soukup T., Čermák P., Rozsypal J., Večerek M.","Ultrasound medical image recognition with artificial intelligence for Parkinson's disease classification",2012,"MIPRO 2012 - 35th International Convention on Information and Communication Technology, Electronics and Microelectronics - Proceedings",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865087619&partnerID=40&md5=3decfc77772fc82640bf8072e2ceb31d","This paper shows how to classify the medical ultrasound images by using artificial intelligence with experimental software with MATLAB. The main goal is a classification of ROI substantia nigra in midbrain. This classification of the images is useful to detection Parkinson's disease. This work is based on image processing and is realized with the help of artificial intelligence which has been experimentally simulated in MATLAB software environment. This method is well applicable with Neural Network Toolbox in MATLAB. © 2012 MIPRO.",,"Matlab- software; Medical image recognition; Medical ultrasound images; Midbrain; Neural network toolboxes; Parkinson's disease; Substantia nigra; Artificial intelligence; Communication; Image processing; Image recognition; Information technology; Medical imaging; Microelectronics; Neurodegenerative diseases; Ultrasonics; Ultrasonic applications",Conference Paper,Scopus,2-s2.0-84865087619
"Rahmat N.A., Musirin I.","Differential Evolution Ant Colony Optimization (DEACO) technique in solving economic load dispatch problem",2012,"2012 IEEE International Power Engineering and Optimization Conference, PEOCO 2012 - Conference Proceedings",12,10.1109/PEOCO.2012.6230872,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864965758&doi=10.1109%2fPEOCO.2012.6230872&partnerID=40&md5=e6dee96c7077f458ec11b933a0aa429d","Electric utilities are the companies responsible for ensuring energy supply meets their customers' requirement. While ensuring the energy is generated in the right amount, they have to guarantee that the energy is generated within feasible cost. Economic Load Dispatch (ELD) problem involves the scheduling of generating unit outputs that can satisfy load demand at minimum operating cost. Several approaches have been applied to yield the best solution for the problem, such as Genetic Algorithm, Bees Algorithm and Neural Network Algorithm. This paper presents Differential Evolution Ant Colony Optimization (DEACO) to optimize Economic Load Dispatch in power system. Implementation of the IEEE Reliability Test System (RTS) demonstrated that this technique is feasible to crack the economic problem. Comparative studies with respect to ACO and the traditional ELD techniques designate that the proposed DEACO outperformed these two techniques. © 2012 IEEE.","Ant Colony Optimization (ACO); Differential Evolution (DE); Differential Evolution Ant Colony Optimization (DEACO); Economic Load Dispatch (ELD); pheromone","Ant Colony Optimization (ACO); Bees algorithms; Comparative studies; Differential Evolution; Economic load dispatch; Economic load dispatch problem; Economic problems; Energy supplies; Generating unit; IEEE-reliability test system; Load demand; Neural network algorithm; pheromone; Electric utilities; Evolutionary algorithms; Optimization; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84864965758
"Kordjamshidi P., Frasconi P., Van Otterlo M., Moens M.-F., De Raedt L.","Relational learning for spatial relation extraction from natural language",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-31951-8_20,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864833378&doi=10.1007%2f978-3-642-31951-8_20&partnerID=40&md5=2708c4e14453528b33ea0655c52f2762","Automatically extracting spatial information is a challenging novel task with many applications. We formalize it as an information extraction step required for a mapping from natural language to a formal spatial representation. Sentences may give rise to multiple spatial relations between words representing landmarks, trajectors and spatial indicators. Our contribution is to formulate the extraction task as a relational learning problem, for which we employ the recently introduced kLog framework. We discuss representational and modeling aspects, kLog's flexibility in our task and we present current experimental results. © 2012 Springer-Verlag Berlin Heidelberg.",,"Information Extraction; Modeling aspects; Natural languages; Novel task; Relational learning; Spatial indicators; Spatial informations; Spatial relations; Spatial representations; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84864833378
"Zhong J.-H., Zhang J.","Ant colony optimization algorithm for lifetime maximization in wireless sensor network with mobile sink",2012,"GECCO'12 - Proceedings of the 14th International Conference on Genetic and Evolutionary Computation",12,10.1145/2330163.2330328,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864674646&doi=10.1145%2f2330163.2330328&partnerID=40&md5=d5c65b83ab84742d8edb9b355585a78f","In wireless sensor networks (WSNs), sensors near the sink can be burdened with a large amount of traffic, because they have to transmit data generated by themselves and those far away from the sink. Hence the sensors near the sink would deplete their energy much faster than the others, which results in a short network lifetime. Using mobile sink is an effective way to tackle this issue. This paper explores the problem of determining the optimal movements of the mobile sink to maximize the network lifetime. A novel ant colony optimization algorithm (ACO), namely the ACO-MSS, is developed to solve the problem. The proposed ACO-MSS takes advantage of the global search ability of ACO and adopts effective heuristic information to find a near globally optimal solution. Multiple practical factors such as the forbidden regions and the maximum moving distance of the sink are taken into account to facilitate the real applications. The proposed ACO-MSS is validated by a series of simulations on WSNs with different characteristics. The simulation results demonstrate the effectiveness of the proposed algorithms. © 2012 ACM.","ant colony optimization; lifetime maximization; mobile sink scheduling; wireless sensor network","Ant Colony Optimization (ACO); Ant Colony Optimization algorithms; Forbidden region; Global search ability; Heuristic information; Lifetime maximization; Mobile sinks; Network lifetime; Optimal movements; Optimal solutions; Real applications; Transmit data; Wireless sensor network (WSNs); Artificial intelligence; Satellites; Sensors; Wireless sensor networks; Algorithms",Conference Paper,Scopus,2-s2.0-84864674646
"de Bono B., Hunter P.","Integrating knowledge representation and quantitative modelling in physiology",2012,"Biotechnology Journal",12,10.1002/biot.201100304,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864818456&doi=10.1002%2fbiot.201100304&partnerID=40&md5=a077450e1057b17a68cc4bccc92654ca","A wealth of potentially shareable resources, such as data and models, is being generated through the study of physiology by computational means. Although in principle the resources generated are reusable, in practice, few can currently be shared. A key reason for this disparity stems from the lack of consistent cataloguing and annotation of these resources in a standardised manner. Here, we outline our vision for applying community-based modelling standards in support of an automated integration of models across physiological systems and scales. Two key initiatives, the Physiome Project and the European contribution - the Virtual Phsysiological Human Project, have emerged to support this multiscale model integration, and we focus on the role played by two key components of these frameworks, model encoding and semantic metadata annotation. We present examples of biomedical modelling scenarios (the endocrine effect of atrial natriuretic peptide, and the implications of alcohol and glucose toxicity) to illustrate the role that encoding standards and knowledge representation approaches, such as ontologies, could play in the management, searching and visualisation of physiology models, and thus in providing a rational basis for healthcare decisions and contributing towards realising the goal of of personalized medicine. © 2012 WILEY-VCH Verlag GmbH & Co. KGaA, Weinheim.","Computational physiology; Medical biotechnology; Multiscale modelling; Personalized medicine; Physiome project","Computational physiology; Medical biotechnology; Multi-scale modelling; Personalized medicines; Physiome; Encoding (symbols); Glucose; Health care; Knowledge representation; Metadata; Physiology; Semantics; Virtual reality; Visualization; Physiological models; article; health care system; medical information; personalized medicine; priority journal; quantitative analysis; Artificial Intelligence; Biotechnology; Computational Biology; Computer Graphics; Humans; Individualized Medicine; Models, Biological; Physiology",Article,Scopus,2-s2.0-84864818456
"You M., Liu J., Li G.-Z., Chen Y.","Embedded Feature Selection for Multi-label Classification of Music Emotions",2012,"International Journal of Computational Intelligence Systems",12,10.1080/18756891.2012.718113,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864681750&doi=10.1080%2f18756891.2012.718113&partnerID=40&md5=22b6f6a494655d2700698156aed0311a","When detecting of emotions from music, many features are extracted from the original music data. However, there are redundant or irrelevant features, which will reduce the performance of classification models. Considering the feature problems, we propose an embedded feature selection method, called Multi-label Embedded Feature Selection (MEFS), to improve classification performance by selecting features. MEFS embeds classifier and considers the label correlation. Other three representative multi-label feature selection methods, known as LP-Chi, max and avg, together with four multi-label classification algorithms, is included for performance comparison. Experimental results show that the performance of our MEFS algorithm is superior to those filter methods in the music emotion dataset. © 2012 Copyright the authors.","Embedded feature selection; Multi-label learning; Music emotion","Classification algorithm; Classification models; Classification performance; Data sets; Feature selection methods; Filter method; Multi-label; Music data; Music emotion; Performance comparison; Artificial intelligence; Computer science; Algorithms",Article,Scopus,2-s2.0-84864681750
"Peng Y., Zhang Y., Kou G., Shi Y.","A multicriteria decision making approach for estimating the number of clusters in a data set",2012,"PLoS ONE",12,10.1371/journal.pone.0041713,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864400807&doi=10.1371%2fjournal.pone.0041713&partnerID=40&md5=32f2bee852a991e95ab6044541ad391b","Determining the number of clusters in a data set is an essential yet difficult step in cluster analysis. Since this task involves more than one criterion, it can be modeled as a multiple criteria decision making (MCDM) problem. This paper proposes a multiple criteria decision making (MCDM)-based approach to estimate the number of clusters for a given data set. In this approach, MCDM methods consider different numbers of clusters as alternatives and the outputs of any clustering algorithm on validity measures as criteria. The proposed method is examined by an experimental study using three MCDM methods, the well-known clustering algorithm-k-means, ten relative measures, and fifteen public-domain UCI machine learning data sets. The results show that MCDM methods work fairly well in estimating the number of clusters in the data and outperform the ten relative measures considered in the study. © 2012 Peng et al.",,"article; classification algorithm; cluster analysis; decision support system; experimental study; human; intermethod comparison; mathematical model; multicriteria decision making; Preference Ranking Organization Method for Enrichment of Evaluations; process development; scoring system; Technique for Order Preference by Similarity to Ideal Solution; validation process; Weighted Sum Method; Algorithms; Artificial Intelligence; Cluster Analysis; Decision Making",Article,Scopus,2-s2.0-84864400807
"Mao C., Yu B.X., Chen J.","Swarm intelligence-based test data generation for structural testing",2012,"Proceedings - 2012 IEEE/ACIS 11th International Conference on Computer and Information Science, ICIS 2012",12,10.1109/ICIS.2012.103,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864071659&doi=10.1109%2fICIS.2012.103&partnerID=40&md5=3ac26a50c38ddfa37618d475cc68775e","Automated generation of test data has always been a challenging problem in the area of software testing. Recently, meta-heuristic search (MHS) techniques have been proven to be a powerful tool to solve this difficulty. In the paper, we introduce an up-to-date search technique, i.e. particle swarm optimization (PSO), to settle this difficulty. After the basic idea of PSO is addressed, the overall framework of PSO-based test data generation is discussed. Here, the inputs of program under test are encoded into particles. During the search process, PSO algorithm is used to generate test inputs with the highest possible coverage rate. Once a set of test inputs is produced, test driver will seed them into program to run and collect coverage information simultaneously. Then, the value of fitness function for branch coverage can be calculated based on such information, which can direct the algorithm optimization in next iteration. In order to validate our method, five realworld programs are used for experimental analysis. The results show that PSO-based method outperforms other algorithms such as GA both in the coverage effect of test data and the convergence speed. © 2012 IEEE.","Branch coverage; Convergence speed; Fitness function; PSO; Test data generation","Branch coverage; Convergence speed; Fitness functions; PSO; Test data generation; Artificial intelligence; Data communication systems; Genetic algorithms; Heuristic algorithms; Information science; Test facilities; Software testing",Conference Paper,Scopus,2-s2.0-84864071659
"Tsai K.-C., Jian J.-W., Yang E.-W., Hsu P.-C., Peng H.-P., Chen C.-T., Chen J.-B., Chang J.-Y., Hsu W.-L., Yang A.-S.","Prediction of Carbohydrate binding sites on protein surfaces with 3-dimensional probability density distributions of interacting atoms",2012,"PLoS ONE",12,10.1371/journal.pone.0040846,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864371562&doi=10.1371%2fjournal.pone.0040846&partnerID=40&md5=10ac307a9c1fe8754472710a7b918953","Non-covalent protein-carbohydrate interactions mediate molecular targeting in many biological processes. Prediction of non-covalent carbohydrate binding sites on protein surfaces not only provides insights into the functions of the query proteins; information on key carbohydrate-binding residues could suggest site-directed mutagenesis experiments, design therapeutics targeting carbohydrate-binding proteins, and provide guidance in engineering protein-carbohydrate interactions. In this work, we show that non-covalent carbohydrate binding sites on protein surfaces can be predicted with relatively high accuracy when the query protein structures are known. The prediction capabilities were based on a novel encoding scheme of the three-dimensional probability density maps describing the distributions of 36 non-covalent interacting atom types around protein surfaces. One machine learning model was trained for each of the 30 protein atom types. The machine learning algorithms predicted tentative carbohydrate binding sites on query proteins by recognizing the characteristic interacting atom distribution patterns specific for carbohydrate binding sites from known protein structures. The prediction results for all protein atom types were integrated into surface patches as tentative carbohydrate binding sites based on normalized prediction confidence level. The prediction capabilities of the predictors were benchmarked by a 10-fold cross validation on 497 non-redundant proteins with known carbohydrate binding sites. The predictors were further tested on an independent test set with 108 proteins. The residue-based Matthews correlation coefficient (MCC) for the independent test was 0.45, with prediction precision and sensitivity (or recall) of 0.45 and 0.49 respectively. In addition, 111 unbound carbohydrate-binding protein structures for which the structures were determined in the absence of the carbohydrate ligands were predicted with the trained predictors. The overall prediction MCC was 0.49. Independent tests on anti-carbohydrate antibodies showed that the carbohydrate antigen binding sites were predicted with comparable accuracy. These results demonstrate that the predictors are among the best in carbohydrate binding site predictions to date. © 2012 Tsai et al.",,"antibody; carbohydrate antibody; carbohydrate ligand; ligand; unclassified drug; accuracy; amino acid composition; analytic method; analytical parameters; antibody combining site; article; atom; binding site; binding site geometry; cross validation; hydrogen bond; learning algorithm; non covalent carbohydrate binding site; nonbiological model; prediction; protein carbohydrate interaction; protein structure; quality control; sensitivity analysis; surface property; three dimensional probability density map; validation process; Artificial Intelligence; Binding Sites; Carbohydrates; Databases, Protein; Models, Molecular; Proteins; Sequence Analysis, Protein",Article,Scopus,2-s2.0-84864371562
"Boyer F., Lebastard V.","Exploration of objects by an underwater robot with electric sense",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-31525-1_5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864001682&doi=10.1007%2f978-3-642-31525-1_5&partnerID=40&md5=12384d6789bb53a1e59c879ca103ef30","In this article, we propose a solution to the underwater exploration of objects using a new sensor inspired from the electric fish. The solution is free of any model and is just based on the combination of elementary behaviors, each of these behaviors being achieved through direct feedback of the electric measurements. The solution is robust, cheap and easy to implement. After, stating and interpreting it, the article ends with a few experimental results consisting in exploring small and large unknown objects. © 2012 Springer-Verlag.",,"Electric fish; Elementary behaviors; Underwater exploration; Underwater robots; Unknown objects; Artificial intelligence; Biomimetics",Conference Paper,Scopus,2-s2.0-84864001682
"Lu X., Bressan S.","Sampling connected induced subgraphs uniformly at random",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-31235-9_13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863438815&doi=10.1007%2f978-3-642-31235-9_13&partnerID=40&md5=6a8b73bfc9b74723a922ecc63f850407","A recurrent challenge for modern applications is the processing of large graphs. The ability to generate representative samples of smaller size is useful not only to circumvent scalability issues but also, per se, for statistical analysis and other data mining tasks. For such purposes adequate sampling techniques must be devised. We are interested, in this paper, in the uniform random sampling of a connected subgraph from a graph. We require that the sample contains a prescribed number of vertices. The sampled graph is the corresponding induced graph. We devise, present and discuss several algorithms that leverage three different techniques: Rejection Sampling, Random Walk and Markov Chain Monte Carlo. We empirically evaluate and compare the performance of the algorithms. We show that they are effective and efficient but that there is a trade-off, which depends on the density of the graphs and the sample size. We propose one novel algorithm, which we call Neighbour Reservoir Sampling (NRS), that very successfully realizes the trade-off between effectiveness and efficiency. © 2012 Springer-Verlag.",,"Data mining tasks; Induced subgraphs; Large graphs; Markov chain Monte Carlo; Modern applications; Novel algorithm; Random sampling; Random Walk; Representative sample; Sample sizes; Sampling technique; Scalability issue; Subgraphs; Three different techniques; Artificial intelligence; Algorithms",Conference Paper,Scopus,2-s2.0-84863438815
"Doerr B., Fouz M., Friedrich T.","Asynchronous rumor spreading in preferential attachment graphs",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-31155-0_27,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863110368&doi=10.1007%2f978-3-642-31155-0_27&partnerID=40&md5=ac22860c40940a2f4116d8a3f960ffdc","We show that the asynchronous push-pull protocol spreads rumors in preferential attachment graphs (as defined by Barabási and Albert) in time to all but a lower order fraction of the nodes with high probability. This is significantly faster than what synchronized protocols can achieve; an obvious lower bound for these is the average distance, which is known to be Θ(logn/loglogn). © 2012 Springer-Verlag.",,"Average Distance; High probability; Lower bounds; Preferential attachments; Rumor spreading; Artificial intelligence; Algorithms",Conference Paper,Scopus,2-s2.0-84863110368
"Haque B.","Overcoming the challenges of automating and integrating virtual product development processes",2012,"International Journal of Computer Applications in Technology",12,10.1504/IJCAT.2012.048203,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864537064&doi=10.1504%2fIJCAT.2012.048203&partnerID=40&md5=592246b807b7b74177ecbef08655c743","Design of complex aerospace products and their manufacturing processes requires the creation of many different computer models representing the designs, at varying levels of fidelity, for multiple engineering domain analyses/simulations. Multiple disparate computational technologies are used in the process. In order to reduce lead-times and enable rapid design search and optimisation, engineers spend significant resources and time automating the creation of geometry and finite element models, integrating these models, and auto executing the various engineering codes. This paper presents a 'common computational' approach to address the challenges of automation and integration and discusses its benefits over traditional approaches. Copyright © 2012 Inderscience Enterprises Ltd.","Artificial intelligence; CAD/CAE; Computer aided design and engineering; Design process automation; KBE; Knowledge based engineering; Knowledge based systems; Knowledge driven design; Object-oriented design systems; Virtual product development; VPD","CAD/CAE; Design process; KBE; Knowledge-based engineering; Object-oriented design; Virtual product development; VPD; Artificial intelligence; Computer aided design; Knowledge based systems; Product design; Product development; Models",Article,Scopus,2-s2.0-84864537064
"Tassopoulos I.X., Beligiannis G.N.","Using particle swarm optimization to solve effectively the school timetabling problem",2012,"Soft Computing",12,10.1007/s00500-012-0809-5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862186388&doi=10.1007%2fs00500-012-0809-5&partnerID=40&md5=908ee359856516ab0b4a3ef373e22699","A new hybrid adaptive algorithm based on particle swarm optimization (PSO) is designed, developed and applied to the high school timetabling problem. The proposed PSO algorithm is used to create feasible and efficient timetables for high schools in Greece. Experiments with real-world data coming from different high schools have been conducted to show the efficiency of the proposed PSO algorithm. As well as that, the algorithm has been compared with four other effective techniques found in the literature to demonstrate its efficiency and superior performance. In order to have a fair comparison with these algorithms, we decided to use the exact same input instances used by these algorithms. The proposed PSO algorithm outperforms, in most cases, other existing attempts to solve the same problem as shown by experimental results. © 2012 Springer-Verlag.","Artificial intelligence; Computer application; Educational organizations; Particle swarm optimization; School timetabling problem","Educational organizations; High school; Its efficiencies; PSO algorithms; Real world data; School timetabling problems; Adaptive algorithms; Artificial intelligence; Computer applications; Particle swarm optimization (PSO); Scheduling; Societies and institutions; Problem solving",Article,Scopus,2-s2.0-84862186388
"Khansa L., Forcade J., Nambari G., Parasuraman S., Cox P.","Proposing an intelligent cloud-based electronic health record system",2012,"International Journal of Business Data Communications and Networking",12,10.4018/jbdcn.2012070104,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877914835&doi=10.4018%2fjbdcn.2012070104&partnerID=40&md5=100a8be265ec0415100f40d480e39358","With the aging United States population, healthcare costs have considerably increased and are expected to keep rising in the foreseeable future. In this paper, the authors propose an intelligent cloud-based electronic health record (ICEHR) system that has the potential to reduce medical errors and improve patients' quality of life, in addition to reducing costs and increasing the productivity of healthcare organizations. They developed a set of best practices that encompass end-user policies and regulations, identity and access management, network resilience and service level agreements, advanced computational power, ""Big Data"" mining abilities, and other operational/managerial controls that are meant to improve the privacy and security of the ICEHR, and make it inherently compliant to healthcare regulations. These best practices serve as a framework that offers a single interconnection agreement between the cloud host and healthcare entities, and streamlines access to private patient information based on a unified set of access principles. Copyright © 2012, IGI Global.","Clinical decision support system; Cloud computing; Electronic health record system; Health insurance portability and accountability act of 1996 (HIPAA); Privacy; Security","Clinical decision support systems; Electronic health record; Electronic health record systems; Health insurance portability and accountability acts; Healthcare organizations; Identity and access managements; Security; Service Level Agreements; Artificial intelligence; Cloud computing; Data privacy; Decision support systems; Health care; Health insurance; Information management",Article,Scopus,2-s2.0-84877914835
"Šuster P., Jadlovská A.","Modeling and control design of magnetic levitation system",2012,"IEEE 10th Jubilee International Symposium on Applied Machine Intelligence and Informatics, SAMI 2012 - Proceedings",12,10.1109/SAMI.2012.6208976,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862736350&doi=10.1109%2fSAMI.2012.6208976&partnerID=40&md5=f8da3ee044c4400262c1068d9c35a79e","The aim of this paper is to design of control algorithm for the Magnetic levitation system using an exact input - output feedback linearization method. In this paper is given nonlinear simulation model of the Magnetic levitation based on mathematical model of the Magnetic levitation system. Designed control algorithm together with simulation model of the Magnetic levitation is implemented into control structure with purpose of control on steady state defined by a reference trajectory, which is verified in Matlab/Simulink language. © 2012 IEEE.","control design; exact linearization method; magnetic levitation; modeling","Control design; Control structure; Exact linearization; Magnetic levitation systems; MATLAB /simulink; Modeling and control; Nonlinear simulations; Output feedback linearization; Reference trajectories; Simulation model; Steady state; Artificial intelligence; Computer simulation; Control; Feedback linearization; Information science; Magnetic levitation; Magnetic levitation vehicles; Mathematical models; Models; Algorithms",Conference Paper,Scopus,2-s2.0-84862736350
"Kaveh A., Talatahari S.","A hybrid CSS and PSO algorithm for optimal design of structures",2012,"Structural Engineering and Mechanics",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863193991&partnerID=40&md5=adb60f84a61e082c61cf408d183bcde9","A new hybrid meta-heuristic optimization algorithm is presented for design of structures. The algorithm is based on the concepts of the charged system search (CSS) and the particle swarm optimization (PSO) algorithms. The CSS is inspired by the Coulomb and Gauss's laws of electrostatics in physics, the governing laws of motion from the Newtonian mechanics, and the PSO is based on the swarm intelligence and utilizes the information of the best fitness historically achieved by the particles (local best) and by the best among all the particles (global best). In the new hybrid algorithm, each agent is affected by local and global best positions stored in the charged memory considering the governing laws of electrical physics. Three different types of structures are optimized as the numerical examples with the new algorithm. Comparison of the results of the hybrid algorithm with those of other metaheuristic algorithms proves the robustness of the new algorithm.","Charged system search; Frames; Grillage systems; Hybrid methods; Particle swarm optimization; Structural optimum design; Truss structures","Charged systems; Frames; Hybrid method; Structural optimum design; Truss structure; Algorithms; Artificial intelligence; Particle swarm optimization (PSO)",Article,Scopus,2-s2.0-84863193991
"Schmidl D., Philippen P., Lorenz D., Rössel C., Geimer M., An Mey D., Mohr B., Wolf F.","Performance analysis techniques for task-based OpenMP applications",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-30961-8_15,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862227441&doi=10.1007%2f978-3-642-30961-8_15&partnerID=40&md5=4708ce65169cc3da9538d04121a7b912","Version 3.0 of the OpenMP specification introduced the task construct for the explicit expression of dynamic task parallelism. Although automated load-balancing capabilities make it an attractive parallelization approach for programmers, the difficulty of integrating this new dimension of parallelism into traditional models of performance data has so far prevented the emergence of appropriate performance tools. Based on our earlier work, where we have introduced instrumentation for task-based programs, we present initial concepts for analyzing the data delivered by this instrumentation. We define three typical performance problems related to tasking and show how they can be visually explored using event traces. Special emphasis is placed on the event model used to capture the execution of task instances and on how the time consumed by the program is mapped onto tasks in the most meaningful way. We illustrate our approach with practical examples. © 2012 Springer-Verlag.",,"Dynamic tasks; Event model; Load-Balancing; New dimensions; OpenMP applications; Parallelizations; Performance analysis techniques; Performance data; Performance problems; Performance tools; Task instance; Task-based; Artificial intelligence; Application programming interfaces (API)",Conference Paper,Scopus,2-s2.0-84862227441
"Sen S., Mottu J.-M., Tisi M., Cabot J.","Using models of partial knowledge to test model transformations",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-30476-7_2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862223772&doi=10.1007%2f978-3-642-30476-7_2&partnerID=40&md5=cf880540b63e30f7623e6a99b9cba961","Testers often use partial knowledge to build test models. This knowledge comes from sources such as requirements, known faults, existing inputs, and execution traces. In Model-Driven Engineering, test inputs are models executed by model transformations. Modelers build them using partial knowledge while meticulously satisfying several well-formedness rules imposed by the modelling language. This manual process is tedious and language constraints can force users to create complex models even for representing simple knowledge. In this paper, we want to simplify the development of test models by presenting an integrated methodology and semi-automated tool that allow users to build only small partial test models directly representing their testing intent. We argue that partial models are more readable and maintainable and can be automatically completed to full input models while considering language constraints. We validate this approach by evaluating the size and fault-detecting effectiveness of partial models compared to traditionally-built test models. We show that they can detect the same bugs/faults with a greatly reduced development effort. © 2012 Springer-Verlag.",,"Complex model; Execution trace; Input models; Integrated methodology; Manual process; Model transformation; Model-driven Engineering; Modelling language; Partial knowledge; Semi-automated; Test inputs; Test models; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84862223772
"Canetti R., Dachman-Soled D., Vaikuntanathan V., Wee H.","Efficient password authenticated key exchange via oblivious transfer",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-30057-8_27,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861670666&doi=10.1007%2f978-3-642-30057-8_27&partnerID=40&md5=380d4c3f03a5810517f5d7f41d0a4a63","We present a new framework for constructing efficient password authenticated key exchange (PAKE) protocols based on oblivious transfer (OT). Using this framework, we obtain: an efficient and simple UC-secure PAKE protocol that is secure against adaptive corruptions without erasures. efficient and simple PAKE protocols under the Computational Diffie-Hellman (CDH) assumption and the hardness of factoring. (Previous efficient constructions rely on hash proof systems, which appears to be inherently limited to decisional assumptions.) All of our constructions assume a common reference string (CRS) but do not rely on random oracles. © 2012 International Association for Cryptologic Research.","adaptive security; oblivious transfer; Password Authenticated Key Exchange; search assumptions; UC security","Adaptive security; Oblivious transfer; Password-authenticated key exchange; search assumptions; UC security; Adaptive security; Oblivious transfer; Password-authenticated key exchange; Search assumptions; UC security; Artificial intelligence; Authentication; Computation theory; Public key cryptography; Public key cryptography; Cryptography",Conference Paper,Scopus,2-s2.0-84861670666
"Zhang J., Zhou J., He K., Li H.","Image edge detection using quantum ant colony optimization",2012,"International Journal of Digital Content Technology and its Applications",12,10.4156/jdcta.vol6.issue11.24,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863305167&doi=10.4156%2fjdcta.vol6.issue11.24&partnerID=40&md5=a9c9bd47f9b50c99aae493a91b8c66e4","Ant colony optimization algorithm (ACO) which performs well in discrete optimization has already been used widely and successfully in digital image processing. Slow convergence, however, is an obvious drawback of the traditional ACO. A quantum ant colony algorithm (QACO), based on the concept and principles of quantum computing can overcome this defect. In this study, a QACO-based edge detection algorithm is proposed. Quantum bit (qubit) and quantum rotation gate are introduced into QACO to represent and update the pheromone respectively. Experiments and comparisons show that QACO is an efficient and effective approach in image edge detection.","Ant colony optimization; Edge detection; Quantum ant colony optimization","Ant colony algorithms; Ant Colony Optimization (ACO); Ant Colony Optimization algorithms; Discrete optimization; Edge detection algorithms; Image edge detection; Quantum bit (qubit); Quantum Computing; Quantum rotation; Artificial intelligence; Edge detection; Image processing; Quantum computers; Algorithms",Article,Scopus,2-s2.0-84863305167
"Massanet S., Torrens J.","Intersection of Yager's implications with QL and D-implications",2012,"International Journal of Approximate Reasoning",12,10.1016/j.ijar.2011.11.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858800306&doi=10.1016%2fj.ijar.2011.11.001&partnerID=40&md5=e68f39ef143be3d68b0cf9d120272ea7","The intersection of the different classes of implications is one of the most popular topics nowadays due to the large number of construction methods of these operators. In this paper, we deal with the characterization of the intersection of Yager's implications with QL and D-implications. Some initial steps have already been made with the intersection of Yager's implications with (S, N), R and QL-implications, however some questions remain unanswered. In particular, we solve an open problem related to the characterization of those implications that are both QL-implications and f-generated implications with f(0) < ∞, fully determining the expression of the QL-implications generated by a continuous t-conorm that belong to the considered intersection. Furthermore, we perform a similar study for D-implications and finally, we study the intersection of Yager's implications with their -conjugates. © 2011 Elsevier Inc. All rights reserved.","f-Implication; g-Implication; Implication function; Yager's implications","Construction method; f-Implication; g-Implication; Implication function; Open problems; T-conorms; Yager's implications; Artificial intelligence; Software engineering",Article,Scopus,2-s2.0-84858800306
"Huang H.-H., Hsu J.S.-C., Ku C.-Y.","Understanding the role of computer-mediated counter-argument in countering confirmation bias",2012,"Decision Support Systems",12,10.1016/j.dss.2012.03.009,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862526079&doi=10.1016%2fj.dss.2012.03.009&partnerID=40&md5=db48d87fbe46fd4901441d2fed0948b3","Confirmation bias has long been discussed in the behavioral decision-making research stream. Although decision support systems were designed to counter cognitive biases and speed up information processing, confirmation bias still can be observed during the decision-making process and causes some unwanted behaviors, such as selective reading. An experimental design was conducted to examine the impact of confirmation bias in a computer-supported decision-making context. In addition, we attempted to explore whether the providing of computer-mediated counter-argument can effectively eliminate the impact caused by selective reading. The experiment results show that confirmation bias can be observed when decision makers possess strong preconceptions and selective reading behaviors, caused by confirmation bias, resulting in skewed adjustment and high confidence. This means that computer-mediated counter-arguments can effectively reduce the effects caused by confirmation bias as well as lead to higher satisfaction with the decision outcome. Lastly, the research results were discussed and implications of this finding for academics and practitioners were provided. © 2012 Elsevier B.V. All rights reserved.","Computer-mediated counter-argument; Confirmation bias; De-bias; Decision support systems","Cognitive bias; Computer-mediated counter-argument; Confirmation bias; De-bias; Decision makers; Decision making process; Decision outcome; High confidence; Research results; Artificial intelligence; Data processing; Decision support systems; Decision making",Article,Scopus,2-s2.0-84862526079
"Bhowmik M., Pal M.","Some results on generalized interval-valued intuitionistic fuzzy sets",2012,"International Journal of Fuzzy Systems",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863972755&partnerID=40&md5=0e522755888aff58e350c8a5e9a0fd12","In this paper, we define some results on generalized interval-valued intuitionistic fuzzy sets (GIVIFSs). In fact, all interval-valued intuitionistic fuzzy sets (IVIFSs) are GIVIFSs but all GIVIFSs are not IVIFSs. We define two composite relations, four types of reflexivity and irreflexivity of GIVIFSs with some of their properties. We define generalized interval-valued intuitionistic fuzzy relation (GIVIFR) with some results. Also we define two operators C and I with some properties over GIVIFSs. Finally, an illustrative example is given to using GIVIFSs in a decision making problem. © 2012 TFSA.","C and I operators; GIVIFR; GIVIFS; IVIFS","Decision-making problem; GIVIFR; GIVIFS; Illustrative examples; Interval-valued; Interval-valued intuitionistic fuzzy sets; Intuitionistic fuzzy relations; IVIFS; Artificial intelligence; Software engineering; Fuzzy sets",Conference Paper,Scopus,2-s2.0-84863972755
"Julai S., Tokhi M.","Active vibration control of flexible plate structures with distributed disturbances",2012,"Journal of Low Frequency Noise Vibration and Active Control",12,10.1260/0263-0923.31.2.123,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867295509&doi=10.1260%2f0263-0923.31.2.123&partnerID=40&md5=b0bd785d5cebc3e769df0a163654adb8","This paper presents the development of an active vibration control (AVC) system with distributed disturbances using genetic algorithms, particle swarm optimization, and ant colony optimization. The approaches are realized with multiple-input multiple-output and multiple-input single-output control configurations in a flexible plate structure. A simulation environment characterizing a thin, square plate, with all edges clamped, is developed using the finite difference method as a platform for test and verification of the developed control approaches. Simulations are carried out with random disturbance signal. The control design comprises a direct minimization of the error (observed) signal by allowing a collective determination of detection and secondary source locations together with controller parameters. The algorithms are formulated with a fitness function based on the mean square of the observed vibration level. In this manner, knowledge of the input/output characterization of the system is not required for design of the controller. The performance of the system is assessed and analyzed both in the time and frequency domains and it is demonstrated that significant vibration reduction is achieved with the proposed schemes.","Active vibration control; ant colony optimization; flexible plate structure; genetic algorithms; particle swarm optimization","Artificial intelligence; Controllers; Flexible structures; Genetic algorithms; MIMO systems; Particle swarm optimization (PSO); Active vibration controls; Ant Colony Optimization (ACO); Control approach; Control configuration; Control design; Controller parameter; Direct minimization; Fitness functions; Flexible plates; Input/output; Mean square; Multiple input single outputs; Random disturbances; Simulation environment; Source location; Square plates; Time and frequency domains; Vibration level; Vibration reductions; Vibration control",Article,Scopus,2-s2.0-84867295509
"Schuemie M.J., Sen E., 't Jong G.W., Van Soest E.M., Sturkenboom M.C., Kors J.A.","Automating classification of free-text electronic health records for epidemiological studies",2012,"Pharmacoepidemiology and Drug Safety",12,10.1002/pds.3205,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862170821&doi=10.1002%2fpds.3205&partnerID=40&md5=e255c1eb26450c8efecc2f8b777b66d7","Purpose: Increasingly, patient information is stored in electronic medical records, which could be reused for research. Often these records comprise unstructured narrative data, which are cumbersome to analyze. The authors investigated whether text mining can make these data suitable for epidemiological studies and compared a concept recognition approach and a range of machine learning techniques that require a manually annotated training set. The authors show how this training set can be created with minimal effort by using a broad database query. Methods: The approaches were tested on two data sets: a publicly available set of English radiology reports for which International Classification of Diseases, Ninth Revision, Clinical Modification code needed to be assigned and a set of Dutch GP records that needed to be classified as either liver disorder cases or noncases. Performance was tested against a manually created gold standard. Results: The best overall performance was achieved by a combination of a manually created filter for removing negations and speculations and rule learning algorithms such as RIPPER, with high scores on both the radiology reports (positive predictive value=0.88, sensitivity=0.85, specificity=1.00) and the GP records (positive predictive value=0.89, sensitivity =0.91, specificity =0.76). Conclusions: Although a training set still needs to be created manually, text mining can help reduce the amount of manual work needed to incorporate narrative data in an epidemiological study and will make the data extraction more reproducible. An advantage of machine learning is that it is able to pick up specific language use, such as abbreviations and synonyms used by physicians. © 2012 John Wiley & Sons, Ltd.","Case definition; Free text; Machine learning; Method; Text mining","article; data base; electronic medical record; epidemiology; human; language; learning algorithm; liver disease; machine learning; patient information; physician; predictive value; priority journal; radiology; scoring system; sensitivity and specificity; training; Algorithms; Artificial Intelligence; Automatic Data Processing; Decision Trees; Electronic Health Records; Epidemiologic Studies; International Classification of Diseases; Workflow",Article,Scopus,2-s2.0-84862170821
"Rendl A., Prandtstetter M., Hiermann G., Puchinger J., Raidl G.","Hybrid heuristics for multimodal homecare scheduling",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-29828-8_22,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861435805&doi=10.1007%2f978-3-642-29828-8_22&partnerID=40&md5=63440528cd9c129fe5e36dfa0b61f9a8","We focus on hybrid solution methods for a large-scale real-world multimodal homecare scheduling (MHS) problem, where the objective is to find an optimal roster for nurses who travel in tours from patient to patient, using different modes of transport. In a first step, we generate a valid initial solution using Constraint Programming (CP). In a second step, we improve the solution using one of the following metaheuristic approaches: (1) variable neighborhood descent, (2) variable neighborhood search, (3) an evolutionary algorithm, (4) scatter search and (5) a simulated annealing hyper heuristic. Our evaluation, based on computational experiments, demonstrates how hybrid approaches are particularly strong in finding promising solutions for large real-world MHS problem instances. © 2012 Springer-Verlag.",,"Computational experiment; Constraint programming; Different modes; Homecare; Hybrid approach; Hybrid heuristics; Hybrid solution methods; Hyperheuristic; Initial solution; Meta-heuristic approach; Multi-modal; Problem instances; Scatter search; Variable neighborhood search; Artificial intelligence; Combinatorial optimization; Computer programming; Scheduling; Simulated annealing; Constraint theory",Conference Paper,Scopus,2-s2.0-84861435805
"Alatrista Salas H., Bringay S., Flouvat F., Selmaoui-Folcher N., Teisseire M.","The pattern next door: Towards spatio-sequential pattern discovery",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-30220-6_14,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861424440&doi=10.1007%2f978-3-642-30220-6_14&partnerID=40&md5=fba1737d18d9990be512a368797278ec","Health risks management such as epidemics study produces large quantity of spatio-temporal data. The development of new methods able to manage such specific characteristics becomes crucial. To tackle this problem, we define a theoretical framework for extracting spatio-temporal patterns (sequences representing evolution of locations and their neighborhoods over time). Classical frequency support doesn't consider the pattern neighbor neither its evolution over time. We thus propose a new interestingness measure taking into account both spatial and temporal aspects. An algorithm based on pattern-growth approach with efficient successive projections over the database is proposed. Experiments conducted on real datasets highlight the relevance of our method. © 2012 Springer-Verlag.",,"Interestingness measures; Pattern discovery; Real data sets; Risks management; Spatio-temporal data; Spatiotemporal patterns; Temporal aspects; Theoretical framework; Artificial intelligence; Data mining",Conference Paper,Scopus,2-s2.0-84861424440
"Letichevsky A.A., Letychevskyi O.A., Peschanenko V.S.","Insertion modeling system",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-29709-0_23,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862106910&doi=10.1007%2f978-3-642-29709-0_23&partnerID=40&md5=c4a25fa15a3b3a3048200738174f8394","The paper relates to practical aspects of insertion modeling. Insertion modeling system is an environment for the development of insertion machines, used to represent insertion models of distributed systems. The architecture of insertion machines and insertion modeling system IMS is presented. Insertion machine for program verification is specified as an example, and as a starting point of 'verifiable programming' project. © 2012 Springer-Verlag Berlin Heidelberg.",,"Distributed systems; Insertion machine; Modeling systems; Program Verification; Artificial intelligence; Information science",Conference Paper,Scopus,2-s2.0-84862106910
"Pietraszek J.","Fuzzy regression compared to classical experimental design in the case of flywheel assembly",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-29347-4_36,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861021102&doi=10.1007%2f978-3-642-29347-4_36&partnerID=40&md5=e2df627d8f7dd81f1cea30c44b31ef22","This paper presents the fuzzy regression approach to the automotive industry optimization problem. The flywheel assembly process is subject to investigation, as its parameters require optimization. The paper contains: problem definition, presentation of the measured data and the final analysis with two alternative approaches: the fuzzy regression and the classical regression. The benefits of the fuzzy regression approach are shown in the case of small size samples. © 2012 Springer-Verlag Berlin Heidelberg.","design of experiment; fuzzy regression","Alternative approach; Assembly process; Fuzzy regressions; Measured data; Optimization problems; Problem definition; Small size samples; Artificial intelligence; Automotive industry; Design of experiments; Flywheels; Optimization; Soft computing; Wheels; Regression analysis",Conference Paper,Scopus,2-s2.0-84861021102
"Fister Jr. I., Fister I., Brest J.","A hybrid artificial bee colony algorithm for graph 3-coloring",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-29353-5_8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860678410&doi=10.1007%2f978-3-642-29353-5_8&partnerID=40&md5=e8a689da639f9be930d846c1b7cfef1f","The Artificial Bee Colony (ABC) is the name of an optimization algorithm that was inspired by the intelligent behavior of a honey bee swarm. It is widely recognized as a quick, reliable, and efficient methods for solving optimization problems. This paper proposes a hybrid ABC (HABC) algorithm for graph 3-coloring, which is a well-known discrete optimization problem. The results of HABC are compared with results of the well-known graph coloring algorithms of today, i.e., the Tabucol and Hybrid Evolutionary algorithm (HEA), and results of the traditional evolutionary algorithm with SAW method (EA-SAW). Extensive experimentations has shown that the HABC matched the competitive results of the best graph coloring algorithms, and did better than the traditional heuristics EA-SAW when solving equi-partite, flat, and random generated medium-sized graphs. © 2012 Springer-Verlag.","artificial bee colony optimization; bee's behavior; combinatorial optimization; graph 3-coloring; swarm intelligence","3-coloring; Artificial bee colonies; Artificial bee colony algorithms; bee's behavior; Discrete optimization problems; Graph colorings; Honey bee; Hybrid evolutionary algorithm; Intelligent behavior; Optimization algorithms; Optimization problems; Swarm Intelligence; Artificial intelligence; Combinatorial optimization; Graph theory; Optimization; Soft computing; Evolutionary algorithms",Conference Paper,Scopus,2-s2.0-84860678410
"Szczepanik M., Poteralski A., Ptaszny J., Burczyński T.","Hybrid particle swarm optimizer and its application in identification of room acoustic properties",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-29353-5_45,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860690245&doi=10.1007%2f978-3-642-29353-5_45&partnerID=40&md5=bf783bfcb99462545033d4cb33cc2414","The paper deals with an application of an hybrid particle swarm optimizer (HPSO) to identification problems. The HPSO is applied to identify complex impedances of room walls and it is based on the mechanism discovered in the nature during observations of the animals social behaviour and supplemented with some additional gradient information. The numerical example demonstrate that the method based on hybrid swarm optimization is an effective technique for computing in identification problems. © 2012 Springer-Verlag.","acoustics; hybrid computational optimization algorithm; identification; method of fundamental solutions; particle swarm optimizer","Complex impedance; Gradient informations; Hybrid computational; Hybrid particles; Identification problem; Method of fundamental solutions; Numerical example; Particle swarm optimizers; Social behaviour; Swarm optimization; Acoustic properties; Acoustics; Algorithms; Animals; Architectural acoustics; Artificial intelligence; Bioacoustics; Identification (control systems); Numerical analysis; Soft computing; Particle swarm optimization (PSO)",Conference Paper,Scopus,2-s2.0-84860690245
"Ding X., Rose J.P., Van Gelder J.","Developability assessment of clinical drug products with maximum absorbable doses",2012,"International Journal of Pharmaceutics",12,10.1016/j.ijpharm.2012.02.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859104300&doi=10.1016%2fj.ijpharm.2012.02.003&partnerID=40&md5=349d2b73e0dd716d78bcaa8df5da2994","Maximum absorbable dose refers to the maximum amount of an orally administered drug that can be absorbed in the gastrointestinal tract. Maximum absorbable dose, or Dabs, has proved to be an important parameter for quantifying the absorption potential of drug candidates. The purpose of this work is to validate the use of Dabs in a developability assessment context, and to establish appropriate protocol and interpretation criteria for this application. Three methods for calculating Dabs were compared by assessing how well the methods predicted the absorption limit for a set of real clinical candidates. Dabs was calculated for these clinical candidates by means of a simple equation and two computer simulation programs, GastroPlus™ and an program developed at Eli Lilly and Company. Results from single dose escalation studies in Phase I clinical trials were analyzed to identify the maximum absorbable doses for these compounds. Compared to the clinical results, the equation and both simulation programs provide conservative estimates of Dabs, but in general Dabs from the computer simulations are more accurate, which may find obvious advantage for the simulations in developability assessment. Computer simulations also revealed the complex behavior associated with absorption saturation and suggested in most cases that the Dabs limit is not likely to be achieved in a typical clinical dose range. On the basis of the validation findings, an approach is proposed for assessing absorption potential, and best practices are discussed for the use of Dabs estimates to inform clinical formulation development strategies. © 2012 Elsevier B.V. All rights reserved.","Developability assessment; Drug absorption; GastroPlus™; Maximum absorbable dose; Simulation","atenolol; carbamazepine; cimetidine; desipramine; enalaprilat; fluindostatin; fosinopril; furosemide; hydrochlorothiazide; ketoprofen; metoprolol; naproxen; phenazone; propranolol; ranitidine; terbutaline; verapamil; article; computer program; computer simulation; drug absorption; drug formulation; drug penetration; gastrointestinal absorption; jejunum; maximum absorbable dose; pharmacological parameters; priority journal; validation study; Administration, Oral; Algorithms; Area Under Curve; Artificial Intelligence; Chemistry, Pharmaceutical; Chromatography, High Pressure Liquid; Computer Simulation; Drug Design; Intestinal Absorption; Particle Size; Permeability; Pharmaceutical Preparations; Solubility; Spectrophotometry, Ultraviolet",Article,Scopus,2-s2.0-84859104300
"Lu Y., Huo X., Tsiotras P.","A beamlet-based graph structure for path planning using multiscale information",2012,"IEEE Transactions on Automatic Control",12,10.1109/TAC.2012.2191836,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860425586&doi=10.1109%2fTAC.2012.2191836&partnerID=40&md5=a6e3c7d49613680a5564a4129c1155e0","Path-planning problems are fundamental in many applications, such as transportation, artificial intelligence, control of autonomous vehicles, and many more. In this paper, we consider the deterministic path-planning problem, equivalently, the single-pair shortest path problem on a given grid-like graph structure. Current commonly used algorithms in this area include the ${\rm A} \ast algorithm, Dijkstra's algorithm, and their numerous variants. We propose an innovative beamlet-based graph structure for path planning that utilizes multiscale information of the environment. This information is collected via a bottom-up fusion algorithm. This new graph structure goes beyond nearest-neighbor connectivity, incorporating long-distance interactions between the nodes of the graph. Based on this new graph structure, we obtain a multiscale version of ${\rm A} \ast , which is advantageous when preprocessing is allowable and feasible. Compared to the benchmark ${\rm A} \ast algorithm, the use of multiscale information leads to an improvement in terms of computational complexity. Numerical experiments indicate an even more favorable behavior than the one predicted by the theoretical complexity analysis. © 2012 IEEE.","${\rm A} \ast; beamlet-like structure; bottom-up fusion algorithm; Dijkstra's algorithm; dynamic programming; path-planning","Autonomous Vehicles; Dijkstra's algorithms; Fusion algorithms; Graph structures; Multiscale information; Path planning problems; Shortest path problem; Theoretical complexity; Algorithms; Artificial intelligence; Dynamic programming; Graphic methods; Information use; Motion planning; Graph theory",Article,Scopus,2-s2.0-84860425586
"Liu C., Qi G., Wang H., Yu Y.","Reasoning with large scale ontologies in fuzzy pD* using MapReduce",2012,"IEEE Computational Intelligence Magazine",12,10.1109/MCI.2012.2188589,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860186585&doi=10.1109%2fMCI.2012.2188589&partnerID=40&md5=84b43048396af33369cbbfeb4fcd5a6b","The MapReduce framework has proved to be very efficient for data-intensive tasks. Earlier work has successfully applied MapReduce for large scale RDFS/OWL reasoning. In this paper, we move a step forward by considering scalable reasoning on semantic data under fuzzy pD* semantics (i.e., an extension of OWL pD* semantics with fuzzy vagueness). To the best of our knowledge, this is the first work to investigate how MapReduce can be applied to solve the scalability issue of fuzzy reasoning in OWL. While most of the optimizations considered by the existing MapReduce framework for pD* semantics are also applicable for fuzzy pD* semantics, unique challenges arise when we handle the fuzzy information. Key challenges are identified with solution proposed for each of these challenges. Furthermore, a prototype system is implemented for the evaluation purpose. The experimental results show that the running time of our system is comparable with that of WebPIE, the state-of-the-art inference engine for scalable reasoning in pD* semantics. © 2012 IEEE.",,"Fuzzy information; Fuzzy PD; Fuzzy reasoning; Large-scale ontologies; Map-reduce; Prototype system; Running time; Scalability issue; Scalable reasonings; Semantic data; Artificial intelligence; Computer science; Semantics",Article,Scopus,2-s2.0-84860186585
"Egghe L., Rousseau R.","The Hirsch index of a shifted Lotka function and its relation with the impact factor",2012,"Journal of the American Society for Information Science and Technology",12,10.1002/asi.22617,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860626339&doi=10.1002%2fasi.22617&partnerID=40&md5=0987b68de936e84a16d13f2ba74f90ae","Based on earlier results about the shifted Lotka function, we prove an implicit functional relation between the Hirsch index (h-index) and the total number of sources (T). It is shown that the corresponding function, h(T), is concavely increasing. Next, we construct an implicit relation between the h-index and the impact factor IF (an average number of items per source). The corresponding function h(IF) is increasing and we show that if the parameter C in the numerator of the shifted Lotka function is high, then the relation between the h-index and the impact factor is almost linear. © 2012 ASIS&T.","bibliometrics","Average numbers; Bibliometrics; Functional relation; H indices; Impact factor; Artificial intelligence; Software engineering; Indexing (of information)",Article,Scopus,2-s2.0-84860626339
"Schneider M., Chen T., Viswanathan G., Yuan W.","Cardinal directions between complex regions",2012,"ACM Transactions on Database Systems",12,10.1145/2188349.2188350,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863607820&doi=10.1145%2f2188349.2188350&partnerID=40&md5=e19bdc038e6327028f74495ae8fef2ec","Besides topological relationships and approximate relationships, cardinal directions like north and southwest have turned out to be an important class of qualitative spatial relationships. They are of interdisciplinary interest in fields like cognitive science, robotics, artificial intelligence, and qualitative spatial reasoning. In spatial databases and Geographic Information Systems (GIS) they are frequently used as join and selection criteria in spatial queries. However, the available computational models of cardinal directions suffer a number of problems like the use of too coarse approximations of the two spatial operand objects in terms of single representative points or minimum bounding rectangles, the lacking property of converseness of the cardinal directions computed, and the limited applicability to simple instead of complex regions only. This article proposes and formally defines a novel two-phase model, called the Objects Interaction Matrix (OIM) model, that solves these problems, and determines cardinal directions for even complex regions. The model consists of a tiling phase and an interpretation phase. In the tiling phase, a tiling strategy first determines the zones belonging to the nine cardinal directions of each individual region object and then intersects them. The result leads to a bounded grid called objects interaction grid. For each grid cell the information about the region objects that intersect it is stored in an objects interaction matrix. In the subsequent interpretation phase, a well-defined interpretation method is applied to such a matrix and determines the cardinal direction. Spatial example queries illustrate our new cardinal direction concept that is embedded in a spatial extension of SQL and provides user-defined cardinal direction predicates. © 2012 ACM.","Cardinal direction; Directional relationship; Spatial database","Cardinal direction; Cognitive science; Computational model; Directional relationship; Grid cells; In-field; Interaction matrices; Interpretation methods; Minimum bounding rectangle; Qualitative spatial reasoning; Selection criteria; Spatial database; Spatial extension; Spatial queries; Spatial relationships; Topological relationships; Two-phase model; Artificial intelligence; Query languages; Geographic information systems",Article,Scopus,2-s2.0-84863607820
"Domínguez A., Martínez R.S., De Juan J.A., Martínez-Romero A., Tarjuelo J.M.","Simulation of maize crop behavior under deficit irrigation using MOPECO model in a semi-arid environment",2012,"Agricultural Water Management",12,10.1016/j.agwat.2012.01.006,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858001533&doi=10.1016%2fj.agwat.2012.01.006&partnerID=40&md5=8c87c461066c14df5d063b9464acec20","Through the experimental data of a three year maize field test, this paper shows the calibration (year 2003) and validation (years 2001 and 2002) processes for the simulation of FAO-700 maize crop (Dracma and Brasco hybrids) under deficit irrigation conditions in Castilla-La Mancha (Spain) using the MOPECO model. Key objectives of this research were: (1) determining the length of growth stages using the growing-degree-days (GDD) method; (2) an analysis of the effect of saline irrigation water on crop yield; and (3) a sensitivity analysis of the main factors affecting gross margin (i.e. irrigation water cost, harvest sale price, and uniformity of irrigation). Results show that MOPECO is suitable for simulating the yield versus total water (gross irrigation+effective rainfall) relationships under the climatic and soil conditions in this study (RMSE=1199kgha -1, relative error=7.7%, and similarity rate=0.94, between observed and simulated yields). GDD for the whole growth cycle is around 1802.8°C, while calibrated crop coefficient (Kc) and crop yield response (ky) values for the four growth stages proposed by FAO-56 are similar to those presented in the literature, which provides evidence that this methodology is appropriate for simulating the behavior of herbaceous crops such as maize under different scenarios of water supply. The electrical conductivity of the irrigation water in the area (0.85dSm -1) does not significantly affect the final yield of this crop (up to 4.7% higher without considering the effect of salinity) for the irrigation amount commonly applied in the area (around 650mm). Under the current harvest sale price scenario (0.15€kg -1), costs have greatly decreased the profitability of maize in the area, reaching 400€ha -1. Changing the management of the crop for deficit irrigation strategies may save water that can be used by other crops, thereby obtaining a higher gross margin on farms. © 2012 Elsevier B.V..","Decision support system; Gross margin; Growing-degree-days; Uniformity of irrigation; Water productivity; Zea mays L.","Crop coefficient; Crop yield; Deficit irrigation; Effective rainfall; Electrical conductivity; Experimental data; Field test; Gross margin; Growing-degree-days; Growth cycle; Growth stages; Irrigation waters; Key objective; Relative errors; Sale price; Saline irrigation; Semi-arid environments; Soil conditions; Water productivity; Zea mays; Artificial intelligence; Computer simulation; Cost benefit analysis; Costs; Crops; Decision support systems; Electric conductivity; Grain (agricultural product); Profitability; Water management; Water supply; Irrigation; agricultural modeling; behavioral ecology; calibration; crop yield; decision support system; growing season; growth rate; irrigation system; maize; model validation; profitability; salinity; semiarid region; water management; water supply; yield response; Castilla-La Mancha; Spain; Zea mays",Article,Scopus,2-s2.0-84858001533
"Li L., Yan S., Yu X., Tan Y.K., Li H.","Robust multiperson detection and tracking for mobile service and social robots",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",12,10.1109/TSMCB.2012.2192107,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866491536&doi=10.1109%2fTSMCB.2012.2192107&partnerID=40&md5=a8ecbaac31e3d5bebc638f5b723fc3d1","This paper proposes an efficient system which integrates multiple vision models for robust multiperson detection and tracking for mobile service and social robots in public environments. The core technique is a novel maximum likelihood (ML)-based algorithm which combines the multimodel detections in mean-shift tracking. First, a likelihood probability which integrates detections and similarity to local appearance is defined. Then, an expectation- maximization (EM)-like mean-shift algorithm is derived under the ML framework. In each iteration, the E-step estimates the associations to the detections, and the M-step locates the new position according to the ML criterion. To be robust to the complex crowded scenarios for multiperson tracking, an improved sequential strategy to perform the mean-shift tracking is proposed. Under this strategy, human objects are tracked sequentially according to their priority order. To balance the efficiency and robustness for real-time performance, at each stage, the first two objects from the list of the priority order are tested, and the one with the higher score is selected. The proposed method has been successfully implemented on real-world service and social robots. The vision system integrates stereo-based and histograms-of-oriented-gradients-based human detections, occlusion reasoning, and sequential mean-shift tracking. Various examples to show the advantages and robustness of the proposed system for multiperson tracking from mobile robots are presented. Quantitative evaluations on the performance of multiperson tracking are also performed. Experimental results indicate that significant improvements have been achieved by using the proposed method. © 1996-2012 IEEE.","Human detection; human tracking; human-robot interaction; mean-shift tracking; multiobject tracking","Detection and tracking; Efficient systems; Expectation Maximization; Human detection; Human Tracking; Mean shift; Mean shift algorithm; Mobile service; Multi-model; Multi-object tracking; New position; Occlusion reasoning; Priority order; Public environment; Quantitative evaluation; Real time performance; Social robots; Stereo-based; Vision model; Vision systems; Algorithms; Computer vision; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; human; methodology; robotics; whole body imaging; Algorithms; Artificial Intelligence; Humans; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Robotics; Whole Body Imaging",Article,Scopus,2-s2.0-84866491536
"Axelrod R.","Launching ""The Evolution of Cooperation""",2012,"Journal of Theoretical Biology",12,10.1016/j.jtbi.2011.04.015,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857921153&doi=10.1016%2fj.jtbi.2011.04.015&partnerID=40&md5=062d9505e367c16fe81f4a0fda600be8","This article describes three aspects of the author's early work on the evolution of the cooperation. First, it explains how the idea for a computer tournament for the iterated Prisoner's Dilemma was inspired by the artificial intelligence research on computer checkers and computer chess. Second, it shows how the vulnerability of simple reciprocity of misunderstanding or misimplementation can be eliminated with the addition of some degree of generosity or contrition. Third, it recounts the unusual collaboration between the author, a political scientist, and William D. Hamilton, an evolutionary biologist. © 2011 Elsevier Ltd.","Cooperation; Generosity; Interdisciplinary research; Prisoner's dilemma; Reciprocity","artificial intelligence; computer simulation; game theory; interdisciplinary approach; article; artificial intelligence; computer program; cooperation; game; human; interdisciplinary research; politics; priority journal; prisoners dilemma; process development; social aspect; social behavior; social interaction; undergraduate student; Animals; Biological Evolution; Cooperative Behavior; Game Theory; Genetics, Population; History, 20th Century; Interprofessional Relations; United States",Article,Scopus,2-s2.0-84857921153
"Xie H., Li Q., Mao X.","Context-aware personalized search based on user and resource profiles in folksonomies",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-29253-8_9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859705013&doi=10.1007%2f978-3-642-29253-8_9&partnerID=40&md5=6ac7cc1434df7d270a172db85b972767","The explosion of collaborative tagging data nowadays prompts an urgent demand upon Web 2.0 communities in assisting users to search interested resources quickly and effectively. Such a requirement entails much research on utilization of tag-based user and resource profiles so as to provide a personalized search in folksonomies. However, one major shortage for existing methods is their uniform treatment of user profile in the same way for each query, hence the search context for each query is ignored. In this paper, we focus on addressing this problem by modeling the search context. To capture and understand user intention, a nested context model is proposed. Furthermore, we conduct the experimental evaluation upon a real life data set, and the experimental result demonstrates that our approach is more effective than baselines. © 2012 Springer-Verlag Berlin Heidelberg.",,"Collaborative tagging; Context models; Context-Aware; Experimental evaluation; Folksonomies; Personalized search; Real life data; Resource profile; Tag-based; User intention; User profile; Web 2.0; Artificial intelligence; Websites",Conference Paper,Scopus,2-s2.0-84859705013
"Amtoft T., Dodds J., Zhang Z., Appel A., Beringer L., Hatcliff J., Ou X., Cousino A.","A certificate infrastructure for machine-checked proofs of conditional information flow",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-28641-4_20,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859309070&doi=10.1007%2f978-3-642-28641-4_20&partnerID=40&md5=9944fe18a075b8a6369fcf2874d65723","In previous work, we have proposed a compositional framework for stating and automatically verifying complex conditional information flow policies using a relational Hoare logic. The framework allows developers and verifiers to work directly with the source code using source-level code contracts. In this work, we extend that approach so that the algorithm for verifying code compliance to an information flow contract emits formal certificates of correctness that are checked in the Coq proof assistant. This framework is implemented in the context of SPARK - a subset of Ada that has been used in a number of industrial contexts for implementing certified safety and security critical systems. © 2012 Springer-Verlag.",,"Code compliance; Coq proof assistant; Critical systems; Hoare Logic; Industrial context; Information flow policies; Information flows; Machine-checked proofs; Source codes; Code compliance; Coq proof assistant; Industrial context; Information flow policies; Information flows; Machine-checked proofs; Relational hoare logic; Safety and securities; Artificial intelligence; Accident prevention; Codes (symbols); Accident prevention; Theorem proving",Conference Paper,Scopus,2-s2.0-84859309070
"Cortier V., Wiedling C.","A formal analysis of the Norwegian e-voting protocol",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-28641-4_7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859338722&doi=10.1007%2f978-3-642-28641-4_7&partnerID=40&md5=48b69edf3554624350061ea2483c7015","Norway has used e-voting in its last political election in September 2011, with more than 25 000 voters using the e-voting option. The underlying protocol is a new protocol designed by the ERGO group, involving several actors (a bulletin box but also a receipt generator, a decryption service, and an auditor). Of course, trusting the correctness and security of e-voting protocols is crucial in that context. Formal definitions of properties such as privacy, coercion-resistance or verifiability have been recently proposed, based on equivalence properties. In this paper, we propose a formal analysis of the protocol used in Norway, w.r.t. privacy, considering several corruption scenarios. Part of this study has conducted using the ProVerif tool, on a simplified model. © 2012 Springer-Verlag.","e-voting; formal methods; privacy","Coercion-resistance; E-Voting; E-voting protocols; Formal analysis; Formal definition; New protocol; Simplified models; Verifiability; Coercion-resistance; E-Voting; E-voting protocols; Formal analysis; Formal definition; New protocol; Proverif; Verifiability; Artificial intelligence; Data privacy; Artificial intelligence; Computer science; Computers; Data privacy; Formal methods; Formal methods",Conference Paper,Scopus,2-s2.0-84859338722
"Goga O., Teixeira R.","Speed measurements of residential internet access",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-28537-0_17,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859085257&doi=10.1007%2f978-3-642-28537-0_17&partnerID=40&md5=5b96e9460099d5201dad55c1498e9fb3","The spread of residential broadband Internet access is raising the question of how to measure Internet speed. We argue that available bandwidth is a key metric of access link speed. Unfortunately, the performance of available bandwidth estimation tools has rarely been tested from hosts connected to residential networks. This paper compares the accuracy and overhead of state-of-the-art available bandwidth estimation tools from hosts connected to commercial ADSL and cable networks. Our results show that, when using default settings, some tools underestimate the available bandwidth by more than 60%. We demonstrate using controlled testbeds that this happens because current home gateways have a limited packet forwarding rate. © 2012 Springer-Verlag Berlin Heidelberg.",,"Access links; Available bandwidth; Available bandwidth estimation; Broadband internet access; Cable networks; Default setting; Home gateway; Internet access; Packet forwarding; Residential networks; Speed measurement; Artificial intelligence; Internet",Conference Paper,Scopus,2-s2.0-84859085257
"Lei X.-J., Huang X., Wu S., Guo L.","Joint strength based ant colony optimization clustering algorithm for PPI networks",2012,"Tien Tzu Hsueh Pao/Acta Electronica Sinica",12,10.3969/j.issn.0372-2112.2012.04.012,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862676994&doi=10.3969%2fj.issn.0372-2112.2012.04.012&partnerID=40&md5=7453c62b65e4244c047cb12be38aeda1","Due to the sale-free and small-world characters of Protein-Protein Interaction (PPI) network data, current clustering algorithms did not perform well. According to the topological structural characters of PPI networks, this paper proposed an ant colony optimization clustering algorithm based on joint strength (JSACO). This method modified the pickup/drop rules of ACO algorithm by means of introducing the concept of joint strength, which regarded the joint strength as pickup rule to cluster the protein nodes. In addition, the protein nodes which had the low joint strength were abandoned in accordance with drop rule and the final clustering result was obtained. Finally the PPI data in MIPS database was used to test the algorithm and the clustering result was compared with other PPI clustering methods. The simulation results show that JSACO algorithm performs better in terms of precision value and consumes less time.","ACO algorithm; Clustering; Joint strength; PPI Network","ACO algorithms; Ant Colony Optimization (ACO); Clustering; Clustering methods; Clustering results; Joint strength; Protein-protein interaction networks; Small worlds; Structural character; Artificial intelligence; Proteins; Clustering algorithms",Article,Scopus,2-s2.0-84862676994
"Umarov I., Mozgovoy M., Rogers P.C.","Believable and effective AI agents in virtual worlds: Current state and future perspectives",2012,"International Journal of Gaming and Computer-Mediated Simulations",12,10.4018/jgcms.2012040103,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870277781&doi=10.4018%2fjgcms.2012040103&partnerID=40&md5=c35b1557d0c57488cbaa3b30f5afbe35","The rapid development of complex virtual worlds (most notably, in 3D computer and video games) introduces new challenges for the creation of virtual agents, controlled by artificial intelligence (AI) systems. Two important subproblems in this topic area which need to be addressed are (a) believability and (b) effectiveness of agents' behavior, i.e., human-likeness of the characters and high ability to achieving their own goals. In this paper, the authors study current approaches to believability and effectiveness of AI behavior in virtual worlds. They examine the concepts of believability and effectiveness, and analyze several successful attempts to address these challenges. Copyright © 2012, IGI Global.","AI agents; AI for games; Artificial intelligence (AI); Behavior capture; Believability; Learning by observation","Behavior capture; Believability; Future perspectives; Learning by observation; Sub-problems; Topic areas; Video game; Virtual agent; Virtual worlds; Artificial intelligence; Virtual reality; Interactive computer graphics",Article,Scopus,2-s2.0-84870277781
"Salamin H., Vinciarelli A.","Automatic role recognition in multiparty conversations: An approach based on turn organization, prosody, and conditional random fields",2012,"IEEE Transactions on Multimedia",12,10.1109/TMM.2011.2173927,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859021710&doi=10.1109%2fTMM.2011.2173927&partnerID=40&md5=c2dfbcafb5de9add532addbd63078e1c","Roles are a key aspect of social interactions, as they contribute to the overall predictability of social behavior (a necessary requirement to deal effectively with the people around us), and they result in stable, possibly machine-detectable behavioral patterns (a key condition for the application of machine intelligence technologies). This paper proposes an approach for the automatic recognition of roles in conversational broadcast data, in particular, news and talk shows. The approach makes use of behavioral evidence extracted from speaker turns and applies conditional random fields to infer the roles played by different individuals. The experiments are performed over a large amount of broadcast material (around 50 h), and the results show an accuracy higher than 85%. © 2011 IEEE.","Conditional random fields (CRFs); prosody; role recognition; turn organization","Automatic recognition; Behavioral patterns; Broadcast data; Conditional random field; Conditional random fields (CRFs); Machine intelligence; Multi-party conversations; prosody; role recognition; Social behavior; Social interactions; Talk shows; Artificial intelligence; Random processes; Content based retrieval",Article,Scopus,2-s2.0-84859021710
"Taner A.","Prediction of moisture dependent some physical properties of wheat using artificial neural network and fuzzy logic",2012,"Energy Education Science and Technology Part A: Energy Science and Research",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861981219&partnerID=40&md5=b45a8faf895a04f8510e6746b3dd11dd","Artificial intelligence systems are widely accepted as a technology offering an alternative way to tackle complex and ill-defined problems. They can learn from examples, are fault tolerant in the sense that they are able to handle noisy and incomplete data, are able to deal with non-linear problems, and once trained can perform prediction and generalization at high speed. In this study, the mass, geometric mean diameter and rupture force of wheat seed were measured at different levels of moisture content (9.93-19.01% w.b.). An artificial neural network (ANN), fuzzy logic (FL) and regression models were developed to predict the mass, geometric mean diameter and rupture force of wheat seed. The ANN and FL models had one input parameter and three output parameters. The results obtained with the experimental methods were compared with ANN, FL and regression model results. The results showed that ANN, FL and regression model can be alternative approaches for the predicting of physical properties of wheat seed, but the best results have been obtained with the ANN model. © Sila Science.","Artificial Neural Network; Fuzzy Logic; Physical Properties; Wheat","Alternative approach; Artificial intelligence systems; Artificial neural network and fuzzy logic; Experimental methods; Fault-tolerant; Geometric mean; Incomplete data; Input parameter; Nonlinear problems; Output parameters; Regression model; Rupture forces; Wheat; Wheat seeds; Fuzzy logic; Neural networks; Physical properties; Regression analysis; Forecasting",Article,Scopus,2-s2.0-84861981219
"Córcoles J.I., De Juan J.A., Ortega J.F., Tarjuelo J.M., Moreno M.A.","Evaluation of Irrigation Systems by Using Benchmarking Techniques",2012,"Journal of Irrigation and Drainage Engineering",12,10.1061/(ASCE)IR.1943-4774.0000386,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859042454&doi=10.1061%2f%28ASCE%29IR.1943-4774.0000386&partnerID=40&md5=aae8c2bd6f8de43bb5818c2e273ecd7d","Water scarcity, which is typical of arid and semiarid regions such as Castilla-La Mancha (Spain), makes necessary the efficient use of water and energy resources. For this aim, there are several decision support system tools, including the benchmarking technique. The aim of this work is to compare two of the most extended irrigation systems (sprinkler and drip irrigation systems) in Castilla-La Mancha (Spain) by using performance indicators related to management of irrigated areas. The benchmarking technique was applied during three irrigation seasons (2006-2008) in six water users associations (WUAs) in Castilla-La Mancha Region (Spain). The indicators utilized in the benchmarking techniques were classified into two groups: descriptive and performance indicators. The information required to calculate the indicators was obtained from managers and farmers of each WUA, complemented with data obtained by using specialized equipment. To determine the grouping and differences between WUAs with different irrigation systems, a cluster analysis was applied. The results obtained showed a notable difference between WUAs with drip irrigation systems and WUAs with sprinkler irrigation systems. Regarding crop water management, in drip irrigation systems, the amount of irrigation water applied in most of the plots is slightly higher than crop water requirements, which were computed considering a regulated deficit irrigation management. This can be explained by the fact that farmers of areas with drip irrigation systems did not apply deficit irrigation management. © 2012 American Society of Civil Engineers.","Benchmarking; Irrigation management; Irrigation system; Performance indicators","Arid and semi-arid regions; Benchmarking techniques; Crop water requirements; Deficit irrigation; Drip irrigation systems; Efficient use of water; Irrigation management; Irrigation system; Irrigation systems; Irrigation waters; Performance indicators; Regulated deficit irrigation; Specialized equipment; Sprinkler irrigation; Water scarcity; Water users associations; Arid regions; Artificial intelligence; Cluster analysis; Crops; Decision support systems; Energy resources; Sprinkler systems (irrigation); Water management; Water supply; Benchmarking; benchmarking; crop production; decision support system; drip irrigation; irrigation system; performance assessment; resource scarcity; water management; water use; Castilla-La Mancha; Spain",Article,Scopus,2-s2.0-84859042454
"Cho C., Ostrovsky R., Scafuro A., Visconti I.","Simultaneously resettable arguments of knowledge",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-28914-9_30,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858322538&doi=10.1007%2f978-3-642-28914-9_30&partnerID=40&md5=3248b375e1e81cd464e04b321d871dea","In this work, we study simultaneously resettable arguments of knowledge. As our main result, we show a construction of a constant-round simultaneously resettable witness-indistinguishable argument of knowledge (simres AoK, for short) for any NP language. We also show two applications of simres AoK: the first constant-round simultaneously resettable zero-knowledge argument of knowledge in the Bare Public-Key Model; and the first simultaneously resettable identification scheme which follows the knowledge extraction paradigm. © 2012 Springer-Verlag.",,"Identification scheme; Knowledge extraction; Public-key model; Zero knowledge; Identification scheme; Knowledge extraction; Public-key model; Zero knowledge; Artificial intelligence; Artificial intelligence; Computers; Cryptography; Cryptography",Conference Paper,Scopus,2-s2.0-84858322538
"Knoop J., Kovács L., Zwirchmayr J.","R-TuBound: Loop bounds for WCET analysis (tool paper)",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-28717-6_34,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858314395&doi=10.1007%2f978-3-642-28717-6_34&partnerID=40&md5=c0f683c3067a8c42c1bc7f898512fa01","We describe the structure and the usage of a new software tool, called r-TuBound, for deriving symbolic loop iteration bounds in the worst-case execution time (WCET) analysis of programs. r-TuBound implements algorithms for pattern-based recurrence solving and program flow refinement, and it was successfully tested on a wide range of examples. The purpose of this article is to illustrate what r-TuBound can do and how it can be used to derive the WCET of programs. © 2012 Springer-Verlag.",,"Loop iteration; Program flow; Worst-case execution time analysis; Artificial intelligence; Software testing",Conference Paper,Scopus,2-s2.0-84858314395
"Bench-Capon T.J.M.","Representing Popov v Hayashi with dimensions and factors",2012,"Artificial Intelligence and Law",12,10.1007/s10506-012-9118-7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861527204&doi=10.1007%2fs10506-012-9118-7&partnerID=40&md5=8a5f67de8d2af59585945a2e40039249","Modelling reasoning with legal cases has been a central concern of AI and Law since the 1980s. The approach which represents cases as factors and dimensions has been a central part of that work. In this paper I consider how several varieties of the approach can be applied to the interesting case of Popov v Hayashi. After briefly reviewing some of the key landmarks of the approach, the case is represented in terms of factors and dimensions, and further explored using theory construction and argumentation schemes approaches. © 2012 Springer Science+Business Media B.V.","Case based reasoning; Dimensions; Factors","AI and law; Argumentation schemes; Dimensions; Factors; Legal case; Theory construction; Artificial intelligence; Management; Case based reasoning",Conference Paper,Scopus,2-s2.0-84861527204
"Bilgin C.C., Ray S., Baydil B., Daley W.P., Larsen M., Yener B.","Multiscale feature analysis of salivary gland branching morphogenesis",2012,"PLoS ONE",12,10.1371/journal.pone.0032906,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857695989&doi=10.1371%2fjournal.pone.0032906&partnerID=40&md5=c02eec0b39b4d876cfcdd980342e0cee","Pattern formation in developing tissues involves dynamic spatio-temporal changes in cellular organization and subsequent evolution of functional adult structures. Branching morphogenesis is a developmental mechanism by which patterns are generated in many developing organs, which is controlled by underlying molecular pathways. Understanding the relationship between molecular signaling, cellular behavior and resulting morphological change requires quantification and categorization of the cellular behavior. In this study, tissue-level and cellular changes in developing salivary gland in response to disruption of ROCK-mediated signaling by are modeled by building cell-graphs to compute mathematical features capturing structural properties at multiple scales. These features were used to generate multiscale cell-graph signatures of untreated and ROCK signaling disrupted salivary gland organ explants. From confocal images of mouse submandibular salivary gland organ explants in which epithelial and mesenchymal nuclei were marked, a multiscale feature set capturing global structural properties, local structural properties, spectral, and morphological properties of the tissues was derived. Six feature selection algorithms and multiway modeling of the data was performed to identify distinct subsets of cell graph features that can uniquely classify and differentiate between different cell populations. Multiscale cell-graph analysis was most effective in classification of the tissue state. Cellular and tissue organization, as defined by a multiscale subset of cell-graph features, are both quantitatively distinct in epithelial and mesenchymal cell types both in the presence and absence of ROCK inhibitors. Whereas tensor analysis demonstrate that epithelial tissue was affected the most by inhibition of ROCK signaling, significant multiscale changes in mesenchymal tissue organization were identified with this analysis that were not identified in previous biological studies. We here show how to define and calculate a multiscale feature set as an effective computational approach to identify and quantify changes at multiple biological scales and to distinguish between different states in developing tissues. © 2012 Bilgin et al.",,"4 (1 aminoethyl) n (4 pyridyl)cyclohexanecarboxamide; Rho associated coiled coil kinase 1; Rho kinase; unclassified drug; protein kinase inhibitor; Rho kinase; animal cell; animal tissue; article; branching morphogenesis; calculation; cell function; cell graph; cell level; cell population; cell shape; cell type; classification algorithm; controlled study; embryo; epithelium; explant; female; image analysis; mathematical analysis; mathematical model; mesenchyme; mouse; multiscale feature analysis; nonhuman; organ shape; physical parameters; signal transduction; submandibular gland; tensor analysis; tissue level; tissue shape; animal; artificial intelligence; biological model; cell nucleus; computer graphics; cytology; drug antagonism; drug effect; epithelium cell; growth, development and aging; mesoderm; metabolism; molecular imaging; morphogenesis; reproducibility; salivary gland; Animals; Artificial Intelligence; Cell Nucleus; Computer Graphics; Epithelial Cells; Mesoderm; Mice; Models, Biological; Molecular Imaging; Morphogenesis; Protein Kinase Inhibitors; Reproducibility of Results; rho-Associated Kinases; Salivary Glands; Signal Transduction",Article,Scopus,2-s2.0-84857695989
"Liu F.","A dual population parallel ant colony optimization algorithm for solving the traveling salesman problem",2012,"Journal of Convergence Information Technology",12,10.4156/jcit.vol7.issue5.9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859321088&doi=10.4156%2fjcit.vol7.issue5.9&partnerID=40&md5=fe5bfd56122c18042d3f70c7ff0b77c1","In allusion to the phenomenon of stagnation and precocity during evolution in ant colony optimization (ACO) algorithm, this paper proposed a dual population parallel ant colony optimization (DPPACO) algorithm, which was applied to the traveling salesman problem. The DPPACO algorithm separated the ants into soldier ant population and worker ant population which evolve separately by parallel method and exchanges information timely. The dynamic equilibrium between solution diversity and convergence speed is achieved by using the effect of the soldier ant's distribution to worker ants' movement choice. The DPPACO algorithm can enlarge searching range and avoid local minimum, prevent local convergence caused by misbalance of the pheromone and can improve the searching performance of the algorithm effectively. The proposed algorithm is applied in the traveling salesman problem by using the 17 data sets obtained from the TSPLIB. We compare the experimental results of the proposed DPPACO method with the traditional methods. The experimental results demonstrate that the proposed algorithm has a better global searching ability, higher convergence speed and solution diversity.","Ant colony optimization; Diversity; Dual population; Parallel; Traveling salesman problem","Ant Colony Optimization (ACO); Diversity; Dual population; Parallel; Traveling salesman problems (TSP); Artificial intelligence; Evolutionary algorithms; Traveling salesman problem",Article,Scopus,2-s2.0-84859321088
"Marsal J., Stöckle C.O.","Use of CropSyst as a decision support system for scheduling regulated deficit irrigation in a pear orchard",2012,"Irrigation Science",12,10.1007/s00271-011-0273-5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856782188&doi=10.1007%2fs00271-011-0273-5&partnerID=40&md5=d68c440fa01957dd7f36a61e78e1be53","Prediction of plant water status is necessary for the judicious application of regulated deficit irrigation. CropSyst, a generic crop growth model that is applicable to fruit trees, was used to forecast plant water potential for irrigation management recommendations in a pear orchard. Plant water potential is predicted along with tree transpiration using Ohm's law analogy. The parameters of the model were adjusted by using field measured data on a lysimeter-grown pear tree. After adjustment, and using the same lysimeter data, a satisfactory agreement was found between simulated and measured tree transpiration, light interception, and stem water potential. Model simulations were also performed for other independent field data. These corresponded to eight different conditions of a deficit-irrigated field experiment in a pear orchard. Each condition differed in soil texture, time of irrigation cut-off, crop load, and tree leaf area. Deficit irrigation was managed first by withholding irrigation until reaching a threshold in midday stem water potential of -1.5 MPa. Subsequently, irrigation was applied at fixed proportions of full irrigation requirements. Simulations with CropSyst were used as decision support system that could work independently of stem water potential measurements. Simulations in all eight sites were satisfactory at providing adequate time without irrigation during the first part of the deficit period. A highly significant relationship (r 2 = 0.71) between predicted and measured stem water potentials was found for a simulation period of 40 days. Simulations for longer periods (i.e. 74 days) decreased the r 2 to 0.61, and for this reason after resuming irrigation, slight deviations were found for the average stem water potential in two out of five sites. In conclusion, CropSyst produced relevant information for managing deficit irrigation in simulation periods shorter than 40 days. © 2011 Springer-Verlag.",,"Crop growth model; Crop load; Cut-off; Deficit irrigation; Field data; Field experiment; Field-measured data; Fruit trees; Irrigation management; Leaf area; Light interception; Model simulation; Ohm's law; Plant water; Plant water potential; Regulated deficit irrigation; Soil textures; Water potential; Artificial intelligence; Computer simulation; Decision support systems; Fruits; Irrigation; Orchards; Plants (botany); Transpiration; Forestry; agricultural management; data set; decision making; fruit; growth modeling; interception; irrigation; leaf area; lysimeter; numerical model; orchard; plant water relations; soil texture; threshold; transpiration; Pyrus",Article,Scopus,2-s2.0-84856782188
"Bettella F., Rasinski D., Knapp E.W.","Protein secondary structure prediction with SPARROW",2012,"Journal of Chemical Information and Modeling",12,10.1021/ci200321u,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857613586&doi=10.1021%2fci200321u&partnerID=40&md5=d5ac46dcbe97d2f6cd8cbd55346f4df2","A first step toward predicting the structure of a protein is to determine its secondary structure. The secondary structure information is generally used as starting point to solve protein crystal structures. In the present study, a machine learning approach based on a complete set of two-class scoring functions was used. Such functions discriminate between two specific structural classes or between a single specific class and the rest. The approach uses a hierarchical scheme of scoring functions and a neural network. The parameters are determined by optimizing the recall of learning data. Quality control is performed by predicting separate independent test data. A first set of scoring functions is trained to correlate the secondary structures of residues with profiles of sequence windows of width 15, centered at these residues. The sequence profiles are obtained by multiple sequence alignment with PSI-BLAST. A second set of scoring functions is trained to correlate the secondary structures of the center residues with the secondary structures of all other residues in the sequence windows used in the first step. Finally, a neural network is trained using the results from the second set of scoring functions as input to make a decision on the secondary structure class of the residue in the center of the sequence window. Here, we consider the three-class problem of helix, strand, and other secondary structures. The corresponding prediction scheme ""SPARROW"" was trained with the ASTRAL40 database, which contains protein domain structures with less than 40% sequence identity. The secondary structures were determined with DSSP. In a loose assignment, the helix class contains all DSSP helix types (α, 3-10, π), the strand class contains β-strand and β-bridge, and the third class contains the other structures. In a tight assignment, the helix and strand classes contain only α-helix and β-strand classes, respectively. A 10-fold cross validation showed less than 0.8% deviation in the fraction of correct structure assignments between true prediction and recall of data used for training. Using sequences of 140,000 residues as a test data set, 80.46% ± 0.35% of secondary structures are predicted correctly in the loose assignment, a prediction performance, which is very close to the best results in the field. Most applications are done with the loose assignment. However, the tight assignment yields 2.25% better prediction performance. With each individual prediction, we also provide a confidence measure providing the probability that the prediction is correct. The SPARROW software can be used and downloaded on the Web page http://agknapp.chemie.fu-berlin.de/sparrow/. © 2012 American Chemical Society.",,"Confidence Measure; Cross validation; Individual prediction; Learning data; Machine-learning; Multiple sequence alignments; Prediction performance; Prediction schemes; Protein domains; Protein secondary-structure prediction; PSI-BLAST; Scoring functions; Secondary structures; Sequence identity; Structural class; Test data; Web page; Forecasting; Proteins; Teaching; protein; amino acid sequence; article; artificial intelligence; artificial neural network; chemical structure; chemistry; computer program; molecular genetics; protein secondary structure; sequence alignment; sequence analysis; Amino Acid Sequence; Artificial Intelligence; Models, Molecular; Molecular Sequence Data; Neural Networks (Computer); Protein Structure, Secondary; Proteins; Sequence Alignment; Sequence Analysis, Protein; Software",Conference Paper,Scopus,2-s2.0-84857613586
"Ko J., Klues K., Richter C., Hofer W., Kusy B., Bruenig M., Schmid T., Wang Q., Dutta P., Terzis A.","Low power or high performance? A tradeoff whose time has come (and nearly gone)",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-28169-3_7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863145030&doi=10.1007%2f978-3-642-28169-3_7&partnerID=40&md5=1b666abc6f225f983c6e0fd76aac8b0e","Some have argued that the dichotomy between high-performance operation and low resource utilization is false - an artifact that will soon succumb to Moore's Law and careful engineering. If such claims prove to be true, then the traditional 8/16- vs. 32-bit power-performance tradeoffs become irrelevant, at least for some low-power embedded systems. We explore the veracity of this thesis using the 32-bit ARM Cortex-M3 microprocessor and find quite substantial progress but not deliverance. The Cortex-M3, compared to 8/16-bit microcontrollers, reduces latency and energy consumption for computationally intensive tasks as well as achieves near parity on code density. However, it still incurs a ∼2x overhead in power draw for ""traditional"" sense-store-send-sleep applications. These results suggest that while 32-bit processors are not yet ready for applications with very tight power requirements, they are poised for adoption everywhere else. Moore's Law may yet prevail. © 2012 Springer-Verlag.",,"32-Bit Processors; Code density; High-performance operation; Low Power; Moore's Law; Power draw; Power requirement; Resource utilizations; Artificial intelligence; Energy utilization",Conference Paper,Scopus,2-s2.0-84863145030
"Vendramin A.C.K., Munaretto A., Delgado M.R., Viana A.C.","GrAnt: Inferring best forwarders from complex networks' dynamics through a greedy Ant Colony Optimization",2012,"Computer Networks",12,10.1016/j.comnet.2011.10.028,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859105976&doi=10.1016%2fj.comnet.2011.10.028&partnerID=40&md5=a3efdd03fb2345b72a3ae6b742737b25","This paper presents a new prediction-based forwarding protocol for complex and dynamic delay tolerant networks (DTNs). The proposed protocol is called GrAnt (Greedy Ant), as it uses the Ant Colony Optimization (ACO) metaheuristic with a greedy transition rule. This allows GrAnt to select the most promising forwarder nodes or allow for the exploitation of previously found good paths. The main motivation for using ACO is to take advantage of its population-based search and the rapid adaptation of its learning framework. Considering data from heuristic functions and pheromone concentration, the GrAnt protocol includes three modules: routing, scheduling, and buffer management. To the best of our knowledge, this is the first unicast protocol that employs a greedy ACO and that (1) infers best promising forwarders from nodes' social connectivity, (2) determines the best paths a message must follow to eventually reach its destination while limiting message replications and droppings, and (3) performs message transmission scheduling and buffer space management. GrAnt is compared to the Epidemic and PROPHET protocols in two different mobility scenarios: one activity-based scenario (Working Day) and another based on Points of Interest. Simulation results obtained by the ONE simulator show that, in both scenarios, GrAnt achieves a higher delivery ratio, lower message redundancy, and fewer dropped messages than Epidemic or PROPHET. © 2011 Elsevier B.V. All rights reserved.","Bio-inspired optimization; Delay tolerant networks; Node centrality; Routing protocol; Social network","Activity-based; Ant Colony Optimization (ACO); Best paths; Buffer management; Buffer Space Management; Complex networks; Delay tolerant networks; Delivery ratio; Heuristic functions; Learning frameworks; Message redundancy; Message transmissions; Metaheuristic; Node centrality; Points of interest; Prediction-based; Rapid adaptation; Social Networks; Transition rule; Unicast protocol; Heuristic algorithms; Routing protocols; Scheduling; Wireless networks; Artificial intelligence",Article,Scopus,2-s2.0-84859105976
"Mentes A., Helvacioglu I.H.","Fuzzy decision support system for spread mooring system selection",2012,"Expert Systems with Applications",12,10.1016/j.eswa.2011.09.016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80255132788&doi=10.1016%2fj.eswa.2011.09.016&partnerID=40&md5=3a8a04a1366d8712cd9b71b1799899df","Spread mooring systems are associated with high level uncertainties and risks during tanker loading/unloading operations. In addition, the design of such complex systems consists of many subjective and imprecise parameters. Therefore, in the present study, a fuzzy based decision support methodology is designed to overcome aforementioned characteristics. The fuzzy methodology incorporates qualitative and partially known information into the decision support model and provides a robust mathematical framework for modeling of the spread mooring systems. Proposed model is based on the analytic hierarchy method (AHP) and the technique for order performance by similarity to ideal solution (TOPSIS) methods in a fuzzy environment where the vagueness and subjectivity are handled with linguistic and fuzzy values. The fuzzy AHP method is used to analyze the structure of the mooring system selection problem and determine the weights of the attributes while fuzzy TOPSIS method is employed for ranking the spread mooring systems. The case study demonstrates the effectiveness and feasibility of the proposed methodology. © 2011 Elsevier Ltd. All rights reserved.","AHP; Decision making; Fuzzy multiple attribute decision making; Fuzzy set theory; Spread mooring system; TOPSIS","AHP; Analytic hierarchy; Decision support models; Decision supports; Fuzzy AHP; Fuzzy decision support system; Fuzzy environments; Fuzzy multiple attribute decision making; FUZZY TOPSIS; Ideal solutions; Mathematical frameworks; Mooring system; Spread mooring systems; TOPSIS; Artificial intelligence; Decision making; Decision support systems; Decision theory; Fuzzy logic; Fuzzy sets; Hierarchical systems; Mooring; Tankers (ships); Fuzzy set theory",Article,Scopus,2-s2.0-80255132788
"Okan Sakar C., Kursun O.","A method for combining mutual information and canonical correlation analysis: Predictive Mutual Information and its use in feature selection",2012,"Expert Systems with Applications",12,10.1016/j.eswa.2011.09.020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80255131236&doi=10.1016%2fj.eswa.2011.09.020&partnerID=40&md5=3dad79bdc01d3f72ce9d957b0e8e2b2b","Feature selection is a critical step in many artificial intelligence and pattern recognition problems. Shannon's Mutual Information (MI) is a classical and widely used measure of dependence measure that serves as a good feature selection algorithm. However, as it is a measure of mutual information in average, under-sampled classes (rare events) can be overlooked by this measure, which can cause critical false negatives (missing a relevant feature very predictive of some rare but important classes). Shannon's mutual information requires a well sampled database, which is not typical of many fields of modern science (such as biomedical), in which there are limited number of samples to learn from, or at least, not all the classes of the target function (such as certain phenotypes in biomedical) are well-sampled. On the other hand, Kernel Canonical Correlation Analysis (KCCA) is a nonlinear correlation measure effectively used to detect independence but its use for feature selection or ranking is limited due to the fact that its formulation is not intended to measure the amount of information (entropy) of the dependence. In this paper, we propose a hybrid measure of relevance, Predictive Mutual Information (PMI) based on MI, which also accounts for predictability of signals from each other in its calculation as in KCCA. We show that PMI has more improved feature detection capability than MI, especially in catching suspicious coincidences that are rare but potentially important not only for experimental studies but also for building computational models. We demonstrate the usefulness of PMI, and superiority over MI, on both toy and real datasets. © 2011 Elsevier Ltd. All rights reserved.","Canonical correlation; Gebelein's Maximal Correlation; Imbalanced datasets; Mutual information; Statistical dependence; Suspicious coincidences","Canonical correlations; Imbalanced Data-sets; Maximal correlation; Mutual informations; Statistical dependence; Suspicious coincidences; Artificial intelligence; Correlation methods; Feature extraction",Article,Scopus,2-s2.0-80255131236
"Malon C., Brachtel E., Cosatto E., Graf H.P., Kurata A., Kuroda M., Meyer J.S., Saito A., Wu S., Yagi Y.","Mitotic figure recognition: Agreement among pathologists and computerized detector",2012,"Analytical Cellular Pathology",12,10.3233/ACP-2011-0029,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863076347&doi=10.3233%2fACP-2011-0029&partnerID=40&md5=e22f0902048ef3dff3a37421d3b4b23a","Despite the prognostic importance of mitotic count as one of the components of the Bloom-Richardson grade [3], several studies ([2, 9, 10]) have found that pathologists' agreement on the mitotic grade is fairly modest. Collecting a set of more than 4,200 candidate mitotic figures, we evaluate pathologists' agreement on individual figures, and train a computerized system for mitosis detection, comparing its performance to the classifications of three pathologists. The system's and the pathologists' classifications are based on evaluation of digital micrographs of hematoxylin and eosin stained breast tissue. On figures where the majority of pathologists agree on a classification, we compare the performance of the trained system to that of the individual pathologists. We find that the level of agreement of the pathologists ranges from slight to moderate, with strong biases, and that the system performs competitively in rating the ground truth set. This study is a step towards automatic mitosis count to accelerate a pathologist's work and improve reproducibility. © 2012-IOS Press and the authors. All rights reserved.",,"hematoxylin; article; breast; microphotography; mitosis; pathologist; priority journal; Algorithms; Artificial Intelligence; Automation; Breast Neoplasms; Female; Humans; Image Interpretation, Computer-Assisted; Mitosis; Mitotic Index; Neoplasm Grading; Pathology, Clinical; Physicians; Reproducibility of Results",Article,Scopus,2-s2.0-84863076347
"Xu J., Faruque J., Beaulieu C.F., Rubin D., Napel S.","A comprehensive descriptor of shape: Method and application to content-based retrieval of similar appearing lesions in medical images",2012,"Journal of Digital Imaging",12,10.1007/s10278-011-9388-8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861309078&doi=10.1007%2fs10278-011-9388-8&partnerID=40&md5=563460ab1bc990b980886d97bbbf8c16","We have developed a method to quantify the shape of liver lesions in CT images and to evaluate its performance for retrieval of images with similarly-shaped lesions. We employed a machine learning method to combine several shape descriptors and defined similarity measures for a pair of shapes as a weighted combination of distances calculated based on each feature. We created a dataset of 144 simulated shapes and established several reference standards for similarity and computed the optimal weights so that the retrieval result agrees best with the reference standard. Then we evaluated our method on a clinical database consisting of 79 portal-venous-phase CT liver images, where we derived a reference standard of similarity from radiologists' visual evaluation. Normalized Discounted Cumulative Gain (NDCG) was calculated to compare this ordering with the expected ordering based on the reference standard. For the simulated lesions, the mean NDCG values ranged from 91% to 100%, indicating that our methods for combining features were very accurate in representing true similarity. For the clinical images, the mean NDCG values were still around 90%, suggesting a strong correlation between the computed similarity and the independent similarity reference derived the radiologists. © Society for Imaging Informatics in Medicine 2011.","Image analysis; Image processing; Image retrieval","Clinical database; Clinical images; CT Image; Data sets; Descriptors; Liver images; Machine learning methods; Medical images; Optimal weight; Reference standard; Shape descriptors; Similarity measure; Strong correlation; Visual evaluation; Computerized tomography; Image analysis; Image processing; Learning systems; Image retrieval; adult; aged; algorithm; article; artificial intelligence; cohort analysis; comparative study; computer assisted diagnosis; computer assisted tomography; diagnostic imaging; female; human; information retrieval; liver; liver disease; liver tumor; male; methodology; middle aged; pathology; radiography; standard; Adult; Aged; Aged, 80 and over; Algorithms; Artificial Intelligence; Cohort Studies; Diagnostic Imaging; Female; Humans; Information Storage and Retrieval; Liver; Liver Diseases; Liver Neoplasms; Male; Middle Aged; Radiographic Image Interpretation, Computer-Assisted; Reference Standards; Tomography, X-Ray Computed",Article,Scopus,2-s2.0-84861309078
"D'Amato E., Daniele E., Mallozzi L., Petrone G.","Equilibrium strategies via GA to stackelberg games under multiple follower's best reply",2012,"International Journal of Intelligent Systems",12,10.1002/int.21514,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855174065&doi=10.1002%2fint.21514&partnerID=40&md5=f291ef2b042b701acb657a471f8e164c","In this paper, we study a two-person game between one leader and one follower, called the Stackelberg game. The leader player enounces a decision before the others, and the follower takes into account this decision and solves an optimization problem that may have multiple solutions. Then, the leader optimizes his objective by assuming a given follower's reaction depending on his behavior. We consider in this paper a hierarchical equilibrium solution for a two-level game, particularly the strong Stackelberg solutions that corresponds to an optimistic leader's point of view and we give a numerical procedure based on a genetic algorithm (GA) evolution process to compute them. The use of a multimodal genetic algorithm allows us to approach the possible multiple solutions to the lower level problem. The algorithm convergence is illustrated by means of some test cases. Copyright © 2011 Wiley Periodicals, Inc.",,"Algorithm convergence; Equilibrium solutions; Equilibrium strategy; Evolution process; Multi-modal; Multiple solutions; Numerical procedures; Optimization problems; Stackelberg Games; Stackelberg solution; Test case; Two person game; Artificial intelligence; Software engineering; Genetic algorithms",Conference Paper,Scopus,2-s2.0-84855174065
"Sanín C., Mancilla-Amaya L., Haoxi Z., Szczerbicki E.","Decisional DNA: The concept and its implementation platforms",2012,"Cybernetics and Systems",12,10.1080/01969722.2012.654069,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858040782&doi=10.1080%2f01969722.2012.654069&partnerID=40&md5=8a270207e47ce152629702cdf48bbbb5","Knowledge and experience engineering techniques are becoming increasingly useful and popular components of hybrid integrated systems used to solve complex real-life problems in different disciplines. These techniques offer features such as learning from experience, handling noisy and incomplete data, helping with decision making, and predicting capabilities. In this article, we present a number of different applications of a multidomain knowledge representation structure called decisional DNA that can be implemented and shared for the exploitation of embedded knowledge within different technologies. Copyright © 2012 Taylor & Francis Group, LLC.","artificial intelligence; decision making; decisional DNA; knowledge engineering; knowledge representation; set of experience knowledge structure","decisional DNA; Engineering techniques; Implementation platforms; Incomplete data; Integrated systems; Knowledge and experience; Multi domains; Real-life problems; Set of experience knowledge structure; Artificial intelligence; Decision making; Knowledge engineering; Knowledge representation; DNA",Article,Scopus,2-s2.0-84858040782
"Samari D., Azadi H., Zarafshani K., Hosseininia G., Witlox F.","Determining appropriate forestry extension model: Application of AHP in the Zagros area, Iran",2012,"Forest Policy and Economics",12,10.1016/j.forpol.2011.10.006,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856511325&doi=10.1016%2fj.forpol.2011.10.006&partnerID=40&md5=9a0ba3aa161da07074ebe83c688c0ced","Determining an appropriate forestry extension model remains as a major challenge if sustainable forest management is a goal. This article was an attempt to show how the analytical hierarchy process can effectively be helpful in selecting appropriate model for forestry extension. The results revealed that the present situation fails to regard the 'privatized extension' as an appropriate model for the Zagros area in Iran. The results also showed while the beneficiaries select 'cooperative extension system' as the most appropriate model, it has no tangible difference with 'public extension system' as the second preferred option. Accordingly, a hybrid forestry extension model was recommended as an appropriate model. © 2011 Elsevier B.V.","Analytical hierarchy process; Decision support system; Forestry extension; Multi criteria decision making; Sustainable forest management","Analytical Hierarchy Process; Decision supports; Forestry extension; Multi criteria decision making; Sustainable forest management; Artificial intelligence; Decision support systems; Privatization; Timber; Forestry; Artificial Intelligence; Decision Making; Mathematical Models; Sustainable Forest Management",Article,Scopus,2-s2.0-84856511325
"Guy R.T., Santago P., Langefeld C.D.","Bootstrap aggregating of alternating decision trees to detect sets of SNPs that associate with disease",2012,"Genetic Epidemiology",12,10.1002/gepi.21608,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859050402&doi=10.1002%2fgepi.21608&partnerID=40&md5=5087cf2c209e218142f91977c3bcdee2","Complex genetic disorders are a result of a combination of genetic and nongenetic factors, all potentially interacting. Machine learning methods hold the potential to identify multilocus and environmental associations thought to drive complex genetic traits. Decision trees, a popular machine learning technique, offer a computationally low complexity algorithm capable of detecting associated sets of single nucleotide polymorphisms (SNPs) of arbitrary size, including modern genome-wide SNP scans. However, interpretation of the importance of an individual SNP within these trees can present challenges.We present a new decision tree algorithm denoted as Bagged Alternating Decision Trees (BADTrees) that is based on identifying common structural elements in a bootstrapped set of Alternating Decision Trees (ADTrees). The algorithm is order nk2, where n is the number of SNPs considered and k is the number of SNPs in the tree constructed. Our simulation study suggests that BADTrees have higher power and lower type I error rates than ADTrees alone and comparable power with lower type I error rates compared to logistic regression. We illustrate the application of these data using simulated data as well as from the Lupus Large Association Study 1 (7,822 SNPs in 3,548 individuals). Our results suggest that BADTrees hold promise as a low computational order algorithm for detecting complex combinations of SNP and environmental factors associated with disease. © 2012 Wiley Periodicals, Inc.","Gene-gene interaction; Genetic association; Machine learning; Multi-locus models","algorithm; alternating decision tree; article; bagged alternating decision tree; bootstrapping; computer program; decision tree; environmental factor; gene interaction; genetic association; genetic disorder; genetic marker; heredity; human; major clinical study; prediction; simulation; single nucleotide polymorphism; Alternating Decision Tree; Article; Bagged Alternating Decision Tree algorithm; controlled study; gene deletion; gene frequency; gene insertion; genetic algorithm; genotype; heterozygosity; HLA system; homozygosity; phenotype; signal noise ratio; systematic error; Algorithms; Artificial Intelligence; Computer Simulation; Decision Trees; Genetic Diseases, Inborn; Genetic Markers; Genome-Wide Association Study; HLA Antigens; Humans; Linkage Disequilibrium; Models, Genetic; Models, Statistical; Molecular Epidemiology; Polymorphism, Single Nucleotide; Reproducibility of Results; Software",Article,Scopus,2-s2.0-84859050402
"Pǎun A., Sidoroff M.","Sequentiality induced by spike number in SNP systems: Small universal machines",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-28024-5_22,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856101852&doi=10.1007%2f978-3-642-28024-5_22&partnerID=40&md5=a4bbf8cb759d8e53f34a88c20396ffde","In this paper we consider sequential SNP systems where the sequentiality of the system is induced by the max-spike: the neuron with the maximum number of spikes out of the neurons that can spike at one step will fire. This corresponds to a global view of the whole network that makes the system sequential. We continue the study in the direction of max-spike and show that systems with 132 neurons are universal. This improves a recent result in the area. © 2012 Springer-Verlag.",,"Global view; One step; Sequentiality; Universal machines; Artificial intelligence; Bioinformatics",Conference Paper,Scopus,2-s2.0-84856101852
"Cabrera-Mora F., Xiao J.","A flooding algorithm for multirobot exploration",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",12,10.1109/TSMCB.2011.2179799,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861196470&doi=10.1109%2fTSMCB.2011.2179799&partnerID=40&md5=8f9fe2a2d6937e07652dd332c2ed7fd8","In this paper, we present a multirobot exploration algorithm that aims at reducing the exploration time and to minimize the overall traverse distance of the robots by coordinating the movement of the robots performing the exploration. Modeling the environment as a tree, we consider a coordination model that restricts the number of robots allowed to traverse an edge and to enter a vertex during each step. This coordination is achieved in a decentralized manner by the robots using a set of active landmarks that are dropped by them at explored vertices. We mathematically analyze the algorithm on trees, obtaining its main properties and specifying its bounds on the exploration time. We also define three metrics of performance for multirobot algorithms. We simulate and compare the performance of this new algorithm with those of our multirobot depth first search (MR-DFS) approach presented in our recent paper and classic single-robot DFS. © 2012 IEEE.","Distributed robotics; multirobot exploration; networked robots","Coordination model; Depth first search; Distributed robotics; Flooding algorithms; Metrics of performance; Multi-robot exploration; Multirobots; Networked robot; Algorithms; Forestry; Industrial robots; Trees (mathematics); Algorithms; Forestry; Robots; algorithm; article; artificial intelligence; automated pattern recognition; biomimetics; computer simulation; decision support system; exploratory behavior; methodology; theoretical model; Algorithms; Artificial Intelligence; Biomimetics; Computer Simulation; Decision Support Techniques; Exploratory Behavior; Models, Theoretical; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84861196470
"Cano-Izquierdo J.-M., Ibarrola J., Almonacid M.","Improving motor imagery classification with a new BCI design using neuro-fuzzy S-dFasArt",2012,"IEEE Transactions on Neural Systems and Rehabilitation Engineering",12,10.1109/TNSRE.2011.2169991,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856472922&doi=10.1109%2fTNSRE.2011.2169991&partnerID=40&md5=5cb08f6386123b2910caf8194ce33bb0","This paper presents an algorithm based on neural networks and fuzzy theory (S-dFasArt) to classify spontaneous mental activities from electroencephalogram (EEG) signals, in order to operate a noninvasive brain-computer interface. The focus is placed on the three-class problem, left-hand movement imagination, right movement imagination and word generation. The algorithm allows a supervised classification of temporal patterns improving the classification rates of the BCI Competition III (Data Set V: multiclass problem, continuous EEG). Using the precomputed data supplied for the competition and following the rules established there, a new method based on S-dFasArt, along with rule prune and voting strategy is proposed. The results have been compared with other published methods improving their success rates. © 2006 IEEE.","Brain-computer interfaces; classification algorithms; fuzzy neural networks","classification algorithms; Classification rates; Data sets; Electroencephalogram signals; Fuzzy theory; Fuzzy-neural; Mental activity; Motor imagery classification; Multi-class problems; Neuro-Fuzzy; Supervised classification; Temporal pattern; Voting strategies; Algorithms; Brain; Classification (of information); Electroencephalography; Fuzzy neural networks; Interfaces (computer); Brain computer interface; algorithm; article; artificial intelligence; biological model; brain; classification; computer interface; electroencephalography; fuzzy logic; hand; hemispheric dominance; human; imagination; innervation; long term memory; movement (physiology); nerve cell plasticity; physiology; short term memory; signal processing; statistics; Algorithms; Artificial Intelligence; Brain; Classification; Electroencephalography; Functional Laterality; Fuzzy Logic; Hand; Humans; Imagination; Memory, Long-Term; Memory, Short-Term; Models, Neurological; Movement; Neuronal Plasticity; Signal Processing, Computer-Assisted; User-Computer Interface",Article,Scopus,2-s2.0-84856472922
"Dvořák W., Spanring C.","Comparing the expressiveness of argumentation semantics",2012,"Frontiers in Artificial Intelligence and Applications",12,10.3233/978-1-61499-111-3-261,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876237408&doi=10.3233%2f978-1-61499-111-3-261&partnerID=40&md5=aa9de5aeefbd875022541e43a17637a2","In this work we complement recent investigations of the intertranslatability of argumentation semantics. Our focus is on the expressiveness of argumentation semantics and thus we expand the area of interest beyond efficiently computable translations. To this end we provide new translations between semantics as well as new translational impossibility results. This allows us to draw a hierarchy for the expressiveness of argumentation semantics. © 2012 The authors and IOS Press. All rights reserved.","Abstract argumentation; Argumentation semantics; Intertranslatability","Artificial intelligence; Abstract argumentation; Area of interest; Argumentation semantics; Impossibility results; Intertranslatability; Semantics",Conference Paper,Scopus,2-s2.0-84876237408
"Sun J.","Why different people prefer different systems for different tasks: An activity perspective on technology adoption in a dynamic user environment",2012,"Journal of the American Society for Information Science and Technology",12,10.1002/asi.21670,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-83655184809&doi=10.1002%2fasi.21670&partnerID=40&md5=dbe3edf74cc314f5bbfe5f3f0831d0a7","In a contemporary user environment, there are often multiple information systems available for a certain type of task. Based on the premises of Activity Theory, this study examines how user characteristics, system experiences, and task situations influence an individual's preferences among different systems in terms of user readiness to interact with each. It hypothesizes that system experiences directly shape specific user readiness at the within-subject level, user characteristics and task situations make differences in general user readiness at the between-subject level, and task situations also affect specific user readiness through the mediation of system experiences. An empirical study was conducted, and the results supported the hypothesized relationships. The findings provide insights on how to enhance technology adoption by tailoring system development and management to various task contexts and different user groups. © 2011 ASIS&T.",,"Activity Theory; Empirical studies; Multiple information systems; System development; Technology adoption; User characteristics; User groups; Artificial intelligence; Software engineering; Activity coefficients",Article,Scopus,2-s2.0-83655184809
"Khaleghi A., Ryabko D., Mary J., Preux P.","Online clustering of processes",2012,"Journal of Machine Learning Research",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877745287&partnerID=40&md5=21d0bf0c83bebc6cc1631f1c02d0fe06","The problem of online clustering is considered in the case where each data point is a sequence generated by a stationary ergodic process. Data arrive in an online fashion so that the sample received at every timestep is either a continuation of some previously received sequence or a new sequence. The dependence between the sequences can be arbitrary. No parametric or independence assumptions are made; the only assumption is that the marginal distribution of each sequence is stationary and ergodic. A novel, computationally efficient algorithm is proposed and is shown to be asymptotically consistent (under a natural notion of consistency). The performance of the proposed algorithm is evaluated on simulated data, as well as on real datasets (motion classification).",,"Algorithms; Classification (of information); Computationally efficient; Ergodic process; Independence assumption; Marginal distribution; Motion classification; On-line fashion; Online-clustering; Real data sets; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84877745287
"Tran T.T., Beck J.C.","Logic-based Benders decomposition for alternative resource scheduling with sequence dependent setups",2012,"Frontiers in Artificial Intelligence and Applications",12,10.3233/978-1-61499-098-7-774,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878773101&doi=10.3233%2f978-1-61499-098-7-774&partnerID=40&md5=7490c490b2f0fadf38f1893a2a378395","We study an unrelated parallel machines scheduling problem with sequence and machine dependent setup times. A logic-based Benders decomposition approach is proposed to minimize the makespan. This approach is a hybrid model that makes use of a mixed integer programming master problem and a specialized solver for travelling salesman subproblems. The master problem is used to assign jobs to machines while the subproblems obtain optimal schedules on each machine given the master problem assignments. Computational results show that the Benders model is able to find optimal solutions up to six orders of magnitude faster as well as solving problems six times the size previously possible with a mixed integer programming model in the literature and twice the size that a branch-and-bound algorithm can solve for similar problems. We further relax the Benders decomposition to accept suboptimal schedules and demonstrate the ability to parameterize solution quality while outperforming state-of-the-art metaheuristics both in terms of solution quality and mean run-time. © 2012 The Author(s).",,"Algorithms; Artificial intelligence; Branch and bound method; Computation theory; Integer programming; Optimization; Scheduling; Stochastic programming; Traveling salesman problem; Benders decomposition; Branch-and-bound algorithms; Computational results; Logic-based benders decompositions; Mixed integer programming; Mixed integer programming model; Sequence dependent setups; Unrelated parallel machines; Problem solving",Conference Paper,Scopus,2-s2.0-84878773101
"Allen G.I.","Sparse higher-order principal components analysis",2012,"Journal of Machine Learning Research",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954243593&partnerID=40&md5=637f5731425172437d79995ad8ca84cf","Traditional tensor decompositions such as the CANDECOMP / PARAFAC (CP) and Tucker decompositions yield higher-order principal components that have been used to understand tensor data in areas such as neuroimaging, microscopy, chemometrics, and remote sensing. Sparsity in high-dimensional matrix factorizations and principal components has been well-studied exhibiting many benefits; less attention has been given to sparsity in tensor decompositions. We propose two novel tensor decompositions that incorporate sparsity: the Sparse Higher-Order SVD and the Sparse CP Decomposition. The latter solves an 1-norm penalized relaxation of the single-factor CP optimization problem, thereby automatically selecting relevant features for each tensor factor. Through experiments and a scientific data analysis example, we demonstrate the utility of our methods for dimension reduction, feature selection, signal recovery, and exploratory data analysis of high-dimensional tensors.",,"Artificial intelligence; Data handling; Decomposition; Information analysis; Neuroimaging; Optimization; Remote sensing; Tensors; Exploratory data analysis; Matrix factorizations; Optimization problems; Principal Components; Principal components analysis; Scientific data analysis; Tensor decomposition; Tucker decompositions; Principal component analysis",Conference Paper,Scopus,2-s2.0-84954243593
"Parsons S., Atkinson K., Haigh K., Levitt K., McBurney P., Rowe J., Singh M.P., Sklar E.","Argument schemes for reasoning about trust",2012,"Frontiers in Artificial Intelligence and Applications",12,10.3233/978-1-61499-111-3-430,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876270149&doi=10.3233%2f978-1-61499-111-3-430&partnerID=40&md5=abeddbfe6dab57352b34229522546d18","Trust is a natural mechanism by which an autonomous party can deal with the inherent uncertainty regarding the behaviors of other parties and the uncertainty in the information it shares with those parties. Trust is thus crucial in any decentralized system. We build on recent efforts to use argumentation to reason about trust. Specifically, we provide a set of schemes, abstract patterns of reasoning that apply in multiple situations, geared toward trust. We describe, in the form of a set of critical questions, the situations in which the schemes may default. © 2012 The authors and IOS Press. All rights reserved.","Argument schemes; Critical questions; Trust","Argument schemes; Critical questions; Decentralized system; Trust; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84876270149
"Eriksson B., Balzano L., Nowak R.","High-rank matrix completion",2012,"Journal of Machine Learning Research",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881499145&partnerID=40&md5=3057f981a71517c405df625aae820ff1","This paper considers the problem of completing a matrix with many missing entries under the assumption that the columns of the matrix belong to a union of multiple low-rank subspaces. This generalizes the standard low-rank matrix completion problem to situations in which the matrix rank can be quite high or even full rank. Since the columns belong to a union of subspaces, this problem may also be viewed as a missing-data version of the subspace clustering problem. Let Xbe an n×N matrix whose (complete) columns lie in a union of at most k subspaces, each of rank ≤ r < n, and assume N ≥ kn. The main result of the paper shows that under mild assumptions each column of X can be perfectly recovered with high probability from an incomplete version so long as at least CrN log2(n) entries of Xare observed uniformly at random, with C > 1 a constant depending on the usual incoherence conditions, the geometrical arrangement of subspaces, and the distribution of columns over the subspaces. The result is illustrated with numerical experiments and an application to Internet distance matrix completion and topology identification.",,"Clustering algorithms; Probability distributions; Geometrical arrangements; Internet distances; Low-rank matrix completions; Matrix completion; Numerical experiments; Sub-Space Clustering; Topology identification; Union of subspaces; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84881499145
"Chang L., Liao C., Lin W.B., Chen L.-L., Zheng X.","A hybrid method based on differential evolution and continuous ant colony optimization and its application on wideband antenna design",2012,"Progress in Electromagnetics Research",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-81755177594&partnerID=40&md5=8bdb60e18bd11368c0ee1fb2a7aa31e8","An evolutionary learning algorithm based on differential evolution strategy (DES) and continuous ant colony optimization (CACO) for wideband antenna design is proposed. The advantages of this hybrid method are demonstrated with several mathematical functions and a linear array pattern synthesis. This method is applied to design an E-shaped wideband patch antenna, which achieves the impedance bandwidth 4.8 ~ 6.53 GHz. We compare the hybrid method with the traditional DES and CACO optimization algorithms, and the advantage of this hybrid method over the DES and the CACO is also demonstrated.",,"Ant-colony optimization; Differential Evolution; Differential evolution strategy; Evolutionary Learning; Hybrid method; Impedance bandwidths; Linear arrays; Mathematical functions; Optimization algorithms; Wideband antenna; Wideband patch antennas; Antennas; Artificial intelligence; Bandwidth; Constrained optimization; Design; Evolutionary algorithms; Functions; Learning algorithms; Electric impedance",Article,Scopus,2-s2.0-81755177594
"Chen M., Xu Z.E., Weinberger K.Q., Chapelle O., Kedem D.","Classifier cascade for minimizing feature evaluation cost",2012,"Journal of Machine Learning Research",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898796721&partnerID=40&md5=5127c0e6daf5835e8821e65c845314e7","Machine learning algorithms are increasingly used in large-scale industrial settings. Here, the operational cost during test-time has to be taken into account when an algorithm is designed. This operational cost is affected by the average running time and the computation time required for feature extraction. When a diverse set of features is used, the latter can vary drastically. In this paper we propose an algorithm that constructs a cascade of classifiers which explicitly trades-off operational cost and classifier accuracy while accounting for on-demand feature extraction costs. Different from previous work, our algorithm reoptimizes trained classifiers and allows expensive features to be scheduled at any stage within the cascade to minimize overall cost. Experiments on actual web-search ranking data sets demonstrate that our framework leads to drastic test-time improvements.",,"Algorithms; Artificial intelligence; Costs; Extraction; Feature extraction; Learning algorithms; Learning systems; World Wide Web; Average running time; Cascade of classifiers; Computation time; Feature evaluation; Industrial settings; Overall costs; Test time; Web search rankings; Classification (of information)",Conference Paper,Scopus,2-s2.0-84898796721
"Ishiguro K., Ueda N., Sawada H.","Subset infinite relational models",2012,"Journal of Machine Learning Research",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880710954&partnerID=40&md5=f1bf4ef911b6ea827f91c83bb03684a4","We propose a new probabilistic generative model for analyzing sparse and noisy pairwise relational data, such as friend-links on social net- work services and customer records in online shops. Real-world relational data often include a large portion of non-informative pairwise data entries. Many existing stochastic blockmodels suffer from these irrelevant data entries because of their rather simpler forms of priors. The pro-posed model incorporates a latent variable that explicitly indicates whether each data entry is relevant or not to diminish bad effects associated with such irrelevant data. Through experiments using synthetic and real data sets, we show that the proposed model can extract clusters with stronger relations among data within the cluster than clusters obtained by the conventional model.",,"Stochastic systems; Conventional modeling; Generative model; Infinite relational models; Latent variable; Online shops; Real-world; Relational data; Synthetic and real data; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84880710954
"Särkkä S., Hartikainen J.","Infinite-dimensional kalman filtering approach to spatio-temporal Gaussian process regression",2012,"Journal of Machine Learning Research",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878049984&partnerID=40&md5=8775f20654b7ec67b1fa545fa0dcff33","We show how spatio-temporal Gaussian process (GP) regression problems (or the equivalent Kriging problems) can be formulated as infinite-dimensional Kalman filtering and Rauch-Tung-Striebel (RTS) smoothing problems, and present a procedure for converting spatio-temporal covariance functions into infinite-dimensional stochastic differential equations (SDEs). The resulting infinitedimensional SDEs belong to the class of stochastic pseudo-differential equations and can be numerically treated using the methods developed for deterministic counterparts of the equations. The scaling of the computational cost in the proposed approach is linear in the number of time steps as opposed to the cubic scaling of the direct GP regression solution. We also show how separable covariance functions lead to a finite-dimensional Kalman filtering and RTS smoothing problem, present analytical and numerical examples, and discuss numerical methods for computing the solutions.",,"Artificial intelligence; Differential equations; Gaussian distribution; Gaussian noise (electronic); Kalman filters; Numerical methods; Regression analysis; Stochastic systems; Computational costs; Covariance function; Finite dimensional; Gaussian process regression; Infinite dimensional; Regression problem; Smoothing problems; Stochastic differential equations; Filtration",Conference Paper,Scopus,2-s2.0-84878049984
"Gisbrecht A., Lueks W., Mokbel B., Hammer B.","Out-of-sample kernel extensions for nonparametric dimensionality reduction",2012,"ESANN 2012 proceedings, 20th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905445275&partnerID=40&md5=108d295f6621bec4fb9186d074aab2ef","Nonparametric dimensionality reduction (DR) techniques such as locally linear embedding or t-distributed stochastic neighbor (t- SNE) embedding constitute standard tools to visualize high dimensional and complex data in the Euclidean plane. With increasing data volumes and streaming applications, it is often no longer possible to project all data points at once. Rather, out-of-sample extensions (OOS) derived from a small subset of all data points are used. In this contribution, we propose a kernel mapping for OOS in contrast to direct techniques based on the DR method. This can be trained based on a given example set, or it can be trained indirectly based on the cost function of the DR technique. Considering t-SNE as an example and several benchmarks, we show that a kernel mapping outperforms direct OOS as provided by t-SNE. © 2012, i6doc.com publication. All rights reserved.",,"Artificial intelligence; Complex networks; Cost functions; Geometry; Mapping; Neural networks; Stochastic systems; Dimensionality reduction; Euclidean planes; High-dimensional; Kernel mapping; Locally linear embedding; Out-of-sample extension; Standard tools; Streaming applications; Learning systems",Conference Paper,Scopus,2-s2.0-84905445275
"Brubaker M.A., Salzmann M., Urtasun R.","A family of MCMC methods on implicitly defined manifolds",2012,"Journal of Machine Learning Research",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903760234&partnerID=40&md5=be861d8e2ac409e64909f1d742aa8ff6","Traditional MCMC methods are only applicable to distributions defined on Rn. However, there exist many application domains where the distributions cannot easily be defined on a Euclidean space. To address this limitation, we propose a general constrained version of Hamiltonian Monte Carlo, and give conditions under which the Markov chain is convergent. Based on this general framework we define a family of MCMC methods which can be applied to sample from distributions on non-linear manifolds. We demonstrate the effectiveness of our approach on a variety of problems including sampling from the Bingham-von Mises-Fisher distribution, collaborative filtering and pose estimation.",,"Artificial intelligence; Markov processes; Bingham; Euclidean spaces; MCMC method; Nonlinear manifolds; Pose estimation; Von Mises-Fisher distribution; Collaborative filtering",Conference Paper,Scopus,2-s2.0-84903760234
"Sharpnack J., Rinaldo A., Singh A.","Sparsistency of the edge lasso over graphs",2012,"Journal of Machine Learning Research",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941123585&partnerID=40&md5=d7f87704db6397ead62fe40ad1bfd667","The fused lasso was proposed recently to enable recovery of high-dimensional patterns which are piece-wise constant on a graph, by penalizing the 1-norm of differences of measurements at vertices that share an edge. While there have been some attempts at coming up with efficient algorithms for solving the fused lasso optimization, a theoretical analysis of its performance is mostly lacking except for the simple linear graph topology. In this paper, we investigate sparsistency of fused lasso for general graph structures, i.e. its ability to correctly recover the exact support of piece-wise constant graphstructured patterns asymptotically (for largescale graphs). To emphasize this distinction over previous work, we will refer to it as Edge Lasso. We focus on the (structured) normal means setting, and our results provide necessary and sufficient conditions on the graph properties as well as the signal-to-noise ratio needed to ensure sparsistency. We examplify our results using simple graph-structured patterns, and demonstrate that in some cases fused lasso is sparsistent at very weak signal-to-noise ratios (scaling as √ (log n)/|A|, where n is the number of vertices in the graph and A is the smallest set of vertices with constant activation). In other cases, it performs no better than thresholding the difference of measurements at vertices which share an edge (which requires signal-to-noise ratio that scales as √log n). © Copyright 2012 by the authors.",,"Algorithms; Artificial intelligence; Graph theory; Graphic methods; Radioactivity logging; Topology; Fused lassos; General graph; Graph properties; Graph structured pattern; High-dimensional; Piece-wise constants; Thresholding; Weak signals; Signal to noise ratio",Conference Paper,Scopus,2-s2.0-84941123585
"Spolaôr N., Cherman E.A., Monard M.C., Lee H.D.","Filter approach feature selection methods to support multi-label learning based on relieff and information gain",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952021169&partnerID=40&md5=23e02fb117a36eb9dfda3de85101df2e","In multi-label learning, each example in the dataset is associated with a set of labels, and the task of the generated classifier is to predict the label set of unseen examples. Feature selection is an important task in machine learning, which aims to find a small number of features that describes the dataset as well as, or even better, than the original set of features does. This can be achieved by removing irrelevant and/or redundant features according to some importance criterion. Although effective feature selection methods to support classification for single-label data are abound, this is not the case for multi-label data. This work proposes two multi-label feature selection methods which use the filter approach. This approach evaluates statistics of the data independently of any particular classifier. To this end, ReliefF, a single-label feature selection method and an adaptation of the Information Gain measure for multi-label data are used to find the features that should be selected. Both methods were experimentally evaluated in ten benchmark datasets, taking into account the reduction in the number of features as well as the quality of the generated classifiers, showing promising results. © Springer-Verlag Berlin Heidelberg 2012.",,"Artificial intelligence; Benchmarking; Feature extraction; Learning algorithms; Learning systems; Quality control; Benchmark datasets; Feature selection methods; Filter approach; Information gain; Multi-label; Multi-label learning; Redundant features; ReliefF; Classification (of information)",Conference Paper,Scopus,2-s2.0-84952021169
"Bhaskar P., Pakray P., Banerjee S., Banerjee S., Bandyopadhyay S., Gelbukh A.","Question answering system for QA4MRE@CLEF 2012",2012,"CEUR Workshop Proceedings",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922023895&partnerID=40&md5=9cf8329fa826f1966a5394e6e126120a","The article presents the experiments carried out as part of the participation in the main task of QA4MRE@CLEF 2012. In the developed system, we first combine the question and each answer option to form the Hypothesis (H). Stop words are removed from each H and query words are identified to retrieve the most relevant sentences from the associated document using Lucene. Relevant sentences are retrieved from the associated document based on the TF-IDF of the matching query words along with n-gram overlap of the sentence with the H. Each retrieved sentence defines the Text T. Each T-H pair is assigned a ranking score that works on textual entailment principle. A validate weight is automatically assigned to each answer options based on their ranking. A parallel procedure also generates the possible answer patterns from given questions and answer options. Each sentence in the associated document is assigned an inference score with respect to each answer pattern. Evaluated inference score for each answer option is multiplied by the validate weight based on their ranking. The answer option that receives the highest selection score is identified as the most relevant option and selected as the answer to the given question.","Named entity; QA4MRE data sets; Question answering technique; Textual entailment","Artificial intelligence; Text processing; Document-based; Matching query; Named entities; QA4MRE data sets; Query words; Question Answering; Question answering systems; Textual entailment; Natural language processing systems",Conference Paper,Scopus,2-s2.0-84922023895
"Rahman A., Hossain A., Zahirul A.A.H.M., Rashid M.","Fuzzy knowledge-based model for prediction of traction force of an electric golf car",2012,"Journal of Terramechanics",12,10.1016/j.jterra.2011.08.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859268311&doi=10.1016%2fj.jterra.2011.08.001&partnerID=40&md5=018385369dec12f8114e5579884d6d93","The methods of artificial intelligence are widely used in soft computing technology due to its remarkable prediction accuracy. However, artificial intelligent models are trained using large amount of data obtained from the operation of the off-road vehicle. In contrast, fuzzy knowledge-based models are developed by using the experience of the traction in order to maintain the vehicle traction as required with utilizing optimum power. The main goal of this paper is to describe fuzzy knowledge-based model to be practically applicable to a reasonably wide class of unknown nonlinear systems. Compared with conventional control approach, fuzzy logic approach is more efficient for nonlinear dynamic systems and embedding existing structured human knowledge into workable mathematics. The purpose of this study is to investigate the relationship between vehicle's input parameters of power supply (PI) and moisture content (MC) and output parameter of traction force (TF). Experiment has been conducted in the field to investigate the vehicle traction and the result has been compared with the developed fuzzy logic system (FLS) based on Mamdani approach. Results show that the mean relative error of actual and predicted values from the FLS model on TF is found as 7%, which is less than the acceptable limit of 10%. The goodness of fit of the prediction value from FLS is found close to 1.0 as expected and hence shows the good performance of the developed system. © 2011 Published by Elsevier Ltd. on behalf of ISTVS.","Artificial intelligent system; Fuzzy logic system; Traction","Artificial intelligence; Computation theory; Computer circuits; Dynamical systems; Electric power systems; Electric traction; Forecasting; Intelligent systems; Knowledge based systems; Nonlinear dynamical systems; Off road vehicles; Soft computing; Traction (friction); Vehicles; Artificial intelligent; Conventional control; Fuzzy logic approach; Fuzzy logic system; Mamdani approaches; Mean relative error; Prediction accuracy; Unknown nonlinear systems; Fuzzy logic",Article,Scopus,2-s2.0-84859268311
"Bresolin D., Della Monica D., Montanari A., Sala P., Sciavicco G.","Interval temporal logics over finite linear orders: The complete picture",2012,"Frontiers in Artificial Intelligence and Applications",12,10.3233/978-1-61499-098-7-199,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878786149&doi=10.3233%2f978-1-61499-098-7-199&partnerID=40&md5=194e5915d8a974c7b33fbde0edf30b4c","Interval temporal logics provide a natural framework for temporal reasoning about interval structures over linearly ordered domains, where intervals are taken as the primitive ontological entities. In this paper, we identify all fragments of Halpern and Shoham's interval temporal logic HS whose finite satisfiability problem is decidable. We classify them in terms of both relative expressive power and complexity. We show that there are exactly 62 expressively-different decidable fragments, whose complexity ranges from NP-complete to non-primitive recursive (all other HS fragments have been already shown to be undecidable). © 2012 The Author(s).",,"Artificial intelligence; Computability and decidability; Formal logic; Hydraulic structures; Expressive power; Finite satisfiability; Interval structures; Interval temporal logic; Linear order; NP Complete; Ordered domains; Temporal reasoning; Temporal logic",Conference Paper,Scopus,2-s2.0-84878786149
"Tachmazidis I., Antoniou G., Flouris G., Kotoulas S., McCluskey L.","Large-scale parallel stratified defeasible reasoning",2012,"Frontiers in Artificial Intelligence and Applications",12,10.3233/978-1-61499-098-7-738,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878790110&doi=10.3233%2f978-1-61499-098-7-738&partnerID=40&md5=6cbb72d5b46d74d3de97b54676e7e56e","We are recently experiencing an unprecedented explosion of available data coming from the Web, sensors readings, scientific databases, government authorities and more. Such datasets could benefit from the introduction of rule sets encoding commonly accepted rules or facts, application- or domain-specific rules, commonsense knowledge etc. This raises the question of whether, how, and to what extent knowledge representation methods are capable of handling huge amounts of data for these applications. In this paper, we consider inconsistency-tolerant reasoning in the form of defeasible logic, and analyze how parallelization, using the MapReduce framework, can be used to reason with defeasible rules over huge datasets. We extend previous work by dealing with predicates of arbitrary arity, under the assumption of stratification. Moving from unary to multi-arity predicates is a decisive step towards practical applications, e.g. reasoning with linked open (RDF) data. Our experimental results demonstrate that defeasible reasoning with millions of data is performant, and has the potential to scale to billions of facts. © 2012 The Author(s).",,"Knowledge representation; Commonsense knowledge; Defeasible reasoning; Defeasible rules; Government authorities; Inconsistency-tolerant reasonings; Knowledge representation method; Mapreduce frameworks; Scientific database; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84878790110
"Benevenuto F., Rodrigues T., Veloso A., Almeida J., Goncalves M., Almeida V.","Practical detection of spammers and content promoters in online video sharing systems",2012,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics",12,10.1109/TSMCB.2011.2173799,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861182449&doi=10.1109%2fTSMCB.2011.2173799&partnerID=40&md5=c9a5fab8784d6d6f24393eeb7ea566df","A number of online video sharing systems, out of which YouTube is the most popular, provide features that allow users to post a video as a response to a discussion topic. These features open opportunities for users to introduce polluted content, or simply pollution, into the system. For instance, spammers may post an unrelated video as response to a popular one, aiming at increasing the likelihood of the response being viewed by a larger number of users. Moreover, content promoters may try to gain visibility to a specific video by posting a large number of (potentially unrelated) responses to boost the rank of the responded video, making it appear in the top lists maintained by the system. Content pollution may jeopardize the trust of users on the system, thus compromising its success in promoting social interactions. In spite of that, the available literature is very limited in providing a deep understanding of this problem. In this paper, we address the issue of detecting video spammers and promoters. Towards that end, we first manually build a test collection of real YouTube users, classifying them as spammers, promoters, and legitimate users. Using our test collection, we provide a characterization of content, individual, and social attributes that help distinguish each user class. We then investigate the feasibility of using supervised classification algorithms to automatically detect spammers and promoters, and assess their effectiveness in our test collection. While our classification approach succeeds at separating spammers and promoters from legitimate users, the high cost of manually labeling vast amounts of examples compromises its full potential in realistic scenarios. For this reason, we further propose an active learning approach that automatically chooses a set of examples to label, which is likely to provide the highest amount of information, drastically reducing the amount of required training data while maintaining comparable classification effectiveness. © 2012 IEEE.",,"Active Learning; Amount of information; Classification approach; High costs; Legitimate users; Online video; Realistic scenario; Social interactions; Spammers; Supervised classification; Test Collection; Training data; User class; YouTube; Online systems; Pollution; Websites; Classification (of information); algorithm; article; artificial intelligence; automated pattern recognition; computer simulation; decision support system; information retrieval; Internet; methodology; online system; theoretical model; videorecording; Algorithms; Artificial Intelligence; Computer Simulation; Decision Support Techniques; Information Storage and Retrieval; Internet; Models, Theoretical; Online Systems; Pattern Recognition, Automated; Video Recording",Article,Scopus,2-s2.0-84861182449
"Carpentier A., Munos R.","Bandit Theory meets Compressed Sensing for high-dimensional Stochastic Linear Bandit",2012,"Journal of Machine Learning Research",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062989&partnerID=40&md5=d8118905e1d43e9c3c02fb59eb298a41","We consider a linear stochastic bandit problem where the dimension K of the unknown parameter θ is larger than the sampling budget n. Since usual linear bandit algorithms have a regret of order O(Kpn), it is in general impossible to obtain a sub-linear regret without further assumption. In this paper we make the assumption that θ is S-sparse, i.e. has at most S-non-zero components, and that the set of arms is the unit ball for the ∥. ∥2 norm. We combine ideas from Compressed Sensing and Bandit Theory to derive an algorithm with a regret bound in O(S√n). We detail an application to the problem of optimizing a function that depends on many variables but among which only a small number of them (initially unknown) are relevant.",,"Artificial intelligence; Budget control; Signal reconstruction; Stochastic systems; Bandit problems; Bandit theories; High-dimensional; Regret bounds; Unit ball; Compressed sensing",Conference Paper,Scopus,2-s2.0-84896062989
"Nikitina N., Rudolph S.","ExpExpExplosion: Uniform interpolation in general εL terminologies",2012,"Frontiers in Artificial Intelligence and Applications",12,10.3233/978-1-61499-098-7-618,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878779710&doi=10.3233%2f978-1-61499-098-7-618&partnerID=40&md5=d82aec1c0b0d973ac37c17d43752b148","Although εL is a popular logic used in large existing knowledge bases, to the best of our knowledge no procedure has yet been proposed that computes uniform εL interpolants of general εL terminologies. Up to now, also the bounds on the size of uniform εL interpolants remain unknown. In this paper, we propose an approach based on proof theory and the theory of formal tree languages to computing a finite uniform interpolant for a general εL terminology if it exists. Further, we show that, if such a finite uniform εL interpolant exists, then there exists one that is at most triple exponential in the size of the original TBox, and that, in the worst-case, no shorter interpolants exist, thereby establishing the triple exponential tight bounds on their size. © 2012 The Author(s).",,"Artificial intelligence; Computation theory; Terminology; Interpolants; Knowledge basis; Proof theory; Tight bound; Tree languages; Uniform interpolations; Interpolation",Conference Paper,Scopus,2-s2.0-84878779710
"Canakoglu A., Ghisalberti G., Masseroli M.","Integration of biomolecular interaction data in a genomic and proteomic data warehouse to support biomedical knowledge discovery",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-35686-5_10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871531296&doi=10.1007%2f978-3-642-35686-5_10&partnerID=40&md5=822d1c47f01b4f1060f95bb0eb683342","The growing available genomic and proteomic information gives new opportunities for novel research approaches and biomedical discoveries through effective data management and analysis support. Integration and comprehensive evaluation of available controlled data can highlight information patterns leading to unveil new biomedical knowledge. For this purpose, the University Politecnico di Milano, is developing a software framework to create and maintain a Genomic and Proteomic Data Warehouse (GPDW) that integrates information from many data sources on the basis of a conceptual data model that relates molecular entities and biomedical features. Here we illustrate and discuss the extension of framework for integrating biomolecular interaction data in the GPDW. The comprehensive and mining of the reliable interaction data together with the other biomolecular information in the GPDW constitutes a powerful computational support for novel biomedical knowledge discoveries. © Springer-Verlag 2012.","Automatic data parsing and integration; Data warehousing; Proteomic and genomic interaction data","Biomolecular information; Biomolecular interactions; Comprehensive evaluation; Conceptual data models; Data-sources; Information patterns; Molecular entities; Proteomic and genomic interaction data; Research approach; Software frameworks; Artificial intelligence; Bioinformatics; Computer programming; Genes; Information management; Integration; Data warehouses",Conference Paper,Scopus,2-s2.0-84871531296
"Bernstein D.J., Lange T.","Computing small discrete logarithms faster",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-34931-7_19,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871565804&doi=10.1007%2f978-3-642-34931-7_19&partnerID=40&md5=c5a5555dc48f40190901e31c24a7cd53","Computations of small discrete logarithms are feasible even in ""secure"" groups, and are used as subroutines in several cryptographic protocols in the literature. For example, the Boneh-Goh-Nissim degree-2-homomorphic public-key encryption system uses generic square-root discrete-logarithm methods for decryption. This paper shows how to use a small group-specific table to accelerate these subroutines. The cost of setting up the table grows with the table size, but the acceleration also grows with the table size. This paper shows experimentally that computing a discrete logarithm in an interval of order ℓ takes only 1.93·ℓ1/3 multiplications on average using a table of size ℓ1/3 precomputed with 1.21·ℓ2/3 multiplications, and computing a discrete logarithm in a group of order ℓ takes only 1.77 •·ℓ 1/3 multiplications on average using a table of size ℓ1/3 precomputed with 1.24·ℓ2/3 multiplications. © Springer-Verlag 2012.","Discrete logarithms; Precomputation; Random walks","Cryptographic protocols; Discrete logarithms; Pre-computation; Public-key encryption; Random Walk; Square-root; Table size; Artificial intelligence; Public key cryptography",Conference Paper,Scopus,2-s2.0-84871565804
"Bilò D., Gualà L., Proietti G.","Bounded-distance network creation games",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-35311-6_6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871394594&doi=10.1007%2f978-3-642-35311-6_6&partnerID=40&md5=b5d4fd031559cbdde643bb558a4a8312","A network creation game simulates a decentralized and non-cooperative building of a communication network. Informally, there are n players sitting on the network nodes, which attempt to establish a reciprocal communication by activating, incurring a certain cost, any of their incident links. The goal of each player is to have all the other nodes as close as possible in the resulting network, while buying as few links as possible. According to this intuition, any model of the game must then appropriately address a balance between these two conflicting objectives. Motivated by the fact that a player might have a strong requirement about its centrality in the network, in this paper we introduce a new setting in which if a player maintains its (either maximum or average) distance to the other nodes within a given bound, then its cost is simply equal to the number of activated edges, otherwise its cost is unbounded. We study the problem of understanding the structure of pure Nash equilibria of the resulting games, that we call MaxBD and SumBD, respectively. For both games, we show that when distance bounds associated with players are non-uniform, then equilibria can be arbitrarily bad. On the other hand, for MaxBD, we show that when nodes have a uniform bound R on the maximum distance, then the Price of Anarchy (PoA) is lower and upper bounded by 2 and O (n 1/⌊log3 R⌋+1) for R ≥ 3 (i.e., the PoA is constant as soon as R is Ω(nε), for some ε &gt; 0), while for the interesting case R = 2, we are able to prove that the PoA is Ω(√n) and O(√n log n). For the uniform SumBD we obtain similar (asymptotically) results, and moreover we show that the PoA becomes constant as soon as the bound on the average distance is 2ω(√log n). © 2012 Springer-Verlag.",,"Average Distance; Distance bound; Maximum distance; Network creation; Network node; Non-cooperative; Price of anarchy; Pure Nash equilibrium; Reciprocal communications; Artificial intelligence; Internet; Costs",Conference Paper,Scopus,2-s2.0-84871394594
"Maya A., Nisan N.","Incentive compatible two player cake cutting",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-35311-6_13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871360819&doi=10.1007%2f978-3-642-35311-6_13&partnerID=40&md5=a225bbe9221e17cd99ed3a0f7746aa87","We characterize methods of dividing a cake between two bidders in a way that is incentive-compatible and Pareto-efficient. In our cake cutting model, each bidder desires a subset of the cake (with a uniform value over this subset), and is allocated some subset. Our characterization proceeds via reducing to a simple one-dimensional version of the problem, and yields, for example, a tight bound on the social welfare achievable. © 2012 Springer-Verlag.",,"Cutting model; Incentive compatible; Social welfare; Tight bound; Artificial intelligence; Internet",Conference Paper,Scopus,2-s2.0-84871360819
"Zhang Q.","Reconstruction of intermediate view based on depth map enhancement",2012,"Journal of Multimedia",11,10.4304/jmm.7.6.415-419,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871191549&doi=10.4304%2fjmm.7.6.415-419&partnerID=40&md5=8e687423b0e1553597720402a023ec52","Reconstruction of intermediate view from arbitrary view is important in Free View Video (FVV) system. In this paper, the Total Least Square method is employed to remove the mismatch and noise in the background area of the disparity map, which improves the quality of rendering result considerably. The Experimental results verify the validity of our proposed method. © 2012 ACADEMY PUBLISHER.","Depth map; Reference software for depth estimation (DERS); Reference software for view synthesis(VSRS)","Depth Estimation; Depth Map; Disparity map; Intermediate view; Quality of rendering results; Total least squares; View synthesis; Artificial intelligence; Multimedia systems; Least squares approximations",Article,Scopus,2-s2.0-84871191549
"Aerts D., Broekaert J., Gabora L., Veloz T.","The Guppy effect as interference",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-35659-9_4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870853227&doi=10.1007%2f978-3-642-35659-9_4&partnerID=40&md5=30dec7e7df03c260dc1ec12956055b2a","People use conjunctions and disjunctions of concepts in ways that violate the rules of classical logic, such as the law of compositionality. Specifically, they overextend conjunctions of concepts, a phenomenon referred to as the Guppy Effect. We build on previous efforts to develop a quantum model [1,2,3], that explains the Guppy Effect in terms of interference. Using a well-studied data set with 16 exemplars that exhibit the Guppy Effect, we developed a 17-dimensional complex Hilbert space that models the data and demonstrates the relationship between overextension and interference. We view the interference effect as, not a logical fallacy on the conjunction, but a signal that out of the two constituent concepts, a new concept has emerged. © 2012 Springer-Verlag.","concept combination; Guppy effect; interference; quantum cognition; theory of concepts","Classical logic; Compositionality; concept combination; Data sets; Guppy effect; Interference effects; quantum cognition; Quantum models; theory of concepts; Wave interference; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84870853227
"Klein B., Laiseca X., Casado-Mansilla D., López-De-Ipiña D., Nespral A.P.","Detection and extracting of emergency knowledge from twitter streams",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-35377-2_64,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870374902&doi=10.1007%2f978-3-642-35377-2_64&partnerID=40&md5=2c3922a95041ed2d732db3a082b379bb","Increasingly, more important information is being shared through Twitter. New opportunities arise to use this tool to detect emergencies and extract crucial information about the scope and nature of that event. A major challenge for the extraction of emergency event information from Twitter is represented by the unstructured and noisy nature of tweets. Within the SABESS project we propose a combined structural and content based analysis approach. We use social network analysis to identify reliable tweets and content analysis techniques to summarize key emergency facts. © 2012 Springer-Verlag.","Emergency detection; natural language processing; social network analysis","Content analysis; Content-based analysis; Emergency detection; NAtural language processing; Social Network Analysis; Artificial intelligence; Natural language processing systems; Ubiquitous computing; Social networking (online)",Conference Paper,Scopus,2-s2.0-84870374902
"Evers C., Kniewel R., Geihs K., Schmidt L.","Achieving user participation for adaptive applications",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-35377-2_28,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870342249&doi=10.1007%2f978-3-642-35377-2_28&partnerID=40&md5=e5fe4eaf37d8fac7fb2bbfb5989af471","Adaptive applications establish the basis for many ubiquitous computing scenarios as they can dynamically adapt to changing contexts. But adaptive applications lack of success when the adaptive behaviour does not correspond to the user's interaction habits. A user study revealed that such applications are not satisfying for complex scenarios with a high degree of user interaction. We claim that there must be a trade-off between automation and user participation. By extending an existing adaptation middleware with capabilities to respect user preference and interaction behaviour we demonstrate how to integrate the user in the self-adaptation loop. Interdisciplinary results from the fields of usability engineering and interaction design include the need for an adaptation notification concept to avoid mismatching adaptation behaviour. © 2012 Springer-Verlag.",,"Adaptive application; Adaptive behaviour; Interaction design; Self adaptation; User interaction; User participation; User study; Artificial intelligence; Middleware; Ubiquitous computing; Usability engineering; Behavioral research",Conference Paper,Scopus,2-s2.0-84870342249
"Pepels T., Winands M.H.M.","Enhancements for Monte-Carlo Tree Search in Ms Pac-Man",2012,"2012 IEEE Conference on Computational Intelligence and Games, CIG 2012",11,10.1109/CIG.2012.6374165,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871968674&doi=10.1109%2fCIG.2012.6374165&partnerID=40&md5=9a55aaaea1da6b47fd27bae7e88a0c83","In this paper enhancements for the Monte-Carlo Tree Search (MCTS) framework are investigated to play Ms Pac-Man. MCTS is used to find an optimal path for an agent at each turn, determining the move to make based on randomised simulations. Ms Pac-Man is a real-time arcade game, in which the protagonist has several independent goals but no conclusive terminal state. Unlike games such as Chess or Go there is no state in which the player wins the game. Furthermore, the Pac-Man agent has to compete with a range of different ghost agents, hence limited assumptions can be made about the opponent's behaviour. In order to expand the capabilities of existing MCTS agents, five enhancements are discussed: 1) a variable depth tree, 2) playout strategies for the ghost-team and Pac-Man, 3) including long-term goals in scoring, 4) endgame tactics, and 5) a Last-Good-Reply policy for memorising rewarding moves during playouts. An average performance gain of 40,962 points, compared to the average score of the top scoring Pac-Man agent during the CIG'11, is achieved by employing these methods. © 2012 IEEE.",,"Long-term goals; MONTE CARLO; Optimal paths; Performance Gain; Terminal state; Tree search; Variable depth; Artificial intelligence; Computational methods",Conference Paper,Scopus,2-s2.0-84871968674
"Kusin F.M., Jarvis A.P., Gandy C.J.","Hydraulic performance assessment of passive coal mine water treatment systems in the UK",2012,"Ecological Engineering",11,10.1016/j.ecoleng.2012.08.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868628134&doi=10.1016%2fj.ecoleng.2012.08.008&partnerID=40&md5=7959d243f8eede84e6714d71271d9587","Hydraulic performance assessment of passive treatment systems has been conducted for UK's Coal Authority mine water treatment systems. The study aims to improve the understanding of the hydraulic factors that govern contaminant behaviour, such that future design of treatment systems is able to optimise treatment efficiency and make performance more predictable, and improve performance over the long-term. Assessment of the hydraulic behaviour (i.e. residence time and flow pattern) of the treatment systems was accomplished by means of tracer tests. The tracer tests were undertaken at eight UK Coal Authority mine water treatment systems (lagoons and wetlands) within Northern England (main study areas) and part of southern Scotland. A modelling approach using a tanks-in-series (TIS) model was adopted to precisely analyse and characterise the residence time distributions (RTDs), in an effort to account for the different flow patterns across the treatment systems. Generally, lagoon RTDs are characterised by a greater flow dispersion compared to wetlands (i.e. higher dispersion number, D and lower number of TIS, n). Consequently, the hydraulic efficiency, e λ for lagoons is much lower than wetlands (mean of 0.20 for lagoons compared to 0.66 for wetlands). Implications for design and maintenance of mine water treatment systems are discussed. © 2012 Elsevier B.V.","Hydraulic; Mine water; Settlement lagoon; Wetland","Dispersion number; Flow dispersions; Future designs; Hydraulic behaviour; Hydraulic efficiency; Hydraulic performance; Mine waters; Northern England; Passive treatment; Residence time; Residence time distributions; Scotland; Settlement lagoon; Study areas; Tanks-in-series; Tracer tests; Treatment efficiency; Treatment systems; Artificial intelligence; Coal mines; Flow patterns; Hydraulics; Lakes; Mine flooding; Radioactive tracers; Wetlands; Groundwater; coal mine; design; flow pattern; hydraulic conductivity; hydrological modeling; lagoon; performance assessment; water treatment; wetland; England; Scotland; United Kingdom",Article,Scopus,2-s2.0-84868628134
"Teppan E.C., Felfernig A.","Minimization of decoy effects in recommender result sets",2012,"Web Intelligence and Agent Systems",11,10.3233/WIA-2012-0253,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874387906&doi=10.3233%2fWIA-2012-0253&partnerID=40&md5=213d30d0d4e5fb1a4e29d72060352c2b","Recommender systems are common web applications which support users in finding suitable products in large and/or complex product domains. Although state-of-the-art systems manage to accomplish the task of finding and presenting suitable products they show big deficits in their models of human behavior. Time limitations, cognitive capacities and willingness to cognitive effort bound rational decision making which can lead to unforeseen side effects and consequently to sub-optimal decisions. Decoy effects are cognitive phenomena which are omni-present on result pages but state-of-the-art recommender systems are completely unaware of such effects. Due to the fact that such effects constitute one source of irrational decisions their identification and, if necessary, the neutralization of their biasing potential is extremely important. This paper introduces an approach for identifying and minimizing decoy effects on recommender result pages. To support the suggested approach we present the results of a corresponding user study which clearly proves the concept. Moreover, this paper also investigates whether the decreasing impact of decoys on uncertainty levels during decision making is affected by the decoy minimization approach. © 2012 - IOS Press and the authors. All rights reserved.","decision support systems; decoy minimization; online decision making; Recommender systems","Cognitive capacity; Cognitive efforts; Complex products; Human behaviors; On-line decision makings; Rational decision making; Side effect; State-of-the-art system; User study; WEB application; Artificial intelligence; Decision making; Decision support systems; Online systems; Recommender systems; Cognitive systems",Article,Scopus,2-s2.0-84874387906
"Retnasamy V., Sauli Z., Aziz M.H.A., Hatta R.M., Shapri A.H.M., Taniselass S.","Shear stress analysis study using surface morphology correlation with aluminium ball adhesion",2012,"Proceedings of International Conference on Computational Intelligence, Modelling and Simulation",11,10.1109/CIMSim.2012.86,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872506927&doi=10.1109%2fCIMSim.2012.86&partnerID=40&md5=7a3e82f0859f4fc557eb709dc0f8b4e0","Wire bonding method are utilized to facilitate the interconnection in the demanding development of integrated circuits. Wire bond shear test method is utilized in the industry to scrutinize the quality of the wire bond. This paper discusses about the simulation of wire bond shear test using aluminium wire. This study is focused on evaluating the effects of bond pad surface on the stress response of aluminium ball bond during wire bond shear test. The aluminium ball bond will bonded on three types of bond pad surface, flat surface, hemisphere surface and sharp groove surface and the stress response of each type of bond pad surface are then compared. The results obtained showed that the aluminium ball bond bonded to the sharp groove surface acquired the highest stress response of 311.55 MPa. The simulation was done using Ansys version 11. © 2012 IEEE.","Wire bond shear test; aluminium wire;bond pad surface","Aluminium wires; Bond pad; Flat surfaces; Shear tests; Stress response; Wire bonding; Wire bonds; Aluminum; Artificial intelligence; Shear flow; Stress analysis; Wire; Electronics packaging",Conference Paper,Scopus,2-s2.0-84872506927
"Jiang J.Q., Wu M.","Predicting multiplex subcellular localization of proteins using protein-protein interaction network: a comparative study.",2012,"BMC bioinformatics",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877144416&partnerID=40&md5=a3d3a19712d4f33bf0e4d7ada786624d","Proteins that interact in vivo tend to reside within the same or ""adjacent"" subcellular compartments. This observation provides opportunities to reveal protein subcellular localization in the context of the protein-protein interaction (PPI) network. However, so far, only a few efforts based on heuristic rules have been made in this regard. We systematically and quantitatively validate the hypothesis that proteins physically interacting with each other probably share at least one common subcellular localization. With the result, for the first time, four graph-based semi-supervised learning algorithms, Majority, χ2-score, GenMultiCut and FunFlow originally proposed for protein function prediction, are introduced to assign ""multiplex localization"" to proteins. We analyze these approaches by performing a large-scale cross validation on a Saccharomyces cerevisiae proteome compiled from BioGRID and comparing their predictions for 22 protein subcellular localizations. Furthermore, we build an ensemble classifier to associate 529 unlabeled and 137 ambiguously-annotated proteins with subcellular localizations, most of which have been verified in the previous experimental studies. Physical interaction of proteins has actually provided an essential clue for their co-localization. Compared to the local approaches, the global algorithms consistently achieve a superior performance.",,"protein; proteome; algorithm; article; artificial intelligence; biology; comparative study; metabolism; methodology; protein database; protein protein interaction; Saccharomyces cerevisiae; Algorithms; Artificial Intelligence; Computational Biology; Databases, Protein; Protein Interaction Maps; Proteins; Proteome; Saccharomyces cerevisiae",Article,Scopus,2-s2.0-84877144416
"Sauli Z., Retnasamy V., Rahman N.A., Norhaimi W.M.W., Ramli N., Vairavan R.","Shearing speed induced stress comparison on gold and copper ball interconnection",2012,"Proceedings of International Conference on Computational Intelligence, Modelling and Simulation",11,10.1109/CIMSim.2012.85,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872503063&doi=10.1109%2fCIMSim.2012.85&partnerID=40&md5=410e322fa8daf3c7266007a13189af99","As the trend for semiconductor packaging is heading towards BGA and flip chip interconnection methods, the conventional wire bonding process still dominates the industry primarily due the flexibility of wire bonds. The reliability of the bonded wires is assessed through wire bond shear test. In this study, the effects of shear ram speed on the stress response of bonded wires during wire bond shear test were investigated. The stress response of two wire materials, gold(Au) and copper(Cu) at varied shear ram speed were evaluated.A 3D non-linear finite element model was developed for the simulation. The shear ram speed of 100 μm/sand 1mm/s were used in this study. The results showed that the shear ram speed has significant effects on the stress response of the bonded wire. The simulation was done using Ansys version 11. © 2012 IEEE.","Wirebond; Shear Test","Bonded wires; Flip-chip interconnection; Induced stress; Non-linear finite element model; Ram speed; Semiconductor packaging; Shear tests; Stress response; Wire bonding; Wire bonds; Artificial intelligence; Copper; Electronics packaging; Finite element method; Gold; Speed; Three dimensional computer graphics; Wire; Shear flow",Conference Paper,Scopus,2-s2.0-84872503063
"Dong W., Pentland A.S., Heller K.A.","Graph-coupled HMMs for modeling the spread of infection",2012,"Uncertainty in Artificial Intelligence - Proceedings of the 28th Conference, UAI 2012",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885962408&partnerID=40&md5=7831813ef128ee54dc8cc519adacb0e4","We develop Graph-Coupled Hidden Markov Models (GCHMMs) for modeling the spread of infectious disease locally within a social network. Unlike most previous research in epidemiology, which typically models the spread of infection at the level of entire populations, we successfully leverage mobile phone data collected from 84 people over an extended period of time to model the spread of infection on an individual level. Our model, the GCHMM, is an extension of widely-used Coupled Hidden Markov Models (CHMMs), which allow dependencies between state transitions across multiple Hidden Markov Models (HMMs), to situations in which those dependencies are captured through the structure of a graph, or to social networks that may change over time. The benefit of making infection predictions on an individual level is enormous, as it allows people to receive more personalized and relevant health advice.",,"Coupled hidden Markov models; Individual levels; Infectious disease; Mobile phone datum; Multiple hidden markov model (HMMs); State transitions; Artificial intelligence; Hidden Markov models; Population statistics; Social networking (online)",Conference Paper,Scopus,2-s2.0-84885962408
"Noran O.","Achieving a sustainable interoperability of standards",2012,"Annual Reviews in Control",11,10.1016/j.arcontrol.2012.09.014,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869094884&doi=10.1016%2fj.arcontrol.2012.09.014&partnerID=40&md5=fc9aed61ce116465c58946642a2a2850","Agile enterprises and networks are required to continuously evolve so as to adequately respond to a dynamic, global and very competitive business environment. Unfortunately, this continuous change requirement affects the capacity of the enterprise to effectively interoperate internally and externally. Adding to this problem, the standards typically used as pillars for enterprise and network management and (inter)operation are themselves subject to continuous change and often bring their own interoperability, inconsistency and overlap problems. It is essential that such concerns are identified and addressed in a sustainable manner, i.e. taking into account the life cycles of all entities involved as their interoperability capabilities and requirements change along their lives. This paper focuses on interoperability issues specific to standards and proposes a novel and original way forward based on principles tested in previous case studies combining elements from the Enterprise Interoperability, Architecture and Integration, Collaborative Networks and Artificial Intelligence bodies of knowledge. © 2012 Elsevier Ltd. All rights reserved.","Expert system; Framework; Interoperability; Standard","Agile enterprise; Collaborative network; Competitive business; Framework; Requirements change; Artificial intelligence; Expert systems; Industry; Life cycle; Network management; Standards; Interoperability",Article,Scopus,2-s2.0-84869094884
"Zhou J., Cao Z.","TIS: A threshold incentive scheme for secure and reliable data forwarding in vehicular Delay Tolerant Networks",2012,"GLOBECOM - IEEE Global Telecommunications Conference",11,10.1109/GLOCOM.2012.6503241,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877645780&doi=10.1109%2fGLOCOM.2012.6503241&partnerID=40&md5=107dceb69d25f1a01e8fb3b0e937f06b","Delay Tolerant Networks (DTNs) have been utilized in wide-raging applications where a continuous end-to-end connection is unavailable and the message transmission is fulfilled by the cooperation among DTN nodes and follows a store-carry-and-forward manner such as vehicular networks. A series of incentive schemes were proposed to stimulate the selfish nodes to take on the energy-consuming job of forwarding bundles. However, the Onboard Units (OBU) equipped on vehicles are vulnerable to node compromise attack and the existing incentive mechanisms cannot well resist such sophisticated attacks sponsored by colluding vehicles and model the continuous collaboration among vehicles. In this paper, a novel threshold credit-based incentive mechanism is proposed based on the modified model of population dynamics to efficiently prevent the node compromise attacks, stimulate the cooperation among intermediate nodes, maximize vehicular nodes' interests and realize the fairness of possessing the same opportunity to forward packets for credits. Then, based on it, a secure and reliable packet forwarding protocol TIS is proposed. By devising a time order-preserving aggregated signature scheme, it also effectively solves the open problem of resisting the layer-adding attack launched by the collusion of intermediate vehicles. Finally, the security analysis and the extensive simulations show the effectiveness of our proposed TIS in resisting the sophisticated attacks mentioned above and the efficiency in terms of high reliability, high delivery ratio and low average delay in vehicular DTNs. © 2012 IEEE.",,"End-to-end connections; Extensive simulations; Incentive mechanism; Intermediate node; Message transmissions; Security analysis; Vehicular delay-tolerant networks; Vehicular networks; Artificial intelligence; Communication; Vehicles; Wireless networks; Delay tolerant networks",Conference Paper,Scopus,2-s2.0-84877645780
"Wang Y., Zhang J., Zhao Y., Wang J., Gu W.","Routing and spectrum assignment by means of ant colony optimization in flexible bandwidth networks",2012,"National Fiber Optic Engineers Conference, NFOEC 2012",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893052056&partnerID=40&md5=945dfef3358c1e23032a7f9a1660b1f7","We propose a dynamic ACO-based routing and spectrum assignment algorithm in the flexible bandwidth optical networks. Simulation results show that the novel strategy achieves a lower blocking probability with high adaptability to the traffic rate variation. © 2012 OSA.",,"Flexible bandwidth; Novel strategies; Routing and spectrum assignments; Traffic rate; Artificial intelligence; Algorithms",Conference Paper,Scopus,2-s2.0-84893052056
"Manzotti R.","The computational stance is unfit for consciousness",2012,"International Journal of Machine Consciousness",11,10.1142/S1793843012400239,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872706252&doi=10.1142%2fS1793843012400239&partnerID=40&md5=f6846379cf8c330b4ce323104e739e17","It is customary to assume that agents receive information from the environment through their sensors. It is equally customary to assume that an agent is capable of information processing and thus of computation. These two assumptions may be misleading, particularly because so much basic theoretical work relies on the concepts of information and computation. In similarity with Dennett's intentional stance, I suggest that a lot of discussions in cognitive science, neuroscience and artificial intelligence is biased by a naïve notion of computation resulting from the adoption of a computational stance. As a case study, I will focus on David Chalmers' view of computation in cognitive agents. In particular, I will challenge the thesis of computational sufficiency. I will argue that computation is no more than the ascription of an abstract model to a series of states and dynamic transitions in a physical agent. As a result, computation is akin to center of masses and other epistemic shortcuts that are insufficient to be the underpinnings of a baffling-yet-physical phenomenon like consciousness. © 2012 World Scientific Publishing Company.","agents; AI; computation; consciousness; Information","Abstract models; Center of mass; Cognitive agents; Cognitive science; consciousness; Dynamic transitions; Information; Physical agents; Agents; Calculations; Data processing; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84872706252
"Mohamed A.Z., Lee S.H., Hsu H.Y., Nath N.","A faster path planner using accelerated particle swarm optimization",2012,"Artificial Life and Robotics",11,10.1007/s10015-012-0051-3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871058480&doi=10.1007%2fs10015-012-0051-3&partnerID=40&md5=76d8161fa2c5a98c8c4436dadb102ad4","The idea of placing small mobile robots to move around in a large building to detect potential intruders has been around for some time. However, there are still two major hurdles to overcome: to locate itself in the environment and to make a decision on how to move around safely and effectively at a reasonable computation cost. This paper describes a mathematical model for developing a scheme for an autonomous low cost mobile robot system using visual simultaneous localization and mapping and accelerated particle swarm intelligent path planner. The results indicated that this system could provide a solution for the problem of indoor mobile robot navigation. Advances in computer technology make this technique a cost effective solution for a future home service robot. © 2012 ISAROB.","Artificial intelligence; Global path planning; Local path planning; SLAM; Swarm robotics","Accelerated particles; Computation costs; Computer technology; Cost-effective solutions; Global path planning; Home service robot; Indoor mobile robots; Large buildings; Local path-planning; Low costs; Mobile robot systems; Path planners; Simultaneous localization and mapping; SLAM; Swarm robotics; Artificial intelligence; Mathematical models; Mobile robots; Particle accelerators; Particle swarm optimization (PSO); Robotics; Motion planning",Article,Scopus,2-s2.0-84871058480
"Parsapoor M., Bilstrup U.","Brain Emotional Learning Based Fuzzy Inference System (BELFIS) for solar activity forecasting",2012,"Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI",11,10.1109/ICTAI.2012.78,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876835024&doi=10.1109%2fICTAI.2012.78&partnerID=40&md5=6ccc232c1c9c728cfaa79c3c158a752b","This paper presents a new architecture based on a brain emotional learning model that can be used in a wide varieties of AI applications such as prediction, identification and classification. The architecture is referred to as: Brain Emotional Learning Based Fuzzy Inference System (BELFIS) and it is developed from merging the idea of prior emotional models with fuzzy inference systems. The main aim of this model is presenting a desirable learning model for chaotic system prediction imitating the brain emotional network. In this research work, the model is used for predicting the solar activity, since it has been recognized as a threat to critical infrastructures in modern society. Specifically sunspot numbers are predicted by applying the proposed brain emotional learning model. The prediction results are compared with the outcomes of using other previous models like the locally linear model tree (LOLIMOT) and radial bias function (RBF) and adaptive neuro-fuzzy inference system (ANFIS). © 2012 IEEE.","brain emotional learning; fuzzy inference system; multi-year ahead prediction; solar activity forecasting; solar cycle 23; sunspot chaotic time series","Brain emotional learning; Chaotic time series; Fuzzy inference systems; Solar activity; Solar cycle; Artificial intelligence; Chaotic systems; Fuzzy systems; Intelligent control; Network architecture; Solar energy; Forecasting",Conference Paper,Scopus,2-s2.0-84876835024
"Martin C.M., Vogel C., Grady D., Zarabzadeh A., Hederman L., Kellett J., Smith K., O'Shea B.","Implementation of complex adaptive chronic care: The Patient Journey Record system (PaJR)",2012,"Journal of Evaluation in Clinical Practice",11,10.1111/j.1365-2753.2012.01880.x,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868194259&doi=10.1111%2fj.1365-2753.2012.01880.x&partnerID=40&md5=8233ec46aab1fa9de01a456c9171661e","Background The Patient Journey Record system (PaJR) is an application of a complex adaptive chronic care model in which early detection of adverse changes in patient biopsychosocial trajectories prompts tailored care, constitute the cornerstone of the model. Aims To evaluate the PaJR system's impact on care and the experiences of older people with chronic illness, who were at risk of repeat admissions over 12 months. Design Community-based cohort study - random assignment into intervention and usual care group, with process and outcome evaluation. Study population Adult and older patients with multiple morbidity, one or more chronic diseases with one or more overnight hospitalizations, and seven or more general practice visits in the past 6 months. Complex intervention PaJR lay care guides/advocates call patients and their caregivers. The care guides summarize their semi-structured conversations about health concerns and well-being. Predictive modelling and rules-based algorithms trigger alerts in relation to online call summaries. Alerts are acted upon according to agreed guidelines. Analysis Descriptive and comparative statistics. Outcomes Impact on unplanned emergency ambulatory care sensitive admissions (ACSC) with an overnight stay; sensitivity of alerts and predictions; rates of care guides-supported activities. Findings Five part-time lay care guides and a care manager monitored 153 intervention patients for 500 person months with 5050 phone calls. The 153 patients in the intervention group were comparable to the 61 controls. The intervention group reported in 50% of calls that their health limited their social activities; and one-third of calls reported immediate health concerns. Predictive analytics were highly sensitive to risk of hospitalization. ACSC admissions were reduced by 50% compared to controls across the sites. Discussion The initial implementation of a complex patient-centred adaptive chronic care model using lay care guides, supported by machine learning, appeared sensitive to risk of hospitalization and capable of stabilizing illness journeys in older patients with multi-morbidity. Conclusion Actions based on alerts produced in this study appeared to significantly reduce hospitalizations. This paves the way for further testing of the model. © 2012 Blackwell Publishing Ltd.","artificial intelligence; biopsychosocial; care guides; chronic disease; complex adaptive chronic care; complex adaptive systems; machine learning; multi-morbidity; non-linear dynamics","article; artificial intelligence; caregiver; chronic disease; health care utilization; health promotion; hospital admission; hospitalization; human; machine learning; major clinical study; medical documentation; outcome assessment; patient care; Patient Journey Record system; practice guideline; priority journal; social support; Aged; Aged, 80 and over; Algorithms; Artificial Intelligence; Chronic Disease; Clinical Protocols; Cohort Studies; Disease Management; Emergency Service, Hospital; Environment; Female; Health Promotion; Health Services; Health Status; Humans; Male; Middle Aged; Social Support",Article,Scopus,2-s2.0-84868194259
"Cholleti S., Post A., Gao J., Lin X., Bornstein W., Cantrell D., Saltz J.","Leveraging derived data elements in data analytic models for understanding and predicting hospital readmissions.",2012,"AMIA ... Annual Symposium proceedings / AMIA Symposium. AMIA Symposium",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878355148&partnerID=40&md5=d8e0a454a65bf1749548e33a58283853","Hospital readmissions depend on numerous factors. Automated risk calculation using electronic health record (EHR) data could allow targeting care to prevent them. EHRs usually are incomplete with respect to data relevant to readmissions prediction. Lack of standard data representations in EHRs restricts generalizability of predictive models. We propose developing such models by first generating derived variables that characterize clinical phenotype. This reduces the number of variables, reduces noise, introduces clinical knowledge into model building, and abstracts away the underlying data representation, thus facilitating use of standard data mining algorithms. We combined this pre-processing step with a random forest algorithm to compute risk for readmission within 30 days for patients in ten disease categories. Results were promising for encounters that our algorithm assigned very high or very low risk. Assigning patients to either of these two risk groups could be of value to patient care teams aiming to prevent readmissions.",,"algorithm; article; artificial intelligence; computer simulation; data mining; electronic medical record; female; hospital readmission; human; male; mathematical computing; risk factor; statistical model; Algorithms; Artificial Intelligence; Computer Simulation; Data Mining; Electronic Health Records; Female; Humans; Logistic Models; Male; Mathematical Computing; Patient Readmission; Risk Factors",Article,Scopus,2-s2.0-84878355148
"Chen Y., Mazlack L.J., Lu L.J.","Inferring fuzzy cognitive map models for gene regulatory networks from gene expression data",2012,"Proceedings - 2012 IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2012",11,10.1109/BIBM.2012.6392627,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872554131&doi=10.1109%2fBIBM.2012.6392627&partnerID=40&md5=19d6fbb519f30559c07a859f3f68ec91","Gene Regulatory Networks (GRNs) represent the causal relations among the genes and provide insight on the cellular functions and the mechanism of the diseases. GRNs can be inferred from gene expression data by a number of algorithms, e.g. Boolean networks, Bayesian networks, and differential equations. While reliable inference of GRNs is still an open problem, new algorithms need to be developed. Fuzzy Cognitive Maps (FCMs) is used to represent GRNs in this paper. Most of the FCM learning algorithms are able to learn FCMs with less than 40 nodes. A new algorithm that is able to learn FCMs with more than 100 nodes is proposed. The proposed method is based on Ant Colony Optimization (ACO). A decomposed approach is proposed to reduce the dimension of the problem; therefore the FCM learning algorithm is more scalable (the dimension of the problem to be solved in one ACO run equals to the number of nodes or genes). The proposed approach is tested on data from DREAM project. The experiment results suggest the proposed approach outperforms several other algorithms. © 2012 IEEE.","ant colony optimization; fuzzy cognitive map; gene expression; gene regulatory network; learning algorithm","Ant Colony Optimization (ACO); Boolean Networks; Causal relations; Cellular function; Fuzzy cognitive map; Gene Expression Data; Gene regulatory networks; Artificial intelligence; Bayesian networks; Bioinformatics; Differential equations; Fuzzy rules; Fuzzy systems; Gene expression; Inference engines; Large scale systems; Learning algorithms",Conference Paper,Scopus,2-s2.0-84872554131
"Kiesel S., Burns E., Ruml W.","Abstraction-guided sampling for motion planning",2012,"Proceedings of the 5th Annual Symposium on Combinatorial Search, SoCS 2012",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893375868&partnerID=40&md5=856f16c4a924ca632a8839a3b01fd7db","Motion planning in continuous space is a fundamental robotics problem that has been approached from many perspectives. Rapidly-exploring Random Trees (RRTs) use sampling to efficiently traverse the continuous and high dimensional state space. Heuristic graph search methods use lower bounds on solution cost to focus effort on portions of the space that are likely to be traversed by low-cost solutions. In this work, we bring these two ideas together in a technique called f-biasing: we use estimates of solution cost, computed as in heuristic search, to guide sparse sampling, as in RRTs. We see this new technique as strengthening the connections between motion planning in robotics and combinatorial search in artificial intelligence. Copyright © 2012, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Combinatorial search; Continuous spaces; Heuristic graph search; Heuristic search; High-dimensional; Low-cost solution; Rapidly-exploring random trees; Sparse sampling; Artificial intelligence; Costs; Heuristic algorithms; Motion planning; Robotics; Cost benefit analysis",Conference Paper,Scopus,2-s2.0-84893375868
"Othman M., Bhuiyan N., Gouw G.J.","Integrating workers' differences into workforce planning",2012,"Computers and Industrial Engineering",11,10.1016/j.cie.2012.06.015,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868207774&doi=10.1016%2fj.cie.2012.06.015&partnerID=40&md5=35046cbb41570d81572ba1772783753c","In today's competitive market, manufacturers need to work hard towards improving their production system performance in order to satisfy customer demands. In such a situation, most companies develop production systems that can help in quality improvement, cost reduction and throughput time reduction. In this research, we consider a workforce planning (WP) model including some human aspects such as skills, training, and workers' personalities and motivation. A multi-objective non-linear programming model is developed in order to minimize the hiring, firing, training and overtime costs and minimize the number of fired most productive workers. The purpose is to determine the number of workers for each worker type, the number of workers trained, and the number of overtime hours. Moreover, a decision support system (DSS) based on the proposed model is introduced using the Excel-LINGO software interfacing feature. The results indicate that the proposed model can provide a promising workforce planning approach to easily apply it in practice. © 2012 Elsevier Ltd. All rights reserved.","Decision support system; Ergonomics; Human factors; Production planning; Workforce planning","Competitive markets; Customer demands; Human aspects; Multi objective; Overtime costs; Production Planning; Production system; Quality improvement; Software interfacing; Throughput time; Workforce planning; Artificial intelligence; Customer satisfaction; Decision support systems; Ergonomics; Human engineering; Planning; Production control; Production engineering",Article,Scopus,2-s2.0-84868207774
"Mountassir A., Benbrahim H., Berrada I.","A cross-study of sentiment classification on arabic corpora",2012,"Res. and Dev. in Intelligent Syst. XXIX: Incorporating Applications and Innovations in Intel. Sys. XX - AI 2012, 32nd SGAI Int. Conf. on Innovative Techniques and Applications of Artificial Intel.",11,10.1007/978-1-4471-4739-8-21,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881507047&doi=10.1007%2f978-1-4471-4739-8-21&partnerID=40&md5=558cbf80cf3c4c368bae6571c00c9274","Sentiment Analysis is a research area where the studies focus on processing and analyzing the opinions available on the web. Several interesting and advanced works were performed on English. In contrast, very few works were conducted on Arabic. This paper presents the study we have carried out to investigate supervised sentiment classification in an Arabic context. We use two Arabic Corpora which are different in many aspects. We use three common classifiers known by their effectiveness, namely Naïve Bayes, Support Vector Machines and k-Nearest Neighbor. We investigate some settings to identify those that allow achieving the best results. These settings are about stemming type, term frequency thresholding, term weighting and n-gram words. We show that Naïve Bayes and Support Vector Machines are competitively effective; however k- Nearest Neighbor's effectiveness depends on the corpus. Through this study, we recommend to use light-stemming rather than stemming, to remove terms that occur once, to combine unigram and bigram words and to use presence-based weighting rather than frequency-based one. Our results show also that classification performance may be influenced by documents length, documents homogeneity and the nature of document authors. However, the size of data sets does not have an impact on classification results. © Springer-Verlag London 2012.",,"Classification performance; Classification results; K-nearest neighbors; Nearest neighbors; Sentiment analysis; Sentiment classification; Term Frequency; Term weighting; Artificial intelligence; Classification (of information); Information retrieval systems; Support vector machines; Text processing",Conference Paper,Scopus,2-s2.0-84881507047
"Nishiyama Y., Boularias A., Gretton A., Fukumizu K.","Hilbert space embeddings of pomdps",2012,"Uncertainty in Artificial Intelligence - Proceedings of the 28th Conference, UAI 2012",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879146831&partnerID=40&md5=86bca34d23f32b7a7ad97cc04025bb72","A nonparametric approach for policy learning for POMDPs is proposed. The approach represents distributions over the states, observations, and actions as embeddings in feature spaces, which are reproducing kernel Hilbert spaces. Distributions over states given the observations are obtained by applying the kernel Bayes' rule to these distribution embeddings. Policies and value functions are defined on the feature space over states, which leads to a feature space expression for the Bellman equation. Value iteration may then be used to estimate the optimal value function and associated policy. Experimental results confirm that the correct policy is learned using the feature space representation.",,"Bellman equations; Feature space; Nonparametric approaches; Optimal value functions; Policy learning; Reproducing Kernel Hilbert spaces; Value functions; Value iteration; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84879146831
"Saleem K., Fisal N.","Enhanced Ant Colony algorithm for self-optimized data assured routing in wireless sensor networks",2012,"IEEE International Conference on Networks, ICON",11,10.1109/ICON.2012.6506595,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877859371&doi=10.1109%2fICON.2012.6506595&partnerID=40&md5=b9dfdf6664749f64fa7b2dac7d8e71e9","Wireless sensor network (WSN) deploys tiny wireless sensor nodes to communicate with each other with limited processing speed, power and security measures. A recent WSN routing protocol defined as Secure Real-Time Load Distribution (SRTLD) has been developed to provide realtime transfer, high delivery ratio, and longer sensor node lifetime. SRTLD has been compared with LQER, MMSpeed, RTPC and RPAR. However, SRTLD uses broadcast packets to perform neighbour discovery for every packet transfer every hop, thus consume high energy. A novel Biological inspired self-organized Secure Autonomous Routing Protocol (BIOSARP) to enhance SRTLD with self-optimized and autonomous secure routing mechanism. The BIOSARP routing protocol depends on the optimal forwarding decision obtained by Ant Colony Optimization (ACO). The pheromone value in ACO is computed based on end-to-end delay, remaining battery power, and packet reception rate metrics similar to SRTLD. The proposed BIOSARP has been designed to reduce overhead broadcast packet in order to minimize the delay, packet loss and power consumption in WSN. In this paper we have presented the improved ACO algorithm that has been utilized in BIOSARP to perform self-optimized routing in WSN. BIOSARP has been studied and verified through simulation in network simulator 2 (NS-2). In simulation study BIOSARP normalized overhead is 12.1% less as compare to E&D ANTS and achieves 14% higher delivery ratio with 9% less power consumption when compared to SRTLD. Hence, the results confirm that BIOSARP offers better performance and can be practically implemented in WSN applications as structural and environmental monitoring, battlefield surveillance. © 2012 IEEE.","Ant Colony Optimization; Assurance; Autonomous; Biological Inspired; Intelligent Routing; Wireless Sensor Network","Ant Colony Optimization (ACO); Assurance; Autonomous; Battlefield surveillance; Biological Inspired; Environmental Monitoring; Intelligent routing; Remaining battery power; Ant colony optimization; Artificial intelligence; Routing protocols; Sensor nodes; Wireless sensor networks; Algorithms",Conference Paper,Scopus,2-s2.0-84877859371
"Nocera A., Ursino D.","PHIS: A system for scouting potential hubs and for favoring their ""growth"" in a Social Internetworking Scenario",2012,"Knowledge-Based Systems",11,10.1016/j.knosys.2012.07.009,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867856485&doi=10.1016%2fj.knosys.2012.07.009&partnerID=40&md5=7b7cabb6a7f724ab0b1505ec38ee879a","In this paper we propose ΦΦ (PHIS), a system for scouting current and potential hubs in a Social Internetworking Scenario (SIS) and for favoring the ""growth"" of these last ones in such a way that they can become real hubs quickly. ΦΦ operates on a SIS, rather than on a single social network. It uses a hypergraph-based model to represent the SIS. In order to verify if a user is a (current or potential) hub, it considers a range of criteria encompassing her reputation, her capability of receiving and delivering information, her position in the social networks of the SIS and the wideness of her profile. The same criteria are also exploited by ΦΦ to organize a campaign of stimulations to ""train"" a potential hub to become a real one. © 2012 Elsevier B.V. All rights reserved.","Centrality; Hub; Information flow; Social Internetworking System; Social network; Trust and reputation; User profile","Centrality; Hub; Information flows; Internetworking; Social Networks; Trust and reputation; User profile; Artificial intelligence; Software engineering; Social networking (online)",Article,Scopus,2-s2.0-84867856485
"Cabrera E., Luque E., Taboada M., Epelde F., Iglesias M.L.","ABMS optimization for emergency departments",2012,"Proceedings - Winter Simulation Conference",11,10.1109/WSC.2012.6465116,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874696361&doi=10.1109%2fWSC.2012.6465116&partnerID=40&md5=799b43cbaa12e4ecd0cc863400e3a865","This article presents an agent-based modeling and simulation to design a decision support system for healthcare emergency department (ED) to aid in setting up management guidelines to improve it. This ongoing research is being performed by the Research Group in Individual Oriented Modeling at the Universitat Autònoma de Barcelona with close collaboration of the hospital staff team of Sabadell. The objective of the proposed procedure is to optimize the performance of such complex and dynamic healthcare EDs, which are overcrowded. Exhaustive search optimization is used to find the optimal ED staff configuration, which includes doctors, triage nurses, and admission personnel, i.e., a multi-dimensional and multi-objective problem. An index is proposed to minimize patient stay time in the ED. The model is implemented using NetLogo. The results obtained by using alternatives Monte Carlo and Pipeline schemes are promising. The impact of these schemes to reduce the computational resources used is described. © 2012 IEEE.",,"Agent-based modeling and simulation; Barcelona; Computational resources; Emergency departments; Exhaustive search; MONTE CARLO; Multi-objective problem; NetLogo; Pipeline scheme; Research groups; Artificial intelligence; Computer simulation; Decision support systems; Emergency rooms; Health care; Optimization",Conference Paper,Scopus,2-s2.0-84874696361
"Choe Y., Ahn S., Chung M.J.","Fast point cloud segmentation for an intelligent vehicle using sweeping 2D laser scanners",2012,"2012 9th International Conference on Ubiquitous Robots and Ambient Intelligence, URAI 2012",11,10.1109/URAI.2012.6462925,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874689769&doi=10.1109%2fURAI.2012.6462925&partnerID=40&md5=a29335784bdb78f2a1d84864b4dcd72c","The previously developed radially bounded nearest neighbor (RBNN) algorithm have been shown a good performance for 3D point cloud segmentation in indoor scenarios. In outdoor scenarios however it is hard to adapt the original RBNN to an intelligent vehicle directly due to several drawbacks. In this paper, drawbacks of RBNN are addressed and we propose an enhanced RBNN for an intelligent vehicle operating in urban environments by proposing the ground elimination and the distance-varying radius. After the ground removal, objects can be remained to segment without merging the ground and objects, whereas the original RBNN with the fixed radius induced over-segmentation or under-segmentation. We design the distance-varying radius which is varied properly from the distance between a laser scanner and scanning objects. The proposed distance-varying radius is successfully induced to segment objects without over or under segmentation. In the experimental results, we have shown that the enhance RBNN is preferable to segment urban structures in terms of time consumption, and even segmentation rates. Copyright © 2012 IEEE.","Intelligent Vehicle; Laser Scanner; Point Clouds; Segmentation","2D laser scanners; 3D point cloud; Laser scanner; Nearest neighbors; Over segmentation; Point cloud; Point cloud segmentation; Time consumption; Urban environments; Urban structure; Artificial intelligence; Image segmentation; Laser applications; Intelligent vehicle highway systems",Conference Paper,Scopus,2-s2.0-84874689769
"Fan S.-L., Yu M., Jiang G.-Y., Shao F., Peng Z.-J.","A digital watermarking algorithm based on region of interest for 3D image",2012,"Proceedings of the 2012 8th International Conference on Computational Intelligence and Security, CIS 2012",11,10.1109/CIS.2012.129,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873540155&doi=10.1109%2fCIS.2012.129&partnerID=40&md5=191f64dd8a39dfc940fce3727249214c","A digital watermarking algorithm based on depth perceptual region of interest (DP-ROI) for copyright protection of three dimension (3D) images is proposed. First, DP-ROI is defined according to the three-dimensional depth sensation of human visual system and extracted through depth image and gray image. Then, gray image is transformed with discrete cosine transformation (DCT). Several middle frequency DCT coefficients in DP-ROI are adjusted with smaller strength to embed watermark. Finally, by computing the correlation of DCT middle frequency coefficients, and blind watermark is extracted. The experimental results show that the proposed watermarking algorithm has good robustness against image processing attacks, such as JPEG compression, Gaussian noise addition through selection of watermark embedding strength. © 2012 IEEE.","depth perceptual region-of-interest; three-dimensional image; watermarking","3-D image; Blind watermark; Copyright protections; DCT coefficients; Depth image; Depth sensations; Discrete cosine transformation; Frequency coefficient; Gray image; Human Visual System; JPEG compression; Region of interest; Three dimensional images; Three dimensions; Watermark embedding; Watermarking algorithms; Artificial intelligence; Copyrights; Digital image storage; Digital watermarking; Discrete cosine transforms; Gaussian noise (electronic); Image compression; Three dimensional computer graphics; Watermarking; Three dimensional",Conference Paper,Scopus,2-s2.0-84873540155
"Mohamed M.M.A., Far B.","A fast technique for white blood cells nuclei automatic segmentation based on gram-schmidt orthogonalization",2012,"Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI",11,10.1109/ICTAI.2012.133,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876837868&doi=10.1109%2fICTAI.2012.133&partnerID=40&md5=1c77b1573a6c00126399ed5d23c2b910","Blood testing is one of the most important clinical examinations. Counting different blood cells is a significant process in a clinical laboratory. Manual microscopic evaluation is compulsory in case there is suspicious abnormality in the blood sample. Yet, the manual inspection is time-consuming and requires adequate technical knowledge. Therefore, automatic medical diagnosis systems are necessary to help physicians to diagnose diseases in a fast and nonetheless competent way. Cell automatic classification has wider interest especially for clinics and laboratories. Segmentation is the most important step for automatic classification success. This paper represents an efficient technique for automatic blood cell nuclei segmentation. This technique is relying on enhancing the color of the target object, nucleus, and filtering the image. Small objects are eliminated employing morphological operations. A set of 365 blood images was used to quantitatively evaluate this segmentation technique. Assessment of the proposed technique on the blood image set gives 85.4% accuracy. In comparison to other published technique that was implemented and executed on the same dataset, the proposed segmentation technique performance was found to be superior. A differential segmentation performance evaluation was performed on the five normal white blood cell types to compare isolated performance. Eosin Phil was found to have the highest segmentation accuracy with 90.1%. Lymphocyte and Basophil have the lowest accuracy with 78.3% and 78.6% respectively. The blood images dataset and the source code are published on MATLAB file exchange website for comparison and re-production. © 2012 IEEE.","Blood cell; Dataset; Images; Leucocyte; MATLAB; Segmentation; Source code; WBC; white blood cells","Blood cells; Dataset; Images; Leucocyte; Source codes; WBC; White blood cells; Artificial intelligence; Blood; Cells; Diagnosis; Mathematical morphology; MATLAB; Image segmentation",Conference Paper,Scopus,2-s2.0-84876837868
"Kong X., Hu C., Ma H., Han C.","A unified self-stabilizing neural network algorithm for principal and minor components extraction",2012,"IEEE Transactions on Neural Networks and Learning Systems",11,10.1109/TNNLS.2011.2178564,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872975921&doi=10.1109%2fTNNLS.2011.2178564&partnerID=40&md5=fb04dbf88d89443226258ba7f5cfe588","Recently, many unified learning algorithms have been developed for principal component analysis and minor component analysis. These unified algorithms can be used to extract principal components and, if altered simply by the sign, can also serve as a minor component extractor. This is of practical significance in the implementations of algorithms. This paper proposes a unified self-stabilizing neural network learning algorithm for principal and minor components extraction, and studies the stability of the proposed unified algorithm via the fixed-point analysis method. The proposed unified self-stabilizing algorithm for principal and minor components extraction is extended for tracking the principal subspace (PS) and minor subspace (MS). The averaging differential equation and the energy function associated with the unified algorithm for tracking PS and MS are given. It is shown that the averaging differential equation will globally asymptotically converge to an invariance set, and the corresponding energy function exhibit a unique global minimum attained if and only if its state matrices span the PS or MS of the autocorrelation matrix of a vector data stream. It is concluded that the proposed unified algorithm for tracking PS and MS can efficiently track an orthonormal basis of the PS or MS. Simulations are carried out to further illustrate the theoretical results achieved. © 2012 IEEE.","Feature extraction; learning algorithm minor component analysis; minor subspace; neural networks; principal component analysis; principal subspace","Autocorrelation matrix; Minor component analysis; Minor subspaces; Neural network algorithm; Neural network learning algorithm; Principal Components; Principal subspace; Self-stabilizing algorithm; Differential equations; Extraction; Feature extraction; Learning algorithms; Neural networks; Vectors; Principal component analysis; algorithm; artificial intelligence; automated pattern recognition; computer simulation; feedback system; principal component analysis; procedures; statistical model; Algorithms; Artificial Intelligence; Computer Simulation; Feedback; Models, Statistical; Pattern Recognition, Automated; Principal Component Analysis",Article,Scopus,2-s2.0-84872975921
"Galvan E., Harris C., Dusparic I., Clarke S., Cahill V.","Reducing electricity costs in a dynamic pricing environment",2012,"2012 IEEE 3rd International Conference on Smart Grid Communications, SmartGridComm 2012",11,10.1109/SmartGridComm.2012.6485978,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876054172&doi=10.1109%2fSmartGridComm.2012.6485978&partnerID=40&md5=d16f2bc9b80ca189a479ae48f7bbbdc7","Smart Grid technologies are becoming increasingly dynamic, so the use of computational intelligence is becoming more and more common to support the grid to automatically and intelligently respond to certain requests (e.g., reducing electricity costs giving a pricing history). In this work, we propose the use of a particular computational intelligence approach, denominated Distributed W-Learning, that aims to reduce electricity costs in a dynamic environment (e.g., changing prices over a period of time) by turning electric devices on (i.e., clothes dryer, electric vehicle) at residential level, at times when the electricity price is the lowest, while also, balancing the use of energy by avoiding turning on the devices at the same time. We make this problem as realistic as possible, by considering the use of real-world constraints (e.g., time to complete a task, boundary times within which a device can be used). Our results clearly indicate that the use of computational intelligence can be beneficial in this type of dynamic and complex problems. © 2012 IEEE.",,"Complex problems; Dynamic environments; Dynamic pricing; Electric devices; Electricity costs; Electricity prices; Real-world; Smart Grid technologies; Artificial intelligence; Electric vehicles; Electricity; Smart power grids; Cost reduction",Conference Paper,Scopus,2-s2.0-84876054172
"Liu Q., Ihler A.","Belief propagation for structured decision making",2012,"Uncertainty in Artificial Intelligence - Proceedings of the 28th Conference, UAI 2012",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885978015&partnerID=40&md5=1d373c85b8ae2f439275a00ea6ac8994","Variational inference algorithms such as belief propagation have had tremendous impact on our ability to learn and use graphical models, and give many insights for developing or understanding exact and approximate inference. However, variational approaches have not been widely adoped for decision making in graphical models, often formulated through influence diagrams and including both centralized and decentralized (or multi-agent) decisions. In this work, we present a general variational framework for solving structured cooperative decisionmaking problems, use it to propose several belief propagation-like algorithms, and analyze them both theoretically and empirically.",,"Belief propagation; Cooperative decision-making; Exact and approximate inferences; GraphicaL model; Influence diagram; Variational approaches; Variational framework; Variational inference; Algorithms; Artificial intelligence; Decision making; Graphic methods; Inference engines",Conference Paper,Scopus,2-s2.0-84885978015
"Chen Y.-S., Chen Y.-R.","Context-oriented data acquisition and integration platform for internet of things",2012,"Proceedings - 2012 Conference on Technologies and Applications of Artificial Intelligence, TAAI 2012",11,10.1109/TAAI.2012.64,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873391591&doi=10.1109%2fTAAI.2012.64&partnerID=40&md5=d50690e8e8462c5565dd3ba4b65cf765","In this paper, a data acquisition and integration platform for internet of things is proposed. The platform is developed under a cloud computing environment using context-oriented approaches. It collects sensor data from different types of sensor devices, including such as RFID, ZigBee sensors, GPS devices, temperature sensors, humidity sensors, luminance sensors, etc. First we are devoted to the study of deployment, management, and control of different types of sensors for automatic acquisition of sensor data and its related ambient information, both of which will be stored in the IoT repository in a cloud environment. Then, contextoriented mechanisms are developed to produce context data. With the devised context broker, the data retrieved from the IoT repository can be used to produce the contextual portfolio, which is annotated with semantic description. The contextual portfolio will then be stored into a cloud database as the User Portfolio. Finally, services for accessing the User Portfolio in the cloud are developed on a middleware platform, which is compliant with the OSGi standard. With the proposed platform, the acquired data is integrated into semantic contexts, which can be easily shared and reused among different mobile applications. Also, the context information can enhance mobile applications' usability by adapting to conditions that directly affect their operations. © 2012 IEEE.","Context data; Internet of things; Middleware; Wireless sensor networks","Ambient information; Automatic acquisition; Cloud database; Computing environments; Context broker; Context data; Context information; Integration platform; Internet of Things (IOT); Middleware platforms; Mobile applications; Semantic context; Semantic descriptions; Sensor data; Sensor device; Zigbee sensors; Artificial intelligence; Computer systems; Data acquisition; Internet; Middleware; Wireless sensor networks; Zigbee; Sensors",Conference Paper,Scopus,2-s2.0-84873391591
"Parker B., Mustafa A.M., Khan L.","Novel class detection and feature via a tiered ensemble approach for stream mining",2012,"Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI",11,10.1109/ICTAI.2012.168,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876829802&doi=10.1109%2fICTAI.2012.168&partnerID=40&md5=67e1c7ef60366548635a4ccde0e506c1","Static data mining assumptions with regard to features and labels often fail the streaming context. Features evolve, concepts drift, and novel classes are introduced. Therefore, any classification algorithm that intends to operate on streaming data must have mechanisms to mitigate the obsolescence of classifiers trained early in the stream. This is typically accomplished by either continually updating a monolithic model, or incrementally updating an ensemble. Traditional static data mining algorithms futile in a streaming context (and often in a distributed sensor network) due to their need to iterate over the entire data set locally. Our approach - named HSMiner (Hierarchical Stream Miner) - takes a hierarchical decomposition approach to the ensemble classifier concept. By breaking the classification problem into tiers, we can better prune the irrelevant features and counter individual classification error through weighted voting and boosting. In addition, the atomic decomposition of feature inputs enables straightforward mapping to distributing the ensemble among resources in the network. The implementation proves to be fast and very memory conservative, and we emulate a distributed environment via signal-linked threads. We examine the theoretical and empirical analysis of our approach, specifically examining trade-offs of three different novel class detection variations, and compare these results to a similar method using benchmark data sets. © 2012 IEEE.","concept drift; distributed stream mining; feature evolution; hierarchical ensembles; novel class detection","Classification algorithm; Concept drifts; Distributed environments; Distributed sensor networks; feature evolution; Hierarchical decompositions; Hierarchical ensemble; Stream mining; Algorithms; Artificial intelligence; Obsolescence; Sensor networks; Data mining",Conference Paper,Scopus,2-s2.0-84876829802
"Krueger T., Gascon H., Krämer N., Rieck K.","Learning stateful models for network honeypots",2012,"Proceedings of the ACM Conference on Computer and Communications Security",11,10.1145/2381896.2381904,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869823115&doi=10.1145%2f2381896.2381904&partnerID=40&md5=9cc4c57ccbe01dcb76280b536d37bb50","Attacks like call fraud and identity theft often involve sophisticated stateful attack patterns which, on top of normal communication, try to harm systems on a higher semantic level than usual attack scenarios. To detect these kind of threats via specially deployed honeypots, at least a minimal understanding of the inherent state machine of a specific service is needed to lure potential attackers and to keep a communication for a suficiently large number of steps. To this end we propose PRISMA, a method for protocol inspec- tion and state machine analysis, which infers a functional state machine and message format of a protocol from network trafic alone. We apply our method to three real-life network traces ranging from 10,000 up to 2 million messages of both binary and textual protocols. We show that PRISMA is capable of simulating complete and correct sessions based on the learned models. A case study on malware trafic reveals the different states of the execution, rendering PRISMA a valuable tool for malware analysis.","Clustering; Honeypots; Markov Models; Non-negative Matrix Factorization; State Machine Inference","Clustering; Honeypots; Markov model; Nonnegative matrix factorization; State machine; Artificial intelligence; Communication; Crime; Markov processes; Computer crime",Conference Paper,Scopus,2-s2.0-84869823115
"Lin J., Liu P., Jing J.","Using signaling games to model the multi-step attack-defense scenarios on confidentiality",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-34266-0_7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869465454&doi=10.1007%2f978-3-642-34266-0_7&partnerID=40&md5=bf7749b726f0102895e651d97676d833","In the multi-step attack-defense scenarios (MSADSs), each rational player (the attacker or the defender) tries to maximize his payoff, but the uncertainty about his opponent prevents him from taking the suitable actions. The defender doesn't know the attacker's target list, and may deploy unnecessary but costly defenses to protect machines not in the target list. Similarly, the attacker doesn't know the deployed protections, and may spend lots of time and effort on a well-protected machine. We develop a repeated two-way signaling game to model the MSADSs on confidentiality, and show how to find the actions maximizing the expected payoffs through the equilibrium. In the proposed model, on receiving each intrusion detection system alert (i.e., a signal), the defender follows the equilibrium to gradually reduce the uncertainty about the attacker's targets and calculate the defenses maximizing his expected payoff. © 2012 Springer-Verlag.","Attack graph; game theory; multi-step attack-defense scenario; signaling game","Attack graph; Intrusion Detection Systems; Multi-step; Signaling game; Artificial intelligence; Game theory",Conference Paper,Scopus,2-s2.0-84869465454
"Rashedi E., Nezamabadi-Pour H.","Improving the precision of CBIR systems by feature selection using binary gravitational search algorithm",2012,"AISP 2012 - 16th CSI International Symposium on Artificial Intelligence and Signal Processing",11,10.1109/AISP.2012.6313714,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869160220&doi=10.1109%2fAISP.2012.6313714&partnerID=40&md5=1b0d41acb519929001b7a407162c99aa","In this paper, feature selection using binary gravitational search algorithm is utilized to improve the precision of CBIR systems. Content-based image retrieval, CBIR, is one of the most challenging problems in the field of pattern recognition. The performance of a CBIR system is hardly depends on the features that are extracted from images. Thus, selecting most relevant features leads to higher accuracy by reducing the semantic gap between high level features and low level features. Gravitational search algorithm is one of the recent heuristic search algorithms that in this paper, its power is compared with genetic algorithm and binary particle swarm optimization in feature selection. The proposed method is examined in Corel database. Results confirm the efficiency of BGSA to increase the precision of CBIR systems. © 2012 IEEE.","Binary gravitational search algorithm; Content base image retrieval; Feature selection","Binary particle swarm optimization; CBIR system; Content based image retrieval; Gravitational search algorithms; Heuristic search algorithms; High-level features; Low-level features; Semantic gap; Artificial intelligence; Content based retrieval; Feature extraction; Heuristic algorithms; Pattern recognition; Semantics; Signal processing; Learning algorithms",Conference Paper,Scopus,2-s2.0-84869160220
"Michail O., Chatzigiannakis I., Spirakis P.G.","Brief announcement: Naming and counting in anonymous unknown dynamic networks",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-33651-5_46,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868355102&doi=10.1007%2f978-3-642-33651-5_46&partnerID=40&md5=2470f730112a679d2e62cf515c958939","Contribution. We study the fundamental naming and counting problems in networks that are anonymous, unknown, and possibly dynamic. Network dynamicity is modeled by the 1-interval connectivity model [KLO10]. We first prove that on static networks with broadcast counting is impossible to solve without a leader and that naming is impossible to solve even with a leader and even if nodes know n. These impossibilities carry over to dynamic networks as well. With a leader we solve counting in linear time. Then we focus on dynamic networks with broadcast. We show that if nodes know an upper bound on the maximum degree that will ever appear then they can obtain an upper bound on n. Finally, we replace broadcast with one-to-each, in which a node may send a different message to each of its neighbors. This variation is then proved to be computationally equivalent to a full-knowledge model with unique names. © 2012 Springer-Verlag.",,"Connectivity model; Counting problems; Dynamic network; Linear time; Maximum degree; ON dynamics; Static networks; Upper Bound; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868355102
"Aguilera D., Gómez C., Olivé A.","A method for the definition and treatment of conceptual schema quality issues",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-34002-4_39,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868311324&doi=10.1007%2f978-3-642-34002-4_39&partnerID=40&md5=dbd5badc269c5a169085624e9dd60be3","In the literature, there are many proposals of quality properties of conceptual schemas, but only a few of them (mainly those related to syntax) have been integrated into the development environments used by professionals and students. A possible explanation of this unfortunate fact may be that the proposals have been defined in disparate ways, which makes it difficult to integrate them into those environments. In this paper we define quality properties in terms of quality issues, which essentially are conditions that should not happen, and we propose a unified method for their definition and treatment. We show that our method is able to define most of the existing quality properties in a uniform way and makes it possible to integrate quality issues into development environments. The method can be adapted to several languages. We present a prototype implementation of our method as an Eclipse plugin. We have evaluated the potential usefulness of our method by analyzing the presence of a set of quality issues in a set of conceptual schemas developed by students as part of their projects. © 2012 Springer-Verlag.","Conceptual Modeling; Methodologies & Tools; Quality Issues","Conceptual modeling; Conceptual schemas; Development environment; Eclipse plugin; Prototype implementations; Quality properties; Unified method; Artificial intelligence; Data mining",Conference Paper,Scopus,2-s2.0-84868311324
"Lucanu D., Şerbǎnuţǎ T.F., Roşu G.","Double-struck K framework distilled",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-34005-5_3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868318567&doi=10.1007%2f978-3-642-34005-5_3&partnerID=40&md5=3c4398fb3420f966e8d4e21d0ca9a342","Double-struck K is a rewrite-based executable semantic framework in which programming languages, type systems, and formal analysis tools can be defined using configurations, computations and rules. Configurations organize the state in units called cells, which are labeled and can be nested. Computations are special nested list structures sequentializing computational tasks, such as fragments of program. Double-struck K (rewrite) rules make it explicit which parts of the term they read-only, write-only, read-write, or do not care about. This makes Double-struck K suitable for defining truly concurrent languages even in the presence of sharing. Computations are like any other terms in a rewriting environment: they can be matched, moved from one place to another, modified, or deleted. This makes Double-struck K suitable for defining control-intensive features such as abrupt termination, exceptions or call/cc. This paper presents an overview of Double-struck K Framework and the Double-struck K tool, focusing on the interaction between the tool and Maude. © 2012 Springer-Verlag.",,"Computational task; Concurrent languages; Executable semantic framework; Formal analysis tools; Type systems; Artificial intelligence; Semantics",Conference Paper,Scopus,2-s2.0-84868318567
"Suzuki I., Hara K., Shimbo M., Matsumoto Y., Saerens M.","Investigating the effectiveness of Laplacian-based Kernels in hub reduction",2012,"Proceedings of the National Conference on Artificial Intelligence",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868299046&partnerID=40&md5=19a031abda5e09d7381c2721f9fb6ce1","A ""hub"" is an object closely surrounded by, or very similar to, many other objects in the dataset. Recent studies by Radovanović et al. indicate that in high dimensional spaces, hubs almost always emerge, and objects close to the data centroid tend to become hubs. In this paper, we show that the family of kernels based on the graph Laplacian makes all objects in the dataset equally similar to the centroid, and thus they are expected to make less hubs when used as a similarity measure. We investigate this hypothesis using both synthetic and real-world data. It turns out that these kernels suppress hubs in some cases but not always, and the results seem to be affected by the size of the data. However, for the datasets in which hubs are indeed reduced by the Laplacian-based kernels, these kernels work well in ranking and classification tasks. This result suggests that the amount of hubs, which can be readily computed in an unsupervised fashion, can be a yardstick of whether Laplacian-based kernels work effectively for a given data. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Classification tasks; Data sets; Graph Laplacian; High dimensional spaces; Real world data; Similarity measure; Artificial intelligence; Classification (of information); Laplace transforms",Conference Paper,Scopus,2-s2.0-84868299046
"Djuric N., Grbovic M., Vucetic S.","Convex kernelized sorting",2012,"Proceedings of the National Conference on Artificial Intelligence",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868268588&partnerID=40&md5=4fbe3562abd28a42095260369cec8710","Kernelized sorting is a method for aligning objects across two domains by considering within-domain similarity, without a need to specify a cross-domain similarity measure. In this paper we present the Convex Kernelized Sorting method where, unlike in the previous approaches, the cross-domain object matching is formulated as a convex optimization problem, leading to simpler optimization and global optimum solution. Our method outputs soft alignments between objects, which can be used to rank the best matches for each object, or to visualize the object matching and verify the correct choice of the kernel. It also allows for computing hard one-to-one alignments by solving the resulting Linear Assignment Problem. Experiments on a number of cross-domain matching tasks show the strength of the proposed method, which consistently achieves higher accuracy than the existing methods. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Best match; Convex optimization problems; Cross-domain; Global optimum solutions; Linear assignment problem; Object matching; Similarity measure; Sorting method; Two domains; Alignment; Convex optimization; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868268588
"He J., Zhou M., Jiang L.","Generating Chinese classical poems with statistical machine translation models",2012,"Proceedings of the National Conference on Artificial Intelligence",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868269975&partnerID=40&md5=073ee6e525562c8059b61cbb6878b64c","This paper describes a statistical approach to generation of Chinese classical poetry and proposes a novel method to automatically evaluate poems. The system accepts a set of keywords representing the writing intents from a writer and generates sentences one by one to form a completed poem. A statistical machine translation (SMT) system is applied to generate new sentences, given the sentences generated previously. For each line of sentence a specific model specially trained for that line is used, as opposed to using a single model for all sentences. To enhance the coherence of sentences on every line, a coherence model using mutual information is applied to select candidates with better consistency with previous sentences. In addition, we demonstrate the effectiveness of the BLEU metric for evaluation with a novel method of generating diverse references. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Coherence models; Mutual informations; Statistical approach; Statistical machine translation; Linguistics; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868269975
"Chakraborty P., Marwah M., Arlitt M., Ramakrishnan N.","Fine-grained photovoltaic output prediction using a Bayesian ensemble",2012,"Proceedings of the National Conference on Artificial Intelligence",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868273830&partnerID=40&md5=ee58a227aefb38f25ea371f7e2d6bac2","Local and distributed power generation is increasingly reliant on renewable power sources, e.g., solar (photovoltaic or PV) and wind energy. The integration of such sources into the power grid is challenging, however, due to their variable and intermittent energy output. To effectively use them on a large scale, it is essential to be able to predict power generation at a fine-grained level. We describe a novel Bayesian ensemble methodology involving three diverse predictors. Each predictor estimates mixing coefficients for integrating PV generation output profiles but captures fundamentally different characteristics. Two of them employ classical parameterized (naive Bayes) and non-parametric (nearest neighbor) methods to model the relationship between weather forecasts and PV output. The third predictor captures the sequentiality implicit in PV generation and uses motifs mined from historical data to estimate the most likely mixture weights using a stream prediction methodology. We demonstrate the success and superiority of our methods on real PV data from two locations that exhibit diverse weather conditions. Predictions from our model can be harnessed to optimize scheduling of delay tolerant workloads, e.g., in a data center. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Data centers; Delay tolerant; Energy output; Historical data; Mixing coefficient; Naive bayes; Nearest neighbors; Non-parametric; Parameterized; Photovoltaic; Power grids; Power sources; Prediction methodology; PV generation; Sequentiality; Weather conditions; Weather forecasts; Artificial intelligence; Distributed power generation; Weather forecasting; Wind power; Solar power generation",Conference Paper,Scopus,2-s2.0-84868273830
"Wang D., Li T., Ogihara M.","Generating pictorial storylines via minimum-weight connected dominating set approximation in multi-view graphs",2012,"Proceedings of the National Conference on Artificial Intelligence",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868278518&partnerID=40&md5=cc70ca6d08e776f18dc26160ad282e77","This paper introduces a novel framework for generating pictorial storylines for given topics from text and image data on the Internet. Unlike traditional text summarization and timeline generation systems, the proposed framework combines text and image analysis and delivers a storyline containing textual, pictorial, and structural information to provide a sketch of the topic evolution. A key idea in the framework is the use of an approximate solution for the dominating set problem. Given a collection of topic-related objects consisting of images and their text descriptions, a weighted multi-view graph is first constructed to capture the contextual and temporal relationships among these objects. Then the objects are selected by solving the minimum-weighted connected dominating set problem defined on this graph. Comprehensive experiments on real-world data sets demonstrate the effectiveness of the proposed framework. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Approximate solution; Connected Dominating Set; Dominating set problems; Generation systems; Image data; Multi-views; Real world data; Storylines; Structural information; Temporal relationships; Text summarization; Virtual reality; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868278518
"Fang J., Mitra P., Tang Z., Giles C.L.","Table header detection and classification",2012,"Proceedings of the National Conference on Artificial Intelligence",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868278666&partnerID=40&md5=dd322e83c6fd1daacbb4e8d14516eac6","In digital libraries, a table, as a specific document component as well as a condensed way to present structured and relational data, contains rich information and often the only source of that information. In order to explore, retrieve, and reuse that data, tables should be identified and the data extracted. Table recognition is an old field of research. However, due to the diversity of table styles, the results are still far from satisfactory, and not a single algorithm performs well on all different types of tables. In this paper, we randomly take samples from the CiteSeer to investigate diverse table styles for automatic table extraction. We find that table headers are one of the main characteristics of complex table styles. We identify a set of features that can be used to segregate headers from tabular data and build a classifier to detect table headers. Our empirical evaluation on PDF documents shows that using a Random Forest classifier achieves an accuracy of 92%. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"CiteSeer; Empirical evaluations; Old-field; PDF document; Random forest classifier; Relational data; Tabular data; Decision trees; Digital libraries; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868278666
"Flammini F., Marrone S., Mazzocca N., Nardone R., Vittorini V.","Model-driven V&V processes for computer based control systems: A unifying perspective",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-34032-1_20,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868274424&doi=10.1007%2f978-3-642-34032-1_20&partnerID=40&md5=f8243910d6463fde9f180ec9356d6337","A recent trend in software engineering is to support the development process by providing flexible tool chains allowing for effective Model-Driven approaches. These solutions are very appealing in industrial settings since they enable the creation of development and verification processes, enhancing abstraction and reuse, and hence improving productivity. This paper addresses advantages and challenges in extending Model-Driven approaches to system engineering and specifically to verification and validation (V&V) of critical computer-based systems. Specifically, the paper highlights the needs for real-world industrial contexts and proposes the definition of a unifying Model-Driven process for V&V of functional and non-functional system properties. Some enabling techniques which aim at improving the reuse of Model-Driven artifacts are addressed to deal with process scalability and effectiveness. Two sample applications are described for ERTMS/ETCS signalling system in order to show the advantages of the approach: formal modeling for performance evaluation of message delivery between train and track controllers and test case generation for the verification of functional requirements of trains outdistancing. © 2012 Springer-Verlag.","Critical Systems; Domain Specific Languages; Model-Driven Engineering; Railway Systems; Verification & Validation","Computer based control systems; Computer-based system; Critical systems; Development process; Domain specific languages; Enabling techniques; Flexible tool; Formal modeling; Functional requirement; Industrial context; Industrial settings; Message delivery; Model driven approach; Model-driven; Model-driven Engineering; Non-functional; Performance evaluation; Railway system; Recent trends; Signalling systems; System property; Test case generation; Verification and validation; Verification process; Artificial intelligence; Software engineering",Conference Paper,Scopus,2-s2.0-84868274424
"Paparrizou A., Stergiou K.","An efficient higher-order consistency algorithm for table constraints",2012,"Proceedings of the National Conference on Artificial Intelligence",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868298721&partnerID=40&md5=3bb5d0ddd0390b7045fcc95d1e997805","Table constraints are very important in constraint programming as they are present in many real problems from areas such as configuration and databases. As a result, numerous specialized algorithms that achieve generalized arc consistency (GAC) on table constraints have been proposed. Since these algorithms achieve GAC, they operate on one constraint at a time. In this paper we propose an efficient algorithm for table constraints that achieves a stronger local consistency than GAC. This algorithm, called maxRPWC+, is based on the local consistency maxRPWC and allows the efficient handling of intersecting table constraints. Experimental results from benchmark problems demonstrate that maxRPWC+ is clearly more robust than a state-of-the-art GAC algorithm in classes of problems with interleaved table constraints, being orders of magnitude faster in some of these classes. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Bench-mark problems; Consistency algorithms; Constraint programming; Generalized arc consistencies; Local consistency; Orders of magnitude; Real problems; Table constraints; Artificial intelligence; Computer programming; Constraint theory; Algorithms",Conference Paper,Scopus,2-s2.0-84868298721
"Imprialou M., Stoilos G., Cuenca Grau B.","Benchmarking ontology-based query rewriting systems",2012,"Proceedings of the National Conference on Artificial Intelligence",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868270846&partnerID=40&md5=90b2bd4ec4d84aea3a6b3700c76f560d","Query rewriting is a prominent reasoning technique in ontology-based data access applications. A wide variety of query rewriting algorithms have been proposed in recent years and implemented in highly optimised reasoning systems. Query rewriting systems are complex software programs; even if based on provably correct algorithms, sophisticated optimisations make the systems more complex and errors become more likely to happen. In this paper, we present an algorithm that, given an ontology as input, synthetically generates ""relevant"" test queries. Intuitively, each of these queries can be used to verify whether the system correctly performs a certain set of ""inferences"", each of which can be traced back to axioms in the input ontology. Furthermore, we present techniques that allow us to determine whether a system is unsound and/or incomplete for a given test query and ontology. Our evaluation shows that most publicly available query rewriting systems are unsound and/or incomplete, even on commonly used benchmark ontologies; more importantly, our techniques revealed the precise causes of their correctness issues and the systems were then corrected based on our feedback. Finally, since our evaluation is based on a larger set of test queries than existing benchmarks, which are based on hand-crafted queries, it also provides a better understanding of the scalability behaviour of each system. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Complex software; Ontology-based data access; Ontology-based query; Optimisations; Query rewritings; Reasoning system; Reasoning techniques; Algorithms; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868270846
"Bui H.H., Huynh T.N., De Braz R.S.","Exact lifted inference with distinct soft evidence on every object",2012,"Proceedings of the National Conference on Artificial Intelligence",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868266545&partnerID=40&md5=853676ac6c149179b02b55bef507d9dc","The presence of non-symmetric evidence has been a barrier for the application of lifted inference since the evidence destroys the symmetry of the first-order probabilistic model. In the extreme case, if distinct soft evidence is obtained about each individual object in the domain then, often, all current exact lifted inference methods reduce to traditional inference at the ground level. However, it is of interest to ask whether the symmetry of the model itself before evidence is obtained can be exploited. We present new results showing that this is, in fact, possible. In particular, we show that both exact maximum a posteriori (MAP) and marginal inference can be lifted for the case of distinct soft evidence on a unary Markov Logic predicate. Our methods result in efficient procedures for MAP and marginal inference for a class of graphical models previously thought to be intractable. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"First-order; GraphicaL model; Ground level; Individual objects; Inference methods; Markov logic; Maximum a posteriori; New results; Probabilistic models; Soft evidences; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868266545
"Brill M., Fischer F.","The price of neutrality for the ranked pairs method",2012,"Proceedings of the National Conference on Artificial Intelligence",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868283500&partnerID=40&md5=a8e10304ff4fcabe998e2c47f81990fe","The complexity of the winner determination problem has been studied for almost all common voting rules. A notable exception, possibly caused by some confusion regarding its exact definition, is the method of ranked pairs. The original version of the method, due to Tideman, yields a social preference function that is irresolute and neutral. A variant introduced subsequently uses an exogenously given tie-breaking rule and therefore fails neutrality. The latter variant is the one most commonly studied in the area of computational social choice, and it is easy to see that its winner determination problem is computationally tractable. We show that by contrast, computing the set of winners selected by Tideman's original ranked pairs method is NP-complete, thus revealing a trade-off between tractability and neutrality. In addition, several known results concerning the hardness of manipulation and the complexity of computing possible and necessary winners are shown to follow as corollaries from our findings. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"NP Complete; Social choice; Social preference; Tie-breaking; Voting rules; Winner determination problem; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868283500
"Lin C.H., Mausam, Weld D.S.","Dynamically switching between synergistic workflows for crowdsourcing",2012,"Proceedings of the National Conference on Artificial Intelligence",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868278309&partnerID=40&md5=7d92a42b1f53f432a580c6ef02fbb152","To ensure quality results from unreliable crowdsourced workers, task designers often construct complex workflows and aggregate worker responses from redundant runs. Frequently, they experiment with several alternative workflows to accomplish the task, and eventually deploy the one that achieves the best performance during early trials. Surprisingly, this seemingly natural design paradigm does not achieve the full potential of crowdsourcing. In particular, using a single workflow (even the best) to accomplish a task is suboptimal. We show that alternative workflows can compose synergistically to yield much higher quality output. We formalize the insight with a novel probabilistic graphical model. Based on this model, we design and implement AGENTHUNT, a POMDP-based controller that dynamically switches between these workflows to achieve higher returns on investment. Additionally, we design offline and online methods for learning model parameters. Live experiments on Amazon Mechanical Turk demonstrate the superiority of AGENTHUNT for the task of generating NLP training data, yielding up to 50% error reduction and greater net utility compared to previous methods. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Complex workflows; Crowdsourcing; Error reduction; Learning models; Mechanical turks; Natural design; Offline; Online methods; Probabilistic graphical models; Returns on investment; Training data; Work-flows; Design; Experiments; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868278309
"Xue Y., Choi A., Darwiche A.","Basing decisions on sentences in decision diagrams",2012,"Proceedings of the National Conference on Artificial Intelligence",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868293474&partnerID=40&md5=0737afa7d361c4f0e47361923afc1aca","The Sentential Decision Diagram (SDD) is a recently proposed representation of Boolean functions, containing Ordered Binary Decision Diagrams (OBDDs) as a distinguished subclass. While OBDDs are characterized by total variable orders, SDDs are characterized by dissections of variable orders, known as vtrees. Despite this generality, SDDs retain a number of properties, such as canonicity and a polytime Apply operator, that have been critical to the practical success of OBDDs. Moreover, upper bounds on the size of SDDs were also given, which are tighter than comparable upper bounds on the size of OBDDs. In this paper, we analyze more closely some of the theoretical properties of SDDs and their size. In particular, we consider the impact of basing decisions on sentences (using dissections as in SDDs), in comparison to basing decisions on variables (using total variable orders as in OBDDs). Here, we identify a class of Boolean functions where basing decisions on sentences using dissections of a variable order can lead to exponentially more compact SDDs, compared to OBDDs based on the same variable order. Moreover, we identify a fundamental property of the decompositions that underlie SDDs and use it to show how certain changes to a vtree can also lead to exponential differences in the size of an SDD. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Decision diagram; Fundamental properties; Ordered binary decision diagrams; Upper Bound; Variable order; Boolean functions; Decision theory; Dissection; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84868293474
"Biazzo V., Gilio A., Sanfilippo G.","Coherent conditional previsions and proper scoring rules",2012,"Communications in Computer and Information Science",11,10.1007/978-3-642-31724-8_16,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868266864&doi=10.1007%2f978-3-642-31724-8_16&partnerID=40&md5=ac7405f911d51b763f21b503616215d7","In this paper we study the relationship between the notion of coherence for conditional prevision assessments on a family of finite conditional random quantities and the notion of admissibility with respect to bounded strictly proper scoring rules. Our work extends recent results given by the last two authors of this paper on the equivalence between coherence and admissibility for conditional probability assessments. In order to prove that admissibility implies coherence a key role is played by the notion of Bregman divergence. © 2012 Springer-Verlag.","admissibility; Bregman divergence; coherence; Conditional prevision assessments; conditional scoring rules; proper scoring rules; strong dominance; weak dominance","admissibility; Bregman divergences; Conditional prevision assessments; Proper scoring rules; Scoring rules; strong dominance; weak dominance; Artificial intelligence; Coherent light; Data processing; Knowledge based systems; Information management",Conference Paper,Scopus,2-s2.0-84868266864
"Luo Z., Osborne M., Petrović S., Wang T.","Improving twitter retrieval by exploiting structural information",2012,"Proceedings of the National Conference on Artificial Intelligence",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868276153&partnerID=40&md5=74444b192348e9b37646822df23073ac","Most Twitter search systems generally treat a tweet as a plain text when modeling relevance. However, a series of conventions allows users to tweet in structural ways using combination of different blocks of texts. These blocks include plain texts, hashtags, links, mentions, etc. Each block encodes a variety of communicative intent and sequence of these blocks captures changing discourse. Previous work shows that exploiting the structural information can improve the structured document (e.g., web pages) retrieval. In this paper we utilize the structure of tweets, induced by these blocks, for Twitter retrieval. A set of features, derived from the blocks of text and their combinations, is used into a learning-to-rank scenario. We show that structuring tweets can achieve state-of-the-art performance. Our approach does not rely upon social media features, but when we do add this additional information, performance improves significantly. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Communicative intent; Plain text; Search system; Social media; State-of-the-art performance; Structural information; Structured document; Artificial intelligence; Social networking (online); Information retrieval",Conference Paper,Scopus,2-s2.0-84868276153
"Edelkamp S., Federholzner T., Kissmann P.","Searching with partial belief states in general games with incomplete information",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-33347-7_3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868224175&doi=10.1007%2f978-3-642-33347-7_3&partnerID=40&md5=6b4bca3192e6e6b87f68519269a1f62a","In this paper we present a full-fledged player for general games with incomplete information specified in the game description language GDL-II. To deal with uncertainty we introduce a method that operates on partial belief states, which correspond to a subset of the set of states building a full belief state. To search for a partial belief state we present depth-first and Monte-Carlo methods. All can be combined with any traditional general game player, e.g., using minimax or UCT search. Our general game player is shown to be effective in a number of benchmarks and the UCT variant compares positively with the one-and-only winner of an incomplete information track at an international general game playing competition. © 2012 Springer-Verlag.",,"Depth first; Description languages; Game players; Games with incomplete information; General game playing; Incomplete information; Minimax; Artificial intelligence; Potassium iodide",Conference Paper,Scopus,2-s2.0-84868224175
"Mas M., Monserrat M., Ruiz-Aguilera D., Torrens J.","On migrative t-conorms and uninorms",2012,"Communications in Computer and Information Science",11,10.1007/978-3-642-31718-7_30,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868113325&doi=10.1007%2f978-3-642-31718-7_30&partnerID=40&md5=9cb2d4a0c415347160fa59a60651a377","In this paper the notions of α-migrative t-conorms over a fixed t-conorm S 0, and α-migrative uninorms over another fixed uninorm U 0 with the same neutral element are introduced. All continuous t-conorms that are α-migrative over the maximum, the probabilistic sum and the Łukasiewicz t-conorm are characterized. Uninorms belonging to one of the classes, idempotent or representable that are α-migrative over a uninorm U 0 in or are also characterized. © 2012 Springer-Verlag Berlin Heidelberg.","Aggregation function; migrativity; t-conorm; t-norm; uninorm","Aggregation functions; Migrativity; T-conorms; t-norm; Uninorms; Artificial intelligence; Data processing; Information management; Knowledge based systems; Mathematical operators",Conference Paper,Scopus,2-s2.0-84868113325
"Aiche F., Dubois D.","Possibility and gradual number approaches to ranking methods for random fuzzy intervals",2012,"Communications in Computer and Information Science",11,10.1007/978-3-642-31718-7_2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868121903&doi=10.1007%2f978-3-642-31718-7_2&partnerID=40&md5=439b3c97c9e2520dd299656c0a3e8513","This paper deals with methods for ranking fuzzy intervals in connection with probabilistic and interval orderings. According to the interpretation of a fuzzy interval, various such extensions can be laid bare. In this paper, we especially consider extensions of probabilistic orderings using possibilistic interpretations of fuzzy intervals, crisp substitutes thereof, and gradual numbers. This framework can encompass the comparison of fuzzy random variables: coupling one form of probabilistic comparison with one form of interval comparison induces a method for ranking random fuzzy intervals. © 2012 Springer-Verlag Berlin Heidelberg.",,"Fuzzy interval; Fuzzy random variable; Interval comparisons; One-form; Possibilistic; Ranking methods; Artificial intelligence; Data processing; Knowledge based systems; Information management",Conference Paper,Scopus,2-s2.0-84868121903
"Ferreira A.A., Gonçalves M.A., Almeida J.M., Laender A.H.F., Veloso A.","A tool for generating synthetic authorship records for evaluating author name disambiguation methods",2012,"Information Sciences",11,10.1016/j.ins.2012.04.022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861613741&doi=10.1016%2fj.ins.2012.04.022&partnerID=40&md5=322cf1bc8e6c6a6712d50cadc973b7d6","The author name disambiguation task has to deal with uncertainties related to the possible many-to-many correspondences between ambiguous names and unique authors. Despite the variety of name disambiguation methods available in the literature to solve the problem, most of them are rarely compared against each other. Moreover, they are often evaluated without considering a time evolving digital library, susceptible to dynamic (and therefore challenging) patterns such as the introduction of new authors and the change of researchers' interests over time. In order to facilitate the evaluation of name disambiguation methods in various realistic scenarios and under controlled conditions, in this article we propose SyGAR, a new Synthetic Generator of Authorship Records that generates citation records based on author profiles. SyGAR can be used to generate successive loads of citation records simulating a living digital library that evolves according to various publication patterns. We validate SyGAR by comparing the results produced by three representative name disambiguation methods on real as well as synthetically generated collections of citation records. We also demonstrate its applicability by evaluating those methods on a time evolving digital library collection generated with the tool, considering several dynamic and realistic scenarios. © 2012 Elsevier Inc. All rights reserved.","Author name disambiguation; Bibliographic citation; Digital library; Synthetic generator","Bibliographic citation; Controlled conditions; Digital library collections; Name disambiguation; Realistic scenario; Synthetic generator; Artificial intelligence; Software engineering; Digital libraries",Article,Scopus,2-s2.0-84861613741
"Yang X., Zhang Y., Yang J.","Local and Global Measurements of MGRS Rules",2012,"International Journal of Computational Intelligence Systems",11,10.1080/18756891.2012.747655,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869190625&doi=10.1080%2f18756891.2012.747655&partnerID=40&md5=2dbf28f046617c473dc6356ba25d1f74","Since the multigranulation rough sets (MGRS) can be considered as the compositions of multi-independent Pawlak's rough sets, the multigranulation rough set rules (MGRS rules) are then the compositions of decision rules, which are supported by multi-independent Pawlak's rough sets. To measure MGRS rules, both the local and global views are employed in this paper. In local view, the support, certainty and coverage factors are proposed to measure MGRS rules, which are support by an object; while in global view, these three factors are proposed to measure MGRS rules, which are coexisting in a decision system. The necessary conditions for these factors to achieve maximal and minimal values are also addressed. Some numerical examples are employed to substantiate the conceptual arguments. © 2012 Copyright the authors.","Certainty factor; coverage factor; decision rule; multigranulation rough set; support factor","Certainty factors; Coverage factors; Decision rules; Decision systems; Global measurements; Global view; Minimal value; Numerical example; Rough set; Artificial intelligence; Computer science; Rough set theory",Article,Scopus,2-s2.0-84869190625
"Yang F., Grigsby P.W.","Delineation of FDG-PET tumors from heterogeneous background using spectral clustering",2012,"European Journal of Radiology",11,10.1016/j.ejrad.2012.01.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867021866&doi=10.1016%2fj.ejrad.2012.01.001&partnerID=40&md5=33c6f52b960316a5e7b6db3b847c0c91","This paper explored the feasibility of using spectral clustering to segment FDG-PET tumor in the presence of heterogeneous background. Spectral clustering refers to a class of clustering methods which employ the eigenstructure of a similarity matrix to partition image voxels into disjoint clusters. The similarity between two voxels was measured with the intensity distance scaled by voxel-varying factors capturing local statistics and the number of clusters was inferred based on rotating the eigenvector matrix for the maximally sparse representation. Metrics used to evaluate the segmentation accuracy included: Dice coefficient, Jaccard coefficient, false positive dice, false negative dice, symmetric mean absolute surface distance, and absolute volumetric difference. Comparison of segmentation results between the presented method and the adaptive thresholding method on the simulated PET data shows the former attains an overall better detection accuracy. Applying the presented method on patient data gave segmentation results in fairly good agreement with physician manual annotations. These results indicate that the presented method have the potential to accurately delineate complex shaped FDG-PET tumors containing inhomogeneous activities in the presence of heterogeneous background. © 2012 Elsevier Ireland Ltd. All rights reserved.","Digital phantom; FDG-PET; Heterogeneous background; Spectral clustering; Tumor segmentation","fluorodeoxyglucose f 18; article; cluster analysis; diagnostic accuracy; gold standard; human; oncological parameters; positron emission tomography; priority journal; tumor segmentation; Algorithms; Artifacts; Artificial Intelligence; Fluorodeoxyglucose F18; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Neoplasms; Pattern Recognition, Automated; Positron-Emission Tomography; Radiopharmaceuticals; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84867021866
"Adamskiy D., Koolen W.M., Chernov A., Vovk V.","A closer look at adaptive regret",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-34106-9_24,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867856259&doi=10.1007%2f978-3-642-34106-9_24&partnerID=40&md5=01b1a615294f58b7f0ad99bdf8048ef6","For the prediction with expert advice setting, we consider methods to construct algorithms that have low adaptive regret. The adaptive regret of an algorithm on a time interval [t 1,t 2] is the loss of the algorithm there minus the loss of the best expert. Adaptive regret measures how well the algorithm approximates the best expert locally, and it is therefore somewhere between the classical regret (measured on all outcomes) and the tracking regret, where the algorithm is compared to a good sequence of experts. We investigate two existing intuitive methods to derive algorithms with low adaptive regret, one based on specialist experts and the other based on restarts. Quite surprisingly, we show that both methods lead to the same algorithm, namely Fixed Share, which is known for its tracking regret. Our main result is a thorough analysis of the adaptive regret of Fixed Share. We obtain the exact worst-case adaptive regret for Fixed Share, from which the classical tracking bounds can be derived. We also prove that Fixed Share is optimal, in the sense that no algorithm can have a better adaptive regret bound. © 2012 Springer-Verlag.","adaptive regret; Fixed Share; Online learning; specialist experts","adaptive regret; Fixed Share; Online learning; Prediction with expert advice; specialist experts; Time interval; Artificial intelligence; Algorithms",Conference Paper,Scopus,2-s2.0-84867856259
"Guo R., Hoiem D.","Beyond the line of sight: Labeling the underlying surfaces",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-33715-4_55,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867859453&doi=10.1007%2f978-3-642-33715-4_55&partnerID=40&md5=073a6a138085624bd069c47fa147674e","Scene understanding requires reasoning about both what we can see and what is occluded. We offer a simple and general approach to infer labels of occluded background regions. Our approach incorporates estimates of visible surrounding background, detected objects, and shape priors from transferred training regions. We demonstrate the ability to infer the labels of occluded background regions in both the outdoor StreetScenes dataset and an indoor scene dataset using the same approach. Our experiments show that our method outperforms competent baselines. © 2012 Springer-Verlag.",,"Background region; Data sets; General approach; Line of Sight; Scene understanding; Shape priors; Underlying surface; Artificial intelligence; Computer vision",Conference Paper,Scopus,2-s2.0-84867859453
"Mersmann O., Bischl B., Bossek J., Trautmann H., Wagner M., Neumann F.","Local search and the traveling salesman problem: A feature-based characterization of problem hardness",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-34413-8_9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867884399&doi=10.1007%2f978-3-642-34413-8_9&partnerID=40&md5=0ca153a59dc3031e8a2084127bf0a2d9","With this paper we contribute to the understanding of the success of 2-opt based local search algorithms for solving the traveling salesman problem (TSP). Although 2-opt is widely used in practice, it is hard to understand its success from a theoretical perspective. We take a statistical approach and examine the features of TSP instances that make the problem either hard or easy to solve. As a measure of problem difficulty for 2-opt we use the approximation ratio that it achieves on a given instance. Our investigations point out important features that make TSP instances hard or easy to be approximated by 2-opt. © 2012 Springer-Verlag.","2-opt; Classification; Feature Selection; MARS; TSP","2-opt; Approximation ratios; Feature-based; Local search; Local search algorithm; MARS; Problem difficulty; Statistical approach; TSP; Artificial intelligence; Classification (of information); Feature extraction; Traveling salesman problem",Conference Paper,Scopus,2-s2.0-84867884399
"Smith B.M., Zhang L.","Joint face alignment with non-parametric shape models",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-33712-3_4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867860951&doi=10.1007%2f978-3-642-33712-3_4&partnerID=40&md5=11b93712a6485b67a4c39545899f8064","We present a joint face alignment technique that takes a set of images as input and produces a set of shape- and appearance-consistent face alignments as output. Our method is an extension of the recent localization method of Belhumeur et al. [1], which combines the output of local detectors with a non-parametric set of face shape models. We are inspired by the recent joint alignment method of Zhao et al. [20], which employs a modified Active Appearance Model (AAM) approach to align a batch of images. We introduce an approach for simultaneously optimizing both a local appearance constraint, which couples the local estimates between multiple images, and a global shape constraint, which couples landmarks and images across the image set. In video sequences, our method greatly improves the temporal stability of landmark estimates without compromising accuracy relative to ground truth. © 2012 Springer-Verlag.",,"Active appearance models; Alignment methods; Face alignment; Face shapes; Ground truth; Image sets; Localization method; Multiple image; Non-parametric; Shape constraints; Shape model; Temporal stability; Video sequences; Artificial intelligence; Computer vision",Conference Paper,Scopus,2-s2.0-84867860951
"Miori V., Russo D.","Anticipating health hazards through an ontology-based, IoT domotic environment",2012,"Proceedings - 6th International Conference on Innovative Mobile and Internet Services in Ubiquitous Computing, IMIS 2012",11,10.1109/IMIS.2012.109,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867708463&doi=10.1109%2fIMIS.2012.109&partnerID=40&md5=ad058ef88221559e01625009b605988f","Embedded intelligence entails a vision of the Internet of Things oriented to ""Ambient Intelligence"", which can be defined as the capacity to collect and analyze the digital traces left by people when they interact with the environment and with such ""intelligent things"". In such a vision, the ultimate aim is to acquire knowledge about everyday life and human behavior. We apply these concepts to the automated home, in which even nowadays devices (sensors and actuators) are already Internet-capable ""things"" endowed with their own intelligence. This work introduces a semantic knowledge representation for domotics following the principles of the semantic Web (Web 3.0). We have implemented specific ontologies that automatically take account of distributed environmental context information. The first result was to achieve a natural abstraction of different, incompatible devices in order to support interoperability in a consistent technology-independent manner and impart awareness to the domestic environment in which ""things"" such as furniture and other objects, as well as the rooms themselves, take on computational significance. This goal includes leveraging the development of a comprehensive intelligent ecosystem that, by applying machine learning and artificial intelligence techniques, gains knowledge of the occupants' behaviors and habits and can thereby adapt itself to the specific setting and anticipate their needs without direct human intervention. The same modeling process serves as the basis for implementation of an e-health system that can anticipate, and thereby prevent, possible health hazards before emergency situations arise (especially for the sick and elderly). © 2012 IEEE.","Ambient Intelligent; DomoNet; domotics; E-Health; home automation; Machine Learning; Ontology; SOA; XML","Ambient intelligent; DomoNet; Domotics; Ehealth; Home automation; SOA; Artificial intelligence; Health hazards; Knowledge management; Knowledge representation; Learning systems; Ontology; Ubiquitous computing; XML; Semantic Web",Conference Paper,Scopus,2-s2.0-84867708463
"Atzmueller M., Doerfel S., Hotho A., Mitzlaff F., Stumme G.","Face-to-face contacts at a conference: Dynamics of communities and roles",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-33684-3_2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867626379&doi=10.1007%2f978-3-642-33684-3_2&partnerID=40&md5=5c33567363ee54cd3359a14c39d8bd59","This paper focuses on the community analysis of conference participants using their face-to-face contacts, visited talks, and tracks in a social and ubiquitous conferencing scenario. We consider human face-to-face contacts and perform a dynamic analysis of the number of contacts and their lengths. On these dimensions, we specifically investigate user-interaction and community structure according to different special interest groups during a conference. Additionally, using the community information, we examine different roles and their characteristic elements. The analysis is grounded using real-world conference data capturing community information about participants and their face-to-face contacts. The analysis results indicate, that the face-to-face contacts show inherent community structure grounded using the special interest groups. Furthermore, we provide individual and community-level properties, traces of different behavioral patterns, and characteristic (role) profiles. © 2012 Springer-Verlag.",,"Behavioral patterns; Community analysis; Community structures; Data capturing; Special interest groups; Artificial intelligence; Social sciences",Conference Paper,Scopus,2-s2.0-84867626379
