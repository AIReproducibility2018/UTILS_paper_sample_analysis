Authors,Title,Year,Source title,Cited by,DOI,Link,Abstract,Author Keywords,Index Keywords,Document Type,Source,EID
"Liu G., Lin Z., Yan S., Sun J., Yu Y., Ma Y.","Robust recovery of subspace structures by low-rank representation",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",665,10.1109/TPAMI.2012.88,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870197517&doi=10.1109%2fTPAMI.2012.88&partnerID=40&md5=52ec1b7fa3b33d15dbf5dfa4297e57a8","In this paper, we address the subspace clustering problem. Given a set of data samples (vectors) approximately drawn from a union of multiple subspaces, our goal is to cluster the samples into their respective subspaces and remove possible outliers as well. To this end, we propose a novel objective function named Low-Rank Representation (LRR), which seeks the lowest rank representation among all the candidates that can represent the data samples as linear combinations of the bases in a given dictionary. It is shown that the convex program associated with LRR solves the subspace clustering problem in the following sense: When the data is clean, we prove that LRR exactly recovers the true subspace structures; when the data are contaminated by outliers, we prove that under certain conditions LRR can exactly recover the row space of the original data and detect the outlier as well; for data corrupted by arbitrary sparse errors, LRR can also approximately recover the row space with theoretical guarantees. Since the subspace membership is provably determined by the row space, these further imply that LRR can perform robust subspace clustering and error correction in an efficient and effective way. © 1979-2012 IEEE.","Low-rank representation; outlier detection; segmentation; subspace clustering","Convex programs; Data sample; Linear combinations; Low-rank representation; Objective functions; Outlier Detection; Subspace clustering; Clustering algorithms; Error correction; Image segmentation; Recovery; Statistics; Vectors; algorithm; article; artificial intelligence; automated pattern recognition; computer simulation; methodology; signal processing; theoretical model; Algorithms; Artificial Intelligence; Computer Simulation; Models, Theoretical; Pattern Recognition, Automated; Signal Processing, Computer-Assisted",Article,Scopus,2-s2.0-84870197517
"Elhamifar E., Vidal R.","Sparse subspace clustering: Algorithm, theory, and applications",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",490,10.1109/TPAMI.2013.57,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884541998&doi=10.1109%2fTPAMI.2013.57&partnerID=40&md5=c2025a6c2b3569ad85a643a57b4dfc2b","Many real-world problems deal with collections of high-dimensional data, such as images, videos, text, and web documents, DNA microarray data, and more. Often, such high-dimensional data lie close to low-dimensional structures corresponding to several classes or categories to which the data belong. In this paper, we propose and study an algorithm, called sparse subspace clustering, to cluster data points that lie in a union of low-dimensional subspaces. The key idea is that, among the infinitely many possible representations of a data point in terms of other points, a sparse representation corresponds to selecting a few points from the same subspace. This motivates solving a sparse optimization program whose solution is used in a spectral clustering framework to infer the clustering of the data into subspaces. Since solving the sparse optimization program is in general NP-hard, we consider a convex relaxation and show that, under appropriate conditions on the arrangement of the subspaces and the distribution of the data, the proposed minimization program succeeds in recovering the desired sparse representations. The proposed algorithm is efficient and can handle data points near the intersections of subspaces. Another key advantage of the proposed algorithm with respect to the state of the art is that it can deal directly with data nuisances, such as noise, sparse outlying entries, and missing entries, by incorporating the model of the data into the sparse optimization program. We demonstrate the effectiveness of the proposed algorithm through experiments on synthetic data as well as the two real-world problems of motion segmentation and face clustering. © 1979-2012 IEEE.","(ℓ1)-minimization; clustering; convex programming; face clustering; High-dimensional data; intrinsic low-dimensionality; motion segmentation; principal angles; sparse representation; spectral clustering; subspaces","clustering; Face clustering; High dimensional data; Low dimensionality; Motion segmentation; Principal angles; Sparse representation; Spectral clustering; subspaces; Convex programming; Optimization; Relaxation processes; Clustering algorithms; algorithm; anatomy and histology; artificial intelligence; automated pattern recognition; biometry; computer assisted diagnosis; face; human; procedures; sample size; article; automated pattern recognition; biometry; computer assisted diagnosis; face; histology; methodology; Algorithms; Artificial Intelligence; Biometry; Face; Humans; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Sample Size; Algorithms; Artificial Intelligence; Biometry; Face; Humans; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Sample Size",Article,Scopus,2-s2.0-84884541998
"Kosinski M., Stillwell D., Graepel T.","Private traits and attributes are predictable from digital records of human behavior",2013,"Proceedings of the National Academy of Sciences of the United States of America",434,10.1073/pnas.1218772110,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876061994&doi=10.1073%2fpnas.1218772110&partnerID=40&md5=bf2a7e6dde703f281ac3fbc04f24310e","We show that easily accessible digital records of behavior, Facebook Likes, can be used to automatically and accurately predict a range of highly sensitive personal attributes including: sexual orientation, ethnicity, religious and political views, personality traits, intelligence, happiness, use of addictive substances, parental separation, age, and gender. The analysis presented is based on a dataset of over 58,000 volunteers who provided their Facebook Likes, detailed demographic profiles, and the results of several psychometric tests. The proposed model uses dimensionality reduction for preprocessing the Likes data, which are then entered into logistic/linear regression to predict individual psychodemographic profiles from Likes. The model correctly discriminates between homosexual and heterosexual men in 88% of cases, African Americans and Caucasian Americans in 95% of cases, and between Democrat and Republican in 85% of cases. For the personality trait ""Openness,"" prediction accuracy is close to the test-retest accuracy of a standard personality test. We give examples of associations between attributes and Likes and discuss implications for online personalization and privacy.","Big data; Computational social science; Data mining; Machine learning; Psychological assessment; Social networks","accuracy; adult; article; behavior; character; demography; drug dependence; ethnicity; female; happiness; heterosexuality; homosexuality; human; intelligence; major clinical study; male; online analysis; personality; personality test; politics; prediction; priority journal; privacy; psychometry; religion; sexuality; test retest reliability; Artificial Intelligence; Behavior; Data Mining; Emotions; Female; Heterosexuality; Homosexuality; Humans; Male; Models, Theoretical; Personality; Personality Inventory; Politics; Psychometrics; Regression Analysis; Reproducibility of Results; Social Support",Article,Scopus,2-s2.0-84876061994
"Mostaghim S., Teich J.","Strategies for finding good local guides in multi-objective particle swarm optimization (MOPSO)",2013,"2003 IEEE Swarm Intelligence Symposium, SIS 2003 - Proceedings",431,10.1109/SIS.2003.1202243,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942162725&doi=10.1109%2fSIS.2003.1202243&partnerID=40&md5=a1fea9cc7937415fa7ac1558ceb10a03","In multi-objective particle swarm optimization (MOPSO) methods, selecting the best local guide (the global best particle) for each particle of the population from a set of Pareto-optimal solutions has a great impact on the convergence and diversity of solutions, especially when optimizing problems with high number of objectives. This paper introduces the Sigma method as a new method for finding best local guides for each particle of the population. The Sigma method is implemented and is compared with another method, which uses the strategy of an existing MOPSO method for finding the local guides. These methods are examined for different test functions and the results are compared with the results of a multi-objective evolutionary algorithm (MOEA). © 2003 IEEE.","Computational modeling; Evolutionary computation; Iterative methods; Optimization methods; Particle swarm optimization; Search methods; Stochastic processes; Testing","Artificial intelligence; Evolutionary algorithms; Iterative methods; Pareto principle; Particle swarm optimization (PSO); Random processes; Stochastic systems; Testing; Computational model; Diversity of solutions; Multi objective evolutionary algorithms; Multi objective particle swarm optimization; Optimization method; Pareto optimal solutions; Search method; Test functions; Multiobjective optimization",Conference Paper,Scopus,2-s2.0-84942162725
"Higashi N., Iba H.","Particle swarm optimization with Gaussian mutation",2013,"2003 IEEE Swarm Intelligence Symposium, SIS 2003 - Proceedings",391,10.1109/SIS.2003.1202250,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942114552&doi=10.1109%2fSIS.2003.1202250&partnerID=40&md5=93ef01a785918c47348439a7df5ebd10","In this paper we present particle swarm optimization with Gaussian mutation combining the idea of the particle swarm with concepts from evolutionary algorithms. This method combines the traditional velocity and position update rules with the ideas of Gaussian mutation. This model is tested and compared with the standard PSO and standard GA. The comparative experiments have been conducted on unimodal functions and multimodal functions. PSO with Gaussian mutation is able to obtain a result superior to GA. We also apply the PSO with Gaussian mutation to a gene network. Consequently, it has succeeded in acquiring better results than those by GA and PSO alone. © 2003 IEEE.","Biological system modeling; Birds; Evolutionary computation; Genetic mutations; Genetic programming; Particle swarm optimization; Simulated annealing; Testing","Artificial intelligence; Biological systems; Birds; Evolutionary algorithms; Gaussian distribution; Genetic algorithms; Genetic programming; Simulated annealing; Testing; Biological system modeling; Comparative experiments; Gaussian mutation; Genetic mutations; Multi modal function; Particle swarm; Position updates; Unimodal functions; Particle swarm optimization (PSO)",Conference Paper,Scopus,2-s2.0-84942114552
"Kennedy J.","Bare bones particle swarms",2013,"2003 IEEE Swarm Intelligence Symposium, SIS 2003 - Proceedings",371,10.1109/SIS.2003.1202251,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942163347&doi=10.1109%2fSIS.2003.1202251&partnerID=40&md5=ca9fed3d86cf1d3aa72ad76835170e54","The particle swarm algorithm has just enough moving parts to make it hard to understand. The formula is very simple, it is even easy to describe the working of the algorithm verbally, yet it is very difficult to grasp in one's mind how the particles oscillate around centers that are constantly changing; how they influence one another; how the various parameters affect the trajectory of the particle; how the topology of the swarm affects its performance; and so on. This paper strips away some traditional features of the particle swarm in the search for the properties that make it work. The particle swarm algorithm is modified by eliminating the velocity formula. Variations are compared. In the process some of the mysteries of the algorithm are revealed, we discover its similarity to other stochastic population-based problem solving methods, and new avenues of investigation are suggested or implied. © 2003 IEEE.","Bones; Lattices; Particle swarm optimization; Performance evaluation; Problem-solving; Statistics; Stochastic processes; Strips; Testing; Topology","Artificial intelligence; Bone; Crystal lattices; Particle swarm optimization (PSO); Problem solving; Random processes; Statistics; Stochastic systems; Testing; Topology; Moving parts; Particle swarm; Particle swarm algorithm; Performance evaluation; Problem Solving methods; Stochastic population; Strips; Algorithms",Conference Paper,Scopus,2-s2.0-84942163347
"Peram T., Veeramachaneni K., Mohan C.K.","Fitness-distance-ratio based particle swarm optimization",2013,"2003 IEEE Swarm Intelligence Symposium, SIS 2003 - Proceedings",315,10.1109/SIS.2003.1202264,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942155292&doi=10.1109%2fSIS.2003.1202264&partnerID=40&md5=beb07ee71bf00c7a44cf42c0d0f455e2","This paper presents a modification of the particle swarm optimization algorithm (PSO) intended to combat the problem of premature convergence observed in many applications of PSO. The proposed new algorithm moves particles towards nearby particles of higher fitness, instead of attracting each particle towards just the best position discovered so far by any particle. This is accomplished by using the ratio of the relative fitness and the distance of other particles to determine the direction in which each component of the particle position needs to be changed. The resulting algorithm (FDR-PSO) is shown to perform significantly better than the original PSO algorithm and some of its variants, on many different benchmark optimization problems. Empirical examination of the evolution of the particles demonstrates that the convergence of the algorithm does not occur at an early phase of particle evolution, unlike PSO. Avoiding premature convergence allows FDR-PSO to continue search for global optima in difficult multimodal optimization problems. © 2003 IEEE.","Animals; Application software; Cognition; Computer science; Convergence; Evolutionary computation; Particle swarm optimization; Performance analysis; Power engineering and energy; Problem-solving","Algorithms; Animals; Application programs; Artificial intelligence; Computer science; Evolutionary algorithms; Health; Optimization; Problem solving; Application softwares; Cognition; Convergence; Performance analysis; Power engineering and energies; Particle swarm optimization (PSO)",Conference Paper,Scopus,2-s2.0-84942155292
"Cambria E., Schuller B., Xia Y., Havasi C.","New avenues in opinion mining and sentiment analysis",2013,"IEEE Intelligent Systems",286,10.1109/MIS.2013.30,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880219830&doi=10.1109%2fMIS.2013.30&partnerID=40&md5=26b7053580403fbbc6c66037611c4a4a","The distillation of knowledge from the Web&mdash;also known as opinion mining and sentiment analysis&mdash;is a task that has recently raised growing interest for purposes such as customer service, predicting financial markets, monitoring public security, investigating elections, and measuring a health-related quality of life. This article considers past, present, and future trends of sentiment analysis by delving into the evolution of different tools and techniques&mdash;from heuristics to discourse structure, from coarse- to fine-grained analysis, and from keyword- to concept-level opinion mining. © 2001-2011 IEEE.","AI; intelligent systems; NLP; opinion mining; sentiment analysis","Customer services; Discourse structure; Fine-grained analysis; Health-related quality of lives; NLP; Opinion mining; Sentiment analysis; Tools and techniques; Artificial intelligence; Distillation; Intelligent systems; Data mining",Article,Scopus,2-s2.0-84880219830
"Hu X., Eberhart R.C., Shi Y.","Engineering optimization with particle swarm",2013,"2003 IEEE Swarm Intelligence Symposium, SIS 2003 - Proceedings",257,10.1109/SIS.2003.1202247,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942081912&doi=10.1109%2fSIS.2003.1202247&partnerID=40&md5=fb956da2182a17b07b3d6ac28e746519","The paper presents a modified particle swarm optimization (PSO) algorithm for engineering optimization problems with constraints. PSO is started with a group of feasible solutions and a feasibility function is used to check if the newly explored solutions satisfy all the constraints. All the particles keep only those feasible solutions in their memory. Several engineering design optimization problems were tested and the results show that PSO is an efficient and general approach to solve most nonlinear optimization problems with inequity constraints. © 2003 IEEE.","Biomedical engineering; Constraint optimization; Design engineering; Design optimization; Equations; Evolutionary computation; Particle swarm optimization; Power line communications; Stochastic processes; Testing","Algorithms; Artificial intelligence; Biomedical engineering; Constrained optimization; Evolutionary algorithms; Nonlinear programming; Optimization; Random processes; Stochastic systems; Testing; Constraint optimizations; Design Engineering; Design optimization; Equations; Power line communications; Particle swarm optimization (PSO)",Conference Paper,Scopus,2-s2.0-84942081912
"Garg S., Gentry C., Halevi S.","Candidate multilinear maps from ideal lattices",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",241,10.1007/978-3-642-38348-9_1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879875394&doi=10.1007%2f978-3-642-38348-9_1&partnerID=40&md5=6c8f761966236063f0155040832bbbcf","We describe plausible lattice-based constructions with properties that approximate the sought-after multilinear maps in hard-discrete-logarithm groups, and show an example application of such multi-linear maps that can be realized using our approximation. The security of our constructions relies on seemingly hard problems in ideal lattices, which can be viewed as extensions of the assumed hardness of the NTRU function. © 2013 International Association for Cryptologic Research.",,"Hard problems; Lattice-based; Multilinear maps; Artificial intelligence; Computer science; Cryptography",Conference Paper,Scopus,2-s2.0-84879875394
"Liu J., Musialski P., Wonka P., Ye J.","Tensor completion for estimating missing values in visual data",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",229,10.1109/TPAMI.2012.39,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870175618&doi=10.1109%2fTPAMI.2012.39&partnerID=40&md5=523e4e95c02c2b9f9cf6c764a8ab0f84","In this paper, we propose an algorithm to estimate missing values in tensors of visual data. The values can be missing due to problems in the acquisition process or because the user manually identified unwanted outliers. Our algorithm works even with a small amount of samples and it can propagate structure to fill larger missing regions. Our methodology is built on recent studies about matrix completion using the matrix trace norm. The contribution of our paper is to extend the matrix case to the tensor case by proposing the first definition of the trace norm for tensors and then by building a working algorithm. First, we propose a definition for the tensor trace norm that generalizes the established definition of the matrix trace norm. Second, similarly to matrix completion, the tensor completion is formulated as a convex optimization problem. Unfortunately, the straightforward problem extension is significantly harder to solve than the matrix case because of the dependency among multiple constraints. To tackle this problem, we developed three algorithms: simple low rank tensor completion (SiLRTC), fast low rank tensor completion (FaLRTC), and high accuracy low rank tensor completion (HaLRTC). The SiLRTC algorithm is simple to implement and employs a relaxation technique to separate the dependant relationships and uses the block coordinate descent (BCD) method to achieve a globally optimal solution; the FaLRTC algorithm utilizes a smoothing scheme to transform the original nonsmooth problem into a smooth one and can be used to solve a general tensor trace norm minimization problem; the HaLRTC algorithm applies the alternating direction method of multipliers (ADMMs) to our problem. Our experiments show potential applications of our algorithms and the quantitative evaluation indicates that our methods are more accurate and robust than heuristic approaches. The efficiency comparison indicates that FaLTRC and HaLRTC are more efficient than SiLRTC and between FaLRTC and HaLRTC the former is more efficient to obtain a low accuracy solution and the latter is preferred if a high-accuracy solution is desired. © 1979-2012 IEEE.","sparse learning; Tensor completion; trace norm","Acquisition process; Alternating direction methods; Convex optimization problems; Coordinate descent; Heuristic approach; High-accuracy; Low rank; Matrix case; Matrix completion; Matrix trace; Minimization problems; Missing values; Multiple constraint; Non-smooth; Optimal solutions; Potential applications; Quantitative evaluation; Relaxation techniques; sparse learning; Tensor completion; Trace-norms; Visual data; Algorithms; Convex optimization; Heuristic methods; Matrix algebra; Tensors; article; artifact; artificial intelligence; automated pattern recognition; computer assisted diagnosis; image enhancement; methodology; sample size; signal processing; Artifacts; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Sample Size; Signal Processing, Computer-Assisted",Article,Scopus,2-s2.0-84870175618
"Gudise V.G., Venayagamoorthy G.K.","Comparison of particle swarm optimization and backpropagation as training algorithms for neural networks",2013,"2003 IEEE Swarm Intelligence Symposium, SIS 2003 - Proceedings",225,10.1109/SIS.2003.1202255,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942134374&doi=10.1109%2fSIS.2003.1202255&partnerID=40&md5=26030817cffbece689232f1e35a3ebd5","Particle swarm optimization (PSO) motivated by the social behavior of organisms, is a step up to existing evolutionary algorithms for optimization of continuous nonlinear functions. Backpropagation (BP) is generally used for neural network training. Choosing a proper algorithm for training a neural network is very important. In this paper, a comparative study is made on the computational requirements of the PSO and BP as training algorithms for neural networks. Results are presented for a feedforward neural network learning a nonlinear function and these results show that the feedforward neural network weights converge faster with the PSO than with the BP algorithm. © 2003 IEEE.","Artificial neural networks; Backpropagation algorithms; Computer networks; Educational institutions; Feedforward neural networks; Neural networks; Neurons; Particle swarm optimization; Pattern recognition; Venus","Algorithms; Artificial intelligence; Backpropagation; Backpropagation algorithms; Computer networks; Evolutionary algorithms; Functions; Neural networks; Neurons; Particle swarm optimization (PSO); Pattern recognition; Comparative studies; Computational requirements; Continuous nonlinear functions; Educational institutions; Neural network training; Nonlinear functions; Training algorithms; Venus; Feedforward neural networks",Conference Paper,Scopus,2-s2.0-84942134374
"Chaouachi A., Kamel R.M., Andoulsi R., Nagasaka K.","Multiobjective intelligent energy management for a microgrid",2013,"IEEE Transactions on Industrial Electronics",214,10.1109/TIE.2012.2188873,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870550979&doi=10.1109%2fTIE.2012.2188873&partnerID=40&md5=fa6d4d0bfb12583a7d691a6c3c90d59c","In this paper, a generalized formulation for intelligent energy management of a microgrid is proposed using artificial intelligence techniques jointly with linear-programming-based multiobjective optimization. The proposed multiobjective intelligent energy management aims to minimize the operation cost and the environmental impact of a microgrid, taking into account its preoperational variables as future availability of renewable energies and load demand (LD). An artificial neural network ensemble is developed to predict 24-h-ahead photovoltaic generation and 1-h-ahead wind power generation and LD. The proposed machine learning is characterized by enhanced learning model and generalization capability. The efficiency of the microgrid operation strongly depends on the battery scheduling process, which cannot be achieved through conventional optimization formulation. In this paper, a fuzzy logic expert system is used for battery scheduling. The proposed approach can handle uncertainties regarding to the fuzzy environment of the overall microgrid operation and the uncertainty related to the forecasted parameters. The results show considerable minimization on operation cost and emission level compared to literature microgrid energy management approaches based on opportunity charging and Heuristic Flowchart (HF) battery management. © 1982-2012 IEEE.","Fuzzy logic (FL); microgrid; multiobjective intelligent energy management (MIEM); neural network ensemble (NNE); short-term forecasting","Charging (batteries); Electric power generation; Environmental impact; Expert systems; Fuzzy logic; Multiobjective optimization; Neural networks; Operating costs; Scheduling; Artificial intelligence techniques; Artificial neural network ensembles; Battery Management; Conventional optimization; Emission level; Enhanced learning; Fuzzy environments; Generalization capability; Intelligent energy management; Load demand; Micro grid; Multi objective; Neural network ensembles; Operation cost; Photovoltaic generation; Renewable energies; Scheduling process; Short-term forecasting; Energy management",Article,Scopus,2-s2.0-84870550979
"Gao S., Tsang I.W.-H., Chia L.-T.","Laplacian sparse coding, Hypergraph Laplacian sparse coding, and applications",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",190,10.1109/TPAMI.2012.63,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870191664&doi=10.1109%2fTPAMI.2012.63&partnerID=40&md5=c53b42af916ff6da9e9279d8e2675921","Sparse coding exhibits good performance in many computer vision applications. However, due to the overcomplete codebook and the independent coding process, the locality and the similarity among the instances to be encoded are lost. To preserve such locality and similarity information, we propose a Laplacian sparse coding (LSc) framework. By incorporating the similarity preserving term into the objective of sparse coding, our proposed Laplacian sparse coding can alleviate the instability of sparse codes. Furthermore, we propose a Hypergraph Laplacian sparse coding (HLSc), which extends our Laplacian sparse coding to the case where the similarity among the instances defined by a hypergraph. Specifically, this HLSc captures the similarity among the instances within the same hyperedge simultaneously, and also makes the sparse codes of them be similar to each other. Both Laplacian sparse coding and Hypergraph Laplacian sparse coding enhance the robustness of sparse coding. We apply the Laplacian sparse coding to feature quantization in Bag-of-Words image representation, and it outperforms sparse coding and achieves good performance in solving the image classification problem. The Hypergraph Laplacian sparse coding is also successfully used to solve the semi-auto image tagging problem. The good performance of these applications demonstrates the effectiveness of our proposed formulations in locality and similarity preservation. © 1979-2012 IEEE.","hypergraph Laplacian sparse coding; image classification; Laplacian sparse coding; locality preserving; semi-auto image tagging","A-Laplacian; Bag of words; Codebooks; Coding process; Computer vision applications; Hypergraph; Image representations; Laplacians; locality preserving; Over-complete; semi-auto image tagging; Sparse codes; Sparse coding; Tagging problem; Computer vision; Image classification; Image coding; Laplace transforms; Graph theory; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; decision support system; information processing; methodology; Algorithms; Artificial Intelligence; Data Compression; Decision Support Techniques; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84870191664
"Demirkan H., Delen D.","Leveraging the capabilities of service-oriented decision support systems: Putting analytics and big data in cloud",2013,"Decision Support Systems",179,10.1016/j.dss.2012.05.048,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877771183&doi=10.1016%2fj.dss.2012.05.048&partnerID=40&md5=691078fabfafadfff9b35fab39a60014","Using service-oriented decision support systems (DSS in cloud) is one of the major trends for many organizations in hopes of becoming more agile. In this paper, after defining a list of requirements for service-oriented DSS, we propose a conceptual framework for DSS in cloud, and discus about research directions. A unique contribution of this paper is its perspective on how to servitize the product oriented DSS environment, and demonstrate the opportunities and challenges of engineering service oriented DSS in cloud. When we define data, information and analytics as services, we see that traditional measurement mechanisms, which are mainly time and cost driven, do not work well. Organizations need to consider value of service level and quality in addition to the cost and duration of delivered services. DSS in CLOUD enables scale, scope and speed economies. This article contributes new knowledge in service science by tying the information technology strategy perspectives to the database and design science perspectives for a broader audience.","Analytics-as-a-service; Big data; Cloud computing; Data-as-a-service; Information-as-a-service; Service orientation; Service science","Analytics-as-a-service; Big datum; Data-as-a-service; Information-as-a-service; Service orientation; Service science; Artificial intelligence; Cloud computing; Decision support systems; Design; Information technology; Digital storage",Article,Scopus,2-s2.0-84877771183
"Mohammad S.M., Turney P.D.","Crowdsourcing a word-emotion association lexicon",2013,"Computational Intelligence",170,10.1111/j.1467-8640.2012.00460.x,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881408290&doi=10.1111%2fj.1467-8640.2012.00460.x&partnerID=40&md5=2000572e3ec305111de8494538a1bb1e","Even though considerable attention has been given to the polarity of words (positive and negative) and the creation of large polarity lexicons, research in emotion analysis has had to rely on limited and small emotion lexicons. In this paper, we show how the combined strength and wisdom of the crowds can be used to generate a large, high-quality, word-emotion and word-polarity association lexicon quickly and inexpensively. We enumerate the challenges in emotion annotation in a crowdsourcing scenario and propose solutions to address them. Most notably, in addition to questions about emotions associated with terms, we show how the inclusion of a word choice question can discourage malicious data entry, help to identify instances where the annotator may not be familiar with the target term (allowing us to reject such annotations), and help to obtain annotations at sense level (rather than at word level). We conducted experiments on how to formulate the emotion-annotation questions, and show that asking if a term is associated with an emotion leads to markedly higher interannotator agreement than that obtained by asking if a term evokes an emotion. © 2012 National Research Council Canada.","affect; crowdsourcing; emotion lexicon; emotions; Mechanical Turk; polarity; polarity lexicon; semantic orientation; sentiment analysis; word-emotion associations","affect; Crowdsourcing; emotion lexicon; emotions; Mechanical turks; polarity; polarity lexicon; Semantic orientation; Sentiment analysis; Artificial intelligence; Computational methods; Semantics",Conference Paper,Scopus,2-s2.0-84881408290
"Hu X., Eberhart R.C., Shi Y.","Particle swarm with extended memory for multiobjective optimization",2013,"2003 IEEE Swarm Intelligence Symposium, SIS 2003 - Proceedings",161,10.1109/SIS.2003.1202267,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942099370&doi=10.1109%2fSIS.2003.1202267&partnerID=40&md5=8b8a8107e10a1c2744d305a62b685ce1","This paper presents a modified dynamic neighborhood particle swarm optimization (DNPSO) algorithm for multiobjective optimization problems. PSO is modified by using a dynamic neighborhood strategy, new particle memory updating, and one-dimension optimization to deal with multiple objectives. An extended memory is introduced to store global Pareto optimal solutions to reduce computation time. Several benchmark cases were tested and the results show that the modified DNPSO is much more efficient than the original DNPSO and other multiobjective optimization techniques. © 2003 IEEE.","Benchmark testing; Biomedical engineering; Equations; Evolutionary computation; Optimization methods; Particle swarm optimization; Random number generation; Stochastic processes","Algorithms; Artificial intelligence; Biomedical engineering; Evolutionary algorithms; Optimization; Pareto principle; Particle swarm optimization (PSO); Random number generation; Random processes; Stochastic systems; Benchmark testing; Dynamic neighborhood; Equations; Multi-objective optimization problem; Multi-objective optimization techniques; Multiple-objectives; Optimization method; Pareto optimal solutions; Multiobjective optimization",Conference Paper,Scopus,2-s2.0-84942099370
"Birge B.","PSOt - A particle swarm optimization toolbox for use with Matlab",2013,"2003 IEEE Swarm Intelligence Symposium, SIS 2003 - Proceedings",155,10.1109/SIS.2003.1202265,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942083352&doi=10.1109%2fSIS.2003.1202265&partnerID=40&md5=a4ea95cc83a6cfa8950ecfdd460ed9bd","A particle swarm optimization toolbox (PSOt) for use with the Matlab scientific programming environment has been developed. PSO is introduced briefly and then the use of the toolbox is explained with some examples. A link to downloadable code is provided. © 2003 IEEE.","Artificial neural networks; Computational intelligence; Control systems; Genetic algorithms; MATLAB; Matrix converters; Neural networks; Particle swarm optimization; Power engineering and energy; Programming environments","Artificial intelligence; Control systems; Genetic algorithms; MATLAB; Matrix converters; Neural networks; A-particles; Power engineering and energies; Programming environment; Scientific programming; Particle swarm optimization (PSO)",Conference Paper,Scopus,2-s2.0-84942083352
"Xue B., Zhang M., Browne W.N.","Particle swarm optimization for feature selection in classification: A multi-objective approach",2013,"IEEE Transactions on Cybernetics",154,10.1109/TSMCB.2012.2227469,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887799080&doi=10.1109%2fTSMCB.2012.2227469&partnerID=40&md5=77e7aaf2d80ac386fa7702f464f25d81","Classification problems often have a large number of features in the data sets, but not all of them are useful for classification. Irrelevant and redundant features may even reduce the performance. Feature selection aims to choose a small number of relevant features to achieve similar or even better classification performance than using all features. It has two main conflicting objectives of maximizing the classification performance and minimizing the number of features. However, most existing feature selection algorithms treat the task as a single objective problem. This paper presents the first study on multi-objective particle swarm optimization (PSO) for feature selection. The task is to generate a Pareto front of nondominated solutions (feature subsets). We investigate two PSO-based multi-objective feature selection algorithms. The first algorithm introduces the idea of nondominated sorting into PSO to address feature selection problems. The second algorithm applies the ideas of crowding, mutation, and dominance to PSO to search for the Pareto front solutions. The two multi-objective algorithms are compared with two conventional feature selection methods, a single objective feature selection method, a two-stage feature selection algorithm, and three well-known evolutionary multi-objective algorithms on 12 benchmark data sets. The experimental results show that the two PSO-based multi-objective algorithms can automatically evolve a set of nondominated solutions. The first algorithm outperforms the two conventional methods, the single objective method, and the two-stage algorithm. It achieves comparable results with the existing three well-known multi-objective algorithms in most cases. The second algorithm achieves better results than the first algorithm and all other methods mentioned previously. © 2013 IEEE.","Feature selection; multi-objective optimization; particle swarm optimization (PSO)","Classification performance; Evolutionary multi-objective algorithms; Feature selection algorithm; Feature selection methods; Feature selection problem; Minimizing the number of; Multi objective algorithm; Multi objective particle swarm optimization; Classification (of information); Feature extraction; Multiobjective optimization; Particle swarm optimization (PSO); Algorithms; algorithm; article; artificial intelligence; automated pattern recognition; decision support system; information retrieval; methodology; Algorithms; Artificial Intelligence; Decision Support Techniques; Information Storage and Retrieval; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84887799080
"Lu J., Tan Y.-P., Wang G.","Discriminative multimanifold analysis for face recognition from a single training sample per person",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",154,10.1109/TPAMI.2012.70,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870217050&doi=10.1109%2fTPAMI.2012.70&partnerID=40&md5=1abccf663757e36e3f1df0dc03c5c7c2","Conventional appearance-based face recognition methods usually assume that there are multiple samples per person (MSPP) available for discriminative feature extraction during the training phase. In many practical face recognition applications such as law enhancement, e-passport, and ID card identification, this assumption, however, may not hold as there is only a single sample per person (SSPP) enrolled or recorded in these systems. Many popular face recognition methods fail to work well in this scenario because there are not enough samples for discriminant learning. To address this problem, we propose in this paper a novel discriminative multimanifold analysis (DMMA) method by learning discriminative features from image patches. First, we partition each enrolled face image into several nonoverlapping patches to form an image set for each sample per person. Then, we formulate the SSPP face recognition as a manifold-manifold matching problem and learn multiple DMMA feature spaces to maximize the manifold margins of different persons. Finally, we present a reconstruction-based manifold-manifold distance to identify the unlabeled subjects. Experimental results on three widely used face databases are presented to demonstrate the efficacy of the proposed approach. © 1979-2012 IEEE.","Face recognition; manifold learning; single training sample per person; subspace learning","Appearance based; Discriminative feature extraction; Discriminative features; E-passport; Face database; Face images; Face recognition methods; Feature space; ID cards; Image patches; Image sets; Manifold learning; Matching problems; Multiple samples; Nonoverlapping; Single sample; Single training sample; Subspace learning; Training phase; Feature extraction; Sampling; Face recognition; algorithm; article; artificial intelligence; automated pattern recognition; biometry; computer assisted diagnosis; discriminant analysis; face; histology; human; image subtraction; methodology; sample size; signal processing; Algorithms; Artificial Intelligence; Biometry; Discriminant Analysis; Face; Humans; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Sample Size; Signal Processing, Computer-Assisted; Subtraction Technique",Article,Scopus,2-s2.0-84870217050
"Dissanayake S.D., Armstrong J.","Comparison of ACO-OFDM, DCO-OFDM and ADO-OFDM in IM/DD systems",2013,"Journal of Lightwave Technology",153,10.1109/JLT.2013.2241731,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873654795&doi=10.1109%2fJLT.2013.2241731&partnerID=40&md5=46865a224887033a35416bcbf743536b","In this paper, three forms of orthogonal frequency division multiplexing (OFDM) designed for intensity modulated/direct detection (IM/DD) optical systems are compared. These are asymmetrically clipped optical OFDM (ACO-OFDM), DC biased optical OFDM (DCO-OFDM) and asymmetrically clipped DC biased optical OFDM (ADO-OFDM). ADO-OFDM is a new technique that combines aspects of ACO-OFDM and DCO-OFDM by simultaneously transmitting ACO-OFDM on the odd subcarriers and DCO-OFDM on the even subcarriers. The odd subcarriers are demodulated as in a conventional ACO-OFDM receiver and the even subcarriers are demodulated using a form of interference cancellation. ADO-OFDM is shown to be more optically power efficient than conventional ACO-OFDM and DCO-OFDM, for some bit rate/normalized bandwidths. It is also shown that by varying the proportion of optical power on the ACO-OFDM component, the DC bias level of DCO-OFDM and the constellations sent on the odd and even subcarriers, the optical power efficiency of ADO-OFDM can be changed. © 1983-2012 IEEE.","ACO-OFDM; ADO-OFDM; DCO-OFDM; IM/DD; optical systems","ACO-OFDM; ADO-OFDM; Asymmetrically clipped Optical OFDM; Bit rates; DC bias; DC biased; DCO-OFDM; IM/DD; Intensity-modulated; Interference cancellation; Optical OFDM; Optical power; Power efficient; Sub-carriers; Artificial intelligence; Optical fiber communication; Optical systems; Orthogonal frequency division multiplexing",Article,Scopus,2-s2.0-84873654795
"Hosni A., Rhemann C., Bleyer M., Rother C., Gelautz M.","Fast cost-volume filtering for visual correspondence and beyond",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",147,10.1109/TPAMI.2012.156,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871808947&doi=10.1109%2fTPAMI.2012.156&partnerID=40&md5=477f6525acd077edb00e195346d5c91b","Many computer vision tasks can be formulated as labeling problems. The desired solution is often a spatially smooth labeling where label transitions are aligned with color edges of the input image. We show that such solutions can be efficiently achieved by smoothing the label costs with a very fast edge-preserving filter. In this paper, we propose a generic and simple framework comprising three steps: 1) constructing a cost volume, 2) fast cost volume filtering, and 3) Winner-Takes-All label selection. Our main contribution is to show that with such a simple framework state-of-the-art results can be achieved for several computer vision applications. In particular, we achieve 1) disparity maps in real time whose quality exceeds those of all other fast (local) approaches on the Middlebury stereo benchmark, and 2) optical flow fields which contain very fine structures as well as large displacements. To demonstrate robustness, the few parameters of our framework are set to nearly identical values for both applications. Also, competitive results for interactive image segmentation are presented. With this work, we hope to inspire other researchers to leverage this framework to other application areas. © 2012 IEEE.","interactive image segmentation; optical flow; Stereo matching","Application area; Color edges; Computer vision applications; Disparity map; Edge-preserving filter; Fine structures; Input image; Interactive image segmentation; Large displacements; Real time; Stereo matching; Computer vision; Costs; Image segmentation; Optical flows; Benchmarking; algorithm; article; artificial intelligence; automated pattern recognition; colorimetry; computer assisted diagnosis; image enhancement; methodology; reproducibility; sensitivity and specificity; Algorithms; Artificial Intelligence; Colorimetry; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84871808947
"Coron J.-S., Lepoint T., Tibouchi M.","Practical multilinear maps over the integers",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",138,10.1007/978-3-642-40041-4_26,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884473382&doi=10.1007%2f978-3-642-40041-4_26&partnerID=40&md5=9b91494e0aaf26cc275c24e70f51698c","Extending bilinear elliptic curve pairings to multilinear maps is a long-standing open problem. The first plausible construction of such multilinear maps has recently been described by Garg, Gentry and Halevi, based on ideal lattices. In this paper we describe a different construction that works over the integers instead of ideal lattices, similar to the DGHV fully homomorphic encryption scheme. We also describe a different technique for proving the full randomization of encodings: instead of Gaussian linear sums, we apply the classical leftover hash lemma over a quotient lattice. We show that our construction is relatively practical: for reasonable security parameters a one-round 7-party Diffie-Hellman key exchange requires less than 40 seconds per party. Moreover, in contrast with previous work, multilinear analogues of useful, base group assumptions like DLIN appear to hold in our setting. © 2013 International Association for Cryptologic Research.",,"Diffie-Hellman key exchange; Elliptic curve pairing; Encodings; Fully homomorphic encryption schemes; Gaussians; Leftover hash lemma; Multilinear maps; Security parameters; Artificial intelligence; Computer science; Cryptography",Conference Paper,Scopus,2-s2.0-84884473382
"Peer E.S., Van Den Bergh F., Engelbrecht A.P.","Using neighbourhoods with the guaranteed convergence PSO",2013,"2003 IEEE Swarm Intelligence Symposium, SIS 2003 - Proceedings",133,10.1109/SIS.2003.1202274,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942080360&doi=10.1109%2fSIS.2003.1202274&partnerID=40&md5=df659bb6ff67149e5bf4e5194a4e5f6c","The standard particle swarm optimiser (PSO) may prematurely converge on suboptimal solutions that are not even guaranteed to be local extrema. The guaranteed convergence modifications to the PSO algorithm ensure that the PSO at least converges on a local extremum at the expense of even faster convergence. This faster convergence means that less of the search space is explored reducing the opportunity of the swarm to find better local extrema. Various neighbourhood topologies inhibit premature convergence by preserving swarm diversity during the search. This paper investigates the performance of the guaranteed convergence PSO (GCPSO) using different neighbourhood topologies and compares the results with their standard PSO counterparts. © 2003 IEEE.","Acceleration; Africa; Augmented virtuality; Convergence; Equations; Iterative algorithms; Particle swarm optimization; Space exploration; Testing; Topology","Acceleration; Algorithms; Artificial intelligence; Iterative methods; Space research; Testing; Topology; Africa; Augmented virtualities; Convergence; Equations; Iterative algorithm; Space explorations; Particle swarm optimization (PSO)",Conference Paper,Scopus,2-s2.0-84942080360
"Hussein M.E., Torki M., Gowayyed M.A., El-Saban M.","Human action recognition using a temporal hierarchy of covariance descriptors on 3D joint locations",2013,"IJCAI International Joint Conference on Artificial Intelligence",129,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896061825&partnerID=40&md5=bf7d2359ac27626564292fa3c3f384f6","Human action recognition from videos is a challenging machine vision task with multiple important application domains, such as humanrobot/ machine interaction, interactive entertainment, multimedia information retrieval, and surveillance. In this paper, we present a novel approach to human action recognition from 3D skeleton sequences extracted from depth data. We use the covariance matrix for skeleton joint locations over time as a discriminative descriptor for a sequence. To encode the relationship between joint movement and time, we deploy multiple covariance matrices over sub-sequences in a hierarchical fashion. The descriptor has a fixed length that is independent from the length of the described sequence. Our experiments show that using the covariance descriptor with an off-the-shelf classification algorithm outperforms the state of the art in action recognition on multiple datasets, captured either via a Kinect-type sensor or a sophisticated motion capture system. We also include an evaluation on a novel large dataset using our own annotation.",,"Action recognition; Classification algorithm; Covariance descriptor; Covariance matrices; Human-action recognition; Interactive entertainment; Motion capture system; Multimedia information retrieval; Artificial intelligence; Classification (of information); Computer vision; Covariance matrix; Gesture recognition; Musculoskeletal system; Security systems; Three dimensional; Motion estimation",Conference Paper,Scopus,2-s2.0-84896061825
"Hu X., Eberhart R.C., Shi Y.","Swarm intelligence for permutation optimization: A case study of n-queens problem",2013,"2003 IEEE Swarm Intelligence Symposium, SIS 2003 - Proceedings",129,10.1109/SIS.2003.1202275,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942097672&doi=10.1109%2fSIS.2003.1202275&partnerID=40&md5=b520d9774d8b2a3fb1c56c7dc7ab9d63","This paper introduces a modified particle swarm optimizer which deals with permutation problems. Particles are defined as permutations of a group of unique values. Velocity updates are redefined based on the similarity of two particles. Particles change their permutations with a random rate defined by their velocities. A mutation factor is introduced to prevent the current pBest from becoming stuck at local minima. Preliminary study on the n-queens problem shows that the modified PSO is promising in solving constraint satisfaction problems. © 2003 IEEE.","Artificial intelligence; Artificial neural networks; Benchmark testing; Biomedical engineering; Computer aided software engineering; Concurrent computing; Genetic algorithms; Genetic mutations; Optical computing; Particle swarm optimization","Artificial intelligence; Biomedical engineering; Computer aided software engineering; Constraint satisfaction problems; Evolutionary algorithms; Genetic algorithms; Neural networks; Optical data processing; Optimization; Problem solving; Software engineering; Software testing; Benchmark testing; Concurrent computing; Genetic mutations; Modified particle swarm optimizer; Mutation factor; N-queens problems; Permutation problems; Swarm Intelligence; Particle swarm optimization (PSO)",Conference Paper,Scopus,2-s2.0-84942097672
"Yang A.Y., Zhou Z., Balasubramanian A.G., Sastry S.S., Ma Y.","Fast ℓ1-minimization algorithms for robust face recognition",2013,"IEEE Transactions on Image Processing",128,10.1109/TIP.2013.2262292,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879044578&doi=10.1109%2fTIP.2013.2262292&partnerID=40&md5=3319009ec16ee1a8836f54019bd2ca45","ℓ1-minimization refers to finding the minimum ℓ1-norm solution to an underdetermined linear system b=Ax. Under certain conditions as described in compressive sensing theory, the minimum ℓ1-norm solution is also the sparsest solution. In this paper, we study the speed and scalability of its algorithms. In particular, we focus on the numerical implementation of a sparsity-based classification framework in robust face recognition, where sparse representation is sought to recover human identities from high-dimensional facial images that may be corrupted by illumination, facial disguise, and pose variation. Although the underlying numerical problem is a linear program, traditional algorithms are known to suffer poor scalability for large-scale applications. We investigate a new solution based on a classical convex optimization framework, known as augmented Lagrangian methods. We conduct extensive experiments to validate and compare its performance against several popular ℓ1-minimization solvers, including interior-point method, Homotopy, FISTA, SESOP-PCD, approximate message passing, and TFOCS. To aid peer evaluation, the code for all the algorithms has been made publicly available. © 1992-2012 IEEE.","ℓ1-minimization; augmented Lagrangian methods; face recognition","Augmented Lagrangian methods; Classification framework; Interior-point method; Large-scale applications; Minimization algorithms; Numerical implementation; Optimization framework; Underdetermined linear systems; Algorithms; Convex optimization; Linear systems; Scalability; Face recognition; algorithm; anatomy and histology; artificial intelligence; automated pattern recognition; computer assisted diagnosis; face; human; image enhancement; image subtraction; procedures; reproducibility; robotics; sensitivity and specificity; article; automated pattern recognition; computer assisted diagnosis; face; histology; methodology; robotics; Algorithms; Artificial Intelligence; Face; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Robotics; Sensitivity and Specificity; Subtraction Technique; Algorithms; Artificial Intelligence; Face; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Robotics; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-84879044578
"Thornton C., Hutter F., Hoos H.H., Leyton-Brown K.","Auto-WEKA: Combined selection and hyperparameter optimization of classification algorithms",2013,"Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",125,10.1145/2487575.2487629,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018371540&doi=10.1145%2f2487575.2487629&partnerID=40&md5=d49eb38a8848ae4595d0b2edb12fd8d1","Many different machine learning algorithms exist; taking into account each algorithm's hyperparameters, there is a staggeringly large number of possible alternatives overall. We consider the problem of simultaneously selecting a learning algorithm and setting its hyperparameters, going beyond previous work that attacks these issues separately. We show that this problem can be addressed by a fully automated approach, leveraging recent innovations in Bayesian optimization. Specifically, we consider a wide range of feature selection techniques (combining 3 search and 8 evaluator methods) and all classification approaches implemented in WEKA's standard distribution, spanning 2 ensemble methods, 10 meta-methods, 27 base classifiers, and hyperparameter settings for each classifier. On each of 21 popular datasets from the UCI repository, the KDD Cup 09, variants of the MNIST dataset and CIFAR-10, we show classification performance often much better than using standard selection and hyperparameter optimization methods. We hope that our approach will help non-expert users to more effectively identify machine learning algorithms and hyperparameter settings appropriate to their applications, and hence to achieve improved performance. Copyright © 2013 ACM.","Hyperparameter optimization; Model selection; Weka","Artificial intelligence; Classification (of information); Data mining; Education; Learning systems; Optimization; Bayesian optimization; Classification algorithm; Classification approach; Classification performance; Hyper-parameter optimizations; Model Selection; Standard distributions; Weka; Learning algorithms",Conference Paper,Scopus,2-s2.0-85018371540
"Wang H., Chen B., Liu X., Liu K., Lin C.","Robust adaptive fuzzy tracking control for pure-feedback stochastic nonlinear systems with input constraints",2013,"IEEE Transactions on Cybernetics",123,10.1109/TCYB.2013.2240296,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890040569&doi=10.1109%2fTCYB.2013.2240296&partnerID=40&md5=506fc8c625497e590ba590d00a37f3d2","This paper is concerned with the problem of adaptive fuzzy tracking control for a class of pure-feedback stochastic nonlinear systems with input saturation. To overcome the design difficulty from nondifferential saturation nonlinearity, a smooth nonlinear function of the control input signal is first introduced to approximate the saturation function; then, an adaptive fuzzy tracking controller based on the mean-value theorem is constructed by using backstepping technique. The proposed adaptive fuzzy controller guarantees that all signals in the closed-loop system are bounded in probability and the system output eventually converges to a small neighborhood of the desired reference signal in the sense of mean quartic value. Simulation results further illustrate the effectiveness of the proposed control scheme. © 2013 IEEE.","Adaptive fuzzy control; Backstepping; Nonsymmetric input saturation; Pure-feedback nonlinear stochastic systems","Adaptive fuzzy control; Adaptive fuzzy controller; Backstepping technique; Bounded in probabilities; Input saturation; Non-linear stochastic systems; Saturation nonlinearity; Stochastic nonlinear systems; Backstepping; Fuzzy control; Stochastic control systems; Nonlinear feedback; algorithm; article; artificial intelligence; automated pattern recognition; decision support system; feedback system; fuzzy logic; methodology; nonlinear system; statistics; Algorithms; Artificial Intelligence; Decision Support Techniques; Feedback; Fuzzy Logic; Nonlinear Dynamics; Pattern Recognition, Automated; Stochastic Processes",Article,Scopus,2-s2.0-84890040569
"Cimatti A., Griggio A., Schaafsma B.J., Sebastiani R.","The MathSAT5 SMT solver",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",121,10.1007/978-3-642-36742-7_7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874415102&doi=10.1007%2f978-3-642-36742-7_7&partnerID=40&md5=29f63ed016b9a284d7e00c93994dcf3f","MathSAT is a long-term project, which has been jointly carried on by FBK-IRST and University of Trento, with the aim of developing and maintaining a state-of-the-art SMT tool for formal verification (and other applications). MathSAT5 is the latest version of the tool. It supports most of the SMT-LIB theories and their combinations, and provides many functionalities (like e.g. unsat cores, interpolation, AllSMT). MathSAT5 improves its predecessor MathSAT4 in many ways, also providing novel features: first, a much improved incrementality support, which is vital in SMT applications; second, a full support for the theories of arrays and floating point; third, sound SAT-style Boolean formula preprocessing for SMT formulae; finally, a framework allowing users for plugging their custom tuned SAT solvers. MathSAT5 is freely available, and it is used in numerous internal projects, as well as by a number of industrial partners. © 2013 Springer-Verlag.",,"Boolean formulae; Floating points; Formal verifications; Industrial partners; Internal project; It supports; Long-term projects; SAT solvers; Smt libs; Smt solvers; University of Trento; Artificial intelligence; Algorithms",Conference Paper,Scopus,2-s2.0-84874415102
"Akay B.","A study on particle swarm optimization and artificial bee colony algorithms for multilevel thresholding",2013,"Applied Soft Computing Journal",117,10.1016/j.asoc.2012.03.072,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881609326&doi=10.1016%2fj.asoc.2012.03.072&partnerID=40&md5=e6201439cb45aa38acb5afa13e5d93a2","Segmentation is a critical task in image processing. Bi-level segmentation involves dividing the whole image into partitions based on a threshold value, whereas multilevel segmentation involves multiple threshold values. A successful segmentation assigns proper threshold values to optimise a criterion such as entropy or between-class variance. High computational cost and inefficiency of an exhaustive search for the optimal thresholds leads to the use of global search heuristics to set the optimal thresholds. An emerging area in global heuristics is swarm-intelligence, which models the collective behaviour of the organisms. In this paper, two successful swarm-intelligence-based global optimisation algorithms, particle swarm optimisation (PSO) and artificial bee colony (ABC), have been employed to find the optimal multilevel thresholds. Kapur's entropy, one of the maximum entropy techniques, and between-class variance have been investigated as fitness functions. Experiments have been performed on test images using various numbers of thresholds. The results were assessed using statistical tools and suggest that Otsu's technique, PSO and ABC show equal performance when the number of thresholds is two, while the ABC algorithm performs better than PSO and Otsu's technique when the number of thresholds is greater than two. Experiments based on Kapur's entropy indicate that the ABC algorithm can be efficiently used in multilevel thresholding. Moreover, segmentation methods are required to have a minimum running time in addition to high performance. Therefore, the CPU times of ABC and PSO have been investigated to check their validity in real-time. The CPU time results show that the algorithms are scalable and that the running times of the algorithms seem to grow at a linear rate as the problem size increases. © 2012 Elsevier B.V. All rights reserved.","Artificial bee colony; Between-class variance; Image segmentation; Kapur's entropy; Multilevel thresholding; Particle swarm optimization","Artificial intelligence; Entropy; Evolutionary algorithms; Global optimization; Heuristic algorithms; Image processing; Image segmentation; Maximum entropy methods; Particle swarm optimization (PSO); Statistical mechanics; Swarm intelligence; Artificial bee colonies; Artificial bee colonies (ABC); Artificial bee colony algorithms; Between-class variances; Collective behaviour; Multilevel segmentation; Multilevel thresholding; Particle swarm optimisation; Optimization",Article,Scopus,2-s2.0-84881609326
"Gao W.-F., Liu S.-Y., Huang L.-L.","A novel artificial bee colony algorithm based on modified search equation and orthogonal learning",2013,"IEEE Transactions on Cybernetics",116,10.1109/TSMCB.2012.2222373,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883744292&doi=10.1109%2fTSMCB.2012.2222373&partnerID=40&md5=a09571aeb12c4bdf8219eeb64a3aad3b","The artificial bee colony (ABC) algorithm is a relatively new optimization technique which has been shown to be competitive to other population-based algorithms. However, ABC has an insufficiency regarding its solution search equation, which is good at exploration but poor at exploitation. To address this concerning issue, we first propose an improved ABC method called as CABC where a modified search equation is applied to generate a candidate solution to improve the search ability of ABC. Furthermore, we use the orthogonal experimental design (OED) to form an orthogonal learning (OL) strategy for variant ABCs to discover more useful information from the search experiences. Owing to OED's good character of sampling a small number of well representative combinations for testing, the OL strategy can construct a more promising and efficient candidate solution. In this paper, the OL strategy is applied to three versions of ABC, i.e., the standard ABC, global-best-guided ABC (GABC), and CABC, which yields OABC, OGABC, and OCABC, respectively. The experimental results on a set of 22 benchmark functions demonstrate the effectiveness and efficiency of the modified search equation and the OL strategy. The comparisons with some other ABCs and several state-of-the-art algorithms show that the proposed algorithms significantly improve the performance of ABC. Moreover, OCABC offers the highest solution quality, fastest global convergence, and strongest robustness among all the contenders on almost all the test functions. © 2012 IEEE.","Artificial bee colony (ABC) algorithm; Orthogonal experimental design (OED); Orthogonal learning (OL); Search equation","Artificial bee colony algorithms; Artificial bee colony algorithms (ABC); Effectiveness and efficiencies; Orthogonal experimental design; Orthogonal learning (OL); Population-based algorithm; Search equation; State-of-the-art algorithms; Statistics; Evolutionary algorithms; algorithm; animal; animal behavior; article; artificial intelligence; automated pattern recognition; bee; biomimetics; human; methodology; physiology; animal behavior; automated pattern recognition; bee; biomimetics; physiology; procedures; Algorithms; Animals; Artificial Intelligence; Bees; Behavior, Animal; Biomimetics; Humans; Pattern Recognition, Automated; Algorithms; Animals; Artificial Intelligence; Bees; Behavior, Animal; Biomimetics; Humans; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84883744292
"Mohabatkar H., Beigi M.M., Abdolahi K., Mohsenzadeh S.","Prediction of allergenic proteins by means of the concept of Chou's pseudo amino acid composition and a machine learning approach",2013,"Medicinal Chemistry",116,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873572573&partnerID=40&md5=125a773e5d460b8850b152ed7365e556","Because of the importance of proteins in inducing allergenic reactions, the ability of predicting their potential allergenicity has become an important issue. Bioinformatics presents valuable tools for analyzing allergens and these complementary approaches can help traditional techniques to study allergens. This work proposes a computational method for predicting the allergenic proteins. The prediction was performed using pseudo-amino acid composition (PseAAC) and Support Vector Machines (SVMs). The predictor efficiency was evaluated by fivefold cross-validation. The overall prediction accuracies and Matthew's correlation coefficient (MCC) obtained by this method were 91.19% and 0.82, respectively. Furthermore, the minimum Redundancy and Maximum Relevance (mRMR) feature selection method was utilized for measuring the effect and power of each feature. Interestingly, in our study all six characters (hydrophobicity, hydrophilicity, side chain mass, pK1, pK2 and pI) are present among the 10 higher ranked features obtained from the mRMR feature selection method. © 2013 Bentham Science Publishers.","Allergenic proteins; Bioinformatics; Chou's Pseudo amino acid composition; Support Vector Machines","allergen; accuracy; allergenicity; amino acid composition; article; bioinformatics; classification algorithm; hydrophilicity; hydrophobicity; machine learning; predictive value; priority journal; productivity; structure activity relation; support vector machine; Allergens; Amino Acids; Artificial Intelligence; Computational Biology",Article,Scopus,2-s2.0-84873572573
"Bagstad K.J., Johnson G.W., Voigt B., Villa F.","Spatial dynamics of ecosystem service flows: A comprehensive approach to quantifying actual services",2013,"Ecosystem Services",113,10.1016/j.ecoser.2012.07.012,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879424766&doi=10.1016%2fj.ecoser.2012.07.012&partnerID=40&md5=b3eff83036cdebf48802416d735ff7ba","Recent ecosystem services research has highlighted the importance of spatial connectivity between ecosystems and their beneficiaries. Despite this need, a systematic approach to ecosystem service flow quantification has not yet emerged. In this article, we present such an approach, which we formalize as a class of agent-based models termed ""Service Path Attribution Networks"" (SPANs). These models, developed as part of the Artificial Intelligence for Ecosystem Services (ARIES) project, expand on ecosystem services classification terminology introduced by other authors. Conceptual elements needed to support flow modeling include a service's rivalness, its flow routing type (e.g., through hydrologic or transportation networks, lines of sight, or other approaches), and whether the benefit is supplied by an ecosystem's provision of a beneficial flow to people or by absorption of a detrimental flow before it reaches them. We describe our implementation of the SPAN framework for five ecosystem services and discuss how to generalize the approach to additional services. SPAN model outputs include maps of ecosystem service provision, use, depletion, and flows under theoretical, possible, actual, inaccessible, and blocked conditions. We highlight how these different ecosystem service flow maps could be used to support various types of decision making for conservation and resource management planning. © 2013.","Artificial Intelligence for Ecosystem Services (ARIES); Beneficiaries; Ecosystem services; Service Path Attribution Network (SPAN); Spatial flows",,Article,Scopus,2-s2.0-84879424766
"Li W., Wang X.","Locally aligned feature transforms across views",2013,"Proceedings of the IEEE Computer Society Conference on Computer  Vision and Pattern Recognition",112,10.1109/CVPR.2013.461,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887369602&doi=10.1109%2fCVPR.2013.461&partnerID=40&md5=668f18410bf675ce42e5e3a70e26cd1c","In this paper, we propose a new approach for matching images observed in different camera views with complex cross-view transforms and apply it to person re-identification. It jointly partitions the image spaces of two camera views into different configurations according to the similarity of cross-view transforms. The visual features of an image pair from different views are first locally aligned by being projected to a common feature space and then matched with softly assigned metrics which are locally optimized. The features optimal for recognizing identities are different from those for clustering cross-view transforms. They are jointly learned by utilizing sparsity-inducing norm and information theoretical regularization. This approach can be generalized to the settings where test images are from new camera views, not the same as those in the training set. Extensive experiments are conducted on public datasets and our own dataset. Comparisons with the state-of-the-art metric learning and person re-identification methods show the superior performance of our approach. © 2013 IEEE.",,"Common features; Feature transform; Image space; Metric learning; New approaches; Person re identifications; Training sets; Visual feature; Artificial intelligence; Optimization; Pattern recognition; Cameras",Conference Paper,Scopus,2-s2.0-84887369602
"Maji S., Berg A.C., Malik J.","Efficient classification for additive kernel SVMs",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",112,10.1109/TPAMI.2012.62,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870201308&doi=10.1109%2fTPAMI.2012.62&partnerID=40&md5=21bd2c508548294f6165ca0a4db8493b","We show that a class of nonlinear kernel SVMs admits approximate classifiers with runtime and memory complexity that is independent of the number of support vectors. This class of kernels, which we refer to as additive kernels, includes widely used kernels for histogram-based image comparison like intersection and chi-squared kernels. Additive kernel SVMs can offer significant improvements in accuracy over linear SVMs on a wide variety of tasks while having the same runtime, making them practical for large-scale recognition or real-time detection tasks. We present experiments on a variety of datasets, including the INRIA person, Daimler-Chrysler pedestrians, UIUC Cars, Caltech-101, MNIST, and USPS digits, to demonstrate the effectiveness of our method for efficient evaluation of SVMs with additive kernels. Since its introduction, our method has become integral to various state-of-the-art systems for PASCAL VOC object detection/image classification, ImageNet Challenge, TRECVID, etc. The techniques we propose can also be applied to settings where evaluation of weighted additive kernels is required, which include kernelized versions of PCA, LDA, regression, k-means, as well as speeding up the inner loop of SVM classifier training algorithms. © 1979-2012 IEEE.","additive kernels; efficient classifiers; Image classification; support vector machines","Chi-Squared; Daimler-chrysler; Data sets; Image comparison; Inner loops; K-means; Memory complexity; Nonlinear kernels; Object Detection; Real-time detection; Runtimes; State-of-the-art system; Support vector; SVM classifiers; TRECVID; Artificial intelligence; Computer vision; Image classification; Support vector machines; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; computer simulation; decision support system; methodology; support vector machine; theoretical model; Algorithms; Artificial Intelligence; Computer Simulation; Decision Support Techniques; Image Interpretation, Computer-Assisted; Models, Theoretical; Pattern Recognition, Automated; Support Vector Machines",Article,Scopus,2-s2.0-84870201308
"Gray K.R., Aljabar P., Heckemann R.A., Hammers A., Rueckert D.","Random forest-based similarity measures for multi-modal classification of Alzheimer's disease",2013,"NeuroImage",111,10.1016/j.neuroimage.2012.09.065,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868215748&doi=10.1016%2fj.neuroimage.2012.09.065&partnerID=40&md5=75d22ac1e108b007e22f896d13056b07","Neurodegenerative disorders, such as Alzheimer's disease, are associated with changes in multiple neuroimaging and biological measures. These may provide complementary information for diagnosis and prognosis. We present a multi-modality classification framework in which manifolds are constructed based on pairwise similarity measures derived from random forest classifiers. Similarities from multiple modalities are combined to generate an embedding that simultaneously encodes information about all the available features. Multi-modality classification is then performed using coordinates from this joint embedding. We evaluate the proposed framework by application to neuroimaging and biological data from the Alzheimer's Disease Neuroimaging Initiative (ADNI). Features include regional MRI volumes, voxel-based FDG-PET signal intensities, CSF biomarker measures, and categorical genetic information. Classification based on the joint embedding constructed using information from all four modalities out-performs the classification based on any individual modality for comparisons between Alzheimer's disease patients and healthy controls, as well as between mild cognitive impairment patients and healthy controls. Based on the joint embedding, we achieve classification accuracies of 89% between Alzheimer's disease patients and healthy controls, and 75% between mild cognitive impairment patients and healthy controls. These results are comparable with those reported in other recent studies using multi-kernel learning. Random forests provide consistent pairwise similarity measures for multiple modalities, thus facilitating the combination of different types of feature data. We demonstrate this by application to data in which the number of features differs by several orders of magnitude between modalities. Random forest classifiers extend naturally to multi-class problems, and the framework described here could be applied to distinguish between multiple patient groups in the future. © 2012 Elsevier Inc..","Alzheimer's disease; Cerebrospinal fluid biomarkers; Genetics; Magnetic resonance imaging; Mild cognitive impairment; Multi-modality classification; Positron emission tomography","amyloid beta protein; apolipoprotein E; fluorodeoxyglucose f 18; tau protein; accuracy; aged; Alzheimer disease; APOE gene; article; cerebrospinal fluid; classification; classifier; controlled study; female; genotype; human; kernel method; major clinical study; male; mild cognitive impairment; neuroimaging; nuclear magnetic resonance imaging; positron emission tomography; priority journal; random forest; Aged; Alzheimer Disease; Artificial Intelligence; Biological Markers; Female; Humans; Magnetic Resonance Imaging; Male; Models, Theoretical; Positron-Emission Tomography",Article,Scopus,2-s2.0-84868215748
"Cheng C., Yang H., Lyu M.R., King I.","Where you like to go next: Successive point-of-interest recommendation",2013,"IJCAI International Joint Conference on Artificial Intelligence",110,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896061752&partnerID=40&md5=36aa9e1b7a964f00aa61d6eb46ad01b0","Personalized point-of-interest (POI) recommendation is a significant task in location-based social networks (LBSNs) as it can help provide better user experience as well as enable third-party services, e.g., launching advertisements. To provide a good recommendation, various research has been conducted in the literature. However, pervious efforts mainly consider the ""check-ins"" in a whole and omit their temporal relation. They can only recommend POI globally and cannot know where a user would like to go tomorrow or in the next few days. In this paper, we consider the task of successive personalized POI recommendation in LBSNs, which is a much harder task than standard personalized POI recommendation or prediction. To solve this task, we observe two prominent properties in the check-in sequence: personalized Markov chain and region localization. Hence, we propose a novel matrix factorization method, namely FPMCLR, to embed the personalized Markov chains and the localized regions. Our proposed FPMC-LR not only exploits the personalized Markov chain in the check-in sequence, but also takes into account users' movement constraint, i.e., moving around a localized region. More importantly, utilizing the information of localized regions, we not only reduce the computation cost largely, but also discard the noisy information to boost recommendation. Results on two real-world LBSNs datasets demonstrate the merits of our proposed FPMC-LR.",,"Check-in; Computation costs; Location-based social networks; Matrix factorizations; Real-world; Temporal relation; User experience; Artificial intelligence; Location based services; Markov processes",Conference Paper,Scopus,2-s2.0-84896061752
"Modares H., Lewis F.L., Naghibi-Sistani M.-B.","Adaptive optimal control of unknown constrained-input systems using policy iteration and neural networks",2013,"IEEE Transactions on Neural Networks and Learning Systems",110,10.1109/TNNLS.2013.2276571,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885176157&doi=10.1109%2fTNNLS.2013.2276571&partnerID=40&md5=3e59f4e3d2cc5b8077cb6fbb76f5670a","This paper presents an online policy iteration (PI) algorithm to learn the continuous-time optimal control solution for unknown constrained-input systems. The proposed PI algorithm is implemented on an actor-critic structure where two neural networks (NNs) are tuned online and simultaneously to generate the optimal bounded control policy. The requirement of complete knowledge of the system dynamics is obviated by employing a novel NN identifier in conjunction with the actor and critic NNs. It is shown how the identifier weights estimation error affects the convergence of the critic NN. A novel learning rule is developed to guarantee that the identifier weights converge to small neighborhoods of their ideal values exponentially fast. To provide an easy-to-check persistence of excitation condition, the experience replay technique is used. That is, recorded past experiences are used simultaneously with current data for the adaptation of the identifier weights. Stability of the whole system consisting of the actor, critic, system state, and system identifier is guaranteed while all three networks undergo adaptation. Convergence to a near-optimal control law is also shown. The effectiveness of the proposed method is illustrated with a simulation example. © 2013 IEEE.","Input constraints; neural networks; optimal control; reinforcement learning; unknown dynamics","Adaptive optimal control; Input constraints; Near-optimal control; Neural networks (NNS); Optimal control solution; Optimal controls; Persistence of excitation; Simulation example; Algorithms; Control; Neural networks; Online systems; Optimal control systems; Reinforcement learning; Iterative methods; algorithm; artificial intelligence; artificial neural network; computer simulation; feedback system; learning; nonlinear system; signal processing; theoretical model; Algorithms; Artificial Intelligence; Computer Simulation; Feedback; Learning; Models, Theoretical; Neural Networks (Computer); Nonlinear Dynamics; Signal Processing, Computer-Assisted",Article,Scopus,2-s2.0-84885176157
"Geng X., Yin C., Zhou Z.-H.","Facial age estimation by learning from label distributions",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",108,10.1109/TPAMI.2013.51,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883152047&doi=10.1109%2fTPAMI.2013.51&partnerID=40&md5=c1119df93b2bec64114ce5fcb8a00869","One of the main difficulties in facial age estimation is that the learning algorithms cannot expect sufficient and complete training data. Fortunately, the faces at close ages look quite similar since aging is a slow and smooth process. Inspired by this observation, instead of considering each face image as an instance with one label (age), this paper regards each face image as an instance associated with a label distribution. The label distribution covers a certain number of class labels, representing the degree that each label describes the instance. Through this way, one face image can contribute to not only the learning of its chronological age, but also the learning of its adjacent ages. Two algorithms, named IIS-LLD and CPNN, are proposed to learn from such label distributions. Experimental results on two aging face databases show remarkable advantages of the proposed label distribution learning algorithms over the compared single-label learning algorithms, either specially designed for age estimation or for general purpose. © 1979-2012 IEEE.","Age estimation; face image; label distribution; machine learning","Age estimation; Chronological age; Face database; Face images; Label distribution; Number of class; Training data; Artificial intelligence; Computer vision; Learning systems; Learning algorithms; aging; algorithm; anatomy and histology; artificial intelligence; automated pattern recognition; biometry; computer assisted diagnosis; face; human; image enhancement; physiology; procedures; reproducibility; sensitivity and specificity; aging; article; automated pattern recognition; biometry; computer assisted diagnosis; face; histology; methodology; physiology; Aging; Algorithms; Artificial Intelligence; Biometry; Face; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Aging; Algorithms; Artificial Intelligence; Biometry; Face; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84883152047
"Wu Z., Liu X., Ni Z., Yuan D., Yang Y.","A market-oriented hierarchical scheduling strategy in cloud workflow systems",2013,"Journal of Supercomputing",108,10.1007/s11227-011-0578-4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872360361&doi=10.1007%2fs11227-011-0578-4&partnerID=40&md5=9fb0602bfb2d9fe91f42a1e7ec373587","A cloud workflow system is a type of platform service which facilitates the automation of distributed applications based on the novel cloud infrastructure. One of the most important aspects which differentiate a cloud workflow system from its other counterparts is the market-oriented business model. This is a significant innovation which brings many challenges to conventional workflow scheduling strategies. To investigate such an issue, this paper proposes a market-oriented hierarchical scheduling strategy in cloud workflow systems. Specifically, the service-level scheduling deals with the Task-to-Service assignment where tasks of individual workflow instances are mapped to cloud services in the global cloud markets based on their functional and non-functional QoS requirements; the task-level scheduling deals with the optimisation of the Task-to-VM (virtual machine) assignment in local cloud data centres where the overall running cost of cloud workflow systems will be minimised given the satisfaction of QoS constraints for individual tasks. Based on our hierarchical scheduling strategy, a package based random scheduling algorithm is presented as the candidate service-level scheduling algorithm and three representative metaheuristic based scheduling algorithms including genetic algorithm (GA), ant colony optimisation (ACO), and particle swarm optimisation (PSO) are adapted, implemented and analysed as the candidate task-level scheduling algorithms. The hierarchical scheduling strategy is being implemented in our SwinDeW-C cloud workflow system and demonstrating satisfactory performance. Meanwhile, the experimental results show that the overall performance of ACO based scheduling algorithm is better than others on three basic measurements: the optimisation rate on makespan, the optimisation rate on cost and the CPU time. © 2011 Springer Science+Business Media, LLC.","Cloud computing; Cloud workflow system; Hierarchical scheduling; Metaheuristics; Workflow scheduling","Ant colony optimisation; Business models; Cloud data; Cloud services; CPU time; Distributed applications; Global clouds; Hierarchical scheduling; Makespan; Meta heuristics; Metaheuristic; Non-functional; Optimisations; Particle swarm optimisation; QoS constraints; QoS requirements; Running cost; Virtual machines; Work-flow systems; Workflow scheduling; Artificial intelligence; Cloud computing; Commerce; Computer systems; Hierarchical systems; Parallel architectures; Particle swarm optimization (PSO); Quality of service; Scheduling algorithms; Work simplification; Scheduling",Article,Scopus,2-s2.0-84872360361
"Koo H.I., Kim D.H.","Scene text detection via connected component clustering and nontext filtering",2013,"IEEE Transactions on Image Processing",107,10.1109/TIP.2013.2249082,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876208085&doi=10.1109%2fTIP.2013.2249082&partnerID=40&md5=27aaba983fde830a1b0a5201c121aeba","In this paper, we present a new scene text detection algorithm based on two machine learning classifiers: one allows us to generate candidate word regions and the other filters out nontext ones. To be precise, we extract connected components (CCs) in images by using the maximally stable extremal region algorithm. These extracted CCs are partitioned into clusters so that we can generate candidate regions. Unlike conventional methods relying on heuristic rules in clustering, we train an AdaBoost classifier that determines the adjacency relationship and cluster CCs by using their pairwise relations. Then we normalize candidate word regions and determine whether each region contains text or not. Since the scale, skew, and color of each candidate can be estimated from CCs, we develop a text/nontext classifier for normalized images. This classifier is based on multilayer perceptrons and we can control recall and precision rates with a single free parameter. Finally, we extend our approach to exploit multichannel information. Experimental results on ICDAR 2005 and 2011 robust reading competition datasets show that our method yields the state-of-the-art performance both in speed and accuracy. © 1992-2012 IEEE.","CC clustering; connected component (CC)-based approach; machine learning classifier; nontext filtering; scene text detection","CC clustering; Connected component; Learning classifiers; Non-text filtering; Scene Text; Adaptive boosting; Learning systems; Heuristic methods; algorithm; artificial intelligence; automated pattern recognition; cluster analysis; factual database; image processing; procedures; article; image processing; methodology; Algorithms; Artificial Intelligence; Cluster Analysis; Databases, Factual; Image Processing, Computer-Assisted; Pattern Recognition, Automated; Algorithms; Artificial Intelligence; Cluster Analysis; Databases, Factual; Image Processing, Computer-Assisted; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84876208085
"Riedel S., Yao L., McCallum A., Marlin B.M.","Relation extraction with matrix factorization and universal schemas",2013,"NAACL HLT 2013 - 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Main Conference",107,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926180274&partnerID=40&md5=91e94b22ee57cbbaa27e98f40f3462d7","Traditional relation extraction predicts relations within some fixed and finite target schema. Machine learning approaches to this task require either manual annotation or, in the case of distant supervision, existing structured sources of the same schema. The need for existing datasets can be avoided by using a universal schema: the union of all involved schemas (surface form predicates as in OpenIE, and relations in the schemas of preexisting databases). This schema has an almost unlimited set of relations (due to surface forms), and supports integration with existing structured data (through the relation types of existing databases). To populate a database of such schema we present matrix factorization models that learn latent feature vectors for entity tuples and relations. We show that such latent models achieve substantially higher accuracy than a traditional classification approach. More importantly, by operating simultaneously on relations observed in text and in pre-existing structured DBs such as Freebase, we are able to reason about unstructured and structured data in mutually-supporting ways. By doing so our approach outperforms stateof- the-Art distant supervision. © 2013 Association for Computational Linguistics.",,"Artificial intelligence; Classification (of information); Computational linguistics; Database systems; Extraction; Factorization; Learning systems; Classification approach; Feature vectors; Machine learning approaches; Manual annotation; Matrix factorizations; Relation extraction; State of the art; Structured data; Matrix algebra",Conference Paper,Scopus,2-s2.0-84926180274
"Wu J., Lin W., Shi G., Liu A.","Perceptual quality metric with internal generative mechanism",2013,"IEEE Transactions on Image Processing",105,10.1109/TIP.2012.2214048,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871653541&doi=10.1109%2fTIP.2012.2214048&partnerID=40&md5=dbdac052fdda173fd85c5827282634d2","Objective image quality assessment (IQA) aims to evaluate image quality consistently with human perception. Most of the existing perceptual IQA metrics cannot accurately represent the degradations from different types of distortion, e.g., existing structural similarity metrics perform well on content-dependent distortions while not as well as peak signal-to-noise ratio (PSNR) on content-independent distortions. In this paper, we integrate the merits of the existing IQA metrics with the guide of the recently revealed internal generative mechanism (IGM). The IGM indicates that the human visual system actively predicts sensory information and tries to avoid residual uncertainty for image perception and understanding. Inspired by the IGM theory, we adopt an autoregressive prediction algorithm to decompose an input scene into two portions, the predicted portion with the predicted visual content and the disorderly portion with the residual content. Distortions on the predicted portion degrade the primary visual information, and structural similarity procedures are employed to measure its degradation; distortions on the disorderly portion mainly change the uncertain information and the PNSR is employed for it. Finally, according to the noise energy deployment on the two portions, we combine the two evaluation results to acquire the overall quality score. Experimental results on six publicly available databases demonstrate that the proposed metric is comparable with the state-of-the-art quality metrics. © 1992-2012 IEEE.","Human visual system; image decomposition; image quality assessment (IQA); internal generative mechanism (IGM)","Auto-regressive; Evaluation results; Generative mechanism; Human perception; Human Visual System; Image decomposition; Image perception; Image quality assessment; Noise energy; Objective image quality assessment; Overall quality; Peak signal-to-noise ratio; Perceptual quality; Prediction algorithms; Quality metrics; Residual content; Sensory information; Structural similarity; Uncertain informations; Visual content; Visual information; Algorithms; Image quality; algorithm; animal; article; artificial intelligence; Bayes theorem; factual database; human; image processing; methodology; signal noise ratio; theoretical model; videorecording; vision; Algorithms; Animals; Artificial Intelligence; Bayes Theorem; Databases, Factual; Humans; Image Processing, Computer-Assisted; Models, Theoretical; Signal-To-Noise Ratio; Video Recording; Visual Perception",Article,Scopus,2-s2.0-84871653541
"Tamilselvan P., Wang P.","Failure diagnosis using deep belief learning based health state classification",2013,"Reliability Engineering and System Safety",101,10.1016/j.ress.2013.02.022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875848937&doi=10.1016%2fj.ress.2013.02.022&partnerID=40&md5=67e0fc4b6e31f694cdc27487f336e214","Effective health diagnosis provides multifarious benefits such as improved safety, improved reliability and reduced costs for operation and maintenance of complex engineered systems. This paper presents a novel multi-sensor health diagnosis method using deep belief network (DBN). DBN has recently become a popular approach in machine learning for its promised advantages such as fast inference and the ability to encode richer and higher order network structures. The DBN employs a hierarchical structure with multiple stacked restricted Boltzmann machines and works through a layer by layer successive learning process. The proposed multi-sensor health diagnosis methodology using DBN based state classification can be structured in three consecutive stages: first, defining health states and preprocessing sensory data for DBN training and testing; second, developing DBN based classification models for diagnosis of predefined health states; third, validating DBN classification models with testing sensory dataset. Health diagnosis using DBN based health state classification technique is compared with four existing diagnosis techniques. Benchmark classification problems and two engineering health diagnosis applications: aircraft engine health diagnosis and electric power transformer health diagnosis are employed to demonstrate the efficacy of the proposed approach. © 2013 Elsevier Ltd. All rights reserved.","Artificial intelligence in diagnosis Classification; Deep belief networks; Fault diagnosis","Aircraft engines; Artificial intelligence; Bayesian networks; Complex networks; Failure analysis; Health; Learning systems; Power transformers; Sensors; Statistical tests; Benchmark classification; Complex engineered systems; Deep belief network (DBN); Deep belief networks; Electric power transformers; Hierarchical structures; Operation and maintenance; Restricted boltzmann machine; Classification (of information)",Article,Scopus,2-s2.0-84875848937
"Liu L., Shao L.","Learning discriminative representations from RGB-D video data",2013,"IJCAI International Joint Conference on Artificial Intelligence",100,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062906&partnerID=40&md5=75e6f548b2802908c2f3a1d45836e5f0","Recently, the low-cost Microsoft Kinect sensor, which can capture real-time high-resolution RGB and depth visual information, has attracted increasing attentions for a wide range of applications in computer vision. Existing techniques extract hand-tuned features from the RGB and the depth data separately and heuristically fuse them, which would not fully exploit the complementarity of both data sources. In this paper, we introduce an adaptive learning methodology to automatically extract (holistic) spatio-temporal features, simultaneously fusing the RGB and depth information, from RGBD video data for visual recognition tasks. We address this as an optimization problem using our proposed restricted graph-based genetic programming (RGGP) approach, in which a group of primitive 3D operators are first randomly assembled as graph-based combinations and then evolved generation by generation by evaluating on a set of RGBD video samples. Finally the best-performed combination is selected as the (near-)optimal representation for a pre-defined task. The proposed method is systematically evaluated on a new hand gesture dataset, SKIG, that we collected ourselves and the public MSRDailyActivity3D dataset, respectively. Extensive experimental results show that our approach leads to significant advantages compared with state-of-the-art hand-crafted and machine-learned features.",,"Adaptive learning; Depth information; High resolution; Microsoft Kinect sensors; Optimization problems; Spatio-temporal features; Visual information; Visual recognition; Artificial intelligence; Genetic algorithms; Genetic programming; Graphic methods; Optimization; Video recording",Conference Paper,Scopus,2-s2.0-84896062906
"Garg S., Gentry C., Halevi S., Sahai A., Waters B.","Attribute-based encryption for circuits from multilinear maps",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",100,10.1007/978-3-642-40084-1_27,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884481521&doi=10.1007%2f978-3-642-40084-1_27&partnerID=40&md5=d1d392c1f49aad25e2fad379fc22fe71","In this work, we provide the first construction of Attribute- Based Encryption (ABE) for general circuits. Our construction is based on the existence of multilinear maps. We prove selective security of our scheme in the standard model under the natural multilinear generalization of the BDDH assumption. Our scheme achieves both Key-Policy and Ciphertext-Policy variants of ABE. Our scheme and its proof of security directly translate to the recent multilinear map framework of Garg, Gentry, and Halevi. © 2013 International Association for Cryptologic Research.",,"Attribute-based encryptions; Ciphertext policies; First constructions; Key policies; Multilinear maps; The standard model; Artificial intelligence; Computer science; Cryptography",Conference Paper,Scopus,2-s2.0-84884481521
"Jiang L.L., Maskell D.L., Patra J.C.","A novel ant colony optimization-based maximum power point tracking for photovoltaic systems under partially shaded conditions",2013,"Energy and Buildings",100,10.1016/j.enbuild.2012.12.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872289297&doi=10.1016%2fj.enbuild.2012.12.001&partnerID=40&md5=91f6a6c0d02aa4e5285a9ed8fbcf0be0","In order to achieve maximum efficiency a photovoltaic (PV) arrays should operate at their maximum power point (MPP). Therefore, an MPP tracking (MPPT) scheme is implemented between the PV system and the load to obtain maximum power. When the irradiance distribution on the PV arrays is uniform, many traditional MPPT techniques can track the MPP effectively. However, when the PV arrays are partially shaded, multiple MPPs show up, which usually results in the failure of finding the global MPP. Some researchers have reported this problem and tried to solve it, but most of the MPP control schemes are relatively complicated or fail to guarantee the MPP under all shading circumstances. In order to overcome this difficulty, this paper presents a novel ant colony optimization (ACO)-based MPPT scheme for PV systems. A new control scheme is also introduced based on the proposed MPPT method. This heuristic algorithm based technique not only ensures the ability to find the global MPP, but also gives a simpler control scheme and lower system cost. The feasibility of this proposed method is verified with the irradiance of various shading patterns by simulation. In addition, the performance comparison with other traditional MPPT techniques, such as: constant voltage tracking (CVT), perturb and observe (P&O), particle swarm optimization (PSO), is also presented. The results show that the proposed algorithm can track the global MPP effectively, and is robust to various shading patterns. © 2012 Elsevier B.V.","Ant colony optimization; Maximum power point tracking; Partially shaded conditions; Photovoltaic systems","Ant colonies; Ant Colony Optimization (ACO); Constant voltage; Control schemes; Irradiance distribution; Maximum Efficiency; Maximum power; Maximum power point; Maximum Power Point Tracking; Partially shaded conditions; Performance comparison; Perturb and observe; Photovoltaic arrays; Photovoltaic systems; PV arrays; PV system; System costs; Heuristic algorithms; Particle swarm optimization (PSO); Photovoltaic cells; Artificial intelligence",Article,Scopus,2-s2.0-84872289297
"Bezrukov I., Mantlik F., Schmidt H., Schölkopf B., Pichler B.J.","MR-based PET attenuation correction for PET/MR imaging",2013,"Seminars in Nuclear Medicine",100,10.1053/j.semnuclmed.2012.08.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870010313&doi=10.1053%2fj.semnuclmed.2012.08.002&partnerID=40&md5=5d4a39a51f27b7c59e3a08269075c0ae","Recent progress has allowed hybrid positron emission tomography/magnetic resonance (PET/MR) systems to make the transition from research prototypes to systems with full potential for clinical imaging. Options for directly measuring the attenuation maps, as is possible with PET/computed tomography or PET transmission scans, are not included in PET/MR scanners. New methods to compute attenuation maps from MR data have therefore been developed. © 2013 Elsevier Inc.",,"adipose tissue; air; article; artifact reduction; bone tissue; computer assisted emission tomography; constants and coefficients; human; image reconstruction; lung parenchyma; machine learning; Monte Carlo method; muscle tissue; nuclear magnetic resonance imaging; phantom; positron emission tomography; radiation attenuation; whole body CT; whole body MRI; whole body PET; Animals; Artificial Intelligence; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Positron-Emission Tomography",Article,Scopus,2-s2.0-84870010313
"Cai X., Nie F., Huang H.","Multi-view K-means clustering on big data",2013,"IJCAI International Joint Conference on Artificial Intelligence",99,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062416&partnerID=40&md5=a4797b95b727290d3276256bdcbb3dad","In past decade, more and more data are collected from multiple sources or represented by multiple views, where different views describe distinct perspectives of the data. Although each view could be individually used for finding patterns by clustering, the clustering performance could be more accurate by exploring the rich information among multiple views. Several multi-view clustering methods have been proposed to unsupervised integrate different views of data. However, they are graph based approaches, e.g. based on spectral clustering, such that they cannot handle the large-scale data. How to combine these heterogeneous features for unsupervised large-scale data clustering has become a challenging problem. In this paper, we propose a new robust large-scale multi-view clustering method to integrate heterogeneous representations of largescale data. We evaluate the proposed new methods by six benchmark data sets and compared the performance with several commonly used clustering approaches as well as the baseline multi-view clustering methods. In all experimental results, our proposed methods consistently achieve superiors clustering performances.",,"Clustering approach; Heterogeneous features; K-means clustering; Large-scale datum; Multi-view clustering; Multiple source; Multiple views; Spectral clustering; Artificial intelligence; Benchmarking; Clustering algorithms; Cluster analysis",Conference Paper,Scopus,2-s2.0-84896062416
"Beg I., Rashid T.","TOPSIS for hesitant fuzzy linguistic term sets",2013,"International Journal of Intelligent Systems",98,10.1002/int.21623,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885951008&doi=10.1002%2fint.21623&partnerID=40&md5=e87f3e6b7b3bd12c86bbb2a06771a038","We propose a new method to aggregate the opinion of experts or decision makers on different criteria, regarding a set of alternatives, where the opinion of the experts is represented by hesitant fuzzy linguistic term sets. An illustrative example is provided to elaborate the proposed method for selection of the best alternative. © 2013 Wiley Periodicals, Inc.",,"Decision makers; Hesitant fuzzy linguistic term sets; Artificial intelligence; Software engineering; Linguistics",Article,Scopus,2-s2.0-84885951008
"Brehmer M., Munzner T.","A multi-level typology of abstract visualization tasks",2013,"IEEE Transactions on Visualization and Computer Graphics",98,10.1109/TVCG.2013.124,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886683619&doi=10.1109%2fTVCG.2013.124&partnerID=40&md5=bb3c4858d3576ec3e9b4f1cc34e346c5","The considerable previous work characterizing visualization usage has focused on low-level tasks or interactions and high-level tasks, leaving a gap between them that is not addressed. This gap leads to a lack of distinction between the ends and means of a task, limiting the potential for rigorous analysis. We contribute a multi-level typology of visualization tasks to address this gap, distinguishing why and how a visualization task is performed, as well as what the task inputs and outputs are. Our typology allows complex tasks to be expressed as sequences of interdependent simpler tasks, resulting in concise and flexible descriptions for tasks of varying complexity and scope. It provides abstract rather than domain-specific descriptions of tasks, so that useful comparisons can be made between visualization systems targeted at different application domains. This descriptive power supports a level of analysis required for the generation of new designs, by guiding the translation of domain-specific problems into abstract tasks, and for the qualitative evaluation of visualization usage. We demonstrate the benefits of our approach in a detailed case study, comparing task descriptions from our typology to those derived from related work. We also discuss the similarities and differences between our typology and over two dozen extant classification systems and theoretical frameworks from the literatures of visualization, human-computer interaction, information retrieval, communications, and cartography. © 1995-2012 IEEE.","qualitative evaluation; task and requirements analysis; Typology; visualization models","Classification system; Qualitative evaluations; Requirements analysis; Rigorous analysis; Theoretical framework; Typology; Visualization models; Visualization system; Maps; Visualization; algorithm; artificial intelligence; computer interface; computer simulation; human; physiology; reproducibility; sensitivity and specificity; task performance; theoretical model; vision; article; physiology; vision; Algorithms; Artificial Intelligence; Computer Simulation; Humans; Models, Theoretical; Reproducibility of Results; Sensitivity and Specificity; Task Performance and Analysis; User-Computer Interface; Visual Perception; Algorithms; Artificial Intelligence; Computer Simulation; Humans; Models, Theoretical; Reproducibility of Results; Sensitivity and Specificity; Task Performance and Analysis; User-Computer Interface; Visual Perception",Article,Scopus,2-s2.0-84886683619
"Mcmahan H.B., Holt G., Sculley D., Young M., Ebner D., Grady J., Nie L., Phillips T., Davydov E., Golovin D., Chikkerur S., Liu D., Wattenberg M., Hrafnkelsson A.M., Boulos T., Kubica J.","Ad click prediction: A view from the trenches",2013,"Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",98,10.1145/2487575.2488200,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022224234&doi=10.1145%2f2487575.2488200&partnerID=40&md5=cc611ac1eb1a2cbfdde65c6f7c459ba1","Predicting ad click{through rates (CTR) is a massive-scale learning problem that is central to the multi-billion dollar online advertising industry. We present a selection of case studies and topics drawn from recent experiments in the setting of a deployed CTR prediction system. These include improvements in the context of traditional supervised learning based on an FTRL-Proximal online learning algorithm (which has excellent sparsity and convergence properties) and the use of per-coordinate learning rates. We also explore some of the challenges that arise in a real-world system that may appear at first to be outside the domain of traditional machine learning research. These include useful tricks for memory savings, methods for assessing and visualizing performance, practical methods for providing confidence estimates for predicted probabilities, calibration methods, and methods for automated management of features. Finally, we also detail several directions that did not turn out to be beneficial for us, despite promising results elsewhere in the literature. The goal of this paper is to highlight the close relationship between theoretical advances and practical engineering in this industrial setting, and to show the depth of challenges that appear when applying traditional machine learning methods in a complex dynamic system. Copyright © 2013 ACM.","Data mining; Large-scale learning; Online advertising","Artificial intelligence; Data mining; E-learning; Forecasting; Learning algorithms; Learning systems; Marketing; Professional aspects; Complex dynamic systems; Convergence properties; Large-scale learning; Machine learning methods; Machine learning research; Online advertising; Online learning algorithms; Practical engineering; Education",Conference Paper,Scopus,2-s2.0-85022224234
"Hartley R., Trumpf J., Dai Y., Li H.","Rotation averaging",2013,"International Journal of Computer Vision",98,10.1007/s11263-012-0601-0,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878743245&doi=10.1007%2fs11263-012-0601-0&partnerID=40&md5=67cc8e30dc301126ef49bc6646d78673","This paper is conceived as a tutorial on rotation averaging, summarizing the research that has been carried out in this area; it discusses methods for single-view and multiple-view rotation averaging, as well as providing proofs of convergence and convexity in many cases. However, at the same time it contains many new results, which were developed to fill gaps in knowledge, answering fundamental questions such as radius of convergence of the algorithms, and existence of local minima. These matters, or even proofs of correctness have in many cases not been considered in the Computer Vision literature. We consider three main problems: single rotation averaging, in which a single rotation is computed starting from several measurements; multiple-rotation averaging, in which absolute orientations are computed from several relative orientation measurements; and conjugate rotation averaging, which relates a pair of coordinate frames. This last is related to the hand-eye coordination problem and to multiple-camera calibration. © 2013 Springer Science+Business Media New York.","Angular distance; Chordal distance; conjugate rotation; Geodesic distance; L1 mean; L2 mean; Quaternion distance","Angular distance; Chordal distance; Geodesic distances; L<sub>1</sub> mean; L<sub>2</sub> mean; Quaternion distance; Artificial intelligence; Software engineering; Rotation",Article,Scopus,2-s2.0-84878743245
"Modgil S., Prakken H.","A general account of argumentation with preferences",2013,"Artificial Intelligence",98,10.1016/j.artint.2012.10.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879958953&doi=10.1016%2fj.artint.2012.10.008&partnerID=40&md5=2fc0a144776ad50ed9cd291b5b934316","This paper builds on the recent ASPIC+ formalism, to develop a general framework for argumentation with preferences. We motivate a revised definition of conflict free sets of arguments, adapt ASPIC+ to accommodate a broader range of instantiating logics, and show that under some assumptions, the resulting framework satisfies key properties and rationality postulates. We then show that the generalised framework accommodates Tarskian logic instantiations extended with preferences, and then study instantiations of the framework by classical logic approaches to argumentation. We conclude by arguing that ASPIC+'s modelling of defeasible inference rules further testifies to the generality of the framework, and then examine and counter recent critiques of Dung's framework and its extensions to accommodate preferences. © 2012 Elsevier B.V. All rights reserved.","Argumentation; Dung; Non-monotonic reasoning; Preferences","Artificial intelligence; Argumentation; Classical logic; Conflict free; Dung; Inference rules; Non-monotonic reasoning; Preferences; Rationality postulates; Network function virtualization",Article,Scopus,2-s2.0-84879958953
"Jorge Cardoso M., Leung K., Modat M., Keihaninejad S., Cash D., Barnes J., Fox N.C., Ourselin S.","STEPS: Similarity and Truth Estimation for Propagated Segmentations and its application to hippocampal segmentation and brain parcelation",2013,"Medical Image Analysis",97,10.1016/j.media.2013.02.006,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878630983&doi=10.1016%2fj.media.2013.02.006&partnerID=40&md5=d064f72ff4191d156f8b228ffa376bdc","Anatomical segmentation of structures of interest is critical to quantitative analysis in medical imaging. Several automated multi-atlas based segmentation propagation methods that utilise manual delineations from multiple templates appear promising. However, high levels of accuracy and reliability are needed for use in diagnosis or in clinical trials. We propose a new local ranking strategy for template selection based on the locally normalised cross correlation (LNCC) and an extension to the classical STAPLE algorithm by Warfield et al. (2004), which we refer to as STEPS for Similarity and Truth Estimation for Propagated Segmentations. It addresses the well-known problems of local vs. global image matching and the bias introduced in the performance estimation due to structure size. We assessed the method on hippocampal segmentation using a leave-one-out cross validation with optimised model parameters; STEPS achieved a mean Dice score of 0.925 when compared with manual segmentation. This was significantly better in terms of segmentation accuracy when compared to other state-of-the-art fusion techniques. Furthermore, due to the finer anatomical scale, STEPS also obtains more accurate segmentations even when using only a third of the templates, reducing the dependence on large template databases. Using a subset of Alzheimer's Disease Neuroimaging Initiative (ADNI) scans from different MRI imaging systems and protocols, STEPS yielded similarly accurate segmentations (Dice=0.903). A cross-sectional and longitudinal hippocampal volumetric study was performed on the ADNI database. Mean±SD hippocampal volume (mm3) was 5195±656 for controls; 4786±781 for MCI; and 4427±903 for Alzheimer's disease patients and hippocampal atrophy rates (%/year) of 1.09±3.0, 2.74±3.5 and 4.04±3.6 respectively. Statistically significant (p &lt; 10 - 3) differences were found between disease groups for both hippocampal volume and volume change rates. Finally, STEPS was also applied in a multi-label segmentation propagation scenario using a leave-one-out cross validation, in order to parcellate 83 separate structures of the brain. Comparisons of STEPS with state-of-the-art multi-label fusion algorithms showed statistically significant segmentation accuracy improvements (p &lt; 10 - 4) in several key structures. © 2013 Elsevier B.V.","Brain parcelation; Hippocampus segmentation; Label propagation; Local similarity metric","Hippocampal segmentations; Hippocampus segmentation; Label propagation; Leave-one-out cross validations; Local similarity; Multi atlas-based segmentations; Performance estimation; Segmentation propagation; Algorithms; Diagnosis; Estimation; Image matching; Image segmentation; Neuroimaging; algorithm; Alzheimer disease; article; brain; brain atrophy; data base; diagnostic imaging; hippocampus; human; neuroimaging; nuclear magnetic resonance imaging; priority journal; quantitative analysis; scoring system; similarity and truth estimation for propagated segmentations; Algorithms; Alzheimer Disease; Artificial Intelligence; Hippocampus; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84878630983
"Belhumeur P.N., Jacobs D.W., Kriegman D.J., Kumar N.","Localizing parts of faces using a consensus of exemplars",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",96,10.1109/TPAMI.2013.23,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887569228&doi=10.1109%2fTPAMI.2013.23&partnerID=40&md5=86fa94d33ccd7b15b3ac8bd33960f4e7","We present a novel approach to localizing parts in images of human faces. The approach combines the output of local detectors with a nonparametric set of global models for the part locations based on over 1,000 hand-labeled exemplar images. By assuming that the global models generate the part locations as hidden variables, we derive a Bayesian objective function. This function is optimized using a consensus of models for these hidden variables. The resulting localizer handles a much wider range of expression, pose, lighting, and occlusion than prior ones. We show excellent performance on real-world face datasets such as Labeled Faces in tH.W.ld (LFW) and a new Labeled Face Parts in tH.W.ld (LFPW) and show that our localizer achieves state-of-the-art performance on the less challenging BioID dataset. © 2013 IEEE.","biometrics; faces; fiducial points; Part localization","faces; Fiducial points; Global models; Hidden variable; Non-parametric; Objective functions; Part localization; State-of-the-art performance; Artificial intelligence; Computer vision; Biometrics; algorithm; automated pattern recognition; Bayes theorem; consensus; face; human; pattern recognition; theoretical model; Algorithms; Bayes Theorem; Consensus; Face; Humans; Models, Theoretical; Pattern Recognition, Automated; Pattern Recognition, Visual",Article,Scopus,2-s2.0-84887569228
"Bede B., Stefanini L.","Generalized differentiability of fuzzy-valued functions",2013,"Fuzzy Sets and Systems",95,10.1016/j.fss.2012.10.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884670400&doi=10.1016%2fj.fss.2012.10.003&partnerID=40&md5=8ed228cfd371f179d1ed7185287cf513","In the present paper, using novel generalizations of the Hukuhara difference for fuzzy sets, we introduce and study new generalized differentiability concepts for fuzzy valued functions. Several properties of the new concepts are investigated and they are compared to similar fuzzy differentiabilities finding connections between them. Characterization and relatively simple expressions are provided for the new derivatives. © 2012 Elsevier B.V. All rights reserved.","Fuzzy-valued function; Generalized fuzzy derivative; Generalized Hukuhara differentiability; Strongly generalized differentiability","Artificial intelligence; Fuzzy sets; Differentiability; Fuzzy derivatives; Fuzzy-valued function; Generalized differentiability; Hukuhara difference; Simple expression; Difference equations",Article,Scopus,2-s2.0-84884670400
"Shin H.-C., Orton M.R., Collins D.J., Doran S.J., Leach M.O.","Stacked autoencoders for unsupervised feature learning and multiple organ detection in a pilot study using 4D patient data",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",94,10.1109/TPAMI.2012.277,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879853539&doi=10.1109%2fTPAMI.2012.277&partnerID=40&md5=06fbd9b2781bbca19338ef69b4160479","Medical image analysis remains a challenging application area for artificial intelligence. When applying machine learning, obtaining ground-truth labels for supervised learning is more difficult than in many more common applications of machine learning. This is especially so for datasets with abnormalities, as tissue types and the shapes of the organs in these datasets differ widely. However, organ detection in such an abnormal dataset may have many promising potential real-world applications, such as automatic diagnosis, automated radiotherapy planning, and medical image retrieval, where new multimodal medical images provide more information about the imaged tissues for diagnosis. Here, we test the application of deep learning methods to organ identification in magnetic resonance medical images, with visual and temporal hierarchical features learned to categorize object classes from an unlabeled multimodal DCE-MRI dataset so that only a weakly supervised training is required for a classifier. A probabilistic patch-based method was employed for multiple organ detection, with the features learned from the deep learning model. This shows the potential of the deep learning model for application to medical images, despite the difficulty of obtaining libraries of correctly labeled training datasets and despite the intrinsic abnormalities present in patient datasets. © 1979-2012 IEEE.","biomedical image processing; Edge and feature detection; machine learning; object recognition; pixel classification","Bio-medical image processing; Edge and feature detection; Hierarchical features; Multimodal medical images; Pixel classification; Radiotherapy planning; Unsupervised feature learning; Weakly supervised trainings; Artificial intelligence; Hospital data processing; Magnetic resonance; Object recognition; Statistical tests; Learning systems; artificial intelligence; automated pattern recognition; factual database; human; image enhancement; image processing; nuclear magnetic resonance imaging; pilot study; procedures; article; image processing; methodology; nuclear magnetic resonance imaging; Artificial Intelligence; Databases, Factual; Humans; Image Enhancement; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Pattern Recognition, Automated; Pilot Projects; Artificial Intelligence; Databases, Factual; Humans; Image Enhancement; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Pattern Recognition, Automated; Pilot Projects",Article,Scopus,2-s2.0-84879853539
"Buil-Aranda C., Hogan A., Umbrich J., Vandenbussche P.-Y.","SPARQL web-querying infrastructure: Ready for action?",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",92,10.1007/978-3-642-41338-4_18,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891943229&doi=10.1007%2f978-3-642-41338-4_18&partnerID=40&md5=22a83664c357dc6ec44b675293547f8d","Hundreds of public SPARQL endpoints have been deployed on the Web, forming a novel decentralised infrastructure for querying billions of structured facts from a variety of sources on a plethora of topics. But is this infrastructure mature enough to support applications? For 427 public SPARQL endpoints registered on the DataHub, we conduct various experiments to test their maturity. Regarding discoverability, we find that only one-third of endpoints make descriptive meta-data available, making it difficult to locate or learn about their content and capabilities. Regarding interoperability, we find patchy support for established SPARQL features like ORDER BY as well as (understandably) for new SPARQL 1.1 features. Regarding efficiency, we show that the performance of endpoints for generic queries can vary by up to 3-4 orders of magnitude. Regarding availability, based on a 27-month long monitoring experiment, we show that only 32.2% of public endpoints can be expected to have (monthly) ""two-nines"" uptimes of 99-100%. © 2013 Springer-Verlag.",,"Decentralised; Orders of magnitude; SPARQL 1.1; Artificial intelligence; Computer science; Computers; Experiments",Conference Paper,Scopus,2-s2.0-84891943229
"Shi J., Ha S.D., Zhou Y., Schoofs F., Ramanathan S.","A correlated nickelate synaptic transistor",2013,"Nature Communications",92,10.1038/ncomms3676,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887288780&doi=10.1038%2fncomms3676&partnerID=40&md5=337da3d621f81ea02c3a013a32da8f38","Inspired by biological neural systems, neuromorphic devices may open up new computing paradigms to explore cognition, learning and limits of parallel computation. Here we report the demonstration of a synaptic transistor with SmNiO 3, a correlated electron system with insulator-metal transition temperature at 130 C in bulk form. Non-volatile resistance and synaptic multilevel analogue states are demonstrated by control over composition in ionic liquid-gated devices on silicon platforms. The extent of the resistance modulation can be dramatically controlled by the film microstructure. By simulating the time difference between postneuron and preneuron spikes as the input parameter of a gate bias voltage pulse, synaptic spike-timing-dependent plasticity learning behaviour is realized. The extreme sensitivity of electrical properties to defects in correlated oxides may make them a particularly suitable class of materials to realize artificial biological circuits that can be operated at and above room temperature and seamlessly integrated into conventional electronic circuits. © 2013 Macmillan Publishers Limited. All rights reserved.",,"ionic liquid; metal derivative; metal oxide; nickelate; silicon; unclassified drug; cognition; correlation; electrical property; integrated approach; neurology; oxide; paradigm shift; temperature effect; article; conductance; film; learning; nerve cell; nerve cell plasticity; room temperature; semiconductor; signal transduction; stoichiometry; synapse; transition temperature; Animals; Artificial Intelligence; Calcium Compounds; Electric Conductivity; Electrons; Humans; Ionic Liquids; Models, Neurological; Neurons; Nickel; Oxides; Silicon; Synapses; Titanium; Transistors, Electronic",Article,Scopus,2-s2.0-84887288780
"Bustince H., Fernandez J., Kolesárová A., Mesiar R.","Generation of linear orders for intervals by means of aggregation functions",2013,"Fuzzy Sets and Systems",92,10.1016/j.fss.2012.07.015,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875739316&doi=10.1016%2fj.fss.2012.07.015&partnerID=40&md5=d486347f821602d357c7052d2de5209d","The problem of choosing an appropriate total order is crucial for many applications that make use of extensions of fuzzy sets. In this work we introduce the concept of an admissible order as a total order that extends the usual partial order between intervals. We propose a method to build these admissible orders in terms of two aggregation functions and we prove that some of the most used examples of total orders that appear in the literature are specific cases of our construction. © 2012 Elsevier B.V.","A-IFS Atanassov's intuitionistic fuzzy set; Aggregation function; Interval-valued fuzzy set; Linear order","Aggregation functions; Atanassov's intuitionistic fuzzy sets; Interval-valued fuzzy sets; Linear order; Partial order; Total order; Artificial intelligence; Fuzzy sets",Article,Scopus,2-s2.0-84875739316
"Qian G., Wang H., Feng X.","Generalized hesitant fuzzy sets and their application in decision support system",2013,"Knowledge-Based Systems",92,10.1016/j.knosys.2012.08.019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870057407&doi=10.1016%2fj.knosys.2012.08.019&partnerID=40&md5=9b2de6f9310cf3382e89e660b6f19017","Hesitant fuzzy sets are very useful to deal with group decision making problems when experts have a hesitation among several possible memberships for an element to a set. During the evaluating process in practice, however, these possible memberships may be not only crisp values in [0, 1], but also interval values. In this study, we extend hesitant fuzzy sets by intuitionistic fuzzy sets and refer to them as generalized hesitant fuzzy sets. Zadeh's fuzzy sets, intuitionistic fuzzy sets and hesitant fuzzy sets are special cases of the new fuzzy sets. We redefine some basic operations of generalized hesitant fuzzy sets, which are consistent with those of hesitant fuzzy sets. Some arithmetic operations and relationships among them are discussed as well. We further introduce the comparison law to distinguish two generalized hesitant fuzzy sets according to score function and consistency function. Besides, the proposed extension principle enables decision makers to employ aggregation operators of intuitionistic fuzzy sets to aggregate a set of generalized hesitant fuzzy sets for decision making. The rationality of applying the proposed techniques is clarified by a practical example. At last, the proposed techniques are devoted to a decision support system. © 2012 Elsevier B.V. All rights reserved.","Aggregation operator; Decision support system; Group decision making; Hesitant fuzzy sets; Intuitionistic fuzzy sets; Multi criteria decision making","Aggregation operator; Group Decision Making; Hesitant fuzzy sets; Intuitionistic fuzzy sets; Multi criteria decision making; Artificial intelligence; Decision making; Decision support systems; Mathematical operators; Fuzzy sets",Article,Scopus,2-s2.0-84870057407
"Mendes R., Kennedy J., Neves J.","Watch thy neighbor or how the swarm can learn from its environment",2013,"2003 IEEE Swarm Intelligence Symposium, SIS 2003 - Proceedings",92,10.1109/SIS.2003.1202252,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942113543&doi=10.1109%2fSIS.2003.1202252&partnerID=40&md5=77099ff933e14bcaf2cc83c08e8f233a","Particle swarm optimization is a novel algorithm where a population of candidate problem solution vectors evolves ""social"" norms by being influenced by their topological neighbors. Until now, an individual was influenced by its best performance acquired in the past and the best experience observed in its neighborhood. In this paper, we introduce new ways an individual can be influenced by its neighbors. © 2003 IEEE.","Equations; Evolutionary computation; Information analysis; Particle swarm optimization; Statistics; Topology; Watches","Algorithms; Artificial intelligence; Information analysis; Particle swarm optimization (PSO); Statistics; Topology; Watches; Equations; Novel algorithm; Problem solutions; Evolutionary algorithms",Conference Paper,Scopus,2-s2.0-84942113543
"Čehovin L., Kristan M., Leonardis A.","Robust visual tracking using an adaptive coupled-layer visual model",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",91,10.1109/TPAMI.2012.145,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874519372&doi=10.1109%2fTPAMI.2012.145&partnerID=40&md5=512fc7d529c948427bedcae1156e7e6d","This paper addresses the problem of tracking objects which undergo rapid and significant appearance changes. We propose a novel coupled-layer visual model that combines the target's global and local appearance by interlacing two layers. The local layer in this model is a set of local patches that geometrically constrain the changes in the target's appearance. This layer probabilistically adapts to the target's geometric deformation, while its structure is updated by removing and adding the local patches. The addition of these patches is constrained by the global layer that probabilistically models the target's global visual properties, such as color, shape, and apparent local motion. The global visual properties are updated during tracking using the stable patches from the local layer. By this coupled constraint paradigm between the adaptation of the global and the local layer, we achieve a more robust tracking through significant appearance changes. We experimentally compare our tracker to 11 state-of-the-art trackers. The experimental results on challenging sequences confirm that our tracker outperforms the related trackers in many cases by having a smaller failure rate as well as better accuracy. Furthermore, the parameter analysis shows that our tracker is stable over a range of parameter values. © 1979-2012 IEEE.","Image processing and computer vision; tracking","Coupled constraints; Failure rate; Geometric deformations; Image processing and computer vision; Local motions; Parameter analysis; Parameter values; Robust tracking; Tracking objects; Visual model; Visual properties; Visual Tracking; Artificial intelligence; Computer vision; Surface discharges; Image processing; algorithm; article; human; image processing; methodology; movement (physiology); theoretical model; videorecording; Algorithms; Humans; Image Processing, Computer-Assisted; Models, Theoretical; Movement; Video Recording",Article,Scopus,2-s2.0-84874519372
"Gong B., Grauman K., Sha F.","Connecting the dots with landmarks: Discriminatively learning domain-invariant features for unsupervised domain adaptation",2013,"30th International Conference on Machine Learning, ICML 2013",91,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897476317&partnerID=40&md5=64043e74ed6154586d037d00e5c36e9b","Learning domain-invariant features is of vital importance to unsupervised domain adaptation, where classifiers trained on the source domain need to be adapted to a different target domain for which no labeled examples are available. In this paper, we propose a novel approach for learning such features. The central idea is to exploit the existence of landmarks, which are a subset of labeled data instances in the source domain that are distributed most similarly to the target domain. Our approach automatically discovers the landmarks and use them to bridge the source to the target by constructing provably easier auxiliary domain adaptation tasks. The solutions of those auxiliary tasks form the basis to compose invariant features for the original task. We show how this composition can be optimized discriminatively without requiring labels from the target domain. We validate the method on standard benchmark datasets for visual object recognition and sentiment analysis of text. Empirical results show the proposed method outperforms the state-of-the-art significantly. Copyright 2013 by the author(s).",,"Artificial intelligence; Software engineering; Benchmark datasets; Domain adaptation; Invariant features; Labeled data; Sentiment analysis; Target domain; Visual object recognition; Learning systems",Conference Paper,Scopus,2-s2.0-84897476317
"Soualhi A., Clerc G., Razik H.","Detection and diagnosis of faults in induction motor using an improved artificial ant clustering technique",2013,"IEEE Transactions on Industrial Electronics",89,10.1109/TIE.2012.2230598,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877754137&doi=10.1109%2fTIE.2012.2230598&partnerID=40&md5=662afe6dda1f7b0d2d1320550492b889","The presence of electrical and mechanical faults in the induction motors (IMs) can be detected by analysis of the stator current spectrum. However, when an IM is fed by a frequency converter, the spectral analysis of stator current signal becomes difficult. For this reason, the monitoring must depend on multiple signatures in order to reduce the effect of harmonic disturbance on the motor-phase current. The aim of this paper is the description of a new approach for fault detection and diagnosis of IMs using signal-based method. It is based on signal processing and an unsupervised classification technique called the artificial ant clustering. The proposed approach is tested on a squirrel-cage IM of 5.5 kW in order to detect broken rotor bars and bearing failure at different load levels. The experimental results prove the efficiency of our approach compared with supervised classification methods in condition monitoring of electrical machines. © 1982-2012 IEEE.","Artificial intelligence; fault detection; fault diagnosis; feature extraction; induction motors (IMs); monitoring; motor-current signal analysis; pattern recognition (PR); signal processing; squirrel-cage motors","Artificial intelligence; Condition monitoring; Electric fault currents; Electric machinery; Failure analysis; Feature extraction; Induction motors; Monitoring; Signal detection; Signal processing; Spectrum analysis; Stators; Detection and diagnosis; Fault detection and diagnosis; Harmonic disturbances; Multiple signatures; Squirrel-cage; Stator current signal; Supervised classification; Unsupervised classification; Fault detection",Article,Scopus,2-s2.0-84877754137
"Cuevas E., Cienfuegos M., Zaldívar D., Pérez-Cisneros M.","A swarm optimization algorithm inspired in the behavior of the social-spider",2013,"Expert Systems with Applications",88,10.1016/j.eswa.2013.05.041,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879491396&doi=10.1016%2fj.eswa.2013.05.041&partnerID=40&md5=9f872001bfe5d363ed9a47b7f7e7471a","Swarm intelligence is a research field that models the collective behavior in swarms of insects or animals. Several algorithms arising from such models have been proposed to solve a wide range of complex optimization problems. In this paper, a novel swarm algorithm called the Social Spider Optimization (SSO) is proposed for solving optimization tasks. The SSO algorithm is based on the simulation of cooperative behavior of social-spiders. In the proposed algorithm, individuals emulate a group of spiders which interact to each other based on the biological laws of the cooperative colony. The algorithm considers two different search agents (spiders): males and females. Depending on gender, each individual is conducted by a set of different evolutionary operators which mimic different cooperative behaviors that are typically found in the colony. In order to illustrate the proficiency and robustness of the proposed approach, it is compared to other well-known evolutionary methods. The comparison examines several standard benchmark functions that are commonly considered within the literature of evolutionary algorithms. The outcome shows a high performance of the proposed method for searching a global optimum with several benchmark functions. © 2013 Elsevier B.V. All rights reserved.","Bio-inspired algorithms; Global optimization; Swarm algorithms","Bio-inspired algorithms; Co-operative behaviors; Collective behavior; Complex optimization problems; Evolutionary method; Evolutionary operators; Swarm algorithms; Swarm optimization algorithms; Artificial intelligence; Behavioral research; Global optimization; Optimization; Evolutionary algorithms",Article,Scopus,2-s2.0-84879491396
"Secrest B.R., Lamont G.B.","Visualizing particle swarm optimization - Gaussian particle swarm optimization",2013,"2003 IEEE Swarm Intelligence Symposium, SIS 2003 - Proceedings",86,10.1109/SIS.2003.1202268,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942085623&doi=10.1109%2fSIS.2003.1202268&partnerID=40&md5=ae5df7f399054fccdb7b79cf219996e6","Particle swarm optimization (PSO) conjures an image of particles searching for the optima the way bees buzz around flowers. One approach at visualizing the swarm graphs where all the particles are each generation, thus demonstrating the random nature associated with swarms of insects. Another approach is to show successive bests, thus showing the way that the swarm progresses. Some have even looked at the specific search path of the particle that eventually finds the optima. These approaches provide limited understanding of PSO. This paper presents a new visualization approach based on the probability distribution of the swarm, thus the random nature of PSO is properly visualized. The visualization allows better understanding of how to tune the algorithm and depicts weaknesses. A new algorithm based on moving the swarm a Gaussian distance from the global and local best is presented. Gaussian particle swarm optimization (GPSO) is compared to PSO. © 2003 IEEE.","Equations; Extraterrestrial measurements; Insects; Laboratories; Particle measurements; Particle swarm optimization; Planetary orbits; Sun; Velocity measurement; Visualization","Artificial intelligence; Flow visualization; Gaussian distribution; Laboratories; Orbits; Probability distributions; Sun; Velocity measurement; Visualization; Equations; Extraterrestrial measurements; Insects; Particle measurement; Planetary orbits; Particle swarm optimization (PSO)",Conference Paper,Scopus,2-s2.0-84942085623
"Kasabov N., Dhoble K., Nuntalid N., Indiveri G.","Dynamic evolving spiking neural networks for on-line spatio- and spectro-temporal pattern recognition",2013,"Neural Networks",85,10.1016/j.neunet.2012.11.014,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875878715&doi=10.1016%2fj.neunet.2012.11.014&partnerID=40&md5=a47f6e8cc4ed3e5d7c8cacb3d7351d7c","On-line learning and recognition of spatio- and spectro-temporal data (SSTD) is a very challenging task and an important one for the future development of autonomous machine learning systems with broad applications. Models based on spiking neural networks (SNN) have already proved their potential in capturing spatial and temporal data. One class of them, the evolving SNN (eSNN), uses a one-pass rank-order learning mechanism and a strategy to evolve a new spiking neuron and new connections to learn new patterns from incoming data. So far these networks have been mainly used for fast image and speech frame-based recognition. Alternative spike-time learning methods, such as Spike-Timing Dependent Plasticity (STDP) and its variant Spike Driven Synaptic Plasticity (SDSP), can also be used to learn spatio-temporal representations, but they usually require many iterations in an unsupervised or semi-supervised mode of learning. This paper introduces a new class of eSNN, dynamic eSNN, that utilise both rank-order learning and dynamic synapses to learn SSTD in a fast, on-line mode. The paper also introduces a new model called deSNN, that utilises rank-order learning and SDSP spike-time learning in unsupervised, supervised, or semi-supervised modes. The SDSP learning is used to evolve dynamically the network changing connection weights that capture spatio-temporal spike data clusters both during training and during recall. The new deSNN model is first illustrated on simple examples and then applied on two case study applications: (1) moving object recognition using address-event representation (AER) with data collected using a silicon retina device; (2) EEG SSTD recognition for brain-computer interfaces. The deSNN models resulted in a superior performance in terms of accuracy and speed when compared with other SNN models that use either rank-order or STDP learning. The reason is that the deSNN makes use of both the information contained in the order of the first input spikes (which information is explicitly present in input data streams and would be crucial to consider in some tasks) and of the information contained in the timing of the following spikes that is learned by the dynamic synapses as a whole spatio-temporal pattern. © 2012 Elsevier Ltd.","Dynamic synapses; EEG pattern recognition; Evolving connectionist systems; Moving object recognition; Rank-order coding; Spatio-temporal pattern recognition; Spike time based learning; Spiking neural networks","Dynamic synapsis; EEG pattern recognition; Evolving connectionist systems; Moving object recognition; Rank-order coding; Spatiotemporal patterns; Spiking neural networks; Time based; Digital storage; Electroencephalography; Learning algorithms; Learning systems; Neural networks; Object recognition; Plasticity; Speech recognition; Dynamic models; accuracy; algorithm; article; brain computer interface; long term depression; long term potentiation; mathematical computing; mathematical model; nerve cell network; nerve cell plasticity; pattern recognition; priority journal; spatiotemporal analysis; spike wave; Action Potentials; Artificial Intelligence; Connectome; Electroencephalography; Humans; Models, Neurological; Motion Perception; Nerve Net; Neural Networks (Computer); Neuronal Plasticity; Neurons; Pattern Recognition, Visual; Recognition (Psychology); Time Perception",Article,Scopus,2-s2.0-84875878715
"Cook D.J., Krishnan N.C., Rashidi P.","Activity discovery and activity recognition: A new partnership",2013,"IEEE Transactions on Cybernetics",84,10.1109/TSMCB.2012.2216873,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881180779&doi=10.1109%2fTSMCB.2012.2216873&partnerID=40&md5=a17c665502295fc45e2bcf1b2506fafa","Activity recognition has received increasing attention from the machine learning community. Of particular interest is the ability to recognize activities in real time from streaming data, but this presents a number of challenges not faced by traditional offline approaches. Among these challenges is handling the large amount of data that does not belong to a predefined class. In this paper, we describe a method by which activity discovery can be used to identify behavioral patterns in observational data. Discovering patterns in the data that does not belong to a predefined class aids in understanding this data and segmenting it into learnable classes. We demonstrate that activity discovery not only sheds light on behavioral patterns, but it can also boost the performance of recognition algorithms. We introduce this partnership between activity discovery and online activity recognition in the context of the CASAS smart home project and validate our approach using CASAS data sets. © 2012 IEEE.","Activity recognition; Out of vocabulary detection; Sequence discovery","Activity discoveries; Activity recognition; Behavioral patterns; Machine learning communities; Off-line approaches; Out-of-vocabulary detection; Recognition algorithm; Sequence discovery; Automation; Intelligent buildings; Pattern recognition; actimetry; algorithm; ambulatory monitoring; article; artificial intelligence; automated pattern recognition; daily life activity; human; methodology; movement (physiology); physiology; telemedicine; actimetry; ambulatory monitoring; automated pattern recognition; movement (physiology); physiology; procedures; telemedicine; Actigraphy; Activities of Daily Living; Algorithms; Artificial Intelligence; Humans; Monitoring, Ambulatory; Movement; Pattern Recognition, Automated; Telemedicine; Actigraphy; Activities of Daily Living; Algorithms; Artificial Intelligence; Humans; Monitoring, Ambulatory; Movement; Pattern Recognition, Automated; Telemedicine",Article,Scopus,2-s2.0-84881180779
"You Z.-H., Lei Y.-K., Zhu L., Xia J., Wang B.","Prediction of protein-protein interactions from amino acid sequences with ensemble extreme learning machines and principal component analysis",2013,"BMC Bioinformatics",84,10.1186/1471-2105-14-S8-S10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883162967&doi=10.1186%2f1471-2105-14-S8-S10&partnerID=40&md5=b908237c28dda72799275057850262c9","Background: Protein-protein interactions (PPIs) play crucial roles in the execution of various cellular processes and form the basis of biological mechanisms. Although large amount of PPIs data for different species has been generated by high-throughput experimental techniques, current PPI pairs obtained with experimental methods cover only a fraction of the complete PPI networks, and further, the experimental methods for identifying PPIs are both time-consuming and expensive. Hence, it is urgent and challenging to develop automated computational methods to efficiently and accurately predict PPIs.Results: We present here a novel hierarchical PCA-EELM (principal component analysis-ensemble extreme learning machine) model to predict protein-protein interactions only using the information of protein sequences. In the proposed method, 11188 protein pairs retrieved from the DIP database were encoded into feature vectors by using four kinds of protein sequences information. Focusing on dimension reduction, an effective feature extraction method PCA was then employed to construct the most discriminative new feature set. Finally, multiple extreme learning machines were trained and then aggregated into a consensus classifier by majority voting. The ensembling of extreme learning machine removes the dependence of results on initial random weights and improves the prediction performance.Conclusions: When performed on the PPI data of Saccharomyces cerevisiae, the proposed method achieved 87.00% prediction accuracy with 86.15% sensitivity at the precision of 87.59%. Extensive experiments are performed to compare our method with state-of-the-art techniques Support Vector Machine (SVM). Experimental results demonstrate that proposed PCA-EELM outperforms the SVM method by 5-fold cross-validation. Besides, PCA-EELM performs faster than PCA-SVM based method. Consequently, the proposed approach can be considered as a new promising and powerful tools for predicting PPI with excellent performance and less time. © 2013 You et al.; licensee BioMed Central Ltd.",,"Biological mechanisms; Experimental methods; Experimental techniques; Extreme learning machine; Feature extraction methods; Principal Components; Protein-protein interactions; State-of-the-art techniques; Amino acids; Feature extraction; Forecasting; Knowledge acquisition; Principal component analysis; Support vector machines; Yeast; Proteins; amino acid sequence; article; artificial intelligence; Helicobacter pylori; metabolism; principal component analysis; protein analysis; protein protein interaction; Saccharomyces cerevisiae; support vector machine; Amino Acid Sequence; Artificial Intelligence; Helicobacter pylori; Principal Component Analysis; Protein Interaction Mapping; Protein Interaction Maps; Saccharomyces cerevisiae; Support Vector Machines",Article,Scopus,2-s2.0-84883162967
"Imran M., Elbassuoni S., Castillo C., Diaz F., Meier P.","Extracting information nuggets from disaster- Related messages in social media",2013,"ISCRAM 2013 Conference Proceedings - 10th International Conference on Information Systems for Crisis Response and Management",84,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905650414&partnerID=40&md5=1bee829b30b7b4fc5cef786436b63bbf","Microblogging sites such as Twitter can play a vital role in spreading information during ""natural"" or man-made disasters. But the volume and velocity of tweets posted during crises today tend to be extremely high, making it hard for disaster-affected communities and professional emergency responders to process the information in a timely manner. Furthermore, posts tend to vary highly in terms of their subjects and usefulness; from messages that are entirely off-topic or personal in nature, to messages containing critical information that augments situational awareness. Finding actionable information can accelerate disaster response and alleviate both property and human losses. In this paper, we describe automatic methods for extracting information from microblog posts. Specifically, we focus on extracting valuable ""information nuggets"", brief, self-contained information items relevant to disaster response. Our methods leverage machine learning methods for classifying posts and information extraction. Our results, validated over one large disaster-related dataset, reveal that a careful design can yield an effective system, paving the way for more sophisticated data analysis and visualization systems.","Information extraction; Social media; Supervised classification; Twitter","Artificial intelligence; Data visualization; Disasters; Information retrieval; Information systems; Learning systems; Social networking (online); Emergency responders; Extracting information; Machine learning methods; Situational awareness; Social media; Supervised classification; Twitter; Visualization system; Emergency services",Conference Paper,Scopus,2-s2.0-84905650414
"Wen S., Bao G., Zeng Z., Chen Y., Huang T.","Global exponential synchronization of memristor-based recurrent neural networks with time-varying delays",2013,"Neural Networks",82,10.1016/j.neunet.2013.10.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887376596&doi=10.1016%2fj.neunet.2013.10.001&partnerID=40&md5=73b1414a9225ce73ae659d14f0e07a92","This paper deals with the problem of global exponential synchronization of a class of memristor-based recurrent neural networks with time-varying delays based on the fuzzy theory and Lyapunov method. First, a memristor-based recurrent neural network is designed. Then, considering the state-dependent properties of the memristor, a new fuzzy model employing parallel distributed compensation (PDC) gives a new way to analyze the complicated memristor-based neural networks with only two subsystems. Comparisons between results in this paper and in the previous ones have been made. They show that the results in this paper improve and generalized the results derived in the previous literature. An example is also given to illustrate the effectiveness of the results. © 2013 Elsevier Ltd.","Memristor; Recurrent neural networks; Synchronization; Time-varying delays","Fuzzy modeling; Fuzzy theory; Global exponential synchronizations; Memristor; Parallel distributed compensation; State-dependent; Time-varying delay; Lyapunov methods; Passive filters; Recurrent neural networks; Synchronization; Time varying control systems; Memristors; article; artificial neural network; fuzzy system; global exponential synchronization; information processing; Lyapunov method; machine learning; mathematical computing; memristor based recurrent neural network; priority journal; process optimization; system analysis; Memristor; Recurrent neural networks; Synchronization; Time-varying delays; Artificial Intelligence; Computer Simulation; Fuzzy Logic; Models, Neurological; Neural Networks (Computer); Nonlinear Dynamics",Article,Scopus,2-s2.0-84887376596
"Borkin M.A., Vo A.A., Bylinskii Z., Isola P., Sunkavalli S., Oliva A., Pfister H.","What makes a visualization memorable",2013,"IEEE Transactions on Visualization and Computer Graphics",82,10.1109/TVCG.2013.234,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886711492&doi=10.1109%2fTVCG.2013.234&partnerID=40&md5=2fe1f2b941fb18e14d89bc17a26e1895","An ongoing debate in the Visualization community concerns the role that visualization types play in data understanding. In human cognition, understanding and memorability are intertwined. As a first step towards being able to ask questions about impact and effectiveness, here we ask: 'What makes a visualization memorable' We ran the largest scale visualization study to date using 2,070 single-panel visualizations, categorized with visualization type (e.g., bar chart, line graph, etc.), collected from news media sites, government reports, scientific journals, and infographic sources. Each visualization was annotated with additional attributes, including ratings for data-ink ratios and visual densities. Using Amazon's Mechanical Turk, we collected memorability scores for hundreds of these visualizations, and discovered that observers are consistent in which visualizations they find memorable and forgettable. We find intuitive results (e.g., attributes like color and the inclusion of a human recognizable object enhance memorability) and less intuitive results (e.g., common graphs are less memorable than unique visualization types). Altogether our findings suggest that quantifying memorability is a general metric of the utility of information, an essential step towards determining how to design effective visualizations. © 2013 IEEE.","information visualization; memorability; Visualization taxonomy","Amazon's mechanical turks; Data understanding; Government reports; Human cognition; Information visualization; memorability; Scientific journals; Utility of information; Data visualization; Graph theory; Information systems; Visualization; article; artificial intelligence; association; computer assisted diagnosis; computer interface; human; memory; methodology; pattern recognition; physiology; task performance; computer assisted diagnosis; memory; pattern recognition; physiology; procedures; Artificial Intelligence; Cues; Humans; Image Interpretation, Computer-Assisted; Memory; Pattern Recognition, Visual; Task Performance and Analysis; User-Computer Interface; Artificial Intelligence; Cues; Humans; Image Interpretation, Computer-Assisted; Memory; Pattern Recognition, Visual; Task Performance and Analysis; User-Computer Interface",Article,Scopus,2-s2.0-84886711492
"Yu C., Lumezanu C., Zhang Y., Singh V., Jiang G., Madhyastha H.V.","FlowSense: Monitoring network utilization with zero measurement cost",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",82,10.1007/978-3-642-36516-4-4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875613505&doi=10.1007%2f978-3-642-36516-4-4&partnerID=40&md5=2ae85ab02640993b48eaf3a84ac6ae0f","Flow-based programmable networks must continuously monitor performance metrics, such as link utilization, in order to quickly adapt forwarding rules in response to changes in workload. However, existing monitoring solutions either require special instrumentation of the network or impose significant measurement overhead. In this paper, we propose a push-based approach to performance monitoring in flow-based networks, where we let the network inform us of performance changes, rather than query it ourselves on demand. Our key insight is that control messages sent by switches to the controller carry information that allows us to estimate performance. In OpenFlow networks, PacketIn and FlowRemoved messages - sent by switches to the controller upon the arrival of a new flow or upon the expiration of a flow entry, respectively - enable us to compute the utilization of links between switches. We conduct a) experiments on a real testbed, and b) simulations with real enterprise traces, to show accuracy, and that it can refresh utilization information frequently (e.g., at most every few seconds) given a constant stream of control messages. Since the number of control messages may be limited by the properties of traffic (e.g., long flows trigger sparse FlowRemoved's) or by the choices made by operators (e.g., proactive or wildcard rules eliminate or limit PacketIn's), we discuss how our proposed passive approach can be combined with active approaches with low overhead. © 2013 Springer-Verlag Berlin Heidelberg.",,"Control messages; Low overhead; Measurement costs; Monitoring network; Openflow networks; Performance metrics; Performance monitoring; Programmable network; Artificial intelligence; Controllers",Conference Paper,Scopus,2-s2.0-84875613505
"Kabra M., Robie A.A., Rivera-Alba M., Branson S., Branson K.","JAABA: Interactive machine learning for automatic annotation of animal behavior",2013,"Nature Methods",82,10.1038/nmeth.2281,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872005234&doi=10.1038%2fnmeth.2281&partnerID=40&md5=d808ec25c85aa8b30a11425780af3a13","We present a machine learning-based system for automatically computing interpretable, quantitative measures of animal behavior. Through our interactive system, users encode their intuition about behavior by annotating a small set of video frames. These manual labels are converted into classifiers that can automatically annotate behaviors in screen-scale data sets. Our general-purpose system can create a variety of accurate individual and social behavior classifiers for different organisms, including mice and adult and larval Drosophila. © 2013 Nature America, Inc. All rights reserved.",,"animal behavior; article; automation; Drosophila; Janelia Automatic Animal Behavior Annotator; larva; machine learning; nonhuman; online system; priority journal; quantitative analysis; social behavior; species; videorecording; Algorithms; Animals; Artificial Intelligence; Behavior, Animal; Diagnosis, Computer-Assisted; Drosophila melanogaster; Larva; Mice; Animalia; Mus",Article,Scopus,2-s2.0-84872005234
"Song Q., Liu F., Cao J., Yu W.","M-matrix strategies for pinning-controlled leader-following consensus in multiagent systems with nonlinear dynamics",2013,"IEEE Transactions on Cybernetics",81,10.1109/TSMCB.2012.2227723,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890049700&doi=10.1109%2fTSMCB.2012.2227723&partnerID=40&md5=3a099e49bbfdb5927c484343fcd99905","This paper considers the leader-following consensus problem for multiagent systems with inherent nonlinear dynamics. Some M-matrix strategies are developed to address several challenging issues in the pinning control of multiagent systems by using algebraic graph theory and the properties of nonnegative matrices. It is shown that second-order leader-following consensus in a nonlinear multiagent system can be reached if the virtual leader has a directed path to every follower and a derived quantity is greater than a positive threshold. In particular, this paper analytically proves that leader-following consensus may be easier to be achieved by pinning more agents or increasing the pinning feedback gains. A selective pinning scheme is then proposed for nonlinear multiagent systems with directed network topologies. Numerical results are given to verify the theoretical analysis. © 2013 IEEE.","Directed spanning tree; leader-following consensus; M-matrix; multiagent system; pinning control","Algebraic graph theory; Directed network topology; Directed spanning trees; Leader following; M-matrix; Non-negative matrix; Nonlinear multi-agent systems; Pinning control; Dynamics; Electric network topology; Graph theory; Multi agent systems; Electric ship equipment; algorithm; article; artificial intelligence; automated pattern recognition; computer simulation; decision support system; methodology; nonlinear system; theoretical model; Algorithms; Artificial Intelligence; Computer Simulation; Decision Support Techniques; Models, Theoretical; Nonlinear Dynamics; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84890049700
"Asman A.J., Landman B.A.","Non-local statistical label fusion for multi-atlas segmentation",2013,"Medical Image Analysis",81,10.1016/j.media.2012.10.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883291089&doi=10.1016%2fj.media.2012.10.002&partnerID=40&md5=f3e3ce0908c10d267528d270a013959b","Multi-atlas segmentation provides a general purpose, fully-automated approach for transferring spatial information from an existing dataset ("" atlases"") to a previously unseen context ("" target"") through image registration. The method to resolve voxelwise label conflicts between the registered atlases ("" label fusion"") has a substantial impact on segmentation quality. Ideally, statistical fusion algorithms (e.g., STAPLE) would result in accurate segmentations as they provide a framework to elegantly integrate models of rater performance. The accuracy of statistical fusion hinges upon accurately modeling the underlying process of how raters err. Despite success on human raters, current approaches inaccurately model multi-atlas behavior as they fail to seamlessly incorporate exogenous intensity information into the estimation process. As a result, locally weighted voting algorithms represent the de facto standard fusion approach in clinical applications. Moreover, regardless of the approach, fusion algorithms are generally dependent upon large atlas sets and highly accurate registration as they implicitly assume that the registered atlases form a collectively unbiased representation of the target. Herein, we propose a novel statistical fusion algorithm, Non-Local STAPLE (NLS). NLS reformulates the STAPLE framework from a non-local means perspective in order to learn what label an atlas would have observed, given perfect correspondence. Through this reformulation, NLS (1) seamlessly integrates intensity into the estimation process, (2) provides a theoretically consistent model of multi-atlas observation error, and (3) largely diminishes the need for large atlas sets and very high-quality registrations. We assess the sensitivity and optimality of the approach and demonstrate significant improvement in two empirical multi-atlas experiments. © 2012 Elsevier B.V.","Label fusion; Multi-atlas segmentation; Non-local means; Rater models; STAPLE","Clinical application; Intensity information; Label fusions; Non-local means; Segmentation quality; Spatial informations; STAPLE; Statistical label fusions; Image registration; Algorithms; accuracy; algorithm; article; computer assisted tomography; controlled study; human; image processing; information processing; multi atlas segmentation; neuroimaging; nuclear magnetic resonance imaging; partition coefficient; priority journal; registration; thyroid gland; algorithm; artificial intelligence; audiovisual equipment; automated pattern recognition; computer assisted diagnosis; computer simulation; image enhancement; image subtraction; methodology; reproducibility; sensitivity and specificity; statistical analysis; Algorithms; Artificial Intelligence; Computer Simulation; Data Interpretation, Statistical; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Models, Anatomic; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-84883291089
"Lu S., Hillmansen S., Ho T.K., Roberts C.","Single-train trajectory optimization",2013,"IEEE Transactions on Intelligent Transportation Systems",81,10.1109/TITS.2012.2234118,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878741979&doi=10.1109%2fTITS.2012.2234118&partnerID=40&md5=a0c25821b67a9c4d4d9b0f9721b9f42f","An energy-efficient train trajectory describing the motion of a single train can be used as an input to a driver guidance system or to an automatic train control system. The solution for the best trajectory is subject to certain operational, geographic, and physical constraints. There are two types of strategies commonly applied to obtain the energy-efficient trajectory. One is to allow the train to coast, thus using its available time margin to save energy. The other one is to control the speed dynamically while maintaining the required journey time. This paper proposes a distance-based train trajectory searching model, upon which three optimization algorithms are applied to search for the optimum train speed trajectory. Instead of searching for a detailed complicated control input for the train traction system, this model tries to obtain the speed level at each preset position along the journey. Three commonly adopted algorithms are extensively studied in a comparative style. It is found that the ant colony optimization (ACO) algorithm obtains better balance between stability and the quality of the results, in comparison with the genetic algorithm (GA). For offline applications, the additional computational effort required by dynamic programming (DP) is outweighed by the quality of the solution. It is recommended that multiple algorithms should be used to identify the optimum single-train trajectory and to improve the robustness of searched results. © 2000-2011 IEEE.","Ant colony optimization (ACO); dynamic programming (DP); energy saving strategy; rail traction systems; single-train trajectory","Ant Colony Optimization (ACO); Ant Colony Optimization algorithms; Energy-saving strategies; Off-line applications; Optimization algorithms; Rail traction; Train traction systems; Trajectory optimization; Ant colony optimization; Artificial intelligence; Constrained optimization; Dynamic positioning; Dynamic programming; Energy efficiency; Genetic algorithms; Traction control; Trajectories; Driver training",Article,Scopus,2-s2.0-84878741979
"Gennaro R., Gentry C., Parno B., Raykova M.","Quadratic span programs and succinct NIZKs without PCPs",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",80,10.1007/978-3-642-38348-9_37,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883413063&doi=10.1007%2f978-3-642-38348-9_37&partnerID=40&md5=9283e9d30a213cd37b6a83d143eaa593","We introduce a new characterization of the NP complexity class, called Quadratic Span Programs (QSPs), which is a natural extension of span programs defined by Karchmer and Wigderson. Our main motivation is the quick construction of succinct, easily verified arguments for NP statements. To achieve this goal, QSPs use a new approach to the well-known technique of arithmetization of Boolean circuits. Our new approach yields dramatic performance improvements. Using QSPs, we construct a NIZK argument - in the CRS model - for Circuit-SAT consisting of just 7 group elements. The CRS size and prover computation are quasi-linear, making our scheme seemingly quite practical, a result supported by our implementation. Indeed, our NIZK argument attains the shortest proof, most efficient prover, and most efficient verifier of any known technique. We also present a variant of QSPs, called Quadratic Arithmetic Programs (QAPs), that ""naturally"" compute arithmetic circuits over large fields, along with succinct NIZK constructions that use QAPs. Finally, we show how QSPs and QAPs can be used to efficiently and publicly verify outsourced computations, where a client asks a server to compute F(x) for a given function F and must verify the result provided by the server in considerably less time than it would take to compute F from scratch. The resulting schemes are the most efficient, general purpose publicly verifiable computation schemes. © 2013 International Association for Cryptologic Research.",,"Arithmetic circuit; Boolean circuit; Complexity class; Natural extension; New approaches; Publicly verifiable; Quadratic span; Span programs; Artificial intelligence; Computer science; Cryptography",Conference Paper,Scopus,2-s2.0-84883413063
"Luo H., Du B., Huang G.Q., Chen H., Li X.","Hybrid flow shop scheduling considering machine electricity consumption cost",2013,"International Journal of Production Economics",79,10.1016/j.ijpe.2013.01.028,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886724001&doi=10.1016%2fj.ijpe.2013.01.028&partnerID=40&md5=b7d9a5282b106c3444f0cfacf2ba70d8","Hybrid flow shop (HFS) scheduling has been extensively examined and the main objective has been to improve production efficiency. However, limited attention has been paid to the consideration of energy consumption with the advent of green manufacturing. This paper proposes a new ant colony optimization (MOACO) meta-heuristic considering not only production efficiency but also electric power cost (EPC) with the presence of time-of-use (TOU) electricity prices. The solution is encoded as a permutation of jobs. A list schedule algorithm is applied to construct the sequence by artificial ants and generate a complete schedule. A right-shift procedure is then used to adjust the start time of operations aiming to minimize the EPC for the schedule. In terms of theoretical research aspect, the results from computational experiments indicate that the efficiency and effectiveness of the proposed MOACO are comparable to NSGA-II and SPEA2. In terms of practical application aspect, the guideline about how to set preference over multiple objectives has been studied. This result has significant managerial implications in real life production. The parameter analysis also shows that durations of TOU periods and processing speed of machines have great influence on scheduling results as longer off-peak period and use of faster machines provide more flexibility for shifting high-energy operations to off-peak periods. © 2013 Elsevier B.V.","Ant colony optimization; Electric power cost; Hybrid flow shop; Makespan; Multi-objective optimization; Scheduling","Computational experiment; Electric power; Electricity-consumption; Hybrid flow shop; Hybrid flow shop scheduling; Hybrid flow-shop scheduling (HFS); Makespan; Managerial implications; Ant colony optimization; Artificial intelligence; Costs; Efficiency; Electricity; Energy utilization; Multiobjective optimization; Scheduling algorithms; Scheduling",Article,Scopus,2-s2.0-84886724001
"Leaman R., Doǧan R.I., Lu Z.","DNorm: Disease name normalization with pairwise learning to rank",2013,"Bioinformatics",79,10.1093/bioinformatics/btt474,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890064920&doi=10.1093%2fbioinformatics%2fbtt474&partnerID=40&md5=7b74779fbc4360028d49c58f3d698ae5","Motivation: Despite the central role of diseases in biomedical research, there have been much fewer attempts to automatically determine which diseases are mentioned in a text-the task of disease name normalization (DNorm)-compared with other normalization tasks in biomedical text mining research. Methods: In this article we introduce the first machine learning approach for DNorm, using the NCBI disease corpus and the MEDIC vocabulary, which combines MeSH® and OMIM. Our method is a high-performing and mathematically principled framework for learning similarities between mentions and concept names directly from training data. The technique is based on pairwise learning to rank, which has not previously been applied to the normalization task but has proven successful in large optimization problems for information retrieval. Results: We compare our method with several techniques based on lexical normalization and matching, MetaMap and Lucene. Our algorithm achieves 0.782 micro-averaged F-measure and 0.809 macroaveraged F-measure, an increase over the highest performing baseline method of 0.121 and 0.098, respectively. © The Author 2013. Published by Oxford University Press. All rights reserved.",,"algorithm; article; artificial intelligence; controlled vocabulary; data mining; diseases; human; Internet; Medline; nomenclature; Algorithms; Artificial Intelligence; Data Mining; Disease; Humans; Internet; PubMed; Terminology as Topic; Vocabulary, Controlled; Algorithms; Artificial Intelligence; Data Mining; Disease; Humans; Internet; PubMed; Terminology as Topic; Vocabulary, Controlled",Article,Scopus,2-s2.0-84890064920
"Liu S., Yamada M., Collier N., Sugiyama M.","Change-point detection in time-series data by relative density-ratio estimation",2013,"Neural Networks",79,10.1016/j.neunet.2013.01.012,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875257444&doi=10.1016%2fj.neunet.2013.01.012&partnerID=40&md5=8ccdd3a6fe71318c6ca047f1503fa58b","The objective of change-point detection is to discover abrupt property changes lying behind time-series data. In this paper, we present a novel statistical change-point detection algorithm based on non-parametric divergence estimation between time-series samples from two retrospective segments. Our method uses the relative Pearson divergence as a divergence measure, and it is accurately and efficiently estimated by a method of direct density-ratio estimation. Through experiments on artificial and real-world datasets including human-activity sensing, speech, and Twitter messages, we demonstrate the usefulness of the proposed method. © 2013 Elsevier Ltd.","Change-point detection; Distribution comparison; Kernel methods; Relative density-ratio estimation; Time-series data","Change point detection; Distribution comparisons; Kernel methods; Relative density-ratio estimations; Time-series data; Artificial intelligence; Cognitive systems; Estimation; article; change point detection; human activities; Internet; learning algorithm; mathematical computing; nonparametric test; priority journal; regression analysis; relative density ratio estimation; speech; time series analysis; Algorithms; Gene Expression Profiling; Humans; Models, Statistical; Retrospective Studies; Specific Gravity; Time Factors",Article,Scopus,2-s2.0-84875257444
"Chavarriaga R., Sagha H., Calatroni A., Digumarti S.T., Tröster G., Millán J.D.R., Roggen D.","The Opportunity challenge: A benchmark database for on-body sensor-based activity recognition",2013,"Pattern Recognition Letters",77,10.1016/j.patrec.2012.12.014,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885071495&doi=10.1016%2fj.patrec.2012.12.014&partnerID=40&md5=37e1fbdb14ac01ba0aef190cf3f5e705","There is a growing interest on using ambient and wearable sensors for human activity recognition, fostered by several application domains and wider availability of sensing technologies. This has triggered increasing attention on the development of robust machine learning techniques that exploits multimodal sensor setups. However, unlike other applications, there are no established benchmarking problems for this field. As a matter of fact, methods are usually tested on custom datasets acquired in very specific experimental setups. Furthermore, data is seldom shared between different groups. Our goal is to address this issue by introducing a versatile human activity dataset recorded in a sensor-rich environment. This database was the basis of an open challenge on activity recognition. We report here the outcome of this challenge, as well as baseline performance using different classification techniques. We expect this benchmarking database will motivate other researchers to replicate and outperform the presented results, thus contributing to further advances in the state-of-the-art of activity recognition methods. © 2012 Elsevier B.V. All rights reserved.","Activity recognition; Body-sensor networks; Machine learning; Metrics; Performance evaluation; ROC analysis","Artificial intelligence; Benchmarking; Body sensor networks; Database systems; Learning systems; Pattern recognition; Wearable technology; Activity recognition; Base-line performance; Classification technique; Human activity recognition; Machine learning techniques; Metrics; Performance evaluation; ROC analysis; Wearable sensors",Article,Scopus,2-s2.0-84885071495
"Zhang J., Liu B., Tang J., Chen T., Li J.","Social influence locality for modeling retweeting behaviors",2013,"IJCAI International Joint Conference on Artificial Intelligence",76,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896061359&partnerID=40&md5=6ba5ccab89f050a559716be512b91c66","We study an interesting phenomenon of social influence locality in a large microblogging network, which suggests that users' behaviors are mainly influenced by close friends in their ego networks. We provide a formal definition for the notion of social influence locality and develop two instantiation functions based on pairwise influence and structural diversity. The defined influence locality functions have strong predictive power. Without any additional features, we can obtain a F1-score of 71.65% for predicting users' retweet behaviors by training a logistic regression classifier based on the defined functions. Our analysis also reveals several intriguing discoveries. For example, though the probability of a user retweeting a microblog is positively correlated with the number of friends who have retweeted the microblog, it is surprisingly negatively correlated with the number of connected circles that are formed by those friends.",,"Defined functions; Ego networks; Formal definition; Logistic regression classifier; Microblogging; Predictive power; Social influence; Structural diversity; Artificial intelligence; Economic and social effects",Conference Paper,Scopus,2-s2.0-84896061359
"He R., Zheng W.-S., Hu B.-G., Kong X.-W.","Two-stage nonnegative sparse representation for large-scale face recognition",2013,"IEEE Transactions on Neural Networks and Learning Systems",76,10.1109/TNNLS.2012.2226471,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884945640&doi=10.1109%2fTNNLS.2012.2226471&partnerID=40&md5=b85b0e9c9b865077e6cac7288434c25a","This paper proposes a novel nonnegative sparse representation approach, called two-stage sparse representation (TSR), for robust face recognition on a large-scale database. Based on the divide and conquer strategy, TSR decomposes the procedure of robust face recognition into outlier detection stage and recognition stage. In the first stage, we propose a general multisubspace framework to learn a robust metric in which noise and outliers in image pixels are detected. Potential loss functions, including L1, L 2,1, and correntropy are studied. In the second stage, based on the learned metric and collaborative representation, we propose an efficient nonnegative sparse representation algorithm to find an approximation solution of sparse representation. According to the L1 ball theory in sparse representation, the approximated solution is unique and can be optimized efficiently. Then a filtering strategy is developed to avoid the computation of the sparse representation on the whole large-scale dataset. Moreover, theoretical analysis also gives the necessary condition for nonnegative least squares technique to find a sparse solution. Extensive experiments on several public databases have demonstrated that the proposed TSR approach, in general, achieves better classification accuracy than the state-of-the-art sparse representation methods. More importantly, a significant reduction of computational costs is reached in comparison with sparse representation classifier; this enables the TSR to be more suitable for robust face recognition on a large-scale dataset. © 2012 IEEE.","Correntropy; L1 regularization; Large-scale; Nonnegative sparse representation; Robust face recognition","Approximation solution; Classification accuracy; Collaborative representations; Correntropy; L<sub>1</sub> regularization; Large-scale; Nonnegative least squares; Sparse representation; Approximation algorithms; Statistics; Face recognition; algorithm; anatomy and histology; artificial intelligence; automated pattern recognition; computer assisted diagnosis; face; image enhancement; procedures; standards; Algorithms; Artificial Intelligence; Face; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84884945640
"Chan C.H., Tahir M.A., Kittler J., Pietikäinen M.","Multiscale local phase quantization for robust component-based face recognition using kernel fusion of multiple descriptors",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",76,10.1109/TPAMI.2012.199,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875461472&doi=10.1109%2fTPAMI.2012.199&partnerID=40&md5=008390b11a550d5ff988d04470154a68","Face recognition subject to uncontrolled illumination and blur is challenging. Interestingly, image degradation caused by blurring, often present in real-world imagery, has mostly been overlooked by the face recognition community. Such degradation corrupts face information and affects image alignment, which together negatively impact recognition accuracy. We propose a number of countermeasures designed to achieve system robustness to blurring. First, we propose a novel blur-robust face image descriptor based on Local Phase Quantization (LPQ) and extend it to a multiscale framework (MLPQ) to increase its effectiveness. To maximize the insensitivity to misalignment, the MLPQ descriptor is computed regionally by adopting a component-based framework. Second, the regional features are combined using kernel fusion. Third, the proposed MLPQ representation is combined with the Multiscale Local Binary Pattern (MLBP) descriptor using kernel fusion to increase insensitivity to illumination. Kernel Discriminant Analysis (KDA) of the combined features extracts discriminative information for face recognition. Last, two geometric normalizations are used to generate and combine multiple scores from different face image scales to further enhance the accuracy. The proposed approach has been comprehensively evaluated using the combined Yale and Extended Yale database B (degraded by artificially induced linear motion blur) as well as the FERET, FRGC 2.0, and LFW databases. The combined system is comparable to state-of-the-art approaches using similar system configurations. The reported work provides a new insight into the merits of various face representation and fusion methods, as well as their role in dealing with variable lighting and blur degradation. © 1979-2012 IEEE.","face image representation; Face recognition; kernel discriminant analysis; kernel fusion; local binary pattern; local phase quantization","Face images; Kernel discriminant analysis; Kernel fusion; Local binary patterns; Local phase quantizations; Artificial intelligence; Computer vision; Face recognition; algorithm; article; artificial intelligence; biometry; discriminant analysis; face; factual database; histology; human; image processing; methodology; Algorithms; Artificial Intelligence; Biometric Identification; Databases, Factual; Discriminant Analysis; Face; Humans; Image Processing, Computer-Assisted",Article,Scopus,2-s2.0-84875461472
"Dimitrov S., Haas H.","Information rate of OFDM-based optical wireless communication systems with nonlinear distortion",2013,"Journal of Lightwave Technology",76,10.1109/JLT.2012.2236642,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873817112&doi=10.1109%2fJLT.2012.2236642&partnerID=40&md5=25cf10e6b954bb33dabd6b0beeab153d","In this paper, a piecewise polynomial function is proposed as a generalized model for the nonlinear transfer characteristic of the transmitter for optical wireless communications (OWC). The two general multicarrier modulation formats for OWC based on orthogonal frequency-division multiplexing (OFDM), direct-current-biased optical OFDM (DCO-OFDM) and asymmetrically clipped optical OFDM (ACO-OFDM), are studied. The nonlinear distortion of the electrical signal-to-noise ratio (SNR) at the receiver is derived in closed form, and it is verified by means of a Monte Carlo simulation. This flexible and accurate model allows for the application of pre-distortion and linearization of the dynamic range of the transmitter between points of minimum and maximum radiated optical power. Through scaling and DC-biasing the transmitted signal is optimally conditioned in accord with the optical power constraints of the transmitter front-end, i.e., minimum, average and maximum radiated optical power. The mutual information of the optimized optical OFDM (O-OFDM) schemes is presented as a measure of the capacity of these OWC systems under an average electrical power constraint. When the additional DC bias power is neglected, DCO-OFDM is shown to achieve the Shannon capacity when the optimization is employed, while ACO-OFDM exhibits a 3-dB gap which grows with higher information rate targets. When the DC bias power is counted towards the signal power, DCO-OFDM outperforms ACO-OFDM for the majority of average optical power levels with the increase of the information rate target or the dynamic range. The results can be considered as a lower bound on the O-OFDM system capacity. © 1983-2012 IEEE.","Mutual information; nonlinear distortion; optical devices; orthogonal frequency-division multiplexing (OFDM); wireless communication","Closed form; DC bias; Dynamic range; Electrical power; Generalized models; Higher information rate; Information rates; Lower bounds; Monte Carlo Simulation; Mutual informations; Nonlinear transfer; Optical OFDM; Optical power; Optical wireless communication systems; Optical wireless communications; Piecewise polynomial functions; Pre-distortion; Shannon capacity; Signal power; Signaltonoise ratio (SNR); System Capacity; Transmitted signal; Wireless communications; Artificial intelligence; Distortion (waves); Monte Carlo methods; Nonlinear distortion; Optical devices; Optical fiber communication; Optimization; Transmitters; Wireless telecommunication systems; Orthogonal frequency division multiplexing",Article,Scopus,2-s2.0-84873817112
"Coello C.A.C., Becerra R.L.","Evolutionary multiobjective optimization using a cultural algorithm",2013,"2003 IEEE Swarm Intelligence Symposium, SIS 2003 - Proceedings",76,10.1109/SIS.2003.1202240,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942116158&doi=10.1109%2fSIS.2003.1202240&partnerID=40&md5=39e930280ed1b1a82f076f2d409cac3b","In this paper, we present the first proposal to use a cultural algorithm to solve multiobjective optimization problems. Our proposal uses evolutionary programming, Pareto ranking and elitism (i.e., an external population). The approach proposed is validated using several examples taken from the specialized literature. Our results are compared with respect to the NSGA-II, which is an algorithm representative of the state-of-the-art in evolutionary multiobjective optimization. The performance of our approach indicates that cultural algorithms are a viable alternative for multiobjective optimization. © 2003 IEEE.","Constraint optimization; Cultural differences; Evolutionary computation; Genetic programming; Mathematical programming; Pareto optimization; Proposals; Sociology; Zinc","Algorithms; Artificial intelligence; Computer programming; Constrained optimization; Evolutionary algorithms; Genetic algorithms; Genetic programming; Mathematical programming; Optimization; Pareto principle; Zinc; Constraint optimizations; Cultural difference; Pareto optimization; Proposals; Sociology; Multiobjective optimization",Conference Paper,Scopus,2-s2.0-84942116158
"Wu Z.-G., Shi P., Su H., Chu J.","Dissipativity analysis for discrete-time stochastic neural networks with time-varying delays",2013,"IEEE Transactions on Neural Networks and Learning Systems",75,10.1109/TNNLS.2012.2232938,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884919926&doi=10.1109%2fTNNLS.2012.2232938&partnerID=40&md5=897b731bc818c27b79ce222f1ebb4b14","In this paper, the problem of dissipativity analysis is discussed for discrete-time stochastic neural networks with time-varying discrete and finite-distributed delays. The discretized Jensen inequality and lower bounds lemma are adopted to deal with the involved finite sum quadratic terms, and a sufficient condition is derived to ensure the considered neural networks to be globally asymptotically stable in the mean square and strictly (Q, S, R)-γ-dissipative, which is delay-dependent in the sense that it depends on not only the discrete delay but also the finite-distributed delay. Based on the dissipativity criterion, some special cases are also discussed. Compared with the existing ones, the merit of the proposed results in this paper lies in their reduced conservatism and less decision variables. Three examples are given to illustrate the effectiveness and benefits of our theoretical results. © 2013 IEEE.","Delay-dependent; Dissipativity; Neural networks; Stochastic systems; Time-delays","Decision variables; Delay-dependent; Dissipativity; Dissipativity analysis; Globally asymptotically stable; Jensen inequality; Stochastic neural network; Time-varying delay; Artificial intelligence; Computer networks; Stochastic systems; Neural networks; artificial neural network; statistics; time; Neural Networks (Computer); Stochastic Processes; Time Factors",Article,Scopus,2-s2.0-84884919926
"Suk H.-I., Lee S.-W.","A novel bayesian framework for discriminative feature extraction in brain-computer interfaces",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",75,10.1109/TPAMI.2012.69,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871786784&doi=10.1109%2fTPAMI.2012.69&partnerID=40&md5=17bff6a8324d434f9ea611c48823d82f","As there has been a paradigm shift in the learning load from a human subject to a computer, machine learning has been considered as a useful tool for Brain-Computer Interfaces (BCIs). In this paper, we propose a novel Bayesian framework for discriminative feature extraction for motor imagery classification in an EEG-based BCI in which the class-discriminative frequency bands and the corresponding spatial filters are optimized by means of the probabilistic and information-theoretic approaches. In our framework, the problem of simultaneous spatiospectral filter optimization is formulated as the estimation of an unknown posterior probability density function (pdf) that represents the probability that a single-trial EEG of predefined mental tasks can be discriminated in a state. In order to estimate the posterior pdf, we propose a particle-based approximation method by extending a factored-sampling technique with a diffusion process. An information-theoretic observation model is also devised to measure discriminative power of features between classes. From the viewpoint of classifier design, the proposed method naturally allows us to construct a spectrally weighted label decision rule by linearly combining the outputs from multiple classifiers. We demonstrate the feasibility and effectiveness of the proposed method by analyzing the results and its success on three public databases. © 2012 IEEE.","Brain-Computer Interface (BCI); Discriminative feature extraction; ElectroEncephaloGraphy (EEG); motor imagery classification; spatiospectral filter optimization","Approximation methods; Bayesian frameworks; Brain-computer interfaces (BCI); Classifier design; Decision rules; Diffusion process; Discriminative feature extraction; Filter optimization; Human subjects; Information-theoretic approach; Mental tasks; Motor imagery classification; Multiple classifiers; Observation model; Paradigm shifts; Posterior probability; Public database; Single-trial EEG; Spatial filters; Approximation theory; Electroencephalography; Electrophysiology; Feature extraction; Frequency bands; Information theory; Interfaces (computer); Optimization; Probability density function; Brain computer interface; algorithm; article; artificial intelligence; automated pattern recognition; Bayes theorem; brain computer interface; discriminant analysis; electroencephalography; human; imagination; methodology; motor cortex; movement (physiology); physiology; reproducibility; sensitivity and specificity; Algorithms; Artificial Intelligence; Bayes Theorem; Brain-Computer Interfaces; Discriminant Analysis; Electroencephalography; Humans; Imagination; Motor Cortex; Movement; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84871786784
"Binz T., Breitenbücher U., Haupt F., Kopp O., Leymann F., Nowak A., Wagner S.","OpenTOSCA - A runtime for TOSCA-based cloud applications",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",74,10.1007/978-3-642-45005-1_62,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892400015&doi=10.1007%2f978-3-642-45005-1_62&partnerID=40&md5=7304d12ed20f583823afbc54a9c08bca","TOSCA is a new standard facilitating platform independent description of Cloud applications. OpenTOSCA is a runtime for TOSCA-based Cloud applications. The runtime enables fully automated plan-based deployment and management of applications defined in the OASIS TOSCA packaging format CSAR. This paper outlines the core concepts of TOSCA and provides a system overview on OpenTOSCA by describing its modular and extensible architecture, as well as presenting our prototypical implementation. We demonstrate the use of OpenTOSCA by deploying and instantiating the school management and learning application Moodle. © 2013 Springer-Verlag.","Automation; Cloud Applications; Management; Portabilit; TOSCA","Cloud applications; Fully automated; Plan-based; Platform independent; Portabilit; Prototypical implementation; Runtimes; TOSCA; Automation; Computer science; Computers; Management; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84892400015
"Gao S., Kong S., Clarke E.M.","dReal: An SMT solver for nonlinear theories over the reals",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",73,10.1007/978-3-642-38574-2_14,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879951008&doi=10.1007%2f978-3-642-38574-2_14&partnerID=40&md5=a96b76886e4f8bddefe7fe9665c54528","We describe the open-source tool dReal, an SMT solver for nonlinear formulas over the reals. The tool can handle various nonlinear real functions such as polynomials, trigonometric functions, exponential functions, etc. dReal implements the framework of δ-complete decision procedures: It returns either unsat or δ-sat on input formulas, where δ is a numerical error bound specified by the user. dReal also produces certificates of correctness for both δ-sat (a solution) and unsat answers (a proof of unsatisfiability). © 2013 Springer-Verlag.",,"Decision procedure; Non-linear theory; Numerical errors; Open source tools; Real functions; Smt solvers; Trigonometric functions; Artificial intelligence; Computer science; Tools",Conference Paper,Scopus,2-s2.0-84879951008
"Hunter A.","A probabilistic approach to modelling uncertain logical arguments",2013,"International Journal of Approximate Reasoning",73,10.1016/j.ijar.2012.08.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870065608&doi=10.1016%2fj.ijar.2012.08.003&partnerID=40&md5=c506313bb5a2a8ae204f501f9506d408","Argumentation can be modelled at an abstract level using a directed graph where each node denotes an argument and each arc denotes an attack by one argument on another. Since arguments are often uncertain, it can be useful to quantify the uncertainty associated with each argument. Recently, there have been proposals to extend abstract argumentation to take this uncertainty into account. This assigns a probability value for each argument that represents the degree to which the argument is believed to hold, and this is then used to generate a probability distribution over the full subgraphs of the argument graph, which in turn can be used to determine the probability that a set of arguments is admissible or an extension. In order to more fully understand uncertainty in argumentation, in this paper, we extend this idea by considering logic-based argumentation with uncertain arguments. This is based on a probability distribution over models of the language, which can then be used to give a probability distribution over arguments that are constructed using classical logic. We show how this formalization of uncertainty of logical arguments relates to uncertainty of abstract arguments, and we consider a number of interesting classes of probability assignments. © 2012 Elsevier Inc. All rights reserved.","Argument systems; Computational models of argument; Logic-based argumentation; Logical arguments; Probabilistic argumentation; Uncertain arguments","Argument systems; Computational model; Logic-based argumentation; Logical arguments; Probabilistic argumentation; Uncertain arguments; Artificial intelligence; Software engineering; Probability distributions",Article,Scopus,2-s2.0-84870065608
"Singh H., Ansari H.R., Raghava G.P.S.","Improved Method for Linear B-Cell Epitope Prediction Using Antigen's Primary Sequence",2013,"PLoS ONE",72,10.1371/journal.pone.0062216,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877146650&doi=10.1371%2fjournal.pone.0062216&partnerID=40&md5=115a58eaa387d1a61b49c641c69362b6","One of the major challenges in designing a peptide-based vaccine is the identification of antigenic regions in an antigen that can stimulate B-cell's response, also called B-cell epitopes. In the past, several methods have been developed for the prediction of conformational and linear (or continuous) B-cell epitopes. However, the existing methods for predicting linear B-cell epitopes are far from perfection. In this study, an attempt has been made to develop an improved method for predicting linear B-cell epitopes. We have retrieved experimentally validated B-cell epitopes as well as non B-cell epitopes from Immune Epitope Database and derived two types of datasets called Lbtope_Variable and Lbtope_Fixed length datasets. The Lbtope_Variable dataset contains 14876 B-cell epitope and 23321 non-epitopes of variable length where as Lbtope_Fixed length dataset contains 12063 B-cell epitopes and 20589 non-epitopes of fixed length. We also evaluated the performance of models on above datasets after removing highly identical peptides from the datasets. In addition, we have derived third dataset Lbtope_Confirm having 1042 epitopes and 1795 non-epitopes where each epitope or non-epitope has been experimentally validated in at least two studies. A number of models have been developed to discriminate epitopes and non-epitopes using different machine-learning techniques like Support Vector Machine, and K-Nearest Neighbor. We achieved accuracy from ~54% to 86% using diverse s features like binary profile, dipeptide composition, AAP (amino acid pair) profile. In this study, for the first time experimentally validated non B-cell epitopes have been used for developing method for predicting linear B-cell epitopes. In previous studies, random peptides have been used as non B-cell epitopes. In order to provide service to scientific community, a web server LBtope has been developed for predicting and designing B-cell epitopes (http://crdd.osdd.net/raghava/lbtope/). © 2013 Singh et al.",,"dipeptide; epitope; amino acid composition; article; comparative study; controlled study; epitope mapping; k nearest neighbor; measurement accuracy; peptide analysis; reference database; sensitivity and specificity; sequence analysis; support vector machine; web browser; Amino Acid Sequence; Artificial Intelligence; Benchmarking; Computational Biology; Epitope Mapping; Epitopes, B-Lymphocyte; Physicochemical Processes",Article,Scopus,2-s2.0-84877146650
"Ke L., Zhang Q., Battiti R.","MOEA/D-ACO: A multiobjective evolutionary algorithm using decomposition and AntColony",2013,"IEEE Transactions on Cybernetics",71,10.1109/TSMCB.2012.2231860,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887023875&doi=10.1109%2fTSMCB.2012.2231860&partnerID=40&md5=08b0a28ae429be89778a4075572a7452","Combining ant colony optimization (ACO) and the multiobjective evolutionary algorithm (EA) based on decomposition (MOEA/D), this paper proposes a multiobjective EA, i.e., MOEA/D-ACO. Following other MOEA/D-like algorithms, MOEA/D-ACO decomposes a multiobjective optimization problem into a number of single-objective optimization problems. Each ant (i.e., agent) is responsible for solving one subproblem. All the ants are divided into a few groups, and each ant has several neighboring ants. An ant group maintains a pheromone matrix, and an individual ant has a heuristic information matrix. During the search, each ant also records the best solution found so far for its subproblem. To construct a new solution, an ant combines information from its group's pheromone matrix, its own heuristic information matrix, and its current solution. An ant checks the new solutions constructed by itself and its neighbors, and updates its current solution if it has found a better one in terms of its own objective. Extensive experiments have been conducted in this paper to study and compare MOEA/D-ACO with other algorithms on two sets of test problems. On the multiobjective 0-1 knapsack problem, MOEA/D-ACO outperforms the MOEA/D with conventional genetic operators and local search on all the nine test instances. We also demonstrate that the heuristic information matrices in MOEA/D-ACO are crucial to the good performance of MOEA/D-ACO for the knapsack problem. On the biobjective traveling salesman problem, MOEA/D-ACO performs much better than the BicriterionAnt on all the 12 test instances. We also evaluate the effects of grouping, neighborhood, and the location information of current solutions on the performance of MOEA/D-ACO. The work in this paper shows that reactive search optimization scheme, i.e., the 'learning while optimizing' principle, is effective in improving multiobjective optimization algorithms. © 2013 IEEE.","Ant colony optimization (ACO); Decomposition; Multiobjective 0-1 knapsack problem (MOKP); Multiobjective optimization; Multiobjective traveling salesman problem (MTSP); Reactive search optimization (RSO)","Ant Colony Optimization (ACO); Bi-objective traveling salesman problems; Multi objective evolutionary algorithms; Multi-objective 0-1 knapsack; Multi-objective optimization problem; Multi-objective traveling salesman problems; Reactive search; Single objective optimization; Ant colony optimization; Artificial intelligence; Combinatorial optimization; Constrained optimization; Decomposition; Multiobjective optimization; Traveling salesman problem; Algorithms; algorithm; animal; ant; article; artificial intelligence; automated pattern recognition; biomimetics; decision support system; evolution; methodology; physiology; Algorithms; Animals; Ants; Artificial Intelligence; Biological Evolution; Biomimetics; Decision Support Techniques; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84887023875
"Ali E.S., Abd-Elazim S.M.","BFOA based design of PID controller for two area Load Frequency Control with nonlinearities",2013,"International Journal of Electrical Power and Energy Systems",70,10.1016/j.ijepes.2013.02.030,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876138806&doi=10.1016%2fj.ijepes.2013.02.030&partnerID=40&md5=443523c26568507e856d04122d897d56","This paper presents an application of the novel artificial intelligent search technique to find the parameters optimization of nonlinear Load Frequency Controller (LFC) considering Proportional Integral Derivative controller (PID) for a power system. A two area non reheat thermal system is considered to be equipped with PID controller. Bacterial Foraging Optimization Algorithm (BFOA) is employed to search for optimal controller parameters to minimize the time domain objective function. The performance of the proposed technique has been evaluated with the performance of the conventional Ziegler Nichols (ZN) and Genetic Algorithm (GA) in order to demonstrate the superior efficiency of the proposed BFOA in tuning PID controller. By comparison with the conventional technique and GA, the effectiveness of the proposed BFOA is validated over different operating conditions, and system parameters variations. © 2013 Elsevier Ltd. All rights reserved.","Bacteria Foraging; Genetic Algorithm; Load Frequency Control; Nonlinearities","Bacteria foraging; Bacterial Foraging Optimization Algorithm (BFOA); Conventional techniques; Different operating conditions; Load-frequency control; Parameters optimization; Proportional integral derivative controllers; System parameters variations; Artificial intelligence; Control nonlinearities; Electric control equipment; Electric frequency control; Genetic algorithms; Optimization; Press load control; Electric load management",Article,Scopus,2-s2.0-84876138806
"Fernández A., López V., Galar M., Del Jesus M.J., Herrera F.","Analysing the classification of imbalanced data-sets with multiple classes: Binarization techniques and ad-hoc approaches",2013,"Knowledge-Based Systems",70,10.1016/j.knosys.2013.01.018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874667219&doi=10.1016%2fj.knosys.2013.01.018&partnerID=40&md5=2c9178602a811139839b1a2c61b01b7b","The imbalanced class problem is related to the real-world application of classification in engineering. It is characterised by a very different distribution of examples among the classes. The condition of multiple imbalanced classes is more restrictive when the aim of the final system is to obtain the most accurate precision for each of the concepts of the problem. The goal of this work is to provide a thorough experimental analysis that will allow us to determine the behaviour of the different approaches proposed in the specialised literature. First, we will make use of binarization schemes, i.e., one versus one and one versus all, in order to apply the standard approaches to solving binary class imbalanced problems. Second, we will apply several ad hoc procedures which have been designed for the scenario of imbalanced data-sets with multiple classes. This experimental study will include several well-known algorithms from the literature such as decision trees, support vector machines and instance-based learning, with the intention of obtaining global conclusions from different classification paradigms. The extracted findings will be supported by a statistical comparative analysis using more than 20 data-sets from the KEEL repository. © 2013 Elsevier B.V. All rights reserved.","Cost-sensitive learning; Imbalanced data-sets; Multi-classification; Pairwise learning; Preprocessing","Cost-sensitive learning; Imbalanced Data-sets; Multi-classification; Pairwise learning; Preprocessing; Artificial intelligence; Software engineering; Decision trees",Article,Scopus,2-s2.0-84874667219
"Vikhorev K., Greenough R., Brown N.","An advanced energy management framework to promote energy awareness",2013,"Journal of Cleaner Production",70,10.1016/j.jclepro.2012.12.012,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873394857&doi=10.1016%2fj.jclepro.2012.12.012&partnerID=40&md5=e13ad4b9e206c8340c2a7ff829b3dc44","Increasing energy costs, new environmental legislation, and concerns over energy security are driving efforts to increase industrial energy efficiency across the European Union and the world. Manufacturers are keen to identify the most cost-effective techniques to increase energy efficiency in their factories. To achieve the desired efficiency improvements, energy use should be measured in more detail and in real-time, to derive an awareness of the energy use patterns of every part of the manufacturing system. In this paper, we propose a framework for energy monitoring and management in the factory. This will allow decision support systems and enterprise services to take into consideration the energy used by each individual productive asset and related energy using processes, to facilitate both global and local energy optimization. The proposed framework incorporates standards for energy data exchange, on-line energy data analysis, performance measurement and display of energy usage. © 2012 Elsevier Ltd. All rights reserved.","Complex event processing; Data stream analysis; Energy management; Sustainable manufacturing","Complex event processing; Data stream; Efficiency improvement; Energy awareness; Energy cost; Energy monitoring; Energy security; Energy usage; Energy use; Enterprise services; Environmental legislations; European union; Industrial energy efficiency; Local energy; Management frameworks; Performance measurements; Sustainable manufacturing; Artificial intelligence; Decision support systems; Electronic data interchange; Energy efficiency; Manufacture; Energy management",Article,Scopus,2-s2.0-84873394857
"Mei J.-P., Kwoh C.-K., Yang P., Li X.-L., Zheng J.","Drug-target interaction prediction by learning from local information and neighbors",2013,"Bioinformatics",70,10.1093/bioinformatics/bts670,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872509876&doi=10.1093%2fbioinformatics%2fbts670&partnerID=40&md5=be9f012e8d1ebd7e992699cf422a2577","Motivation: In silico methods provide efficient ways to predict possible interactions between drugs and targets. Supervised learning approach, bipartite local model (BLM), has recently been shown to be effective in prediction of drug-target interactions. However, for drug-candidate compounds or target-candidate proteins that currently have no known interactions available, its pure 'local' model is not able to be learned and hence BLM may fail to make correct prediction when involving such kind of new candidates.Results: We present a simple procedure called neighbor-based interaction-profile inferring (NII) and integrate it into the existing BLM method to handle the new candidate problem. Specifically, the inferred interaction profile is treated as label information and is used for model learning of new candidates. This functionality is particularly important in practice to find targets for new drug-candidate compounds and identify targeting drugs for new target-candidate proteins. Consistent good performance of the new BLM-NII approach has been observed in the experiment for the prediction of interactions between drugs and four categories of target proteins. Especially for nuclear receptors, BLM-NII achieves the most significant improvement as this dataset contains many drugs/targets with no interactions in the cross-validation. This demonstrates the effectiveness of the NII strategy and also shows the great potential of BLM-NII for prediction of compound-protein interactions. © 2012 The Author. Published by Oxford University Press. All rights reserved.",,"cell receptor; drug; protein; algorithm; article; artificial intelligence; chemistry; drug development; drug effect; theoretical model; Algorithms; Artificial Intelligence; Drug Discovery; Models, Theoretical; Pharmaceutical Preparations; Proteins; Receptors, Cytoplasmic and Nuclear",Article,Scopus,2-s2.0-84872509876
"Kulkarni G., Premraj V., Ordonez V., Dhar S., Li S., Choi Y., Berg A.C., Berg T.L.","Baby talk: Understanding and generating simple image descriptions",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",69,10.1109/TPAMI.2012.162,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887601544&doi=10.1109%2fTPAMI.2012.162&partnerID=40&md5=562129ef41f008c22b15a5ea7646214c","We present a system to automatically generate natural language descriptions from images. This system consists of two parts. The first part, content planning, smooths the output of computer vision-based detection and recognition algorithms with statistics mined from large pools of visually descriptive text to determine the best content words to use to describe an image. The second step, surface realization, chooses words to construct natural language sentences based on the predicted content and general statistics from natural language. We present multiple approaches for the surface realization step and evaluate each using automatic measures of similarity to human generated reference descriptions. We also collect forced choice human evaluations between descriptions from the proposed generation system and descriptions from competing approaches. The proposed system is very effective at producing relevant sentences for images. It also generates descriptions that are notably more true to the specific image content than previous work. © 2013 IEEE.","Computer vision; image description generation","General statistics; Generation systems; Human evaluation; Image descriptions; Natural languages; Recognition algorithm; Reference descriptions; Vision-based detection; Artificial intelligence; Computer vision; Character recognition; algorithm; human; Algorithms; Humans",Article,Scopus,2-s2.0-84887601544
"Leitão P., Mařík V., Vrba P.","Past, present, and future of industrial agent applications",2013,"IEEE Transactions on Industrial Informatics",69,10.1109/TII.2012.2222034,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886643332&doi=10.1109%2fTII.2012.2222034&partnerID=40&md5=72c3d3fbe99b403847b97be3126b834f","Industrial agents technology leverages the benefits of multiagent systems, distributed computing, artificial intelligence techniques and semantics in the field of production, services and infrastructure sectors, providing a new way to design and engineer control solutions based on the decentralization of control over distributed structures. The key drivers for this application are the benefits of agent-based industrial systems, namely in terms of robustness, scalability, reconfigurability and productivity, all of which translate to a greater competitive advantage. This manuscript monitors the chronology of research and development of the industrial applications of multiagent and holonic systems. It provides the comprehensive overview of methodologies, architectures and applications of agents in industrial domain from early nineties up to present. It also gives an outlook of the current trends as well as challenges and possible future application domains of industrial agents. © 2005-2012 IEEE.","Holonic systems; industrial agents; multiagent systems (MAS); production planning; real-time control; semantic technologies manufacturing","Artificial intelligence techniques; Competitive advantage; Distributed structures; Holonic system; Infrastructure sector; Production Planning; Research and development; Semantic technologies; Competition; Industrial applications; Multi agent systems; Production control; Real time control; Semantics; Industry",Article,Scopus,2-s2.0-84886643332
"Lyubashevsky V., Peikert C., Regev O.","A toolkit for ring-LWE cryptography",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",69,10.1007/978-3-642-38348-9_3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883318384&doi=10.1007%2f978-3-642-38348-9_3&partnerID=40&md5=3bba931d429ea9a43d821cf0598b6baa","Recent advances in lattice cryptography, mainly stemming from the development of ring-based primitives such as ring-LWE, have made it possible to design cryptographic schemes whose efficiency is competitive with that of more traditional number-theoretic ones, along with entirely new applications like fully homomorphic encryption. Unfortunately, realizing the full potential of ring-based cryptography has so far been hindered by a lack of practical algorithms and analytical tools for working in this context. As a result, most previous works have focused on very special classes of rings such as power-of-two cyclotomics, which significantly restricts the possible applications. We bridge this gap by introducing a toolkit of fast, modular algorithms and analytical techniques that can be used in a wide variety of ring-based cryptographic applications, particularly those built around ring-LWE. Our techniques yield applications that work in arbitrary cyclotomic rings, with no loss in their underlying worst-case hardness guarantees, and very little loss in computational efficiency, relative to power-of-two cyclotomics. To demonstrate the toolkit's applicability, we develop two illustrative applications: a public-key cryptosystem and a ""somewhat homomorphic"" symmetric encryption scheme. Both apply to arbitrary cyclotomics, have tight parameters, and very efficient implementations. © 2013 International Association for Cryptologic Research.",,"Cryptographic applications; Cryptographic schemes; Efficient implementation; Fully homomorphic encryption; Modular algorithms; New applications; Public key cryptosystems; Symmetric encryption schemes; Artificial intelligence; Computer science; Public key cryptography",Conference Paper,Scopus,2-s2.0-84883318384
"Zeng J., Qiao W.","Short-term solar power prediction using a support vector machine",2013,"Renewable Energy",69,10.1016/j.renene.2012.10.009,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869888400&doi=10.1016%2fj.renene.2012.10.009&partnerID=40&md5=eb19e0a3ebad748689bddd3a9d75355d","This paper proposes a least-square (LS) support vector machine (SVM)-based model for short-term solar power prediction (SPP). The input of the model includes historical data of atmospheric transmissivity in a novel two-dimensional (2D) form and other meteorological variables, including sky cover, relative humidity, and wind speed. The output of the model is the predicted atmospheric transmissivity, which then is converted to solar power according to the latitude of the site and the time of the day. Computer simulations are carried out to validate the proposed model by using the data obtained from the National Solar Radiation Database (NSRDB). Results show that the proposed model not only significantly outperforms a reference autoregressive (AR) model but also achieves better results than a radial basis function neural network (RBFNN)-based model in terms of prediction accuracy. The superiority of using transmissivity over sigmoid functions for data normalization is testified. Simulation studies also show that the use of additional meteorological variables, especially sky cover, improves the accuracy of SPP. © 2012 Elsevier Ltd.","Autoregressive (AR) model; Radial basis function neural network (RBFNN); Short term; Solar power prediction (SPP); Support vector machine (SVM)","Atmospheric transmissivity; Auto regressive models; Data normalization; Historical data; Least squares; Meteorological variables; Power predictions; Prediction accuracy; Radial basis function neural networks; Short term; Sigmoid function; Simulation studies; Transmissivity; Wind speed; Solar energy; Sun; Support vector machines; Computer simulation; accuracy assessment; alternative energy; artificial intelligence; artificial neural network; atmospheric transport; computer simulation; database; least squares method; model validation; prediction; regression analysis; relative humidity; renewable resource; solar power; solar radiation; two-dimensional modeling; wind velocity",Article,Scopus,2-s2.0-84869888400
"Widrow B., Greenblatt A., Kim Y., Park D.","The No-Prop algorithm: A new learning algorithm for multilayer neural networks",2013,"Neural Networks",69,10.1016/j.neunet.2012.09.020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870473129&doi=10.1016%2fj.neunet.2012.09.020&partnerID=40&md5=cd3140f0517b5aa705c5a687f0d930ff","A new learning algorithm for multilayer neural networks that we have named No-Propagation (No-Prop) is hereby introduced. With this algorithm, the weights of the hidden-layer neurons are set and fixed with random values. Only the weights of the output-layer neurons are trained, using steepest descent to minimize mean square error, with the LMS algorithm of Widrow and Hoff. The purpose of introducing nonlinearity with the hidden layers is examined from the point of view of Least Mean Square Error Capacity (LMS Capacity), which is defined as the maximum number of distinct patterns that can be trained into the network with zero error. This is shown to be equal to the number of weights of each of the output-layer neurons. The No-Prop algorithm and the Back-Prop algorithm are compared. Our experience with No-Prop is limited, but from the several examples presented here, it seems that the performance regarding training and generalization of both algorithms is essentially the same when the number of training patterns is less than or equal to LMS Capacity. When the number of training patterns exceeds Capacity, Back-Prop is generally the better performer. But equivalent performance can be obtained with No-Prop by increasing the network Capacity by increasing the number of neurons in the hidden layer that drives the output layer. The No-Prop algorithm is much simpler and easier to implement than Back-Prop. Also, it converges much faster. It is too early to definitively say where to use one or the other of these algorithms. This is still a work in progress. © 2012 Elsevier Ltd.","Backpropagation; Neural networks; Training algorithm","Hidden layers; Least mean square error; LMS algorithms; Network Capacity; Output layer; Random values; Steepest descent; Training algorithms; Training patterns; Work in progress; Backpropagation; Multilayer neural networks; Neural networks; Neurons; Learning algorithms; analytical error; article; back propagation; cell count; cell size; cell structure; classifier; learning algorithm; Least Mean Square Error Capacity; linear system; nerve cell network; No Propagation algorithm; nonlinear system; priority journal; statistical analysis; weight; algorithm; artificial neural network; back propagation algorithm; classification algorithm; intermethod comparison; no propagation algorithm; algorithm; artificial intelligence; artificial neural network; computer; computer program; computer simulation; human; Algorithms; Artificial Intelligence; Computer Simulation; Computers; Humans; Neural Networks (Computer); Software",Article,Scopus,2-s2.0-84870473129
"Fister Jr. I., Fister D., Yang X.-S.","A hybrid bat algorithm",2013,"Elektrotehniski Vestnik/Electrotechnical Review",68,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881121704&partnerID=40&md5=541b186c55f07575aed4988d57335985","Swarm intelligence is a very powerful technique appropriate to optimization. In this paper, we presenta new swarm intelligence algorithm, which is based on the bat algorithm. Bat algorithm has been hybridized with differential evolution strategies. This hybridization showed very promising results on standard benchmark functions and also significantly improved the original bat algorithm.","Bat algorithm; Differential evolution; Optimization; Swarm intelligence","Bat algorithms; Benchmark functions; Differential Evolution; Differential evolution strategy; Swarm Intelligence; Swarm intelligence algorithms; Evolutionary algorithms; Optimization; Artificial intelligence",Article,Scopus,2-s2.0-84881121704
"Choi K., Suh Y.","A new similarity function for selecting neighbors for each target item in collaborative filtering",2013,"Knowledge-Based Systems",68,10.1016/j.knosys.2012.07.019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870054259&doi=10.1016%2fj.knosys.2012.07.019&partnerID=40&md5=a427d30e21d839f24d193b7585f3cb6c","As one of the collaborative filtering (CF) techniques, memory-based CF technique which recommends items to users based on rating information of like-minded users (called neighbors) has been widely used and has also proven to be useful in many practices in the age of information overload. However, there is still considerable room for improving the quality of recommendation. Shortly, similarity functions in traditional CF compute a similarity between a target user and the other user without considering a target item. More specifically, they give an equal weight to each of the co-rated items rated by both users. Neighbors of a target user, therefore, are identical for all target items. However, a reasonable assumption is that the similarity between a target item and each of the co-rated items should be considered when finding neighbors of a target user. Additionally, a different set of neighbors should be selected for each different target item. Thus, the objective of this paper is to propose a new similarity function in order to select different neighbors for each different target item. In the new similarity function, the rating of a user on an item is weighted by the item similarity between the item and the target item. Experimental results from MovieLens dataset and Netflix dataset provide evidence that our recommender model considerably outperforms the traditional CF-based recommender model. © 2012 Elsevier B.V. All rights reserved.","Collaborative filtering; Information overload; Item similarity; Recommendation system","Collaborative filtering; Data sets; Information overloads; Item similarity; Rating information; Similarity functions; Artificial intelligence; Recommender systems; Software engineering; Rating",Article,Scopus,2-s2.0-84870054259
"Liu B., Huang J., Kulikowski C., Yang L.","Robust visual tracking using local sparse appearance model and k-selection",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",67,10.1109/TPAMI.2012.215,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887572897&doi=10.1109%2fTPAMI.2012.215&partnerID=40&md5=7d4bfc1a53bc18904fa5301ecab76b75","Online learned tracking is widely used for its adaptive ability to handle appearance changes. However, it introduces potential drifting problems due to the accumulation of errors during the self-updating, especially for occluded scenarios. The recent literature demonstrates that appropriate combinations of trackers can help balance the stability and flexibility requirements. We have developed a robust tracking algorithm using a local sparse appearance model (SPT) and K-Selection. A static sparse dictionary and a dynamically updated online dictionary basis distribution are used to model the target appearance. A novel sparse representation-based voting map and a sparse constraint regularized mean shift are proposed to track the object robustly. Besides these contributions, we also introduce a new selection-based dictionary learning algoritH.W.th a locally constrained sparse representation, called K-Selection. Based on a set of comprehensive experiments, our algorithm has demonstrated better performance than alternatives reported in the recent literature. © 2013 IEEE.","appearance model; dictionary learning; K-selection; Sparse representation; tracking","Adaptive ability; Appearance modeling; Better performance; Dictionary learning; K-selection; Online dictionaries; Sparse dictionaries; Sparse representation; Artificial intelligence; Computer vision; Surface discharges; Algorithms",Article,Scopus,2-s2.0-84887572897
"Chen J., Wee H.","Fully, (almost) tightly secure IBE and dual system groups",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",67,10.1007/978-3-642-40084-1_25,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884487333&doi=10.1007%2f978-3-642-40084-1_25&partnerID=40&md5=9f470504019b8cc8743bb5013f9bfc1f","We present the first fully secure Identity-Based Encryption scheme (IBE) from the standard assumptions where the security loss depends only on the security parameter and is independent of the number of secret key queries. This partially answers an open problem posed by Waters (Eurocrypt 2005). Our construction combines the Waters' dual system encryption methodology (Crypto 2009) with the Naor-Reingold pseudo-random function (J. ACM, 2004) in a novel way. The security of our scheme relies on the DLIN assumption in prime-order groups. Along the way, we introduce a novel notion of dual system groups and a new randomization and parameter-hiding technique for prime-order bilinear groups. © 2013 International Association for Cryptologic Research.",,"Bilinear groups; Dual system; Dual system encryptions; Fully secure; Identity Based Encryption; Pseudo-random functions; Security parameters; Standard assumptions; Artificial intelligence; Computer science; Cryptography",Conference Paper,Scopus,2-s2.0-84884487333
"Pluhacek M., Senkerik R., Davendra D., Kominkova Oplatkova Z., Zelinka I.","On the behavior and performance of chaos driven PSO algorithm with inertia weight",2013,"Computers and Mathematics with Applications",67,10.1016/j.camwa.2013.01.016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880206476&doi=10.1016%2fj.camwa.2013.01.016&partnerID=40&md5=a10b9f645a6373536a86a107b15c3717","In this paper, the utilization of chaos pseudorandom number generators based on three different chaotic maps to alter the behavior and overall performance of PSO algorithm is proposed. This paper presents results of testing the performance and behavior of the proposed algorithm on typical benchmark functions that represent unimodal and multimodal problems. The promising results are analyzed and discussed. © 2013 Elsevier Ltd. All rights reserved.","Chaos; Evolutionary algorithms; Optimization; Particle swarm; Swarm intelligence","Benchmark functions; Chaotic map; Inertia weight; Multimodal problems; Particle swarm; Pseudo random number generators; PSO algorithms; Swarm Intelligence; Algorithms; Artificial intelligence; Chaos theory; Chaotic systems; Evolutionary algorithms; Optimization; Random number generation; Benchmarking",Article,Scopus,2-s2.0-84880206476
"Nahar J., Imam T., Tickle K.S., Chen Y.-P.P.","Association rule mining to detect factors which contribute to heart disease in males and females",2013,"Expert Systems with Applications",67,10.1016/j.eswa.2012.08.028,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870239197&doi=10.1016%2fj.eswa.2012.08.028&partnerID=40&md5=b07f574d13b2f5ece8eed28cfb5e6c22","This paper investigates the sick and healthy factors which contribute to heart disease for males and females. Association rule mining, a computational intelligence approach, is used to identify these factors and the UCI Cleveland dataset, a biological database, is considered along with the three rule generation algorithms - Apriori, Predictive Apriori and Tertius. Analyzing the information available on sick and healthy individuals and taking confidence as an indicator, females are seen to have less chance of coronary heart disease then males. Also, the attributes indicating healthy and sick conditions were identified. It is seen that factors such as chest pain being asymptomatic and the presence of exercise-induced angina indicate the likely existence of heart disease for both men and women. However, resting ECG being either normal or hyper and slope being flat are potential high risk factors for women only. For men, on the other hand, only a single rule expressing resting ECG being hyper was shown to be a significant factor. This means, for women, resting ECG status is a key distinct factor for heart disease prediction. Comparing the healthy status of men and women, slope being up, number of coloured vessels being zero, and oldpeak being less than or equal to 0.56 indicate a healthy status for both genders. © 2012 Elsevier Ltd. All rights reserved.","Association rule mining; Cleveland data; Computational intelligence; Female; Healthy; Heart disease; Male; Sick","Cleveland; Female; Healthy; Heart disease; Male; Sick; Artificial intelligence; Diseases; Electrocardiography; Cardiology",Article,Scopus,2-s2.0-84870239197
"Gascon H., Yamaguchi F., Arp D., Rieck K.","Structural detection of Android malware using embedded call graphs",2013,"Proceedings of the ACM Conference on Computer and Communications Security",66,10.1145/2517312.2517315,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889046959&doi=10.1145%2f2517312.2517315&partnerID=40&md5=3aa068a35b123a9ebd5d1f34936feb74","The number of malicious applications targeting the Android system has literally exploded in recent years. While the security community, well aware of this fact, has proposed several methods for detection of Android malware, most of these are based on permission and API usage or the identification of expert features. Unfortunately, many of these approaches are susceptible to instruction level obfuscation techniques. Previous research on classic desktop malware has shown that some high level characteristics of the code, such as function call graphs, can be used to find similarities between samples while being more robust against certain obfuscation strategies. However, the identification of similarities in graphs is a non-trivial problem whose complexity hinders the use of these features for malware detection. In this paper, we explore how recent developments in machine learning classification of graphs can be efficiently applied to this problem. We propose a method for malware detection based on efficient embeddings of function call graphs with an explicit feature map inspired by a linear-time graph kernel. In an evaluation with 12,158 malware samples our method, purely based on structural features, outperforms several related approaches and detects 89% of the malware with few false alarms, while also allowing to pin-point malicious code structures within Android applications. © 2013 ACM.","graph kernels; machine learning; malware detection","Android applications; Function-call graphs; Graph kernels; Level characteristic; Machine learning classification; Malware detection; Structural detections; Structural feature; Application programming interfaces (API); Artificial intelligence; Learning systems; Robots; Computer crime",Conference Paper,Scopus,2-s2.0-84889046959
"Grosenick L., Klingenberg B., Katovich K., Knutson B., Taylor J.E.","Interpretable whole-brain prediction analysis with GraphNet",2013,"NeuroImage",66,10.1016/j.neuroimage.2012.12.062,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874528278&doi=10.1016%2fj.neuroimage.2012.12.062&partnerID=40&md5=3647356d7ec07c859317389827c01ceb","Multivariate machine learning methods are increasingly used to analyze neuroimaging data, often replacing more traditional ""mass univariate"" techniques that fit data one voxel at a time. In the functional magnetic resonance imaging (fMRI) literature, this has led to broad application of ""off-the-shelf"" classification and regression methods. These generic approaches allow investigators to use ready-made algorithms to accurately decode perceptual, cognitive, or behavioral states from distributed patterns of neural activity. However, when applied to correlated whole-brain fMRI data these methods suffer from coefficient instability, are sensitive to outliers, and yield dense solutions that are hard to interpret without arbitrary thresholding. Here, we develop variants of the Graph-constrained Elastic-Net (GraphNet), a fast, whole-brain regression and classification method developed for spatially and temporally correlated data that automatically yields interpretable coefficient maps (Grosenick et al., 2009b). GraphNet methods yield sparse but structured solutions by combining structured graph constraints (based on knowledge about coefficient smoothness or connectivity) with a global sparsity-inducing prior that automatically selects important variables. Because GraphNet methods can efficiently fit regression or classification models to whole-brain, multiple time-point data sets and enhance classification accuracy relative to volume-of-interest (VOI) approaches, they eliminate the need for inherently biased VOI analyses and allow whole-brain fitting without the multiple comparison problems that plague mass univariate and roaming VOI (""searchlight"") methods. As fMRI data are unlikely to be normally distributed, we (1) extend GraphNet to include robust loss functions that confer insensitivity to outliers, (2) equip them with ""adaptive"" penalties that asymptotically guarantee correct variable selection, and (3) develop a novel sparse structured Support Vector GraphNet classifier (SVGN). When applied to previously published data (Knutson et al., 2007), these efficient whole-brain methods significantly improved classification accuracy over previously reported VOI-based analyses on the same data (Grosenick et al., 2008; Knutson et al., 2007) while discovering task-related regions not documented in the original VOI approach. Critically, GraphNet estimates fit to the Knutson et al. (2007) data generalize well to out-of-sample data collected more than three years later on the same task but with different subjects and stimuli (Karmarkar et al., submitted for publication). By enabling robust and efficient selection of important voxels from whole-brain data taken over multiple time points (> 100,000 ""features""), these methods enable data-driven selection of brain areas that accurately predict single-trial behavior within and across individuals. © 2013 Elsevier Inc.",,"accuracy; article; automation; brain region; classification algorithm; classifier; correlation coefficient; functional magnetic resonance imaging; GraphNet; knowledge; neuroimaging; normal distribution; prediction; priority journal; regression analysis; support vector machine; univariate analysis; Algorithms; Artificial Intelligence; Brain; Brain Mapping; Humans; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging",Article,Scopus,2-s2.0-84874528278
"Díaz-Gimeno P., Ruiz-Alonso M., Blesa D., Bosch N., Martínez-Conejero J.A., Alamá P., Garrido N., Pellicer A., Simón C.","The accuracy and reproducibility of the endometrial receptivity array is superior to histology as a diagnostic method for endometrial receptivity",2013,"Fertility and Sterility",66,10.1016/j.fertnstert.2012.09.046,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873295219&doi=10.1016%2fj.fertnstert.2012.09.046&partnerID=40&md5=10f5a31f9a049f2e62e1066ed1b834c7","Objective: To compare the accuracy and reproducibility of the endometrial receptivity array (ERA) versus standard histologic methods. Design: A comparative prospective study (May 2008-May 2012). Setting: University- Affiliated infertility clinic. Patient(s): Eighty-six healthy oocyte donors, regularly cycling, aged 20-34 years with a body mass index (BMI) of 19-25 kg/m2. Intervention(s): Endometrial biopsies were collected throughout the menstrual cycle. For the accuracy study, 79 samples were grouped into two cohorts: the training set (n = 79) for ERA machine-learning training and dating, and a dating subset (n = 49) for comparison between histologic and ERA dating. For the reproducibility study, seven women underwent ERA testing and it was repeated in the same patients on the same day of their cycle 29-40 months later. Main Outcome Measure(s): Concordance of histologic and ERA dating related to LH as a reference, and interobserver variability between pathologists were statistically analyzed by the quadratic weighted Kappa index. The ERA reproducibility was tested and its gene expression visualized by principal component analysis. Result(s): For each pathologist, concordance against LH peak yielded values of 0.618 (0.446-0.791) and 0.685 (0.545-0.824). Interobserver variability between pathologists yielded a Kappa index of 0.622 (0.435-0.839). Concordance for ERA dating against LH peak showed a value of 0.922 (0.815-1.000). Reproducibility of the ERA test was 100% consistent. Conclusion(s): The ERA is more accurate than histologic dating and is a completely reproducible method for the diagnosis of endometrial dating and receptivity status.","consistency; diagnostic accuracy; Endometrial receptivity; histologic dating; machine-learning prediction","luteinizing hormone; adult; article; body mass; cohort analysis; comparative study; decidualization; diagnostic accuracy; diagnostic test; diagnostic test accuracy study; donor; endometrial receptivity array; endometrium biopsy; female; gene expression; histology; human; machine learning; major clinical study; menstrual cycle; oocyte; principal component analysis; priority journal; prospective study; reproducibility; Adult; Artificial Intelligence; Biological Markers; Biopsy; Diagnosis, Computer-Assisted; Endometrium; Female; Humans; Ovulation Detection; Reproducibility of Results; Sensitivity and Specificity; Young Adult",Article,Scopus,2-s2.0-84873295219
"Kang Q., Zhou M., An J., Wu Q.D.","Swarm intelligence approaches to optimal power flow problem with distributed generator failures in power networks",2013,"IEEE Transactions on Automation Science and Engineering",66,10.1109/TASE.2012.2204980,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876138680&doi=10.1109%2fTASE.2012.2204980&partnerID=40&md5=ddf971cb2b5fe40f2e96f21759c47c77","Distributed generation becomes more and more important in modern power systems. However, the increasing use of distributed generators causes the concerns on the increasing system risk due to their likely failure or uncontrollable power outputs based on such renewable energy sources as wind and the sun. This work for the first time formulates an optimal power flow problem by considering controllable and uncontrollable distributed generators in power networks. The problem for the cases of single and multiple generator failures is addressed as an example. The methods are presented to find its power output solution of controllable online generators via particle swarm optimization and group search optimizer for coping with the difficult scenarios in a power network. The proposed methods are tested on an IEEE 14-bus system, and several population initialization strategies are investigated and compared for the algorithms. The simulation results confirm their effectiveness for optimal power management and effective control of a power network. © 2004-2012 IEEE.","Distributed generation (DG); generator failure; optimal power flow (OPF); power system; swarm intelligence","Distributed generators; Group search optimizer (GSO); Optimal power flow problem; Optimal power flows; Population initializations; Power networks; Renewable energy source; Swarm Intelligence; Acoustic generators; Artificial intelligence; Electric load flow; Electric network analysis; Energy management; Particle swarm optimization (PSO); Renewable energy resources; Standby power systems; Distributed power generation",Article,Scopus,2-s2.0-84876138680
"Neri F., Mininno E., Iacca G.","Compact particle swarm optimization",2013,"Information Sciences",65,10.1016/j.ins.2013.03.026,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876911116&doi=10.1016%2fj.ins.2013.03.026&partnerID=40&md5=3aca62e33892e0a053ce4c77d9897328","Some real-world optimization problems are plagued by a limited hardware availability. This situation can occur, for example, when the optimization must be performed on a device whose hardware is limited due to cost and space limitations. This paper addresses this class of optimization problems and proposes a novel algorithm, namely compact Particle Swarm Optimization (cPSO). The proposed algorithm employs the search logic typical of Particle Swarm Optimization (PSO) algorithms, but unlike classical PSO algorithms, does not use a swarm of particles and does not store neither the positions nor the velocities. On the contrary, cPSO employs a probabilistic representation of the swarm's behaviour. This representation allows a modest memory usage for the entire algorithmic functioning, the amount of memory used is the same as what is needed for storing five solutions. A novel interpretation of compact optimization is also given in this paper. Numerical results show that cPSO appears to outperform other modern algorithms of the same category (i.e. which attempt to solve the optimization despite a modest memory usage). In addition, cPSO displays a very good performance with respect to its population-based version and a respectable performance also with respect to some more complex population-based algorithms. A real world application in the field of power engineering and energy generation is given. The presented case study shows how, on a model of an actual power plant, an advanced control system can be online and real-time optimized. In this application example the calculations are embedded directly on the real-time control system. © 2013 Elsevier Inc. All rights reserved.","Compact optimization; Limited memory problems Swarm intelligence Computational intelligence optimization Real-time application; Optimization; Particle Swarm","Advanced control systems; Particle swarm; Particle swarm optimization algorithm; Population-based algorithm; Power engineering and energies; Probabilistic representation; Real-world optimization; Swarm Intelligence; Algorithms; Artificial intelligence; Hardware; Optimization; Particle swarm optimization (PSO)",Article,Scopus,2-s2.0-84876911116
"Hosseinifard B., Moradi M.H., Rostami R.","Classifying depression patients and normal subjects using machine learning techniques and nonlinear features from EEG signal",2013,"Computer Methods and Programs in Biomedicine",65,10.1016/j.cmpb.2012.10.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874239884&doi=10.1016%2fj.cmpb.2012.10.008&partnerID=40&md5=1c086bca5b84c3428cca6e57b81bb86e","Diagnosing depression in the early curable stages is very important and may even save the life of a patient. In this paper, we study nonlinear analysis of EEG signal for discriminating depression patients and normal controls. Forty-five unmedicated depressed patients and 45 normal subjects were participated in this study. Power of four EEG bands and four nonlinear features including detrended fluctuation analysis (DFA), higuchi fractal, correlation dimension and lyapunov exponent were extracted from EEG signal. For discriminating the two groups, k-nearest neighbor, linear discriminant analysis and logistic regression as the classifiers are then used. Highest classification accuracy of 83.3% is obtained by correlation dimension and LR classifier among other nonlinear features. For further improvement, all nonlinear features are combined and applied to classifiers. A classification accuracy of 90% is achieved by all nonlinear features and LR classifier. In all experiments, genetic algorithm is employed to select the most important features. The proposed technique is compared and contrasted with the other reported methods and it is demonstrated that by combining nonlinear features, the performance is enhanced. This study shows that nonlinear analysis of EEG can be a useful method for discriminating depressed patients and normal subjects. It is suggested that this analysis may be a complementary tool to help psychiatrists for diagnosing depressed patients. © 2012 Elsevier Ireland Ltd.","Correlation dimension; Depression; Detrended fluctuation analysis; EEG; Higuchi fractal; Lyapunov exponent","Classification accuracy; Complementary tools; Correlation dimensions; Depression; Detrended fluctuation analysis; EEG signals; Important features; K-nearest neighbors; Linear discriminant analysis; Logistic regressions; Lyapunov exponent; Machine learning techniques; Nonlinear features; Normal controls; Differential equations; Electroencephalography; Fractal dimension; Learning systems; Logistics; Lyapunov functions; Nonlinear analysis; Lyapunov methods; adult; article; clinical article; controlled study; correlation dimension analysis; depression; detrended fluctuation analysis; electroencephalography; female; fractal analysis; genetic algorithm; higuchi fractal analysis; human; k nearest neighbor; linear system; logistic regression analysis; lyapunov exponent analysis; machine learning; male; nonlinear system; patient coding; Adult; Algorithms; Artificial Intelligence; Brain Mapping; Computer Simulation; Depression; Discriminant Analysis; Electroencephalography; Female; Fractals; Humans; Least-Squares Analysis; Logistic Models; Male; Middle Aged; Models, Statistical; Reproducibility of Results; Signal Processing, Computer-Assisted; Software",Article,Scopus,2-s2.0-84874239884
"Marques-Silva J., Heras F., Janota M., Previti A., Belov A.","On computing Minimal Correction Subsets",2013,"IJCAI International Joint Conference on Artificial Intelligence",64,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896061227&partnerID=40&md5=bc970bf06b3a8684f5d0af8a4c7f7ab3","A set of constraints that cannot be simultaneously satisfied is over-constrained. Minimal relaxations and minimal explanations for over-constrained problems find many practical uses. For Boolean formulas, minimal relaxations of over-constrained problems are referred to as Minimal Correction Subsets (MCSes). MCSes find many applications, including the enumeration of MUSes. Existing approaches for computing MCSes either use a Maximum Satisfiability (MaxSAT) solver or iterative calls to a Boolean Satisfiability (SAT) solver. This paper shows that existing algorithms for MCS computation can be inefficient, and so inadequate, in certain practical settings. To address this problem, this paper develops a number of novel techniques for improving the performance of existing MCS computation algorithms. More importantly, the paper proposes a novel algorithm for computing MCSes. Both the techniques and the algorithm are evaluated empirically on representative problem instances, and are shown to yield the most efficient and robust solutions for MCS computation.",,"Boolean satisfiability; Computation algorithm; Maximum satisfiability; Novel techniques; Over-constrained; Over-constrained problem; Problem instances; Robust solutions; Algorithms; Artificial intelligence; Boolean algebra; Iterative methods; Computational efficiency",Conference Paper,Scopus,2-s2.0-84896061227
"Alexandridis A.K., Zapranis A.D.","Wavelet neural networks: A practical guide",2013,"Neural Networks",64,10.1016/j.neunet.2013.01.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873637949&doi=10.1016%2fj.neunet.2013.01.008&partnerID=40&md5=dfd8e42479b9e73932913c9587eb744d","Wavelet networks (WNs) are a new class of networks which have been used with great success in a wide range of applications. However a general accepted framework for applying WNs is missing from the literature. In this study, we present a complete statistical model identification framework in order to apply WNs in various applications. The following subjects were thoroughly examined: the structure of a WN, training methods, initialization algorithms, variable significance and variable selection algorithms, model selection methods and finally methods to construct confidence and prediction intervals. In addition the complexity of each algorithm is discussed. Our proposed framework was tested in two simulated cases, in one chaotic time series described by the Mackey-Glass equation and in three real datasets described by daily temperatures in Berlin, daily wind speeds in New York and breast cancer classification. Our results have shown that the proposed algorithms produce stable and robust results indicating that our proposed framework can be applied in various applications. © 2013 Elsevier Ltd.","Confidence intervals; Model identification; Model selection; Prediction intervals; Variable selection; Wavelet networks","Confidence interval; Model identification; Model Selection; Prediction interval; Variable selection; Wavelet network; Classification (of information); Algorithms; algorithm; article; artificial neural network; breast cancer; cancer classification; mathematical analysis; prediction; priority journal; simulation; statistical model; temperature; wavelet neural network; Algorithms; Artificial Intelligence; Brain; Breast Neoplasms; Computer Simulation; Confidence Intervals; Feedback, Physiological; Female; Humans; Models, Neurological; Nerve Net; Neural Networks (Computer); Nonlinear Dynamics; Predictive Value of Tests; Temperature; Wavelet Analysis; Wind",Article,Scopus,2-s2.0-84873637949
"Monroe M., Lan R., Lee H., Plaisant C., Shneiderman B.","Temporal event sequence simplification",2013,"IEEE Transactions on Visualization and Computer Graphics",63,10.1109/TVCG.2013.200,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886664973&doi=10.1109%2fTVCG.2013.200&partnerID=40&md5=595e0b34ac1a1c9749f856d0f73afa80","Electronic Health Records (EHRs) have emerged as a cost-effective data source for conducting medical research. The difficulty in using EHRs for research purposes, however, is that both patient selection and record analysis must be conducted across very large, and typically very noisy datasets. Our previous work introduced EventFlow, a visualization tool that transforms an entire dataset of temporal event records into an aggregated display, allowing researchers to analyze population-level patterns and trends. As datasets become larger and more varied, however, it becomes increasingly difficult to provide a succinct, summarizing display. This paper presents a series of user-driven data simplifications that allow researchers to pare event records down to their core elements. Furthermore, we present a novel metric for measuring visual complexity, and a language for codifying disjoint strategies into an overarching simplification framework. These simplifications were used by real-world researchers to gain new and valuable insights from initially overwhelming datasets. © 2013 IEEE.","electronic heath records; Event sequences; simplification; temporal query","Data simplification; Electronic health record (EHRs); electronic heath records; Event sequence; simplification; Temporal event sequences; Temporal queries; Visualization tools; Graphical user interfaces; Research; Data processing; algorithm; article; artificial intelligence; automated pattern recognition; computer graphics; computer interface; data base; data mining; electronic medical record; methodology; reproducibility; sensitivity and specificity; automated pattern recognition; data mining; procedures; Algorithms; Artificial Intelligence; Computer Graphics; Data Mining; Database Management Systems; Electronic Health Records; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; User-Computer Interface; Algorithms; Artificial Intelligence; Computer Graphics; Data Mining; Database Management Systems; Electronic Health Records; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; User-Computer Interface",Article,Scopus,2-s2.0-84886664973
"Xu X., Hou Z., Lian C., He H.","Online learning control using adaptive critic designs with sparse kernel machines",2013,"IEEE Transactions on Neural Networks and Learning Systems",63,10.1109/TNNLS.2012.2236354,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884922436&doi=10.1109%2fTNNLS.2012.2236354&partnerID=40&md5=396dfccba1e97673012f5d7cdffab9fe","In the past decade, adaptive critic designs (ACDs), including heuristic dynamic programming (HDP), dual heuristic programming (DHP), and their action-dependent ones, have been widely studied to realize online learning control of dynamical systems. However, because neural networks with manually designed features are commonly used to deal with continuous state and action spaces, the generalization capability and learning efficiency of previous ACDs still need to be improved. In this paper, a novel framework of ACDs with sparse kernel machines is presented by integrating kernel methods into the critic of ACDs. To improve the generalization capability as well as the computational efficiency of kernel machines, a sparsification method based on the approximately linear dependence analysis is used. Using the sparse kernel machines, two kernel-based ACD algorithms, that is, kernel HDP (KHDP) and kernel DHP (KDHP), are proposed and their performance is analyzed both theoretically and empirically. Because of the representation learning and generalization capability of sparse kernel machines, KHDP and KDHP can obtain much better performance than previous HDP and DHP with manually designed neural networks. Simulation and experimental results of two nonlinear control problems, that is, a continuous-action inverted pendulum problem and a ball and plate control problem, demonstrate the effectiveness of the proposed kernel ACD methods. © 2013 IEEE.","Adaptive critic designs; Approximate dynamic programming; Kernel machines; Learning control; Markov decision processes; Reinforcement learning","Adaptive critic designs; Approximate dynamic programming; Kernel machine; Learning control; Markov Decision Processes; Dynamical systems; Heuristic programming; Learning algorithms; Markov processes; Neural networks; Reinforcement learning; E-learning; algorithm; artificial intelligence; computer simulation; decision support system; feedback system; human; learning; online system; probability; Algorithms; Artificial Intelligence; Computer Simulation; Decision Support Techniques; Feedback; Humans; Learning; Markov Chains; Online Systems",Article,Scopus,2-s2.0-84884922436
"Lusci A., Pollastri G., Baldi P.","Deep architectures and deep learning in chemoinformatics: The prediction of aqueous solubility for drug-like molecules",2013,"Journal of Chemical Information and Modeling",63,10.1021/ci400187y,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880542260&doi=10.1021%2fci400187y&partnerID=40&md5=4b75f4f7417b9ab5417269443257cc50","Shallow machine learning methods have been applied to chemoinformatics problems with some success. As more data becomes available and more complex problems are tackled, deep machine learning methods may also become useful. Here, we present a brief overview of deep learning methods and show in particular how recursive neural network approaches can be applied to the problem of predicting molecular properties. However, molecules are typically described by undirected cyclic graphs, while recursive approaches typically use directed acyclic graphs. Thus, we develop methods to address this discrepancy, essentially by considering an ensemble of recursive neural networks associated with all possible vertex-centered acyclic orientations of the molecular graph. One advantage of this approach is that it relies only minimally on the identification of suitable molecular descriptors because suitable representations are learned automatically from the data. Several variants of this approach are applied to the problem of predicting aqueous solubility and tested on four benchmark data sets. Experimental results show that the performance of the deep learning methods matches or exceeds the performance of other state-of-the-art methods according to several evaluation metrics and expose the fundamental limitations arising from training sets that are too small or too noisy. A Web-based predictor, AquaSol, is available online through the ChemDB portal (cdb.ics.uci.edu) together with additional material. © 2013 American Chemical Society.",,"Acyclic orientation; Directed acyclic graph (DAG); Fundamental limitations; Machine learning methods; Molecular descriptors; Molecular properties; Recursive neural networks; State-of-the-art methods; Directed graphs; Forecasting; Molecular graphics; Molecules; Neural networks; Solubility; Learning systems; acetic acid; drug; water; drug; water; article; artificial intelligence; artificial neural network; chemistry; computer graphics; drug database; information science; Internet; methodology; solubility; information science; procedures; Acetic Acid; Artificial Intelligence; Computer Graphics; Databases, Pharmaceutical; Informatics; Internet; Neural Networks (Computer); Pharmaceutical Preparations; Solubility; Water; Acetic Acid; Artificial Intelligence; Computer Graphics; Databases, Pharmaceutical; Informatics; Internet; Neural Networks (Computer); Pharmaceutical Preparations; Solubility; Water",Article,Scopus,2-s2.0-84880542260
"Khosravian M., Faramarzi F.K., Beigi M.M., Behbahani M., Mohabatkar H.","Predicting antibacterial peptides by the concept of Chou's pseudo-amino acid composition and machine learning methods",2013,"Protein and Peptide Letters",63,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876370111&partnerID=40&md5=3539a5391ad473bdb8dc93636d019fa1","Microbial resistance to antibiotics is a rising concern among health care professionals, driving them to search for alternative therapies. In the past few years, antimicrobial peptides (AMPs) have attracted a lot of attention as a substitute for conventional antibiotics. Antimicrobial peptides have a broad spectrum of activity and can act as antibacterial, antifungal, antiviral and sometimes even as anticancer drugs. The antibacterial peptides have little sequence homology, despite common properties. Since there is a need to develop a computational method for predicting the antibacterial peptides, in the present study, we have applied the concept of Chou's pseudo-amino acid composition (PseAAC) and machine learning methods for their classification. Our results demonstrate that using the concept of PseAAC and applying Support Vector Machine (SVM) can provide useful information to predict antibacterial peptides. © 2013 Bentham Science Publishers.","Antibacterial peptides; Bioinformatics; Chou's pseudo amino acid composition; Clustering; Fivefold cross-validation; Machine learning methods","polypeptide antibiotic agent; accuracy; amino acid composition; article; artificial neural network; bioinformatics; Chou pseudoamino acid composition; cluster analysis; controlled study; correlation coefficient; data analysis; mathematical analysis; Matthew correlation coefficient; multi layer perceptron; prediction; process development; quality control; sensitivity and specificity; support vector machine; validation process; Amino Acids; Anti-Bacterial Agents; Artificial Intelligence; Computational Biology; Peptides",Article,Scopus,2-s2.0-84876370111
"Hensman J., Fusi N., Lawrence N.D.","Gaussian processes for big data",2013,"Uncertainty in Artificial Intelligence - Proceedings of the 29th Conference, UAI 2013",62,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888155846&partnerID=40&md5=0f3ec08677064759df0c24fbca89ef22","We introduce stochastic variational inference for Gaussian process models. This enables the application of Gaussian process (GP) models to data sets containing millions of data points. We show how GPs can be variationally decomposed to depend on a set of globally relevant inducing variables which factorize the model in the necessary manner to perform variational inference. Our approach is readily extended to models with non-Gaussian likelihoods and latent variable models based around Gaussian processes. We demonstrate the approach on a simple toy problem and two real world data sets.",,"Big datum; Data points; Gaussian process; Gaussian process models; Gaussian Processes; Latent variable models; Non-Gaussian; Variational inference; Artificial intelligence; Gaussian distribution; Gaussian noise (electronic)",Conference Paper,Scopus,2-s2.0-84888155846
"Meier S., Schmidt B., Cremers C., Basin D.","The TAMARIN prover for the symbolic analysis of security protocols",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",62,10.1007/978-3-642-39799-8_48,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881193419&doi=10.1007%2f978-3-642-39799-8_48&partnerID=40&md5=4942edd6c038ade15bfe4c92f4f68aac","The Tamarin prover supports the automated, unbounded, symbolic analysis of security protocols. It features expressive languages for specifying protocols, adversary models, and properties, and support for efficient deduction and equational reasoning. We provide an overview of the tool and its applications. © 2013 Springer-Verlag.",,"Adversary models; Equational reasoning; ITS applications; Security protocols; Symbolic analysis; Artificial intelligence; Computer science; Computer aided analysis",Conference Paper,Scopus,2-s2.0-84881193419
"Thooyamani K.P., Khanaa V., Udayakumar R.","An integrated agent system for e-mail coordination using jade",2013,"Indian Journal of Science and Technology",62,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880659611&partnerID=40&md5=7d88a0f1c2d6839328eb2681211ee741","With growing usage of computer applications there is growing dependence of business over computer aided services like e-mails. The CRM or E-Mail based marketing companies are becoming pervasive. There is always a greater need to prioritize the e-mails which has direct impact on the ROI - Return on Investment to run these companies in a way that to customers satisfied. In this paper, a method is proposed to prioritize the unread E-Mails based on users 'interest and priority. This is implemented in this system using agent technology. The Agent mechanism is guided using JADE Middleware and it's underlying architecture. The Intelligent agent used here can learn the interest and priority of the user and use this knowledge to rearrange the unread mails. The Text corpus in the mail would undergo tokenization and each token would be matched with knowledgebase by the agent. The 90 percent precision of the prioritization. Thereby, the E-Mails are ranked according to interest and priority of the user.","Artificial intelligence; E-Commerce; Intelligent agent; Tokenization",,Article,Scopus,2-s2.0-84880659611
"Van Valkenhoef G., Tervonen T., Zwinkels T., De Brock B., Hillege H.","ADDIS: A decision support system for evidence-based medicine",2013,"Decision Support Systems",62,10.1016/j.dss.2012.10.005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878239536&doi=10.1016%2fj.dss.2012.10.005&partnerID=40&md5=4aadf8d2270c97526b1add9aae9ef0b1","Clinical trials are the main source of information for the efficacy and safety evaluation of medical treatments. Although they are of pivotal importance in evidence-based medicine, there is a lack of usable information systems providing data-analysis and decision support capabilities for aggregate clinical trial results. This is partly caused by unavailability (i) of trial data in a structured format suitable for re-analysis, and (ii) of a complete data model for aggregate level results. In this paper, we develop a unifying data model that enables the development of evidence-based decision support in the absence of a complete data model. We describe the supported decision processes and show how these are implemented in the open source ADDIS software. ADDIS enables semi-automated construction of meta-analyses, network meta-analyses and benefit-risk decision models, and provides visualization of all results. © 2012 Elsevier B.V.","Clinical trial; Data model; Decision analysis; Evidence synthesis; Evidence-based medicine","Clinical trial; Decision models; Decision process; Decision supports; Evidence-based medicine; Medical treatment; Safety evaluations; Unifying data models; Artificial intelligence; Data structures; Decision support systems; Decision theory; Experiments; Medical applications; Models; Open systems; Aggregates",Article,Scopus,2-s2.0-84878239536
"Mohammadzade H., Hatzinakos D.","Iterative closest normal point for 3D face recognition",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",62,10.1109/TPAMI.2012.107,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871728689&doi=10.1109%2fTPAMI.2012.107&partnerID=40&md5=b9bbf75581ed46544b2c553ca9641206","The common approach for 3D face recognition is to register a probe face to each of the gallery faces and then calculate the sum of the distances between their points. This approach is computationally expensive and sensitive to facial expression variation. In this paper, we introduce the iterative closest normal point method for finding the corresponding points between a generic reference face and every input face. The proposed correspondence finding method samples a set of points for each face, denoted as the closest normal points. These points are effectively aligned across all faces, enabling effective application of discriminant analysis methods for 3D face recognition. As a result, the expression variation problem is addressed by minimizing the within-class variability of the face samples while maximizing the between-class variability. As an important conclusion, we show that the surface normal vectors of the face at the sampled points contain more discriminatory information than the coordinates of the points. We have performed comprehensive experiments on the Face Recognition Grand Challenge database, which is presently the largest available 3D face database. We have achieved verification rates of 99.6 and 99.2 percent at a false acceptance rate of 0.1 percent for the all versus all and ROC III experiments, respectively, which, to the best of our knowledge, have seven and four times less error rates, respectively, compared to the best existing methods on this database. © 2012 IEEE.","3D registration; expression variation; face recognition; LDA; point correspondence; surface normal vector; Three-dimensional","3D registration; expression variation; LDA; Point correspondence; Surface normals; Database systems; Discriminant analysis; Experiments; Iterative methods; Three dimensional; Three dimensional computer graphics; Face recognition; algorithm; article; artificial intelligence; automated pattern recognition; biometry; computer assisted diagnosis; face; histology; human; image enhancement; image subtraction; methodology; photography; reproducibility; sensitivity and specificity; three dimensional imaging; Algorithms; Artificial Intelligence; Biometry; Face; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Pattern Recognition, Automated; Photography; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-84871728689
"Oreifej O., Li X., Shah M.","Simultaneous video stabilization and moving object detection in turbulence",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",62,10.1109/TPAMI.2012.97,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871801633&doi=10.1109%2fTPAMI.2012.97&partnerID=40&md5=288dd671c5869a7d11cbccf6b7cf9115","Turbulence mitigation refers to the stabilization of videos with nonuniform deformations due to the influence of optical turbulence. Typical approaches for turbulence mitigation follow averaging or dewarping techniques. Although these methods can reduce the turbulence, they distort the independently moving objects, which can often be of great interest. In this paper, we address the novel problem of simultaneous turbulence mitigation and moving object detection. We propose a novel three-term low-rank matrix decomposition approach in which we decompose the turbulence sequence into three components: the background, the turbulence, and the object. We simplify this extremely difficult problem into a minimization of nuclear norm, Frobenius norm, and ℓ1 norm. Our method is based on two observations: First, the turbulence causes dense and Gaussian noise and therefore can be captured by Frobenius norm, while the moving objects are sparse and thus can be captured by ℓ1 norm. Second, since the object's motion is linear and intrinsically different from the Gaussian-like turbulence, a Gaussian-based turbulence model can be employed to enforce an additional constraint on the search space of the minimization. We demonstrate the robustness of our approach on challenging sequences which are significantly distorted with atmospheric turbulence and include extremely tiny moving objects. © 2012 IEEE.","moving object detection; particle advection; rank optimization; restoring force; Three-term decomposition; turbulence mitigation","Dewarping; Frobenius norm; Low-rank matrices; Moving objects; Moving-object detection; Non-uniform deformation; Optical turbulence; Particle advection; Restoring forces; Search spaces; Three component; Three-term; Video stabilization; Atmospheric turbulence; Gaussian noise (electronic); Stabilization; Turbulence models; Object recognition; algorithm; article; artifact; artificial intelligence; automated pattern recognition; computer assisted diagnosis; image enhancement; methodology; motion; photography; reproducibility; sensitivity and specificity; statistical analysis; videorecording; Algorithms; Artifacts; Artificial Intelligence; Data Interpretation, Statistical; Image Enhancement; Image Interpretation, Computer-Assisted; Motion; Pattern Recognition, Automated; Photography; Reproducibility of Results; Sensitivity and Specificity; Video Recording",Article,Scopus,2-s2.0-84871801633
"Fader A., Zettlemoyer L., Etzioni O.","Paraphrase-driven learning for open question answering",2013,"ACL 2013 - 51st Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference",62,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906932025&partnerID=40&md5=0805489273fb6d8e4a4f7fee5547457f","We study question answering as a machine learning problem, and induce a function that maps open-domain questions to queries over a database of web extractions. Given a large, community-authored, question-paraphrase corpus, we demonstrate that it is possible to learn a semantic lexicon and linear ranking function without manually annotating questions. Our approach automatically generalizes a seed lexicon and includes a scalable, parallelized perceptron parameter estimation scheme. Experiments show that our approach more than quadruples the recall of the seed lexicon, with only an 8% loss in precision. © 2013 Association for Computational Linguistics.",,"Artificial intelligence; Computational linguistics; Query languages; Semantics; Linear ranking; Machine learning problem; Perceptron; Question Answering; Semantic lexicon; Web extraction; Natural language processing systems",Conference Paper,Scopus,2-s2.0-84906932025
"Hu X., Tang J., Zhang Y., Liu H.","Social spammer detection in microblogging",2013,"IJCAI International Joint Conference on Artificial Intelligence",61,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063139&partnerID=40&md5=20b782e1f1e7445a322cd63d7b435e6d","The availability of microblogging, like Twitter and Sina Weibo, makes it a popular platform for spammers to unfairly overpower normal users with unwanted content via social networks, known as social spamming. The rise of social spamming can significantly hinder the use of microblogging systems for effective information dissemination and sharing. Distinct features of microblogging systems present new challenges for social spammer detection. First, unlike traditional social networks, microblogging allows to establish some connections between two parties without mutual consent, which makes it easier for spammers to imitate normal users by quickly accumulating a large number of ""human"" friends. Second, microblogging messages are short, noisy, and unstructured. Traditional social spammer detection methods are not directly applicable to microblogging. In this paper, we investigate how to collectively use network and content information to perform effective social spammer detection in microblogging. In particular, we present an optimization formulation that models the social network and content information in a unified framework. Experiments on a real-world Twitter dataset demonstrate that our proposed method can effectively utilize both kinds of information for social spammer detection.",,"Content information; Microblogging; Optimization formulations; Popular platform; Real-world; Sina-weibo; Spammer detections; Unified framework; Artificial intelligence; Information dissemination; Internet; Spamming; Social networking (online)",Conference Paper,Scopus,2-s2.0-84896063139
"Onal C.D., Rus D.","Autonomous undulatory serpentine locomotion utilizing body dynamics of a fluidic soft robot",2013,"Bioinspiration and Biomimetics",61,10.1088/1748-3182/8/2/026003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878317610&doi=10.1088%2f1748-3182%2f8%2f2%2f026003&partnerID=40&md5=34785156aeccdd16e2fb56c82e9a95e4","Soft robotics offers the unique promise of creating inherently safe and adaptive systems. These systems bring man-made machines closer to the natural capabilities of biological systems. An important requirement to enable self-contained soft mobile robots is an on-board power source. In this paper, we present an approach to create a bio-inspired soft robotic snake that can undulate in a similar way to its biological counterpart using pressure for actuation power, without human intervention. With this approach, we develop an autonomous soft snake robot with on-board actuation, power, computation and control capabilities. The robot consists of four bidirectional fluidic elastomer actuators in series to create a traveling curvature wave from head to tail along its body. Passive wheels between segments generate the necessary frictional anisotropy for forward locomotion. It takes 14 h to build the soft robotic snake, which can attain an average locomotion speed of 19 mm s -1. © 2013 IOP Publishing Ltd.",,"Body dynamics; Control capabilities; Frictional anisotropy; Human intervention; Power sources; Snake robots; Soft robot; Soft robotics; Adaptive systems; Robotics; Robots; Serpentine; Biped locomotion; animal; artificial intelligence; biological model; biomimetics; computer simulation; device failure analysis; devices; equipment design; flow kinetics; hardness; locomotion; physiology; robotics; snake; article; biomimetics; equipment; equipment failure; flow kinetics; locomotion; physiology; robotics; snake; Animals; Artificial Intelligence; Biomimetics; Computer Simulation; Equipment Design; Equipment Failure Analysis; Hardness; Locomotion; Models, Biological; Rheology; Robotics; Snakes; Animals; Artificial Intelligence; Biomimetics; Computer Simulation; Equipment Design; Equipment Failure Analysis; Hardness; Locomotion; Models, Biological; Rheology; Robotics; Snakes",Article,Scopus,2-s2.0-84878317610
"Kisi O., Shiri J., Tombul M.","Modeling rainfall-runoff process using soft computing techniques",2013,"Computers and Geosciences",61,10.1016/j.cageo.2012.07.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870159777&doi=10.1016%2fj.cageo.2012.07.001&partnerID=40&md5=6611a5b88eb3f470cbcc40db456fc8e3","Rainfall-runoff process was modeled for a small catchment in Turkey, using 4 years (1987-1991) of measurements of independent variables of rainfall and runoff values. The models used in the study were Artificial Neural Networks (ANNs), Adaptive Neuro-Fuzzy Inference System (ANFIS) and Gene Expression Programming (GEP) which are Artificial Intelligence (AI) approaches. The applied models were trained and tested using various combinations of the independent variables. The goodness of fit for the model was evaluated in terms of the coefficient of determination (R2), root mean square error (RMSE), mean absolute error (MAE), coefficient of efficiency (CE) and scatter index (SI). A comparison was also made between these models and traditional Multi Linear Regression (MLR) model. The study provides evidence that GEP (with RMSE=17.82l/s, MAE=6.61l/s, CE=0.72 and R2=0.978) is capable of modeling rainfall-runoff process and is a viable alternative to other applied artificial intelligence and MLR time-series methods. © 2012 Elsevier Ltd.","Gene expression programming; Neural networks; Neuro-fuzzy system; Rainfall-runoff process","Adaptive neuro-fuzzy inference system; Coefficient of determination; Gene expression programming; Goodness of fit; Independent variables; Mean absolute error; Multi-linear regression; Neurofuzzy system; Rainfall-runoff process; Root mean square errors; Scatter index; Small catchment; Softcomputing techniques; Fuzzy systems; Mean square error; Neural networks; Soft computing; Rain; artificial intelligence; artificial neural network; error analysis; fuzzy mathematics; modeling; rainfall-runoff modeling; regression analysis; time series analysis; Turkey",Article,Scopus,2-s2.0-84870159777
"Furman D., Jojic V., Kidd B., Shen-Orr S., Price J., Jarrell J., Tse T., Huang H., Lund P., Maecker H.T., Utz P.J., Dekker C.L., Koller D., Davis M.M.","Apoptosis and other immune biomarkers predict influenza vaccine responsiveness",2013,"Molecular Systems Biology",61,10.1038/msb.2013.15,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876516949&doi=10.1038%2fmsb.2013.15&partnerID=40&md5=be608f2c7d4f94d1662d00c801e4b774","Despite the importance of the immune system in many diseases, there are currently no objective benchmarks of immunological health. In an effort to identifying such markers, we used influenza vaccination in 30 young (20-30 years) and 59 older subjects (60 to >89 years) as models for strong and weak immune responses, respectively, and assayed their serological responses to influenza strains as well as a wide variety of other parameters, including gene expression, antibodies to hemagglutinin peptides, serum cytokines, cell subset phenotypes and in vitro cytokine stimulation. Using machine learning, we identified nine variables that predict the antibody response with 84% accuracy. Two of these variables are involved in apoptosis, which positively associated with the response to vaccination and was confirmed to be a contributor to vaccine responsiveness in mice. The identification of these biomarkers provides new insights into what immune features may be most important for immune health. © 2013 EMBO and Macmillan Publishers Limited.","aging; apoptosis; influenza; systems immunology; vaccinology","biological marker; cytokine; hemagglutinin; influenza vaccine; biological marker; cytokine; influenza vaccine; subunit vaccine; virus antibody; antibody; influenza vaccine; peptide derivative; adult; aged; antibody response; apoptosis; article; clinical article; controlled study; cytokine production; female; gene expression; human; immune response; immune system; immunoassay; influenza; machine learning; male; phenotype; prediction; priority journal; serology; T lymphocyte subpopulation; treatment response; vaccination; virus strain; age; animal; apoptosis; artificial intelligence; blood; drug effect; humoral immunity; immunology; influenza; Influenza virus A; middle aged; mouse; prognosis; vaccination; very elderly; accuracy; animal cell; animal experiment; Article; human experiment; immune response; immunostimulation; in vitro study; influenza vaccination; nonhuman; normal human; Adult; Age Factors; Aged; Aged, 80 and over; Animals; Antibodies, Viral; Apoptosis; Artificial Intelligence; Biological Markers; Cytokines; Female; Humans; Immunity, Humoral; Influenza A virus; Influenza Vaccines; Influenza, Human; Male; Mice; Middle Aged; Prognosis; Vaccination; Vaccines, Subunit; Mus",Article,Scopus,2-s2.0-84876516949
"Qian M., Zhai C.","Robust Unsupervised Feature Selection",2013,"IJCAI International Joint Conference on Artificial Intelligence",60,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063375&partnerID=40&md5=a5ebb367bad0a0fad99980956773a6f6","A new unsupervised feature selection method, i.e., Robust Unsupervised Feature Selection (RUFS), is proposed. Unlike traditional unsupervised feature selection methods, pseudo cluster labels are learned via local learning regularized robust nonnegative matrix factorization. During the label learning process, feature selection is performed simultaneously by robust joint l 2,1 norms minimization. Since RUFS utilizes l2,1 norm minimization on processes of both label learning and feature learning, outliers and noise could be effectively handled and redundant or noisy features could be effectively reduced. Our method adopts the advantages of robust nonnegative matrix factorization, local learning, and robust feature learning. In order to make RUFS be scalable, we design a (projected) limited-memory BFGS based iterative algorithm to efficiently solve the optimization problem of RUFS in terms of both memory consumption and computation complexity. Experimental results on different benchmark real world datasets show the promising performance of RUFS over the state-of-the-arts.",,"Computation complexity; Feature learning; Iterative algorithm; Memory consumption; Nonnegative matrix factorization; Optimization problems; Real-world datasets; Unsupervised feature selection; Algorithms; Benchmarking; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84896063375
"Gowayyed M.A., Torki M., Hussein M.E., El-Saban M.","Histogram of Oriented Displacements (HOD): Describing trajectories of human joints for action recognition",2013,"IJCAI International Joint Conference on Artificial Intelligence",60,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063976&partnerID=40&md5=e67be50c57135828f2c499ce218e149b","Creating descriptors for trajectories has many applications in robotics/human motion analysis and video copy detection. Here, we propose a novel descriptor for 2D trajectories: Histogram of Oriented Displacements (HOD). Each displacement in the trajectory votes with its length in a histogram of orientation angles. 3D trajectories are described by the HOD of their three projections. We use HOD to describe the 3D trajectories of body joints to recognize human actions, which is a challenging machine vision task, with applications in human-robot/machine interaction, interactive entertainment, multimedia information retrieval, and surveillance. The descriptor is fixed-length, scale-invariant and speed-invariant. Experiments on MSR-Action3D and HDM05 datasets show that the descriptor outperforms the state-of-the-art when using off-the-shelf classification tools.",,"Action recognition; Classification tool; Interactive entertainment; Motion analysis; Multimedia information retrieval; Orientation angles; Scale-invariant; Video copy detection; Artificial intelligence; Computer vision; Graphic methods; Image recognition; Motion estimation; Three dimensional; Trajectories",Conference Paper,Scopus,2-s2.0-84896063976
"Soares J., Morais H., Sousa T., Vale Z., Faria P.","Day-ahead resource scheduling including demand response for electric vehicles",2013,"IEEE Transactions on Smart Grid",60,10.1109/TSG.2012.2235865,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875050366&doi=10.1109%2fTSG.2012.2235865&partnerID=40&md5=320f52ce785a8c2ad08635fc53b4b5ae","The energy resource scheduling is becoming increasingly important, as the use of distributed resources is intensified and massive gridable vehicle (V2G) use is envisaged. This paper presents a methodology for day-ahead energy resource scheduling for smart grids considering the intensive use of distributed generation and V2G. The main focus is the comparison of different EV management approaches in the day-ahead energy resources management, namely uncontrolled charging, smart charging, V2G and Demand Response (DR) programs in the V2G approach. Three different DR programs are designed and tested (trip reduce, shifting reduce and reduce+shifting). Other important contribution of the paper is the comparison between deterministic and computational intelligence techniques to reduce the execution time. The proposed scheduling is solved with a modified particle swarm optimization. Mixed integer non-linear programming is also used for comparison purposes. Full ac power flow calculation is included to allow taking into account the network constraints. A case study with a 33-bus distribution network and 2000 V2G resources is used to illustrate the performance of the proposed method. © 2012 IEEE.","Demand response; electric vehicle; energy resource management; particle swarm optimization","Computational intelligence techniques; Demand response; Demand response programs; Distributed resources; Energy resource managements; Mixed-integer nonlinear programming; Modified particle swarm optimization; Uncontrolled charging; Artificial intelligence; Distributed power generation; Electric vehicles; Energy resources; Nonlinear programming; Particle swarm optimization (PSO); Scheduling",Article,Scopus,2-s2.0-84875050366
"Ferrucci D., Levas A., Bagchi S., Gondek D., Mueller E.T.","Watson: Beyond jeopardy!",2013,"Artificial Intelligence",60,10.1016/j.artint.2012.06.009,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878844149&doi=10.1016%2fj.artint.2012.06.009&partnerID=40&md5=adedd86d1d00a5ba8c4e49c1606a4883","This paper presents a vision for applying the Watson technology to health care and describes the steps needed to adapt and improve performance in a new domain. Specifically, it elaborates upon a vision for an evidence-based clinical decision support system, based on the DeepQA technology, that affords exploration of a broad range of hypotheses and their associated evidence, as well as uncovers missing information that can be used in mixed-initiative dialog. It describes the research challenges, the adaptation approach, and finally reports results on the first steps we have taken toward this goal. © 2012 Elsevier B.V.",,"Clinical decision support systems; Evidence-based; Improve performance; Missing information; Mixed-initiative; Research challenges; Artificial intelligence; Decision support systems; Health care",Article,Scopus,2-s2.0-84878844149
"Dou W., Yu L., Wang X., Ma Z., Ribarsky W.","HierarchicalTopics: Visually exploring large text collections using topic hierarchies",2013,"IEEE Transactions on Visualization and Computer Graphics",59,10.1109/TVCG.2013.162,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886651912&doi=10.1109%2fTVCG.2013.162&partnerID=40&md5=02b953bc29c7479650d4b1384137db6b","Analyzing large textual collections has become increasingly challenging given the size of the data available and the rate that more data is being generated. Topic-based text summarization methods coupled with interactive visualizations have presented promising approaches to address the challenge of analyzing large text corpora. As the text corpora and vocabulary grow larger, more topics need to be generated in order to capture the meaningful latent themes and nuances in the corpora. However, it is difficult for most of current topic-based visualizations to represent large number of topics without being cluttered or illegible. To facilitate the representation and navigation of a large number of topics, we propose a visual analytics system-HierarchicalTopic (HT). HT integrates a computational algorithm, Topic Rose Tree, with an interactive visual interface. The Topic Rose Tree constructs a topic hierarchy based on a list of topics. The interactive visual interface is designed to present the topic content as well as temporal evolution of topics in a hierarchical fashion. User interactions are provided for users to make changes to the topic hierarchy based on their mental model of the topic space. To qualitatively evaluate HT, we present a case study that showcases how HierarchicalTopics aid expert users in making sense of a large number of topics and discovering interesting patterns of topic groups. We have also conducted a user study to quantitatively evaluate the effect of hierarchical topic structure. The study results reveal that the HT leads to faster identification of large number of relevant topics. We have also solicited user feedback during the experiments and incorporated some suggestions into the current version of HierarchicalTopics. © 1995-2012 IEEE.","Hierarchical topic representation; rose tree; topic modeling; visual analytics","Computational algorithm; Hierarchical topic representation; Interactive visualizations; Rose tree; Temporal evolution; Text summarization; Topic Modeling; Visual analytics; Forestry; Visualization; Trees (mathematics); Algorithms; Computation; Forestry; algorithm; artificial intelligence; automated pattern recognition; computer assisted diagnosis; computer graphics; computer interface; computer program; documentation; image enhancement; information retrieval; natural language processing; procedures; article; automated pattern recognition; computer assisted diagnosis; information retrieval; methodology; Algorithms; Artificial Intelligence; Computer Graphics; Documentation; Image Enhancement; Image Interpretation, Computer-Assisted; Information Storage and Retrieval; Natural Language Processing; Pattern Recognition, Automated; Software; User-Computer Interface; Algorithms; Artificial Intelligence; Computer Graphics; Documentation; Image Enhancement; Image Interpretation, Computer-Assisted; Information Storage and Retrieval; Natural Language Processing; Pattern Recognition, Automated; Software; User-Computer Interface",Article,Scopus,2-s2.0-84886651912
"Samek W., Meinecke F.C., Muller K.-R.","Transferring subspaces between subjects in brain - Computer interfacing",2013,"IEEE Transactions on Biomedical Engineering",59,10.1109/TBME.2013.2253608,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880856659&doi=10.1109%2fTBME.2013.2253608&partnerID=40&md5=1ebf3a96624bf5a4649d41a13c72ba5c","Compensating changes between a subjects' training and testing session in brain-computer interfacing (BCI) is challenging but of great importance for a robust BCI operation. We show that such changes are very similar between subjects, and thus can be reliably estimated using data from other users and utilized to construct an invariant feature space. This novel approach to learning from other subjects aims to reduce the adverse effects of common nonstationarities, but does not transfer discriminative information. This is an important conceptual difference to standard multisubject methods that, e.g., improve the covariance matrix estimation by shrinking it toward the average of other users or construct a global feature space. These methods do not reduces the shift between training and test data and may produce poor results when subjects have very different signal characteristics. In this paper, we compare our approach to two state-of-the-art multisubject methods on toy data and two datasets of EEG recordings from subjects performing motor imagery. We show that it can not only achieve a significant increase in performance, but also that the extracted change patterns allow for a neurophysiologically meaningful interpretation. © 1964-2012 IEEE.","Brain-computer interface (BCI); common spatial patterns (CSP); nonstationarity; transfer learning","Brain-computer interfacing; Common spatial patterns; Covariance matrix estimation; Discriminative informations; Non-stationarities; Signal characteristic; Training and testing; Transfer learning; Covariance matrix; Electroencephalography; Interfaces (computer); Brain computer interface; article; brain computer interface; electroencephalogram; human; human experiment; information; learning; neurophysiology; normal human; training; visual stimulation; Algorithms; Artificial Intelligence; Brain; Brain Mapping; Brain-Computer Interfaces; Electroencephalography; Evoked Potentials; Humans; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84880856659
"O'Leary D.E.","Artificial intelligence and big data",2013,"IEEE Intelligent Systems",59,10.1109/MIS.2013.39,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880170201&doi=10.1109%2fMIS.2013.39&partnerID=40&md5=60212104427440a9799ed77ec716b22b","AI has been used in several different ways to facilitate capturing and structuring big data, and AI has been used to analyze big data for key insights. Some of the basic concerns and uses are examined here, while future articles will present case studies that analyze emerging issues and approaches integrating AI and big data. © 2001-2011 IEEE.","AI; artificial intelligence; big data; intelligent systems; parallelization; visualization","Big datum; Parallelizations; Artificial intelligence; Flow visualization; Intelligent systems; Data visualization",Article,Scopus,2-s2.0-84880170201
"Zhuang Y., Wang Y., Wu F., Zhang Y., Lu W.","Supervised coupled dictionary learning with group structures for multi-modal retrieval",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",58,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893365380&partnerID=40&md5=73c510e8399aa0eb6213bce48e53e644","A better similarity mapping function across heterogeneous high-dimensional features is very desirable for many applications involving multi-modal data. In this paper, we introduce coupled dictionary learning (DL) into supervised sparse coding for multi-modal (crossmedia) retrieval. We call this Supervised coupleddictionary learning with group structures for Multi-Modal retrieval (SliM2). SliM2 formulates the multimodal mapping as a constrained dictionary learning problem. By utilizing the intrinsic power of DL to deal with the heterogeneous features, SliM2 extends unimodal DL to multi-modal DL. Moreover, the label information is employed in SliM2 to discover the shared structure inside intra-modality within the same class by a mixed norm (i.e., ℓ1=ℓ2-norm). As a result, the multimodal retrieval is conducted via a set of jointly learned mapping functions across multi-modal data. The experimental results show the effectiveness of our proposed model when applied to cross-media retrieval. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Cross-media retrieval; Dictionary learning; Heterogeneous features; High-dimensional features; Label information; Mapping functions; Shared structures; Similarity mappings; Artificial intelligence; Modal analysis",Conference Paper,Scopus,2-s2.0-84893365380
"Carmena J.M.","Advances in Neuroprosthetic Learning and Control",2013,"PLoS Biology",58,10.1371/journal.pbio.1001561,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878331756&doi=10.1371%2fjournal.pbio.1001561&partnerID=40&md5=e2b9927c75762bb214e940e5839a58de","Significant progress has occurred in the field of brain-machine interfaces (BMI) since the first demonstrations with rodents, monkeys, and humans controlling different prosthetic devices directly with neural activity. This technology holds great potential to aid large numbers of people with neurological disorders. However, despite this initial enthusiasm and the plethora of available robotic technologies, existing neural interfaces cannot as yet master the control of prosthetic, paralyzed, or otherwise disabled limbs. Here I briefly discuss recent advances from our laboratory into the neural basis of BMIs that should lead to better prosthetic control and clinically viable solutions, as well as new insights into the neurobiology of action. © 2013 Jose M.",,"algorithm; article; biomimetics; brain computer interface; brain machine interface; closed loop decoder adaptation; decoding; evoked response; human; learning argument; machine learning; neurologic disease; neurophysiology; neuroprosthesis; prosthetic control; prosthetic motor memory; robotics; task performance; Animals; Artificial Intelligence; Haplorhini; Humans; Neural Prostheses; Robotics; User-Computer Interface; Rodentia",Article,Scopus,2-s2.0-84878331756
"Yoo S.J.","Distributed consensus tracking for multiple uncertain nonlinear strict-feedback systems under a directed graph",2013,"IEEE Transactions on Neural Networks and Learning Systems",58,10.1109/TNNLS.2013.2238554,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875888694&doi=10.1109%2fTNNLS.2013.2238554&partnerID=40&md5=48f2a01c1fa09bb4d9de97211581ac3e","In this brief, we study the distributed consensus tracking control problem for multiple strict-feedback systems with unknown nonlinearities under a directed graph topology. It is assumed that the leader's output is time-varying and has been accessed by only a small fraction of followers in a group. The distributed dynamic surface design approach is proposed to design local consensus controllers in order to guarantee the consensus tracking between the followers and the leader. The function approximation technique using neural networks is employed to compensate unknown nonlinear terms induced from the controller design procedure. From the Lyapunov stability theorem, it is shown that the consensus errors are cooperatively semiglobally uniformly ultimately bounded and converge to an adjustable neighborhood of the origin. © 2012 IEEE.","Consensus; function approximation technique; networked nonlinear systems; unmatched uncertainties","Consensus; Distributed consensus; Function approximation techniques; Lyapunov stability theorem; Semi-globally uniformly ultimately bounded; Strict feedback systems; Tracking control problem; Unmatched uncertainty; Artificial intelligence; Computer networks; Directed graphs",Article,Scopus,2-s2.0-84875888694
"Pan W., Chen L.","GBPR: Group preference based bayesian personalized ranking for one-class collaborative filtering",2013,"IJCAI International Joint Conference on Artificial Intelligence",57,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063244&partnerID=40&md5=f75deb48d2e911cfac7df6826cad8c27","One-class collaborative filtering or collaborative ranking with implicit feedback has been steadily receiving more attention, mostly due to the ""oneclass"" characteristics of data in various services, e.g., ""like"" in Facebook and ""bought"" in Amazon. Previous works for solving this problem include pointwise regression methods based on absolute rating assumptions and pairwise ranking methods with relative score assumptions, where the latter was empirically found performing much better because it models users' ranking-related preferences more directly. However, the two fundamental assumptions made in the pairwise ranking methods, (1) individual pairwise preference over two items and (2) independence between two users, may not always hold. As a response, we propose a new and improved assumption, group Bayesian personalized ranking (GBPR), via introducing richer interactions among users. In particular, we introduce group preference, to relax the aforementioned individual and independence assumptions. We then design a novel algorithm correspondingly, which can recommend items more accurately as shown by various ranking-oriented evaluation metrics on four real-world datasets in our experiments.",,"Evaluation metrics; Implicit feedback; Independence assumption; Novel algorithm; Preference-based; Ranking methods; Real-world datasets; Regression method; Algorithms; Artificial intelligence; Regression analysis; Collaborative filtering",Conference Paper,Scopus,2-s2.0-84896063244
"Peiravian N., Zhu X.","Machine learning for Android malware detection using permission and API calls",2013,"Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI",57,10.1109/ICTAI.2013.53,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897734703&doi=10.1109%2fICTAI.2013.53&partnerID=40&md5=92bbe15c4fed7d2aef89d8db6e580c32","The Google Android mobile phone platform is one of the most anticipated smartphone operating systems on the market. The open source Android platform allows developers to take full advantage of the mobile operation system, but also raises significant issues related to malicious applications. On one hand, the popularity of Android absorbs attention of most developers for producing their applications on this platform. The increased numbers of applications, on the other hand, prepares a suitable prone for some users to develop different kinds of malware and insert them in Google Android market or other third party markets as safe applications. In this paper, we propose to combine permission and API (Application Program Interface) calls and use machine learning methods to detect malicious Android Apps. In our design, the permission is extracted from each App's profile information and the APIs are extracted from the packed App file by using packages and classes to represent API calls. By using permissions and API calls as features to characterize each Apps, we can learn a classifier to identify whether an App is potentially malicious or not. An inherent advantage of our method is that it does not need to involve any dynamical tracing of the system calls but only uses simple static analysis to find system functions involved in each App. In addition, because permission settings and APIs are alwaysavailable for each App, our method can be generalized to all mobile applications. Experiments on real-world Apps with more than 1200 malware and 1200 benign samples validate the algorithm performance. © 2013 IEEE.","Android; API calls; Malware detection; Permissions; Smartphone Security","Android; API calls; Malware detection; Permissions; Smartphone securities; Artificial intelligence; Commerce; Computer crime; Learning systems; Open systems; Robots; Signal encoding; Smartphones; Static analysis; Tools; Application programming interfaces (API)",Conference Paper,Scopus,2-s2.0-84897734703
"Tang J., Hu X., Gao H., Liu H.","Exploiting local and global social context for recommendation",2013,"IJCAI International Joint Conference on Artificial Intelligence",57,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062618&partnerID=40&md5=52cfc377c8e44aa527ff15017c9e59e2","With the fast development of social media, the information overload problem becomes increasingly severe and recommender systems play an important role in helping online users find relevant information by suggesting information of potential interests. Social activities for online users produce abundant social relations. Social relations provide an independent source for recommendation, presenting both opportunities and challenges for traditional recommender systems. Users are likely to seek suggestions from both their local friends and users with high global reputations, motivating us to exploit social relations from local and global perspectives for online recommender systems in this paper. We develop approaches to capture local and global social relations, and propose a novel framework LOCABAL taking advantage of both local and global social context for recommendation. Empirical results on real-world datasets demonstrate the effectiveness of our proposed framework and further experiments are conducted to understand how local and global social context work for the proposed framework.",,"Global perspective; Independent sources; Information overloads; Online recommender systems; Real-world datasets; Social activities; Social context; Social relations; Artificial intelligence; Recommender systems; Online systems",Conference Paper,Scopus,2-s2.0-84896062618
"Menden M.P., Iorio F., Garnett M., McDermott U., Benes C.H., Ballester P.J., Saez-Rodriguez J.","Machine Learning Prediction of Cancer Cell Sensitivity to Drugs Based on Genomic and Chemical Properties",2013,"PLoS ONE",57,10.1371/journal.pone.0061318,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876958088&doi=10.1371%2fjournal.pone.0061318&partnerID=40&md5=c776458c082fd935fe2927b43ce0594c","Predicting the response of a specific cancer to a therapy is a major goal in modern oncology that should ultimately lead to a personalised treatment. High-throughput screenings of potentially active compounds against a panel of genomically heterogeneous cancer cell lines have unveiled multiple relationships between genomic alterations and drug responses. Various computational approaches have been proposed to predict sensitivity based on genomic features, while others have used the chemical properties of the drugs to ascertain their effect. In an effort to integrate these complementary approaches, we developed machine learning models to predict the response of cancer cell lines to drug treatment, quantified through IC50 values, based on both the genomic features of the cell lines and the chemical properties of the considered drugs. Models predicted IC50 values in a 8-fold cross-validation and an independent blind test with coefficient of determination R2 of 0.72 and 0.64 respectively. Furthermore, models were able to predict with comparable accuracy (R2 of 0.61) IC50s of cell lines from a tissue not used in the training stage. Our in silico models can be used to optimise the experimental design of drug-cell screenings by estimating a large proportion of missing IC50 values rather than experimentally measuring them. The implications of our results go beyond virtual drug screening design: potentially thousands of drugs could be probed in silico to systematically test their potential efficacy as anti-tumour agents based on their structure, thus providing a computational framework to identify new drug repositioning opportunities as well as ultimately be useful for personalized medicine by linking the genomic traits of patients to drug sensitivity. © 2013 Menden et al.",,"accuracy; article; artificial neural network; biochemistry; cancer cell culture; computer model; computer program; controlled study; copy number variation; dose response; drug screening; drug selectivity; drug sensitivity; drug structure; experimental design; genetic variability; genomics; IC 50; information processing; machine learning; microsatellite instability; personalized medicine; physical chemistry; predictive value; simplified molecular input line entry system format; validation process; analysis of variance; artificial intelligence; computer simulation; drug resistance; genetics; genomics; human; IC50; Neoplasms; pharmacogenetics; procedures; workflow; antineoplastic agent; Analysis of Variance; Antineoplastic Agents; Artificial Intelligence; Computer Simulation; Drug Resistance, Neoplasm; Genomics; Humans; Inhibitory Concentration 50; Neoplasms; Pharmacogenetics; Workflow",Article,Scopus,2-s2.0-84876958088
"Mavrovouniotis M., Yang S.","Ant colony optimization with immigrants schemes for the dynamic travelling salesman problem with traffic factors",2013,"Applied Soft Computing Journal",57,10.1016/j.asoc.2013.05.022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885369640&doi=10.1016%2fj.asoc.2013.05.022&partnerID=40&md5=6532ece85f0c8bfbea280aff4fc62752","Traditional ant colony optimization (ACO) algorithms have difficulty in addressing dynamic optimization problems (DOPs). This is because once the algorithm converges to a solution and a dynamic change occurs, it is difficult for the population to adapt to a new environment since high levels of pheromone will be generated to a single trail and force the ants to follow it even after a dynamic change. A good solution to address this problem is to increase the diversity via transferring knowledge from previous environments to the pheromone trails using immigrants schemes. In this paper, an ACO framework for dynamic environments is proposed where different immigrants schemes, including random immigrants, elitism-based immigrants, and memory-based immigrants, are integrated into ACO algorithms for solving DOPs. From this framework, three ACO algorithms, where immigrant ants are generated using the aforementioned immigrants schemes and replace existing ants in the current population, are proposed and investigated. Moreover, two novel types of dynamic travelling salesman problems (DTSPs) with traffic factors, i.e., under random and cyclic dynamic environments, are proposed for the experimental study. The experimental results based on different DTSP test cases show that each proposed algorithm performs well on different environmental cases and that the proposed algorithms outperform several other peer ACO algorithms. © 2013 Elsevier B.V. All rights reserved.","Ant colony optimization; Dynamic optimization problem; Dynamic travelling salesman problem; Immigrants schemes; Traffic factor","Artificial intelligence; Evolutionary algorithms; Optimization; Traveling salesman problem; Ant Colony Optimization algorithms; Dynamic environments; Dynamic optimization problem (DOP); Immigrants schemes; Pheromone trails; Random immigrants; Traffic factors; Travelling salesman problem; Ant colony optimization",Article,Scopus,2-s2.0-84885369640
"Brajevic I., Tuba M.","An upgraded artificial bee colony (ABC) algorithm for constrained optimization problems",2013,"Journal of Intelligent Manufacturing",56,10.1007/s10845-011-0621-6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880258792&doi=10.1007%2fs10845-011-0621-6&partnerID=40&md5=a963963d7b869e1964e69cb71d4f87ae","Artificial bee colony (ABC) algorithm developed by Karaboga is a nature inspired metaheuristic based on honey bee foraging behavior. It was successfully applied to continuous unconstrained optimization problems and later it was extended to constrained design problems as well. This paper introduces an upgraded artificial bee colony (UABC) algorithm for constrained optimization problems. Our UABC algorithm enhances fine-tuning characteristics of the modification rate parameter and employs modified scout bee phase of the ABC algorithm. This upgraded algorithm has been implemented and tested on standard engineering benchmark problems and the performance was compared to the performance of the latest Akay and Karaboga's ABC algorithm. Our numerical results show that the proposed UABC algorithm produces better or equal best and average solutions in less evaluations in all cases. © 2012 Springer Science+Business Media, LLC.","Artificial bee colony (ABC); Constrained optimization; Nature inspired metaheuristics; Swarm intelligence","Artificial bee colonies; Artificial bee colonies (ABC); Artificial bee colony algorithms (ABC); Bench-mark problems; Constrained optimi-zation problems; Meta heuristics; Swarm Intelligence; Unconstrained optimization problems; Artificial intelligence; Constrained optimization; Algorithms",Article,Scopus,2-s2.0-84880258792
"Salakhutdinov R., Tenenbaum J.B., Torralba A.","Learning with hierarchical-deep models",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",56,10.1109/TPAMI.2012.269,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879873133&doi=10.1109%2fTPAMI.2012.269&partnerID=40&md5=c7eb51d14f287d10c4e7d357f2fd0f58","We introduce HD (or Hierarchical-Deep) models, a new compositional learning architecture that integrates deep learning models with structured hierarchical Bayesian (HB) models. Specifically, we show how we can learn a hierarchical Dirichlet process (HDP) prior over the activities of the top-level features in a deep Boltzmann machine (DBM). This compound HDP-DBM model learns to learn novel concepts from very few training example by learning low-level generic features, high-level features that capture correlations among low-level features, and a category hierarchy for sharing priors over the high-level features that are typical of different kinds of concepts. We present efficient learning and inference algorithms for the HDP-DBM model and show that it is able to learn new concepts from very few examples on CIFAR-100 object recognition, handwritten character recognition, and human motion capture datasets. © 1979-2012 IEEE.","deep Boltzmann machines; Deep networks; hierarchical Bayesian models; one-shot learning","Deep boltzmann machines; Hand written character recognition; Hierarchical bayesian; Hierarchical Bayesian models; Hierarchical dirichlet process (HDP); Human motion capture; Learning architectures; One-shot learning; Bayesian networks; Character recognition; Object recognition; Inference engines; algorithm; artificial intelligence; automated pattern recognition; Bayes theorem; human; motion; procedures; vision; article; automated pattern recognition; methodology; Algorithms; Artificial Intelligence; Bayes Theorem; Humans; Motion; Pattern Recognition, Automated; Visual Perception; Algorithms; Artificial Intelligence; Bayes Theorem; Humans; Motion; Pattern Recognition, Automated; Visual Perception",Article,Scopus,2-s2.0-84879873133
"Wu X., Yu K., Ding W., Wang H., Zhu X.","Online feature selection with streaming features",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",56,10.1109/TPAMI.2012.197,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875452657&doi=10.1109%2fTPAMI.2012.197&partnerID=40&md5=a0d1357c0db5fe87188ce04662442fd8","We propose a new online feature selection framework for applications with streaming features where the knowledge of the full feature space is unknown in advance. We define streaming features as features that flow in one by one over time whereas the number of training examples remains fixed. This is in contrast with traditional online learning methods that only deal with sequentially added observations, with little attention being paid to streaming features. The critical challenges for Online Streaming Feature Selection (OSFS) include 1) the continuous growth of feature volumes over time, 2) a large feature space, possibly of unknown or infinite size, and 3) the unavailability of the entire feature set before learning starts. In the paper, we present a novel Online Streaming Feature Selection method to select strongly relevant and nonredundant features on the fly. An efficient Fast-OSFS algorithm is proposed to improve feature selection performance. The proposed algorithms are evaluated extensively on high-dimensional datasets and also with a real-world case study on impact crater detection. Experimental results demonstrate that the algorithms achieve better compactness and higher prediction accuracy than existing streaming feature selection algorithms. © 1979-2012 IEEE.","Feature selection; streaming features; supervised learning","Critical challenges; Feature selection algorithm; Feature selection methods; High-dimensional; On-line learning methods; Online feature selection; Prediction accuracy; Training example; Artificial intelligence; Computer vision; Feature extraction; Supervised learning; Algorithms",Article,Scopus,2-s2.0-84875452657
"Kang F., Li J., Li H.","Artificial bee colony algorithm and pattern search hybridized for global optimization",2013,"Applied Soft Computing Journal",56,10.1016/j.asoc.2012.12.025,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873671929&doi=10.1016%2fj.asoc.2012.12.025&partnerID=40&md5=68bd5d7757c5e624158da4bdea4d3193","Artificial bee colony algorithm is one of the most recently proposed swarm intelligence based optimization algorithm. A memetic algorithm which combines Hooke-Jeeves pattern search with artificial bee colony algorithm is proposed for numerical global optimization. There are two alternative phases of the proposed algorithm: the exploration phase realized by artificial bee colony algorithm and the exploitation phase completed by pattern search. The proposed algorithm was tested on a comprehensive set of benchmark functions, encompassing a wide range of dimensionality. Results show that the new algorithm is promising in terms of convergence speed, solution accuracy and success rate. The performance of artificial bee colony algorithm is much improved by introducing a pattern search method, especially in handling functions having narrow curving valley, functions with high eccentric ellipse and some complex multimodal functions. © 2013 Elsevier B.V.","Artificial bee colony algorithm; Evolutionary computation; Global optimization; Memetic algorithm; Swarm intelligence","Artificial bee colony algorithms; Benchmark functions; Convergence speed; Exploration phase; Hooke-Jeeves; Memetic algorithms; Multi modal function; Optimization algorithms; Pattern search; Pattern search method; Solution accuracy; Swarm Intelligence; Artificial intelligence; Global optimization; Evolutionary algorithms",Article,Scopus,2-s2.0-84873671929
"Jia K., Wang X., Tang X.","Image transformation based on learning dictionaries across image spaces",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",56,10.1109/TPAMI.2012.95,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871727602&doi=10.1109%2fTPAMI.2012.95&partnerID=40&md5=af8b78f1caae684f77a73ee530c9a529","In this paper, we propose a framework of transforming images from a source image space to a target image space, based on learning coupled dictionaries from a training set of paired images. The framework can be used for applications such as image super-resolution and estimation of image intrinsic components (shading and albedo). It is based on a local parametric regression approach, using sparse feature representations over learned coupled dictionaries across the source and target image spaces. After coupled dictionary learning, sparse coefficient vectors of training image patch pairs are partitioned into easily retrievable local clusters. For any test image patch, we can fast index into its closest local cluster and perform a local parametric regression between the learned sparse feature spaces. The obtained sparse representation (together with the learned target space dictionary) provides multiple constraints for each pixel of the target image to be estimated. The final target image is reconstructed based on these constraints. The contributions of our proposed framework are three-fold. 1) We propose a concept of coupled dictionary learning based on coupled sparse coding which requires the sparse coefficient vectors of a pair of corresponding source and target image patches to have the same support, i.e., the same indices of nonzero elements. 2) We devise a space partitioning scheme to divide the high-dimensional but sparse feature space into local clusters. The partitioning facilitates extremely fast retrieval of closest local clusters for query patches. 3) Benefiting from sparse feature-based image transformation, our method is more robust to corrupted input data, and can be considered as a simultaneous image restoration and transformation process. Experiments on intrinsic image estimation and super-resolution demonstrate the effectiveness and efficiency of our proposed method. © 2012 IEEE.","image mapping; Image transformation; intrinsic images; sparse coding; super-resolution","Image mapping; Image transformations; intrinsic images; Sparse coding; Super resolution; Image reconstruction; Optical resolving power; Image coding; algorithm; article; artificial intelligence; automated pattern recognition; biological model; computer assisted diagnosis; computer simulation; image enhancement; image subtraction; information retrieval; methodology; reproducibility; sensitivity and specificity; statistical model; Algorithms; Artificial Intelligence; Computer Simulation; Image Enhancement; Image Interpretation, Computer-Assisted; Information Storage and Retrieval; Models, Biological; Models, Statistical; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-84871727602
"Sommer C., Gerlich D.W.","Machine learning in cell biology-teaching computers to recognize phenotypes",2013,"Journal of Cell Science",55,10.1242/jcs.123604,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890498817&doi=10.1242%2fjcs.123604&partnerID=40&md5=6b901346bcf9aef7ba8f228bd771273b","Recent advances in microscope automation provide new opportunities for high-throughput cell biology, such as image-based screening. High-complex image analysis tasks often make the implementation of static and predefined processing rules a cumbersome effort. Machine-learning methods, instead, seek to use intrinsic data structure, as well as the expert annotations of biologists to infer models that can be used to solve versatile data analysis tasks. Here, we explain how machine-learning methods work and what needs to be considered for their successful application in cell biology. We outline how microscopy images can be converted into a data representation suitable for machine learning, and then introduce various state-of-the-art machine-learning algorithms, highlighting recent applications in image-based screening. Our Commentary aims to provide the biologist with a guide to the application of machine learning to microscopy assays and we therefore include extensive discussion on how to optimize experimental workflow as well as the data analysis pipeline. © 2013. Published by The Company of Biologists Ltd.","Bioimage informatics; Computer vision; High-content screening; Machine learning; Microscopy","accuracy; actin filament; analytical error; article; artifact; cell structure; classification algorithm; classifier; cytology; image processing; image reconstruction; learning algorithm; machine learning; noise; phenotype; principal component analysis; priority journal; probability; single cell analysis; statistical distribution; support vector machine; variance; Bioimage informatics; Computer vision; High-content screening; Machine learning; Microscopy; Artificial Intelligence; Cytological Techniques; Humans; Image Processing, Computer-Assisted; Phenotype",Article,Scopus,2-s2.0-84890498817
"Fallah-Mehdipour E., Bozorg Haddad O., Mariño M.A.","Prediction and simulation of monthly groundwater levels by genetic programming",2013,"Journal of Hydro-Environment Research",55,10.1016/j.jher.2013.03.005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888128953&doi=10.1016%2fj.jher.2013.03.005&partnerID=40&md5=2f777b520e5e9ee400533fa8e6641056","Groundwater level is an effective parameter in the determination of accuracy in groundwater modeling. Thus, application of simple tools to predict future groundwater levels and fill-in gaps in data sets are important issues in groundwater hydrology. Prediction and simulation are two approaches that use previous and previous-current data sets to complete time series. Artificial intelligence is a computing method that is capable to predict and simulate different system states without using complex relations. This paper investigates the capability of an adaptive neural fuzzy inference system (ANFIS) and genetic programming (GP) as two artificial intelligence tools to predict and simulate groundwater levels in three observation wells in the Karaj plain of Iran. Precipitation and evaporation from a surface water body and water levels in observation wells penetrating an aquifer system are used to fill-in gaps in data sets and estimate monthly groundwater level series. Results show that GP decreases the average value of root mean squared error (RMSE) as the error criterion for the observation wells in the training and testing data sets 8.35 and 11.33 percent, respectively, compared to the average of RMSE by ANFIS in prediction. Similarly, the average value of RMSE for different observation wells used in simulation improves the accuracy of prediction 9.89 and 8.40 percent in the training and testing data sets, respectively. These results indicate that the proposed prediction and simulation approach, based on GP, is an effective tool in determining groundwater levels. © 2013 International Association for Hydro-environment Engineering and Research, Asia Pacific Division.","Adaptive neural fuzzy inference system; Genetic programming; Groundwater level; Prediction; Simulation","Adaptive neural fuzzy inference system (ANFIS); Artificial intelligence tools; Ground-water hydrology; Groundwater modeling; Prediction and simulations; Root mean squared errors; Simulation; Training and testing; Aquifers; Artificial intelligence; Forecasting; Fuzzy systems; Genetic programming; Groundwater; Surface waters; Tools; Water levels; Groundwater resources; aquifer; artificial intelligence; artificial neural network; computer simulation; error analysis; evaporation; groundwater; precipitation (climatology); prediction; water level",Article,Scopus,2-s2.0-84888128953
"Hinterstoisser S., Lepetit V., Ilic S., Holzer S., Bradski G., Konolige K., Navab N.","Model based training, detection and pose estimation of texture-less 3D objects in heavily cluttered scenes",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",55,10.1007/978-3-642-37331-2_42,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875875840&doi=10.1007%2f978-3-642-37331-2_42&partnerID=40&md5=d0375cf11a686b6d4197e7ccc6519756","We propose a framework for automatic modeling, detection, and tracking of 3D objects with a Kinect. The detection part is mainly based on the recent template-based LINEMOD approach [1] for object detection. We show how to build the templates automatically from 3D models, and how to estimate the 6 degrees-of-freedom pose accurately and in real-time. The pose estimation and the color information allow us to check the detection hypotheses and improves the correct detection rate by 13% with respect to the original LINEMOD. These many improvements make our framework suitable for object manipulation in Robotics applications. Moreover we propose a new dataset made of 15 registered, 1100+ frame video sequences of 15 various objects for the evaluation of future competing methods. © 2013 Springer-Verlag.",,"Automatic modeling; Cluttered scenes; Color information; Object Detection; Object manipulation; Pose estimation; Robotics applications; Video sequences; Artificial intelligence; Content based retrieval",Conference Paper,Scopus,2-s2.0-84875875840
"Sheth A., Anantharam P., Henson C.","Physical-cyber-social computing: An early 21st century approach",2013,"IEEE Intelligent Systems",55,10.1109/MIS.2013.20,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874591962&doi=10.1109%2fMIS.2013.20&partnerID=40&md5=247300b819d1cd878e2b0c7ea173a36a","Technology plays an increasingly important role in facilitating and improving personal and social activities, engagements, decision making, interaction with physical and social worlds, insight generation, and just about anything that humans, as intelligent beings, seek to do. The term computing for human experience (CHE) captures technology's human-centric role, emphasizing the unobtrusive, supportive, and assistive part technology plays in improving human experience. Here, the authors present an emerging paradigm called physical-cyber-social (PCS) computing, supporting the CHE vision, which encompasses a holistic treatment of data, information, and knowledge from the PCS worlds to integrate, correlate, interpret, and provide contextually relevant abstractions to humans. They also outline the types of computational operators that make up PCS computing. © 2001-2011 IEEE.","computing for human experience; data-information-knowledge-wisdom; search to solution; physical-cyber-social computing; semantic perception","Assistive; computing for human experience; Human-centric; physical-cyber-social computing; Social activities; Artificial intelligence; Intelligent systems; Semantics",Article,Scopus,2-s2.0-84874591962
"Yu H., Ni J., Zhao J.","ACOSampling: An ant colony optimization-based undersampling method for classifying imbalanced DNA microarray data",2013,"Neurocomputing",55,10.1016/j.neucom.2012.08.018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868621497&doi=10.1016%2fj.neucom.2012.08.018&partnerID=40&md5=08db4b0787a4999e3de35f2ece4588e6","In DNA microarray data, class imbalance problem occurs frequently, causing poor prediction performance for minority classes. Moreover, its other features, such as high-dimension, small sample, high noise etc., intensify this damage. In this study, we propose ACOSampling that is a novel undersampling method based on the idea of ant colony optimization (ACO) to address this problem. The algorithm starts with feature selection technology to eliminate noisy genes in data. Then we randomly and repeatedly divided the original training set into two groups: training set and validation set. In each division, one modified ACO algorithm as a variant of our previous work is conducted to filter less informative majority samples and search the corresponding optimal training sample subset. At last, the statistical results from all local optimal training sample subsets are given in the form of frequence list, where each frequence indicates the importance of the corresponding majority sample. We only extracted those high frequency ones and combined them with all minority samples to construct the final balanced training set. We evaluated the method on four benchmark skewed DNA microarray datasets by support vector machine (SVM) classifier, showing that the proposed method outperforms many other sampling approaches, which indicates its superiority. © 2012 Elsevier B.V.","Ant colony optimization; Class imbalance; DNA microarray; Support vector machine; Undersampling","ACO algorithms; Ant colonies; Ant Colony Optimization (ACO); Class imbalance; Class imbalance problems; DNA micro-array; DNA microarray data; DNA microarray datasets; Filter-less; High frequency; High noise; Local optimal; Optimal training; Prediction performance; Small samples; Training sets; Under-sampling; Artificial intelligence; Data reduction; Optimization; Sampling; Support vector machines; Algorithms; ant colony optimization sampling; article; classification algorithm; data analysis; DNA microarray; intermethod comparison; mathematical computing; priority journal; support vector machine",Article,Scopus,2-s2.0-84868621497
"Latha Shankar B., Basavarajappa S., Chen J.C.H., Kadadevaramath R.S.","Location and allocation decisions for multi-echelon supply chain network - A multi-objective evolutionary approach",2013,"Expert Systems with Applications",55,10.1016/j.eswa.2012.07.065,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867673023&doi=10.1016%2fj.eswa.2012.07.065&partnerID=40&md5=09937f844da946d2d399fd92adfa1b49","This paper aims at multi-objective optimization of single-product for four-echelon supply chain architecture consisting of suppliers, production plants, distribution centers (DCs) and customer zones (CZs). The key design decisions considered are: the number and location of plants in the system, the flow of raw materials from suppliers to plants, the quantity of products to be shipped from plants to DCs, from DCs to CZs so as to minimize the combined facility location and shipment costs subject to a requirement that maximum customer demands be met. To optimize these two objectives simultaneously, four-echelon network model is mathematically represented considering the associated constraints, capacity, production and shipment costs and solved using swarm intelligence based Multi-objective Hybrid Particle Swarm Optimization (MOHPSO) algorithm. This evolutionary based algorithm incorporates non-dominated sorting algorithm into particle swarm optimization so as to allow this heuristic to optimize two objective functions simultaneously. This can be used as decision support system for location of facilities, allocation of demand points and monitoring of material flow for four-echelon supply chain network. © 2012 Elsevier Ltd. All rights reserved.","Evolutionary approach; Four-echelon supply chain architecture; MOHPSO; Non-dominated sorting algorithm; Swarm intelligence","Evolutionary approach; MOHPSO; Non-dominated Sorting; Supply chain architecture; Swarm Intelligence; Algorithms; Artificial intelligence; Decision support systems; Multiobjective optimization; Particle swarm optimization (PSO); Supply chains; Location",Article,Scopus,2-s2.0-84867673023
"Rashedi E., Nezamabadi-Pour H., Saryazdi S.","A simultaneous feature adaptation and feature selection method for content-based image retrieval systems",2013,"Knowledge-Based Systems",55,10.1016/j.knosys.2012.10.011,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871925258&doi=10.1016%2fj.knosys.2012.10.011&partnerID=40&md5=2a9b485cfc705733cd10fe2f33aa86f1","In content-based image retrieval (CBIR) applications, each database needs its corresponding parameter setting for feature extraction. However, most of the CBIR systems perform indexing by a set of fixed and pre-specific parameters. On the other hand, feature selection methods have currently gained considerable popularity to reduce semantic gap. In this regard, this paper is devoted to present a hybrid approach to reduce the semantic gap between low level visual features and high level semantics, through simultaneous feature adaptation and feature selection. In the proposed approach, a hybrid meta-heuristic swarm intelligence-based search technique, called mixed gravitational search algorithm (MGSA), is employed. Some feature extraction parameters (i.e. the parameters of a 6-tap parameterized orthogonal mother wavelet in texture features and quantization levels in color histogram) are optimized to reach a maximum precision of the CBIR systems. Meanwhile, feature subset selection is done for the same purpose. A comparative experimental study with the conventional CBIR system is reported on a database of 1000 images. The obtained results confirm the effectiveness of the proposed adaptive indexing method in the field of CBIR. © 2012 Elsevier B.V. All rights reserved.","Adaptive feature extraction; Feature selection; Gravitational search algorithm; Image retrieval; Parameterized wavelet","CBIR system; Color histogram; Content-Based Image Retrieval; Content-based image retrieval system; Experimental studies; Feature adaptation; Feature selection methods; Feature subset selection; Gravitational search algorithms; High level semantics; Hybrid approach; Indexing methods; Low level; Metaheuristic; Mother wavelets; Parameter setting; Parameterized; Parameterized wavelet; Quantization levels; Search technique; Semantic gap; Texture features; Visual feature; Artificial intelligence; Content based retrieval; Image retrieval; Learning algorithms; Semantics; Feature extraction",Article,Scopus,2-s2.0-84871925258
"Sugano Y., Matsushita Y., Sato Y.","Appearance-based gaze estimation using visual saliency",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",55,10.1109/TPAMI.2012.101,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871733927&doi=10.1109%2fTPAMI.2012.101&partnerID=40&md5=ea0b5dd9fa2d62e3c95d540b6ccf7240","We propose a gaze sensing method using visual saliency maps that does not need explicit personal calibration. Our goal is to create a gaze estimator using only the eye images captured from a person watching a video clip. Our method treats the saliency maps of the video frames as the probability distributions of the gaze points. We aggregate the saliency maps based on the similarity in eye images to efficiently identify the gaze points from the saliency maps. We establish a mapping between the eye images to the gaze points by using Gaussian process regression. In addition, we use a feedback loop from the gaze estimator to refine the gaze probability maps to improve the accuracy of the gaze estimation. The experimental results show that the proposed method works well with different people and video clips and achieves a 3.5-degree accuracy, which is sufficient for estimating a user's attention on a display. © 2012 IEEE.","face and gesture recognition; Gaze estimation; visual attention","Appearance based; Eye images; Face and gesture recognition; Feed-back loop; Gaussian process regression; Gaze estimation; Gaze point; Probability maps; Saliency map; Video clips; Video frame; Visual Attention; Visual saliency; Gesture recognition; Probability distributions; Video cameras; Visualization; Estimation; algorithm; article; artificial intelligence; attention; automated pattern recognition; biological model; biomimetics; computer assisted diagnosis; computer simulation; eye fixation; eye movement; human; methodology; nonlinear system; pattern recognition; physiology; reproducibility; sensitivity and specificity; Algorithms; Artificial Intelligence; Attention; Biomimetics; Computer Simulation; Eye Movements; Fixation, Ocular; Humans; Image Interpretation, Computer-Assisted; Models, Biological; Nonlinear Dynamics; Pattern Recognition, Automated; Pattern Recognition, Visual; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84871733927
"Ujjin S., Bentley P.J.","Particle swarm optimization recommender system",2013,"2003 IEEE Swarm Intelligence Symposium, SIS 2003 - Proceedings",55,10.1109/SIS.2003.1202257,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942123264&doi=10.1109%2fSIS.2003.1202257&partnerID=40&md5=be1f1e76542e56880d6da08bc0dccc4a","Recommender systems are new types of Internet-based software tools, designed to help users find their way through today's complex on-line shops and entertainment Web sites. This paper describes a new recommender system, which employs a particle swarm optimization (PSO) algorithm to learn personal preferences of users and provide tailored suggestions. Experiments are carried out to observe the performance of the system and results are compared to those obtained from the genetic algorithm (GA) recommender system and a standard, non-adaptive system based on the Pearson algorithm. © 2003 IEEE.","Collaboration; Computer science; Educational institutions; Filtering; Internet; Motion pictures; Particle swarm optimization; Recommender systems; Software design; Software tools","Algorithms; Artificial intelligence; Computer aided software engineering; Computer science; Computer software; Educational motion pictures; Filtration; Genetic algorithms; Internet; Motion pictures; Recommender systems; Social networking (online); Software design; A-particles; Collaboration; Educational institutions; Internet based; Non-adaptive system; Particle swarm optimization (PSO)",Conference Paper,Scopus,2-s2.0-84942123264
"Nahar J., Imam T., Tickle K.S., Chen Y.-P.P.","Computational intelligence for heart disease diagnosis: A medical knowledge driven approach",2013,"Expert Systems with Applications",55,10.1016/j.eswa.2012.07.032,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866062110&doi=10.1016%2fj.eswa.2012.07.032&partnerID=40&md5=f0bfb7fe9d2cbedb35baf9eea2e26b19","This paper investigates a number of computational intelligence techniques in the detection of heart disease. Particularly, comparison of six well known classifiers for the well used Cleveland data is performed. Further, this paper highlights the potential of an expert judgment based (i.e., medical knowledge driven) feature selection process (termed as MFS), and compare against the generally employed computational intelligence based feature selection mechanism. Also, this article recognizes that the publicly available Cleveland data becomes imbalanced when considering binary classification. Performance of classifiers, and also the potential of MFS are investigated considering this imbalanced data issue. The experimental results demonstrate that the use of MFS noticeably improved the performance, especially in terms of accuracy, for most of the classifiers considered and for majority of the datasets (generated by converting the Cleveland dataset for binary classification). MFS combined with the computerized feature selection process (CFS) has also been investigated and showed encouraging results particularly for NaiveBayes, IBK and SMO. In summary, the medical knowledge based feature selection method has shown promise for use in heart disease diagnostics. © 2012 Elsevier Ltd. All rights reserved.","Classification; Cleveland data; Computational intelligence; Feature selection; Heart disease","Binary classification; Cleveland; Computational intelligence techniques; Data sets; Expert judgment; Feature selection methods; Heart disease; Heart disease diagnosis; Imbalanced data; Medical knowledge; Performance of classifier; Selection mechanism; Artificial intelligence; Cardiology; Classification (of information); Diseases; Feature extraction; Knowledge based systems; Diagnosis",Article,Scopus,2-s2.0-84866062110
"Brewka G., Ellmauthaler S., Strass H., Wallner J.P., Woltran S.","Abstract dialectical frameworks revisited",2013,"IJCAI International Joint Conference on Artificial Intelligence",54,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062178&partnerID=40&md5=1c89ac1e23ae38640a70100f1f7cd8cb","We present various new concepts and results related to abstract dialectical frameworks (ADFs), a powerful generalization of Dung's argumentation frameworks (AFs). In particular, we show how the existing definitions of stable and preferred semantics which are restricted to the subcase of so-called bipolar ADFs can be improved and generalized to arbitrary frameworks. Furthermore, we introduce preference handling methods for ADFs, allowing for both reasoning with and about preferences. Finally, we present an implementation based on an encoding in answer set programming.",,"Answer set programming; Argumentation frameworks; Preferred semantics; Logic programming; Semantics; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84896062178
"Yang B., Lei Y., Liu D., Liu J.","Social collaborative filtering by trust",2013,"IJCAI International Joint Conference on Artificial Intelligence",54,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896064130&partnerID=40&md5=312ca227eecbbaf1e90e9f872a848ca6","To accurately and actively provide users with their potentially interested information or services is the main task of a recommender system. Collaborative filtering is one of the most widely adopted recommender algorithms, whereas it is suffering the issues of data sparsity and cold start that will severely degrade quality of recommendations. To address such issues, this article proposes a novel method, trying to improve the performance of collaborative filtering recommendation by means of elaborately integrating twofold sparse information, the conventional rating data given by users and the social trust network among the same users. It is a model-based method adopting matrix factorization technique to map users into low-dimensional latent feature spaces in terms of their trust relationship, aiming to reflect users' reciprocal influence on their own opinions more reasonably. The validations against a real-world dataset show that the proposed method performs much better than state-of-the-art recommendation algorithms for social collaborative filtering by trust.",,"Collaborative filtering recommendations; Matrix factorizations; Model-based method; Quality of recommendations; Recommendation algorithms; Recommender algorithms; Trust networks; Trust relationship; Algorithms; Artificial intelligence; Collaborative filtering",Conference Paper,Scopus,2-s2.0-84896064130
"Cheatham M., Hitzler P.","String similarity metrics for ontology alignment",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",54,10.1007/978-3-642-41338-4_19,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891926899&doi=10.1007%2f978-3-642-41338-4_19&partnerID=40&md5=078f617f327dc9f469baa2014378e1c4","Ontology alignment is an important part of enabling the semantic web to reach its full potential. The vast majority of ontology alignment systems use one or more string similarity metrics, but often the choice of which metrics to use is not given much attention. In this work we evaluate a wide range of such metrics, along with string pre-processing strategies such as removing stop words and considering synonyms, on different types of ontologies. We also present a set of guidelines on when to use which metric. We furthermore show that if optimal string similarity metrics are chosen, those alone can produce alignments that are competitive with the state of the art in ontology alignment systems. Finally, we examine the improvements possible to an existing ontology alignment system using an automated string metric selection strategy based upon the characteristics of the ontologies to be aligned. © 2013 Springer-Verlag.",,"Metric selections; Not given; Ontology alignment; Pre-processing; State of the art; Stop word; String similarity; Computer science; Computers; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84891926899
"Shum H.P.H., Ho E.S.L., Jiang Y., Takagi S.","Real-time posture reconstruction for Microsoft Kinect",2013,"IEEE Transactions on Cybernetics",54,10.1109/TCYB.2013.2275945,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890363599&doi=10.1109%2fTCYB.2013.2275945&partnerID=40&md5=ab1104f545fc50d38210a7b4b41329cf","The recent advancement of motion recognition using Microsoft Kinect stimulates many new ideas in motion capture and virtual reality applications. Utilizing a pattern recognition algorithm, Kinect can determine the positions of different body parts from the user. However, due to the use of a single-depth camera, recognition accuracy drops significantly when the parts are occluded. This hugely limits the usability of applications that involve interaction with external objects, such as sport training or exercising systems. The problem becomes more critical when Kinect incorrectly perceives body parts. This is because applications have limited information about the recognition correctness, and using those parts to synthesize body postures would result in serious visual artifacts. In this paper, we propose a new method to reconstruct valid movement from incomplete and noisy postures captured by Kinect. We first design a set of measurements that objectively evaluates the degree of reliability on each tracked body part. By incorporating the reliability estimation into a motion database query during run time, we obtain a set of similar postures that are kinematically valid. These postures are used to construct a latent space, which is known as the natural posture space in our system, with local principle component analysis. We finally apply frame-based optimization in the space to synthesize a new posture that closely resembles the true user posture while satisfying kinematic constraints. Experimental results show that our method can significantly improve the quality of the recognized posture under severely occluded environments, such as a person exercising with a basketball or moving in a small room. © 2013 IEEE.","Human-computer interaction; Kinect; Local principal component analysis; Posture reconstruction","Degree of reliability; Kinect; Kinematic constraints; Local principal component analysis; Pattern recognition algorithms; Principle component analysis; Recognition accuracy; Reliability estimation; Human computer interaction; Pattern recognition; SportS; Virtual reality; Principal component analysis; actimetry; algorithm; artificial intelligence; automated pattern recognition; body posture; computer; computer simulation; computer system; devices; human; image enhancement; physiology; procedures; recreation; three dimensional imaging; transducer; whole body imaging; article; automated pattern recognition; body posture; equipment; methodology; physiology; three dimensional imaging; whole body imaging; Actigraphy; Algorithms; Artificial Intelligence; Computer Peripherals; Computer Simulation; Computer Systems; Humans; Image Enhancement; Imaging, Three-Dimensional; Pattern Recognition, Automated; Posture; Transducers; Video Games; Whole Body Imaging; Actigraphy; Algorithms; Artificial Intelligence; Computer Peripherals; Computer Simulation; Computer Systems; Humans; Image Enhancement; Imaging, Three-Dimensional; Pattern Recognition, Automated; Posture; Transducers; Video Games; Whole Body Imaging",Article,Scopus,2-s2.0-84890363599
"Balla-Arabé S., Gao X., Wang B.","A fast and robust level set method for image segmentation using fuzzy clustering and lattice boltzmann method",2013,"IEEE Transactions on Cybernetics",54,10.1109/TSMCB.2012.2218233,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890441316&doi=10.1109%2fTSMCB.2012.2218233&partnerID=40&md5=6ff88da4a68a6fc70f248f406a0b6077","In the last decades, due to the development of the parallel programming, the lattice Boltzmann method (LBM) has attracted much attention as a fast alternative approach for solving partial differential equations. In this paper, we first designed an energy functional based on the fuzzy c-means objective function which incorporates the bias field that accounts for the intensity inhomogeneity of the real-world image. Using the gradient descent method, we obtained the corresponding level set equation from which we deduce a fuzzy external force for the LBM solver based on the model by Zhao. The method is fast, robust against noise, independent to the position of the initial contour, effective in the presence of intensity inhomogeneity, highly parallelizable and can detect objects with or without edges. Experiments on medical and real-world images demonstrate the performance of the proposed method in terms of speed and efficiency. © 2012 IEEE.","Fuzzy c-means (FCM); Image segmentation; Intensity inhomogeneity; Lattice boltzmann method (LBM); Level set equation (LSE); Partial differential equation (PDE)","Fuzzy C-means; Intensity inhomogeneity; Lattice boltzmann methods (LBM); Level-set equation; Partial differential equations (PDE); Fuzzy systems; Image segmentation; Medical imaging; Numerical methods; Parallel programming; Partial differential equations; Computational fluid dynamics; algorithm; artificial intelligence; automated pattern recognition; cluster analysis; computer assisted diagnosis; fuzzy logic; image enhancement; image subtraction; procedures; article; automated pattern recognition; computer assisted diagnosis; image enhancement; methodology; Algorithms; Artificial Intelligence; Cluster Analysis; Fuzzy Logic; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Subtraction Technique; Algorithms; Artificial Intelligence; Cluster Analysis; Fuzzy Logic; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Subtraction Technique",Article,Scopus,2-s2.0-84890441316
"Faria P., Soares J., Vale Z., Morais H., Sousa T.","Modified particle swarm optimization applied to integrated demand response and DG resources scheduling",2013,"IEEE Transactions on Smart Grid",54,10.1109/TSG.2012.2235866,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875056328&doi=10.1109%2fTSG.2012.2235866&partnerID=40&md5=1f9dec295b46799463158cba95b2a44e","The elastic behavior of the demand consumption jointly used with other available resources such as distributed generation (DG) can play a crucial role for the success of smart grids. The intensive use of Distributed Energy Resources (DER) and the technical and contractual constraints result in large-scale non linear optimization problems that require computational intelligence methods to be solved. © 2013 IEEE.","Demand response; energy resource management; particle swarm optimization; virtual power player","Computational intelligence methods; Demand response; Distributed energy resource; Energy resource managements; Modified particle swarm optimization; Non-linear optimization problems; Resources scheduling; Virtual power players; Artificial intelligence; Distributed power generation; Energy resources; Particle swarm optimization (PSO)",Article,Scopus,2-s2.0-84875056328
"Zhu X., Milanfar P.","Removing atmospheric turbulence via space-invariant deconvolution",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",54,10.1109/TPAMI.2012.82,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870195522&doi=10.1109%2fTPAMI.2012.82&partnerID=40&md5=d430958dfdf69224f4f28c9b19f7df4e","To correct geometric distortion and reduce space and time-varying blur, a new approach is proposed in this paper capable of restoring a single high-quality image from a given image sequence distorted by atmospheric turbulence. This approach reduces the space and time-varying deblurring problem to a shift invariant one. It first registers each frame to suppress geometric deformation through B-spline-based nonrigid registration. Next, a temporal regression process is carried out to produce an image from the registered frames, which can be viewed as being convolved with a space invariant near-diffraction-limited blur. Finally, a blind deconvolution algorithm is implemented to deblur the fused image, generating a final output. Experiments using real data illustrate that this approach can effectively alleviate blur and distortions, recover details of the scene, and significantly improve visual quality. © 1979-2012 IEEE.","atmospheric turbulence; Image restoration; nonrigid image registration; point spread function; sharpness metric","Blind deconvolution algorithms; Deblurring problems; Fused images; Geometric deformations; Geometric distortion; High quality images; Image sequence; Nonrigid image registration; Nonrigid registration; sharpness metric; Shift invariant; Time varying; Visual qualities; Blind source separation; Image reconstruction; Optical transfer function; Atmospheric turbulence; algorithm; article; artifact; artificial intelligence; atmosphere; computer assisted diagnosis; image enhancement; methodology; Algorithms; Artifacts; Artificial Intelligence; Atmosphere; Image Enhancement; Image Interpretation, Computer-Assisted",Article,Scopus,2-s2.0-84870195522
"Sanz B., Santos I., Laorden C., Ugarte-Pedrero X., Bringas P.G., Álvarez G.","PUMA: Permission usage to detect malware in android",2013,"Advances in Intelligent Systems and Computing",54,10.1007/978-3-642-33018-6_30,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868087373&doi=10.1007%2f978-3-642-33018-6_30&partnerID=40&md5=4a33bdc365bf728b407790961f77a26d","The presence of mobile devices has increased in our lives offering almost the same functionality as a personal computer. Android devices have appeared lately and, since then, the number of applications available for this operating system has increased exponentially. Google already has its Android Market where applications are offered and, as happens with every popular media, is prone to misuse. In fact, malware writers insert malicious applications into this market, but also among other alternative markets. Therefore, in this paper, we present PUMA, a new method for detecting malicious Android applications through machine-learning techniques by analysing the extracted permissions from the application itself. © 2013 Springer-Verlag Berlin Heidelberg.","Android; machine learning; malware detection; mobile malware","Artificial intelligence; Commerce; Computer crime; Learning systems; Mobile devices; Personal computers; Soft computing; Android; Machine learning techniques; Malware detection; Malwares; Mobile malware; Robots",Conference Paper,Scopus,2-s2.0-84868087373
"Cai Y., Wang J.","Differential evolution with neighborhood and direction information for numerical optimization",2013,"IEEE Transactions on Cybernetics",53,10.1109/TCYB.2013.2245501,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890074816&doi=10.1109%2fTCYB.2013.2245501&partnerID=40&md5=e077ec1acefb0a2b0d8f1e930783a0e2","Differential evolution (DE) is a simple and powerful population-based evolutionary algorithm, successfully used in various scientific and engineering fields. Although DE has been studied by many researchers, the neighborhood and direction information is not fully and simultaneously exploited in the designing of DE. In order to alleviate this drawback and enhance the performance of DE, we first introduce two novel operators, namely, the neighbor guided selection scheme for parents involved in mutation and the direction induced mutation strategy, to fully exploit the neighborhood and direction information of the population, respectively. By synergizing these two operators, a simple and effective DE framework, which is referred to as the neighborhood and direction information based DE (NDi-DE), is then proposed for enhancing the performance of DE. This way, NDi-DE not only utilizes the information of neighboring individuals to exploit the regions of minima and accelerate convergence but also incorporates the direction information to prevent an individual from entering an undesired region and move to a promising area. Consequently, a good balance between exploration and exploitation can be achieved. In order to test the effectiveness of NDi-DE, the proposed framework is applied to the original DE algorithms, as well as several state-of-the-art DE variants. Experimental results show that NDi-DE is an effective framework to enhance the performance of most of the DE algorithms studied. © 2013 IEEE.","Differential evolution (DE); Direction information; Exploitation; Exploration; Neighborhood information","Differential Evolution; Direction information; Engineering fields; Exploitation; Exploration and exploitation; Induced mutations; Neighborhood information; Numerical optimizations; Natural resources exploration; Evolutionary algorithms; algorithm; article; artificial intelligence; automated pattern recognition; computer simulation; decision support system; evolution; methodology; theoretical model; Algorithms; Artificial Intelligence; Biological Evolution; Computer Simulation; Decision Support Techniques; Models, Theoretical; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84890074816
"Karamshuk D., Noulas A., Scellato S., Nicosia V., Mascolo C.","Geo-spotting: Mining online location-based services for optimal retail store placement",2013,"Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",53,10.1145/2487575.2487616,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984991118&doi=10.1145%2f2487575.2487616&partnerID=40&md5=895f5b40ee69f48ef0b36d1b9eb05314","The problem of identifying the optimal location for a new retail store has been the focus of past research, especially in the field of land economy, due to its importance in the success of a business. Traditional approaches to the problem have factored in demographics, revenue and aggregated human flow statistics from nearby or remote areas. However, the acquisition of relevant data is usually expensive. With the growth of location-based social networks, fine grained data describing user mobility and popularity of places has recently become attainable. In this paper we study the predictive power of various machine learning features on the popularity of retail stores in the city through the use of a dataset collected from Foursquare in New York. The features we mine are based on two general signals: geographic, where features are formulated according to the types and density of nearby places, and user mobility, which includes transitions between venues or the incoming flow of mobile users from distant areas. Our evaluation suggests that the best performing features are common across the three different commercial chains considered in the analysis, although variations may exist too, as explained by heterogeneities in the way retail facilities attract users. We also show that performance improves significantly when combining multiple features in supervised learning algorithms, suggesting that the retail success of a business may depend on multiple factors. Copyright © 2013 ACM.","Location-based services; Machine learning; Optimal retail location","Artificial intelligence; Costs; Data mining; Economics; Education; Learning algorithms; Learning systems; Location; Retail stores; Sales; Location-based social networks; Multiple factors; Multiple features; Online locations; Optimal locations; Predictive power; Retail locations; Traditional approaches; Location based services",Conference Paper,Scopus,2-s2.0-84984991118
"Martinez H.P., Bengio Y., Yannakakis G.","Learning deep physiological models of affect",2013,"IEEE Computational Intelligence Magazine",53,10.1109/MCI.2013.2247823,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876251010&doi=10.1109%2fMCI.2013.2247823&partnerID=40&md5=9cb44aec5abdf4e703fffe6f6cfa001c","More than 15 years after the early studies in Affective Computing (AC), [1] the problem of detecting and modeling emotions in the context of human-computer interaction (HCI) remains complex and largely unexplored. The detection and modeling of emotion is, primarily, the study and use of artificial intelligence (AI) techniques for the construction of computational models of emotion. The key challenges one faces when attempting to model emotion [2] are inherent in the vague definitions and fuzzy boundaries of emotion, and in the modeling methodology followed. In this context, open research questions are still present in all key components of the modeling process. These include, first, the appropriateness of the modeling tool employed to map emotional manifestations and responses to annotated affective states; second, the processing of signals that express these manifestations (i.e., model input); and third, the way affective annotation (i.e., model output) is handled. This paper touches upon all three key components of an affective model (i.e., input, model, output) and introduces the use of deep learning (DL) [3], [4], [5] methodologies for affective modeling from multiple physiological signals. © 2005-2012 IEEE.",,"Affective annotations; Affective Computing; Affective modeling; Computational models of emotions; Human computer interaction (HCI); Modeling methodology; Physiological signals; Research questions; Artificial intelligence; Human computer interaction; Physiology; Signal detection; Physiological models",Article,Scopus,2-s2.0-84876251010
"Otero F.E.B., Freitas A.A., Johnson C.G.","A new sequential covering strategy for inducing classification rules with ant colony algorithms",2013,"IEEE Transactions on Evolutionary Computation",53,10.1109/TEVC.2012.2185846,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873291602&doi=10.1109%2fTEVC.2012.2185846&partnerID=40&md5=a0062892b25a5b810861bfac4e8566b3","Ant colony optimization (ACO) algorithms have been successfully applied to discover a list of classification rules. In general, these algorithms follow a sequential covering strategy, where a single rule is discovered at each iteration of the algorithm in order to build a list of rules. The sequential covering strategy has the drawback of not coping with the problem of rule interaction, i.e., the outcome of a rule affects the rules that can be discovered subsequently since the search space is modified due to the removal of examples covered by previous rules. This paper proposes a new sequential covering strategy for ACO classification algorithms to mitigate the problem of rule interaction, where the order of the rules is implicitly encoded as pheromone values and the search is guided by the quality of a candidate list of rules. Our experiments using 18 publicly available data sets show that the predictive accuracy obtained by a new ACO classification algorithm implementing the proposed sequential covering strategy is statistically significantly higher than the predictive accuracy of state-of-the-art rule induction classification algorithms. © 1997-2012 IEEE.","Ant colony optimization; classification; data mining; rule induction; sequential covering","Ant colony algorithms; Ant Colony Optimization (ACO); Ant Colony Optimization algorithms; Candidate list; Classification algorithm; Classification rules; Data sets; Predictive accuracy; Rule induction; Search spaces; sequential covering; Single-rule; Artificial intelligence; Classification (of information); Data mining; Iterative methods; Algorithms",Article,Scopus,2-s2.0-84873291602
"Ganesan T., Elamvazuthi I., Ku Shaari K.Z., Vasant P.","Swarm intelligence and gravitational search algorithm for multi-objective optimization of synthesis gas production",2013,"Applied Energy",53,10.1016/j.apenergy.2012.09.059,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871717797&doi=10.1016%2fj.apenergy.2012.09.059&partnerID=40&md5=737e3f094bb35fa3fa3d41dd5bf13712","In the chemical industry, the production of methanol, ammonia, hydrogen and higher hydrocarbons require synthesis gas (or syn gas). The main three syn gas production methods are carbon dioxide reforming (CRM), steam reforming (SRM) and partial-oxidation of methane (POM). In this work, multi-objective (MO) optimization of the combined CRM and POM was carried out. The empirical model and the MO problem formulation for this combined process were obtained from previous works. The central objectives considered in this problem are methane conversion, carbon monoxide selectivity and the hydrogen to carbon monoxide ratio. The MO nature of the problem was tackled using the Normal Boundary Intersection (NBI) method. Two techniques (Gravitational Search Algorithm (GSA) and Particle Swarm Optimization (PSO)) were then applied in conjunction with the NBI method. The performance of the two algorithms and the quality of the solutions were gauged by using two performance metrics. Comparative studies and results analysis were then carried out on the optimization results. © 2012 Elsevier Ltd.","Gravitational Search Algorithm (GSA); Multi-objective (MO); Normal Boundary Intersection (NBI); Particle Swarm Optimization (PSO); Performance metrics; Synthesis gas","Artificial intelligence; Carbon dioxide; Carbon monoxide; Chemical industry; Learning algorithms; Methane; Methanol; Particle swarm optimization (PSO); Steam reforming; Synthesis gas; Synthesis gas manufacture; Carbon dioxide reforming; Carbon monoxide selectivity; Comparative studies; Empirical model; Gravitational search algorithms; Higher hydrocarbons; Methane conversions; Multi objective; Multi objective optimizations (MOO); Normal boundary intersections; Performance metrics; Problem formulation; Swarm Intelligence; Syn gas; Synthesis gas production; Multiobjective optimization; algorithm; ammonia; carbon dioxide; carbon monoxide; chemical industry; empirical analysis; gas production; hydrogen; industrial technology; methane; methanol; multiobjective programming; optimization; oxidation; performance assessment",Article,Scopus,2-s2.0-84871717797
"Zhang Y., Wang S., Ji G., Dong Z.","An MR brain images classifier system via particle swarm optimization and kernel support vector machine",2013,"The Scientific World Journal",52,10.1155/2013/130134,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885597453&doi=10.1155%2f2013%2f130134&partnerID=40&md5=3a5a195dedaa050d8717c05cf0621dfb","Automated abnormal brain detection is extremely of importance for clinical diagnosis. Over last decades numerous methods had been presented. In this paper, we proposed a novel hybrid system to classify a given MR brain image as either normal or abnormal. The proposed method first employed digital wavelet transform to extract features then used principal component analysis (PCA) to reduce the feature space. Afterwards, we constructed a kernel support vector machine (KSVM) with RBF kernel, using particle swarm optimization (PSO) to optimize the parameters C and σ. Fivefold cross-validation was utilized to avoid overfitting. In the experimental procedure, we created a 90 images dataset brain downloaded from Harvard Medical School website. The abnormal brain MR images consist of the following diseases: glioma, metastatic adenocarcinoma, metastatic bronchogenic carcinoma, meningioma, sarcoma, Alzheimer, Huntington, motor neuron disease, cerebral calcinosis, Pick's disease, Alzheimer plus visual agnosia, multiple sclerosis, AIDS dementia, Lyme encephalopathy, herpes encephalitis, Creutzfeld-Jakob disease, and cerebral toxoplasmosis. The 5-folded cross-validation classification results showed that our method achieved 97.78% classification accuracy, higher than 86.22% by BP-NN and 91.33% by RBF-NN. For the parameter selection, we compared PSO with those of random selection method. The results showed that the PSO is more effective to build optimal KSVM. © 2013 Yudong Zhang et al.",,"accuracy; adenocarcinoma; agnosia; Alzheimer disease; article; brain calcification; Creutzfeldt Jakob disease; glioma; herpes simplex encephalitis; HIV associated dementia; human; Huntington chorea; kernel method; kernel support vector machine; lung carcinoma; Lyme disease; meningioma; motor neuron disease; multiple sclerosis; neuroimaging; nuclear magnetic resonance imaging; particle swarm optimization; Pick presenile dementia; principal component analysis; process optimization; sarcoma; support vector machine; toxoplasmosis; algorithm; artificial intelligence; brain; image processing; metabolism; pathology; standards; brain; image processing; metabolism; pathology; standard; Algorithms; Artificial Intelligence; Brain; Humans; Image Processing, Computer-Assisted; Principal Component Analysis; Support Vector Machines; Algorithms; Artificial Intelligence; Brain; Humans; Image Processing, Computer-Assisted; Principal Component Analysis; Support Vector Machines",Article,Scopus,2-s2.0-84885597453
"Wahab O.A., Otrok H., Mourad A.","VANET QoS-OLSR: QoS-based clustering protocol for Vehicular Ad hoc Networks",2013,"Computer Communications",52,10.1016/j.comcom.2013.07.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882450579&doi=10.1016%2fj.comcom.2013.07.003&partnerID=40&md5=7b8fb5bd89afe144d1c58ab1424027f6","In this paper, we address the problem of clustering in Vehicular Ad hoc Networks (VANETs) using Quality of Service Optimized Link State Routing (QoS-OLSR) protocol. Several clustering algorithms have been proposed for VANET and MANET. However, the mobility-based algorithms ignore the Quality of Service requirements that are important for VANET safety, emergency, and multimedia services while the QoS-based algorithms ignore the high speed mobility constraints since they are dedicated for Mobile Ad hoc Networks (MANETs). Our solution is a new QoS-based clustering algorithm that considers a tradeoff between QoS requirements and high speed mobility constraints. The goal is to form stable clusters and maintain the stability during communications and link failures while satisfying the Quality of Service requirements. This is achieved by: (1) considering the high mobility metrics while computing the QoS, (2) using Ant Colony Optimization for MPRs selection, and (3) using MPR recovery algorithm able to select alternatives and keep the network connected in case of link failures. Performance analysis and simulation results show that the proposed model can maintain the network stability, reduce the end-to-end delay, increase the packet delivery ratio, and reduce the communications overhead. © 2013 Elsevier B.V. All rights reserved.","Mobility; Quality of Service (QoS); Stability Ant Colony Optimization (ACO); Vehicular Ad hoc Network (VANET)","Ant Colony Optimization (ACO); Mobile adhoc network (MANETs); Mobility constraints; Optimized Link State Routing; Packet delivery ratio; Performance analysis and simulation; Service requirements; Vehicular Adhoc Networks (VANETs); Ad hoc networks; Ant colony optimization; Artificial intelligence; Carrier mobility; Clustering algorithms; Multimedia services; Vehicular ad hoc networks; Quality of service",Article,Scopus,2-s2.0-84882450579
"Ellmauthaler A., Pagliari C.L., Da Silva E.A.B.","Multiscale image fusion using the undecimated wavelet transform with spectral factorization and nonorthogonal filter banks",2013,"IEEE Transactions on Image Processing",52,10.1109/TIP.2012.2226045,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873309289&doi=10.1109%2fTIP.2012.2226045&partnerID=40&md5=4a3be5ae3365964579ea2d87dc9376ac","Multiscale transforms are among the most popular techniques in the field of pixel-level image fusion. However, the fusion performance of these methods often deteriorates for images derived from different sensor modalities. In this paper, we demonstrate that for such images, results can be improved using a novel undecimated wavelet transform (UWT)-based fusion scheme, which splits the image decomposition process into two successive filtering operations using spectral factorization of the analysis filters. The actual fusion takes place after convolution with the first filter pair. Its significantly smaller support size leads to the minimization of the unwanted spreading of coefficient values around overlapping image singularities. This usually complicates the feature selection process and may lead to the introduction of reconstruction errors in the fused image. Moreover, we will show that the nonsubsampled nature of the UWT allows the design of nonorthogonal filter banks, which are more robust to artifacts introduced during fusion, additionally improving the obtained results. The combination of these techniques leads to a fusion framework, which provides clear advantages over traditional multiscale fusion approaches, independent of the underlying fusion rule, and reduces unwanted side effects such as ringing artifacts in the fused reconstruction. © 1992-2012 IEEE.","Image fusion; nonorthogonal filter banks; spectral factorization; undecimated wavelet transform (UWT)","Coefficient values; Filter pairs; Filtering operations; Fused images; Fusion performance; Fusion rule; Image decomposition; Multiscale fusion; Multiscale image fusions; Multiscale transforms; Non-orthogonal; Overlapping images; Pixel-level image fusion; Reconstruction error; Ringing artifacts; Sensor modality; Side effect; Spectral factorizations; Undecimated wavelet transform; Factorization; Filter banks; Wavelet transforms; Image fusion; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; image enhancement; image subtraction; methodology; reproducibility; sensitivity and specificity; signal processing; wavelet analysis; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Signal Processing, Computer-Assisted; Subtraction Technique; Wavelet Analysis",Article,Scopus,2-s2.0-84873309289
"Yang X., Prasad L., Latecki L.J.","Affinity learning with diffusion on tensor product graph",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",52,10.1109/TPAMI.2012.60,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870183752&doi=10.1109%2fTPAMI.2012.60&partnerID=40&md5=7c9d94c6f7c684d761ea485b4ca99cdd","In many applications, we are given a finite set of data points sampled from a data manifold and represented as a graph with edge weights determined by pairwise similarities of the samples. Often the pairwise similarities (which are also called affinities) are unreliable due to noise or due to intrinsic difficulties in estimating similarity values of the samples. As observed in several recent approaches, more reliable similarities can be obtained if the original similarities are diffused in the context of other data points, where the context of each point is a set of points most similar to it. Compared to the existing methods, our approach differs in two main aspects. First, instead of diffusing the similarity information on the original graph, we propose to utilize the tensor product graph (TPG) obtained by the tensor product of the original graph with itself. Since TPG takes into account higher order information, it is not a surprise that we obtain more reliable similarities. However, it comes at the price of higher order computational complexity and storage requirement. The key contribution of the proposed approach is that the information propagation on TPG can be computed with the same computational complexity and the same amount of storage as the propagation on the original graph. We prove that a graph diffusion process on TPG is equivalent to a novel iterative algorithm on the original graph, which is guaranteed to converge. After its convergence we obtain new edge weights that can be interpreted as new, learned affinities. We stress that the affinities are learned in an unsupervised setting. We illustrate the benefits of the proposed approach for data manifolds composed of shapes, images, and image patches on two very different tasks of image retrieval and image segmentation. With learned affinities, we achieve the bull's eye retrieval score of 99.99 percent on the MPEG-7 shape dataset, which is much higher than the state-of-the-art algorithms. When the data points are image patches, the NCut with the learned affinities not only significantly outperforms the NCut with the original affinities, but it also outperforms state-of-the-art image segmentation methods. © 1979-2012 IEEE.","affinity learning; Diffusion process; image retrieval; image segmentation; tensor product graph","affinity learning; Data manifolds; Data points; Data sets; Diffusion process; Edge weights; Finite set; Image patches; Information propagation; Iterative algorithm; Segmentation methods; State-of-the-art algorithms; Storage requirements; Tensor products; Algorithms; Computational complexity; Diffusion; Image retrieval; Image segmentation; Motion Picture Experts Group standards; Digital storage; algorithm; article; artificial intelligence; automated pattern recognition; computer simulation; diffusion; information retrieval; methodology; theoretical model; Algorithms; Artificial Intelligence; Computer Simulation; Diffusion; Information Storage and Retrieval; Models, Theoretical; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84870183752
"Lughofer E.","On-line assurance of interpretability criteria in evolving fuzzy systems - Achievements, new concepts and open issues",2013,"Information Sciences",51,10.1016/j.ins.2013.07.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883213469&doi=10.1016%2fj.ins.2013.07.002&partnerID=40&md5=c9153232ec24156955b0659b99ef0c8b","In this position paper, we are discussing achievements and open issues in the interpretability of evolving fuzzy systems (EFS). In addition to pure on-line complexity reduction approaches, which can be an important direction for increasing the transparency of the evolved fuzzy systems, we examine the state-of-the-art and provide further investigations and concepts regarding the following interpretability aspects: distinguishability, simplicity, consistency, coverage and completeness, feature importance levels, rule importance levels and interpretation of consequents. These are well-known and widely accepted criteria for the interpretability of expert-based and standard data-driven fuzzy systems in batch mode. So far, most have been investigated only rudimentarily in the context of evolving fuzzy systems, trained incrementally from data streams: EFS have focussed mainly on precise modeling, aiming for models of high predictive quality. Only in a few cases, the integration of complexity reduction steps has been handled. This paper thus seeks to close this gap by pointing out new ways of making EFS more transparent and interpretable within the scope of the criteria mentioned above. The role of knowledge expansion, a peculiar concept in EFS, will be also addressed. One key requirement in our investigations is the availability of all concepts for on-line usage, which means they should be incremental or at least allow fast processing. © 2013 Elsevier Inc. All rights reserved.","Evolving fuzzy system Complexity reduction Interpretability criteria Knowledge expansion On-line assurance","Complexity reduction; Distinguishability; Evolving Fuzzy Systems; Interpretability criterion; Knowledge expansion; On-line assurances; On-line complexity; Precise modeling; Artificial intelligence; Software engineering; Fuzzy systems",Article,Scopus,2-s2.0-84883213469
"Holzinger A.","Human-Computer Interaction and Knowledge Discovery (HCI-KDD): What is the benefit of bringing those two fields to work together?",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",51,10.1007/978-3-642-40511-2_22,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885807915&doi=10.1007%2f978-3-642-40511-2_22&partnerID=40&md5=d4b7232a5e4bead3dfa039f8e59628e1","A major challenge in our networked world is the increasing amount of data, which require efficient and user-friendly solutions. A timely example is the biomedical domain: the trend towards personalized medicine has resulted in a sheer mass of the generated (-omics) data. In the life sciences domain, most data models are characterized by complexity, which makes manual analysis very time-consuming and frequently practically impossible. Computational methods may help; however, we must acknowledge that the problem-solving knowledge is located in the human mind and - not in machines. A strategic aim to find solutions for data intensive problems could lay in the combination of two areas, which bring ideal pre-conditions: Human-Computer Interaction (HCI) and Knowledge Discovery (KDD). HCI deals with questions of human perception, cognition, intelligence, decision-making and interactive techniques of visualization, so it centers mainly on supervised methods. KDD deals mainly with questions of machine intelligence and data mining, in particular with the development of scalable algorithms for finding previously unknown relationships in data, thus centers on automatic computational methods. A proverb attributed perhaps incorrectly to Albert Einstein illustrates this perfectly: ""Computers are incredibly fast, accurate, but stupid. Humans are incredibly slow, inaccurate, but brilliant. Together they may be powerful beyond imagination"". Consequently, a novel approach is to combine HCI & KDD in order to enhance human intelligence by computational intelligence. © 2013 IFIP International Federation for Information Processing.","E-Science; HCI-KDD; Human-Computer Interaction (HCI); Interdisciplinary; Intersection science; Knowledge Discovery in Data (KDD)","e-Science; HCI-KDD; Human computer interaction (HCI); Interdisciplinary; Knowledge discovery in data; Artificial intelligence; Computational methods; Information systems; Human computer interaction",Conference Paper,Scopus,2-s2.0-84885807915
"Côté M.-A., Girard G., Boré A., Garyfallidis E., Houde J.-C., Descoteaux M.","Tractometer: Towards validation of tractography pipelines",2013,"Medical Image Analysis",51,10.1016/j.media.2013.03.009,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879884085&doi=10.1016%2fj.media.2013.03.009&partnerID=40&md5=a089ccc89274762a43b9fd181aaee3d2","We have developed the Tractometer: an online evaluation and validation system for tractography processing pipelines. One can now evaluate the results of more than 57,000 fiber tracking outputs using different acquisition settings (b-value, averaging), different local estimation techniques (tensor, q-ball, spherical deconvolution) and different tracking parameters (masking, seeding, maximum curvature, step size). At this stage, the system is solely based on a revised FiberCup analysis, but we hope that the community will get involved and provide us with new phantoms, new algorithms, third party libraries and new geometrical metrics, to name a few. We believe that the new connectivity analysis and tractography characteristics proposed can highlight limits of the algorithms and contribute in solving open questions in fiber tracking: from raw data to connectivity analysis. Overall, we show that (i) averaging improves quality of tractography, (ii) sharp angular ODF profiles helps tractography, (iii) seeding and multi-seeding has a large impact on tractography outputs and must be used with care, and (iv) deterministic tractography produces less invalid tracts which leads to better connectivity results than probabilistic tractography. © 2013 Elsevier B.V.","Connectivity analysis; Diffusion MRI; Tractography; Validation","Connectivity analysis; Diffusion mris; Local estimation; On-line evaluation; Probabilistic tractography; Spherical deconvolution; Tractography; Validation; Algorithms; Magnetic resonance imaging; Pipelines; algorithm; analysis; analytical equipment; analytical parameters; article; connectivity analysis; diffusion weighted imaging; fiber; geometry; human; phantom; priority journal; probability; quantitative analysis; tractography; tractometer; a-ODF; ABC; Analytical Orientation Distribution Function (ODF); Average Bundle Coverage; CC; Cerebral Spinal Fluid; Cg; Cingulum; Connectivity analysis; Constant Solid Angle ODF; Corpus Callosum; Corticospinal Tract; csa-ODF; CSF; CST; diffusion Magnetic Resonance Imaging; Diffusion MRI; Diffusion Tensor Imaging; Diffusion Weighted Imaging; dMRI; DTI; DWI; FA; Fiber ODF; fODF; Fractional Anisotropy; GM; Grey Matter; HARDI; High Angular Resolution Diffusion Imaging; IB; IC; Invalid Bundles; Invalid Connections; maximal SH order 6; NC; No Connections; ODF; Orientation Distribution Function; r6; Region Of Interest; rk; ROI; Runge-Kutta; SD-r6; SLF; Spherical Deconvolution of maximal SH order 6 (r6); Superior Longitudinal Fasciculus; TEND; Tensor Advection; Tractography; Valid Bundles; Valid Connections; Validation; VB; VC; White Matter; WM; Algorithms; Artificial Intelligence; Brain; Diffusion Tensor Imaging; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Internet; Nerve Fibers, Myelinated; Online Systems; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Software",Article,Scopus,2-s2.0-84879884085
"Chenouard N., Bloch I., Olivo-Marin J.-C.","Multiple hypothesis tracking for cluttered biological image sequences",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",51,10.1109/TPAMI.2013.97,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884580799&doi=10.1109%2fTPAMI.2013.97&partnerID=40&md5=b52feedbb24c766a6fad3f3c39b9dad8","In this paper, we present a method for simultaneously tracking thousands of targets in biological image sequences, which is of major importance in modern biology. The complexity and inherent randomness of the problem lead us to propose a unified probabilistic framework for tracking biological particles in microscope images. The framework includes realistic models of particle motion and existence and of fluorescence image features. For the track extraction process per se, the very cluttered conditions motivate the adoption of a multiframe approach that enforces tracking decision robustness to poor imaging conditions and to random target movements. We tackle the large-scale nature of the problem by adapting the multiple hypothesis tracking algorithm to the proposed framework, resulting in a method with a favorable tradeoff between the model complexity and the computational cost of the tracking procedure. When compared to the state-of-the-art tracking techniques for bioimaging, the proposed algorithm is shown to be the only method providing high-quality results despite the critically poor imaging conditions and the dense target presence. We thus demonstrate the benefits of advanced Bayesian tracking techniques for the accurate computational modeling of dynamical biological processes, which is promising for further developments in this domain. © 1979-2012 IEEE.","biological imaging; cluttered images; multiple hypothesis tracking; Particle tracking; target perceivability","Biological imaging; Cluttered images; Multiple hypothesis tracking; Particle tracking; Target perceivability; Algorithms; Communication channels (information theory); Computer vision; Target tracking; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; computer simulation; image enhancement; image subtraction; methodology; statistical analysis; statistical model; automated pattern recognition; computer assisted diagnosis; image enhancement; procedures; Algorithms; Artificial Intelligence; Computer Simulation; Data Interpretation, Statistical; Image Enhancement; Image Interpretation, Computer-Assisted; Models, Statistical; Pattern Recognition, Automated; Subtraction Technique; Algorithms; Artificial Intelligence; Computer Simulation; Data Interpretation, Statistical; Image Enhancement; Image Interpretation, Computer-Assisted; Models, Statistical; Pattern Recognition, Automated; Subtraction Technique",Article,Scopus,2-s2.0-84884580799
"Zhou J., Liu J., Narayan V.A., Ye J.","Modeling disease progression via multi-task learning",2013,"NeuroImage",51,10.1016/j.neuroimage.2013.03.073,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877334125&doi=10.1016%2fj.neuroimage.2013.03.073&partnerID=40&md5=e301b3efdb9acc874a436a909968b7bf","Alzheimer's disease (AD), the most common type of dementia, is a severe neurodegenerative disorder. Identifying biomarkers that can track the progress of the disease has recently received increasing attentions in AD research. An accurate prediction of disease progression would facilitate optimal decision-making for clinicians and patients. A definitive diagnosis of AD requires autopsy confirmation, thus many clinical/cognitive measures including Mini Mental State Examination (MMSE) and Alzheimer's Disease Assessment Scale cognitive subscale (ADAS-Cog) have been designed to evaluate the cognitive status of the patients and used as important criteria for clinical diagnosis of probable AD. In this paper, we consider the problem of predicting disease progression measured by the cognitive scores and selecting biomarkers predictive of the progression. Specifically, we formulate the prediction problem as a multi-task regression problem by considering the prediction at each time point as a task and propose two novel multi-task learning formulations. We have performed extensive experiments using data from the Alzheimer's Disease Neuroimaging Initiative (ADNI). Specifically, we use the baseline MRI features to predict MMSE/ADAS-Cog scores in the next 4. years. Results demonstrate the effectiveness of the proposed multi-task learning formulations for disease progression in comparison with single-task learning algorithms including ridge regression and Lasso. We also perform longitudinal stability selection to identify and analyze the temporal patterns of biomarkers in disease progression. We observe that cortical thickness average of left middle temporal, cortical thickness average of left and right Entorhinal, and white matter volume of left Hippocampus play significant roles in predicting ADAS-Cog at all time points. We also observe that several MRI biomarkers provide significant information for predicting MMSE scores for the first 2. years, however very few are shown to be significant in predicting MMSE score at later stages. The lack of predictable MRI biomarkers in later stages may contribute to the lower prediction performance of MMSE than that of ADAS-Cog in our study and other related studies. © 2013 Elsevier Inc.","ADAS-Cog; Alzheimer's disease; Disease progression; Fused Lasso; MMSE; Multi-task learning","algorithm; Alzheimer disease; Alzheimer's Disease Assessment Scale cognitive subscale; article; brain size; cognition; controlled study; cortical thickness (brain); disease course; disease marker; disease model; entorhinal cortex; hippocampus; intermethod comparison; longitudinal study; mental performance; mental task; mild cognitive impairment; Mini Mental State Examination; nuclear magnetic resonance imaging; prediction; priority journal; rating scale; scoring system; single task learning algorithm; temporal cortex; white matter; Aged; Algorithms; Alzheimer Disease; Artificial Intelligence; Disease Progression; Female; Humans; Magnetic Resonance Imaging; Male; Regression Analysis",Article,Scopus,2-s2.0-84877334125
"Yu K., Jia L., Chen Y., Xu W.","Deep learning: yesterday, today, and tomorrow",2013,"Jisuanji Yanjiu yu Fazhan/Computer Research and Development",51,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885069866&partnerID=40&md5=15d016492506e5595d19498b9c5564ec","Machine learning is an important area of artificial intelligence. Since 1980s, huge success has been achieved in terms of algorithms, theory, and applications. From 2006, a new machine learning paradigm, named deep learning, has been popular in the research community, and has become a huge wave of technology trend for big data and artificial intelligence. Deep learning simulates the hierarchical structure of human brain, processing data from lower level to higher level, and gradually composing more and more semantic concepts. In recent years, Google, Microsoft, IBM, and Baidu have invested a lot of resources into the R&D of deep learning, making significant progresses on speech recognition, image understanding, natural language processing, and online advertising. In terms of the contribution to real-world applications, deep learning is perhaps the most successful progress made by the machine learning community in the last 10 years. In this article, we will give a high-level overview about the past and current stage of deep learning, discuss the main challenges, and share our views on the future development of deep learning.","Deep learning; Image recognition; Machine learning; Natural language processing; Online advertising; Speech recognition","Deep learning; Hierarchical structures; Learning paradigms; Machine learning communities; NAtural language processing; Online advertising; Research communities; Technology trends; Artificial intelligence; Data handling; Image recognition; Learning systems; Marketing; Natural language processing systems; Semantics; Speech recognition; Learning algorithms",Article,Scopus,2-s2.0-84885069866
"Austin P.C., Tu J.V., Ho J.E., Levy D., Lee D.S.","Using methods from the data-mining and machine-learning literature for disease classification and prediction: A case study examining classification of heart failure subtypes",2013,"Journal of Clinical Epidemiology",51,10.1016/j.jclinepi.2012.11.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875243337&doi=10.1016%2fj.jclinepi.2012.11.008&partnerID=40&md5=c199e93a81cd18e2118195542e542436","Objective: Physicians classify patients into those with or without a specific disease. Furthermore, there is often interest in classifying patients according to disease etiology or subtype. Classification trees are frequently used to classify patients according to the presence or absence of a disease. However, classification trees can suffer from limited accuracy. In the data-mining and machine-learning literature, alternate classification schemes have been developed. These include bootstrap aggregation (bagging), boosting, random forests, and support vector machines. Study Design and Setting: We compared the performance of these classification methods with that of conventional classification trees to classify patients with heart failure (HF) according to the following subtypes: HF with preserved ejection fraction (HFPEF) and HF with reduced ejection fraction. We also compared the ability of these methods to predict the probability of the presence of HFPEF with that of conventional logistic regression. Results: We found that modern, flexible tree-based methods from the data-mining literature offer substantial improvement in prediction and classification of HF subtype compared with conventional classification and regression trees. However, conventional logistic regression had superior performance for predicting the probability of the presence of HFPEF compared with the methods proposed in the data-mining literature. Conclusion: The use of tree-based methods offers superior performance over conventional classification and regression trees for predicting and classifying HF subtypes in a population-based sample of patients from Ontario, Canada. However, these methods do not offer substantial improvements over logistic regression for predicting the presence of HFPEF. © 2013 Elsevier B.V. All rights reserved.","Bagging; Boosting; Classification; Classification trees; Heart failure; Prediction; Random forests; Regression methods; Regression trees; Support vector machines","aged; article; boosting; bootstrap aggregation; Canada; cardiac patient; case study; data mining; disease classification; heart ejection fraction; heart failure; human; information processing; logistic regression analysis; machine learning; major clinical study; male; medical literature; population research; priority journal; probability; prognosis; random forest; support vector machine; Artificial Intelligence; Data Mining; Heart Failure; Humans; Predictive Value of Tests; Regression Analysis",Article,Scopus,2-s2.0-84875243337
"Zhang W., Wang J., Wang J., Zhao Z., Tian M.","Short-term wind speed forecasting based on a hybrid model",2013,"Applied Soft Computing Journal",51,10.1016/j.asoc.2013.02.016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893650940&doi=10.1016%2fj.asoc.2013.02.016&partnerID=40&md5=2f2014f8c8f3e90a48e168af6cc3942a","Wind power is currently one of the types of renewable energy with a large generation capacity. However, operation of wind power generation is very challenging because of the intermittent and stochastic nature of the wind speed. Wind speed forecasting is a very important part of wind parks management and the integration of wind power into electricity grids. As an artificial intelligence algorithm, radial basis function neural network (RBFNN) has been successfully applied into solving forecasting problems. In this paper, a novel approach named WTT-SAM-RBFNN for short-term wind speed forecasting is proposed by applying wavelet transform technique (WTT) into hybrid model which hybrids the seasonal adjustment method (SAM) and the RBFNN. Real data sets of wind speed in Northwest China are used to evaluate the forecasting accuracy of the proposed approach. To avoid the randomness caused by the RBFNN model or the RBFNN part of the hybrid model, all simulations in this study are repeated 30 times to get the average. Numerical results show that the WTT-SAM-RBFNN outperforms the persistence method (PM), multilayer perceptron neural network (MLP), RBFNN, hybrid SAM and RBFNN (SAM-RBFNN), and hybrid WTT and RBFNN (WTT-RBFNN). It is concluded that the proposed approach is an effective way to improve the prediction accuracy. © 2013 Elsevier B.V. All rights reserved.","Forecasting; RBF neural networks; Seasonal adjustment; Wavelet transform; Wind speed","Electric power generation; Forecasting; Numerical methods; Radial basis function networks; Speed; Stochastic systems; Wavelet transforms; Wind effects; Wind power; Artificial intelligence algorithms; Multilayer perceptron neural networks; Radial basis function neural networks; RBF Neural Network; Seasonal adjustments; Short-term wind speed forecasting; Wind speed; Wind speed forecasting; Wind",Article,Scopus,2-s2.0-84893650940
"Schaul T.","A video game description language for model-based or interactive learning",2013,"IEEE Conference on Computatonal Intelligence and Games, CIG",50,10.1109/CIG.2013.6633610,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892419403&doi=10.1109%2fCIG.2013.6633610&partnerID=40&md5=f18fb5b1c4f481788bebc4d94f08898f","We propose a powerful new tool for conducting research on computational intelligence and games. 'PyVGDL' is a simple, high-level description language for 2D video games, and the accompanying software library permits parsing and instantly playing those games. The streamlined design of the language is based on defining locations and dynamics for simple building blocks, and the interaction effects when such objects collide, all of which are provided in a rich ontology. It can be used to quickly design games, without needing to deal with control structures, and the concise language is also accessible to generative approaches. We show how the dynamics of many classical games can be generated from a few lines of PyVGDL. The main objective of these generated games is to serve as diverse benchmark problems for learning and planning algorithms; so we provide a collection of interfaces for different types of learning agents, with visual or abstract observations, from a global or first-person viewpoint. To demonstrate the library's usefulness in a broad range of learning scenarios, we show how to learn competent behaviors when a model of the game dynamics is available or when it is not, when full state information is given to the agent or just subjective observations, when learning is interactive or in batch-mode, and for a number of different learning algorithms, including reinforcement learning and evolutionary search. © 2013 IEEE.",,"Bench-mark problems; Description languages; Evolutionary search; High level description; Interactive learning; Learning scenarios; Planning algorithms; Software libraries; Artificial intelligence; Dynamics; Evolutionary algorithms; High level languages; Intelligent agents; Interactive computer graphics; Reinforcement learning; Human computer interaction",Conference Paper,Scopus,2-s2.0-84892419403
"De Giacomo G., Vardi M.Y.","Linear temporal logic and Linear Dynamic Logic on finite traces",2013,"IJCAI International Joint Conference on Artificial Intelligence",50,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063056&partnerID=40&md5=bea8c3434d2961779923b1bbe97b40ad","In this paper we look into the assumption of interpreting LTL over finite traces. In particular we show that LTLf, i.e., LTL under this assumption, is less expressive than what might appear at first sight, and that at essentially no computational cost one can make a significant increase in expressiveness while maintaining the same intuitiveness of LTLf. Indeed, we propose a logic, LDLf for Linear Dynamic Logic over finite traces, which borrows the syntax from Propositional Dynamic Logic (PDL), but is interpreted over finite traces. Satisfiability, validity and logical implication (as well as model checking) for LDLf are PSPACE-complete as for LTLf (and LTL).",,"Computational costs; Finite traces; Linear dynamics; Linear temporal logic; Logical implications; Propositional dynamic logic; PSPACE-complete; Satisfiability; Model checking; Temporal logic; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84896063056
"Loy C.C., Liu C., Gong S.","Person re-identification by manifold ranking",2013,"2013 IEEE International Conference on Image Processing, ICIP 2013 - Proceedings",50,10.1109/ICIP.2013.6738736,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897786051&doi=10.1109%2fICIP.2013.6738736&partnerID=40&md5=12a288fde334e1fc165ed51e4ccc620e","Existing person re-identification methods conventionally rely on labelled pairwise data to learn a task-specific distance metric for ranking. The value of unlabelled gallery instances is generally overlooked. In this study, we show that it is possible to propagate the query information along the unlabelled data manifold in an unsupervised way to obtain robust ranking results. In addition, we demonstrate that the performance of existing supervised metric learning methods can be significantly boosted once integrated into the proposed manifold ranking-based framework. Extensive evaluation is conducted on three benchmark datasets. © 2013 IEEE.","distance metric learning; manifold; person re-identification; ranking; video surveillance","Distance Metric Learning; manifold; Person re identifications; ranking; Video surveillance; Artificial intelligence; Image processing; Security systems",Conference Paper,Scopus,2-s2.0-84897786051
"Carter H., Douville C., Stenson P.D., Cooper D.N., Karchin R.","Identifying Mendelian disease genes with the variant effect scoring tool.",2013,"BMC genomics",50,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874741731&partnerID=40&md5=0b3af6e4cd312beb5ccaf2c182ba2d79","Whole exome sequencing studies identify hundreds to thousands of rare protein coding variants of ambiguous significance for human health. Computational tools are needed to accelerate the identification of specific variants and genes that contribute to human disease. We have developed the Variant Effect Scoring Tool (VEST), a supervised machine learning-based classifier, to prioritize rare missense variants with likely involvement in human disease. The VEST classifier training set comprised ~ 45,000 disease mutations from the latest Human Gene Mutation Database release and another ~45,000 high frequency (allele frequency >1%) putatively neutral missense variants from the Exome Sequencing Project. VEST outperforms some of the most popular methods for prioritizing missense variants in carefully designed holdout benchmarking experiments (VEST ROC AUC = 0.91, PolyPhen2 ROC AUC = 0.86, SIFT4.0 ROC AUC = 0.84). VEST estimates variant score p-values against a null distribution of VEST scores for neutral variants not included in the VEST training set. These p-values can be aggregated at the gene level across multiple disease exomes to rank genes for probable disease involvement. We tested the ability of an aggregate VEST gene score to identify candidate Mendelian disease genes, based on whole-exome sequencing of a small number of disease cases. We used whole-exome data for two Mendelian disorders for which the causal gene is known. Considering only genes that contained variants in all cases, the VEST gene score ranked dihydroorotate dehydrogenase (DHODH) number 2 of 2253 genes in four cases of Miller syndrome, and myosin-3 (MYH3) number 2 of 2313 genes in three cases of Freeman Sheldon syndrome. Our results demonstrate the potential power gain of aggregating bioinformatics variant scores into gene-level scores and the general utility of bioinformatics in assisting the search for disease genes in large-scale exome sequencing studies. VEST is available as a stand-alone software package at http://wiki.chasmsoftware.org and is hosted by the CRAVAT web server at http://www.cravat.us.",,"algorithm; area under the curve; article; artificial intelligence; biology; comparative study; evaluation; exome; genetic database; genetic disorder; genetics; human; methodology; missense mutation; receiver operating characteristic; Algorithms; Area Under Curve; Artificial Intelligence; Computational Biology; Databases, Genetic; Exome; Genetic Diseases, Inborn; Humans; Mutation, Missense; ROC Curve",Article,Scopus,2-s2.0-84874741731
"Yager R.R., Abbasov A.M.","Pythagorean membership grades, complex numbers, and decision making",2013,"International Journal of Intelligent Systems",50,10.1002/int.21584,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875259751&doi=10.1002%2fint.21584&partnerID=40&md5=ac3ec65974f8abb5aee2aafb87d9e453","We describe the idea of Pythagorean membership grades and the related idea of Pythagorean fuzzy subsets. We focus on the negation and its relationship to the Pythagorean theorem. We look at the basic set operations for the case of Pythagorean fuzzy subsets. A relationship is shown between Pythagorean membership grades and complex numbers. We specifically show that Pythagorean membership grades are a subclass of complex numbers called Π-i numbers. We investigate operations that are closed under Π-i numbers. We consider the problem of multicriteria decision making with satisfactions expressed as Pythagorean membership grades, Π-i numbers. We look at the use of the geometric mean and ordered weighted geometric operator for aggregating criteria satisfaction. © 2013 Wiley Periodicals, Inc.",,"Complex number; Fuzzy subset; Geometric mean; Membership grade; Multi criteria decision making; Pythagorean theorem; Set operation; Artificial intelligence; Software engineering; Decision making",Article,Scopus,2-s2.0-84875259751
"Alippi C., Boracchi G., Roveri M.","Just-in-time classifiers for recurrent concepts",2013,"IEEE Transactions on Neural Networks and Learning Systems",50,10.1109/TNNLS.2013.2239309,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875892348&doi=10.1109%2fTNNLS.2013.2239309&partnerID=40&md5=f192933f23d3e00677f2589b52975acb","Just-in-time (JIT) classifiers operate in evolving environments by classifying instances and reacting to concept drift. In stationary conditions, a JIT classifier improves its accuracy over time by exploiting additional supervised information coming from the field. In nonstationary conditions, however, the classifier reacts as soon as concept drift is detected; the current classification setup is discarded and a suitable one activated to keep the accuracy high. We present a novel generation of JIT classifiers able to deal with recurrent concept drift by means of a practical formalization of the concept representation and the definition of a set of operators working on such representations. The concept-drift detection activity, which is crucial in promptly reacting to changes exactly when needed, is advanced by considering change-detection tests monitoring both inputs and classes distributions. © 2012 IEEE.","Adaptive classifiers; concept drift; just-in-time classifiers; recurrent concepts","Adaptive classifiers; Change detection; Concept drifts; Just in time; Non-stationary condition; Recurrent concepts; Stationary conditions; Artificial intelligence; Computer networks; Just in time production",Article,Scopus,2-s2.0-84875892348
"Li X., Dick A., Shen C., Van Den Hengel A., Wang H.","Incremental learning of 3D-DCT compact representations for robust visual tracking",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",50,10.1109/TPAMI.2012.166,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874520839&doi=10.1109%2fTPAMI.2012.166&partnerID=40&md5=85c560c8d9ab8b7d7dc5347def0e1670","Visual tracking usually requires an object appearance model that is robust to changing illumination, pose, and other factors encountered in video. Many recent trackers utilize appearance samples in previous frames to form the bases upon which the object appearance model is built. This approach has the following limitations: 1) The bases are data driven, so they can be easily corrupted, and 2) it is difficult to robustly update the bases in challenging situations. In this paper, we construct an appearance model using the 3D discrete cosine transform (3D-DCT). The 3D-DCT is based on a set of cosine basis functions which are determined by the dimensions of the 3D signal and thus independent of the input video data. In addition, the 3D-DCT can generate a compact energy spectrum whose high-frequency coefficients are sparse if the appearance samples are similar. By discarding these high-frequency coefficients, we simultaneously obtain a compact 3D-DCT-based object representation and a signal reconstruction-based similarity measure (reflecting the information loss from signal reconstruction). To efficiently update the object representation, we propose an incremental 3D-DCT algorithm which decomposes the 3D-DCT into successive operations of the 2D discrete cosine transform (2D-DCT) and 1D discrete cosine transform (1D-DCT) on the input video data. As a result, the incremental 3D-DCT algorithm only needs to compute the 2D-DCT for newly added frames as well as the 1D-DCT along the third dimension, which significantly reduces the computational complexity. Based on this incremental 3D-DCT algorithm, we design a discriminative criterion to evaluate the likelihood of a test sample belonging to the foreground object. We then embed the discriminative criterion into a particle filtering framework for object state inference over time. Experimental results demonstrate the effectiveness and robustness of the proposed tracker. © 1979-2012 IEEE.","appearance model; compact representation; discrete cosine transform (DCT); incremental learning; template matching; Visual tracking","Appearance models; Compact representation; Discrete Cosine Transform(DCT); Incremental learning; Visual Tracking; Discrete cosine transforms; Signal analysis; Signal reconstruction; Template matching; Tracking (position); Video recording; Three dimensional computer graphics; algorithm; article; artificial intelligence; face; histology; human; methodology; theoretical model; three dimensional imaging; videorecording; Algorithms; Artificial Intelligence; Face; Humans; Imaging, Three-Dimensional; Models, Theoretical; Video Recording",Article,Scopus,2-s2.0-84874520839
"Zhao W.-L., Ngo C.-W.","Flip-invariant SIFT for copy and object detection",2013,"IEEE Transactions on Image Processing",50,10.1109/TIP.2012.2226043,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873313507&doi=10.1109%2fTIP.2012.2226043&partnerID=40&md5=3ea76842e56b2e393fd4be5810054565","Scale-invariant feature transform (SIFT) feature has been widely accepted as an effective local keypoint descriptor for its invariance to rotation, scale, and lighting changes in images. However, it is also well known that SIFT, which is derived from directionally sensitive gradient fields, is not flip invariant. In real-world applications, flip or flip-like transformations are commonly observed in images due to artificial flipping, opposite capturing viewpoint, or symmetric patterns of objects. This paper proposes a new descriptor, named flip-invariant SIFT (or F-SIFT), that preserves the original properties of SIFT while being tolerant to flips. F-SIFT starts by estimating the dominant curl of a local patch and then geometrically normalizes the patch by flipping before the computation of SIFT. We demonstrate the power of F-SIFT on three tasks: large-scale video copy detection, object recognition, and detection. In copy detection, a framework, which smartly indices the flip properties of F-SIFT for rapid filtering and weak geometric checking, is proposed. F-SIFT not only significantly improves the detection accuracy of SIFT, but also leads to a more than 50% savings in computational cost. In object recognition, we demonstrate the superiority of F-SIFT in dealing with flip transformation by comparing it to seven other descriptors. In object detection, we further show the ability of F-SIFT in describing symmetric objects. Consistent improvement across different kinds of keypoint detectors is observed for F-SIFT over the original SIFT. © 1992-2012 IEEE.","Flip invariant scale-invariant feature transform (SIFT); geometric verification; object detection; video copy detection","Computational costs; Copy detection; Descriptors; Detection accuracy; Gradient fields; Object Detection; Real-world application; Scale invariant feature transforms; Symmetric patterns; Video copy detection; Detectors; Mathematical transformations; Object recognition; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; image enhancement; methodology; reproducibility; sensitivity and specificity; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84873313507
"Liu X.-J., Yi H., Ni Z.-H.","Application of ant colony optimization algorithm in process planning optimization",2013,"Journal of Intelligent Manufacturing",50,10.1007/s10845-010-0407-2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872370395&doi=10.1007%2fs10845-010-0407-2&partnerID=40&md5=07384912ec34b8ce902de5d5245c103a","One objective of process planning optimization is to cut down the total cost for machining process, and the ant colony optimization (ACO) algorithm is used for the optimization in this paper. Firstly, the process planning problem, considering the selection of machining resources, operations sequence optimization and the manufacturing constraints, is mapped to a weighted graph and is converted to a constraint-based traveling salesman problem. The operation sets for each manufacturing features are mapped to city groups, the costs for machining processes (including machine cost and tool cost) are converted to the weights of the cities; the costs for preparing processes (including machine changing, tool changing and set-up changing) are converted to the 'distance' between cities. Then, the mathematical model for process planning problem is constructed by considering the machining constraints and goal of optimization. The ACO algorithm has been employed to solve the proposed mathematical model. In order to ensure the feasibility of the process plans, the Constraint Matrix and State Matrix are used in this algorithm to show the state of the operations and the searching range of the candidate operations. Two prismatic parts are used to compare the ACO algorithm with tabu search, simulated annealing and genetic algorithm. The computing results show that the ACO algorithm performs well in process planning optimization than other three algorithms. © 2010 Springer Science+Business Media, LLC.","Ant colony optimization; Candidate operation (CO); Operation sequence; Process planning","ACO algorithms; Ant Colony Optimization (ACO); Ant Colony Optimization algorithms; Candidate operation (CO); City groups; Constraint-based; Machine costs; Machining constraints; Machining Process; Manufacturing constraint; Manufacturing features; Operation sequences; Preparing process; Prismatic parts; Process plan; Sequence optimization; State matrices; Weighted graph; Artificial intelligence; Costs; Machining; Machining centers; Manufacture; Mathematical models; Process planning; Simulated annealing; Tabu search; Traveling salesman problem; Algorithms",Article,Scopus,2-s2.0-84872370395
"Yang Y., Kamel M.","Clustering ensemble using swarm intelligence",2013,"2003 IEEE Swarm Intelligence Symposium, SIS 2003 - Proceedings",50,10.1109/SIS.2003.1202249,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942113441&doi=10.1109%2fSIS.2003.1202249&partnerID=40&md5=8aed2cd39f34dfb9b042f16a250204d5","This paper presents a clustering ensemble using three colonies of ants, each colony having different ant speed model: constant, random, and randomly decreasing. The algorithm is a two-phase process. Initially clusterings are visually formed on the plane by ants walking, picking up or dropping down projected data objects with different probability, and then a hypergraph model is used to combine clusterings. Results on synthetic and real data sets are given to show that the number of clusters can be adaptively determined and clustering ensembles can improve the clustering performance. © 2003 IEEE.","Ant colony optimization; Clustering algorithms; Computer architecture; Design engineering; Image processing; Legged locomotion; Machine intelligence; Particle swarm optimization; Systems engineering and theory; Unsupervised learning","Ant colony optimization; Artificial intelligence; Computer architecture; Data visualization; Image processing; Particle swarm optimization (PSO); Unsupervised learning; Clustering Ensemble; Design Engineering; Legged locomotion; Machine intelligence; Number of clusters; Swarm Intelligence; Synthetic and real data; Systems engineering and theories; Clustering algorithms",Conference Paper,Scopus,2-s2.0-84942113441
"Valenza G., Gentili C., Lanatà A., Scilingo E.P.","Mood recognition in bipolar patients through the PSYCHE platform: Preliminary evaluations and perspectives",2013,"Artificial Intelligence in Medicine",50,10.1016/j.artmed.2012.12.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875269658&doi=10.1016%2fj.artmed.2012.12.001&partnerID=40&md5=559abf5b44f18bb91a2784eb9c920ebb","Background: Bipolar disorders are characterized by a series of both depressive and manic or hypomanic episodes. Although common and expensive to treat, the clinical assessment of bipolar disorder is still ill-defined. Objective: In the current literature several correlations between mood disorders and dysfunctions involving the autonomic nervous system (ANS) can be found. The objective of this work is to develop a novel mood recognition system based on a pervasive, wearable and personalized monitoring system using ANS-related biosignals. Materials and methods: The monitoring platform used in this study is the core sensing system of the personalized monitoring systems for care in mental health (PSYCHE) European project. It is comprised of a comfortable sensorized t-shirt that can acquire the inter-beat interval time series, the heart rate, and the respiratory dynamics for long-term monitoring during the day and overnight. In this study, three bipolar patients were followed for a period of 90 days during which up to six monitoring sessions and psychophysical evaluations were performed for each patient. Specific signal processing techniques and artificial intelligence algorithms were applied to analyze more than 120. h of data. Results: Experimental results are expressed in terms of confusion matrices and an exhaustive descriptive statistics of the most relevant features is reported as well. A classification accuracy of about 97% is achieved for the intra-subject analysis. Such an accuracy was found in distinguishing relatively good affective balance state (euthymia) from severe clinical states (severe depression and mixed state) and is lower in distinguishing euthymia from the milder states (accuracy up to 88%). Conclusions: The PSYCHE platform could provide a viable decision support system in order to improve mood assessment in patient care. Evidences about the correlation between mood disorders and ANS dysfunctions were found and the obtained results are promising for an effective biosignal-based mood recognition. © 2012 Elsevier B.V.","Autonomic nervous system monitoring; Bipolar disorder; Hearth rate variability; Mood recognition; Multilayer perceptron; PSYCHE platform; Respiration; Wearable textile system","Autonomic nervous system; Bipolar disorder; Hearth rate variability; Mood recognition; Multi layer perceptron; PSYCHE platform; Respiration; Textile systems; Decision support systems; Monitoring; Sensors; Signal processing; Artificial intelligence; accuracy; adult; algorithm; article; artificial intelligence; bipolar I disorder; bipolar II disorder; breathing rate; case report; clinical evaluation; clinical feature; clinical practice; clothing; controlled study; decision support system; disease severity; female; follow up; heart rate; human; male; medical literature; mental health care; mood disorder; patient care; patient monitoring; personalized medicine; priority journal; psychophysiology; signal processing; statistics; time series analysis; Adult; Affect; Algorithms; Artificial Intelligence; Autonomic Nervous System; Biosensing Techniques; Bipolar Disorder; Clothing; Decision Support Techniques; Diagnosis, Computer-Assisted; Electrocardiography, Ambulatory; Equipment Design; Female; Heart Rate; Humans; Male; Middle Aged; Models, Statistical; Monitoring, Ambulatory; Predictive Value of Tests; Respiratory Rate; Severity of Illness Index; Signal Processing, Computer-Assisted; Time Factors; Transducers",Article,Scopus,2-s2.0-84875269658
"Coradeschi S., Cesta A., Cortellessa G., Coraci L., Gonzalez J., Karlsson L., Furfari F., Loutfi A., Orlandini A., Palumbo F., Pecora F., Von Rump S., Stimec A., Ullberg J., Otslund B.","GiraffPlus: Combining social interaction and long term monitoring for promoting independent living",2013,"2013 6th International Conference on Human System Interactions, HSI 2013",49,10.1109/HSI.2013.6577883,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883702479&doi=10.1109%2fHSI.2013.6577883&partnerID=40&md5=fecd61ea6f73dae0f1bbd041f1efcebb","Early detection and adaptive support to changing individual needs related to ageing is an important challenge in today's society. In this paper we present a system called GiraffPlus that aims at addressing such a challenge and is developed in an on-going European project. The system consists of a network of home sensors that can be automatically configured to collect data for a range of monitoring services; a semi-autonomous telepresence robot; a sophisticated context recognition system that can give high-level and long term interpretations of the collected data and respond to certain events; and personalized services delivered through adaptive user interfaces for primary users. The system performs a range of services including data collection and analysis of long term trends in behaviors and physiological parameters (e.g. relating to sleep or daily activity); warnings, alarms and reminders; and social interaction through the telepresence robot. The latter is based on the Giraff telepresence robot, which is already in place in a number of homes. A distinctive aspect of the project is that the GiraffPlus system will be installed and evaluated in at least 15 homes of elderly people. This paper provides a general overview of the GiraffPlus system and its evaluation. © 2013 IEEE.","Artificial Intelligence; Health Care and Assistive Devices; Human Machine Interaction; Smart Homes","Adaptive user interface; Assistive devices; Context recognition; Human machine interaction; Long term monitoring; Personalized service; Physiological parameters; Smart homes; Artificial intelligence; Automation; Intelligent buildings; Physiological models; Robots; Visual communication; Social sciences",Conference Paper,Scopus,2-s2.0-84883702479
"Chrysoulakis N., Lopes M., San José R., Grimmond C.S.B., Jones M.B., Magliulo V., Klostermann J.E.M., Synnefa A., Mitraka Z., Castro E.A., González A., Vogt R., Vesala T., Spano D., Pigeon G., Freer-Smith P., Staszewski T., Hodges N., Mills G., Cartalis C.","Sustainable urban metabolism as a link between bio-physical sciences and urban planning: The BRIDGE project",2013,"Landscape and Urban Planning",49,10.1016/j.landurbplan.2012.12.005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875275154&doi=10.1016%2fj.landurbplan.2012.12.005&partnerID=40&md5=50f936394d027815fd83eae5df4e0ffd","Urban metabolism considers a city as a system with flows of energy and material between it and the environment. Recent advances in bio-physical sciences provide methods and models to estimate local scale energy, water, carbon and pollutant fluxes. However, good communication is required to provide this new knowledge and its implications to endusers (such as urban planners, architects and engineers). The FP7 project BRIDGE (sustainaBle uRban plannIng Decision support accountinG for urban mEtabolism) aimed to address this gap by illustrating the advantages of considering these issues in urban planning. The BRIDGE Decision Support System (DSS) aids the evaluation of the sustainability of urban planning interventions. The Multi Criteria Analysis approach adopted provides a method to cope with the complexity of urban metabolism. In consultation with targeted end-users, objectives were defined in relation to the interactions between the environmental elements (fluxes of energy, water, carbon and pollutants) and socioeconomic components (investment costs, housing, employment, etc.) of urban sustainability. The tool was tested in five case study cities: Helsinki, Athens, London, Florence and Gliwice; and sub-models were evaluated using flux data selected. This overview of the BRIDGE project covers the methods and tools used to measure and model the physical flows, the selected set of sustainability indicators, the methodological framework for evaluating urban planning alternatives and the resulting DSS prototype. © 2012 Elsevier B.V.","Decision Support Systems; Flux measurements; Modelling; Multi-criteria analysis; Urban metabolism; Urban planning alternatives","Decision support system (dss); Decision supports; Flux measurements; Methodological frameworks; Multi Criteria Analysis; Sustainability indicators; Urban metabolisms; Urban sustainability; Artificial intelligence; Carbon; Decision support systems; Economics; Investments; Metabolism; Models; Physiology; Sustainable development; Water pollution; Urban planning; carbon flux; decision support system; energy flux; environmental impact assessment; multicriteria analysis; planning process; pollutant transport; sustainability; sustainable development; urban ecosystem; urban planning; water exchange; Athens [Attica]; Attica; England; Finland; Firenze [Tuscany]; Florence [Firenze]; Gliwice; Greece; Helsinki; Italy; London [England]; Poland; Slaskie; Tuscany; United Kingdom; Varsinais-Suomi",Article,Scopus,2-s2.0-84875275154
"Waldrop M.M.","Online learning: Campus 2.0",2013,"Nature",49,10.1038/495160a,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874993652&doi=10.1038%2f495160a&partnerID=40&md5=492e28db8f5f67d185d514c799aceacf",[No abstract available],,"academic research; educational development; fragmentation; Internet; social network; teaching; technological development; videography; article; computer program; economics; human; learning curve; machine learning; online system; pedagogics; priority journal; social network; teaching; Accreditation; Algorithms; Artificial Intelligence; Computer-Assisted Instruction; Education, Distance; Feedback; Humans; Internet; Learning; Mentors; Population Density; Public Sector; Research; Self-Help Groups; Social Media; Statistics as Topic; Students; Universities; Vocational Guidance; Maryland; United States",Article,Scopus,2-s2.0-84874993652
"Castronova A.M., Goodall J.L., Elag M.M.","Models as web services using the Open Geospatial Consortium (OGC) Web Processing Service (WPS) standard",2013,"Environmental Modelling and Software",49,10.1016/j.envsoft.2012.11.010,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871646856&doi=10.1016%2fj.envsoft.2012.11.010&partnerID=40&md5=5e3e8a6f9f175d6b05e97fb669014647","Environmental modeling often requires the use of multiple data sources, models, and analysis routines coupled into a workflow to answer a research question. Coupling these computational resources can be accomplished using various tools, each requiring the developer to follow a specific protocol to ensure that components are linkable. Despite these coupling tools, it is not always straight forward to create a modeling workflow due to platform dependencies, computer architecture requirements, and programming language incompatibilities. A service-oriented approach that enables individual models to operate and interact with others using web services is one method for overcoming these challenges. This work advances the idea of service-oriented modeling by presenting a design for a modeling service that builds from the Open Geospatial Consortium (OGC) Web Processing Service (WPS) protocol. We demonstrate how the WPS protocol can be used to create modeling services, and then demonstrate how these modeling services can be brought into workflow environments using generic client-side code. We implemented this approach within the HydroModeler environment, a model coupling tool built on the Open Modeling Interface standard (version 1.4), and show how a hydrology model can be hosted as a WPS web service and used within a client-side workflow. The primary advantage of this approach is that the server-side software follows an established standard that can be leveraged and reused within multiple workflow environments and decision support systems. © 2012 Elsevier Ltd.","Component-based modeling; Integrated modeling; OGC WPS; OpenMI; Service-oriented computing; Web services","Component-based modeling; Integrated modeling; OGC WPS; OpenMI; Service oriented computing; Artificial intelligence; Computer architecture; Decision support systems; Distributed computer systems; Web services; Websites; data set; decision support system; environmental modeling; hydrological modeling; integrated approach; numerical model; software; standard (reference); World Wide Web",Article,Scopus,2-s2.0-84871646856
"Yao K., Gao J., Gao Y.","Some stability theorems of uncertain differential equation",2013,"Fuzzy Optimization and Decision Making",49,10.1007/s10700-012-9139-4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873996460&doi=10.1007%2fs10700-012-9139-4&partnerID=40&md5=63f2ca32b658dc00c24bcba662513309","Canonical process is a type of uncertain process with stationary and independent increments which are normal uncertain variables, and uncertain differential equation is a type of differential equation driven by canonical process. This paper will give a theorem on the Lipschitz continuity of canonical process based on which this paper will also provide a sufficient condition for an uncertain differential equation being stable. © 2012 Springer Science+Business Media New York.","Canonical process; Stability; Uncertain differential equation; Uncertainty theory","Canonical process; Lipschitz continuity; Stability theorems; Sufficient conditions; Uncertain differential equations; Uncertain process; Uncertain variables; Uncertainty theory; Artificial intelligence; Convergence of numerical methods; Decision making; Differential equations",Article,Scopus,2-s2.0-84873996460
"Hsu C.-H., Juang C.-F.","Evolutionary robot wall-following control using type-2 fuzzy controller with species-DE-activated continuous ACO",2013,"IEEE Transactions on Fuzzy Systems",49,10.1109/TFUZZ.2012.2202665,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873383295&doi=10.1109%2fTFUZZ.2012.2202665&partnerID=40&md5=5bff2ebe9f555c5325a372a46276c623","This paper proposes evolutionary wall-following control of a mobile robot using an interval type-2 fuzzy controller (IT2FC) with species-differential- evolution-activated continuous ant colony optimization (SDE-CACO). Both the position and speed of a mobile robot are controlled by using two IT2FCs to improve noise resistance ability. A new cost function is defined to accurately evaluate the wall-following performance of an evolutionary IT2FC. A two-stage training approach is proposed that learns a position IT2FC followed by a speed IT2FC to optimize both the wall-following accuracy and the moving speed. The proposed learning approach avoids the time consuming task of the exhaustive collection of supervised input-output training pairs. All fuzzy rules are generated online using a clustering-based approach during the evolutionary learning process. All of the free parameters in an online-generated IT2FC are optimized using SDE-CACO, in which an SDE mutation operation is incorporated within a continuous ACO to improve its explorative ability. The proposed SDE-CACO is compared with various population-based optimization algorithms to demonstrate its efficiency and effectiveness in the wall-following control problem. This study also includes experiments that demonstrate wall-following control utilizing a real mobile robot. © 2012 IEEE.","Continuous ant colony optimization; differential evolution; evolutionary robots; robot motion control; swarm intelligence; type-2 fuzzy controller (IT2FC)","Ant Colony Optimization (ACO); Differential Evolution; Fuzzy controllers; Robot motion control; Swarm Intelligence; Constrained optimization; Evolutionary algorithms; Mobile robots; Artificial intelligence",Article,Scopus,2-s2.0-84873383295
"Zhang W., Niu P., Li G., Li P.","Forecasting of turbine heat rate with online least squares support vector machine based on gravitational search algorithm",2013,"Knowledge-Based Systems",49,10.1016/j.knosys.2012.10.004,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871926704&doi=10.1016%2fj.knosys.2012.10.004&partnerID=40&md5=e40b6477cbf4057888086fad15edfbb8","Accurate heat rate forecasting is very important in ensuring the economic, efficient, and safe operation of a steam turbine unit. The support vector machine (SVM) is a novel tool from the artificial intelligence field that has been successfully applied to heat rate forecasting. The least squares SVM (LS-SVM) is an improved algorithm based on the SVM. LS-SVM has minimal computational complexity and fast calculation. However, traditional LS-SVM, which was established by using offline data samples, can no longer accurately describe the actual system working condition, thereby resulting in problems when directly used in heat rate prediction. In this paper, a heat rate forecasting method based on online LS-SVM, which possesses dynamic prediction functions, is proposed. To avoid blindness and inaccuracy in parameter selection, the gravitational search algorithm (GSA) is used to optimize the regularization parameter γ and the kernel parameter σ2 of the online LS-SVM modeling. The results confirm the efficiency of the proposed method. © 2012 Elsevier B.V. All rights reserved.","Gravitational search algorithm; Heat rate; Least squares support vector machine; Online learning; Steam turbine","Actual system; Dynamic prediction; Forecasting methods; Gravitational search algorithms; Heat rate; Kernel parameter; Least Square; Least squares support vector machines; Offline data; Online learning; Parameter selection; Regularization parameters; Safe operation; Artificial intelligence; Forecasting; Learning algorithms; Steam turbines; Support vector machines",Article,Scopus,2-s2.0-84871926704
"Liu C., Loy C.C., Gong S., Wang G.","POP: Person re-identification post-rank optimisation",2013,"Proceedings of the IEEE International Conference on Computer Vision",49,10.1109/ICCV.2013.62,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898831069&doi=10.1109%2fICCV.2013.62&partnerID=40&md5=4b5f5eaea3e3c2066bd3d4c1ce46108e","Owing to visual ambiguities and disparities, person re-identification methods inevitably produce sub optimal rank-list, which still requires exhaustive human eyeballing to identify the correct target from hundreds of different likely-candidates. Existing re-identification studies focus on improving the ranking performance, but rarely look into the critical problem of optimising the time-consuming and error-prone post-rank visual search at the user end. In this study, we present a novel one-shot Post-rank Optimization (POP) method, which allows a user to quickly refine their search by either 'one-shot' or a couple of sparse negative selections during a re-identification process. We conduct systematic behavioural studies to understand user's searching behaviour and show that the proposed method allows correct re-identification to converge 2.6 times faster than the conventional exhaustive search. Importantly, through extensive evaluations we demonstrate that the method is capable of achieving significant improvement over the state-of-the-art distance metric learning based ranking models, even with just 'one shot' feedback optimisation, by as much as over 30% performance improvement for rank 1 re-identification on the VIPeR and i-LIDS datasets. © 2013 IEEE.","human computer interaction; information retrieval; manifold; person re-identification; ranking; visual surveillance","Artificial intelligence; Behavioral research; Human computer interaction; Information retrieval; Behavioural studies; Distance Metric Learning; manifold; Negative selection; Person re identifications; ranking; Ranking performance; Visual surveillance; Optimization",Conference Paper,Scopus,2-s2.0-84898831069
"Baydogan M.G., Runger G., Tuv E.","A bag-of-features framework to classify time series",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",48,10.1109/TPAMI.2013.72,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884564759&doi=10.1109%2fTPAMI.2013.72&partnerID=40&md5=86343f74a3ebdc23c3cdcc7c625b8681","Time series classification is an important task with many challenging applications. A nearest neighbor (NN) classifier with dynamic time warping (DTW) distance is a strong solution in this context. On the other hand, feature-based approaches have been proposed as both classifiers and to provide insight into the series, but these approaches have problems handling translations and dilations in local patterns. Considering these shortcomings, we present a framework to classify time series based on a bag-of-features representation (TSBF). Multiple subsequences selected from random locations and of random lengths are partitioned into shorter intervals to capture the local information. Consequently, features computed from these subsequences measure properties at different locations and dilations when viewed from the original series. This provides a feature-based approach that can handle warping (although differently from DTW). Moreover, a supervised learner (that handles mixed data types, different units, etc.) integrates location information into a compact codebook through class probability estimates. Additionally, relevant global features can easily supplement the codebook. TSBF is compared to NN classifiers and other alternatives (bag-of-words strategies, sparse spatial sample kernels, shapelets). Our experimental results show that TSBF provides better results than competitive methods on benchmark datasets from the UCR time series database. © 1979-2012 IEEE.","codebook; feature extraction; Supervised learning","Benchmark datasets; Class probabilities; Codebooks; Dynamic time warping; Location information; Nearest Neighbor classifier; Time series classifications; Time Series Database; Feature extraction; Supervised learning; Time series; algorithm; article; artificial intelligence; automated pattern recognition; computer simulation; methodology; theoretical model; automated pattern recognition; procedures; Algorithms; Artificial Intelligence; Computer Simulation; Models, Theoretical; Pattern Recognition, Automated; Algorithms; Artificial Intelligence; Computer Simulation; Models, Theoretical; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84884564759
"Gupta S., Kapoor P., Chaudhary K., Gautam A., Kumar R., Raghava G.P.S.","In Silico Approach for Predicting Toxicity of Peptides and Proteins",2013,"PLoS ONE",48,10.1371/journal.pone.0073957,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884141339&doi=10.1371%2fjournal.pone.0073957&partnerID=40&md5=9061d6f0ef3cc23d5003e77187b368b4","Background:Over the past few decades, scientific research has been focused on developing peptide/protein-based therapies to treat various diseases. With the several advantages over small molecules, including high specificity, high penetration, ease of manufacturing, peptides have emerged as promising therapeutic molecules against many diseases. However, one of the bottlenecks in peptide/protein-based therapy is their toxicity. Therefore, in the present study, we developed in silico models for predicting toxicity of peptides and proteins.Description:We obtained toxic peptides having 35 or fewer residues from various databases for developing prediction models. Non-toxic or random peptides were obtained from SwissProt and TrEMBL. It was observed that certain residues like Cys, His, Asn, and Pro are abundant as well as preferred at various positions in toxic peptides. We developed models based on machine learning technique and quantitative matrix using various properties of peptides for predicting toxicity of peptides. The performance of dipeptide-based model in terms of accuracy was 94.50% with MCC 0.88. In addition, various motifs were extracted from the toxic peptides and this information was combined with dipeptide-based model for developing a hybrid model. In order to evaluate the over-optimization of the best model based on dipeptide composition, we evaluated its performance on independent datasets and achieved accuracy around 90%. Based on above study, a web server, ToxinPred has been developed, which would be helpful in predicting (i) toxicity or non-toxicity of peptides, (ii) minimum mutations in peptides for increasing or decreasing their toxicity, and (iii) toxic regions in proteins.Conclusion:ToxinPred is a unique in silico method of its kind, which will be useful in predicting toxicity of peptides/proteins. In addition, it will be useful in designing least toxic peptides and discovering toxic regions in proteins. We hope that the development of ToxinPred will provide momentum to peptide/protein-based drug discovery (http://crdd.osdd.net/raghava/toxinpred/). © 2013 Gupta et al.",,"anticancer peptide; antiviral peptide; asparagine; cell penetrating peptide; cysteine; dipeptide; histidine; peptide; polypeptide antibiotic agent; proline; protein; tumor homing peptide; unclassified drug; accuracy; amino acid sequence; article; computer model; computer program; correlation coefficient; machine learning; prediction; protein database; protein motif; quantitative analysis; statistical model; toxicity; Amino Acid Motifs; Animals; Artificial Intelligence; Computer Simulation; Databases, Protein; Humans; Internet; Models, Molecular; Molecular Sequence Data; Peptides; Sensitivity and Specificity; Sequence Analysis, Protein; Software",Article,Scopus,2-s2.0-84884141339
"Liu M., Nguyen P.Q.","Solving BDD by enumeration: An update",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",48,10.1007/978-3-642-36095-4_19,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874342837&doi=10.1007%2f978-3-642-36095-4_19&partnerID=40&md5=b0d6ece984941a12c0c3fdeb3d8a3837","Bounded Distance Decoding (BDD) is a basic lattice problem used in cryptanalysis: the security of most lattice-based encryption schemes relies on the hardness of some BDD, such as LWE. We study how to solve BDD using a classical method for finding shortest vectors in lattices: enumeration with pruning speedup, such as Gama-Nguyen-Regev extreme pruning from EUROCRYPT '10. We obtain significant improvements upon Lindner-Peikert's Search-LWE algorithm (from CT-RSA '11), and update experimental cryptanalytic results, such as attacks on DSA with partially known nonces and GGH encryption challenges. Our work shows that any security estimate of BDD-based cryptosystems must take into account enumeration attacks, and that BDD enumeration can be practical even in high dimension like 350. © 2013 Springer-Verlag.",,"Bounded distance decoding; Classical methods; Encryption schemes; High dimensions; Lattice problems; Lattice-based; Artificial intelligence; Cryptography",Conference Paper,Scopus,2-s2.0-84874342837
"Waltman L., Schreiber M.","On the calculation of percentile-based bibliometric indicators",2013,"Journal of the American Society for Information Science and Technology",48,10.1002/asi.22775,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872900058&doi=10.1002%2fasi.22775&partnerID=40&md5=b2e7f02a94a5b9e9111266d644191eda","A percentile-based bibliometric indicator is an indicator that values publications based on their position within the citation distribution of their field. The most straightforward percentile-based indicator is the proportion of frequently cited publications, for instance, the proportion of publications that belong to the top 10% most frequently cited of their field. Recently, more complex percentile-based indicators have been proposed. A difficulty in the calculation of percentile-based indicators is caused by the discrete nature of citation distributions combined with the presence of many publications with the same number of citations. We introduce an approach to calculating percentile-based indicators that deals with this difficulty in a more satisfactory way than earlier approaches suggested in the literature. We show in a formal mathematical framework that our approach leads to indicators that do not suffer from biases in favor of or against particular fields of science. © 2012 ASIS&;T.","citation analysis","Bibliometric; Citation analysis; Citation distribution; Mathematical frameworks; Artificial intelligence; Software engineering; Publishing",Article,Scopus,2-s2.0-84872900058
"Butcher J.B., Verstraeten D., Schrauwen B., Day C.R., Haycock P.W.","Reservoir computing and extreme learning machines for non-linear time-series data analysis",2013,"Neural Networks",48,10.1016/j.neunet.2012.11.011,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871651156&doi=10.1016%2fj.neunet.2012.11.011&partnerID=40&md5=0a75a18e181ac12aa2b32d715cf6e1d6","Random projection architectures such as Echo state networks (ESNs) and Extreme Learning Machines (ELMs) use a network containing a randomly connected hidden layer and train only the output weights, overcoming the problems associated with the complex and computationally demanding training algorithms traditionally used to train neural networks, particularly recurrent neural networks. In this study an ESN is shown to contain an antagonistic trade-off between the amount of non-linear mapping and short-term memory it can exhibit when applied to time-series data which are highly non-linear. To overcome this trade-off a new architecture, Reservoir with Random Static Projections (R2SP) is investigated, that is shown to offer a significant improvement in performance. A similar approach using an ELM whose input is presented through a time delay (TD-ELM) is shown to further enhance performance where it significantly outperformed the ESN and R2SP as well other architectures when applied to a novel task which allows the short-term memory and non-linearity to be varied. The hard-limiting memory of the TD-ELM appears to be best suited for the data investigated in this study, although ESN-based approaches may offer improved performance when processing data which require a longer fading memory. © 2012 Elsevier Ltd.","Extreme learning machine; Non-linearity; Reservoir computing; Reservoir with random static projections; Short-term memory; Time-series data","Extreme learning machine; Non-linearity; Reservoir Computing; Short term memory; Time-series data; Brain; Data handling; Knowledge acquisition; Network architecture; Recurrent neural networks; Time series analysis; algorithm; article; calculation; data analysis; linear system; machine learning; mathematical computing; mathematical model; nonlinear system; priority journal; short term memory; signal noise ratio; simulation; time series analysis; Artificial Intelligence; Mathematical Computing; Nonlinear Dynamics; Random Allocation; Statistics as Topic",Article,Scopus,2-s2.0-84871651156
"Guo G., Zhang J., Yorke-Smith N.","A novel bayesian similarity measure for recommender systems",2013,"IJCAI International Joint Conference on Artificial Intelligence",47,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062677&partnerID=40&md5=d8ead747507c44a0e7446aba9cb7f3e7","Collaborative filtering, a widely-used user-centric recommendation technique, predicts an item's rating by aggregating its ratings from similar users. User similarity is usually calculated by cosine similarity or Pearson correlation coefficient. However, both of them consider only the direction of rating vectors, and suffer from a range of drawbacks. To solve these issues, we propose a novel Bayesian similarity measure based on the Dirichlet distribution, taking into consideration both the direction and length of rating vectors. Further, our principled method reduces correlation due to chance. Experimental results on six real-world data sets show that our method achieves superior accuracy.",,"Bayesian; Cosine similarity; Dirichlet distributions; Pearson correlation coefficients; Real-world; Recommendation techniques; Similarity measure; User-centric; Artificial intelligence; Correlation methods; Virtual reality; Rating",Conference Paper,Scopus,2-s2.0-84896062677
"Duan H., Li S., Shi Y.","Predator-prey brain storm optimization for DC brushless motor",2013,"IEEE Transactions on Magnetics",47,10.1109/TMAG.2013.2262296,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884683450&doi=10.1109%2fTMAG.2013.2262296&partnerID=40&md5=61198051225de2ea1bc4c47b76b77c76","Brain Storm Optimization (BSO) is a newly-developed swarm intelligence optimization algorithm inspired by a human being's behavior of brainstorming. In this paper, a novel predator-prey BSO model, which is named Predator-prey Brain Storm Optimization (PPBSO), is proposed to solve an optimization problem modeled for a DC brushless motor. The Predator-prey concept is adopted to better utilize the global information and improve the swarm diversity during the evolution process. The proposed algorithm is applied to solve the optimization problems in an electromagnetic field. The comparative results demonstrate that both PPBSO and BSO can succeed in optimizing design variables for a DC brushless motor to maximize its efficiency. Simulation results show PPBSO has better ability to jump out of local optima when compared with the original BSO. In addition, it demonstrates satisfactory stability in repeated experiments. © 1965-2012 IEEE.","Brain storm optimization (BSO); brushless motor; electromagnetics; evolutionary computation; optimization","Brushless motors; Dc brushless motors; Electromagnetics; Evolution process; Global informations; Optimization problems; Optimizing design; Swarm intelligence optimization algorithm; Algorithms; Artificial intelligence; Behavioral research; Brushless DC motors; Electromagnetic fields; Evolutionary algorithms; Storms; Optimization",Article,Scopus,2-s2.0-84884683450
"Domke J.","Learning graphical model parameters with approximate marginal inference",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",47,10.1109/TPAMI.2013.31,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883162364&doi=10.1109%2fTPAMI.2013.31&partnerID=40&md5=9eba05cf719025d38167f88aa8e0a0d5","Likelihood-based learning of graphical models faces challenges of computational complexity and robustness to model misspecification. This paper studies methods that fit parameters directly to maximize a measure of the accuracy of predicted marginals, taking into account both model and inference approximations at training time. Experiments on imaging problems suggest marginalization-based learning performs better than likelihood-based approximations on difficult problems where the model being fit is approximate in nature. © 1979-2012 IEEE.","conditional random fields; Graphical models; inference; machine learning; segmentation","Conditional random field; Fit parameters; GraphicaL model; Imaging problems; inference; Likelihood-based learning; Model misspecification; Training time; Graphic methods; Image segmentation; Learning systems; Speech recognition; algorithm; article; artificial intelligence; automated pattern recognition; computer simulation; image subtraction; methodology; reproducibility; sensitivity and specificity; statistical model; automated pattern recognition; procedures; Algorithms; Artificial Intelligence; Computer Simulation; Models, Statistical; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique; Algorithms; Artificial Intelligence; Computer Simulation; Models, Statistical; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-84883162364
"Li W., Zhao R., Wang X.","Human reidentification with transferred metric learning",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",47,10.1007/978-3-642-37331-2_3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875891971&doi=10.1007%2f978-3-642-37331-2_3&partnerID=40&md5=10e936d14f3c2c9346a23348290fceaf","Human reidentification is to match persons observed in non-overlapping camera views with visual features for inter-camera tracking. The ambiguity increases with the number of candidates to be distinguished. Simple temporal reasoning can simplify the problem by pruning the candidate set to be matched. Existing approaches adopt a fixed metric for matching all the subjects. Our approach is motivated by the insight that different visual metrics should be optimally learned for different candidate sets. We tackle this problem under a transfer learning framework. Given a large training set, the training samples are selected and reweighted according to their visual similarities with the query sample and its candidate set. A weighted maximum margin metric is online learned and transferred from a generic metric to a candidate-set-specific metric. The whole online reweighting and learning process takes less than two seconds per candidate set. Experiments on the VIPeR dataset and our dataset show that the proposed transferred metric learning significantly outperforms directly matching visual features or using a single generic metric learned from the whole training set. © 2013 Springer-Verlag.",,"Learning process; Metric learning; Non-overlapping camera views; Re identifications; Temporal reasoning; Training sample; Transfer learning; Visual similarity; Artificial intelligence; Learning systems",Conference Paper,Scopus,2-s2.0-84875891971
"Singh M., Panigrahi B.K., Abhyankar A.R.","Optimal coordination of directional over-current relays using Teaching Learning-Based Optimization (TLBO) algorithm",2013,"International Journal of Electrical Power and Energy Systems",47,10.1016/j.ijepes.2013.02.011,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875176821&doi=10.1016%2fj.ijepes.2013.02.011&partnerID=40&md5=6e6799523910318f3083848dbdb08500","Relay Coordination in a big distribution system with multiple meshes and bidirectional power feed becomes very Complex for protection engineers. Manual and graph theory based approaches were applied successfully in small power system. In a big distribution system linear and non-linear programming based optimizing techniques are applied for relay coordination. Presently, artificial intelligence techniques are applied for optimal co-ordination of directional overcurrent relays. This paper discusses the application of Teaching Learning Based Optimization (TLBO) algorithm for optimal coordination of DOCR relays in a looped power system. Combination of primary and backup relay is chosen by using Far vector of LINKNET structure, to avoid mis-coordination of relays. Coordination of DOCR is tested for IEEE 3, 4 and 6 bus systems using the TLBO. Also, the objective function is modified to optimize the operating time between backup and primary relays. The results are compared with the optimized values of Time dial setting and Plug setting values obtained from modified differential evolution algorithm. The proposed algorithm TLBO gives optimal coordination margin between 0.3 and 0.8 s and no miscoordination between primary and backup pairs. Results are also verified using Digsilient power factory simulation software. © 2013 Elsevier Ltd. All rights reserved.","Coordination of directional overcurrent relays; Coordination time interval; Digsilient power factory simulation software; LINKNET; Power system; TLBO algorithm","Artificial intelligence techniques; Directional over-current relays; Distribution systems; Factory simulation; LinkNet; Modified differential evolution algorithms; Teaching-learning-based optimizations; Time interval; Artificial intelligence; Electric relays; Graph theory; Learning algorithms; Optimization; Overcurrent protection; Standby power systems; Coordination reactions",Article,Scopus,2-s2.0-84875176821
"Li X., Zheng A., Zhang X., Li C., Zhang L.","Rolling element bearing fault detection using support vector machine with improved ant colony optimization",2013,"Measurement: Journal of the International Measurement Confederation",47,10.1016/j.measurement.2013.04.081,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879089831&doi=10.1016%2fj.measurement.2013.04.081&partnerID=40&md5=58642748395877f43e376883ae316eb1","In support vector machine (SVM), it is quite necessary to optimize the parameters which are the key factors impacting the classification performance. Improved ant colony optimization (IACO) algorithm is proposed to determine the parameters, and then the IACO-SVM algorithm is applied on the rolling element bearing fault detection. Both the optimal and the worst solutions found by the ants are allowed to update the pheromone trail density, and the mesh is applied in the ACO to adjust the range of optimized parameters. The experimental data of rolling bearing vibration signal is used to illustrate the performance of IACO-SVM algorithm by comparing with the parameters in SVM optimized by genetic algorithm (GA), cross-validation and standard ACO methods. The experimental results show that the proposed algorithm of IACO-SVM can give higher recognition accuracy. © 2013 Elsevier Ltd. All rights reserved.","Ant colony optimization; Fault detection; Meshing; Rolling element bearing; Support vector machine","Ant colony optimization; Artificial intelligence; Bearings (machine parts); Fault detection; Genetic algorithms; Classification performance; Experimental datum; Improved ant colony optimization; Meshing; Optimized parameter; Recognition accuracy; Rolling bearing vibration; Rolling Element Bearing; Support vector machines",Article,Scopus,2-s2.0-84879089831
"Porzio G.F., Fornai B., Amato A., Matarese N., Vannucci M., Chiappelli L., Colla V.","Reducing the energy consumption and CO2 emissions of energy intensive industries through decision support systems - An example of application to the steel industry",2013,"Applied Energy",47,10.1016/j.apenergy.2013.05.005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884204458&doi=10.1016%2fj.apenergy.2013.05.005&partnerID=40&md5=340bf84a2539b0c65321e3104afb8dd4","The management of process industries is becoming in the recent years more and more challenging, given the stringent environmental policies as well as raising energy costs and the always-present drive for profit. A way to help plant decision makers in their daily choices is to refer to decision-support tools, which can give advice on the best practices on how to operate a plant in order to reduce the energy consumption and the CO2 emissions keeping at the same time the costs under control. Such an approach can be useful in a variety of industries, particularly the most energy-intensive ones such as iron and steel industries. In this paper, an approach to the realisation of a software system, which allows to generate internal reports on the plant performances, as well as to simulate the plant behaviour in different scenarios, is described. The main production processes (coke plant, blast furnace, steel shop, hot rolling mill) are described and simulated focusing on the prediction of products flow rates and composition, energy consumption and GHGs (Greenhouse Gases) emissions in different operating conditions. The importance of a correct management of the CO2 within the plant is underlined, particularly with regard to the new EU Emission Trading System, which will be based on European benchmarks. The software tool is illustrated and a case study is included, which focuses on the simultaneous minimisation of the CO2 emissions and maximisation of the profit through an optimised management of the by-product gases. The results from the case study show a good potential for process improvement, by a reduction in the cost and environmental impact. © 2013 Elsevier Ltd.","Decision support; Energy consumption reduction; Process optimisation; Simulation","Artificial intelligence; Blast furnaces; Coke plants; Computer software; Cost reduction; Decision support systems; Energy utilization; Environmental impact; Greenhouse gases; Hot rolling mills; Iron and steel industry; Profitability; Steelmaking; Consumption reductions; Decision support tools; Decision supports; Different operating conditions; Energy intensive industries; Environmental policy; Process optimisation; Simulation; Carbon dioxide; carbon dioxide; carbon emission; cost-benefit analysis; decision support system; emission control; energy conservation; energy efficiency; environmental impact; European Union; greenhouse gas; optimization; prediction; primary sector industry; software; Europe",Article,Scopus,2-s2.0-84884204458
"Campello R.J.G.B., Moulavi D., Sander J.","Density-based clustering based on hierarchical density estimates",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",46,10.1007/978-3-642-37456-2_14,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893586407&doi=10.1007%2f978-3-642-37456-2_14&partnerID=40&md5=58061d37ca9fc748171ef95cfd32c03e","We propose a theoretically and practically improved density-based, hierarchical clustering method, providing a clustering hierarchy from which a simplified tree of significant clusters can be constructed. For obtaining a ""flat"" partition consisting of only the most significant clusters (possibly corresponding to different density thresholds), we propose a novel cluster stability measure, formalize the problem of maximizing the overall stability of selected clusters, and formulate an algorithm that computes an optimal solution to this problem. We demonstrate that our approach outperforms the current, state-of-the-art, density-based clustering methods on a wide variety of real world data. © Springer-Verlag 2013.",,"Cluster stability; Density estimates; Density-based; Density-based Clustering; Different densities; Hierarchical clustering methods; Optimal solutions; Overall stabilities; Artificial intelligence; Computer science; Computers; Data mining",Conference Paper,Scopus,2-s2.0-84893586407
"Nguyen T.H., Yang R., Azaria A., Kraus S., Tambe M.","Analyzing the effectiveness of adversary modeling in security games",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",46,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893350174&partnerID=40&md5=6a849de8d66faa3c449770b1f1412789","Recent deployments of Stackelberg security games (SSG) have led to two competing approaches to handle boundedly rational human adversaries: (1) integrating models of human (adversary) decision-making into the game-theoretic algorithms, and (2) applying robust optimization techniques that avoid adversary modeling. A recent algorithm (MATCH) based on the second approach was shown to outperform the leading modeling-based algorithm even in the presence of significant amount of data. Is there then any value in using human behavior models in solving SSGs? Through extensive experiments with 547 human subjects playing 11102 games in total, we emphatically answer the question in the affirmative, while providing the following key contributions: (i) we show that our algorithm, SU-BRQR, based on a novel integration of human behavior model with the subjective utility function, significantly outperforms both MATCH and its improvements; (ii) we are the first to present experimental results with security intelligence experts, and find that even though the experts are more rational than the Amazon Turk workers, SU-BRQR still outperforms an approach assuming perfect rationality (and to a more limited extent MATCH); (iii) we show the advantage of SU-BRQR in a new, large game setting and demonstrate that sufficient data enables it to improve its performance over MATCH. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Adversary modeling; Human behavior modeling; Human behavior models; Human subjects; Integrating model; Robust optimization; Security games; Utility functions; Algorithms; Artificial intelligence; Behavioral research; Social sciences; Rational functions",Conference Paper,Scopus,2-s2.0-84893350174
"Xie D., Boyle A.P., Wu L., Zhai J., Kawli T., Snyder M.","XDynamic trans-acting factor colocalization in human cells",2013,"Cell",46,10.1016/j.cell.2013.09.043,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886780296&doi=10.1016%2fj.cell.2013.09.043&partnerID=40&md5=4da8ff0def620d7fbfc16561889ef53c","Different trans-acting factors (TFs) collaborate and act in concert at distinct loci to perform accurate regulation of their target genes. To date, the cobinding of TF pairs has been investigated in a limited context both in terms of the number of factors within a cell type and across cell types and the extent of combinatorial colocalizations. Here, we use an approach to analyze TF colocalization within a cell type and across multiple cell lines at an unprecedented level. We extend this approach with large-scale mass spectrometry analysis of immunoprecipitations of 50 TFs. Our combined approach reveals large numbers of interesting TF-TF associations. We observe extensive change in TF colocalizations both within a cell type exposed to different conditions and across multiple cell types. We show distinct functional annotations and properties of different TF cobinding patterns and provide insights into the complex regulatory landscape of the cell. © 2013 Elsevier Inc.",,"trans acting factor; article; cell type; dynamics; gene control; gene locus; gene targeting; human; human cell; immunoprecipitation; mass spectrometry; priority journal; protein localization; Artificial Intelligence; Binding Sites; Cell Line; Chromatin Immunoprecipitation; Gene Regulatory Networks; Humans; Regulatory Sequences, Nucleic Acid; Sequence Analysis, DNA; Transcription Factors",Article,Scopus,2-s2.0-84886780296
"Zeeshan Zia M., Stark M., Schiele B., Schindler K.","Detailed 3D representations for object recognition and modeling",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",46,10.1109/TPAMI.2013.87,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884582487&doi=10.1109%2fTPAMI.2013.87&partnerID=40&md5=0be00d1f660be2458e3a0c0b38ea51e7","Geometric 3D reasoning at the level of objects has received renewed attention recently in the context of visual scene understanding. The level of geometric detail, however, is typically limited to qualitative representations or coarse boxes. This is linked to the fact that today's object class detectors are tuned toward robust 2D matching rather than accurate 3D geometry, encouraged by bounding-box-based benchmarks such as Pascal VOC. In this paper, we revisit ideas from the early days of computer vision, namely, detailed, 3D geometric object class representations for recognition. These representations can recover geometrically far more accurate object hypotheses than just bounding boxes, including continuous estimates of object pose and 3D wireframes with relative 3D positions of object parts. In combination with robust techniques for shape description and inference, we outperform state-of-the-art results in monocular 3D pose estimation. In a series of experiments, we analyze our approach in detail and demonstrate novel applications enabled by such an object class representation, such as fine-grained categorization of cars and bicycles, according to their 3D geometry, and ultrawide baseline matching. © 1979-2012 IEEE.","3D representation; recognition; scene understanding; single image 3D reconstruction; ultrawide baseline matching","3D reconstruction; 3d representations; Baseline matching; recognition; Scene understanding; Geometry; Object recognition; Three dimensional; algorithm; artificial intelligence; automated pattern recognition; computer assisted diagnosis; computer simulation; photography; procedures; theoretical model; three dimensional imaging; article; automated pattern recognition; computer assisted diagnosis; methodology; photography; three dimensional imaging; Algorithms; Artificial Intelligence; Computer Simulation; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Models, Theoretical; Pattern Recognition, Automated; Photography; Algorithms; Artificial Intelligence; Computer Simulation; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Models, Theoretical; Pattern Recognition, Automated; Photography",Article,Scopus,2-s2.0-84884582487
"Derbez P., Fouque P.-A., Jean J.","Improved key recovery attacks on reduced-round AES in the single-key setting",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",46,10.1007/978-3-642-38348-9_23,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883331254&doi=10.1007%2f978-3-642-38348-9_23&partnerID=40&md5=6575a1c8c1bf9ba1dd478e19264032e3","In this paper, we revisit meet-in-the-middle attacks on AES in the single-key model and improve on Dunkelman, Keller and Shamir attacks at Asiacrypt 2010. We present the best attack on 7 rounds of AES-128 where data/time/memory complexities are below 2100. Moreover, we are able to extend the number of rounds to reach attacks on 8 rounds for both AES-192 and AES-256. This gives the best attacks on those two versions with a data complexity of 2107 chosen-plaintexts, a memory complexity of 2 96 and a time complexity of 2172 for AES-192 and 2 196 for AES-256. Finally, we also describe the best attack on 9 rounds of AES-256 with 2120 chosen plaintexts and time and memory complexities of 2203. All these attacks have been found by carefully studying the number of reachable multisets in Dunkelman et al. attacks. © 2013 International Association for Cryptologic Research.",,"Chosen plaintexts; Data complexity; Key recovery attacks; Meet-in-the-middle attacks; Memory complexity; Multi-sets; Time complexity; Artificial intelligence; Computer science; Cryptography",Conference Paper,Scopus,2-s2.0-84883331254
"Ravichandran A., Chaudhry R., Vidal R.","Categorizing dynamic textures using a bag of dynamical systems",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",46,10.1109/TPAMI.2012.83,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871810104&doi=10.1109%2fTPAMI.2012.83&partnerID=40&md5=1cab2590daaa19f2870589ca1c2f8d7f","We consider the problem of categorizing video sequences of dynamic textures, i.e., nonrigid dynamical objects such as fire, water, steam, flags, etc. This problem is extremely challenging because the shape and appearance of a dynamic texture continuously change as a function of time. State-of-the-art dynamic texture categorization methods have been successful at classifying videos taken from the same viewpoint and scale by using a Linear Dynamical System (LDS) to model each video, and using distances or kernels in the space of LDSs to classify the videos. However, these methods perform poorly when the video sequences are taken under a different viewpoint or scale. In this paper, we propose a novel dynamic texture categorization framework that can handle such changes. We model each video sequence with a collection of LDSs, each one describing a small spatiotemporal patch extracted from the video. This Bag-of-Systems (BoS) representation is analogous to the Bag-of-Features (BoF) representation for object recognition, except that we use LDSs as feature descriptors. This choice poses several technical challenges in adopting the traditional BoF approach. Most notably, the space of LDSs is not euclidean; hence, novel methods for clustering LDSs and computing codewords of LDSs need to be developed. We propose a framework that makes use of nonlinear dimensionality reduction and clustering techniques combined with the Martin distance for LDSs to tackle these issues. Our experiments compare the proposed BoS approach to existing dynamic texture categorization methods and show that it can be used for recognizing dynamic textures in challenging scenarios which could not be handled by existing methods. © 2012 IEEE.","categorization; Dynamic textures; linear dynamical systems","Bag-of-Features; categorization; Categorization methods; Clustering techniques; Code-words; Dynamic textures; Dynamical objects; Euclidean; Feature descriptors; Function of time; Linear dynamical systems; Nonlinear dimensionality reduction; Novel methods; Technical challenges; Video sequences; Dynamical systems; Linear control systems; Object recognition; Video recording; Textures; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; image enhancement; methodology; reproducibility; sensitivity and specificity; three dimensional imaging; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84871810104
"Zhu S., Chen C., Li W., Yang B., Guan X.","Distributed optimal consensus filter for target tracking in heterogeneous sensor networks",2013,"IEEE Transactions on Cybernetics",45,10.1109/TSMCB.2012.2236647,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890072663&doi=10.1109%2fTSMCB.2012.2236647&partnerID=40&md5=bfc6d7f7a6f4bea16bdd6b5aaccafe1a","This paper is concerned with the problem of filter design for target tracking over sensor networks. Different from most existing works on sensor networks, we consider the heterogeneous sensor networks with two types of sensors different on processing abilities (denoted as type-I and type-II sensors, respectively). However, questions of how to deal with the heterogeneity of sensors and how to design a filter for target tracking over such kind of networks remain largely unexplored. We propose in this paper a novel distributed consensus filter to solve the target tracking problem. Two criteria, namely, unbiasedness and optimality, are imposed for the filter design. The so-called sequential design scheme is then presented to tackle the heterogeneity of sensors. The minimum principle of Pontryagin is adopted for type-I sensors to optimize the estimation errors. As for type-II sensors, the Lagrange multiplier method coupled with the generalized inverse of matrices is then used for filter optimization. Furthermore, it is proven that convergence property is guaranteed for the proposed consensus filter in the presence of process and measurement noise. Simulation results have validated the performance of the proposed filter. It is also demonstrated that the heterogeneous sensor networks with the proposed filter outperform the homogenous counterparts in light of reduction in the network cost, with slight degradation of estimation performance. © 2013 IEEE.","Heterogeneous sensor network; Optimal consensus filter; Target tracking; Unbiased estimate","Consensus filter; Convergence properties; Distributed consensus; Estimation performance; Generalized inverse; Heterogeneous sensor networks; Lagrange multiplier method; Unbiased estimates; Estimation; Filter banks; Lagrange multipliers; Optimization; Target tracking; Sensor networks; algorithm; article; artificial intelligence; automated pattern recognition; computer network; decision support system; methodology; signal processing; transducer; Algorithms; Artificial Intelligence; Computer Communication Networks; Decision Support Techniques; Pattern Recognition, Automated; Signal Processing, Computer-Assisted; Transducers",Article,Scopus,2-s2.0-84890072663
"Rozo L., Calinon S., Caldwell D., Jiménez P., Torras C.","Learning collaborative impedance-based robot behaviors",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",45,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893416519&partnerID=40&md5=c25909847a372ea0fb540eba20525490","Research in learning from demonstration has focused on transferring movements from humans to robots. However, a need is arising for robots that do not just replicate the task on their own, but that also interact with humans in a safe and natural way to accomplish tasks cooperatively. Robots with variable impedance capabilities opens the door to new challenging applications, where the learning algorithms must be extended by encapsulating force and vision information. In this paper we propose a framework to transfer impedancebased behaviors to a torque-controlled robot by kinesthetic teaching. The proposed model encodes the examples as a task-parameterized statistical dynamical system, where the robot impedance is shaped by estimating virtual stiffness matrices from the set of demonstrations. A collaborative assembly task is used as testbed. The results show that the model can be used to modify the robot impedance along task execution to facilitate the collaboration, by triggering stiff and compliant behaviors in an on-line manner to adapt to the user's actions. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Collaborative assembly; Compliant behavior; Kinesthetic teachings; Learning from demonstration; Statistical-dynamical; Variable impedance; Virtual stiffness; Vision information; Artificial intelligence; Dynamical systems; Stiffness matrix; Robots",Conference Paper,Scopus,2-s2.0-84893416519
"Thelwall M., Buckley K.","Topic-based sentiment analysis for the social web: The role of mood and issue-related words",2013,"Journal of the American Society for Information Science and Technology",45,10.1002/asi.22872,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880302858&doi=10.1002%2fasi.22872&partnerID=40&md5=1db92a0ef33c708d08a255905272e175","General sentiment analysis for the social web has become increasingly useful for shedding light on the role of emotion in online communication and offline events in both academic research and data journalism. Nevertheless, existing general-purpose social web sentiment analysis algorithms may not be optimal for texts focussed around specific topics. This article introduces 2 new methods, mood setting and lexicon extension, to improve the accuracy of topic-specific lexical sentiment strength detection for the social web. Mood setting allows the topic mood to determine the default polarity for ostensibly neutral expressive text. Topic-specific lexicon extension involves adding topic-specific words to the default general sentiment lexicon. Experiments with 8 data sets show that both methods can improve sentiment analysis performance in corpora and are recommended when the topic focus is tightest. © 2013 ASIS&T.",,"Academic research; Data journalisms; Offline; On-line communication; Sentiment analysis; Sentiment lexicons; Shedding light; Social webs; Artificial intelligence; Software engineering; Data mining",Article,Scopus,2-s2.0-84880302858
"Liao X., Zhou J., Ouyang S., Zhang R., Zhang Y.","An adaptive chaotic artificial bee colony algorithm for short-term hydrothermal generation scheduling",2013,"International Journal of Electrical Power and Energy Systems",45,10.1016/j.ijepes.2013.04.004,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877791076&doi=10.1016%2fj.ijepes.2013.04.004&partnerID=40&md5=a2d3ac420e807904e5b1901748f79134","Short-term hydrothermal scheduling (SHS) is a complicated nonlinear optimization problem with a set of constraints, which plays an important role in power system operations. In this paper, we propose to use an adaptive chaotic artificial bee colony (ACABC) algorithm to solve the SHS problem. In the proposed method, chaotic search is applied to help the artificial bee colony (ABC) algorithm to escape from a local optimum effectively. Furthermore, an adaptive coordinating mechanism of modification rate in employed bee phase is introduced to increase the ability of the algorithm to avoid premature convergence. Moreover, a new constraint handling method is combined with the ABC algorithm in order to solve the equality coupling constraints. We used a hydrothermal test system to demonstrate the effectiveness of the proposed method. The numerical results obtained by ACABC are compared with those obtained by the adaptive ABC algorithm (AABC), the chaotic ABC algorithm (CABC) and other methods mentioned in literature. The simulation results indicate that the proposed method outperforms those established optimization algorithms. © 2013 Elsevier Ltd. All rights reserved.","Adaptive; Artificial bee colony algorithm; Constrained optimization; Hydrothermal scheduling; Swarm intelligence","Adaptive; Artificial bee colonies; Artificial bee colony algorithms; Artificial bee colony algorithms (ABC); Hydro-thermal scheduling; Non-linear optimization problems; Short-term hydrothermal scheduling; Swarm Intelligence; Artificial intelligence; Computer simulation; Constrained optimization; Evolutionary algorithms; Scheduling; Solar buildings; Numerical methods",Article,Scopus,2-s2.0-84877791076
"Tiwari P., Kurhanewicz J., Madabhushi A.","Multi-kernel graph embedding for detection, Gleason grading of prostate cancer via MRI/MRS",2013,"Medical Image Analysis",45,10.1016/j.media.2012.10.004,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883872846&doi=10.1016%2fj.media.2012.10.004&partnerID=40&md5=e6cbe61903c1ca5b85dcc95ecd9455f9","Even though 1 in 6 men in the US, in their lifetime are expected to be diagnosed with prostate cancer (CaP), only 1 in 37 is expected to die on account of it. Consequently, among many men diagnosed with CaP, there has been a recent trend to resort to active surveillance (wait and watch) if diagnosed with a lower Gleason score on biopsy, as opposed to seeking immediate treatment. Some researchers have recently identified imaging markers for low and high grade CaP on multi-parametric (MP) magnetic resonance (MR) imaging (such as T2 weighted MR imaging (T2w MRI) and MR spectroscopy (MRS)). In this paper, we present a novel computerized decision support system (DSS), called Semi Supervised Multi Kernel Graph Embedding (SeSMiK-GE), that quantitatively combines structural, and metabolic imaging data for distinguishing (a) benign versus cancerous, and (b) high- versus low-Gleason grade CaP regions from in vivo MP-MRI. A total of 29 1.5. Tesla endorectal pre-operative in vivo MP MRI (T2w MRI, MRS) studies from patients undergoing radical prostatectomy were considered in this study. Ground truth for evaluation of the SeSMiK-GE classifier was obtained via annotation of disease extent on the pre-operative imaging by visually correlating the MRI to the ex vivo whole mount histologic specimens. The SeSMiK-GE framework comprises of three main modules: (1) multi-kernel learning, (2) semi-supervised learning, and (3) dimensionality reduction, which are leveraged for the construction of an integrated low dimensional representation of the different imaging and non-imaging MRI protocols. Hierarchical classifiers for diagnosis and Gleason grading of CaP are then constructed within this unified low dimensional representation. Step 1 of the hierarchical classifier employs a random forest classifier in conjunction with the SeSMiK-GE based data representation and a probabilistic pairwise Markov Random Field algorithm (which allows for imposition of local spatial constraints) to yield a voxel based classification of CaP presence. The CaP region of interest identified in Step 1 is then subsequently classified as either high or low Gleason grade CaP in Step 2. Comparing SeSMiK-GE with unimodal T2w MRI, MRS classifiers and a commonly used feature concatenation (COD) strategy, yielded areas (AUC) under the receiver operative curve (ROC) of (a) 0.89. ± 0.09 (SeSMiK), 0.54. ± 0.18 (T2w MRI), 0.61. ± 0.20 (MRS), and 0.64. ± 0.23 (COD) for distinguishing benign from CaP regions, and (b) 0.84. ± 0.07 (SeSMiK),0.54. ± 0.13 (MRI), 0.59. ± 0.19 (MRS), and 0.62. ±0.18 (COD) for distinguishing high and low grade CaP using a leave one out cross-validation strategy, all evaluations being performed on a per voxel basis. Our results suggest that following further rigorous validation, SeSMiK-GE could be developed into a powerful diagnostic and prognostic tool for detection and grading of CaP in vivo and in helping to determine the appropriate treatment option. Identifying low grade disease in vivo might allow CaP patients to opt for active surveillance rather than immediately opt for aggressive therapy such as radical prostatectomy. © 2012 Elsevier B.V.","Data integration; Grading; Graph embedding; Prostate cancer; Semi-supervised","Dimensionality reduction; Graph embeddings; Hierarchical classifiers; Low-dimensional representation; Pair-wise markov random fields; Prostate cancers; Random forest classifier; Semi-supervised; Artificial intelligence; Data integration; Decision support systems; Decision trees; Diagnosis; Disease control; Diseases; Grading; Image segmentation; Magnetic resonance; Magnetic resonance spectroscopy; Markov processes; Patient treatment; Supervised learning; Urology; Magnetic resonance imaging; tumor marker; tumor marker; area under the curve; article; benign tumor; cancer classification; cancer diagnosis; cancer grading; cancer patient; cancer surgery; classification algorithm; clinical assessment tool; clinical evaluation; computer aided design; decision support system; diagnostic test accuracy study; disease severity; ex vivo study; Gleason score; histopathology; human; image analysis; in vivo study; kernel method; male; markov random field algorithm; nuclear magnetic resonance imaging; nuclear magnetic resonance spectroscopy; oncological procedure; preoperative period; priority journal; process optimization; prostate cancer; prostatectomy; random forest; receiver operating characteristic; semi supervised multi kernel graph embedding; sensitivity and specificity; validation process; algorithm; computer assisted diagnosis; metabolism; methodology; nuclear magnetic resonance imaging; nuclear magnetic resonance spectroscopy; prostate tumor; reproducibility; Algorithms; Diagnosis, Computer-Assisted; Humans; Magnetic Resonance Imaging; Magnetic Resonance Spectroscopy; Male; Prostatic Neoplasms; Reproducibility of Results; Sensitivity and Specificity; Tumor Markers, Biological",Article,Scopus,2-s2.0-84883872846
"Wang X., Cheng H., Huang M.","Multi-robot navigation based QoS routing in self-organizing networks",2013,"Engineering Applications of Artificial Intelligence",45,10.1016/j.engappai.2012.01.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870058164&doi=10.1016%2fj.engappai.2012.01.008&partnerID=40&md5=24aba4a33016c71c13dc5a3a08d62934","The technical development drives the future networks to become large-scale, heterogeneous, and dynamic. Bio-inspired networking can help reduce the time-space complexity of the complex network. Due to the good features such as self-organization and self-management, self-organizing network (SON) will most probably be a priority choice for the next generation network. In this paper, a swarm intelligence based Quality of Service (QoS) routing protocol is proposed for SON. The inaccurate routing and QoS information is described with fuzzy mathematics whilst the utilities of both the user and the network service provider are considered by applying game theory. Based on the multi-robot navigation algorithm, the protocol is able to search a routing path which can satisfy the user QoS requirements and achieve the Pareto optimal utilities of the user and the network service provider under Nash equilibrium. The proposed protocol is implemented and evaluated by extensive simulation experiments. The results show that it beats both other swarm intelligence based routing protocols and the traditional Dijkstra algorithm based routing protocol. The searched routing paths support the win-win effect for both the user and the network service provider. © 2012 Elsevier Ltd. All rights reserved.","Multi-robot navigation; Quality of service; Routing; Self-organizing network","Bio-inspired networking; Complex networks; Dijkstra algorithms; Extensive simulations; Future networks; Fuzzy mathematics; Multi-robot navigation; Nash equilibria; Network service providers; Pareto-optimal; QoS requirements; QoS routing; Quality of Service routing; Routing; Routing path; Self management; Self-organizing network; Swarm Intelligence; Technical development; Time-space complexity; Win-win; Algorithms; Artificial intelligence; Game theory; Industrial robots; Mobile telecommunication systems; Multipurpose robots; Routing protocols; Telecommunication networks; Quality of service",Article,Scopus,2-s2.0-84870058164
"Cecilia J.M., García J.M., Nisbet A., Amos M., Ujaldón M.","Enhancing data parallelism for ant colony optimization on GPUs",2013,"Journal of Parallel and Distributed Computing",45,10.1016/j.jpdc.2012.01.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869488080&doi=10.1016%2fj.jpdc.2012.01.002&partnerID=40&md5=7691fa7cf7e983b8bfdca5df593d2bce","Graphics Processing Units (GPUs) have evolved into highly parallel and fully programmable architecture over the past five years, and the advent of CUDA has facilitated their application to many real-world applications. In this paper, we deal with a GPU implementation of Ant Colony Optimization (ACO), a population-based optimization method which comprises two major stages: tour construction and pheromone update. Because of its inherently parallel nature, ACO is well-suited to GPU implementation, but it also poses significant challenges due to irregular memory access patterns. Our contribution within this context is threefold: (1) a data parallelism scheme for tour construction tailored to GPUs, (2) novel GPU programming strategies for the pheromone update stage, and (3) a new mechanism called I-Roulette to replicate the classic roulette wheel while improving GPU parallelism. Our implementation leads to factor gains exceeding 20x for any of the two stages of the ACO algorithm as applied to the TSP when compared to its sequential counterpart version running on a similar single-threaded high-end CPU. Moreover, an extensive discussion focused on different implementation paths on GPUs shows the way to deal with parallel graph connected components. This, in turn, suggests a broader area of inquiry, where algorithm designers may learn to adapt similar optimization methods to GPU architecture. © 2012 Elsevier Inc. All rights reserved.","Ant Colony Optimization; GPU programming; Metaheuristics; Performance analysis; TSP","Ant Colony Optimization (ACO); GPU programming; Meta heuristics; Performance analysis; TSP; Artificial intelligence; Computer graphics; Program processors; Algorithms",Article,Scopus,2-s2.0-84869488080
"Brink S., Nease S., Hasler P., Ramakrishnan S., Wunderlich R., Basu A., Degnan B.","A learning-enabled neuron array IC based upon transistor channel models of biological phenomena",2013,"IEEE Transactions on Biomedical Circuits and Systems",45,10.1109/TBCAS.2012.2197858,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875055083&doi=10.1109%2fTBCAS.2012.2197858&partnerID=40&md5=89bc3540e6417dd9b6aa56c713109e0d","We present a single-chip array of 100 biologically-based electronic neuron models interconnected to each other and the outside environment through 30,000 synapses. The chip was fabricated in a standard 350 nm CMOS IC process. Our approach used dense circuit models of synaptic behavior, including biological computation and learning, as well as transistor channel models. We use Address-Event Representation (AER) spike communication for inputs and outputs to this IC. We present the IC architecture and infrastructure, including IC chip, configuration tools, and testing platform. We present measurement of small network of neurons, measurement of STDP neuron dynamics, and measurement from a compiled spiking neuron WTA topology, all compiled into this IC. © 2012 IEEE.","Electrical implementation of neurobiology; neuromorphic engineering","Address-event representation; Biological computations; Biological phenomena; Electrical implementation of neurobiology; Electronic neurons; Neuromorphic engineering; Testing platforms; Transistor channels; Learning algorithms; Neural networks; Neurons; article; artificial intelligence; artificial neural network; nerve cell; semiconductor; synapse; Artificial Intelligence; Neural Networks (Computer); Neurons; Synapses; Transistors, Electronic",Article,Scopus,2-s2.0-84875055083
"Akpinar S., Mirac Bayhan G., Baykasoglu A.","Hybridizing ant colony optimization via genetic algorithm for mixed-model assembly line balancing problem with sequence dependent setup times between tasks",2013,"Applied Soft Computing Journal",45,10.1016/j.asoc.2012.07.024,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869443174&doi=10.1016%2fj.asoc.2012.07.024&partnerID=40&md5=b13919675ebbe7bd2ea1206c356e83b4","This paper presents a new hybrid algorithm, which executes ant colony optimization in combination with genetic algorithm (ACO-GA), for type I mixed-model assembly line balancing problem (MMALBP-I) with some particular features of real world problems such as parallel workstations, zoning constraints and sequence dependent setup times between tasks. The proposed ACO-GA algorithm aims at enhancing the performance of ant colony optimization by incorporating genetic algorithm as a local search strategy for MMALBP-I with setups. In the proposed hybrid algorithm ACO is conducted to provide diversification, while GA is conducted to provide intensification. The proposed algorithm is tested on 20 representatives MMALBP-I extended by adding low, medium and high variability of setup times. The results are compared with pure ACO pure GA and hGA in terms of solution quality and computational times. Computational results indicate that the proposed ACO-GA algorithm has superior performance. © 2012 Elsevier B.V.","Any colony optimization; Genetic algorithm; Hybrid meta-heuristics; Mixed-model assembly line balancing; Sequence dependent set-up times","Ant Colony Optimization (ACO); Colony optimization; Computational results; Computational time; High variability; Hybrid algorithms; Hybrid metaheuristics; Local search strategy; Mixed-model assembly lines; Real-world problem; Sequence-dependent setup time; Set-up time; Solution quality; Artificial intelligence; Assembly; Genetic algorithms",Article,Scopus,2-s2.0-84869443174
"Delévacq A., Delisle P., Gravel M., Krajecki M.","Parallel ant colony optimization on graphics processing units",2013,"Journal of Parallel and Distributed Computing",45,10.1016/j.jpdc.2012.01.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869495318&doi=10.1016%2fj.jpdc.2012.01.003&partnerID=40&md5=a600384f63ecf9ff144a25156671092f","The purpose of this paper is to propose effective parallelization strategies for the Ant Colony Optimization (ACO) metaheuristic on Graphics Processing Units (GPUs). The Max-Min Ant System (MMAS) algorithm augmented with 3-opt local search is used as a framework for the implementation of the parallel ants and multiple ant colonies general parallelization approaches. The four resulting GPU algorithms are extensively evaluated and compared on both speedup and solution quality on a state-of-the-art Fermi GPU architecture. A rigorous effort is made to keep parallel algorithms true to the original MMAS applied to the Traveling Salesman Problem. We report speedups of up to 23.60 with solution quality similar to the original sequential implementation. With the intent of providing a parallelization framework for ACO on GPUs, a comparative experimental study highlights the performance impact of ACO parameters, GPU technical configuration, memory structures and parallelization granularity. © 2012 Elsevier Inc. All rights reserved.","Ant colony optimization; CUDA; GPU; MMAS; Multiple colonies; Parallel ants; Parallel metaheuristics","Ant Colony Optimization (ACO); CUDA; GPU; MMAS; Multiple colonies; Parallel ants; Parallel metaheuristics; Artificial intelligence; Computer graphics; Computer graphics equipment; Program processors; Traveling salesman problem; Algorithms",Article,Scopus,2-s2.0-84869495318
"Bienvenu M., Rosati R.","Tractable approximations of consistent query answering for robust ontology-based data access",2013,"IJCAI International Joint Conference on Artificial Intelligence",44,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896064277&partnerID=40&md5=66d364a372de58d64b86cfa8b91b4ef6","A robust system for ontology-based data access should provide meaningful answers to queries even when the data conflicts with the ontology. This can be accomplished by adopting an in-consistency-tolerant semantics, with the consistent query answering (CQA) semantics being the most prominent example. Unfortunately, query answering under the CQA semantics has been shown to be computationally intractable, even when extremely simple ontology languages are considered. In this paper, we address this problem by proposing two new families of inconsistency-tolerant semantics which approximate the CQA semantics from above and from below and converge to it in the limit. We study the data complexity of conjunctive query answering under these new semantics, and show a general tractability result for all known first-order rewritable ontology languages. We also analyze the combined complexity of query answering for ontology languages of the DL-Lite family.",,"Combined complexity; Conjunctive queries; Consistent query answering; Data complexity; Ontology language; Ontology-based data access; Query answering; Robust systems; Artificial intelligence; Semantics; Ontology",Conference Paper,Scopus,2-s2.0-84896064277
"Zhao Y., Li B., Qin J., Gao H., Karimi H.R.","H∞ consensus and synchronization of nonlinear systems based on a novel fuzzy model",2013,"IEEE Transactions on Cybernetics",44,10.1109/TCYB.2013.2242197,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890093287&doi=10.1109%2fTCYB.2013.2242197&partnerID=40&md5=2936cbe0c2c971e48576f3a3351b5a42","This paper investigates the H∞ consensus control problem of nonlinear multiagent systems under an arbitrary topological structure. A novel Takagi-Sukeno (T-S) fuzzy modeling method is proposed to describe the problem of nonlinear follower agents approaching a time-varying leader, i.e., the error dynamics between the follower agents and the leader, whose dynamics is evolving according to an isolated unforced nonlinear agent model, is described as a set of T-S fuzzy models. Based on the model, a leader-following consensus algorithm is designed so that, under an arbitrary network topology, all the follower agents reach consensus with the leader subject to external disturbances, preserving a guaranteed H∞ performance level. In addition, we obtain a sufficient condition for choosing the pinned nodes to make the entire multiagent network reach consensus. Moreover, the fuzzy modeling method is extended to solve the synchronization problem of nonlinear systems, and a fuzzy H∞ controller is designed so that two nonlinear systems reach synchronization with a prescribed H∞ performance level. The controller design procedure is greatly simplified by utilization of the proposed fuzzy modeling method. Finally, numerical simulations on chaotic systems and arbitrary nonlinear functions are provided to illustrate the effectiveness of the obtained theoretical results. © 2013 IEEE.","H∞ consensus; Nonlinear multiagent systems; Synchronization; Takagi-Sukeno (T-S) fuzzy models","Consensus algorithms; External disturbances; Fuzzy modeling method; Fuzzy models; Multiagent networks; Nonlinear multi-agent systems; Synchronization problem; Topological structure; Chaotic systems; Electric network topology; Multi agent systems; Nonlinear systems; Synchronization; algorithm; article; artificial intelligence; automated pattern recognition; computer simulation; decision support system; feedback system; fuzzy logic; methodology; nonlinear system; theoretical model; Algorithms; Artificial Intelligence; Computer Simulation; Decision Support Techniques; Feedback; Fuzzy Logic; Models, Theoretical; Nonlinear Dynamics; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84890093287
"Mensink T., Verbeek J., Perronnin F., Csurka G.","Distance-based image classification: Generalizing to new classes at near-zero cost",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",44,10.1109/TPAMI.2013.83,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884582871&doi=10.1109%2fTPAMI.2013.83&partnerID=40&md5=7d9381329b3d12fe246b784070c7bed4","We study large-scale image classification methods that can incorporate new classes and training images continuously over time at negligible cost. To this end, we consider two distance-based classifiers, the k-nearest neighbor (k-NN) and nearest class mean (NCM) classifiers, and introduce a new metric learning approach for the latter. We also introduce an extension of the NCM classifier to allow for richer class representations. Experiments on the ImageNet 2010 challenge dataset, which contains over $(106)$ training images of 1,000 classes, show that, surprisingly, the NCM classifier compares favorably to the more flexible k-NN classifier. Moreover, the NCM performance is comparable to that of linear SVMs which obtain current state-of-the-art performance. Experimentally, we study the generalization performance to classes that were not used to learn the metrics. Using a metric learned on 1,000 classes, we show results for the ImageNet-10K dataset which contains 10,000 classes, and obtain performance that is competitive with the current state-of-the-art while being orders of magnitude faster. Furthermore, we show how a zero-shot class prior based on the ImageNet hierarchy can improve performance when few training images are available. © 1979-2012 IEEE.","image retrieval; k-nearest neighbors classification; large scale image classification; Metric learning; nearest class mean classification; transfer learning; zero-shot learning","Class mean; K-nearest neighbors; Metric learning; Transfer learning; zero-shot learning; Image retrieval; Membership functions; Text processing; Image classification; algorithm; artificial intelligence; automated pattern recognition; computer assisted diagnosis; computer simulation; image enhancement; procedures; theoretical model; article; automated pattern recognition; computer assisted diagnosis; image enhancement; methodology; Algorithms; Artificial Intelligence; Computer Simulation; Image Enhancement; Image Interpretation, Computer-Assisted; Models, Theoretical; Pattern Recognition, Automated; Algorithms; Artificial Intelligence; Computer Simulation; Image Enhancement; Image Interpretation, Computer-Assisted; Models, Theoretical; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84884582871
"Boneh D., Raghunathan A., Segev G.","Function-private identity-based encryption: Hiding the function in functional encryption",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",44,10.1007/978-3-642-40084-1_26,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884473483&doi=10.1007%2f978-3-642-40084-1_26&partnerID=40&md5=1fadc2259ef8844b37b5a084870d447c","We put forward a new notion, function privacy, in identity-based encryption and, more generally, in functional encryption. Intuitively, our notion asks that decryption keys reveal essentially no information on their corresponding identities, beyond the absolute minimum necessary. This is motivated by the need for providing predicate privacy in public-key searchable encryption. Formalizing such a notion, however, is not straightforward as given a decryption key it is always possible to learn some information on its corresponding identity by testing whether it correctly decrypts ciphertexts that are encrypted for specific identities. In light of such an inherent difficulty, any meaningful notion of function privacy must be based on the minimal assumption that, from the adversary's point of view, identities that correspond to its given decryption keys are sampled from somewhat unpredictable distributions. We show that this assumption is in fact sufficient for obtaining a strong and realistic notion of function privacy. Loosely speaking, our framework requires that a decryption key corresponding to an identity sampled from any sufficiently unpredictable distribution is indistinguishable from a decryption key corresponding to an independently and uniformly sampled identity. Within our framework we develop an approach for designing function-private identity-based encryption schemes, leading to constructions that are based on standard assumptions in bilinear groups (DBDH, DLIN) and lattices (LWE). In addition to function privacy, our schemes are also anonymous, and thus yield the first public-key searchable encryption schemes that are provably keyword private: A search key skw enables to identify encryptions of an underlying keyword w, while not revealing any additional information about w beyond the minimum necessary, as long as the keyword w is sufficiently unpredictable. © 2013 International Association for Cryptologic Research.",,"Absolute minimum; Bilinear groups; Decryption keys; Functional encryptions; Identity Based Encryption; Search keys; Searchable encryptions; Standard assumptions; Artificial intelligence; Computer science; Cryptography",Conference Paper,Scopus,2-s2.0-84884473483
"Łapa K., Przybył A., Cpałka K.","A new approach to designing interpretable models of dynamic systems",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",44,10.1007/978-3-642-38610-7_48,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884376016&doi=10.1007%2f978-3-642-38610-7_48&partnerID=40&md5=b0598870078935bda452d819ed864006","In the process of designing automatic control system it is very important to have an accurate model of the controlled process. Approaches to modelling dynamic systems presented in the literature are often approximate, uninterpretable (acting as a black box), not appropriate to work in real-time, so it is not possible to create a hardware emulator on the basis of these approaches. The paper presents a new method to create model of nonlinear dynamic systems which gives a real opportunity for the interpretation of accumulated knowledge. By combining methods of control theory with fuzzy logic rules a good accuracy of the model can be achieved with use of a small number of fuzzy rules. Our method is based on the evolutionary strategy (μ λ). © 2013 Springer-Verlag.",,"Accurate modeling; Black boxes; Combining method; Controlled process; Evolutionary strategies; Fuzzy logic rules; Hardware emulators; New approaches; Artificial intelligence; Evolutionary algorithms; Fuzzy logic; Nonlinear dynamical systems; Soft computing; Dynamical systems",Conference Paper,Scopus,2-s2.0-84884376016
"Vermaas P.E.","The coexistence of engineering meanings of function: Four responses and their methodological implications",2013,"Artificial Intelligence for Engineering Design, Analysis and Manufacturing: AIEDAM",44,10.1017/S0890060413000206,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878425518&doi=10.1017%2fS0890060413000206&partnerID=40&md5=95cf1ddf8520f6b6cf5f532283aaae4c","In this position paper, the ambiguity of functional descriptions in engineering is considered from a methodological point of view. Four responses to this ambiguity are discussed, ranging from defining a single meaning of function and rejecting the different meanings that are currently used in engineering to accepting these meanings as coexisting in engineering and taking function as a family resemblance concept. Rejecting the different meanings is described as the straightforward response to resolving the ambiguity of functional descriptions, yet in engineering research and design methodology it rather seems to be accepted that engineers do use the coexisting meanings side by side. In this paper, explanations are given of why this practice is beneficial to engineering. Then it is explored how the particular meaning that engineers attach to function depends on the tasks for which functional descriptions are used. Finally, the methodological implications of the four responses to the ambiguity of functional descriptions are discussed. Copyright © Cambridge University Press 2013.","Ambiguity of functional descriptions; Conceptual analysis; Engineering design methods; Functions","Ambiguity of functional descriptions; Conceptual analysis; Design Methodology; Engineering design methods; Family resemblance; Point of views; Position papers; Side by sides; Artificial intelligence; Functions; Industrial engineering; Engineering",Article,Scopus,2-s2.0-84878425518
"Horev-Azaria L., Baldi G., Beno D., Bonacchi D., Golla-Schindler U., Kirkpatrick J.C., Kolle S., Landsiedel R., Maimon O., Marche P.N., Ponti J., Romano R., Rossi F., Sommer D., Uboldi C., Unger R.E., Villiers C., Korenstein R.","Predictive Toxicology of cobalt ferrite nanoparticles: Comparative in-vitro study of different cellular models using methods of knowledge discovery from data",2013,"Particle and Fibre Toxicology",44,10.1186/1743-8977-10-32,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880914511&doi=10.1186%2f1743-8977-10-32&partnerID=40&md5=0dafef700cd5d7ca078fac0397e94c24","Background: Cobalt-ferrite nanoparticles (Co-Fe NPs) are attractive for nanotechnology-based therapies. Thus, exploring their effect on viability of seven different cell lines representing different organs of the human body is highly important.Methods: The toxicological effects of Co-Fe NPs were studied by in-vitro exposure of A549 and NCIH441 cell-lines (lung), precision-cut lung slices from rat, HepG2 cell-line (liver), MDCK cell-line (kidney), Caco-2 TC7 cell-line (intestine), TK6 (lymphoblasts) and primary mouse dendritic-cells. Toxicity was examined following exposure to Co-Fe NPs in the concentration range of 0.05 -1.2 mM for 24 and 72 h, using Alamar blue, MTT and neutral red assays. Changes in oxidative stress were determined by a dichlorodihydrofluorescein diacetate based assay. Data analysis and predictive modeling of the obtained data sets were executed by employing methods of Knowledge Discovery from Data with emphasis on a decision tree model (J48).Results: Different dose-response curves of cell viability were obtained for each of the seven cell lines upon exposure to Co-Fe NPs. Increase of oxidative stress was induced by Co-Fe NPs and found to be dependent on the cell type. A high linear correlation (R2=0.97) was found between the toxicity of Co-Fe NPs and the extent of ROS generation following their exposure to Co-Fe NPs. The algorithm we applied to model the observed toxicity belongs to a type of supervised classifier. The decision tree model yielded the following order with decrease of the ranking parameter: NP concentrations (as the most influencing parameter), cell type (possessing the following hierarchy of cell sensitivity towards viability decrease: TK6 &gt; Lung slices &gt; NCIH441 &gt; Caco-2 = MDCK &gt; A549 &gt; HepG2 = Dendritic) and time of exposure, where the highest-ranking parameter (NP concentration) provides the highest information gain with respect to toxicity. The validity of the chosen decision tree model J48 was established by yielding a higher accuracy than that of the well-known "" naive bayes"" classifier.Conclusions: The observed correlation between the oxidative stress, caused by the presence of the Co-Fe NPs, with the hierarchy of sensitivity of the different cell types towards toxicity, suggests that oxidative stress is one possible mechanism for the toxicity of Co-Fe NPs. © 2013 Horev-Azaria et al.; licensee BioMed Central Ltd.","Cobalt-ferrite nanoparticles; Comparative cytotoxicity; Data mining; Nanotoxicology","3 (4,5 dimethyl 2 thiazolyl) 2,5 diphenyltetrazolium bromide; Alamar Blue; cobalt ferrite nanoparticle; dichlorodihydrofluorescein diacetate; dye; metal nanoparticle; neutral red; unclassified drug; accuracy; animal cell; animal tissue; article; cancer cell culture; cell differentiation; cell strain A549; cell strain CACO 2; cell strain HepG2; cell strain NCIH441; cell strain tk6; cell type; cell viability; comparative study; concentration response; controlled study; correlation analysis; cytotoxicity; data analysis; dendritic cell; dose response; human; human cell; human tissue; in vitro study; intestine; kidney; knowledge discovery; leaching; lung; MDCK cell; mouse; nanotoxicology; nonhuman; oxidative stress; prediction; priority journal; rat; sensitivity analysis; time; Algorithms; Animals; Artificial Intelligence; Caco-2 Cells; Cell Survival; Cobalt; Data Mining; Decision Support Techniques; Decision Trees; Dogs; Dose-Response Relationship, Drug; Ferric Compounds; Hep G2 Cells; Humans; Linear Models; Madin Darby Canine Kidney Cells; Metal Nanoparticles; Mice; Oxidative Stress; Primary Cell Culture; Rats; Reactive Oxygen Species; Time Factors; Tissue Culture Techniques; Toxicology; Rattus",Article,Scopus,2-s2.0-84880914511
"Tosato D., Spera M., Cristani M., Murino V.","Characterizing humans on riemannian manifolds",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",44,10.1109/TPAMI.2012.263,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879877489&doi=10.1109%2fTPAMI.2012.263&partnerID=40&md5=70c0334304de11d816bbd5090f72fdb0","In surveillance applications, head and body orientation of people is of primary importance for assessing many behavioral traits. Unfortunately, in this context people are often encoded by a few, noisy pixels so that their characterization is difficult. We face this issue, proposing a computational framework which is based on an expressive descriptor, the covariance of features. Covariances have been employed for pedestrian detection purposes, actually a binary classification problem on Riemannian manifolds. In this paper, we show how to extend to the multiclassification case, presenting a novel descriptor, named weighted array of covariances, especially suited for dealing with tiny image representations. The extension requires a novel differential geometry approach in which covariances are projected on a unique tangent space where standard machine learning techniques can be applied. In particular, we adopt the Campbell-Baker-Hausdorff expansion as a means to approximate on the tangent space the genuine (geodesic) distances on the manifold in a very efficient way. We test our methodology on multiple benchmark datasets, and also propose new testing sets, getting convincing results in all the cases. © 1979-2012 IEEE.","covariance descriptors; Pedestrian characterization; Riemannian manifolds","Binary classification problems; Computational framework; Descriptors; Differential geometry approach; Image representations; Pedestrian detection; Riemannian manifold; Surveillance applications; Learning systems; Geometry; article; artificial intelligence; automated pattern recognition; human; image processing; methodology; automated pattern recognition; image processing; procedures; Artificial Intelligence; Humans; Image Processing, Computer-Assisted; Pattern Recognition, Automated; Artificial Intelligence; Humans; Image Processing, Computer-Assisted; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84879877489
"Wei Q., Dunbrack Jr. R.L.","The Role of Balanced Training and Testing Data Sets for Binary Classifiers in Bioinformatics",2013,"PLoS ONE",44,10.1371/journal.pone.0067863,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879987629&doi=10.1371%2fjournal.pone.0067863&partnerID=40&md5=9965902a2bbe2989553ddf028eb018fa","Training and testing of conventional machine learning models on binary classification problems depend on the proportions of the two outcomes in the relevant data sets. This may be especially important in practical terms when real-world applications of the classifier are either highly imbalanced or occur in unknown proportions. Intuitively, it may seem sensible to train machine learning models on data similar to the target data in terms of proportions of the two binary outcomes. However, we show that this is not the case using the example of prediction of deleterious and neutral phenotypes of human missense mutations in human genome data, for which the proportion of the binary outcome is unknown. Our results indicate that using balanced training data (50% neutral and 50% deleterious) results in the highest balanced accuracy (the average of True Positive Rate and True Negative Rate), Matthews correlation coefficient, and area under ROC curves, no matter what the proportions of the two phenotypes are in the testing data. Besides balancing the data by undersampling the majority class, other techniques in machine learning include oversampling the minority class, interpolating minority-class data points and various penalties for misclassifying the minority class. However, these techniques are not commonly used in either the missense phenotype prediction problem or in the prediction of disordered residues in proteins, where the imbalance problem is substantial. The appropriate approach depends on the amount of available data and the specific problem at hand. © 2013 Wei, Dunbrack.",,"amino acid; amino acid sequence; area under the curve; article; balanced training; bioinformatics; contingency table; gene mutation; human; human genome; machine learning; missense mutation; phenotype; receiver operating characteristic; training; Algorithms; Animals; Artificial Intelligence; Computational Biology; Databases, Genetic; Genetic Association Studies; Humans; Models, Biological; Mutation, Missense; Phenotype; Polymorphism, Genetic; Reproducibility of Results",Article,Scopus,2-s2.0-84879987629
"Wang Z., Xu J.","Predicting protein contact map using evolutionary and physical constraints by integer programming",2013,"Bioinformatics",44,10.1093/bioinformatics/btt211,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879976691&doi=10.1093%2fbioinformatics%2fbtt211&partnerID=40&md5=d0399e052a9ee484113a92a1464334bf","Motivation: Protein contact map describes the pairwise spatial and functional relationship of residues in a protein and contains key information for protein 3D structure prediction. Although studied extensively, it remains challenging to predict contact map using only sequence information. Most existing methods predict the contact map matrix element-by-element, ignoring correlation among contacts and physical feasibility of the whole-contact map. A couple of recent methods predict contact map by using mutual information, taking into consideration contact correlation and enforcing a sparsity restraint, but these methods demand for a very large number of sequence homologs for the protein under consideration and the resultant contact map may be still physically infeasible.Results: This article presents a novel method PhyCMAP for contact map prediction, integrating both evolutionary and physical restraints by machine learning and integer linear programming. The evolutionary restraints are much more informative than mutual information, and the physical restraints specify more concrete relationship among contacts than the sparsity restraint. As such, our method greatly reduces the solution space of the contact map matrix and, thus, significantly improves prediction accuracy. Experimental results confirm that PhyCMAP outperforms currently popular methods no matter how many sequence homologs are available for the protein under consideration. © The Author 2013.",,"protein; protein; artificial intelligence; chemistry; molecular evolution; protein secondary structure; sequence homology; system analysis; article; chemistry; Artificial Intelligence; Evolution, Molecular; Programming, Linear; Protein Structure, Secondary; Proteins; Sequence Homology, Amino Acid; Artificial Intelligence; Evolution, Molecular; Programming, Linear; Protein Structure, Secondary; Proteins; Sequence Homology, Amino Acid",Conference Paper,Scopus,2-s2.0-84879976691
"van Laarhoven T., Marchiori E.","Predicting Drug-Target Interactions for New Drug Compounds Using a Weighted Nearest Neighbor Profile",2013,"PLoS ONE",44,10.1371/journal.pone.0066952,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879466418&doi=10.1371%2fjournal.pone.0066952&partnerID=40&md5=1e373dc820bb65d8cfdeec33f0a8474d","In silico discovery of interactions between drug compounds and target proteins is of core importance for improving the efficiency of the laborious and costly experimental determination of drug-target interaction. Drug-target interaction data are available for many classes of pharmaceutically useful target proteins including enzymes, ion channels, GPCRs and nuclear receptors. However, current drug-target interaction databases contain a small number of drug-target pairs which are experimentally validated interactions. In particular, for some drug compounds (or targets) there is no available interaction. This motivates the need for developing methods that predict interacting pairs with high accuracy also for these 'new' drug compounds (or targets). We show that a simple weighted nearest neighbor procedure is highly effective for this task. We integrate this procedure into a recent machine learning method for drug-target interaction we developed in previous work. Results of experiments indicate that the resulting method predicts true interactions with high accuracy also for new drug compounds and achieves results comparable or better than those of recent state-of-the-art algorithms. Software is publicly available at http://cs.ru.nl/~tvanlaarhoven/drugtarget2013. © 2013 van Laarhoven, Marchiori.",,"aminoglutethimide; benzocaine; carvedilol; clonidine; clozapine; eglumetad; etretinate; halothane; isotretinoin; methoxsalen; metoclopramide; metoprolol; nicotine; nifedipine; nimodipine; norethisterone; proxymetacaine; tazarotene; drug; protein; protein binding; accuracy; algorithm; article; drug interaction; drug target interaction; drug targeting; intermethod comparison; machine learning; prediction; protein targeting; statistical analysis; weighted nearest neighbor; artificial intelligence; biology; drug development; metabolism; molecularly targeted therapy; procedures; Artificial Intelligence; Computational Biology; Drug Discovery; Molecular Targeted Therapy; Pharmaceutical Preparations; Protein Binding; Proteins",Article,Scopus,2-s2.0-84879466418
"Decherchi S., Gastaldo P., Zunino R., Cambria E., Redi J.","Circular-ELM for the reduced-reference assessment of perceived image quality",2013,"Neurocomputing",44,10.1016/j.neucom.2011.12.050,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870245922&doi=10.1016%2fj.neucom.2011.12.050&partnerID=40&md5=552a82cfcbec150563e72a7e15675ab6","Providing a satisfactory visual experience is one of the main goals for present-day electronic multimedia devices. All the enabling technologies for storage, transmission, compression, rendering should preserve, and possibly enhance, the quality of the video signal; to do so, quality control mechanisms are required. These mechanisms rely on systems that can assess the visual quality of the incoming signal consistently with human perception. Computational Intelligence (CI) paradigms represent a suitable technology to tackle this challenging problem. The present research introduces an augmented version of the basic Extreme Learning Machine (ELM), the Circular-ELM (C-ELM), which proves effective in addressing the visual quality assessment problem. The C-ELM model derives from the original Circular BackPropagation (CBP) architecture, in which the input vector of a conventional MultiLayer Perceptron (MLP) is augmented by one additional dimension, the circular input; this paper shows that C-ELM can actually benefit from the enhancement provided by the circular input without losing any of the fruitful properties that characterize the basic ELM framework. In the proposed framework, C-ELM handles the actual mapping of visual signals into quality scores, successfully reproducing perceptual mechanisms. Its effectiveness is proved on recognized benchmarks and for four different types of distortions. © 2012 Elsevier B.V.","Circular backpropagation; Extreme learning machine; Image quality assessment","Circular backpropagation; Control mechanism; Electronic multimedia; Enabling technologies; Extreme learning machine; Human perception; Image quality assessment; Input vector; Multi layer perceptron; Perceptual mechanism; Reduced reference; Video signal; Visual qualities; Visual quality assessment; Visual signals; Artificial intelligence; Backpropagation; Knowledge acquisition; Learning systems; Quality control; Image quality; article; artificial neural network; back propagation; circular back propagation; circular extreme learning machine; computational intelligence; computer model; conceptual framework; controlled study; human; image display; image processing; image quality; intelligence; learning algorithm; machine learning; mathematical analysis; priority journal; quality control; signal processing",Article,Scopus,2-s2.0-84870245922
"Chen Y., Zhang J., Cai D., Liu W., He X.","Nonnegative local coordinate factorization for image representation",2013,"IEEE Transactions on Image Processing",44,10.1109/TIP.2012.2224357,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873311588&doi=10.1109%2fTIP.2012.2224357&partnerID=40&md5=e0b429234db25f12fb878ce2ca4fd723","Recently, nonnegative matrix factorization (NMF) has become increasingly popular for feature extraction in computer vision and pattern recognition. NMF seeks two nonnegative matrices whose product can best approximate the original matrix. The nonnegativity constraints lead to sparse parts-based representations that can be more robust than nonsparse global features. To obtain more accurate control over the sparseness, in this paper, we propose a novel method called nonnegative local coordinate factorization (NLCF) for feature extraction. NLCF adds a local coordinate constraint into the standard NMF objective function. Specifically, we require that the learned basis vectors be as close to the original data points as possible. In this way, each data point can be represented by a linear combination of only a few nearby basis vectors, which naturally leads to sparse representation. Extensive experimental results suggest that the proposed approach provides a better representation and achieves higher accuracy in image clustering. © 1992-2012 IEEE.","Local coordinate coding; nonnegative matrix factorization; sparse learning","Basis vector; Data points; Global feature; Image clustering; Image representations; Linear combinations; Local coordinate; Non-negative matrix; Nonnegative matrix factorization; Nonnegativity constraints; Objective functions; sparse learning; Sparse representation; Computer vision; Factorization; Feature extraction; Matrix algebra; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; image enhancement; information retrieval; methodology; reproducibility; sensitivity and specificity; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Information Storage and Retrieval; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84873311588
"Paulheim H., Bizer C.","Type inference on noisy RDF data",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",43,10.1007/978-3-642-41335-3_32,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891923638&doi=10.1007%2f978-3-642-41335-3_32&partnerID=40&md5=5f46d0bf38c71341e62f51490d695fca","Type information is very valuable in knowledge bases. However, most large open knowledge bases are incomplete with respect to type information, and, at the same time, contain noisy and incorrect data. That makes classic type inference by reasoning difficult. In this paper, we propose the heuristic link-based type inference mechanism SDType, which can handle noisy and incorrect data. Instead of leveraging T-box information from the schema, SDType takes the actual use of a schema into account and thus is also robust to misused schema elements. © 2013 Springer-Verlag.","Link-based Classification; Noisy Data; Type Inference","Actual use; Knowledge basis; Link-based; Noisy data; RDF data; Type inference mechanism; Type inferences; Type information; Computer science; Computers; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84891923638
"Kwon J., Lee K.M.","Highly nonrigid object tracking via patch-based dynamic appearance modeling",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",43,10.1109/TPAMI.2013.32,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882974751&doi=10.1109%2fTPAMI.2013.32&partnerID=40&md5=fbd0b279f21d4f8c0a6ea73472260c6b","A novel tracking algorithm is proposed for targets with drastically changing geometric appearances over time. To track such objects, we develop a local patch-based appearance model and provide an efficient online updating scheme that adaptively changes the topology between patches. In the online update process, the robustness of each patch is determined by analyzing the likelihood landscape of the patch. Based on this robustness measure, the proposed method selects the best feature for each patch and modifies the patch by moving, deleting, or newly adding it over time. Moreover, a rough object segmentation result is integrated into the proposed appearance model to further enhance it. The proposed framework easily obtains segmentation results because the local patches in the model serve as good seeds for the semi-supervised segmentation task. To solve the complexity problem attributable to the large number of patches, the Basin Hopping (BH) sampling method is introduced into the tracking framework. The BH sampling method significantly reduces computational complexity with the help of a deterministic local optimizer. Thus, the proposed appearance model could utilize a sufficient number of patches. The experimental results show that the present approach could track objects with drastically changing geometric appearance accurately and robustly. © 1979-2012 IEEE.","Basin Hopping Sampling; likelihood landscape analysis; local patch-based appearance model; Markov Chain Monte Carlo; nonrigid object; Object tracking","Appearance modeling; Basin hopping; Landscape analysis; Markov Chain Monte-Carlo; Non-rigid objects; Object Tracking; Image segmentation; Tracking (position); Face recognition; algorithm; artificial intelligence; automated pattern recognition; computer assisted diagnosis; computer simulation; image enhancement; image subtraction; procedures; reproducibility; sensitivity and specificity; statistical model; article; automated pattern recognition; computer assisted diagnosis; methodology; Algorithms; Artificial Intelligence; Computer Simulation; Image Enhancement; Image Interpretation, Computer-Assisted; Models, Statistical; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique; Algorithms; Artificial Intelligence; Computer Simulation; Image Enhancement; Image Interpretation, Computer-Assisted; Models, Statistical; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-84882974751
"Zhang Z., Zhao K.","Low-rank matrix approximation with manifold regularization",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",43,10.1109/TPAMI.2012.274,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878141722&doi=10.1109%2fTPAMI.2012.274&partnerID=40&md5=b09777b130235000f6e451bf829543f2","This paper proposes a new model of low-rank matrix factorization that incorporates manifold regularization to the matrix factorization. Superior to the graph-regularized nonnegative matrix factorization, this new regularization model has globally optimal and closed-form solutions. A direct algorithm (for data with small number of points) and an alternate iterative algorithm with inexact inner iteration (for large scale data) are proposed to solve the new model. A convergence analysis establishes the global convergence of the iterative algorithm. The efficiency and precision of the algorithm are demonstrated numerically through applications to six real-world datasets on clustering and classification. Performance comparison with existing algorithms shows the effectiveness of the proposed method for low-rank factorization in general. © 1979-2012 IEEE.","classification; clustering; graph regularization; manifold learning; Matrix factorization","clustering; graph regularization; Low-rank matrix approximations; Manifold learning; Manifold regularizations; Matrix factorizations; Nonnegative matrix factorization; Performance comparison; Classification (of information); Iterative methods; Matrix algebra; Clustering algorithms; algorithm; article; artificial intelligence; automated pattern recognition; cluster analysis; computer simulation; face; factual database; histology; human; image processing; methodology; theoretical model; anatomy and histology; automated pattern recognition; procedures; Algorithms; Artificial Intelligence; Cluster Analysis; Computer Simulation; Databases, Factual; Face; Humans; Image Processing, Computer-Assisted; Models, Theoretical; Pattern Recognition, Automated; Algorithms; Artificial Intelligence; Cluster Analysis; Computer Simulation; Databases, Factual; Face; Humans; Image Processing, Computer-Assisted; Models, Theoretical; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84878141722
"Lee K.H., Verma N.","A low-power processor with configurable embedded machine-learning accelerators for high-order and adaptive analysis of medical-sensor signals",2013,"IEEE Journal of Solid-State Circuits",43,10.1109/JSSC.2013.2253226,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879931688&doi=10.1109%2fJSSC.2013.2253226&partnerID=40&md5=ca0f99222f61cffb500a71a919df9b0f","Low-power sensing technologies have emerged for acquiring physiologically indicative patient signals. However, to enable devices with high clinical value, a critical requirement is the ability to analyze the signals to extract specific medical information. Yet given the complexities of the underlying processes, signal analysis poses numerous challenges. Data-driven methods based on machine learning offer distinct solutions, but unfortunately the computations are not well supported by traditional DSP. This paper presents a custom processor that integrates a CPU with configurable accelerators for discriminative machine-learning functions. A support-vector-machine accelerator realizes various classification algorithms as well as various kernel functions and kernel formulations, enabling range of points within an accuracy-versus- energy and-memory trade space. An accelerator for embedded active learning enables prospective adaptation of the signal models by utilizing sensed data for patient-specific customization, while minimizing the effort from human experts. The prototype is implemented in 130-nm CMOS and operates from 1.2 V-0.55 V (0.7 V for SRAMs). Medical applications for EEG-based seizure detection and ECG-based cardiac-arrhythmia detection are demonstrated using clinical data, while consuming 273 μJ and 124 μJ per detection, respectively; this represents 62.4 × and 144.7 × energy reduction compared to an implementation based on the CPU. A patient-adaptive cardiac-arrhythmia detector is also demonstrated, reducing the analysis-effort required for model customization by 20 ×. © 1966-2012 IEEE.","Active learning (subject-specific adaptation); biomedical electronics; machine learning (artificial intelligence); medical signal processing; support vector machine (SVM)","Adaptive analysis; Biomedical electronics; Classification algorithm; Data-driven methods; Low power processors; Medical information; Medical signal processing; Subject-specific; Artificial intelligence; Detectors; Diseases; Medical applications; Static random access storage; Support vector machines; Vector spaces; Signal detection",Article,Scopus,2-s2.0-84879931688
"Tang Y., Ju P., He H., Qin C., Wu F.","Optimized control of DFIG-based wind generation using sensitivity analysis and particle swarm optimization",2013,"IEEE Transactions on Smart Grid",43,10.1109/TSG.2013.2237795,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875051256&doi=10.1109%2fTSG.2013.2237795&partnerID=40&md5=720a918c1f20cdc639e2e443870d96a6","Optimal control of large-scale wind farm has become a critical issue for the development of renewable energy systems and their integration into the power grid to provide reliable, secure, and efficient electricity. Among many enabling technologies, the latest research results from both the power and energy community and computational intelligence (CI) community have demonstrated that CI research could provide key technical innovations into this challenging problem. In this paper, we propose a sensitivity analysis approach based on both trajectory and frequency domain information integrated with evolutionary algorithm to achieve the optimal control of doubly-fed induction generators (DFIG) based wind generation. Instead of optimizing all the control parameters, our key idea is to use the sensitivity analysis to first identify the critical parameters, the unified dominate control parameters (UDCP), to reduce the optimization complexity. Based on such selected parameters, we then use particle swarm optimization (PSO) to find the optimal values to achieve the control objective. Simulation analysis and comparative studies demonstrate the effectiveness of our approach. © 2012 IEEE.","Computational intelligence; DFIG; optimized control; particle swarm optimization; sensitivity analysis; smart grid","DFIG; Doubly-fed induction generator; Enabling technologies; Large-scale wind farms; Optimized control; Renewable energy systems; Smart grid; Technical innovation; Artificial intelligence; Control; Electric utilities; Particle swarm optimization (PSO); Wind power; Sensitivity analysis",Article,Scopus,2-s2.0-84875051256
"Zou D., Tan P.","CoSLAM: Collaborative visual SLAM in dynamic environments",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",43,10.1109/TPAMI.2012.104,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871798249&doi=10.1109%2fTPAMI.2012.104&partnerID=40&md5=232f6acf2262b33d7f90c89c3c1f5c1f","This paper studies the problem of vision-based simultaneous localization and mapping (SLAM) in dynamic environments with multiple cameras. These cameras move independently and can be mounted on different platforms. All cameras work together to build a global map, including 3D positions of static background points and trajectories of moving foreground points. We introduce intercamera pose estimation and intercamera mapping to deal with dynamic objects in the localization and mapping process. To further enhance the system robustness, we maintain the position uncertainty of each map point. To facilitate intercamera operations, we cluster cameras into groups according to their view overlap, and manage the split and merge of camera groups in real time. Experimental results demonstrate that our system can work robustly in highly dynamic environments and produce more accurate results in static environments. © 2012 IEEE.","dynamic environments; structure-from-motion; swarm; Visual SLAM","3D positions; Dynamic environments; Dynamic objects; Global map; Mapping process; Multiple cameras; Pose estimation; Position uncertainties; Real time; Simultaneous localization and mapping; Split-and-merge; Static background; Static environment; Structure from motion; swarm; System robustness; Vision based; Visual SLAM; Cameras; Mathematical techniques; Robotics; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; image enhancement; methodology; photography; reproducibility; sensitivity and specificity; three dimensional imaging; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Pattern Recognition, Automated; Photography; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84871798249
"Agrawal S., Gorbunov S., Vaikuntanathan V., Wee H.","Functional encryption: New perspectives and lower bounds",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",42,10.1007/978-3-642-40084-1_28,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884471094&doi=10.1007%2f978-3-642-40084-1_28&partnerID=40&md5=b341140710a41625b6b4506f83743f5a","Functional encryption is an emerging paradigm for public-key encryption that enables fine-grained control of access to encrypted data. In this work, we present new lower bounds and impossibility results on functional encryption, as well as new perspectives on security definitions. Our main contributions are as follows: - We show that functional encryption schemes that satisfy even a weak (non-adaptive) simulation-based security notion are impossible to construct in general. This is the first impossibility result that exploits unbounded collusions in an essential way. In particular, we show that there are no such functional encryption schemes for the class of weak pseudo-random functions (and more generally, for any class of incompressible functions). More quantitatively, our technique also gives us a lower bound for functional encryption schemes secure against bounded collusions. To be secure against q collusions, we show that the ciphertext in any such scheme must have size Ω(q). - We put forth and discuss a simulation-based notion of security for functional encryption, with an unbounded simulator (called USIM). We show that this notion interpolates indistinguishability and simulation-based security notions, and is inspired by results and barriers in the zero-knowledge and multi-party computation literature. © 2013 International Association for Cryptologic Research.",,"Fine-grained control; Functional encryptions; Impossibility results; Indistinguishability; Multiparty computation; Pseudo-random functions; Public-key encryption; Simulation-based security; Artificial intelligence; Computer science; Public key cryptography",Conference Paper,Scopus,2-s2.0-84884471094
"Batrinca L., Stratou G., Shapiro A., Morency L.-P., Scherer S.","Cicero - Towards a multimodal virtual audience platform for public speaking training",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",42,10.1007/978-3-642-40415-3_10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883325504&doi=10.1007%2f978-3-642-40415-3_10&partnerID=40&md5=471c79d89b72fac0a50ba07c92c09974","Public speaking performances are not only characterized by the presentation of the content, but also by the presenters' nonverbal behavior, such as gestures, tone of voice, vocal variety, and facial expressions. Within this work, we seek to identify automatic nonverbal behavior descriptors that correlate with expert-assessments of behaviors characteristic of good and bad public speaking performances. We present a novel multimodal corpus recorded with a virtual audience public speaking training platform. Lastly, we utilize the behavior descriptors to automatically approximate the overall assessment of the performance using support vector regression in a speaker-independent experiment and yield promising results approaching human performance. © 2013 Springer-Verlag.","Behavioral Modification; Multimodal Perception; Public Speaking; Training; Virtual Reality","Behavioral modifications; Facial Expressions; Human performance; Multimodal perception; Nonverbal behavior; Public speaking; Support vector regression (SVR); Training platform; Artificial intelligence; Computer science; Personnel training; Virtual reality; Intelligent virtual agents",Conference Paper,Scopus,2-s2.0-84883325504
"Zhang B., Qi H., Ren Y.-T., Sun S.-C., Ruan L.-M.","Application of homogenous continuous Ant Colony Optimization algorithm to inverse problem of one-dimensional coupled radiation and conduction heat transfer",2013,"International Journal of Heat and Mass Transfer",42,10.1016/j.ijheatmasstransfer.2013.07.054,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882306850&doi=10.1016%2fj.ijheatmasstransfer.2013.07.054&partnerID=40&md5=57c0026ef77e922d8f2dcd7c3cc1f514","In this study, a grid-based continuous Ant Colony Optimization (ACO) algorithm was applied to the inverse problem of a one-dimensional (1-D) coupled radiation and conduction heat transfer. To overcome the local convergence of the Basic Ant Colony Optimization (BACO) algorithm for continuous domain problems, a Homogenous Ant Colony Optimization (HACO) algorithm was developed. To illustrate the performances of these algorithms, the thermal conductivity, absorption coefficient and scattering coefficient of the 1-D homogeneous semi-transparent medium were retrieved for four test cases. The sensitivity coefficient with respect to the inversion parameters was analyzed, on the basis of which the objective function was designed. The temperature and radiative heat flux simulated by the Finite Volume Method (FVM) were served as input for the inverse analysis. The HACO algorithm was demonstrated to be effective and robust, which has the potential to be implemented in various inverse heat transfer problems. © 2013 Elsevier Ltd. All rights reserved.","Ant Colony Optimization; Coupled radiation and conduction heat transfer; Inverse problem","Absorption co-efficient; Ant Colony Optimization algorithms; Inverse heat transfer problem; Inversion parameters; Objective functions; Radiative heat fluxes; Scattering co-efficient; Sensitivity coefficient; Ant colony optimization; Artificial intelligence; Finite volume method; Heat flux; Heat transfer; Inverse problems; Thermal conductivity; Algorithms",Article,Scopus,2-s2.0-84882306850
"Schreuder M., Höhne J., Blankertz B., Haufe S., Dickhaus T., Tangermann M.","Optimizing event-related potential based brain-computer interfaces: A systematic evaluation of dynamic stopping methods",2013,"Journal of Neural Engineering",42,10.1088/1741-2560/10/3/036025,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878300244&doi=10.1088%2f1741-2560%2f10%2f3%2f036025&partnerID=40&md5=dff17d2b9b78aeb07b02b46447a18b2e","Objective. In brain-computer interface (BCI) research, systems based on event-related potentials (ERP) are considered particularly successful and robust. This stems in part from the repeated stimulation which counteracts the low signal-to-noise ratio in electroencephalograms. Repeated stimulation leads to an optimization problem, as more repetitions also cost more time. The optimal number of repetitions thus represents a data-dependent trade-off between the stimulation time and the obtained accuracy. Several methods for dealing with this have been proposed as 'early stopping', 'dynamic stopping' or 'adaptive stimulation'. Despite their high potential for BCI systems at the patient's bedside, those methods are typically ignored in current BCI literature. The goal of the current study is to assess the benefit of these methods. Approach. This study assesses for the first time the existing methods on a common benchmark of both artificially generated data and real BCI data of 83 BCI sessions, allowing for a direct comparison between these methods in the context of text entry. Main results. The results clearly show the beneficial effect on the online performance of a BCI system, if the trade-off between the number of stimulus repetitions and accuracy is optimized. All assessed methods work very well for data of good subjects, and worse for data of low-performing subjects. Most methods, however, are robust in the sense that they do not reduce the performance below the baseline of a simple no stopping strategy. Significance. Since all methods can be realized as a module between the BCI and an application, minimal changes are needed to include these methods into existing BCI software architectures. Furthermore, the hyperparameters of most methods depend to a large extend on only a single variable - the discriminability of the training data. For the convenience of BCI practitioners, the present study proposes linear regression coefficients for directly estimating the hyperparameters from the data based on this discriminability. The data that were used in this publication are made publicly available to benchmark future methods. © 2013 IOP Publishing Ltd.",,"Beneficial effects; Event-related potentials; Linear regression coefficients; Low signal-to-noise ratio; On-line performance; Optimization problems; Stopping methods; Systematic evaluation; Bioelectric phenomena; Interfaces (computer); Optimization; Brain computer interface; adaptive behavior; architecture; article; brain computer interface; brain depth stimulation; computer program; controlled study; cost benefit analysis; data base; electroencephalogram; electroencephalography; event related potential; human; intermethod comparison; nerve stimulation; priority journal; publication; repeat procedure; signal noise ratio; task performance; trade union; Algorithms; Artificial Intelligence; Brain; Brain Mapping; Brain-Computer Interfaces; Electroencephalography; Evoked Potentials; Humans; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84878300244
"Zhang H., Yang J., Zhang Y., Huang T.S.","Image and video restorations via nonlocal kernel regression",2013,"IEEE Transactions on Cybernetics",42,10.1109/TSMCB.2012.2222375,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884536129&doi=10.1109%2fTSMCB.2012.2222375&partnerID=40&md5=dd430b1d2808862e1162716ab1d990a4","A nonlocal kernel regression (NL-KR) model is presented in this paper for various image and video restoration tasks. The proposed method exploits both the nonlocal self-similarity and local structural regularity properties in natural images. The nonlocal self-similarity is based on the observation that image patches tend to repeat themselves in natural images and videos, and the local structural regularity observes that image patches have regular structures where accurate estimation of pixel values via regression is possible. By unifying both properties explicitly, the proposed NL-KR framework is more robust in image estimation, and the algorithm is applicable to various image and video restoration tasks. In this paper, we apply the proposed model to image and video denoising, deblurring, and superresolution reconstruction. Extensive experimental results on both single images and realistic video sequences demonstrate that the proposed framework performs favorably with previous works both qualitatively and quantitatively. © 2012 IEEE.","Deblurring; Denoising; Local structural regression; Nonlocal self-similarity; Restoration; Superresolution (SR)","De-noising; Deblurring; Self-similarities; Structural regression; Superresolution; Image reconstruction; Optical resolving power; Regression analysis; Restoration; Image enhancement; algorithm; artificial intelligence; automated pattern recognition; computer assisted diagnosis; image subtraction; photography; procedures; regression analysis; statistical analysis; videorecording; article; automated pattern recognition; computer assisted diagnosis; methodology; photography; videorecording; Algorithms; Artificial Intelligence; Data Interpretation, Statistical; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Photography; Regression Analysis; Subtraction Technique; Video Recording; Algorithms; Artificial Intelligence; Data Interpretation, Statistical; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Photography; Regression Analysis; Subtraction Technique; Video Recording",Article,Scopus,2-s2.0-84884536129
"Kalra A., Ahmad S., Nayak A.","Increasing streamflow forecast lead time for snowmelt-driven catchment based on large-scale climate patterns",2013,"Advances in Water Resources",42,10.1016/j.advwatres.2012.11.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871673033&doi=10.1016%2fj.advwatres.2012.11.003&partnerID=40&md5=1e263e0cdb4d8d052fd2db26fe134353","This study focuses on improving the spring-summer streamflow forecast lead time using large scale climate patterns. An artificial intelligence type data-driven model, Support Vector Machine (SVM), was developed incorporating oceanic-atmospheric oscillations to increase the forecast lead time. The application of SVM model is tested on three unimpaired gages in the North Platte River Basin. Seasonal averages of oceanic-atmospheric indices for the period of 1940-2007 are used to generate spring-summer streamflow volumes with 3-, 6- and 9-month lead times. The results reveal a strong association between coupled indices compared to their individual effects. The best streamflow estimates are obtained at 6-month compared to 3-month and 9-month lead times. The proposed modeling technique is expected to provide useful information to water managers and help in better managing the water resources and the operation of water systems. © 2012 Elsevier Ltd.","Climate variability; Forecast; North Platte; Oscillations; Streamflow; SVM","Climate variability; Forecast; North Platte; Oscillations; SVM; Artificial intelligence; Catchments; Climatology; Forecasting; Support vector machines; Water resources; Stream flow; catchment; climate oscillation; climate variation; forecasting method; hydrological modeling; snowmelt; streamflow; water management; North Platte Basin; United States",Article,Scopus,2-s2.0-84871673033
"Boyen X.","Attribute-based functional encryption on lattices",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",42,10.1007/978-3-642-36594-2_8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873956736&doi=10.1007%2f978-3-642-36594-2_8&partnerID=40&md5=3f329fbc0d0a4a774c8b0ddd52b47434","We introduce a broad lattice manipulation technique for expressive cryptography, and use it to realize functional encryption for access structures from post-quantum hardness assumptions. Specifically, we build an efficient key-policy attribute-based encryption scheme, and prove its security in the selective sense from learning-with-errors intractability in the standard model. © 2013 International Association for Cryptologic Research.",,"Access structure; Attribute-based; Attribute-based encryption schemes; Functional encryptions; Key policies; Manipulation techniques; The standard model; Artificial intelligence; Cryptography",Conference Paper,Scopus,2-s2.0-84873956736
"Bitansky N., Chiesa A., Ishai Y., Paneth O., Ostrovsky R.","Succinct non-interactive arguments via linear interactive proofs",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",42,10.1007/978-3-642-36594-2_18,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873941196&doi=10.1007%2f978-3-642-36594-2_18&partnerID=40&md5=3a76b392d227a78cebc0964807d1f30a","Succinct non-interactive arguments (SNARGs) enable verifying NP statements with lower complexity than required for classical NP verification. Traditionally, the focus has been on minimizing the length of such arguments; nowadays researches have focused also on minimizing verification time, by drawing motivation from the problem of delegating computation. A common relaxation is a preprocessing SNARG, which allows the verifier to conduct an expensive offline phase that is independent of the statement to be proven later. Recent constructions of preprocessing SNARGs have achieved attractive features: they are publicly-verifiable, proofs consist of only O(1) encrypted (or encoded) field elements, and verification is via arithmetic circuits of size linear in the NP statement. Additionally, these constructions seem to have ""escaped the hegemony"" of probabilistically-checkable proofs (PCPs) as a basic building block of succinct arguments. © 2013 International Association for Cryptologic Research.",,"Arithmetic circuit; Basic building block; Delegating computation; Interactive proofs; Lower complexity; Offline; Artificial intelligence; Cryptography",Conference Paper,Scopus,2-s2.0-84873941196
"Tripathy B.C., Dutta A.J.","Lacunary bounded variation sequence of fuzzy real numbers",2013,"Journal of Intelligent and Fuzzy Systems",42,10.3233/IFS-2012-0544,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872278720&doi=10.3233%2fIFS-2012-0544&partnerID=40&md5=f634e2a6cf71cf8f40666ada8fd243dd","In this article we have introduced the concept of lacunary bounded variation sequence of fuzzy real numbers. We have verified some properties like solid, symmetric, convergence free etc. and also proved some inclusion results. © 2013-IOS Press and the authors. All rights reserved.","bounded variation; convergence free; fuzzy real number; Lacunary; solid; symmetric","Bounded variations; convergence free; Lacunary; Real number; symmetric; Artificial intelligence; Solids; Engineering",Article,Scopus,2-s2.0-84872278720
"Mao A., Procaccia A.D., Chen Y.","Better human computation through principled voting",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",41,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879739984&partnerID=40&md5=ad4890c184e8dd0cda0e06c77df7dcc1","Designers of human computation systms often face the need to aggregate noisy information provided by multiple people. While voting is often used for this purpose, the choice of voting method is typically not principled. We conduct extensive experiments on Amazon Mechanical Turk to better understand how different voting rules perform in practice. Our empirical conclusions show that noisy human voting can differ from what popular theoretical models would predict. Our short-term goal is to motivate the design of better human computation systems; our long-term goal is to spark an interaction between researchers in (computational) social choice and human computation. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Amazon mechanical turks; Human computation; Long-term goals; Multiple people; Social choice; Voting method; Voting rules; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84879739984
"Barlett M., Cussens J.","Advances in Bayesian network learning using integer programming",2013,"Uncertainty in Artificial Intelligence - Proceedings of the 29th Conference, UAI 2013",41,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888170812&partnerID=40&md5=8c52c1ed1938586ba590fe69d632e15b","We consider the problem of learning Bayesian networks (BNs) from complete discrete data. This problem of discrete optimisation is formulated as an integer program (IP). We describe the various steps we have taken to allow efficient solving of this IP. These are (i) efficient search for cutting planes, (ii) a fast greedy algorithm to find high-scoring (perhaps not optimal) BNs and (iii) tightening the linear relaxation of the IP. After relating this BN learning problem to set covering and the multidimensional 0-1 knapsack problem, we present our empirical results. These show improvements, sometimes dramatic, over earlier results.",,"Bayesian network learning; Discrete optimisation; Greedy algorithms; Integer program; Learning Bayesian networks; Learning problem; Linear relaxations; Multidimensional 0-1 knapsack problem; Artificial intelligence; Integer programming; Optimization; Bayesian networks",Conference Paper,Scopus,2-s2.0-84888170812
"Corrente S., Greco S., Kadziński M., Słowiński R.","Robust ordinal regression in preference learning and ranking",2013,"Machine Learning",41,10.1007/s10994-013-5365-4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884283756&doi=10.1007%2fs10994-013-5365-4&partnerID=40&md5=7d77d6323e524a07457291620d7d45e4","Multiple Criteria Decision Aiding (MCDA) offers a diversity of approaches designed for providing the decision maker (DM) with a recommendation concerning a set of alternatives (items, actions) evaluated from multiple points of view, called criteria. This paper aims at drawing attention of the Machine Learning (ML) community upon recent advances in a representative MCDA methodology, called Robust Ordinal Regression (ROR). ROR learns by examples in order to rank a set of alternatives, thus considering a similar problem as Preference Learning (ML-PL) does. However, ROR implements the interactive preference construction paradigm, which should be perceived as a mutual learning of the model and the DM. The paper clarifies the specific interpretation of the concept of preference learning adopted in ROR and MCDA, comparing it to the usual concept of preference learning considered within ML. This comparison concerns a structure of the considered problem, types of admitted preference information, a character of the employed preference models, ways of exploiting them, and techniques to arrive at a final ranking. © 2013 The Author(s).","Comparison; Multiple criteria decision aiding; Preference construction; Preference learning; Preference modeling; Ranking; Robust ordinal regression","Comparison; Multiple Criteria Decision Aiding; Preference learning; Preference modeling; Ranking; Robust ordinal regressions; Artificial intelligence; Software engineering; Regression analysis",Article,Scopus,2-s2.0-84884283756
"Ni B., Pei Y., Moulin P., Yan S.","Multilevel depth and image fusion for human activity detection",2013,"IEEE Transactions on Cybernetics",41,10.1109/TCYB.2013.2276433,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890390556&doi=10.1109%2fTCYB.2013.2276433&partnerID=40&md5=f8038ae8d397c20165a32cd7b17ee3c4","Recognizing complex human activities usually requires the detection and modeling of individual visual features and the interactions between them. Current methods only rely on the visual features extracted from 2-D images, and therefore often lead to unreliable salient visual feature detection and inaccurate modeling of the interaction context between individual features. In this paper, we show that these problems can be addressed by combining data from a conventional camera and a depth sensor (e.g., Microsoft Kinect). We propose a novel complex activity recognition and localization framework that effectively fuses information from both grayscale and depth image channels at multiple levels of the video processing pipeline. In the individual visual feature detection level, depth-based filters are applied to the detected human/object rectangles to remove false detections. In the next level of interaction modeling, 3-D spatial and temporal contexts among human subjects or objects are extracted by integrating information from both grayscale and depth images. Depth information is also utilized to distinguish different types of indoor scenes. Finally, a latent structural model is developed to integrate the information from multiple levels of video processing for an activity detection. Extensive experiments on two activity recognition benchmarks (one with depth information) and a challenging grayscale + depth human activity database that contains complex interactions between human-human, human-object, and human-surroundings demonstrate the effectiveness of the proposed multilevel grayscale + depth fusion scheme. Higher recognition and localization accuracies are obtained relative to the previous methods. © 2013 IEEE.","Action recognition and localization; Depth sensor; Spatial and temporal context","Action recognition; Activity recognition; Depth sensors; Human-activity detection; Individual features; Integrating information; Localization accuracy; Spatial and temporal context; Computer vision; Image fusion; Image recognition; Motion estimation; Pipeline processing systems; Sensors; Video signal processing; Chemical detection; actimetry; algorithm; artificial intelligence; automated pattern recognition; computer; computer simulation; computer system; devices; human; image enhancement; image subtraction; procedures; recreation; three dimensional imaging; transducer; whole body imaging; actimetry; article; automated pattern recognition; equipment; methodology; three dimensional imaging; whole body imaging; Actigraphy; Algorithms; Artificial Intelligence; Computer Peripherals; Computer Simulation; Computer Systems; Humans; Image Enhancement; Imaging, Three-Dimensional; Pattern Recognition, Automated; Subtraction Technique; Transducers; Video Games; Whole Body Imaging; Actigraphy; Algorithms; Artificial Intelligence; Computer Peripherals; Computer Simulation; Computer Systems; Humans; Image Enhancement; Imaging, Three-Dimensional; Pattern Recognition, Automated; Subtraction Technique; Transducers; Video Games; Whole Body Imaging",Article,Scopus,2-s2.0-84890390556
"Rasiwasia N., Vasconcelos N.","Latent dirichlet allocation models for image classification",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",41,10.1109/TPAMI.2013.69,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884546915&doi=10.1109%2fTPAMI.2013.69&partnerID=40&md5=4c5487578f7ddd021040525986195e47","Two new extensions of latent Dirichlet allocation (LDA), denoted topic-supervised LDA (ts-LDA) and class-specific-simplex LDA (css-LDA), are proposed for image classification. An analysis of the supervised LDA models currently used for this task shows that the impact of class information on the topics discovered by these models is very weak in general. This implies that the discovered topics are driven by general image regularities, rather than the semantic regularities of interest for classification. To address this, ts - LDA models are introduced which replace the automated topic discovery of LDA with specified topics, identical to the classes of interest for classification. While this results in improvements in classification accuracy over existing LDA models, it compromises the ability of LDA to discover unanticipated structure of interest. This limitation is addressed by the introduction of css-LDA, an LDA model with class supervision at the level of image features. In css-LDA topics are discovered per class, i.e., a single set of topics shared across classes is replaced by multiple class-specific topic sets. The css-LDA model is shown to combine the labeling strength of topic-supervision with the flexibility of topic-discovery. Its effectiveness is demonstrated through an extensive experimental evaluation, involving multiple benchmark datasets, where it is shown to outperform existing LDA-based image classification approaches. © 1979-2012 IEEE.","attributes; graphical models; Image classification; latent Dirichlet allocation; semantic classification","attributes; Classification accuracy; Classification approach; Experimental evaluation; GraphicaL model; Latent Dirichlet allocation; Latent dirichlet allocations; Semantic classification; Indexing (of information); Semantics; Statistics; Image classification; algorithm; artificial intelligence; automated pattern recognition; computer assisted diagnosis; computer simulation; human; image enhancement; procedures; statistical model; article; automated pattern recognition; computer assisted diagnosis; image enhancement; methodology; Algorithms; Artificial Intelligence; Computer Simulation; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Models, Statistical; Pattern Recognition, Automated; Algorithms; Artificial Intelligence; Computer Simulation; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Models, Statistical; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84884546915
"Neftci E., Binas J., Rutishauser U., Chicca E., Indiveri G., Douglas R.J.","Synthesizing cognition in neuromorphic electronic systems",2013,"Proceedings of the National Academy of Sciences of the United States of America",41,10.1073/pnas.1212083110,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883772282&doi=10.1073%2fpnas.1212083110&partnerID=40&md5=8f1beae3da78193396a4aabe4b452f98","The quest to implement intelligent processing in electronic neuromorphic systems lacks methods for achieving reliable behavioral dynamics on substrates of inherently imprecise and noisy neurons. Here we report a solution to this problem that involves first mapping an unreliable hardware layer of spiking silicon neurons into an abstract computational layer composed of generic reliable subnetworks of model neurons and then composing the target behavioral dynamics as a ""soft state machine"" running on these reliable subnets. In the first step, the neural networks of the abstract layer are realized on the hardware substrate bymapping the neuron circuit bias voltages to the model parameters. This mapping is obtained by an automatic method in which the electronic circuit biases are calibrated against the model parameters by a series of population activity measurements. The abstract computational layer is formed by con figuring neural networks as generic soft winner-take-all subnetworks that provide reliable processing by virtue of their active gain, signal restoration, and multistability. The necessary states and transitions of the desired high-level behavior are then easily embedded in the computational layer by introducing only sparse connections between some neurons of the various subnets. We demonstrate this synthesis method for a neuromorphic sensory agent that performs real-time context-dependent classification of motion patterns observed by a silicon retina.","Analog very large-scale integration; Artificial neural systems; Decision making; Sensorimotor; Working memory","silicon; analytic method; article; brain mapping; cognition; cognitive synthesis; computational fluid dynamics; controlled study; feedback system; human; interpersonal communication; nerve cell; nerve cell network; neuromorphic electronic system; priority journal; selective attention; signal transduction; spike; steady state; swta network; working memory; analog very large-scale integration; artificial neural systems; decision making; sensorimotor; working memory; Animals; Artificial Intelligence; Cognition; Humans; Models, Neurological; Neural Networks (Computer); Primates; Semiconductors",Article,Scopus,2-s2.0-84883772282
"Li M., Im J., Beier C.","Machine learning approaches for forest classification and change analysis using multi-temporal Landsat TM images over Huntington Wildlife Forest",2013,"GIScience and Remote Sensing",41,10.1080/15481603.2013.819161,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883537640&doi=10.1080%2f15481603.2013.819161&partnerID=40&md5=9d742ef72fba7368f621a8bed7b86643","This research investigated three machine learning approaches - decision trees, random forest, and support vector machines - to classify local forest communities at the Huntington Wildlife Forest (HWF), located in the central Adirondack Mountains of New York State, and to identify forest type change over a 20-year period using multi-temporal Landsat satellite Thematic Mapper (TM) data. Because some forest species are sensitive to topographic characteristics, three terrain correction methods - C correction, statistical-empirical (SE) correction, and Variable Empirical Coefficient Algorithm (VECA) - were utilized to account for the topographic effects. Results show that the topographic correction slightly improved the classification accuracy although the improvement was not significant based on the McNemar test. Random forest and support vector machines produced higher classification accuracies than decision trees. Besides, random forest- and support vector machine-based multi-temporal classifications better reflected the forest type change seen in the reference data. In addition, topographic features such as elevation and aspect played important roles in characterizing the forest type changes. © 2013 Copyright Taylor and Francis Group, LLC.","change detection; decision trees; forest type classification; random forest; remote sensing; support vector machines; topographic correction","algorithm; artificial intelligence; classification; empirical analysis; forest ecosystem; Landsat thematic mapper; remote sensing; satellite data; terrain; topography; Huntington Wildlife Forest; New York [United States]; United States",Article,Scopus,2-s2.0-84883537640
"Chen B., Chen L., Chen Y.","Efficient ant colony optimization for image feature selection",2013,"Signal Processing",41,10.1016/j.sigpro.2012.10.022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875245118&doi=10.1016%2fj.sigpro.2012.10.022&partnerID=40&md5=f4db75d3dab4facf3ebe89e417b825da","Feature selection (FS) is an important task which can significantly affect the performance of image classification and recognition. In this paper, we present a feature selection algorithm based on ant colony optimization (ACO). For n features, existing ACO-based feature selection methods need to traverse a complete graph with O(n2) edges. However, we propose a novel algorithm in which the artificial ants traverse on a directed graph with only O(2n) arcs. The algorithm incorporates the classification performance and feature set size into the heuristic guidance, and selects a feature set with small size and high classification accuracy. We perform extensive experiments on two large image databases and 15 non-image datasets to show that our proposed algorithm can obtain higher processing speed as well as better classification accuracy using a smaller feature set than other existing methods. © 2012 Elsevier B.V.","Ant colony optimization; Dimensionality reduction; Feature selection; Image classification","Ant Colony Optimization (ACO); Classification accuracy; Classification and recognition; Classification performance; Dimensionality reduction; Feature selection algorithm; Feature selection methods; Image feature selections; Ant colony optimization; Artificial intelligence; Classification (of information); Feature extraction; Image classification; Algorithms",Article,Scopus,2-s2.0-84875245118
"Wei C.-H., Harris B.R., Kao H.-Y., Lu Z.","TmVar: A text mining approach for extracting sequence variants in biomedical literature",2013,"Bioinformatics",41,10.1093/bioinformatics/btt156,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878281265&doi=10.1093%2fbioinformatics%2fbtt156&partnerID=40&md5=57277534bfdf3f23c1af0efbbaa723eb","Motivation: Text-mining mutation information from the literature becomes a critical part of the bioinformatics approach for the analysis and interpretation of sequence variations in complex diseases in the post-genomic era. It has also been used for assisting the creation of disease-related mutation databases. Most of existing approaches are rule-based and focus on limited types of sequence variations, such as protein point mutations. Thus, extending their extraction scope requires significant manual efforts in examining new instances and developing corresponding rules. As such, new automatic approaches are greatly needed for extracting different kinds of mutations with high accuracy.Results: Here, we report tmVar, a text-mining approach based on conditional random field (CRF) for extracting a wide range of sequence variants described at protein, DNA and RNA levels according to a standard nomenclature developed by the Human Genome Variation Society. By doing so, we cover several important types of mutations that were not considered in past studies. Using a novel CRF label model and feature set, our method achieves higher performance than a state-of-the-art method on both our corpus (91.4 versus 78.1% in F-measure) and their own gold standard (93.9 versus 89.4% in F-measure). These results suggest that tmVar is a high-performance method for mutation extraction from biomedical literature. © The Author 2013. Published by Oxford University Press. All rights reserved.",,"article; artificial intelligence; computer program; data mining; evaluation; human; methodology; mutation; nucleotide sequence; Artificial Intelligence; Data Mining; DNA Mutational Analysis; Humans; Mutation; Software",Article,Scopus,2-s2.0-84878281265
"Wang L., Ma C., Wipf P., Liu H., Su W., Xie X.-Q.","Targethunter: An in silico target identification tool for predicting therapeutic potential of small organic molecules based on chemogenomic database",2013,"AAPS Journal",41,10.1208/s12248-012-9449-z,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877055652&doi=10.1208%2fs12248-012-9449-z&partnerID=40&md5=aece8fd80fe425631589df56e4c429a0","Target identification of the known bioactive compounds and novel synthetic analogs is a very important research field in medicinal chemistry, biochemistry, and pharmacology. It is also a challenging and costly step towards chemical biology and phenotypic screening. In silico identification of potential biological targets for chemical compounds offers an alternative avenue for the exploration of ligand-target interactions and biochemical mechanisms, as well as for investigation of drug repurposing. Computational target fishing mines biologically annotated chemical databases and then maps compound structures into chemogenomical space in order to predict the biological targets. We summarize the recent advances and applications in computational target fishing, such as chemical similarity searching, data mining/machine learning, panel docking, and the bioactivity spectral analysis for target identification. We then described in detail a new web-based target prediction tool, TargetHunter (http://www.cbligand.org/TargetHunter). This web portal implements a novel in silico target prediction algorithm, the Targets Associated with its MOst SImilar Counterparts, by exploring the largest chemogenomical databases, ChEMBL. Prediction accuracy reached 91.1% from the top 3 guesses on a subset of high-potency compounds from the ChEMBL database, which outperformed a published algorithm, multiple-category models. TargetHunter also features an embedded geography tool, BioassayGeoMap, developed to allow the user easily to search for potential collaborators that can experimentally validate the predicted biological target(s) or off target(s). TargetHunter therefore provides a promising alternative to bridge the knowledge gap between biology and chemistry, and significantly boost the productivity of chemogenomics researchers for in silico drug design and discovery. © 2013 American Association of Pharmaceutical Scientists.","Chembl; chemogenomics; machine learning; Target hunter; target identification","acetylcholinesterase; biological product; compound chembl 1711746; compound chembl 1724922; compound cid 46907796; cyclooxygenase 1; cyclooxygenase 2; darifenacin; dopamine 1 receptor; epidermal growth factor receptor; epidermal growth factor receptor erbB1; Human immunodeficiency virus proteinase; Human immunodeficiency virus type 1 protease; Human immunodeficiency virus type 1 reverse transcriptase; mitogen activated protein kinase 14; nuclear factor erythroid 2 related factor 2 inhibitor; peroxisome proliferator activated receptor gamma; polythiazide; RNA directed DNA polymerase; small organic molecule; thrombin; uk 201844; unclassified drug; accuracy; algorithm; article; biological activity; chemogenomics; computer model; data analysis software; data base; data mining; drug activity; drug potency; drug structure; genomics; ligand binding; machine learning; mathematical analysis; molecular docking; online system; prediction; protein targeting; Algorithms; Anti-HIV Agents; Antihypertensive Agents; Antineoplastic Agents; Artificial Intelligence; Benzofurans; Computer Graphics; Computer Simulation; Data Mining; Databases, Chemical; Drug Discovery; Drug Repositioning; Models, Molecular; Molecular Docking Simulation; Molecular Structure; Muscarinic Antagonists; Polythiazide; Pyrrolidines; Reproducibility of Results; Software; Structure-Activity Relationship; User-Computer Interface",Article,Scopus,2-s2.0-84877055652
"Chen C.C., Wan Y.-H., Chung M.-C., Sun Y.-C.","An effective recommendation method for cold start new users using trust and distrust networks",2013,"Information Sciences",41,10.1016/j.ins.2012.10.037,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870993555&doi=10.1016%2fj.ins.2012.10.037&partnerID=40&md5=3ac5a965e2c25a8dfafa8c5b2693f898","Recommendation systems analyze the purchasing behavior (e.g.; item ratings) of users to learn about their preferences and recommend products or services that may be of interest to them. However, as new users require time to become familiar with recommendation systems, the systems usually have limited information about newcomers and have difficulty providing appropriate recommendations. This so-called new user cold start phenomenon has a serious impact on the performance of recommendation systems. As a result, there has been increasing research in recent years into new user cold start recommendation methods that try to provide useful item recommendations for cold start new users. The rationale behind much of the research is that recommending items to new users generally creates a sense of belonging and loyalty, and encourages them to frequently utilize recommendation systems. In this paper, we propose a cold start recommendation method for the new user that integrates a user model with trust and distrust networks to identify trustworthy users. The suggestions of these users are then aggregated to provide useful recommendations for cold start new users. Experiments based on the well-known Epinions dataset demonstrate the efficacy of the proposed method. Moreover, the method outperforms well-known recommendation methods for cold start new users in terms of the recall rate, F1 score, coverage rate, users coverage, and execution time, without a significant reduction in the precision of the recommendations. © 2012 Elsevier Inc. All rights reserved.","Collaborative filtering; Recommendation system; Social network","Cold start; Cold-start Recommendations; Collaborative filtering; Coverage rate; Data sets; Execution time; Limited information; Recall rate; Recommendation methods; Sense of belonging; Social Networks; User models; Artificial intelligence; Software engineering; Recommender systems",Article,Scopus,2-s2.0-84870993555
"Liao T.W., Egbelu P.J., Chang P.C.","Simultaneous dock assignment and sequencing of inbound trucks under a fixed outbound truck schedule in multi-door cross docking operations",2013,"International Journal of Production Economics",41,10.1016/j.ijpe.2012.03.037,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869502731&doi=10.1016%2fj.ijpe.2012.03.037&partnerID=40&md5=20cc36ff6b31eddd2051d38402b7a458","This paper studies the simultaneous dock assignment and sequencing of inbound trucks for a multi-door cross docking operation with the objective to minimize total weighted tardiness, under a fixed outbound truck departure schedule. The problem is newly formulated and solved by six different metaheuristic algorithms, which include simulated annealing, tabu search, ant colony optimization, differential evolution, and two hybrid differential- evolution algorithms. To evaluate the total weighted tardiness associated with any given inbound-truck sequence and dock assignment, an operational policy is developed. This policy is employed by every metaheuristic algorithm in searching for the optimal dock assignment and sequence. Each metaheuristic algorithm is tested with 40 problems. The major conclusions are: (1) metaheuristic is generally an effective optimization method for the subject problem; (2) population based metaheuristic algorithms are generally more effective than projection based metaheuristic algorithms; (3) proper selection of algorithmic parameters is important and more critical for projection based metaheuristic algorithms than population based algorithms; (4) the two best algorithms are ant colony optimization and hybrid differential evolution 2; among them, ACO takes less time than hybrid 2 and thus can be declared the best among all the six metaheuristic algorithms tested. © 2012 Elsevier B.V. All rights reserved.","Ant colony optimization; Combinatorial optimization; Cross docking; Differential evolution; Dock assignment; Hybrid metaheuristics; Sequencing; Simulated annealing; Tabu search","Ant Colony Optimization (ACO); Crossdocking; Differential Evolution; Hybrid metaheuristics; Sequencing; Artificial intelligence; Automobiles; Combinatorial optimization; Docking; Docks; Evolutionary algorithms; Hydraulic structures; Packet networks; Simulated annealing; Tabu search; Trucks",Article,Scopus,2-s2.0-84869502731
"Gallagher S.","The socially extended mind",2013,"Cognitive Systems Research",40,10.1016/j.cogsys.2013.03.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883452631&doi=10.1016%2fj.cogsys.2013.03.008&partnerID=40&md5=91510a3cf600a48bd8476e3eb34939cd","This paper contrasts conservative and liberal interpretations of the extended mind hypothesis. The liberal view, defended here, considers cognition to be socially extensive, in a way that goes beyond the typical examples (involving notebooks and various technologies) rehearsed in the extended mind literature, and in a way that takes cognition to involve enactive processes (e.g., social affordances), rather than functional supervenience relations. The socially extended mind is in some cases constituted not only in social interactions with others, but also in ways that involve institutional structures, norms, and practices. Some of the common objections to the extended mind are considered in relation to this liberal interpretation. Implications for critical social theory are explored. © 2013 Elsevier B.V.","Critical theory; Enactivism; Extended mind; Institutions; Parity principle; Social affordances","Affordances; Critical theory; Enactivism; Extended minds; Parity principle; Cognitive systems; Societies and institutions; Artificial intelligence; article; brain function; decision making; human; institutional care; interpersonal communication; legal aspect; memory; mental hospital; politics; priority journal; problem solving; social behavior; social cognition; social environment; social interaction; socially extended mind; theory of mind",Article,Scopus,2-s2.0-84883452631
"Noessner J., Niepert M., Stuckenschmidt H.","RockIt: Exploiting parallelism and symmetry for MAP inference in statistical relational models",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",40,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893407444&partnerID=40&md5=301a4b0c5e61da6caec88ede5ba2f025","ROCKIT is a maximum a-posteriori (MAP) query engine for statistical relational models. MAP inference in graphical models is an optimization problem which can be compiled to integer linear programs (ILPs). We describe several advances in translating MAP queries to ILP instances and present the novel meta-algorithm cutting plane aggregation (CPA). CPA exploits local context-specific symmetries and bundles up sets of linear constraints. The resulting counting constraints lead to more compact ILPs and make the symmetry of the ground model more explicit to state-of-the-art ILP solvers. Moreover, ROCKIT parallelizes most parts of the MAP inference pipeline taking advantage of ubiquitous shared-memory multi-core architectures. We report on extensive experiments with Markov logic network (MLN) benchmarks showing that ROCKIT outperforms the state-of-the-art systems ALCHEMY, MARKOV THEBEAST, and TUFFY both in terms of efficiency and quality of results. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Integer linear programs; Linear constraints; Markov logic networks; Maximum a posteriori; Multicore architectures; Optimization problems; Quality of results; State-of-the-art system; Artificial intelligence; Benchmarking; Computer architecture; Integer programming; Inductive logic programming (ILP)",Conference Paper,Scopus,2-s2.0-84893407444
"Li B., Lee-Urban S., Johnston G., Riedl M.O.","Story generation with crowdsourced plot graphs",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",40,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893368086&partnerID=40&md5=d1ba15c7319a7f0079e08b6491adb0b3","Story generation is the problem of automatically selecting a sequence of events that meet a set of criteria and can be told as a story. Story generation is knowledge-intensive; traditional story generators rely on a priori defined domain models about fictional worlds, including characters, places, and actions that can be performed. Manually authoring the domain models is costly and thus not scalable. We present a novel class of story generation system that can generate stories in an unknown domain. Our system (a) automatically learns a domain model by crowdsourcing a corpus of narrative examples and (b) generates stories by sampling from the space defined by the domain model. A large-scale evaluation shows that stories generated by our system for a previously unknown topic are comparable in quality to simple stories authored by untrained humans. © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Crowdsourcing; Domain model; Sequence of events; Story generation systems; Story generations; Artificial intelligence; Query languages",Conference Paper,Scopus,2-s2.0-84893368086
"Bunkóczi G., Echols N., McCoy A.J., Oeffner R.D., Adams P.D., Read R.J.","Phaser.MRage: Automated molecular replacement",2013,"Acta Crystallographica Section D: Biological Crystallography",40,10.1107/S0907444913022750,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887396993&doi=10.1107%2fS0907444913022750&partnerID=40&md5=cf5b06878e5ff8de9d710ba382ac514e","Phaser.MRage is a molecular-replacement automation framework that implements a full model-generation workflow and provides several layers of model exploration to the user. It is designed to handle a large number of models and can distribute calculations efficiently onto parallel hardware. In addition, phaser.MRage can identify correct solutions and use this information to accelerate the search. Firstly, it can quickly score all alternative models of a component once a correct solution has been found. Secondly, it can perform extensive analysis of identified solutions to find protein assemblies and can employ assembled models for subsequent searches. Thirdly, it is able to use a priori assembly information (derived from, for example, homologues) to speculatively place and score molecules, thereby customizing the search procedure to a certain class of protein molecule (for example, antibodies) and incorporating additional biological information into molecular replacement.",,"amino acid substitution; artificial intelligence; automation; biology; chemical structure; comparative study; computer program; conference paper; methodology; molecular replacement; phaser.MRage; pipeline; protein database; protein multimerization; protein tertiary structure; standard; X ray crystallography; automation; molecular replacement; phaser.MRage; pipeline; Amino Acid Substitution; Artificial Intelligence; Computational Biology; Crystallography, X-Ray; Databases, Protein; Models, Molecular; Protein Multimerization; Protein Structure, Tertiary; Software",Conference Paper,Scopus,2-s2.0-84887396993
"Smadja D., Touboul D., Cohen A., Doveh E., Santhiago M.R., Mello G.R., Krueger R.R., Colin J.","Detection of subclinical keratoconus using an automated decision tree classification",2013,"American Journal of Ophthalmology",40,10.1016/j.ajo.2013.03.034,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880569913&doi=10.1016%2fj.ajo.2013.03.034&partnerID=40&md5=1cd52d5f6d3e190c3842fc5a0b76b5a7","Purpose: To develop a method for automatizing the detection of subclinical keratoconus based on a tree classification. Design: Retrospective case-control study. Methods: setting: University Hospital of Bordeaux. participants: A total of 372 eyes of 197 patients were enrolled: 177 normal eyes of 95 subjects, 47 eyes of 47 patients with forme fruste keratoconus, and 148 eyes of 102 patients with keratoconus. observation procedure: All eyes were imaged with a dual Scheimpflug analyzer. Fifty-five parameters derived from anterior and posterior corneal measurements were analyzed for each eye and a machine learning algorithm, the classification and regression tree, was used to classify the eyes into the 3 above-mentioned conditions. main outcome measures: The performance of the machine learning algorithm for classifying eye conditions was evaluated, and the curvature, elevation, pachymetric, and wavefront parameters were analyzed in each group and compared. Results: The discriminating rules generated with the automated decision tree classifier allowed for discrimination between normal and keratoconus with 100% sensitivity and 99.5% specificity, and between normal and forme fruste keratoconus with 93.6% sensitivity and 97.2% specificity. The algorithm selected as the most discriminant variables parameters related to posterior surface asymmetry and thickness spatial distribution. Conclusion: The machine learning classifier showed very good performance for discriminating between normal corneas and forme fruste keratoconus and provided a tool that is closer to an automated medical reasoning. This might help in the surgical decision before refractive surgery by providing a good sensitivity in detecting ectasia-susceptible corneas. © 2013 Elsevier Inc. All rights reserved.",,"article; decision tree; human; keratoconus; machine learning; major clinical study; priority journal; sensitivity and specificity; Algorithms; Artificial Intelligence; Case-Control Studies; Corneal Topography; Decision Support Techniques; Decision Trees; Humans; Keratoconus; Retrospective Studies; Sensitivity and Specificity",Article,Scopus,2-s2.0-84880569913
"Lin M., Tang K., Yao X.","Dynamic sampling approach to training neural networks for multiclass imbalance classification",2013,"IEEE Transactions on Neural Networks and Learning Systems",40,10.1109/TNNLS.2012.2228231,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875898112&doi=10.1109%2fTNNLS.2012.2228231&partnerID=40&md5=fee80b644c5f1b0530f4665f4a0a7f34","Class imbalance learning tackles supervised learning problems where some classes have significantly more examples than others. Most of the existing research focused only on binary-class cases. In this paper, we study multiclass imbalance problems and propose a dynamic sampling method (DyS) for multilayer perceptrons (MLP). In DyS, for each epoch of the training process, every example is fed to the current MLP and then the probability of it being selected for training the MLP is estimated. DyS dynamically selects informative data to train the MLP. In order to evaluate DyS and understand its strength and weakness, comprehensive experimental studies have been carried out. Results on 20 multiclass imbalanced data sets show that DyS can outperform the compared methods, including pre-sample methods, active learning methods, cost-sensitive methods, and boosting-type methods. © 2012 IEEE.","Cost-sensitive learning; dynamic sampling; multiclass imbalance learning; multilayer perceptrons","Class imbalance learning; Cost-sensitive learning; Dynamic sampling; Dynamic sampling methods; Multi-class imbalanced datum; Multi-layer perceptrons; multiclass imbalance learning; Supervised learning problems; Artificial intelligence; Multilayer neural networks; Computer networks; algorithm; artificial neural network; classification; factual database; human; statistics and numerical data; Algorithms; Databases, Factual; Humans; Neural Networks (Computer)",Article,Scopus,2-s2.0-84875898112
"Galasso F., Cipolla R., Schiele B.","Video segmentation with superpixels",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",40,10.1007/978-3-642-37331-2_57,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875907931&doi=10.1007%2f978-3-642-37331-2_57&partnerID=40&md5=1a4ee022c262f7d6754f69d4dafcc8ab","Due to its importance, video segmentation has regained interest recently. However, there is no common agreement about the necessary ingredients for best performance. This work contributes a thorough analysis of various within- and between-frame affinities suitable for video segmentation. Our results show that a frame-based superpixel segmentation combined with a few motion and appearance-based affinities are sufficient to obtain good video segmentation performance. A second contribution of the paper is the extension of [1] to include motion-cues, which makes the algorithm globally aware of motion, thus improving its performance for video sequences. Finally, we contribute an extension of an established image segmentation benchmark [1] to videos, allowing coarse-to-fine video segmentations and multiple human annotations. Our results are tested on BMDS [2], and compared to existing methods. © 2013 Springer-Verlag.",,"Analysis of various; Appearance based; Coarse-to-fine; Common agreement; Human annotations; Superpixel segmentations; Video segmentation; Video sequences; Artificial intelligence; Image segmentation",Conference Paper,Scopus,2-s2.0-84875907931
"Sedykh A., Fourches D., Duan J., Hucke O., Garneau M., Zhu H., Bonneau P., Tropsha A.","Human intestinal transporter database: QSAR modeling and virtual profiling of drug uptake, efflux and interactions",2013,"Pharmaceutical Research",40,10.1007/s11095-012-0935-x,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876473356&doi=10.1007%2fs11095-012-0935-x&partnerID=40&md5=5ddddb3e061b014e0d5100bd4fc6d3f1","Purpose: Membrane transporters mediate many biological effects of chemicals and play a major role in pharmacokinetics and drug resistance. The selection of viable drug candidates among biologically active compounds requires the assessment of their transporter interaction profiles. Methods: Using public sources, we have assembled and curated the largest, to our knowledge, human intestinal transporter database (>5,000 interaction entries for >3,700 molecules). This data was used to develop thoroughly validated classification Quantitative Structure-Activity Relationship (QSAR) models of transport and/or inhibition of several major transporters including MDR1, BCRP, MRP1-4, PEPT1, ASBT, OATP2B1, OCT1, and MCT1. Results: QSAR models have been developed with advanced machine learning techniques such as Support Vector Machines, Random Forest, and k Nearest Neighbors using Dragon and MOE chemical descriptors. These models afforded high external prediction accuracies of 71-100% estimated by 5-fold external validation, and showed hit retrieval rates with up to 20-fold enrichment in the virtual screening of DrugBank compounds. Conclusions: The compendium of predictive QSAR models developed in this study can be used for virtual profiling of drug candidates and/or environmental agents with the optimal transporter profiles. © 2012 Springer Science+Business Media New York.","ADMET; drug transport; efflux; membrane transport proteins; permeability","apical sodium dependent bile acid transporter; breast cancer resistance protein; carrier protein; monocarboxylate transporter 1; multidrug resistance protein 1; multidrug resistance protein 2; multidrug resistance protein 3; multidrug resistance protein 4; organic anion transporter 2; organic anion transporter 2B; organic cation transporter 1; peptide transporter 1; unclassified drug; accuracy; article; controlled study; drug interaction; drug transport; drug uptake; human; Human Intestinal Transporter Database; k nearest neighbor; machine learning; prediction; priority journal; protein database; quantitative structure activity relation; random forest; support vector machine; validation study; Artificial Intelligence; Biological Transport, Active; Computer Simulation; Databases, Pharmaceutical; Databases, Protein; Humans; Intestines; Membrane Transport Proteins; Models, Biological; Pharmaceutical Preparations; Pharmacokinetics; Quantitative Structure-Activity Relationship",Article,Scopus,2-s2.0-84876473356
"Georgiadis P., Athanasiou E.","Flexible long-term capacity planning in closed-loop supply chains with remanufacturing",2013,"European Journal of Operational Research",40,10.1016/j.ejor.2012.09.021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868483649&doi=10.1016%2fj.ejor.2012.09.021&partnerID=40&md5=96b54efec89cddc424714b7e47b944cc","We deal with long-term demand-driven capacity planning policies in the reverse channel of closed-loop supply chains (CLSCs) with remanufacturing, under high capacity acquisition cost coupled with uncertainty in actual demand, sales patterns, quality and timing of end-of-use product returns. The objective is to facilitate the decision-making when the management faces the dilemma of implementing either a strategy of early large-scale investments to benefit from economies of scale and capacity readiness, or a flexible strategy of low volume but more frequent capacity expansions. We consider a CLSC with two sequential product-types. We study the system's response in terms of transient flows, actual/desired capacity level, capacity expansions/contractions and total supply chain profit, employing a simulation-based system dynamics optimization approach. Extensive numerical investigation covers a broad range of real-world remanufacturable products under alternative scenarios in relation to the market preference over product-types. The key findings propose flexible policies as improved alternatives to large-scale capacity expansions/contractions in terms of adaptability to the actual pattern of end-of-use product returns and involved risk in the investments' turnover. Flexible policies are also proposed as practices to avoid overcapacity phenomena in collection and remanufacturing capacity and as robust policies to product demand. Their implementation is revealed to be even more important for the case of remanufacturing, when a high capacity acquisition unit-cost ratio (remanufacturing/collection) is coupled with strong economies of scale. Finally, results under different information sharing structures show changes in remanufacturing policies, thus justifying the importance of coordination between the decision-maker and the distributor. © 2012 Elsevier B.V. All rights reserved.","Capacity planning; Closed-loop supply chains; Decision support system; Supply chain management; System dynamics","Acquisition costs; Capacity expansion; Capacity planning; Closed-loop supply chain; Decision makers; Economies of scale; Flexible strategies; High capacity; Information sharing; Numerical investigations; Optimization approach; Over capacity; Product demand; Product returns; Remanufacturable products; Reverse channels; System Dynamics; Transient flow; Artificial intelligence; Decision support systems; Profitability; Supply chain management; System theory; Economics",Article,Scopus,2-s2.0-84868483649
"Sacchelli S., De Meo I., Paletto A.","Bioenergy production and forest multifunctionality: A trade-off analysis using multiscale GIS model in a case study in Italy",2013,"Applied Energy",40,10.1016/j.apenergy.2012.11.038,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870817600&doi=10.1016%2fj.apenergy.2012.11.038&partnerID=40&md5=aa4b9b922b16963d1ebccbd14b2b98a2","Environmental assessment needs of Decision Support Systems (DSSs) able to consider several aspects in a unique analysis framework. The complexity of interaction among ecological, economic and political variables and a widespread lack of data availability lead to difficulty in bringing together large scale analysis and local planning systems. This loop can be solved through flexible tools able to relate large scale environmental assessment with medium and small scale DSS, useful for local decisions makers.The present research aims at creating a spatial model, based on Geographic Information Systems (GISs), capable to quantify the potential amount of woody biomass from forest sector at several evaluation scales, to consider the theoretical impact of biomass removal on forest multifunctionality and to estimate the potential trade-off between forest functions in case of bioenergy chain development.The model is carried out basing on multifunctionality aspects and their potential relationship with biomass energy production. The forest functions considered in the model are: (i) soil and water protection, (ii) biodiversity and habitat conservation, (iii) fire risk prevention, (iv) tourist and recreational function and (v) economic evaluation related to timber and bioenergy processing. The structure of the model is based on sub-models that enable biomass chain analysis from large scale to small scale. Large scale analysis is developed using forest yield data, protected areas and main and forest roads localization, geomorphological variables and fire risk maps. An increase of input data (economic and logistic variables) is requested for medium and small scale analysis. The model is tested from national (Italy) to regional (Tuscany region in Central Italy) and provincial scale (province of Trento in North-Eastern Italian Alps). Results stress how the model can be able to depict territorial differences in several contexts and to consider respective influence on estimation of biomass availability. Finally, the Compromise Programming (CP) methodology permits defining the optimal quantity of residues removal in different compartments according to priority forest function. © 2012 Elsevier Ltd.","Forest bioenergy; GIS modeling; Multifunctionality trade-off; Multiscale analysis","Artificial intelligence; Availability; Biodiversity; Biomass; Decision support systems; Fire protection; Forestry; Geographic information systems; Geologic models; Maps; Soil conservation; Statistics; Bio-energy; Bio-mass energy; Bioenergy chains; Biomass availability; Biomass removal; Central Italy; Chain analysis; Compromise programming; Data availability; Economic evaluations; Environmental assessment; Fire risks; Flexible tool; Forest function; Forest roads; Forest sectors; GIS modeling; GIS models; Habitat conservation; Input datas; Large-scale analysis; Local decisions; Local planning; Multi scale analysis; Multifunctionality; Multiscales; Optimal quantity; Political variables; Protected areas; Small scale; Soil and water; Spatial models; Submodels; Trade-off analysis; Tuscany; Woody biomass; Economic analysis; bioenergy; biomass power; data set; decision support system; economic analysis; environmental assessment; GIS; model test; numerical model; planning system; power generation; trade-off; Italy",Article,Scopus,2-s2.0-84870817600
"Xiao J., Ao X.-T., Tang Y.","Solving software project scheduling problems with ant colony optimization",2013,"Computers and Operations Research",40,10.1016/j.cor.2012.05.007,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866151445&doi=10.1016%2fj.cor.2012.05.007&partnerID=40&md5=dbd4f8c071959c90ad73a4f2dd9db6b8","Software project scheduling problem (SPSP) is one of the important and challenging problems faced by the software project managers in the highly competitive software industry. As the problem is becoming an NP-hard problem with the increasing numbers of employees and tasks, only a few algorithms exist and the performance is still not satisfying. To design an effective algorithm for SPSP, this paper proposes an ant colony optimization (ACO) approach which is called ACS-SPSP algorithm. Since a task in software projects involves several employees, in this paper, by splitting tasks and distributing dedications of employees to task nodes we get the construction graph for ACO. Six domain-based heuristics are designed to consider the factors of task efforts, allocated dedications of employees and task importance. Among these heuristic strategies, the heuristic of allocated dedications of employees to other tasks performs well. ACS-SPSP is compared with a genetic algorithm to solve the SPSP on 30 random instances. Experimental results show that the proposed algorithm is promising and can obtain higher hit rates with more accuracy compared to the previous genetic algorithm solution. © 2012 Elsevier Ltd. All rights reserved.","Ant colony optimization; Automatic software management; Scheduling; Software project scheduling","Algorithm solution; Ant Colony Optimization (ACO); Effective algorithms; Heuristic strategy; Hit rate; Random instance; Software industry; Software management; Software project; Artificial intelligence; Computational complexity; Genetic algorithms; Software engineering; Scheduling",Article,Scopus,2-s2.0-84866151445
"Bocchi L., Chen T.-C., Demangeon R., Honda K., Yoshida N.","Monitoring networks through multiparty session types",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",39,10.1007/978-3-642-38592-6_5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884991933&doi=10.1007%2f978-3-642-38592-6_5&partnerID=40&md5=74f805d9166e1bd67161a776efecdf63","In large-scale distributed infrastructures, applications are realised through communications among distributed components. The need for methods for assuring safe interactions in such environments is recognized, however the existing frameworks, relying on centralised verification or restricted specification methods, have limited applicability. This paper proposes a new theory of monitored π-calculus with dynamic usage of multiparty session types (MPST), offering a rigorous foundation for safety assurance of distributed components which asynchronously communicate through multiparty sessions. Our theory establishes a framework for semantically precise decentralised run-time enforcement and provides reasoning principles over monitored distributed applications, which complement existing static analysis techniques. We introduce asynchrony through the means of explicit routers and global queues, and propose novel equivalences between networks, that capture the notion of interface equivalence, i.e. equating networks offering the same services to a user. We illustrate our static-dynamic analysis system with an ATM protocol as a running example and justify our theory with results: satisfaction equivalence, local/global safety and transparency, and session fidelity. © 2013 IFIP International Federation for Information Processing.",,"Analysis techniques; Distributed applications; Distributed components; Distributed infrastructure; Monitoring network; Multiparty sessions; Reasoning principles; Safety assurance; Artificial intelligence; Computer science",Conference Paper,Scopus,2-s2.0-84884991933
"Zou Q., Wang Z., Guan X., Liu B., Wu Y., Lin Z.","An approach for identifying cytokines based on a novel ensemble classifier",2013,"BioMed Research International",39,10.1155/2013/686090,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884255470&doi=10.1155%2f2013%2f686090&partnerID=40&md5=710702fa4fc928662502ae99f3062383","Biology is meaningful and important to identify cytokines and investigate their various functions and biochemical mechanisms. However, several issues remain, including the large scale of benchmark datasets, serious imbalance of data, and discovery of new gene families. In this paper, we employ the machine learning approach based on a novel ensemble classifier to predict cytokines. We directly selected amino acids sequences as research objects. First, we pretreated the benchmark data accurately. Next, we analyzed the physicochemical properties and distribution of whole amino acids and then extracted a group of 120-dimensional (120D) valid features to represent sequences. Third, in the view of the serious imbalance in benchmark datasets, we utilized a sampling approach based on the synthetic minority oversampling technique algorithm and K-means clustering undersampling algorithm to rebuild the training set. Finally, we built a library for dynamic selection and circulating combination based on clustering (LibD3C) and employed the new training set to realize cytokine classification. Experiments showed that the geometric mean of sensitivity and specificity obtained through our approach is as high as 93.3%, which proves that our approach is effective for identifying cytokines. © 2013 Quan Zou et al.",,"amino acid; cytokine; cytokine; amino acid; amino acid composition; amino acid sequence; article; classification; classifier; computer program; machine learning; physical chemistry; protein analysis; sensitivity and specificity; algorithm; artificial intelligence; biology; chemistry; classification; human; isolation and purification; nucleotide sequence; chemistry; isolation and purification; Algorithms; Amino Acids; Artificial Intelligence; Base Sequence; Computational Biology; Cytokines; Humans; Sensitivity and Specificity; Algorithms; Amino Acids; Artificial Intelligence; Base Sequence; Computational Biology; Cytokines; Humans; Sensitivity and Specificity",Article,Scopus,2-s2.0-84884255470
"Nefeslioglu H.A., Sezer E.A., Gokceoglu C., Ayas Z.","A modified analytical hierarchy process (M-AHP) approach for decision support systems in natural hazard assessments",2013,"Computers and Geosciences",39,10.1016/j.cageo.2013.05.010,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879454846&doi=10.1016%2fj.cageo.2013.05.010&partnerID=40&md5=ee2c895452955fb1b7faa86271df707c","The Analytical Hierarchy Process (AHP) is a classic and powerful decision support tool. However, the conventional AHP has some disadvantages originating in the expert decision-making process. To minimize the disadvantages of the conventional AHP, a modified analytical hierarchy process (M-AHP), is suggested in this study. This study is conducted in three stages: (i) the theoretical background for the conventional AHP is introduced, (ii) essentials for the proposed M-AHP technique are given with an example solution for the evaluation of snow avalanche source susceptibility, and (iii) a computer code named M-AHP is presented. By applying the methodology suggested in this study, the consistency ratio value for the comparison matrix and the weight vector never exceeds 0.10. The M-AHP program is a complementary tool for natural hazard, natural resource, or nature preservation researchers who apply the M-AHP technique to their decision support problem. © 2013 Elsevier Ltd.","Avalanche susceptibility; Decision support system; Modified analytical hierarchy process (M-AHP); Natural hazard; Uncertainty","Analytical Hierarchy Process; Complementary tools; Consistency ratio; Decision making process; Decision support problem; Decision support tools; Natural hazard; Uncertainty; Artificial intelligence; Decision support systems; Hazards; Hierarchical systems; analytical hierarchy process; decision support system; hazard assessment; natural hazard; snow avalanche; uncertainty analysis",Article,Scopus,2-s2.0-84879454846
"Das S., Biswas S., Kundu S.","Synergizing fitness learning with proximity-based food source selection in artificial bee colony algorithm for numerical optimization",2013,"Applied Soft Computing Journal",39,10.1016/j.asoc.2013.07.009,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886723307&doi=10.1016%2fj.asoc.2013.07.009&partnerID=40&md5=21aaec8ff415e04e8fa37eb572bc783e","Evolutionary computation (EC) paradigm has undergone extensions in the recent years diverging from the natural process of genetic evolution to the simulation of natural life processes exhibited by the living organisms. Bee colonies exemplify a high level of intrinsic interdependence and co-ordination among its members, and algorithms inspired from the bee colonies have gained recent prominence in the field of swarm based metaheuristics. The artificial bee colony (ABC) algorithm was recently developed, by simulating the minimalistic foraging model of honeybees in search of food sources, for solving real-parameter, non-convex, and non-smooth optimization problems. The single parameter perturbation in classical ABC resulted in fairly commendable performance for simple problems without epistasis of variables (separable). However, it suffered from narrow search zone and slow convergence which eventually led to poor exploitation tendency. Even with the increase in dimensionality, a significant deterioration was observed in the ability of ABC to locate the optimum in a huge search volume. Some of the probable shortcomings in the basic ABC approach, as observed, are the single parameter perturbation instead of a multiple one, ignoring the fitness to reward ratio while selecting food sites, and most importantly the absence of environmental factors in the algorithm design. Research has shown that spatial environmental factors play a crucial role in insect locomotion and foragers seem to learn the direction to be undertaken based on the relative analysis of its proximal surroundings. Most importantly, the mapping of the forager locomotion from three dimensional search spaces to a multidimensional solution space calls forth the implementation of multiple modification schemes. Based on the fundamental observation pertaining to the dynamics of ABC, this article proposes an improved variant of ABC aimed at improving the optimizing ability of the algorithm over an extended set of problems. The hybridization of the proposed fitness learning mechanism with a weighted selection scheme and proximity based stimuli helps to achieve a fine blending of explorative and exploitative behaviour by enhancing both local and global searching ability of the algorithm. This enhances the ability of the swarm agents to detect optimal regions in the unexplored fitness basins. With respect to its immediate surroundings, a proximity based component is added to the normal positional modification of the onlookers and is enacted through an improved probability selection scheme that takes the T/E (total reward to distance) ratio metric into account. The biologically-motivated, hybridized variant of ABC achieves a statistically superior performance on majority of the tested benchmark instances, as compared to some of the most prominent state-of-the-art algorithms, as is demonstrated through a detailed experimental evaluation and verified statistically. © 2013 Elsevier B.V. All rights reserved.","Artificial bee colony algorithm; Biologically-inspired optimization; Exploitation; Exploration; Fitness learning; Swarm intelligence","Artificial bee colony algorithms; Biologically-inspired; Exploitation; Fitness learning; Swarm Intelligence; Artificial intelligence; Benchmarking; Biology; Blending; Evolutionary algorithms; Natural resources exploration; Optimization; Health",Article,Scopus,2-s2.0-84886723307
"Shiri J., Kisi O., Yoon H., Lee K.-K., Hossein Nazemi A.","Predicting groundwater level fluctuations with meteorological effect implications-A comparative study among soft computing techniques",2013,"Computers and Geosciences",39,10.1016/j.cageo.2013.01.007,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876308067&doi=10.1016%2fj.cageo.2013.01.007&partnerID=40&md5=ec64f15d9c8fd72aca17b99bff0b631d","The knowledge of groundwater table fluctuations is important in agricultural lands as well as in the studies related to groundwater utilization and management levels. This paper investigates the abilities of Gene Expression Programming (GEP), Adaptive Neuro-Fuzzy Inference System (ANFIS), Artificial Neural Networks (ANN) and Support Vector Machine (SVM) techniques for groundwater level forecasting in following day up to 7-day prediction intervals. Several input combinations comprising water table level, rainfall and evapotranspiration values from Hongcheon Well station (South Korea), covering a period of eight years (2001-2008) were used to develop and test the applied models. The data from the first six years were used for developing (training) the applied models and the last two years data were reserved for testing. A comparison was also made between the forecasts provided by these models and the Auto-Regressive Moving Average (ARMA) technique. Based on the comparisons, it was found that the GEP models could be employed successfully in forecasting water table level fluctuations up to 7 days beyond data records. © 2013 Elsevier Ltd.","ARMA; Artificial intelligence techniques; Groundwater level fluctuations; Prediction","Adaptive neuro-fuzzy inference system; ARMA; Artificial intelligence techniques; Autoregressive moving average; Gene expression programming; Groundwater level fluctuation; Groundwater level forecasting; Support vector machine techniques; Forecasting; Neural networks; Soft computing; Support vector machines; Water supply; Groundwater; artificial intelligence; artificial neural network; comparative study; evapotranspiration; gene expression; groundwater; prediction; rainfall; water level; water table; Hongcheon; Kangwon; South Korea",Article,Scopus,2-s2.0-84876308067
"Yang Z., Jacob M.","Nonlocal regularization of inverse problems: A unified variational framework",2013,"IEEE Transactions on Image Processing",39,10.1109/TIP.2012.2216278,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879041380&doi=10.1109%2fTIP.2012.2216278&partnerID=40&md5=1c5f66762e820b4ca6641f8baef8e85d","We introduce a unifying energy minimization framework for nonlocal regularization of inverse problems. In contrast to the weighted sum of square differences between image pixels used by current schemes, the proposed functional is an unweighted sum of inter-patch distances. We use robust distance metrics that promote the averaging of similar patches, while discouraging the averaging of dissimilar patches. We show that the first iteration of a majorize-minimize algorithm to minimize the proposed cost function is similar to current nonlocal methods. The reformulation thus provides a theoretical justification for the heuristic approach of iterating nonlocal schemes, which re-estimate the weights from the current image estimate. Thanks to the reformulation, we now understand that the widely reported alias amplification associated with iterative nonlocal methods are caused by the convergence to local minimum of the nonconvex penalty. We introduce an efficient continuation strategy to overcome this problem. The similarity of the proposed criterion to widely used nonquadratic penalties (e.g., total variation and ℓp semi-norms) opens the door to the adaptation of fast algorithms developed in the context of compressive sensing; we introduce several novel algorithms to solve the proposed nonlocal optimization problem. Thanks to the unifying framework, these fast algorithms are readily applicable for a large class of distance metrics. © 1992-2012 IEEE.","Compressed sensing; inverse problems; nonconvex; nonlocal means","Compressive sensing; Energy minimization; Majorize-minimize algorithms; Non-local means; Non-local regularization; Nonconvex; Optimization problems; Variational framework; Algorithms; Compressed sensing; Differential equations; Heuristic methods; Inverse problems; Signal reconstruction; Iterative methods; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; image enhancement; methodology; reproducibility; sensitivity and specificity; automated pattern recognition; computer assisted diagnosis; image enhancement; procedures; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84879041380
"Giuliano V., Giuliano C.","Improved breast cancer detection in asymptomatic women using 3D-automated breast ultrasound in mammographically dense breasts",2013,"Clinical Imaging",39,10.1016/j.clinimag.2012.09.018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875720872&doi=10.1016%2fj.clinimag.2012.09.018&partnerID=40&md5=5875fe4084fe794f4def6cab7350aaff","Automated breast ultrasound (ABUS)was performed in 3418 asymptomatic women with mammographically dense breasts. The addition of ABUS to mammography in women with greater than 50% breast density resulted in the detection of 12.3 per 1,000 breast cancers, compared to 4.6 per 1,000 by mammography alone. The mean tumor size was 14.3 mm and overall attributable risk of breast cancer was 19.92 (95% confidence level, 16.75 - 23.61) in our screened population. These preliminary results may justify the cost-benefit of implementing the judicious us of ABUS in conjunction with mammography in the dense breast screening population. © 2013 Elsevier Inc.","3-dimensional sonography; Breast cancer; Breast screening; Breast ultrasound; Cancer detection; Dense breast","3-dimensional sonography; Breast Cancer; Breast screening; Breast ultrasound; Cancer detection; Dense breast; Diagnosis; Diseases; Ultrasonics; Ultrasonic applications; adult; aged; article; breast cancer; cancer diagnosis; cancer screening; controlled study; cost benefit analysis; diagnostic imaging; diagnostic test accuracy study; diagnostic value; digital mammography; echomammography; female; human; intermethod comparison; major clinical study; mammography; predictive value; priority journal; reliability; risk factor; sensitivity and specificity; tumor volume; Adult; Aged; Aged, 80 and over; Algorithms; Artificial Intelligence; Breast Neoplasms; Densitometry; Female; Florida; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Middle Aged; Pattern Recognition, Automated; Prevalence; Reproducibility of Results; Risk Assessment; Sensitivity and Specificity; Ultrasonography, Mammary; Young Adult",Article,Scopus,2-s2.0-84875720872
"Pang Y., Ji Z., Jing P., Li X.","Ranking graph embedding for learning to rerank",2013,"IEEE Transactions on Neural Networks and Learning Systems",39,10.1109/TNNLS.2013.2253798,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880918462&doi=10.1109%2fTNNLS.2013.2253798&partnerID=40&md5=0f1e7e7aab2b056ee1215f6561d80a0e","Dimensionality reduction is a key step to improving the generalization ability of reranking in image search. However, existing dimensionality reduction methods are typically designed for classification, clustering, and visualization, rather than for the task of learning to rank. Without using of ranking information such as relevance degree labels, direct utilization of conventional dimensionality reduction methods in ranking tasks generally cannot achieve the best performance. In this paper, we show that introducing ranking information into dimensionality reduction significantly increases the performance of image search reranking. The proposed method transforms graph embedding, a general framework of dimensionality reduction, into ranking graph embedding (RANGE) by modeling the global structure and the local relationships in and between different relevance degree sets, respectively. The proposed method also defines three types of edge weight assignment between two nodes: binary, reconstruction, and global. In addition, a novel principal components analysis based similarity calculation method is presented in the stage of global graph construction. Extensive experimental results on the MSRA-MM database demonstrate the effectiveness and superiority of the proposed RANGE method and the image search reranking framework. © 2012 IEEE.","Dimensionality reduction; graph embedding; image search reranking; learning to rank","Dimensionality reduction; Dimensionality reduction method; Generalization ability; Graph embeddings; Image search reranking; Learning to rank; Principal components analysis; Similarity calculation; Artificial intelligence; Computer networks; Principal component analysis",Article,Scopus,2-s2.0-84880918462
"Yeh W.-C.","New parameter-free simplified swarm optimization for artificial neural network training and its application in the prediction of time series",2013,"IEEE Transactions on Neural Networks and Learning Systems",39,10.1109/TNNLS.2012.2232678,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875897462&doi=10.1109%2fTNNLS.2012.2232678&partnerID=40&md5=9649bcb7ae8efa1e85fafde2aa783fad","A new soft computing method called the parameter-free simplified swarm optimization (SSO)-based artificial neural network (ANN), or improved SSO for short, is proposed to adjust the weights in ANNs. The method is a modification of the SSO, and seeks to overcome some of the drawbacks of SSO. In the experiments, the iSSO is compared with five other famous soft computing methods, including the backpropagation algorithm, the genetic algorithm, the particle swarm optimization (PSO) algorithm, cooperative random learning PSO, and the SSO, and its performance is tested on five famous time-series benchmark data to adjust the weights of two ANN models (multilayer perceptron and single multiplicative neuron model). The experimental results demonstrate that iSSO is robust and more efficient than the other five algorithms. © 2012 IEEE.","Artificial intelligence; evolutionary computation; machine learning; neural network","Benchmark data; ITS applications; Multi layer perceptron; Particle swarm optimization algorithm; Random learning; Simplified swarm optimizations (SSO); Single multiplicative neuron models; Soft computing methods; Algorithms; Artificial intelligence; Benchmarking; Evolutionary algorithms; Learning systems; Particle swarm optimization (PSO); Soft computing; Neural networks; artificial intelligence; artificial neural network; computer analysis; forecasting; time; trends; Artificial Intelligence; Computing Methodologies; Forecasting; Neural Networks (Computer); Time Factors",Article,Scopus,2-s2.0-84875897462
"Saba T., Rehman A.","Effects of artificially intelligent tools on pattern recognition",2013,"International Journal of Machine Learning and Cybernetics",39,10.1007/s13042-012-0082-z,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874830216&doi=10.1007%2fs13042-012-0082-z&partnerID=40&md5=4c0df396e0664264fe3809436402622b","Pattern recognition is classification process that attempts to assign each input value to one of a given set of classes. The process of pattern recognition in the state of art has been achieved either by training of artificially intelligent tools or using heuristic rule based approaches. The objective of this paper is to provide a comparative study between artificially trained and heuristics rule based techniques employed for pattern recognition in the state of the art focused on script pattern recognition. It is observed that mainly there are two categories of script pattern recognition techniques. First category involves assistance of artificial intelligent learning and next, is based on heuristic-rules for cursive script pattern segmentation/recognition. Accordingly, a detailed critical study is performed that focuses on size of training/testing data and implication of artificial learning on script pattern recognition accuracy. Moreover, the techniques are described in details that are employed to identify character patterns. Finally, performances of different techniques on benchmark database are compared regarding pattern recognition accuracy, error rate, single or multiple classifiers being employed. Problems that still persist are also highlighted and possible directions are set. © 2012 Springer-Verlag.","Benchmark database; Character segmentation; Features extraction; Genetic algorithm; Heuristic-rule based approaches; Neural validation; Pattern recognition","Benchmark database; Character segmentation; Features extraction; Heuristic-rule based approaches; Neural validation; Artificial intelligence; Genetic algorithms; Pattern recognition",Article,Scopus,2-s2.0-84874830216
"Fleckenstein M., Fischer S., Bohlen O., Bäker B.","Thermal Impedance Spectroscopy - A method for the thermal characterization of high power battery cells",2013,"Journal of Power Sources",39,10.1016/j.jpowsour.2012.07.144,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867389270&doi=10.1016%2fj.jpowsour.2012.07.144&partnerID=40&md5=d0287913dc5d16a55eb5298e179f227a","A new approach of Thermal Impedance Spectroscopy (TIS) is introduced for thermal characterization of battery cells. It examines the transfer behavior between internal heat generation and resulting battery surface temperature in the frequency domain. Compared to previously published TIS methods the internal heat generation forced by electrical operation can be determined without any previous knowledge or assumptions on the electric/electrochemical behavior of the battery cell. The complete procedure is demonstrated by the TIS-application on a cylindrical High-Power Li-Ion cell. By the use of a thermal battery model, its theoretical transfer function can be fitted to the TIS-measurement results in the Nyquist-Plot. Consequently, the specific heat capacity and the heat conductivity of the cell's jelly roll can be derived. A comparison of the exemplary operated TIS to conventional thermal characterization methods shows a difference of 5% in the heat capacity and 12% in the heat conductivity determination. Future improvements on the experimental setup are suggested in order to reach a higher measurement accuracy and additionally, the systematic advantages of the easy to operate and non-destructive TIS-method are presented. © 2012 Elsevier B.V. All rights reserved.","Battery modeling; Heat conductivity measurement; Thermal characterization; Thermal Impedance Spectroscopy","Battery cells; Battery modeling; Conductivity measurements; Frequency domains; High power battery; Internal heat generation; Li-ion cells; Measurement accuracy; Non destructive; Surface temperatures; Thermal batteries; Thermal characterization; Thermal impedance; Atmospheric temperature; Cells; Cytology; Electric batteries; Specific heat; Spectroscopy; Thermal conductivity; Artificial intelligence",Article,Scopus,2-s2.0-84867389270
"Paneque-Gálvez J., Mas J.-F., Moré G., Cristóbal J., Orta-Martínez M., Luz A.C., Guèze M., Macía M.J., Reyes-García V.","Enhanced land use/cover classification of heterogeneous tropical landscapes using support vector machines and textural homogeneity",2013,"International Journal of Applied Earth Observation and Geoinformation",39,10.1016/j.jag.2012.10.007,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880268563&doi=10.1016%2fj.jag.2012.10.007&partnerID=40&md5=8c36fc2db9503b7597b15d80d5652b3a","Land use/cover classification is a key research field in remote sensing and land change science as thematic maps derived from remotely sensed data have become the basis for analyzing many socio-ecological issues. However, land use/cover classification remains a difficult task and it is especially challenging in heterogeneous tropical landscapes where nonetheless such maps are of great importance. The present study aims at establishing an efficient classification approach to accurately map all broad land use/cover classes in a large, heterogeneous tropical area, as a basis for further studies (e.g., land use/cover change, deforestation and forest degradation). Specifically, we first compare the performance of parametric (maximum likelihood), non-parametric (k-nearest neighbor and four different support vector machines - SVM), and hybrid (unsupervised-supervised) classifiers, using hard and soft (fuzzy) accuracy assessments. We then assess, using the maximum likelihood algorithm, what textural indices from the gray-level co-occurrence matrix lead to greater classification improvements at the spatial resolution of Landsat imagery (30 m), and rank them accordingly. Finally, we use the textural index that provides the most accurate classification results to evaluate whether its usefulness varies significantly with the classifier used. We classified imagery corresponding to dry and wet seasons and found that SVM classifiers outperformed all the rest. We also found that the use of some textural indices, but particularly homogeneity and entropy, can significantly improve classifications. We focused on the use of the homogeneity index, which has so far been neglected in land use/cover classification efforts, and found that this index along with reflectance bands significantly increased the overall accuracy of all the classifiers, but particularly of SVM. We observed that improvements in producer's and user's accuracies through the inclusion of homogeneity were different depending on land use/cover classes. Early-growth/degraded forests, pastures, grasslands and savanna were the classes most improved, especially with the SVM radial basis function and SVM sigmoid classifiers, though with both classifiers all land use/cover classes were mapped with producer's and user's accuracies of ~90%. Our classification approach seems very well suited to accurately map land use/cover of heterogeneous landscapes, thus having great potential to contribute to climate change mitigation schemes, conservation initiatives, and the design of management plans and rural development policies. © 2012 Elsevier B.V.","Bolivian Amazon; Hybrid classification; k-Nearest neighbor; Remote sensing; SVM; Texture; Thematic classification comparison","accuracy assessment; algorithm; artificial intelligence; land classification; land cover; land use change; Landsat; landscape; maximum likelihood analysis; nearest neighbor analysis; remote sensing; spatial resolution; Amazonia; Bolivia",Article,Scopus,2-s2.0-84880268563
"Girolami A., Napolitano F., Faraone D., Braghieri A.","Measurement of meat color using a computer vision system",2013,"Meat Science",39,10.1016/j.meatsci.2012.08.010,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866983741&doi=10.1016%2fj.meatsci.2012.08.010&partnerID=40&md5=c1c3ec5458f3524922642bca9398f5aa","The limits of the colorimeter and a technique of image analysis in evaluating the color of beef, pork, and chicken were investigated. The Minolta CR-400 colorimeter and a computer vision system (CVS) were employed to measure colorimetric characteristics. To evaluate the chromatic fidelity of the image of the sample displayed on the monitor, a similarity test was carried out using a trained panel. The panelists found the digital images of the samples visualized on the monitor very similar to the actual ones (P<0.001). During the first similarity test the panelists observed at the same time both the actual meat sample and the sample image on the monitor in order to evaluate the similarity between them (test A). Moreover, the panelists were asked to evaluate the similarity between two colors, both generated by the software Adobe Photoshop CS3 one using the L*, a* and b* values read by the colorimeter and the other obtained using the CVS (test B); which of the two colors was more similar to the sample visualized on the monitor was also assessed (test C). The panelists found the digital images very similar to the actual samples (P<0.001). As to the similarity (test B) between the CVS- and colorimeter-based colors the panelists found significant differences between them (P<0.001). Test C showed that the color of the sample on the monitor was more similar to the CVS generated color than to the colorimeter generated color. The differences between the values of the L*, a*, b*, hue angle and chroma obtained with the CVS and the colorimeter were statistically significant (P<0.05-0.001). These results showed that the colorimeter did not generate coordinates corresponding to the true color of meat. Instead, the CVS method seemed to give valid measurements that reproduced a color very similar to the real one. © 2012 Elsevier Ltd.","Colorimeter; Computer vision; Image analysis; Meat color","Adobe Photoshop; Computer vision system; Digital image; Meat color; Meat samples; Minolta; True colors; Two-color; Colorimeters; Colorimetry; Computer vision; Image analysis; Meats; Testing; Color; animal; article; artificial intelligence; cattle; chicken; color; colorimetry; human; meat; methodology; swine; validation study; Animals; Artificial Intelligence; Cattle; Chickens; Color; Colorimetry; Humans; Meat; Swine",Article,Scopus,2-s2.0-84866983741
"Shibl R., Lawley M., Debuse J.","Factors influencing decision support system acceptance",2013,"Decision Support Systems",39,10.1016/j.dss.2012.09.018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871714951&doi=10.1016%2fj.dss.2012.09.018&partnerID=40&md5=8334d8612b7ffc67cdc6e2f6e168a780","While clinical DSS have many proven benefits, their uptake by GPs (general practitioners) is limited. The purpose of this research was to develop and explore a UTAUT (Unified Theory of Acceptance and Use of Technology) based model of how and why GPs accept DSS. Insight into the reasons why GPs do not use clinical DSS combined with knowledge of why GPs use DSS will allow the development of strategies to facilitate more widespread adoption with consequent improvements across many areas. Depth interviews were conducted with 37 GPs comprising a mix of education backgrounds, experience and gender. The developed model indicated that four main factors influence DSS acceptance and use including usefulness (incorporating consultation issue, professional development and patient presence), facilitating conditions (incorporating workflow, training and integration), ease of use and trust in the knowledge base. © 2012 Elsevier B.V.","Decision support systems; General practitioners; Technology acceptance; UTAUT","Decision supports; Developed model; Ease-of-use; Facilitating conditions; General practitioners; Knowledge base; Professional development; Technology acceptance; Unified theory of acceptance and use of technology; UTAUT; Artificial intelligence; Knowledge based systems; Technology; Decision support systems",Article,Scopus,2-s2.0-84871714951
"Ogliari E., Grimaccia F., Leva S., Mussetta M.","Hybrid predictive models for accurate forecasting in PV systems",2013,"Energies",39,10.3390/en6041918,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877329495&doi=10.3390%2fen6041918&partnerID=40&md5=6d6d6033a7cef5ec0aaaff75cd64a447","The accurate forecasting of energy production from renewable sources represents an important topic also looking at different national authorities that are starting to stimulate a greater responsibility towards plants using non-programmable renewables. In this paper the authors use advanced hybrid evolutionary techniques of computational intelligence applied to photovoltaic systems forecasting, analyzing the predictions obtained by comparingdifferent definitions of the forecasting error. © 2013 by the authors.","Artificial intelligence; Hybrid techniques; Neural networks; PV forecasting","Artificial intelligence; Neural networks; Photovoltaic cells; Energy productions; Evolutionary techniques; Forecasting error; Hybrid techniques; Photovoltaic systems; Predictive models; Renewable sources; Renewables; Forecasting",Article,Scopus,2-s2.0-84877329495
"Hoffart J., Suchanek F.M., Berberich K., Weikum G.","YAGO2: A spatially and temporally enhanced knowledge base from Wikipedia",2013,"IJCAI International Joint Conference on Artificial Intelligence",38,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062289&partnerID=40&md5=f959430a7a2bdae78ed507f7ada7ad98","We present YAGO2, an extension of the YAGO knowledge base, in which entities, facts, and events are anchored in both time and space. YAGO2 is built automatically from Wikipedia, GeoNames, and WordNet. It contains 447 million facts about 9.8 million entities. Human evaluation confirmed an accuracy of 95% of the facts in YAGO2. In this paper, we present the extraction methodology and the integration of the spatio-temporal dimension.",,"Human evaluation; Knowledge base; Spatio-temporal dimensions; Wikipedia; Wordnet; Artificial intelligence; Knowledge based systems",Conference Paper,Scopus,2-s2.0-84896062289
"Artale A., Kontchakov R., Wolter F., Zakharyaschev M.","Temporal description logic for ontology-based data access",2013,"IJCAI International Joint Conference on Artificial Intelligence",38,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063727&partnerID=40&md5=5c191412d58b4ee1913e312063b9ce76","Our aim is to investigate ontology-based data access over temporal data with validity time and ontologies capable of temporal conceptual modelling. To this end, we design a temporal description logic, TQL, that extends the standard ontology language OWL2QL, provides basic means for temporal conceptual modelling and ensures firstorder rewritability of conjunctive queries for suitably defined data instances with validity time.",,"Conceptual modelling; Conjunctive queries; Description logic; First-order; Ontology-based data access; Standard ontology language; Temporal Data; Artificial intelligence; Formal languages; Ontology; Data description",Conference Paper,Scopus,2-s2.0-84896063727
"Leydesdorff L., Rafols I., Chen C.","Interactive overlays of journals and the measurement of interdisciplinarity on the basis of aggregated journal-journal citations",2013,"Journal of the American Society for Information Science and Technology",38,10.1002/asi.22946,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887460767&doi=10.1002%2fasi.22946&partnerID=40&md5=b3ae6cde80ddfe88487861e816830737","Using the option Analyze Results with the Web of Science, one can directly generate overlays onto global journal maps of science. The maps are based on the 10,000+ journals contained in the Journal Citation Reports (JCR) of the Science and Social Sciences Citation Indices (2011). The disciplinary diversity of the retrieval is measured in terms of Rao-Stirling's ""quadratic entropy"" (Izsák & Papp, 1995). Since this indicator of interdisciplinarity is normalized between 0 and 1, interdisciplinarity can be compared among document sets and across years, cited or citing. The colors used for the overlays are based on Blondel, Guillaume, Lambiotte, and Lefebvre's (2008) community-finding algorithms operating on the relations among journals included in the JCR. The results can be exported from VOSViewer with different options such as proportional labels, heat maps, or cluster density maps. The maps can also be web-started or animated (e.g., using PowerPoint). The ""citing"" dimension of the aggregated journal-journal citation matrix was found to provide a more comprehensive description than the matrix based on the cited archive. The relations between local and global maps and their different functions in studying the sciences in terms of journal literatures are further discussed: Local and global maps are based on different assumptions and can be expected to serve different purposes for the explanation. © 2013 ASIS&T.",,"Citation index; Cluster densities; Document sets; Interdisciplinarity; Journal citation reports; Journal literature; Quadratic entropy; Web of Science; Artificial intelligence; Software engineering; Aggregates",Article,Scopus,2-s2.0-84887460767
"Wang Z., Zoghiy M., Hutterz F., Matheson D., De Freitas N.","Bayesian optimization in high dimensions via random embeddings",2013,"IJCAI International Joint Conference on Artificial Intelligence",38,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896058897&partnerID=40&md5=97499838ee4ae202ff60c4906c7bbb84","Bayesian optimization techniques have been successfully applied to robotics, planning, sensor placement, recommendation, advertising, intelligent user interfaces and automatic algorithm configuration. Despite these successes, the approach is restricted to problems of moderate dimension, and several workshops on Bayesian optimization have identified its scaling to high dimensions as one of the holy grails of the field. In this paper, we introduce a novel random embedding idea to attack this problem. The resulting Random EMbedding Bayesian Optimization (REMBO) algorithm is very simple and applies to domains with both categorical and continuous variables. The experiments demonstrate that REMBO can effectively solve high-dimensional problems, including automatic parameter configuration of a popular mixed integer linear programming solver.",,"Automatic algorithms; Bayesian optimization; Continuous variables; High dimensions; High-dimensional problems; Intelligent User Interfaces; Mixed integer linear programming; Sensor placement; Algorithms; Artificial intelligence; Linear programming; Robot programming; Optimization",Conference Paper,Scopus,2-s2.0-84896058897
"Liu L., Shao L., Zhen X., Li X.","Learning discriminative key poses for action recognition",2013,"IEEE Transactions on Cybernetics",38,10.1109/TSMCB.2012.2231959,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890074888&doi=10.1109%2fTSMCB.2012.2231959&partnerID=40&md5=e34bf32a4838fa20ef16e1668750a078","In this paper, we present a new approach for human action recognition based on key-pose selection and representation. Poses in video frames are described by the proposed extensive pyramidal features (EPFs), which include the Gabor, Gaussian, and wavelet pyramids. These features are able to encode the orientation, intensity, and contour information and therefore provide an informative representation of human poses. Due to the fact that not all poses in a sequence are discriminative and representative, we further utilize the AdaBoost algorithm to learn a subset of discriminative poses. Given the boosted poses for each video sequence, a new classifier named weighted local naive Bayes nearest neighbor is proposed for the final action classification, which is demonstrated to be more accurate and robust than other classifiers, e.g., support vector machine (SVM) and naive Bayes nearest neighbor. The proposed method is systematically evaluated on the KTH data set, the Weizmann data set, the multiview IXMAS data set, and the challenging HMDB51 data set. Experimental results manifest that our method outperforms the state-of-the-art techniques in terms of recognition rate. © 2013 IEEE.","AdaBoost; computer vision; extensive pyramidal features (EPFs); human action recognition; pose selection; weighted local naive Bayes nearest neighbor (WLNBNN) classifier","Action classifications; Contour information; extensive pyramidal features (EPFs); Human-action recognition; Nearest neighbors; pose selection; State-of-the-art techniques; Weizmann data sets; Classifiers; Computer vision; Gesture recognition; Motion estimation; Support vector machines; Adaptive boosting; actimetry; algorithm; article; artificial intelligence; automated pattern recognition; body posture; computer assisted diagnosis; discriminant analysis; human; methodology; photography; physiology; videorecording; Actigraphy; Algorithms; Artificial Intelligence; Discriminant Analysis; Humans; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Photography; Posture; Video Recording",Article,Scopus,2-s2.0-84890074888
"Chibani A., Amirat Y., Mohammed S., Matson E., Hagita N., Barreto M.","Ubiquitous robotics: Recent challenges and future trends",2013,"Robotics and Autonomous Systems",38,10.1016/j.robot.2013.04.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885306139&doi=10.1016%2fj.robot.2013.04.003&partnerID=40&md5=a46fbade49efc5ff49baef4c45d3b7ba","Ambient intelligence, ubiquitous and networked robots, and cloud robotics are new research hot topics that have started to gain popularity among the robotics community. They enable robots to acquire richer functionalities and open the way for the composition of a variety of robotic services with three functions: semantic perception, reasoning and actuation. Ubiquitous robots (ubirobots) overcome the limitations of stand-alone robots by integrating them with web services and ambient intelligence technologies. The overlap that exists now between ubirobots and ambient intelligence makes their integration worthwhile. It targets to create a hybrid physical-digital space rich with a myriad of proactive intelligent services that enhance the quality and the way of our living and working. Furthermore, the emergence of cloud computing initiates the massive use of a new generation of ubirobots that enrich their cognitive capabilities and share their knowledge by connecting themselves to cloud infrastructures. The future of ubirobots will certainly be open to an unlimited space of applications such as physical and virtual companions assisting people in their daily living, ubirobots that are able to co-work alongside people and cooperate with them in the same environment, and physical and virtual autonomic guards that are able to protect people, monitor their security and safety, and rescue them in indoor and outdoor spaces. This paper introduces the recent challenges and future trends on these topics. © 2013 Elsevier B.V. All rights reserved.","Ambient intelligence; Cloud robotics; Networked robots; Ubiquitous robots","Ambient intelligence; Cloud infrastructures; Cloud robotics; Cognitive capability; Intelligent Services; Networked robot; Ubiquitous robotics; Ubiquitous robots; Artificial intelligence; Robots; Semantics; Web services; Robotics",Conference Paper,Scopus,2-s2.0-84885306139
"Pokorny C., Klobassa D.S., Pichler G., Erlbeck H., Real R.G.L., Kübler A., Lesenfants D., Habbal D., Noirhomme Q., Risetti M., Mattia D., Müller-Putz G.R.","The auditory P300-based single-switch brain-computer interface: Paradigm transition from healthy subjects to minimally conscious patients",2013,"Artificial Intelligence in Medicine",38,10.1016/j.artmed.2013.07.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886789140&doi=10.1016%2fj.artmed.2013.07.003&partnerID=40&md5=c06edf94c34a8a66da54228704386c94","Objective: Within this work an auditory P300 brain-computer interface based on tone stream segregation, which allows for binary decisions, was developed and evaluated. Methods and materials: Two tone streams consisting of short beep tones with infrequently appearing deviant tones at random positions were used as stimuli. This paradigm was evaluated in 10 healthy subjects and applied to 12 patients in a minimally conscious state (MCS) at clinics in Graz, Würzburg, Rome, and Liège. A stepwise linear discriminant analysis classifier with 10. ×. 10 cross-validation was used to detect the presence of any P300 and to investigate attentional modulation of the P300 amplitude. Results: The results for healthy subjects were promising and most classification results were better than random. In 8 of the 10 subjects, focused attention on at least one of the tone streams could be detected on a single-trial basis. By averaging 10 data segments, classification accuracies up to 90.6. % could be reached. However, for MCS patients only a small number of classification results were above chance level and none of the results were sufficient for communication purposes. Nevertheless, signs of consciousness were detected in 9 of the 12 patients, not on a single-trial basis, but after averaging of all corresponding data segments and computing significant differences. These significant results, however, strongly varied across sessions and conditions. Conclusion: This work shows the transition of a paradigm from healthy subjects to MCS patients. Promising results with healthy subjects are, however, no guarantee of good results with patients. Therefore, more investigations are required before any definite conclusions about the usability of this paradigm for MCS patients can be drawn. Nevertheless, this paradigm might offer an opportunity to support bedside clinical assessment of MCS patients and eventually, to provide them with a means of communication. © 2013 Elsevier B.V.","Auditory P300; Brain-computer interface; Clinical assessment; Minimally conscious state; Tone stream segregation","Auditory P300; Classification accuracy; Classification results; Clinical assessments; Methods and materials; Minimally conscious state; Stepwise linear discriminant analysis; Stream segregation; Artificial intelligence; Medicine; Brain computer interface; adult; article; brain computer interface; clinical article; clinical assessment; computer; controlled study; electroencephalography; female; human; male; minimally conscious state; priority journal; Auditory P300; Brain-computer interface; Clinical assessment; Minimally conscious state; Tone stream segregation; Acoustic Stimulation; Adult; Brain-Computer Interfaces; Electroencephalography; Event-Related Potentials, P300; Female; Humans; Male; Persistent Vegetative State",Article,Scopus,2-s2.0-84886789140
"Garcia-Valverde T., Garcia-Sola A., Hagras H., Dooley J.A., Callaghan V., Botia J.A.","A fuzzy logic-based system for indoor localization using WiFi in ambient intelligent environments",2013,"IEEE Transactions on Fuzzy Systems",38,10.1109/TFUZZ.2012.2227975,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881364653&doi=10.1109%2fTFUZZ.2012.2227975&partnerID=40&md5=855ea7cd9cb5cfb744fc64808758a89f","Ambient intelligence is a new information paradigm, where people are empowered through a digital environment that is 'aware' of their presence and context and is sensitive, adaptive, and responsive to their needs. Hence, one of the important requirements for ambient intelligent environments (AIEs) is the ability to localize the whereabouts of the user in the AIE to address her/his needs. In order to protect user privacy, the use of cameras is not desirable in AIEs, and hence, there is a need to rely on nonintrusive sensors. There are various localization means that are available for outdoor spaces such as those which rely on satellite signals triangulation. However, these outdoor localization means cannot be used in indoor environments. The majority of nonintrusive and noncamera-based indoor localization systems require the installation of extra hardware such as ultrasound emitters/antennas, radio-frequency identification (RFID) antennas, etc. In this paper, we propose a novel indoor localization system that is based on WiFi signals which are free to receive, and they are available in abundance in the majority of domestic spaces. However, free WiFi signals are noisy and uncertain, and their strengths and availability are continuously changing. Hence, we present a fuzzy logic-based system which employs free available WiFi signals to localize a given user in AIEs. The proposed system receives WiFi signals from a large number of existing WiFi access points (up to 170 access points), where no prior knowledge of the access points locations and the environment is required. The system employs an incremental lifelong learning approach to adjust its behavior to the varying and changing WiFi signals to provide a zero-cost localization system which can provide high accuracy in real-world living spaces. We have compared our system in both simulated and real environments with other relevant techniques in the literature, and we have found that our system outperforms the other systems in the offline learning process, whereas our system was the only system which is capable of performing online learning and adaptation. The proposed system was tested in real-world spaces from a living lab intelligent apartment (iSpace) to a town center apartment to a block of offices. In all these experiments, our system has been highly accurate in detecting the user in the given AIEs, and the system was able to adapt its behavior to changes in the AIE or the WiFi signals. We envisage that the proposed system will play an important role in AIEs, especially for privacy concerned situations like elderly care scenarios. © 1993-2012 IEEE.","Ambient intelligence; fuzzy logic systems; localization systems; online learning","Ambient intelligence; Ambient intelligent environments; Fuzzy logic system; Indoor localization systems; Localization system; Offline learning process; Online learning; Outdoor localizations; Apartment houses; Artificial intelligence; Fuzzy logic; Intelligent agents; Online systems; Radio frequency identification (RFID); E-learning",Article,Scopus,2-s2.0-84881364653
"Perlis R.H.","A clinical risk stratification tool for predicting treatment resistance in major depressive disorder",2013,"Biological Psychiatry",38,10.1016/j.biopsych.2012.12.007,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879321551&doi=10.1016%2fj.biopsych.2012.12.007&partnerID=40&md5=7f2c4f8db7d8488acb3195122d0479b4","Background: Early identification of depressed individuals at high risk for treatment resistance could be helpful in selecting optimal setting and intensity of care. At present, validated tools to facilitate this risk stratification are rarely used in psychiatric practice. Methods: Data were drawn from the first two treatment levels of a multicenter antidepressant effectiveness study in major depressive disorder, the STARD (Sequenced Treatment Alternatives to Relieve Depression) cohort. This cohort was divided into training, testing, and validation subsets. Only clinical or sociodemographic variables available by or readily amenable to self-report were considered. Multivariate models were developed to discriminate individuals reaching remission with a first or second pharmacological treatment trial from those not reaching remission despite two trials. Results: A logistic regression model achieved an area under the receiver operating characteristic curve exceeding.71 in training, testing, and validation cohorts and maintained good calibration across cohorts. Performance of three alternative models with machine learning approaches - a naïve Bayes classifier and a support vector machine, and a random forest model - was less consistent. Similar performance was observed between more and less severe depression, men and women, and primary versus specialty care sites. A web-based calculator was developed that implements this tool and provides graphical estimates of risk. Conclusion: Risk for treatment resistance among outpatients with major depressive disorder can be estimated with a simple model incorporating baseline sociodemographic and clinical features. Future studies should examine the performance of this model in other clinical populations and its utility in treatment selection or clinical trial design. © 2013 Society of Biological Psychiatry.","Antidepressant; depression; machine learning; prediction; risk stratification; selective serotonin reuptake inhibitor; treatment-resistant depression","amfebutamone; buspirone; citalopram; sertraline; venlafaxine; adult; African American; aged; area under the curve; article; Bayes theorem; calibration; clinical assessment tool; disease severity; drug substitution; education; energy; false positive result; family; female; human; injury; insomnia; machine learning; major depression; male; marriage; mental patient; posttraumatic stress disorder; prediction; priority journal; recurrent disease; remission; risk assessment; self report; sensitivity and specificity; statistical model; support vector machine; witness; Adolescent; Adult; Aged; Artificial Intelligence; Cohort Studies; Decision Support Techniques; Depressive Disorder, Major; Depressive Disorder, Treatment-Resistant; Female; Humans; Logistic Models; Male; Middle Aged; Multivariate Analysis; Young Adult",Article,Scopus,2-s2.0-84879321551
"Wang Y., Zeng J.","Predicting drug-target interactions using restricted Boltzmann machines",2013,"Bioinformatics",38,10.1093/bioinformatics/btt234,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879970943&doi=10.1093%2fbioinformatics%2fbtt234&partnerID=40&md5=5ad39c3cc39b4cbdaaee4f26fcc09608","Motivation: In silico prediction of drug-target interactions plays an important role toward identifying and developing new uses of existing or abandoned drugs. Network-based approaches have recently become a popular tool for discovering new drug-target interactions (DTIs). Unfortunately, most of these network-based approaches can only predict binary interactions between drugs and targets, and information about different types of interactions has not been well exploited for DTI prediction in previous studies. On the other hand, incorporating additional information about drug-target relationships or drug modes of action can improve prediction of DTIs. Furthermore, the predicted types of DTIs can broaden our understanding about the molecular basis of drug action.Results: We propose a first machine learning approach to integrate multiple types of DTIs and predict unknown drug-target relationships or drug modes of action. We cast the new DTI prediction problem into a two-layer graphical model, called restricted Boltzmann machine, and apply a practical learning algorithm to train our model and make predictions. Tests on two public databases show that our restricted Boltzmann machine model can effectively capture the latent features of a DTI network and achieve excellent performance on predicting different types of DTIs, with the area under precision-recall curve up to 89.6. In addition, we demonstrate that integrating multiple types of DTIs can significantly outperform other predictions either by simply mixing multiple types of interactions without distinction or using only a single interaction type. Further tests show that our approach can infer a high fraction of novel DTIs that has been validated by known experiments in the literature or other databases. These results indicate that our approach can have highly practical relevance to DTI prediction and drug repositioning, and hence advance the drug discovery process. © The Author 2013.",,"drug; protein; algorithm; article; artificial intelligence; computer simulation; drug development; drug effect; drug repositioning; human; metabolism; methodology; drug development; drug effects; procedures; Algorithms; Artificial Intelligence; Computer Simulation; Drug Discovery; Drug Repositioning; Humans; Pharmaceutical Preparations; Proteins; Algorithms; Artificial Intelligence; Computer Simulation; Drug Discovery; Drug Repositioning; Humans; Pharmaceutical Preparations; Proteins",Conference Paper,Scopus,2-s2.0-84879970943
"van der Heijden M., Lucas P.J.F., Lijnse B., Heijdra Y.F., Schermer T.R.J.","An autonomous mobile system for the management of COPD",2013,"Journal of Biomedical Informatics",38,10.1016/j.jbi.2013.03.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878196491&doi=10.1016%2fj.jbi.2013.03.003&partnerID=40&md5=befd653152a5a6ba0747544cb19b31f2","Introduction: Managing chronic disease through automated systems has the potential to both benefit the patient and reduce health-care costs. We have developed and evaluated a disease management system for patients with chronic obstructive pulmonary disease (COPD). Its aim is to predict and detect exacerbations and, through this, help patients self-manage their disease to prevent hospitalisation. Materials: The carefully crafted intelligent system consists of a mobile device that is able to collect case-specific, subjective and objective, physiological data, and to alert the patient by a patient-specific interpretation of the data by means of probabilistic reasoning. Collected data are also sent to a central server for inspection by health-care professionals. Methods: We evaluated the probabilistic model using cross-validation and ROC analyses on data from an earlier study and by an independent data set. Furthermore a pilot with actual COPD patients has been conducted to test technical feasibility and to obtain user feedback. Results: Model evaluation results show that we can reliably detect exacerbations. Pilot study results suggest that an intervention based on this system could be successful. © 2013 Elsevier Inc.","Bayesian networks; Chronic disease management; Decision support systems; EHealth","Autonomous mobile systems; Chronic disease management; Chronic obstructive pulmonary disease; Disease management; Ehealth; Probabilistic models; Probabilistic reasoning; Technical feasibility; Automation; Bayesian networks; Decision support systems; Intelligent systems; Mobile devices; Pulmonary diseases; article; chronic obstructive lung disease; computer aided disease management framework; disease exacerbation; electronic sensor; feasibility study; hospitalization; human; Internet; medical information system; mobile phone; pilot study; prediction; preventive medicine; priority journal; probability; pulse oximeter; receiver operating characteristic; self care; smartphone; spirometer; validation process; Artificial Intelligence; Computer Security; Disease Management; Feasibility Studies; Humans; Internet; Models, Theoretical; Pilot Projects; Probability; Pulmonary Disease, Chronic Obstructive; ROC Curve; Telemedicine",Article,Scopus,2-s2.0-84878196491
"Dyrba M., Ewers M., Wegrzyn M., Kilimann I., Plant C., Oswald A., Meindl T., Pievani M., Bokde A.L.W., Fellgiebel A., Filippi M., Hampel H., Klöppel S., Hauenstein K., Kirste T., Teipel S.J.","Robust Automated Detection of Microstructural White Matter Degeneration in Alzheimer's Disease Using Machine Learning Classification of Multicenter DTI Data",2013,"PLoS ONE",38,10.1371/journal.pone.0064925,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878582129&doi=10.1371%2fjournal.pone.0064925&partnerID=40&md5=66a5eb973347b04e8816cd80403bf7e1","Diffusion tensor imaging (DTI) based assessment of white matter fiber tract integrity can support the diagnosis of Alzheimer's disease (AD). The use of DTI as a biomarker, however, depends on its applicability in a multicenter setting accounting for effects of different MRI scanners. We applied multivariate machine learning (ML) to a large multicenter sample from the recently created framework of the European DTI study on Dementia (EDSD). We hypothesized that ML approaches may amend effects of multicenter acquisition. We included a sample of 137 patients with clinically probable AD (MMSE 20.6±5.3) and 143 healthy elderly controls, scanned in nine different scanners. For diagnostic classification we used the DTI indices fractional anisotropy (FA) and mean diffusivity (MD) and, for comparison, gray matter and white matter density maps from anatomical MRI. Data were classified using a Support Vector Machine (SVM) and a Naïve Bayes (NB) classifier. We used two cross-validation approaches, (i) test and training samples randomly drawn from the entire data set (pooled cross-validation) and (ii) data from each scanner as test set, and the data from the remaining scanners as training set (scanner-specific cross-validation). In the pooled cross-validation, SVM achieved an accuracy of 80% for FA and 83% for MD. Accuracies for NB were significantly lower, ranging between 68% and 75%. Removing variance components arising from scanners using principal component analysis did not significantly change the classification results for both classifiers. For the scanner-specific cross-validation, the classification accuracy was reduced for both SVM and NB. After mean correction, classification accuracy reached a level comparable to the results obtained from the pooled cross-validation. Our findings support the notion that machine learning classification allows robust classification of DTI data sets arising from multiple scanners, even if a new data set comes from a scanner that was not part of the training sample. © 2013 Dyrba et al.",,"aged; Alzheimer disease; analytical parameters; article; automation; Bayes theorem; brain mapping; central nervous system disease; controlled study; diagnostic accuracy; diffusion tensor imaging; disease classification; female; fractional anisotropy; gray matter; human; machine learning; major clinical study; male; mean diffusivity; Mini Mental State Examination; nuclear magnetic resonance imaging; nuclear magnetic resonance scanner; retrospective study; sensitivity and specificity; support vector machine; validation process; white matter degeneration; Aged; Alzheimer Disease; Artificial Intelligence; Brain; Case-Control Studies; Diffusion Tensor Imaging; Female; Humans; Leukoencephalopathies; Male; Middle Aged; Principal Component Analysis; Reproducibility of Results; Retrospective Studies",Article,Scopus,2-s2.0-84878582129
"Gorsevski P.V., Cathcart S.C., Mirzaei G., Jamali M.M., Ye X., Gomezdelcampo E.","A group-based spatial decision support system for wind farm site selection in Northwest Ohio",2013,"Energy Policy",38,10.1016/j.enpol.2012.12.013,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873117178&doi=10.1016%2fj.enpol.2012.12.013&partnerID=40&md5=aa5f254b8690b696db8a5a389b1dbfa2","The purpose of this paper is to demonstrate the benefits of applying a spatial decision support system (SDSS) framework for evaluating the suitability for wind farm siting in Northwest Ohio. The multiple criteria evaluation (MCE) prototype system is intended for regional planning but also for promoting group decision making that could involve participants with different interests in the development of decision alternatives. The framework integrates environmental and economic criteria and builds a hierarchy for wind farm siting using weighted linear combination (WLC) techniques and GIS functionality. The SDSS allows the multiple participants to interact and develop an understanding of the spatial data for assigning importance values to each factor. The WLC technique is used to combine the assigned values with map layers, which are standardized using fuzzy set theory, to produce individual suitability maps. The maps created by personal preferences from the participants are aggregated for producing a group solution using the Borda method. Sensitivity analysis is performed on the group solution to examine how small changes in the factor weights affect the calculated suitability scores. The results from the sensitivity analysis are intended to aid understanding of compromised solutions through changes in the input data from the participant's perspective. © 2012 Elsevier Ltd.","Borda method; Spatial decision support system; Wind farm siting","Borda method; Compromised solution; Economic criteria; Factor weight; Group Decision Making; Group-based; Input datas; Linear combinations; Multiple-criteria evaluation; Prototype system; Spatial data; Spatial decision support systems; Wind farm; Artificial intelligence; Decision making; Decision support systems; Fuzzy set theory; Regional planning; Sensitivity analysis; Site selection; Wind power; Electric utilities; cost-benefit analysis; decision making; decision support system; energy planning; fuzzy mathematics; GIS; regional planning; sensitivity analysis; spatial analysis; wind farm; Ohio; United States",Article,Scopus,2-s2.0-84873117178
"Hinaut X., Dominey P.F.","Real-Time Parallel Processing of Grammatical Structure in the Fronto-Striatal System: A Recurrent Network Simulation Study Using Reservoir Computing",2013,"PLoS ONE",38,10.1371/journal.pone.0052946,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873259375&doi=10.1371%2fjournal.pone.0052946&partnerID=40&md5=5de0a86824b68e94bb05d356a0fdac3a","Sentence processing takes place in real-time. Previous words in the sentence can influence the processing of the current word in the timescale of hundreds of milliseconds. Recent neurophysiological studies in humans suggest that the fronto-striatal system (frontal cortex, and striatum - the major input locus of the basal ganglia) plays a crucial role in this process. The current research provides a possible explanation of how certain aspects of this real-time processing can occur, based on the dynamics of recurrent cortical networks, and plasticity in the cortico-striatal system. We simulate prefrontal area BA47 as a recurrent network that receives on-line input about word categories during sentence processing, with plastic connections between cortex and striatum. We exploit the homology between the cortico-striatal system and reservoir computing, where recurrent frontal cortical networks are the reservoir, and plastic cortico-striatal synapses are the readout. The system is trained on sentence-meaning pairs, where meaning is coded as activation in the striatum corresponding to the roles that different nouns and verbs play in the sentences. The model learns an extended set of grammatical constructions, and demonstrates the ability to generalize to novel constructions. It demonstrates how early in the sentence, a parallel set of predictions are made concerning the meaning, which are then confirmed or updated as the processing of the input sentence proceeds. It demonstrates how on-line responses to words are influenced by previous words in the sentence, and by previous sentences in the discourse, providing new insight into the neurophysiology of the P600 ERP scalp response to grammatical complexity. This demonstrates that a recurrent neural network can decode grammatical structure from sentences in real-time in order to generate a predictive representation of the meaning of the sentences. This can provide insight into the underlying mechanisms of human cortico-striatal function in sentence processing. © 2013 Hinaut, Dominey.",,"article; brain function; brain region; brain size; Brodmann area 47; corpus striatum; discourse analysis; evoked cortical response; frontal cortex; grammar; language processing; mathematical computing; nerve cell network; nerve cell plasticity; neurophysiology; online system; prediction; prefrontal cortex; signal noise ratio; simulation; synapse; Artificial Intelligence; Computer Simulation; Corpus Striatum; Frontal Lobe; Humans; Neural Networks (Computer); Semantics; Speech Perception",Article,Scopus,2-s2.0-84873259375
"Ntirogiannis K., Gatos B., Pratikakis I.","Performance evaluation methodology for historical document image binarization",2013,"IEEE Transactions on Image Processing",38,10.1109/TIP.2012.2219550,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872256597&doi=10.1109%2fTIP.2012.2219550&partnerID=40&md5=0d6a14c80d129c9de382097708e14e08","Document image binarization is of great importance in the document image analysis and recognition pipeline since it affects further stages of the recognition process. The evaluation of a binarization method aids in studying its algorithmic behavior, as well as verifying its effectiveness, by providing qualitative and quantitative indication of its performance. This paper addresses a pixel-based binarization evaluation methodology for historical handwritten/machine-printed document images. In the proposed evaluation scheme, the recall and precision evaluation measures are properly modified using a weighting scheme that diminishes any potential evaluation bias. Additional performance metrics of the proposed evaluation scheme consist of the percentage rates of broken and missed text, false alarms, background noise, character enlargement, and merging. Several experiments conducted in comparison with other pixel-based evaluation measures demonstrate the validity of the proposed evaluation scheme. © 1992-2012 IEEE.","Document image binarization; ground truth; performance evaluation","Background noise; Binarizations; Document image analysis; Document images; Evaluation measures; Evaluation methodologies; Evaluation scheme; False alarms; Ground truth; Historical documents; Performance evaluation; Performance evaluation methodology; Performance metrics; Potential evaluation; Recall and precision; Recognition process; Weighting scheme; Image processing; Mathematical models; Pixels; algorithm; article; artificial intelligence; automated pattern recognition; documentation; handwriting; human; image processing; methodology; printing; reproducibility; Algorithms; Artificial Intelligence; Documentation; Handwriting; Humans; Image Processing, Computer-Assisted; Pattern Recognition, Automated; Printing; Reproducibility of Results",Article,Scopus,2-s2.0-84872256597
"Bai J., Xiang S., Pan C.","A graph-based classification method for hyperspectral images",2013,"IEEE Transactions on Geoscience and Remote Sensing",38,10.1109/TGRS.2012.2205002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872950801&doi=10.1109%2fTGRS.2012.2205002&partnerID=40&md5=081e7deb32f8d70581788dcbc8e9cb49","The goal of this paper is to apply graph cut (GC) theory to the classification of hyperspectral remote sensing images. The task is formulated as a labeling problem on Markov random field (MRF) constructed on the image grid, and GC algorithm is employed to solve this task. In general, a large number of user interactive strikes are necessary to obtain satisfactory segmentation results. Due to the spatial variability of spectral signatures, however, hyperspectral remote sensing images often contain many tiny regions. Labeling all these tiny regions usually needs expensive human labor. To overcome this difficulty, a pixelwise fuzzy classification based on support vector machine (SVM) is first applied. As a result, only pixels with high probabilities are preserved as labeled ones. This generates a pseudouser strike map. This map is then employed for GC to evaluate the truthful likelihoods of class labels and propagate them to the MRF. To evaluate the robustness of our method, we have tested our method on both large and small training sets. Additionally, comparisons are made between the results of SVM, SVM with stacking neighboring vectors, SVM with morphological preprocessing, extraction and classification of homogeneous objects, and our method. Comparative experimental results demonstrate the validity of our method. © 1980-2012 IEEE.","Classification; graph cut (GC); hyperspectral; Markov random field (MRF); support vector machine (SVM)","Class labels; Classification methods; Fuzzy classification; Graph cut; Graph-based; High probability; Homogeneous objects; Human labor; Hyper-spectral images; HyperSpectral; Hyperspectral Remote Sensing Image; Image grids; Markov Random Fields; Morphological preprocessing; Segmentation results; Small training; Spatial variability; Spectral signature; Classification (of information); Graphic methods; Image reconstruction; Image segmentation; Spectroscopy; Support vector machines; algorithm; artificial intelligence; comparative study; data set; fuzzy mathematics; graphical method; image classification; image processing; Markov chain; pixel; spatial variation",Article,Scopus,2-s2.0-84872950801
"Rahmani R., Yusof R., Seyedmahmoudian M., Mekhilef S.","Hybrid technique of ant colony and particle swarm optimization for short term wind energy forecasting",2013,"Journal of Wind Engineering and Industrial Aerodynamics",37,10.1016/j.jweia.2013.10.004,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887255451&doi=10.1016%2fj.jweia.2013.10.004&partnerID=40&md5=a9e541d734995dbde30c5f1f91c548f0","Wind farms are producing a considerable portion of the world renewable energy. Since the output power of any wind farm is highly dependent on the wind speed, the power extracted from a wind park is not always a constant value. In order to have a non-disruptive supply of electricity, it is important to have a good scheduling and forecasting system for the energy output of any wind park. In this paper, a new hybrid swarm technique (HAP) is used to forecast the energy output of a real wind farm located in Binaloud, Iran. The technique consists of the hybridization of the ant colony optimization (ACO) and particle swarm optimization (PSO) which are two meta-heuristic techniques under the category of swarm intelligence. The hybridization of the two algorithms to optimize the forecasting model leads to a higher quality result with a faster convergence profile. The empirical hourly wind power output of Binaloud Wind Farm for 364. days is collected and used to train and test the prepared model. The meteorological data consisting of wind speed and ambient temperature is used as the inputs to the mathematical model. The results indicate that the proposed technique can estimate the output wind power based on the wind speed and the ambient temperature with an MAPE of 3.513%. © 2013 Elsevier Ltd.","Ant Colony Optimization; Hybrid technique; Particle Swarm Optimization; Short term forecasting; Wind energy","Ant Colony Optimization (ACO); Energy forecasting; Forecasting modeling; Hybrid techniques; Meta-heuristic techniques; Meteorological data; Renewable energies; Short-term forecasting; Algorithms; Ant colony optimization; Artificial intelligence; Electric utilities; Forecasting; Mathematical models; Meteorology; Particle swarm optimization (PSO); Temperature; Wind effects; Wind power",Article,Scopus,2-s2.0-84887255451
"Srivastava N., Salakhutdinov R., Hinton G.","Modeling documents with a Deep Boltzmann Machine",2013,"Uncertainty in Artificial Intelligence - Proceedings of the 29th Conference, UAI 2013",37,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888183490&partnerID=40&md5=a217ccd79498c40df0d7897796e909fb","We introduce a type of Deep Boltzmann Machine (DBM) that is suitable for extracting distributed semantic representations from a large unstructured collection of documents. We overcome the apparent difficulty of training a DBM with judicious parameter tying. This enables an efficient pretraining algorithm and a state initialization scheme for fast inference. The model can be trained just as efficiently as a standard Restricted Boltzmann Machine. Our experiments show that the model assigns better log probability to unseen data than the Replicated Softmax model. Features extracted from our model outperform LDA, Replicated Softmax, and DocNADE models on document retrieval and document classification tasks.",,"Collection of documents; Deep boltzmann machines; Document Classification; Document Retrieval; Fast inference; Pre-training; Restricted boltzmann machine; Semantic representation; Artificial intelligence; Inference engines; Information retrieval systems",Conference Paper,Scopus,2-s2.0-84888183490
"Bansal J.C., Sharma H., Arya K.V., Nagar A.","Memetic search in artificial bee colony algorithm",2013,"Soft Computing",37,10.1007/s00500-013-1032-8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883745702&doi=10.1007%2fs00500-013-1032-8&partnerID=40&md5=6226d5cf864486d1c9afd86ecb522e62","Artificial bee colony (ABC) optimization algorithm is relatively a simple and recent population based probabilistic approach for global optimization. ABC has been outperformed over some Nature Inspired Algorithms (NIAs) when tested over benchmark as well as real world optimization problems. The solution search equation of ABC is significantly influenced by a random quantity which helps in exploration at the cost of exploitation of the search space. In the solution search equation of ABC, there is a enough chance to skip the true solution due to large step size. In order to balance between diversity and convergence capability of the ABC, a new local search phase is integrated with the basic ABC to exploit the search space identified by the best individual in the swarm. In the proposed phase, ABC works as a local search algorithm in which, the step size that is required to update the best solution, is controlled by Golden Section Search approach. The proposed strategy is named as Memetic ABC (MeABC). In MeABC, new solutions are generated around the best solution and it helps to enhance the exploitation capability of ABC. MeABC is established as a modified ABC algorithm through experiments over 20 test problems of different complexities and 4 well known engineering optimization problems. © 2013 Springer-Verlag Berlin Heidelberg.","Artificial bee colony; Exploration-exploitation; Memetic algorithm; Swarm intelligence","Artificial bee colonies; Artificial bee colonies (ABC); Artificial bee colony algorithms; Engineering optimization problems; Exploration exploitations; Memetic algorithms; Nature inspired algorithms; Swarm Intelligence; Artificial intelligence; Global optimization; Optimization; Evolutionary algorithms",Article,Scopus,2-s2.0-84883745702
"Bellare M., Hoang V.T., Keelveedhi S.","Instantiating random oracles via UCEs",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",37,10.1007/978-3-642-40084-1_23,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884469524&doi=10.1007%2f978-3-642-40084-1_23&partnerID=40&md5=75edc74ed8513ba0de7c3678f9973d11","This paper provides a (standard-model) notion of security for (keyed) hash functions, called UCE, that we show enables instantiation of random oracles (ROs) in a fairly broad and systematic way. Goals and schemes we consider include deterministic PKE; message-locked encryption; hardcore functions; point-function obfuscation; OAEP; encryption secure for key-dependent messages; encryption secure under related-key attack; proofs of storage; and adaptively-secure garbled circuits with short tokens. We can take existing, natural and efficient ROM schemes and show that the instantiated scheme resulting from replacing the RO with a UCE function is secure in the standard model. In several cases this results in the first standard-model schemes for these goals. The definition of UCE-security itself is quite simple, asking that outputs of the function look random given some ""leakage,"" even if the adversary knows the key, as long as the leakage does not permit the adversary to compute the inputs. © 2013 International Association for Cryptologic Research.",,"Garbled circuits; Hardcore; Random Oracle; Related-key attacks; The standard model; Artificial intelligence; Computer science; Hash functions",Conference Paper,Scopus,2-s2.0-84884469524
"Huang Y., Katz J., Evans D.","Efficient secure two-party computation using symmetric cut-and-choose",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",37,10.1007/978-3-642-40084-1_2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884477218&doi=10.1007%2f978-3-642-40084-1_2&partnerID=40&md5=a813912e2b73bcdf212da72bb3c76ec0","Beginning with the work of Lindell and Pinkas, researchers have proposed several protocols for secure two-party computation based on the cut-and-choose paradigm. In current instantiations of this approach, one party generates κ garbled circuits; some fraction of those are ""checked"" by the other party, and the remaining fraction are evaluated. We introduce here the idea of symmetric cut-and-choose protocols, in which both parties generate κ circuits to be checked by the other party. The main advantage of our technique is that κ can be reduced by a factor of 3 while attaining the same statistical security level as in prior work. Since the number of garbled circuits dominates the costs of the protocol, especially as larger circuits are evaluated, our protocol is expected to run up to 3 times faster than existing schemes. Preliminary experiments validate this claim. © 2013 International Association for Cryptologic Research.",,"Garbled circuits; Secure two-party computations; Several protocols; Statistical securities; Artificial intelligence; Computer science; Cryptography",Conference Paper,Scopus,2-s2.0-84884477218
"Elgendi M.","Fast QRS Detection with an Optimized Knowledge-Based Method: Evaluation on 11 Standard ECG Databases",2013,"PLoS ONE",37,10.1371/journal.pone.0073557,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884189764&doi=10.1371%2fjournal.pone.0073557&partnerID=40&md5=ee86116e6a2609d0c29731243b1235d7","The current state-of-the-art in automatic QRS detection methods show high robustness and almost negligible error rates. In return, the methods are usually based on machine-learning approaches that require sufficient computational resources. However, simple-fast methods can also achieve high detection rates. There is a need to develop numerically efficient algorithms to accommodate the new trend towards battery-driven ECG devices and to analyze long-term recorded signals in a time-efficient manner. A typical QRS detection method has been reduced to a basic approach consisting of two moving averages that are calibrated by a knowledge base using only two parameters. In contrast to high-accuracy methods, the proposed method can be easily implemented in a digital filter design. © 2013 Elgendi.",,"algorithm; article; digital filtering; electrocardiogram; factual database; heart arrhythmia; heart rate variability; human; human experiment; intermethod comparison; knowledge base; machine learning; male; mathematical model; normal human; PR interval; prediction; process development; process optimization; QRS complex; sensitivity and specificity; validation process; Algorithms; Artificial Intelligence; Databases, Factual; Electrocardiography; Signal Processing, Computer-Assisted",Article,Scopus,2-s2.0-84884189764
"Lu S., Ostrovsky R.","How to garble RAM programs?",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",37,10.1007/978-3-642-38348-9_42,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883362941&doi=10.1007%2f978-3-642-38348-9_42&partnerID=40&md5=104b179a6c9d0bea5b96cd069c0e8a46","Assuming solely the existence of one-way functions, we show how to construct Garbled RAM Programs (GRAM) where its size only depends on fixed polynomial in the security parameter times the program running time. We stress that we avoid converting the RAM programs into circuits. As an example, our techniques implies the first garbled binary search program (searching over sorted encrypted data stored in a cloud) which is poly-logarithmic in the data size instead of linear. Our result requires the existence of one-way function and enjoys the same non-interactive properties as Yao's original garbled circuits. © 2013 International Association for Cryptologic Research.","Garbled Circuits; Oblivious RAM; Secure Computation","Binary search; Data size; Encrypted data; Garbled circuits; One-way functions; Running time; Secure computation; Security parameters; Artificial intelligence; Computer science; Cryptography",Conference Paper,Scopus,2-s2.0-84883362941
"Liu E., Jain A.K., Tian J.","A coarse to fine minutiae-based latent palmprint matching",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",37,10.1109/TPAMI.2013.39,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883173650&doi=10.1109%2fTPAMI.2013.39&partnerID=40&md5=e30f604fa8ebb46038b6ac899963a5e9","With the availability of live-scan palmprint technology, high resolution palmprint recognition has started to receive significant attention in forensics and law enforcement. In forensic applications, latent palmprints provide critical evidence as it is estimated that about 30 percent of the latents recovered at crime scenes are those of palms. Most of the available high-resolution palmprint matching algorithms essentially follow the minutiae-based fingerprint matching strategy. Considering the large number of minutiae (about 1,000 minutiae in a full palmprint compared to about 100 minutiae in a rolled fingerprint) and large area of foreground region in full palmprints, novel strategies need to be developed for efficient and robust latent palmprint matching. In this paper, a coarse to fine matching strategy based on minutiae clustering and minutiae match propagation is designed specifically for palmprint matching. To deal with the large number of minutiae, a local feature-based minutiae clustering algorithm is designed to cluster minutiae into several groups such that minutiae belonging to the same group have similar local characteristics. The coarse matching is then performed within each cluster to establish initial minutiae correspondences between two palmprints. Starting with each initial correspondence, a minutiae match propagation algorithm searches for mated minutiae in the full palmprint. The proposed palmprint matching algorithm has been evaluated on a latent-to-full palmprint database consisting of 446 latents and 12,489 background full prints. The matching results show a rank-1 identification accuracy of 79.4 percent, which is significantly higher than the 60.8 percent identification accuracy of a state-of-the-art latent palmprint matching algorithm on the same latent database. The average computation time of our algorithm for a single latent-to-full match is about 141 ms for genuine match and 50 ms for impostor match, on a Windows XP desktop system with 2.2-GHz CPU and 1.00-GB RAM. The computation time of our algorithm is an order of magnitude faster than a previously published state-of-the-art-algorithm. © 1979-2012 IEEE.","latent palmprint matching; match propagation; minutia descriptor; minutiae clustering; Palmprint","Descriptors; Fingerprint matching; Forensic applications; Identification accuracy; Local characteristics; minutiae clustering; Palmprints; Propagation algorithm; Anthropometry; Clustering algorithms; Image matching; Pattern matching; algorithm; article; artificial intelligence; automated pattern recognition; biometry; classification; computer assisted diagnosis; dermatoglyphics; human; image enhancement; image subtraction; information retrieval; methodology; photography; reproducibility; sensitivity and specificity; automated pattern recognition; biometry; classification; computer assisted diagnosis; dermatoglyphics; procedures; Algorithms; Artificial Intelligence; Biometry; Dermatoglyphics; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Information Storage and Retrieval; Pattern Recognition, Automated; Photography; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique; Algorithms; Artificial Intelligence; Biometry; Dermatoglyphics; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Information Storage and Retrieval; Pattern Recognition, Automated; Photography; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-84883173650
"Merigó J.M., Yager R.R.","Generalized moving averages, distance measures and OWA operators",2013,"International Journal of Uncertainty, Fuzziness and Knowlege-Based Systems",37,10.1142/S0218488513500268,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882238133&doi=10.1142%2fS0218488513500268&partnerID=40&md5=b11de561600aea93bba674ccb3684ed0","The concept of moving average is studied. We analyze several extensions by using generalized aggregation operators, obtaining the generalized moving average. The main advantage is that it provides a general framework that includes a wide range of specific cases including the geometric and the quadratic moving average. This analysis is extended by using the generalized ordered weighted averaging (GOWA) and the induced GOWA (IGOWA) operator. Thus, we get the generalized ordered weighted moving average (GOWMA) and the induced GOWMA (IGOWMA) operator. Some of their main properties are studied. We further extend this approach by using distance measures suggesting the concept of distance moving average and generalized distance moving average. We also consider the case with the OWA and the IOWA operator, obtaining the generalized ordered weighted moving averaging distance (GOWMAD) and the induced GOWMAD (IGOWMAD) operator. The paper ends with an application in multi-period decision making. © 2013 World Scientific Publishing Company.","aggregation operators; distance measures; Moving average; OWA operator","Aggregation operator; Distance measure; Generalized distances; Iowa operators; Moving averages; Ordered weighted averaging; OWA operators; Weighted moving averages; Artificial intelligence; Software engineering; Mathematical operators",Article,Scopus,2-s2.0-84882238133
"Salem M., Eyssel F., Rohlfing K., Kopp S., Joublin F.","To Err is Human(-like): Effects of Robot Gesture on Perceived Anthropomorphism and Likability",2013,"International Journal of Social Robotics",37,10.1007/s12369-013-0196-9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880405179&doi=10.1007%2fs12369-013-0196-9&partnerID=40&md5=558861be4402a9fb73f4ab88686d4a93","Previous work has shown that non-verbal behaviors affect anthropomorphic inferences about artificial communicators such as virtual agents or social robots. In an experiment with a humanoid robot we investigated the effects of the robot's hand and arm gestures on the perception of humanlikeness, likability of the robot, shared reality, and future contact intentions after interacting with the robot. For this purpose, the speech-accompanying non-verbal behaviors of the humanoid robot were manipulated in three experimental conditions: (1) no gesture, (2) congruent co-verbal gesture, and (3) incongruent co-verbal gesture. We hypothesized higher ratings on all dependent measures in the two multimodal (i.e., speech and gesture) conditions compared to the unimodal (i.e., speech only) condition. The results confirm our predictions: when the robot used co-verbal gestures during interaction, it was anthropomorphized more, participants perceived it as more likable, reported greater shared reality with it, and showed increased future contact intentions than when the robot gave instructions without gestures. Surprisingly, this effect was particularly pronounced when the robot's gestures were partly incongruent with speech, although this behavior negatively affected the participants' task-related performance. These findings show that communicative non-verbal behaviors displayed by robotic systems affect anthropomorphic perceptions and the mental models humans form of a humanoid robot during interaction. © 2013 Springer Science+Business Media Dordrecht.","Anthropomorphism; Multimodal interaction and conversational skills; Non-verbal cues and expressiveness; Robot companions and social robots; Social human-robot interaction","Anthropomorphism; Multi-Modal Interactions; Non-verbal cues and expressiveness; Social human-robot interactions; Social robots; Artificial intelligence; Human computer interaction; Human robot interaction; Anthropomorphic robots",Article,Scopus,2-s2.0-84880405179
"Dymova L., Sevastjanov P., Tikhonenko A.","An approach to generalization of fuzzy TOPSIS method",2013,"Information Sciences",37,10.1016/j.ins.2013.02.049,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876868616&doi=10.1016%2fj.ins.2013.02.049&partnerID=40&md5=2279fc783569172c8141bd7dd9863649","The TOPSIS method is a technique for establishing order preference by similarity to the ideal solution, and was primarily developed for dealing with real-valued data. This technique is currently one of most popular methods for Multiple Criteria Decision Making (MCDM). In many cases, it is hard to present precisely exact ratings of alternatives with respect to local criteria and as a result these ratings are seen as fuzzy values. A number of papers have been devoted to fuzzy extensions of the TOPSIS method in the literature, but these extensions are not complete since the ideal solutions are usually presented as real values (not by fuzzy values) or as fuzzy values which are not attainable in the decision matrix. In most of these papers, a defuzzification of elements of the fuzzy decision matrix is used, which leads inevitably to a loss of important information and may even produce the wrong results. In this paper, we propose a new direct approach to the fuzzy extension of the TOPSIS method which is free of the limitations of other known approaches. We show that the distances of the alternatives from the ideal solutions may be treated (in some sense) as modified weighted sums of local criteria. It is known that using weighted sums is not the best approach to the aggregation of local criteria in many real-world situations. Therefore, here, we propose the use, in addition to weighted sums, some other types of local criteria aggregation in the TOPSIS method and we develop a method for the generalization of different aggregation modes, providing compromised final results. © 2013 Elsevier Inc. All rights reserved.","Aggregation mode; Fuzzy extension; TOPSIS","Aggregation of local criterion; Decision matrices; Defuzzifications; Fuzzy extension; Multiple criteria decision making; Real world situations; Real-valued data; TOPSIS; Artificial intelligence; Software engineering",Article,Scopus,2-s2.0-84876868616
"Nishi H., Tyagi M., Teng S., Shoemaker B.A., Hashimoto K., Alexov E., Wuchty S., Panchenko A.R.","Cancer Missense Mutations Alter Binding Properties of Proteins and Their Interaction Networks",2013,"PLoS ONE",37,10.1371/journal.pone.0066273,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879157943&doi=10.1371%2fjournal.pone.0066273&partnerID=40&md5=af0ef01aefc1b2ce01208ee112a683a8","Many studies have shown that missense mutations might play an important role in carcinogenesis. However, the extent to which cancer mutations might affect biomolecular interactions remains unclear. Here, we map glioblastoma missense mutations on the human protein interactome, model the structures of affected protein complexes and decipher the effect of mutations on protein-protein, protein-nucleic acid and protein-ion binding interfaces. Although some missense mutations over-stabilize protein complexes, we found that the overall effect of mutations is destabilizing, mostly affecting the electrostatic component of binding energy. We also showed that mutations on interfaces resulted in more drastic changes of amino acid physico-chemical properties than mutations occurring outside the interfaces. Analysis of glioblastoma mutations on interfaces allowed us to stratify cancer-related interactions, identify potential driver genes, and propose two dozen additional cancer biomarkers, including those specific to functions of the nervous system. Such an analysis also offered insight into the molecular mechanism of the phenotypic outcomes of mutations, including effects on complex stability, activity, binding and turnover rate. As a result of mutated protein and gene network analysis, we observed that interactions of proteins with mutations mapped on interfaces had higher bottleneck properties compared to interactions with mutations elsewhere on the protein or unaffected interactions. Such observations suggest that genes with mutations directly affecting protein binding properties are preferably located in central network positions and may influence critical nodes and edges in signal transduction networks.",,"amino acid; tumor marker; article; cancer genetics; carcinogenesis; complex formation; controlled study; gene mapping; glioblastoma; human; missense mutation; mutational analysis; nervous system function; phenotype; physical chemistry; protein binding; protein function; protein interaction; protein metabolism; protein nucleic acid interaction; protein protein interaction; protein stability; protein structure; signal transduction; single nucleotide polymorphism; Artificial Intelligence; Binding Sites; DNA-Binding Proteins; Glioblastoma; Humans; Models, Biological; Mutation, Missense; Phenotype; Protein Binding; Protein Interaction Domains and Motifs; Protein Interaction Maps; Protein Stability; Thermodynamics",Article,Scopus,2-s2.0-84879157943
"El Shafey L., McCool C., Wallace R., Marcel S.","A scalable formulation of probabilistic linear discriminant analysis: Applied to face recognition",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",37,10.1109/TPAMI.2013.38,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878119221&doi=10.1109%2fTPAMI.2013.38&partnerID=40&md5=ba6551b9da32ee147f397d9d5db7f43e","In this paper, we present a scalable and exact solution for probabilistic linear discriminant analysis (PLDA). PLDA is a probabilistic model that has been shown to provide state-of-the-art performance for both face and speaker recognition. However, it has one major drawback: At training time estimating the latent variables requires the inversion and storage of a matrix whose size grows quadratically with the number of samples for the identity (class). To date, two approaches have been taken to deal with this problem, to 1) use an exact solution that calculates this large matrix and is obviously not scalable with the number of samples or 2) derive a variational approximation to the problem. We present a scalable derivation which is theoretically equivalent to the previous nonscalable solution and thus obviates the need for a variational approximation. Experimentally, we demonstrate the efficacy of our approach in two ways. First, on labeled faces in the wild, we illustrate the equivalence of our scalable implementation with previously published work. Second, on the large Multi-PIE database, we illustrate the gain in performance when using more training samples per identity (class), which is made possible by the proposed scalable formulation of PLDA. © 1979-2012 IEEE.","expectation maximization; face verification; PLDA; probablistic model","Expectation - maximizations; Face Verification; PLDA; Probabilistic linear discriminant analysis; Probablistic models; Scalable implementation; State-of-the-art performance; Variational approximation; Artificial intelligence; Computer vision; Face recognition; algorithm; article; biometry; discriminant analysis; face; factual database; histology; human; image processing; methodology; anatomy and histology; biometry; image processing; procedures; Algorithms; Biometric Identification; Databases, Factual; Discriminant Analysis; Face; Humans; Image Processing, Computer-Assisted; Algorithms; Biometric Identification; Databases, Factual; Discriminant Analysis; Face; Humans; Image Processing, Computer-Assisted",Article,Scopus,2-s2.0-84878119221
"Prest A., Ferrari V., Schmid C.","Explicit modeling of human-object interactions in realistic videos",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",37,10.1109/TPAMI.2012.175,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874537383&doi=10.1109%2fTPAMI.2012.175&partnerID=40&md5=f65f536f232c363272796a6a97b1452c","We introduce an approach for learning human actions as interactions between persons and objects in realistic videos. Previous work typically represents actions with low-level features such as image gradients or optical flow. In contrast, we explicitly localize in space and track over time both the object and the person, and represent an action as the trajectory of the object w.r.t. to the person position. Our approach relies on state-of-the-art techniques for human detection [32], object detection [10], and tracking [39]. We show that this results in human and object tracks of sufficient quality to model and localize human-object interactions in realistic videos. Our human-object interaction features capture the relative trajectory of the object w.r.t. the human. Experimental results on the Coffee and Cigarettes dataset [25], the video dataset of [19], and the Rochester Daily Activities dataset [29] show that 1) our explicit human-object model is an informative cue for action recognition; 2) it is complementary to traditional low-level descriptors such as 3D-HOG [23] extracted over human tracks. We show that combining our human-object interaction features with 3D-HOG improves compared to their individual performance as well as over the state of the art [23], [29]. © 1979-2012 IEEE.","Action recognition; human-object interaction; video analysis","Action recognition; Daily activity; Explicit modeling; Human actions; Human detection; Human-object interaction; Image gradients; Individual performance; Low level descriptors; Low-level features; Object Detection; Object track; State of the art; State-of-the-art techniques; Video analysis; Video dataset; Motion estimation; Three dimensional computer graphics; Image recognition; algorithm; article; artificial intelligence; automated pattern recognition; classification; factual database; human; human activities; image processing; methodology; videorecording; Algorithms; Artificial Intelligence; Databases, Factual; Human Activities; Humans; Image Processing, Computer-Assisted; Pattern Recognition, Automated; Video Recording",Article,Scopus,2-s2.0-84874537383
"Chen H., Sun J.","A new approach for global controllability of higher order Boolean control network",2013,"Neural Networks",37,10.1016/j.neunet.2012.12.004,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872178752&doi=10.1016%2fj.neunet.2012.12.004&partnerID=40&md5=692cbda2852d490e79f4caf3263de6d4","Using the semi-tensor product, the global control problem of the higher order Boolean control network with avoiding set is considered. First, the number of different control sequences that drive the higher order Boolean control network from the initial state to the destination state while avoiding undesirable set is provided. Then, the definition of global controllability of higher order Boolean control network is given. Third, by using the classical theory of nonnegative matrices, the necessary and sufficient condition for the global controllability of higher order Boolean control network with avoiding set is presented. Furthermore, a sufficient condition for k fixed-time global controllability of the system is also obtained. At last, an example is given to illustrate the main results. © 2012 Elsevier Ltd.","Boolean control network; Global controllability; Higher order; Semi-tensor product","Classical theory; Control network; Control sequences; Global control; Global controllability; Higher order; Initial state; Non-negative matrix; Semi-tensor product; Sufficient conditions; Artificial intelligence; Cognitive systems; Controllability; article; Boolean control network; kappa fixed time global controllability; mathematical analysis; mathematical model; priority journal; semi tensor product; Mathematical Concepts; Models, Genetic; Neural Networks (Computer)",Article,Scopus,2-s2.0-84872178752
"Grekousis G., Manetos P., Photis Y.N.","Modeling urban evolution using neural networks, fuzzy logic and GIS: The case of the Athens metropolitan area",2013,"Cities",37,10.1016/j.cities.2012.03.006,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871934138&doi=10.1016%2fj.cities.2012.03.006&partnerID=40&md5=b4bf61d6bcbfbeab274f5f91d0973baa","This paper presents an artificial intelligence approach integrated with geographical information systems (GISs) for modeling urban evolution. Fuzzy logic and neural networks are used to provide a synthetic spatiotemporal methodology for the analysis, prediction and interpretation of urban growth. The proposed urban model takes into account the changes over time in population and building use patterns. A GIS is used for handling the spatial and temporal data, performing contingency analysis and mapping the results. Spatial entities with similar characteristics are grouped together in clusters by the use of a fuzzy c-means algorithm. Each cluster represents a specific level of urban growth and development. A two-layer feed-forward multilayer perceptron artificial neural network is then used to predict urban growth. The model, applied to the prefecture of Attica, Greece, delineates the current and future evolution trends of the Athens metropolitan area, which are illustrated by maps of the urban growth dynamics. The proposed methodology aims to assist planners and decision makers in gaining insight into the transition from rural to urban. © 2012 Elsevier Ltd.","Athens metropolitan area; Fuzzy clustering; Neural networks; Urban growth","artificial intelligence; artificial neural network; decision making; fuzzy mathematics; GIS; mapping; metropolitan area; spatial planning; spatiotemporal analysis; urban growth; urban planning; Athens [Attica]; Attica; Greece",Article,Scopus,2-s2.0-84871934138
"Malitsky Y., Sabharwal A., Samulowitz H., Sellmann M.","Algorithm portfolios based on cost-sensitive hierarchical clustering",2013,"IJCAI International Joint Conference on Artificial Intelligence",36,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063318&partnerID=40&md5=d1f095bd0679ca79167aa43b8d9deae4","Different solution approaches for combinatorial problems often exhibit incomparable performance that depends on the concrete problem instance to be solved. Algorithm portfolios aim to combine the strengths of multiple algorithmic approaches by training a classifier that selects or schedules solvers dependent on the given instance. We devise a new classifier that selects solvers based on a cost-sensitive hierarchical clustering model. Experimental results on SAT and MaxSAT show that the new method outperforms the most effective portfolio builders to date.",,"Algorithm portfolios; Algorithmic approach; Combinatorial problem; Cost-sensitive; Hier-archical clustering; Max-SAT; Problem instances; Solution approach; Artificial intelligence; Algorithms",Conference Paper,Scopus,2-s2.0-84896063318
"Lutz C., Seylan I., Toman D., Wolter F.","The combined approach to OBDA: Taming role hierarchies using filters",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",36,10.1007/978-3-642-41335-3_20,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891952522&doi=10.1007%2f978-3-642-41335-3_20&partnerID=40&md5=fb7e43468e56a30cc3e2bec7342c057b","The basic idea of the combined approach to query answering in the presence of ontologies is to materialize the consequences of the ontology in the data and then use a limited form of query rewriting to deal with infinite materializations. While this approach is efficient and scalable for ontologies that are formulated in the basic version of the description logic DL-Lite, it incurs an exponential blowup during query rewriting when DL-Lite is extended with the popular role hierarchies. In this paper, we show how to replace the query rewriting with a filtering technique. This is natural from an implementation perspective and allows us to handle role hierarchies without an exponential blowup. We also carry out an experimental evaluation that demonstrates the scalability of this approach. © 2013 Springer-Verlag.",,"Description logic; Dl-lite; Experimental evaluation; Filtering technique; Query answering; Query rewritings; Role hierarchy; Artificial intelligence; Computer science; Computers; Data description",Conference Paper,Scopus,2-s2.0-84891952522
"Kong H., Akakin H.C., Sarma S.E.","A generalized laplacian of gaussian filter for blob detection and its applications",2013,"IEEE Transactions on Cybernetics",36,10.1109/TSMCB.2012.2228639,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890090297&doi=10.1109%2fTSMCB.2012.2228639&partnerID=40&md5=1289df22d00fd2039a5a9982410ad5a3","In this paper, we propose a generalized Laplacian of Gaussian (LoG) (gLoG) filter for detecting general elliptical blob structures in images. The gLoG filter can not only accurately locate the blob centers but also estimate the scales, shapes, and orientations of the detected blobs. These functions can be realized by generalizing the common 3-D LoG scale-space blob detector to a 5-D gLoG scale-space one, where the five parameters are image- domain coordinates (x, y), scales (σx, σy), and orientation (Θ), respectively. Instead of searching the local extrema of the image's 5-D gLoG scale space for locating blobs, a more feasible solution is given by locating the local maxima of an intermediate map, which is obtained by aggregating the log-scale-normalized convolution responses of each individual gLoG filter. The proposed gLoG-based blob detector is applied to both biomedical images and natural ones such as general road-scene images. For the biomedical applications on pathological and fluorescent microscopic images, the gLoG blob detector can accurately detect the centers and estimate the sizes and orientations of cell nuclei. These centers are utilized as markers for a watershed-based touchingcell splitting method to split touching nuclei and counting cells in segmentation-free images. For the application on road images, the proposed detector can produce promising estimation of texture orientations, achieving an accurate texture-based road vanishing point detection method. The implementation of our method is quite straightforward due to a very small number of tunable parameters. © 2013 IEEE.","Blob detection; generalized Laplacian of Gaussian (LoG) (gLoG); nuclei (cell) splitting; scale space; texture orientation estimation; vanishing point detection","Blob detection; Laplacian of Gaussian; nuclei (cell) splitting; Scale spaces; Texture orientation; Vanishing point detection; Estimation; Image segmentation; Laplace transforms; Medical applications; Roads and streets; Textures; Detectors; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; methodology; normal distribution; statistical analysis; three dimensional imaging; Artificial Intelligence; Data Interpretation, Statistical; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Normal Distribution; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84890090297
"Adaszewski S., Dukart J., Kherif F., Frackowiak R., Draganski B.","How early can we predict Alzheimer's disease using computational anatomy?",2013,"Neurobiology of Aging",36,10.1016/j.neurobiolaging.2013.06.015,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884125608&doi=10.1016%2fj.neurobiolaging.2013.06.015&partnerID=40&md5=a581027cc85718550d5d3c0e4a34c0d2","Computational anatomy with magnetic resonance imaging (MRI) is well established as a noninvasive biomarker of Alzheimer's disease (AD); however, there is less certainty about its dependency on the staging of AD. We use classical group analyses and automated machine learning classification of standard structural MRI scans to investigate AD diagnostic accuracy from the preclinical phase to clinical dementia. Longitudinal data from the Alzheimer's Disease Neuroimaging Initiative were stratified into 4 groups according to the clinical status-(1) AD patients; (2) mild cognitive impairment (MCI) converters; (3) MCI nonconverters; and (4) healthy controls-and submitted to a support vector machine. The obtained classifier was significantly above the chance level (62%) for detecting AD already 4 years before conversion from MCI. Voxel-based univariate tests confirmed the plausibility of our findings detecting a distributed network of hippocampal-temporoparietal atrophy in AD patients. We also identified a subgroup of control subjects with brain structure and cognitive changes highly similar to those observed in AD. Our results indicate that computational anatomy can detect AD substantially earlier than suggested by current models. The demonstrated differential spatial pattern of atrophy between correctly and incorrectly classified AD patients challenges the assumption of a uniform pathophysiological process underlying clinically identified AD. © 2013 Elsevier Inc.","Alzheimer's disease; Biomarker; Mild cognitive impairment; Structural magnetic resonance imaging","aged; Alzheimer disease; amygdaloid nucleus; angular gyrus; article; brain region; brain size; computer assisted diagnosis; computer prediction; controlled study; diagnostic accuracy; disease marker; female; hippocampus; human; information processing; information retrieval; insula; machine learning; major clinical study; male; mild cognitive impairment; nuclear magnetic resonance imaging; parahippocampal gyrus; parietal lobe; predictive value; priority journal; putamen; support vector machine; supramarginal gyrus; thalamus; voxel based morphometry; Alzheimer's disease; Biomarker; Mild cognitive impairment; Structural magnetic resonance imaging; Aged; Aged, 80 and over; Alzheimer Disease; Anatomy; Artificial Intelligence; Atrophy; Brain; Early Diagnosis; Female; Forecasting; Hippocampus; Humans; Magnetic Resonance Imaging; Male; Mild Cognitive Impairment; Parietal Lobe; Temporal Lobe; Time Factors",Article,Scopus,2-s2.0-84884125608
"Chen Q., Leng T., Zheng L., Kutzscher L., Ma J., De Sisternes L., Rubin D.L.","Automated drusen segmentation and quantification in SD-OCT images",2013,"Medical Image Analysis",36,10.1016/j.media.2013.06.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880644934&doi=10.1016%2fj.media.2013.06.003&partnerID=40&md5=169c9c71c7e5fd9aa1eb95c8d7482080","Spectral domain optical coherence tomography (SD-OCT) is a useful tool for the visualization of drusen, a retinal abnormality seen in patients with age-related macular degeneration (AMD); however, objective assessment of drusen is thwarted by the lack of a method to robustly quantify these lesions on serial OCT images. Here, we describe an automatic drusen segmentation method for SD-OCT retinal images, which leverages a priori knowledge of normal retinal morphology and anatomical features. The highly reflective and locally connected pixels located below the retinal nerve fiber layer (RNFL) are used to generate a segmentation of the retinal pigment epithelium (RPE) layer. The observed and expected contours of the RPE layer are obtained by interpolating and fitting the shape of the segmented RPE layer, respectively. The areas located between the interpolated and fitted RPE shapes (which have nonzero area when drusen occurs) are marked as drusen. To enhance drusen quantification, we also developed a novel method of retinal projection to generate an en face retinal image based on the RPE extraction, which improves the quality of drusen visualization over the current approach to producing retinal projections from SD-OCT images based on a summed-voxel projection (SVP), and it provides a means of obtaining quantitative features of drusen in the en face projection. Visualization of the segmented drusen is refined through several post-processing steps, drusen detection to eliminate false positive detections on consecutive slices, drusen refinement on a projection view of drusen, and drusen smoothing. Experimental evaluation results demonstrate that our method is effective for drusen segmentation. In a preliminary analysis of the potential clinical utility of our methods, quantitative drusen measurements, such as area and volume, can be correlated with the drusen progression in non-exudative AMD, suggesting that our approach may produce useful quantitative imaging biomarkers to follow this disease and predict patient outcome. © 2013 Elsevier B.V.","AMD; Drusen segmentation; Projection image; Retinal pigment epithelium; SD-OCT","Age-related macular degeneration; AMD; False positive detection; Projection image; Retinal nerve fiber layers; Retinal pigment epithelium; SD-OCT; Spectral domain optical coherence tomographies; Ophthalmology; Optical tomography; Visualization; Image segmentation; aged; article; drusen; female; human; major clinical study; male; morphology; nerve fiber; outcome assessment; pigment epithelium; prediction; priority journal; retina image; retina macula age related degeneration; spectral domain optical coherence tomography; AMD; Drusen segmentation; Projection image; Retinal pigment epithelium; SD-OCT; Algorithms; Artificial Intelligence; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Optic Disk Drusen; Pattern Recognition, Automated; Reproducibility of Results; Retinoscopy; Sensitivity and Specificity; Tomography, Optical Coherence",Article,Scopus,2-s2.0-84880644934
"Ju W., Greene C.S., Eichinger F., Nair V., Hodgin J.B., Bitzer M., Lee Y.-S., Zhu Q., Kehata M., Li M., Jiang S., Pia Rastaldi M., Cohen C.D., Troyanskaya O.G., Kretzler M.","Defining cell-type specificity at the transcriptional level in human disease",2013,"Genome Research",36,10.1101/gr.155697.113,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884412556&doi=10.1101%2fgr.155697.113&partnerID=40&md5=fea5d9f3570387f5c27bf09b6e955fb5","Cell-lineage-specific transcripts are essential for differentiated tissue function, implicated in hereditary organ failure, and mediate acquired chronic diseases. However, experimental identification of cell-lineage-specific genes in a genome-scale manner is infeasible for most solid human tissues. We developed the first genome-scale method to identify genes with celllineage-specific expression, even in lineages not separable by experimental microdissection. Our machine-learning-based approach leverages high-throughput data from tissue homogenates in a novel iterative statistical framework. We applied this method to chronic kidney disease and identified transcripts specific to podocytes, key cells in the glomerular filter responsible for hereditary and most acquired glomerular kidney disease. In a systematic evaluation of our predictions by immunohistochemistry, our in silico approach was significantly more accurate (65% accuracy in human) than predictions based on direct measurement of in vivo fluorescence-tagged murine podocytes (23%). Our method identified genes implicated as causal in hereditary glomerular disease and involved in molecular pathways of acquired and chronic renal diseases. Furthermore, based on expression analysis of human kidney disease biopsies, we demonstrated that expression of the podocyte genes identified by our approach is significantly related to the degree of renal impairment in patients. Our approach is broadly applicable to define lineage specificity in both cell physiology and human disease contexts. We provide a user-friendly website that enables researchers to apply this method to any cell-lineage or tissue of interest. Identified cell-lineage-specific transcripts are expected to play essential tissue-specific roles in organogenesis and disease and can provide starting points for the development of organ-specific diagnostics and therapies. © 2013 Mesner et al.",,"article; cell lineage; cell specificity; chronic kidney disease; computer model; controlled study; fluorescence analysis; gene expression; genetic disorder; genetic transcription; glomerulus filtration; high throughput sequencing; human; human cell; human tissue; immunohistochemistry; in vivo study; kidney biopsy; microdissection; podocyte; prediction; priority journal; tissue homogenate; Animals; Artificial Intelligence; Biopsy; Cell Differentiation; Cell Lineage; Computational Biology; Computer Simulation; Databases, Genetic; Gene Expression Profiling; Genome, Human; Humans; Kidney Diseases; Mice; Nanotechnology; Organ Specificity; Organogenesis; Podocytes; Renal Insufficiency, Chronic",Article,Scopus,2-s2.0-84884412556
"He W., Mi G., Tan Y.","Parameter optimization of local-concentration model for spam detection by using fireworks algorithm",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",36,10.1007/978-3-642-38703-6_52,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884891447&doi=10.1007%2f978-3-642-38703-6_52&partnerID=40&md5=1d772155579870fa1e359ab6045a4649","This paper proposes a new framework that optimizes anti-spam model with heuristic swarm intelligence optimization algorithms, and this framework could integrate various classifiers and feature extraction methods. In this framework, a swarm intelligence algorithm is utilized to optimize a parameter vector, which is composed of parameters of a feature extraction method and parameters of a classifier, considering the spam detection problem as an optimization process which aims to achieve the lowest error rate. Also, 2 experimental strategies were designed to objectively reflect the performance of the framework. Then, experiments were conducted, using the Fireworks Algorithm (FWA) as the swarm intelligence algorithm, the Local Concentration (LC) approach as the feature extraction method, and SVM as the classifier. Experimental results demonstrate that the framework improves the performance on the corpora PU1, PU2, PU3 and PUA, while the computational efficiency is applicable in real world. © 2013 Springer-Verlag Berlin Heidelberg.","Fireworks Algorithm; Local Concentration Approach; Parameter Optimization; Spam Detection","Experimental strategy; Feature extraction methods; Fireworks algorithms; Local Concentration Approach; Parameter optimization; Spam detection; Swarm intelligence algorithms; Swarm intelligence optimization algorithm; Algorithms; Artificial intelligence; Explosives; Feature extraction; Internet; Optimization; Parameter estimation",Conference Paper,Scopus,2-s2.0-84884891447
"Zhang H., Reardon C., Parker L.E.","Real-time multiple human perception with color-depth cameras on a mobile robot",2013,"IEEE Transactions on Cybernetics",36,10.1109/TCYB.2013.2275291,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890372203&doi=10.1109%2fTCYB.2013.2275291&partnerID=40&md5=f10ac47ddabf073756d7a79aa2b625a5","The ability to perceive humans is an essential requirement for safe and efficient human-robot interaction. In realworld applications, the need for a robot to interact in real time with multiple humans in a dynamic, 3-D environment presents a significant challenge. The recent availability of commercial color-depth cameras allow for the creation of a system that makes use of the depth dimension, thus enabling a robot to observe its environment and perceive in the 3-D space. Here we present a system for 3-D multiple human perception in real time from a moving robot equipped with a color-depth camera and a consumer-grade computer. Our approach reduces computation time to achieve real-time performance through a unique combination of new ideas and established techniques. We remove the ground and ceiling planes from the 3-D point cloud input to separate candidate point clusters. We introduce the novel information concept, depth of interest, which we use to identify candidates for detection, and that avoids the computationally expensive scanning-window methods of other approaches. We utilize a cascade of detectors to distinguish humans from objects, in which we make intelligent reuse of intermediary features in successive detectors to improve computation. Because of the high computational cost of some methods, we represent our candidate tracking algorithm with a decision directed acyclic graph, which allows us to use the most computationally intense techniques only where necessary. We detail the successful implementation of our novel approach on a mobile robot and examine its performance in scenarios with real-world challenges, including occlusion, robot motion, nonupright humans, humans leaving and reentering the field of view (i.e., the reidentification challenge), human-object and human-human interaction. We conclude with the observation that the incorporation of the depth information, together with the use of modern techniques in new ways, we are able to create an accurate system for real-time 3-D perception of humans by a mobile robot. © 2013 IEEE.","3-D vision; Depth of interest; Human detection and tracking; Human perception; RGB-D camera application","3-D vision; Depth of interest; Human detection and tracking; Human perception; Rgb-d cameras; Color; Human computer interaction; Mobile robots; Cameras; actimetry; algorithm; article; artificial intelligence; automated pattern recognition; color; colorimetry; computer; computer simulation; computer system; equipment; human; image enhancement; methodology; motion; recreation; robotics; three dimensional imaging; transducer; whole body imaging; automated pattern recognition; colorimetry; devices; procedures; robotics; three dimensional imaging; whole body imaging; Actigraphy; Algorithms; Artificial Intelligence; Color; Colorimetry; Computer Peripherals; Computer Simulation; Computer Systems; Humans; Image Enhancement; Imaging, Three-Dimensional; Motion; Pattern Recognition, Automated; Robotics; Transducers; Video Games; Whole Body Imaging; Actigraphy; Algorithms; Artificial Intelligence; Color; Colorimetry; Computer Peripherals; Computer Simulation; Computer Systems; Humans; Image Enhancement; Imaging, Three-Dimensional; Motion; Pattern Recognition, Automated; Robotics; Transducers; Video Games; Whole Body Imaging",Article,Scopus,2-s2.0-84890372203
"Cai S., Su K.","Local search for Boolean Satisfiability with configuration checking and subscore",2013,"Artificial Intelligence",36,10.1016/j.artint.2013.09.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884536334&doi=10.1016%2fj.artint.2013.09.001&partnerID=40&md5=e6b41b9aef46bb8c1ef929d1598f4d83","This paper presents and analyzes two new efficient local search strategies for the Boolean Satisfiability (SAT) problem. We start by proposing a local search strategy called configuration checking (CC) for SAT. The CC strategy results in a simple local search algorithm for SAT called Swcc, which shows promising experimental results on random 3-SAT instances, and outperforms TNM, the winner of SAT Competition 2009. However, the CC strategy for SAT is still in a nascent stage, and Swcc cannot yet compete with Sparrow2011, which won SAT Competition 2011 just after Swcc had been designed. The CC strategy seems too strict in that it forbids flipping those variables even with great scores, if they do not satisfy the CC criterion. We improve the CC strategy by adopting an aspiration mechanism, and get a new variable selection heuristic called configuration checking with aspiration (CCA). The CCA heuristic leads to an improved algorithm called Swcca, which exhibits state-of-the-art performance on random 3-SAT instances and crafted ones. The third contribution concerns improving local search algorithms for random k-SAT instances with k>3. Although the SAT community has made great achievements in solving random 3-SAT instances, the progress lags far behind on random k-SAT instances with k>3. This work proposes a new variable property called subscore, which is utilized to break ties in the CCA heuristic when candidate variables for flipping have the same score. The resulting algorithm CCAsubscore is very efficient for solving random k-SAT instances with k>3, and significantly outperforms other state-of-the-art ones. Combining Swcca and CCAsubscore, we obtain a local search SAT solver called CCASat, which was ranked first in the random track of SAT Challenge 2012. Additionally, we perform theoretical analyses on the CC strategy and the subscore property, and show interesting results on these two heuristics. Particularly, our analysis indicates that the CC strategy is more effective for k-SAT with smaller k, while the subscore notion is not suitable for solving random 3-SAT. © 2013 Elsevier B.V.","Configuration checking; Local search; SAT; Subscore","Boolean satisfiability; Configuration checking; Local search; Local search algorithm; Local search strategy; SAT; State-of-the-art performance; Subscore; Artificial intelligence; Learning algorithms",Article,Scopus,2-s2.0-84884536334
"Zalasiński M., Łapa K., Cpałka K.","New algorithm for evolutionary selection of the dynamic signature global features",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",36,10.1007/978-3-642-38610-7_11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884358276&doi=10.1007%2f978-3-642-38610-7_11&partnerID=40&md5=612a17d8c46996b9dece08bc683b1995","Methods using dynamic signature for identity verification may be divided into three main categories: global methods, local function based methods and regional function based methods. Global methods base on a set of global parametric features, which are extracted from signature of user. Global feature extraction methods have been often presented in the literature. Another interesting task is selection of a features group which will be considered individually for each user during training and verification process. In this paper we propose a new approach to automatic evolutionary selection of the dynamic signature global features. Our method was tested with use of the SVC2004 public on-line signature database. © 2013 Springer-Verlag.",,"Dynamic signature; Evolutionary selection; Global methods; Identity verification; Local functions; New approaches; Online signature; Verification process; Artificial intelligence; Soft computing; Feature extraction",Conference Paper,Scopus,2-s2.0-84884358276
"Zalasiński M., Cpałka K.","New approach for the on-line signature verification based on method of horizontal partitioning",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",36,10.1007/978-3-642-38610-7_32,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884368162&doi=10.1007%2f978-3-642-38610-7_32&partnerID=40&md5=aeb8732c0a834ef64fa83a0c99d43a30","Identity verification is one of the biometric issues which may be realized using dynamic signature biometric attribute. One of the methods of signature verification is the method based on partitioning of signature trajectories. In this paper we propose a new method for verification of signature which signals were horizontally partitioned. This method assumes use of all partitions during classification process. Classifier presented in our method is based on the flexible neuro-fuzzy system of the Mamdani type. The algorithm was tested with use of the BioSecure Database (BMDB) distributed by the BioSecure Association. © 2013 Springer-Verlag.",,"Classification process; Dynamic signature; Identity verification; Neurofuzzy system; New approaches; On-line signature verification; Signature verification; Verification of signature; Artificial intelligence; Biometrics; Electronic document identification systems; Soft computing",Conference Paper,Scopus,2-s2.0-84884368162
"Łapa K., Zalasiński M., Cpałka K.","A new method for designing and complexity reduction of neuro-fuzzy systems for nonlinear modelling",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",36,10.1007/978-3-642-38658-9_30,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884408933&doi=10.1007%2f978-3-642-38658-9_30&partnerID=40&md5=4bafa43cbd25d07a39f76b48933c49d2","In this paper we propose a new method for evolutionary selection of parameters and structure of neuro-fuzzy system for nonlinear modelling. This method allows maintain the correct proportions between accuracy, complexity and interpretability of the system. Our algorithm has been tested using well-known benchmarks. © 2013 Springer-Verlag.",,"Complexity reduction; Evolutionary selection; Interpretability; Neurofuzzy system; Non-linear modelling; Artificial intelligence; Fuzzy systems; Soft computing; Nonlinear systems",Conference Paper,Scopus,2-s2.0-84884408933
"Kovac P., Rodic D., Pucovsky V., Savkovic B., Gostimirovic M.","Application of fuzzy logic and regression analysis for modeling surface roughness in face milliing",2013,"Journal of Intelligent Manufacturing",36,10.1007/s10845-012-0623-z,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880318213&doi=10.1007%2fs10845-012-0623-z&partnerID=40&md5=7c4e4eeaeac795f156000216987a0c49","The objective of this study is to examine the influence of machining parameters on surface finish in face milling. A new approach in modeling surface roughness which uses artificial intelligence tools is described in this paper. This paper focuses on developing empirical models using fuzzy logic and regression analysis. The values of surface roughness predicted by these models are then compared. The results showed that the proposed system can significantly increase the accuracy of the product profile when compared to the conventional approaches, like regression analysis. The results indicate that the fuzzy logic modeling technique can be effectively used for the prediction of surface roughness in dry machining. © 2012 Springer Science+Business Media, LLC.","Fuzzy logic; Regression analysis; Surface roughness","Artificial intelligence tools; Conventional approach; Dry machining; Empirical model; Fuzzy logic modeling; Machining parameters; New approaches; Surface finishes; Artificial intelligence; Fuzzy logic; Regression analysis; Statistics; Surface roughness",Article,Scopus,2-s2.0-84880318213
"Sidorov G., Miranda-Jiménez S., Viveros-Jiménez F., Gelbukh A., Castro-Sánchez N., Velásquez F., Díaz-Rangel I., Suárez-Guerra S., Treviño A., Gordon J.","Empirical study of machine learning based approach for opinion mining in tweets",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",36,10.1007/978-3-642-37807-2_1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875822753&doi=10.1007%2f978-3-642-37807-2_1&partnerID=40&md5=78c87982a04333e922fa16b55a31586f","Opinion mining deals with determining of the sentiment orientation- positive, negative, or neutral-of a (short) text. Recently, it has attracted great interest both in academia and in industry due to its useful potential applications. One of the most promising applications is analysis of opinions in social networks. In this paper, we examine how classifiers work while doing opinion mining over Spanish Twitter data. We explore how different settings (n-gram size, corpus size, number of sentiment classes, balanced vs. unbalanced corpus, various domains) affect precision of the machine learning algorithms. We experimented with Naïve Bayes, Decision Tree, and Support Vector Machines. We describe also language specific preprocessing-in our case, for Spanish language-of tweets. The paper presents best settings of parameters for practical applications of opinion mining in Spanish Twitter. We also present a novel resource for analysis of emotions in texts: a dictionary marked with probabilities to express one of the six basic emotions(Probability Factor of Affective use (PFA)(Spanish Emotion Lexicon that contains 2,036 words. © 2013 Springer-Verlag.","Opinion mining; sentiment analysis; sentiment classification; Spanish Emotion Lexicon; Spanish Twitter corpus","Opinion mining; Sentiment analysis; Sentiment classification; Spanish Emotion Lexicon; Spanish Twitter corpus; Artificial intelligence; Decision trees; Learning algorithms; Learning systems; Social networking (online); Data mining",Conference Paper,Scopus,2-s2.0-84875822753
"Cambria E., Mazzocco T., Hussain A.","Application of multi-dimensional scaling and artificial neural networks for biologically inspired opinion mining",2013,"Biologically Inspired Cognitive Architectures",36,10.1016/j.bica.2013.02.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876940246&doi=10.1016%2fj.bica.2013.02.003&partnerID=40&md5=accb7b5a24b7ac06c22862d90c482cee","The way people express their opinions has radically changed in the past few years thanks to the advent of online collaborative media. The distillation of knowledge from this huge amount of unstructured information can be a key factor for marketers who want to create an identity for their product or brand in the minds of their customers. These online social data, however, remain hardly accessible to computers, as they are specifically meant for human consumption. Existing approaches to opinion mining, in fact, are still far from being able to infer the cognitive and affective information associated with natural language as they mainly rely on knowledge bases that are too limited to efficiently process text at concept-level. In this context, standard clustering techniques have been previously employed on an affective common-sense knowledge base in attempt to discover how different natural language concepts are semantically and affectively related to each other and, hence, to accordingly mine on-line opinions. In this work, a novel cognitive model based on the combined use of multi-dimensional scaling and artificial neural networks is exploited for better modelling the way multi-word expressions are organised in a brain-like universe of natural language concepts. The integration of a biologically inspired paradigm with standard principal component analysis helps to better grasp the non-linearities of the resulting vector space and, hence, improve the affective common-sense reasoning capabilities of the system. © 2012 Elsevier B.V. All rights reserved.","AI; ANN; Cognitive modelling; NLP; Sentic computing","ANN; Clustering techniques; Cognitive modelling; Commonsense reasoning; Multi-dimensional scaling; Multi-word expressions; NLP; Sentic Computing; Artificial intelligence; Data mining; Distillation; Knowledge based systems; Principal component analysis; Neural networks",Article,Scopus,2-s2.0-84876940246
"Okamoto T., Takashima K.","Decentralized attribute-based signatures",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",36,10.1007/978-3-642-36362-7_9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873970737&doi=10.1007%2f978-3-642-36362-7_9&partnerID=40&md5=e0e403510566947173ca4fbe0f9ba3a0","We present the first decentralized multi-authority attribute-based signature (DMA-ABS) scheme, in which no central authority and no trusted setup are required. The proposed DMA-ABS scheme for a large class of (non-monotone) predicates is fully secure (adaptive-predicate unforgeable and perfectly private) under a standard assumption, the decisional linear (DLIN) assumption, in the random oracle model. Our DMA-ABS scheme is comparably as efficient as the most efficient ABS scheme. As a by-product, this paper also presents an adaptively secure DMA functional encryption (DMA-FE) scheme under the DLIN assumption. © 2013 International Association for Cryptologic Research.",,"Attribute-based signatures; Fully secure; Functional encryptions; Multi authorities; Random Oracle model; Standard assumptions; Artificial intelligence; Public key cryptography",Conference Paper,Scopus,2-s2.0-84873970737
"Backes A.R., Casanova D., Bruno O.M.","Texture analysis and classification: A complex network-based approach",2013,"Information Sciences",36,10.1016/j.ins.2012.07.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867577309&doi=10.1016%2fj.ins.2012.07.003&partnerID=40&md5=7190be5bb50ed9dabd88c79e1277c947","In this paper, we propose a novel texture analysis method using the complex network theory. We investigated how a texture image can be effectively represented, characterized and analyzed in terms of a complex network. The proposed approach uses degree measurements to compose a set of texture descriptors. The results show that the method is very robust, and it presents a excellent texture discrimination for all considered classes, overcoming traditional texture methods. © 2012 Elsevier Inc. All rights reserved.","Complex network; Texture analysis; Texture recognition","Complex network theory; Complex networks; Degree measurements; Network-based approach; Texture analysis; Texture analysis method; Texture descriptors; Texture discrimination; Texture image; Texture recognition; Artificial intelligence; Software engineering; Textures",Article,Scopus,2-s2.0-84867577309
"Azzopardi G., Azzopardi N.","Trainable COSFIRE filters for keypoint detection and pattern recognition",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",36,10.1109/TPAMI.2012.106,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871734644&doi=10.1109%2fTPAMI.2012.106&partnerID=40&md5=b896a12831b4df36764dd3dd41a06c85","Background: Keypoint detection is important for many computer vision applications. Existing methods suffer from insufficient selectivity regarding the shape properties of features and are vulnerable to contrast variations and to the presence of noise or texture. Methods: We propose a trainable filter which we call Combination Of Shifted FIlter REsponses (COSFIRE) and use for keypoint detection and pattern recognition. It is automatically configured to be selective for a local contour pattern specified by an example. The configuration comprises selecting given channels of a bank of Gabor filters and determining certain blur and shift parameters. A COSFIRE filter response is computed as the weighted geometric mean of the blurred and shifted responses of the selected Gabor filters. It shares similar properties with some shape-selective neurons in visual cortex, which provided inspiration for this work. Results: We demonstrate the effectiveness of the proposed filters in three applications: the detection of retinal vascular bifurcations (DRIVE dataset: 98.50 percent recall, 96.09 percent precision), the recognition of handwritten digits (MNIST dataset: 99.48 percent correct classification), and the detection and recognition of traffic signs in complex scenes (100 percent recall and precision). Conclusions: The proposed COSFIRE filters are conceptually simple and easy to implement. They are versatile keypoint detectors and are highly effective in practical computer vision applications. © 2012 IEEE.","Feature detection; feature representation; medical information systems; object recognition; optical character recognition; shape","Complex scenes; Computer vision applications; Contrast variation; Data sets; Feature detection; Feature representation; Filter response; Geometric mean; Handwritten digit; Keypoint detection; Recall and precision; shape; Shape property; Shift parameters; Visual cortexes; Classification (of information); Computer vision; Gabor filters; Medical information systems; Object recognition; Optical character recognition; Detectors; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; image enhancement; image subtraction; methodology; reproducibility; sensitivity and specificity; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-84871734644
"Falco I.D.","Differential Evolution for automatic rule extraction from medical databases",2013,"Applied Soft Computing Journal",36,10.1016/j.asoc.2012.10.022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881609308&doi=10.1016%2fj.asoc.2012.10.022&partnerID=40&md5=29ee9b0219bfe29045ec4114a634482e","In this paper, a new approach based on Differential Evolution (DE) for the automatic classification of items in medical databases is proposed. Based on it, a tool called DEREx is presented, which automatically extracts explicit knowledge from the database under the form of IF-THEN rules containing AND-connected clauses on the database variables. Each DE individual codes for a set of rules. For each class more than one rule can be contained in the individual, and these rules can be seen as logically connected in OR. Furthermore, all the classifying rules for all the classes are found all at once in one step. DEREx is thought as a useful support to decision making whenever explanations on why an item is assigned to a given class should be provided, as it is the case for diagnosis in the medical domain. The major contribution of this paper is that DEREx is the first classification tool in literature that is based on DE and automatically extracts sets of IF-THEN rules without the intervention of any other mechanism. In fact, all other classification tools based on DE existing in literature either simply find centroids for the classes rather than extracting rules, or are hybrid systems in which DE simply optimizes some parameters whereas the classification capabilities are provided by other mechanisms. For the experiments eight databases from the medical domain have been considered. First, among ten classical DE variants, the most effective of them in terms of highest classification accuracy in a ten-fold cross-validation has been found. Secondly, the tool has been compared over the same eight databases against a set of fifteen classifiers widely used in literature. The results have proven the effectiveness of the proposed approach, since DEREx turns out to be the best performing tool in terms of highest classification accuracy. Also statistical analysis has confirmed that DEREx is the best classifier. When compared to the other rule-based classification tools here used, DEREx needs the lowest average number of rules to face a problem, and the average number of clauses per rule is not very high. In conclusion, the tool here presented is preferable to the other classifiers because it shows good classification accuracy, automatically extracts knowledge, and provides users with it under an easily comprehensible form. © 2012 Elsevier B.V. All rights reserved.","Automatic rule extraction; Classification; Decision Support System; Differential Evolution; Medical diagnosis","Artificial intelligence; Computer aided diagnosis; Database systems; Decision making; Decision support systems; Diagnosis; Evolutionary algorithms; Extraction; Hybrid systems; Medical computing; Optimization; Automatic classification; Automatic rule extraction; Classification accuracy; Classification tool; Cross validation; Differential Evolution; Explicit knowledge; Rule-based classification; Classification (of information)",Article,Scopus,2-s2.0-84881609308
"Dredze M., Paul M.J., Bergsma S., Tran H.","Carmen: A twitter geolocation system with applications to public health",2013,"AAAI Workshop - Technical Report",36,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898859765&partnerID=40&md5=ca98eaf7ba9964798fff3394c0698d31","Public health applications using social media often require accurate, broad-coverage location information. However, the standard information provided by social media APIs, such as Twitter, cover a limited number of messages. This paper presents Carmen, a geolocation system that can determine structured location information for messages provided by the Twitter API. Our system utilizes geocoding tools and a combination of automatic and manual alias resolution methods to infer location structures from GPS positions and user-provided profile data. We show that our system is accurate and covers many locations, and we demonstrate its utility for improving influenza surveillance. Copyright © 2013, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Artificial intelligence; Public health; Tracking (position); Geo coding; Geolocation systems; Location information; Profile data; Resolution methods; Social media; Standard information; Social networking (online)",Conference Paper,Scopus,2-s2.0-84898859765
"Krishnamoorthy N., Malkarnenkar G., Mooney R., Saenko K., Guadarrama S.","Generating natural-language video descriptions using text-mined knowledge",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",35,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893398951&partnerID=40&md5=5414716bb3cbca5b5cfa1312bac379b2","We present a holistic data-driven technique that generates natural-language descriptions for videos. We combine the output of state-of-the-art object and activity detectors with ""real-world"" knowledge to select the most probable subject-verb-object triplet for describing a video. We show that this knowledge, automatically mined from web-scale text corpora, enhances the triplet selection algorithm by providing it contextual information and leads to a four-fold increase in activity identification. Unlike previous methods, our approach can annotate arbitrary videos without requiring the expensive collection and annotation of a similar training video corpus. We evaluate our technique against a baseline that does not use text-mined knowledge and show that humans prefer our descriptions 61% of the time. Copyrightc 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Contextual information; Real-world; Selection algorithm; Text corpora; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84893398951
"Fijani E., Nadiri A.A., Asghari Moghaddam A., Tsai F.T.-C., Dixon B.","Optimization of drastic method by supervised committee machine artificial intelligence to assess groundwater vulnerability for maragheh-bonab plain aquifer, Iran",2013,"Journal of Hydrology",35,10.1016/j.jhydrol.2013.08.038,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884383148&doi=10.1016%2fj.jhydrol.2013.08.038&partnerID=40&md5=60240d8ff252ddced6233d7fa0fd52a7","Contamination of wells with nitrate-N (NO3-N) poses various threats to human health. Contamination of groundwater is a complex process and full of uncertainty in regional scale. Development of an integrative vulnerability assessment methodology can be useful to effectively manage (including prioritization of limited resource allocation to monitor high risk areas) and protect this valuable freshwater source. This study introduces a supervised committee machine with artificial intelligence (SCMAI) model to improve the DRASTIC method for groundwater vulnerability assessment for the Maragheh-Bonab plain aquifer in Iran. Four different AI models are considered in the SCMAI model, whose input is the DRASTIC parameters. The SCMAI model improves the committee machine artificial intelligence (CMAI) model by replacing the linear combination in the CMAI with a nonlinear supervised ANN framework. To calibrate the AI models, NO3-N concentration data are divided in two datasets for the training and validation purposes. The target value of the AI models in the training step is the corrected vulnerability indices that relate to the first NO3-N concentration dataset. After model training, the AI models are verified by the second NO3-N concentration dataset. The results show that the four AI models are able to improve the DRASTIC method. Since the best AI model performance is not dominant, the SCMAI model is considered to combine the advantages of individual AI models to achieve the optimal performance. The SCMAI method re-predicts the groundwater vulnerability based on the different AI model prediction values. The results show that the SCMAI outperforms individual AI models and committee machine with artificial intelligence (CMAI) model. The SCMAI model ensures that no water well with high NO3-N levels would be classified as low risk and vice versa. The study concludes that the SCMAI model is an effective model to improve the DRASTIC model and provides a confident estimate of the pollution risk. © 2013 Elsevier B.V.","Artificial intelligence; DRASTIC; GIS; Groundwater vulnerability; Multimodel analysis","DRASTIC; Groundwater vulnerability; Groundwater vulnerability assessments; Linear combinations; Multi-model; Optimal performance; Vulnerability assessment methodologies; Vulnerability index; Aquifers; Artificial intelligence; Geographic information systems; Groundwater pollution; Health risks; Optimization; Risk assessment; Risk perception; Water wells; Groundwater resources; aquifer characterization; artificial intelligence; data set; GIS; groundwater pollution; groundwater resource; hydrological modeling; nitrate; numerical model; optimization; resource allocation; risk assessment; vulnerability; Bonab; East Azerbaijan; Iran; Maragheh",Article,Scopus,2-s2.0-84884383148
"Amigó E., Carrillo De Albornoz J., Chugur I., Corujo A., Gonzalo J., Martín T., Meij E., De Rijke M., Spina D.","Overview of RepLab 2013: Evaluating online reputation monitoring systems",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",35,10.1007/978-3-642-40802-1_31,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886444283&doi=10.1007%2f978-3-642-40802-1_31&partnerID=40&md5=291a67ce015dc9bffe4e590eef8aed3f","This paper summarizes the goals, organization, and results of the second RepLab competitive evaluation campaign for Online Reputation Management Systems (RepLab 2013). RepLab focused on the process of monitoring the reputation of companies and individuals, and asked participant systems to annotate different types of information on tweets containing the names of several companies: first tweets had to be classified as related or unrelated to the entity; relevant tweets had to be classified according to their polarity for reputation (Does the content of the tweet have positive or negative implications for the reputation of the entity?), clustered in coherent topics, and clusters had to be ranked according to their priority (potential reputation problems had to come first). The gold standard consists of more than 140,000 tweets annotated by a group of trained annotators supervised and monitored by reputation experts. © 2013 Springer-Verlag.","Evaluation Methodologies and Metrics; RepLab; Reputation Management; Sentiment Analysis; Test Collections; Text Clustering","Evaluation methodologies; RepLab; Reputation management; Sentiment analysis; Test Collection; Text Clustering; Artificial intelligence; Computer science; Industry",Conference Paper,Scopus,2-s2.0-84886444283
"Wibowo S., Deng H.","Consensus-based decision support for multicriteria group decision making",2013,"Computers and Industrial Engineering",35,10.1016/j.cie.2013.09.015,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886041955&doi=10.1016%2fj.cie.2013.09.015&partnerID=40&md5=75bba379e50235bd106e017105c5d12f","Consensus decision making is complex and challenging in multicriteria group decision making due to the involvement of several decision makers, the presence of multiple, and often conflicting criteria, and the existence of subjectiveness and imprecision in the decision making process. To ensure effective decisions being made, the interest of all the decision makers usually represented by the degree of consensus in the decision making process has to be adequately considered. This paper presents a consensus-based approach for effectively solving the multicriteria group decision making problem. The subjectiveness and imprecision of the decision making process is adequately handled by using intuitionistic fuzzy numbers. An interactive algorithm is developed for consensus building in the group decision making process. A decision support system framework is presented for improving the effectiveness of the consensus building process. An example is presented for demonstrating the applicability of the proposed approach for solving the multicriteria group decision making problem in real world situations. © 2013 Elsevier Ltd. All rights reserved.","Consensus building; Group decision making; Multicriteria analysis; Uncertainty modeling","Consensus buildings; Decision support system frameworks; Group Decision Making; Group decision making process; Intuitionistic Fuzzy number; Multi Criteria Analysis; Multi-criteria group decision makings; Uncertainty modeling; Artificial intelligence; Decision support systems; Fuzzy sets; Uncertainty analysis; Decision making",Article,Scopus,2-s2.0-84886041955
"Suk H.-I., Shen D.","Deep learning-based feature representation for AD/MCI classification",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",35,10.1007/978-3-642-40763-5_72,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885898432&doi=10.1007%2f978-3-642-40763-5_72&partnerID=40&md5=696e8a5f35a885c8c7bf53daaa500911","In recent years, there has been a great interest in computer-aided diagnosis of Alzheimer's Disease (AD) and its prodromal stage, Mild Cognitive Impairment (MCI). Unlike the previous methods that consider simple low-level features such as gray matter tissue volumes from MRI, mean signal intensities from PET, in this paper, we propose a deep learning-based feature representation with a stacked auto-encoder. We believe that there exist latent complicated patterns, e.g., non-linear relations, inherent in the low-level features. Combining latent information with the original low-level features helps build a robust model for AD/MCI classification with high diagnostic accuracy. Using the ADNI dataset, we conducted experiments showing that the proposed method is 95.9%, 85.0%, and 75.8% accurate for AD, MCI, and MCI-converter diagnosis, respectively. © 2013 Springer-Verlag.",,"Alzheimer's disease; Diagnostic accuracy; Feature representation; Latent information; Low-level features; Mild cognitive impairments (MCI); Nonlinear relations; Signal intensities; Artificial intelligence; Computer science; Computer aided diagnosis",Conference Paper,Scopus,2-s2.0-84885898432
"Gorelick L., Veksler O., Gaed M., Gomez J.A., Moussa M., Bauman G., Fenster A., Ward A.D.","Prostate histopathology: Learning tissue component histograms for cancer detection and classification",2013,"IEEE Transactions on Medical Imaging",35,10.1109/TMI.2013.2265334,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885143428&doi=10.1109%2fTMI.2013.2265334&partnerID=40&md5=b3b97a454c1b43593464d588f88c68bf","Radical prostatectomy is performed on approximately 40% of men with organ-confined prostate cancer. Pathologic information obtained from the prostatectomy specimen provides important prognostic information and guides recommendations for adjuvant treatment. The current pathology protocol in most centers involves primarily qualitative assessment. In this paper, we describe and evaluate our system for automatic prostate cancer detection and grading on hematoxylin & eosin-stained tissue images. Our approach is intended to address the dual challenges of large data size and the need for high-level tissue information about the locations and grades of tumors. Our system uses two stages of AdaBoost-based classification. The first provides high-level tissue component labeling of a superpixel image partitioning. The second uses the tissue component labeling to provide a classification of cancer versus noncancer, and low-grade versus high-grade cancer. We evaluated our system using 991 sub-images extracted from digital pathology images of 50 whole-mount tissue sections from 15 prostatectomy patients. We measured accuracies of 90% and 85% for the cancer versus noncancer and high-grade versus low-grade classification tasks, respectively. This system represents a first step toward automated cancer quantification on prostate digital histopathology imaging, which could pave the way for more accurately informed postprostatectomy patient care. © 1982-2012 IEEE.","Automated prostate cancer detection; Cancer grading; Digital pathology image analysis; Machine learning; Quantitative pathology; Superpixels","Adjuvant treatment; Classification tasks; Digital pathologies; Image partitioning; Prostate cancer detection; Qualitative assessments; Radical prostatectomy; Superpixels; Adaptive boosting; Diseases; Grading; Learning systems; Pathology; Urology; Tissue; eosin; hematoxylin; accuracy; article; cancer classification; cancer diagnosis; cancer grading; cancer patient; Gleason score; histopathology; human; human tissue; image analysis; male; prostate; prostate cancer; tissue section; Artificial Intelligence; Histological Techniques; Humans; Image Interpretation, Computer-Assisted; Male; Prognosis; Prostate; Prostatectomy; Prostatic Neoplasms",Article,Scopus,2-s2.0-84885143428
"Zhang L., Sedykh A., Tripathi A., Zhu H., Afantitis A., Mouchlis V.D., Melagraki G., Rusyn I., Tropsha A.","Identification of putative estrogen receptor-mediated endocrine disrupting chemicals using QSAR- and structure-based virtual screening approaches",2013,"Toxicology and Applied Pharmacology",35,10.1016/j.taap.2013.04.032,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882809589&doi=10.1016%2fj.taap.2013.04.032&partnerID=40&md5=35bedcc63ae0339fed61419e9b795e67","Identification of endocrine disrupting chemicals is one of the important goals of environmental chemical hazard screening. We report on the development of validated in silico predictors of chemicals likely to cause estrogen receptor (ER)-mediated endocrine disruption to facilitate their prioritization for future screening. A database of relative binding affinity of a large number of ERα and/or ERβ ligands was assembled (546 for ERα and 137 for ERβ). Both single-task learning (STL) and multi-task learning (MTL) continuous quantitative structure-activity relationship (QSAR) models were developed for predicting ligand binding affinity to ERα or ERβ. High predictive accuracy was achieved for ERα binding affinity (MTL R2=0.71, STL R2=0.73). For ERβ binding affinity, MTL models were significantly more predictive (R2=0.53, p. &lt;. 0.05) than STL models. In addition, docking studies were performed on a set of ER agonists/antagonists (67 agonists and 39 antagonists for ERα, 48 agonists and 32 antagonists for ERβ, supplemented by putative decoys/non-binders) using the following ER structures (in complexes with respective ligands) retrieved from the Protein Data Bank: ERα agonist (PDB ID: 1L2I), ERα antagonist (PDB ID: 3DT3), ERβ agonist (PDB ID: 2NV7), and ERβ antagonist (PDB ID: 1L2J). We found that all four ER conformations discriminated their corresponding ligands from presumed non-binders. Finally, both QSAR models and ER structures were employed in parallel to virtually screen several large libraries of environmental chemicals to derive a ligand- and structure-based prioritized list of putative estrogenic compounds to be used for in vitro and in vivo experimental validation. © 2013 Elsevier Inc.","Docking; Endocrine disrupting chemicals; Estrogen receptor; Multi-task learning; Quantitative structure-activity relationships modeling; Virtual screening","endocrine disruptor; estrogen receptor alpha; estrogen receptor beta; article; binding affinity; computer model; evaluation; ligand binding; molecular docking; protein conformation; quantitative structure activity relation; screening; 17β-estradiol; absorption, distribution, metabolism, excretion, and toxicity; AD; ADMET; AhR; androgen receptor; applicability domain; AR; area under the curve; aryl hydrocarbon receptor; AUC; Docking; E(2); EDCs; EDKB; EDSP; EF; Endocrine disrupting chemicals; endocrine disrupting chemicals; endocrine disruptor knowledge base; endocrine disruptor screening program; enrichment factor; EPA; ER; Estrogen receptor; estrogen receptor; k-nearest neighbors; kNN; MTL; multi-task learning; Multi-task learning; PDB; Protein Data Bank; QSAR; quantitative structure-activity relationships; Quantitative structure-activity relationships modeling; RBA; receiver operating characteristic; relative binding affinity; relative potency; ROC; RP; SE; sensitivity; single-task learning; SP; specificity; STL; US Environmental Protection Agency; Virtual screening; Algorithms; Artificial Intelligence; Computer Simulation; Endocrine Disruptors; Estrogen Antagonists; Estrogen Receptor alpha; Estrogen Receptor beta; High-Throughput Screening Assays; Humans; Quantitative Structure-Activity Relationship; Receptors, Estrogen; Structure-Activity Relationship; User-Computer Interface",Article,Scopus,2-s2.0-84882809589
"Kellogg S.","How to make a MOOC",2013,"Nature",35,10.1038/nj7458-369a,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880397561&doi=10.1038%2fnj7458-369a&partnerID=40&md5=a0fb85cb9ffeedbfe1254dc063f1f515",[No abstract available],,"educational development; information technology; Internet; science and technology; university sector; article; artificial intelligence; commercial phenomena; human; learning; online system; Paleogene; priority journal; robotics; science; teaching; Internet; methodology; teaching; university; Australia; Melbourne; Victoria [Australia]; Internet; Teaching; Universities",Article,Scopus,2-s2.0-84880397561
"Ferretti V., Pomarico S.","Ecological land suitability analysis through spatial indicators: An application of the Analytic Network Process technique and Ordered Weighted Average approach",2013,"Ecological Indicators",35,10.1016/j.ecolind.2013.06.005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880395554&doi=10.1016%2fj.ecolind.2013.06.005&partnerID=40&md5=d977ee2c9ef2855d7be9209377853c5e","Multicriteria-Spatial Decision Support Systems (MC-SDSS) are increasingly popular tools in decision-making processes and in policy making, thanks to their significant new capabilities in the use of spatial or geospatial information. Many spatial problems are complex and require the use of integrated analysis and models. The present paper illustrates the development of a MC-SDSS approach for studying the ecological connectivity of the Piedmont Region in Italy. The MC-SDSS model considers ecological and environmental spatial indicators which are combined by integrating the Multicriteria Decision Aiding (MCDA) technique named Analytic Network Process (ANP) and the Ordered Weighted Average (OWA) approach. The ANP is used for the elicitation of attribute weights while the OWA operator function is used to generate a wide range of decision alternatives for addressing uncertainty associated with interaction between multiple criteria. The usefulness of the approach is illustrated by different OWA scenarios that report the ecological connectivity index on a scale between 0 and 1. The OWA scenarios are intended to quantify the level of risk taking (i.e., optimistic, pessimistic, and neutral) and to facilitate a better understanding of patterns that emerge from decision alternatives involved in the decision-making process. The purpose of the research is to generate a final map representing the ecological connectivity index of each area in the region under analysis, to be used as a decision variable in spatial planning. In particular, by using the resulting index map as a means of analysis, it is possible to identify, for the sake of nature conservation, some critical areas needing mitigation measures. In addition, areas with high ecological connectivity values can be identified and monitoring procedures can therefore be planned. The study concludes highlighting that the applied methodology is an effective tool in providing decision support for spatial planning and sustainability assessments. © 2013 Elsevier Ltd. All rights reserved.","Analytic Network Process; Ecological suitability analysis; Geographic Information Systems; Indicators; Multicriteria-Spatial Decision Support; Ordered Weighted Average; Systems","Analytic network process; Decision supports; Ecological connectivities; Ecological suitability; Geo-spatial informations; Multi-criteria decision aiding; Ordered weighted average; Sustainability assessment; Artificial intelligence; Computer systems; Concrete pavements; Conservation; Decision making; Decision support systems; Decision theory; Geographic information systems; Indicators (instruments); Maps; Risk management; Statistical methods; Sustainable development; Urban planning; Ecology; analytical framework; bioindicator; decision making; decision support system; GIS; nature conservation; spatial planning; sustainable development; Italy; Piedmont [Italy]",Article,Scopus,2-s2.0-84880395554
"Xu Y., Zeng X., Han L., Yang J.","A supervised multi-spike learning algorithm based on gradient descent for spiking neural networks",2013,"Neural Networks",35,10.1016/j.neunet.2013.02.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875237051&doi=10.1016%2fj.neunet.2013.02.003&partnerID=40&md5=110c278fdbe5832381523c756ff82fb4","We use a supervised multi-spike learning algorithm for spiking neural networks (SNNs) with temporal encoding to simulate the learning mechanism of biological neurons in which the SNN output spike trains are encoded by firing times. We first analyze why existing gradient-descent-based learning methods for SNNs have difficulty in achieving multi-spike learning. We then propose a new multi-spike learning method for SNNs based on gradient descent that solves the problems of error function construction and interference among multiple output spikes during learning. The method could be widely applied to single spiking neurons to learn desired output spike trains and to multilayer SNNs to solve classification problems. By overcoming learning interference among multiple spikes, our method has high learning accuracy when there are a relatively large number of output spikes in need of learning. We also develop an output encoding strategy with respect to multiple spikes for classification problems. This effectively improves the classification accuracy of multi-spike learning compared to that of single-spike learning. © 2013 Elsevier Ltd.","Classification; Multi-spike learning; Single-spike learning; Spike sequence learning; Spiking neural networks","Classification accuracy; Encoding strategy; Learning interference; Learning mechanism; Multi-spike learning; Single-spike learning; Spike sequences; Spiking neural networks; Classification (of information); Encoding (symbols); Learning algorithms; Learning systems; Problem solving; Neural networks; article; artificial intelligence; artificial neural network; human; hyperpolarization; learning; learning algorithm; membrane potential; nerve cell network; postsynaptic potential; priority journal; refractory period; repolarization; spike; synaptic transmission; Action Potentials; Algorithms; Artificial Intelligence; Models, Neurological; Nerve Net; Neural Networks (Computer); Neurons",Article,Scopus,2-s2.0-84875237051
"Wu G., Liu J., Ma M., Qiu D.","A two-phase scheduling method with the consideration of task clustering for earth observing satellites",2013,"Computers and Operations Research",35,10.1016/j.cor.2013.02.009,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875952716&doi=10.1016%2fj.cor.2013.02.009&partnerID=40&md5=2eef2459641b05b9d815fe33b0d1cb3d","Satellite observation scheduling plays a significant role in improving the efficiency of satellite observation systems. Although extensive scheduling algorithms have been proposed for the satellite observation scheduling problem (SOSP), the task clustering strategy has not been taken into account up to now. This paper presents a novel two-phase based scheduling method with the consideration of task clustering for solving SOSP. This method comprises two phases: a task clustering phase and a task scheduling phase. In the task clustering phase, we construct a task clustering graph model and use an improved minimum clique partition algorithm to obtain cluster-tasks. In the task scheduling phase, based on overall tasks and obtained cluster-tasks, we construct an acyclic directed graph model and utilize a hybrid ant colony optimization coming with a mechanism of local search, called ACO-LS, to produce optimal or near optimal schedules. Extensive experimental simulations demonstrate the efficiency of the proposed scheduling method. © 2013 Elsevier Ltd.","Ant colony optimization; Clique partition; Local search; Satellite scheduling; Task clustering","Acyclic directed graph model; Clique partition; Earth observing satellite; Hybrid ant colony optimization; Local search; Satellite observation systems; Satellite scheduling; Task clustering; Ant colony optimization; Artificial intelligence; Graph theory; Multitasking; Satellites; Scheduling algorithms; Scheduling",Article,Scopus,2-s2.0-84875952716
"Moura D.C., Guevara López M.A.","An evaluation of image descriptors combined with clinical data for breast cancer diagnosis",2013,"International Journal of Computer Assisted Radiology and Surgery",35,10.1007/s11548-013-0838-2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880229711&doi=10.1007%2fs11548-013-0838-2&partnerID=40&md5=e3c2a3cdb880edec9c8e947d5de9589d","Purpose: Breast cancer computer-aided diagnosis (CADx) may utilize image descriptors, demographics, clinical observations, or a combination. CADx performance was compared for several image features, clinical descriptors (e.g. age and radiologist's observations), and combinations of both kinds of data. A novel descriptor invariant to rotation, histograms of gradient divergence (HGD), was developed to deal with round-shaped objects, such as masses. HGD was compared with conventional CADx features. Method: HGD and 11 conventional image descriptors were evaluated using cases from two publicly available mammography data sets, the digital database for screening mammography (DDSM) and the breast cancer digital repository (BCDR), with 1,762 and 362 instances, respectively. Three experiments were done for each data set according to the type of lesion (i.e., all lesions, masses, and calcifications), resulting in six scenarios. For each scenario, 100 training and test sets were generated via resampling without replacement and five machine learning classifiers were used to assess the diagnostic performance of the descriptors. Results: Clinical descriptors outperformed image descriptors in the DDSM sample (three out of six scenarios), and combining the two kind of descriptors was advantageous in five out of six scenarios. HGD was the best descriptor (or comparable to best) in 8 out of 12 scenarios, demonstrating promising capabilities to describe masses. Conclusions: The combination of clinical data and image descriptors was advantageous in most mammography CADx scenarios. A new descriptor based on the divergence of the gradient (HGD) was demonstrated to be a feasible predictor of breast masses' diagnosis. © 2013 CARS.","Breast cancer; Clinical data; Computer-aided diagnosis (CADx); Histograms of gradient divergence (HGD); Image descriptors; Machine learning classifiers","article; breast cancer; breast tumor; cancer diagnosis; classifier; computer assisted diagnosis; digital mammography; histogram; histograms of gradient divergence; human; image display; machine learning; priority journal; Algorithms; Artificial Intelligence; Breast Neoplasms; Diagnosis, Computer-Assisted; Female; Humans; Mammography; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84880229711
"Nikolić M., Teodorović D.","Transit network design by Bee Colony Optimization",2013,"Expert Systems with Applications",35,10.1016/j.eswa.2013.05.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878948813&doi=10.1016%2fj.eswa.2013.05.002&partnerID=40&md5=e67045dd5064bea8a93677558e4e472b","The transit network design problem is one of the most significant problems faced by transit operators and city authorities in the world. This transportation planning problem belongs to the class of difficult combinatorial optimization problem, whose optimal solution is difficult to discover. The paper develops a Swarm Intelligence (SI) based model for the transit network design problem. When designing the transit network, we try to maximize the number of satisfied passengers, to minimize the total number of transfers, and to minimize the total travel time of all served passengers. Our approach to the transit network design problem is based on the Bee Colony Optimization (BCO) metaheuristics. The BCO algorithm is a stochastic, random-search technique that belongs to the class of population-based algorithms. This technique uses a similarity among the way in which bees in nature look for food, and the way in which optimization algorithms search for an optimum of a combinatorial optimization problem. The numerical experiments are performed on known benchmark problems. We clearly show that our approach, based on the BCO algorithm, is competitive with other approaches in the literature, and it can generate high-quality solutions. © 2013 Elsevier Ltd. All rights reserved.","Bee Colony Optimization (BCO); Swarm Intelligence; Transit network design","Bee colony optimizations; Combinatorial optimization problems; High-quality solutions; Optimization algorithms; Population-based algorithm; Swarm Intelligence; Transit network design; Transportation planning; Algorithms; Artificial intelligence; Combinatorial optimization; Optimization; Mass transportation",Article,Scopus,2-s2.0-84878948813
"Xu R., Wang Q.","Large-scale extraction of accurate drug-disease treatment pairs from biomedical literature for drug repurposing",2013,"BMC Bioinformatics",35,10.1186/1471-2105-14-181,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878514642&doi=10.1186%2f1471-2105-14-181&partnerID=40&md5=4bcfc0857dac9aae7f6e9ce7e9b82e12","Background: A large-scale, highly accurate, machine-understandable drug-disease treatment relationship knowledge base is important for computational approaches to drug repurposing. The large body of published biomedical research articles and clinical case reports available on MEDLINE is a rich source of FDA-approved drug-disease indication as well as drug-repurposing knowledge that is crucial for applying FDA-approved drugs for new diseases. However, much of this information is buried in free text and not captured in any existing databases. The goal of this study is to extract a large number of accurate drug-disease treatment pairs from published literature.Results: In this study, we developed a simple but highly accurate pattern-learning approach to extract treatment-specific drug-disease pairs from 20 million biomedical abstracts available on MEDLINE. We extracted a total of 34,305 unique drug-disease treatment pairs, the majority of which are not included in existing structured databases. Our algorithm achieved a precision of 0.904 and a recall of 0.131 in extracting all pairs, and a precision of 0.904 and a recall of 0.842 in extracting frequent pairs. In addition, we have shown that the extracted pairs strongly correlate with both drug target genes and therapeutic classes, therefore may have high potential in drug discovery.Conclusions: We demonstrated that our simple pattern-learning relationship extraction algorithm is able to accurately extract many drug-disease pairs from the free text of biomedical literature that are not captured in structured databases. The large-scale, accurate, machine-understandable drug-disease treatment knowledge base that is resultant of our study, in combination with pairs from structured databases, will have high potential in computational drug repurposing tasks. © 2013 Xu and Wang; licensee BioMed Central Ltd.",,"Biomedical abstracts; Biomedical literature; Biomedical research; Computational approach; Relationship extraction; Relationship knowledge; Structured database; Therapeutic class; Algorithms; Database systems; Extraction; Knowledge based systems; Diseases; drug; algorithm; article; artificial intelligence; drug development; drug repositioning; drug therapy; economics; human; knowledge base; Medline; methodology; United States; drug therapy; procedures; trends; Algorithms; Artificial Intelligence; Drug Discovery; Drug Repositioning; Drug Therapy; Humans; Knowledge Bases; MEDLINE; Pharmaceutical Preparations; United States; Algorithms; Artificial Intelligence; Drug Discovery; Drug Repositioning; Drug Therapy; Humans; Knowledge Bases; MEDLINE; Pharmaceutical Preparations; United States",Article,Scopus,2-s2.0-84878514642
"Yuan Y., Pei J., Lai L.","Binding site detection and druggability prediction of protein targets for structure-based drug design",2013,"Current Pharmaceutical Design",35,10.2174/1381612811319120019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877814002&doi=10.2174%2f1381612811319120019&partnerID=40&md5=8733b7ef31c4634945fe83950e03c714","Assessing whether a protein structure is a good target or not before actually doing structure-based drug design on it is an important step to speed up the ligand discovery process. This is known as the ""druggability"" or ""ligandability"" assessment problem that has attracted increasing interest in recent years. The assessment typically includes the detection of ligand-binding sites on the protein surface and the prediction of their abilities to bind drug-like small molecules. A brief summary of the established methods of binding sites detection and druggability(ligandability) prediction, as well as a detailed description of the CAVITY approach developed in the authors' group was given. CAVITY showed good performance on ligand-binding site detection, and was successfully used to predict both the ligandabilities and druggabilities of the detected binding sites. © 2013 Bentham Science Publishers.","Binding site detection; CAVITY; Druggability; Ligandability; Structure-based drug design","ligand; protein; algorithm; article; binding site; drug design; ligand binding; prediction; priority journal; protein analysis; protein structure; structure activity relation; Algorithms; Animals; Artificial Intelligence; Binding Sites; Computational Biology; Drug Design; Drugs, Investigational; Humans; Kinetics; Ligands; Molecular Conformation; Proteins; Structure-Activity Relationship; Surface Properties",Article,Scopus,2-s2.0-84877814002
"Koay C.A., Srinivasan D.","Particle swarm optimization-based approach for generator maintenance scheduling",2013,"2003 IEEE Swarm Intelligence Symposium, SIS 2003 - Proceedings",35,10.1109/SIS.2003.1202263,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942114734&doi=10.1109%2fSIS.2003.1202263&partnerID=40&md5=f85586729843c961194808d1f8a2e685","This paper introduces a particle swarm optimization-based method for solving a multi-objective generator maintenance scheduling problem with many constraints. It is shown that the particle swarm optimization-based approach is effective in obtaining feasible schedules in a reasonable time. Actual data from a practical power system was used in this study and results were compared against those from other evolutionary methods on the same set of data. This paper also introduces a novel concept for the spawning and selection mechanism in a hybrid particle swarm algorithm. The results suggest that this hybrid model converges to a better solution faster than the standard PSO algorithm. It is envisaged that this hybrid approach can be easily implemented for similar optimization and scheduling problems to obtain better convergence. © 2003 IEEE.","Artificial intelligence; Artificial neural networks; Constraint optimization; Evolutionary computation; Genetic algorithms; Hybrid power systems; Particle swarm optimization; Power system modeling; Processor scheduling; Testing","Algorithms; Artificial intelligence; Constrained optimization; Evolutionary algorithms; Genetic algorithms; Multiobjective optimization; Neural networks; Scheduling; Testing; Constraint optimizations; Evolutionary method; Generator maintenance scheduling; Hybrid power systems; Power system model; Processor scheduling; Scheduling problem; Selection mechanism; Particle swarm optimization (PSO)",Conference Paper,Scopus,2-s2.0-84942114734
"Chen Y., Zheng W.X.","Stability analysis of time-delay neural networks subject to stochastic perturbations",2013,"IEEE Transactions on Cybernetics",34,10.1109/TCYB.2013.2240451,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890055057&doi=10.1109%2fTCYB.2013.2240451&partnerID=40&md5=df2ec1f0cde3f85f1b5a18ca2dd0c296","This paper is concerned with the problem of mean-square exponential stability of uncertain neural networks with time-varying delay and stochastic perturbation. Both linear and nonlinear stochastic perturbations are considered. The main features of this paper are twofold: 1) Based on generalized Finsler lemma, some improved delay-dependent stability criteria are established, which are more efficient than the existing ones in terms of less conservatism and lower computational complexity; and 2) when the nonlinear stochastic perturbation acting on the system satisfies a class of Lipschitz linear growth conditions, the restrictive condition P < δ I (or the similar ones) in the existing results can be relaxed under some assumptions. The usefulness of the proposed method is demonstrated by illustrative examples. © 2013 IEEE.","Delay; Generalized Finsler lemma (GFL); Neural networks (NNs); Nonlinear stochastic perturbation; Stability.","Delay; Delay dependent stability criterion; Generalized Finsler lemma (GFL); Mean-square exponential stabilities; Neural networks (NNS); Stochastic perturbations; Time delay neural networks; Uncertain neural networks; Convergence of numerical methods; Neural networks; Stability criteria; Stochastic systems; algorithm; article; artificial intelligence; artificial neural network; automated pattern recognition; computer simulation; methodology; statistical model; statistics; time; Algorithms; Artificial Intelligence; Computer Simulation; Models, Statistical; Neural Networks (Computer); Pattern Recognition, Automated; Stochastic Processes; Time Factors",Article,Scopus,2-s2.0-84890055057
"Wang H., Chen B., Li W.-J.","Collaborative topic regression with social regularization for tag recommendation",2013,"IJCAI International Joint Conference on Artificial Intelligence",34,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062199&partnerID=40&md5=22f8349894f7c5b8d70545cf2f6ba5d4","Recently, tag recommendation (TR) has become a very hot research topic in data mining and related areas. However, neither co-occurrence based methods which only use the item-tag matrix nor content based methods which only use the item content information can achieve satisfactory performance in real TR applications. Hence, how to effectively combine the item-tag matrix, item content information, and other auxiliary information into the same recommendation framework is the key challenge for TR. In this paper, we first adapt the collaborative topic regression (CTR) model, which has been successfully applied for article recommendation, to combine both item-tag matrix and item content information for TR. Furthermore, by extending CTR we propose a novel hierarchical Bayesian model, called CTR with social regularization (CTR-SR), to seamlessly integrate the item-tag matrix, item content information, and social networks between items into the same principled model. Experiments on real data demonstrate the effectiveness of our proposed models.",,"Auxiliary information; Co-occurrence; Content information; Content-based methods; Hierarchical Bayesian modeling; Hot research topics; Tag recommendations; Bayesian networks; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84896062199
"Shan Q., Adams R., Curless B., Furukawa Y., Seitz S.M.","The visual turing test for scene reconstruction",2013,"Proceedings - 2013 International Conference on 3D Vision, 3DV 2013",34,10.1109/3DV.2013.12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886053441&doi=10.1109%2f3DV.2013.12&partnerID=40&md5=18ac1c6bdfb9e0011fd440f9bdf6c09a","We present the first large scale system for capturing and rendering relight able scene reconstructions from massive unstructured photo collections taken under different illumination conditions and viewpoints. We combine photos taken from many sources, Flickr-Based ground-level imagery, oblique aerial views, and street view, to recover models that are significantly more complete and detailed than previously demonstrated. We demonstrate the ability to match both the viewpoint and illumination of arbitrary input photos, enabling a Visual Turing Test in which photo and rendering are viewed side-by-side and the observer has to guess which is which. While we cannot yet fool human perception, the gap is closing. © 2013 IEEE.",,"Arbitrary inputs; Ground-level; Human perception; Illumination conditions; Photo collections; Scene reconstruction; Side by sides; Turing tests; Three dimensional; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84886053441
"Tao D., Jin L., Yang Z., Li X.","Rank preserving sparse learning for kinect based scene classification",2013,"IEEE Transactions on Cybernetics",34,10.1109/TCYB.2013.2264285,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890382943&doi=10.1109%2fTCYB.2013.2264285&partnerID=40&md5=38e6e489fc27e56d100b1cd1c156d7d6","With the rapid development of the RGB-D sensors and the promptly growing population of the low-cost Microsoft Kinect sensor, scene classification, which is a hard, yet important, problem in computer vision, has gained a resurgence of interest recently. That is because the depth of information provided by the Kinect sensor opens an effective and innovative way for scene classification. In this paper, we propose a new scheme for scene classification, which applies locality-constrained linear coding (LLC) to local SIFT features for representing the RGB-D samples and classifies scenes through the cooperation between a new rank preserving sparse learning (RPSL) based dimension reduction and a simple classification method. RPSL considers four aspects: 1) it preserves the rank order information of the within-class samples in a local patch; 2) it maximizes the margin between the between-class samples on the local patch; 3) the L1- norm penalty is introduced to obtain the parsimony property; and 4) it models the classification error minimization by utilizing the least-squares error minimization. Experiments are conducted on the NYU Depth V1 dataset and demonstrate the robustness and effectiveness of RPSL for scene classification. © 2013 IEEE.","Dimension reduction; Kinect sensor; Rank preserving and sparse learning; RGB-D sensor; Scene classification","Dimension reduction; Kinect sensors; Rank preserving and sparse learning; Rgb-d sensors; Scene classification; Sensors; Classification (of information); algorithm; artificial intelligence; automated pattern recognition; computer; computer simulation; computer system; devices; image enhancement; procedures; recreation; three dimensional imaging; transducer; whole body imaging; article; automated pattern recognition; equipment; methodology; three dimensional imaging; whole body imaging; Algorithms; Artificial Intelligence; Computer Peripherals; Computer Simulation; Computer Systems; Image Enhancement; Imaging, Three-Dimensional; Pattern Recognition, Automated; Transducers; Video Games; Whole Body Imaging; Algorithms; Artificial Intelligence; Computer Peripherals; Computer Simulation; Computer Systems; Image Enhancement; Imaging, Three-Dimensional; Pattern Recognition, Automated; Transducers; Video Games; Whole Body Imaging",Article,Scopus,2-s2.0-84890382943
"Gaidon A., Harchaoui Z., Schmid C.","Temporal localization of actions with actoms",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",34,10.1109/TPAMI.2013.65,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884557275&doi=10.1109%2fTPAMI.2013.65&partnerID=40&md5=cf7f39a18faf060382ffa5c5a7c03227","We address the problem of localizing actions, such as opening a door, in hours of challenging video data. We propose a model based on a sequence of atomic action units, termed 'actoms,' that are semantically meaningful and characteristic for the action. Our actom sequence model (ASM) represents an action as a sequence of histograms of actom-anchored visual features, which can be seen as a temporally structured extension of the bag-of-features. Training requires the annotation of actoms for action examples. At test time, actoms are localized automatically based on a nonparametric model of the distribution of actoms, which also acts as a prior on an action's temporal structure. We present experimental results on two recent benchmarks for action localization 'Coffee and Cigarettes' and the 'DLSBP' dataset. We also adapt our approach to a classification-by-localization set-up and demonstrate its applicability on the challenging 'Hollywood 2' dataset. We show that our ASM method outperforms the current state of the art in temporal action localization, as well as baselines that localize actions with a sliding window method. © 1979-2012 IEEE.","Action recognition; actoms; temporal localization; video analysis","Action recognition; actoms; Non-parametric model; Sequence modeling; Sliding window methods; Temporal localization; Temporal structures; Video analysis; Motion estimation; Classification (of information); actimetry; algorithm; artificial intelligence; automated pattern recognition; computer assisted diagnosis; human; image subtraction; procedures; videorecording; whole body imaging; actimetry; article; automated pattern recognition; computer assisted diagnosis; methodology; videorecording; whole body imaging; Actigraphy; Algorithms; Artificial Intelligence; Humans; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Subtraction Technique; Video Recording; Whole Body Imaging; Actigraphy; Algorithms; Artificial Intelligence; Humans; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Subtraction Technique; Video Recording; Whole Body Imaging",Article,Scopus,2-s2.0-84884557275
"Dekel Basha T., Moses Y., Avidan S.","Stereo seam carving a geometrically consistent approach",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",34,10.1109/TPAMI.2013.46,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883154204&doi=10.1109%2fTPAMI.2013.46&partnerID=40&md5=8e772a8e6b5d176d302e69e018ca5404","Image retargeting algorithms attempt to adapt the image content to the screen without distorting the important objects in the scene. Existing methods address retargeting of a single image. In this paper, we propose a novel method for retargeting a pair of stereo images. Naively retargeting each image independently will distort the geometric structure and hence will impair the perception of the 3D structure of the scene. We show how to extend a single image seam carving to work on a pair of images. Our method minimizes the visual distortion in each of the images as well as the depth distortion. A key property of the proposed method is that it takes into account the visibility relations between pixels in the image pair (occluded and occluding pixels). As a result, our method guarantees, as we formally prove, that the retargeted pair is geometrically consistent with a feasible 3D scene, similar to the original one. Hence, the retargeted stereo pair can be viewed on a stereoscopic display or further processed by any computer vision algorithm. We demonstrate our method on a number of challenging indoor and outdoor stereo images. © 1979-2012 IEEE.","geometric consistency; retargeting; Stereo","Computer vision algorithms; geometric consistency; Geometric structure; Image retargeting; retargeting; Stereo; Stereoscopic display; Visual distortion; Algorithms; Computer vision; Pixels; Three dimensional; Face recognition; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; image enhancement; mathematical computing; methodology; reproducibility; sensitivity and specificity; signal processing; three dimensional imaging; automated pattern recognition; computer assisted diagnosis; image enhancement; procedures; three dimensional imaging; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Numerical Analysis, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Signal Processing, Computer-Assisted; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Numerical Analysis, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Signal Processing, Computer-Assisted",Article,Scopus,2-s2.0-84883154204
"Singh K.P., Gupta S., Rai P.","Predicting acute aquatic toxicity of structurally diverse chemicals in fish using artificial intelligence approaches",2013,"Ecotoxicology and Environmental Safety",34,10.1016/j.ecoenv.2013.05.017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880324511&doi=10.1016%2fj.ecoenv.2013.05.017&partnerID=40&md5=cd410fbb733ed8c6d66498a69e9e4275","The research aims to develop global modeling tools capable of categorizing structurally diverse chemicals in various toxicity classes according to the EEC and European Community directives, and to predict their acute toxicity in fathead minnow using set of selected molecular descriptors. Accordingly, artificial intelligence approach based classification and regression models, such as probabilistic neural networks (PNN), generalized regression neural networks (GRNN), multilayer perceptron neural network (MLPN), radial basis function neural network (RBFN), support vector machines (SVM), gene expression programming (GEP), and decision tree (DT) were constructed using the experimental toxicity data. Diversity and non-linearity in the chemicals' data were tested using the Tanimoto similarity index and Brock-Dechert-Scheinkman statistics. Predictive and generalization abilities of various models constructed here were compared using several statistical parameters. PNN and GRNN models performed relatively better than MLPN, RBFN, SVM, GEP, and DT. Both in two and four category classifications, PNN yielded a considerably high accuracy of classification in training (95.85 percent and 90.07 percent) and validation data (91.30 percent and 86.96 percent), respectively. GRNN rendered a high correlation between the measured and model predicted -log LC50 values both for the training (0.929) and validation (0.910) data and low prediction errors (RMSE) of 0.52 and 0.49 for two sets. Efficiency of the selected PNN and GRNN models in predicting acute toxicity of new chemicals was adequately validated using external datasets of different fish species (fathead minnow, bluegill, trout, and guppy). The PNN and GRNN models showed good predictive and generalization abilities and can be used as tools for predicting toxicities of structurally diverse chemical compounds. © 2013 Elsevier Inc.","Acute aquatic toxicity; Artificial intelligence; Fish, diversity; Generalized regression neural network; Nonlinearity; Probabilistic neural network","artificial intelligence; artificial neural network; chemical pollutant; fish; nonlinearity; numerical model; toxicity; acute toxicity; article; artificial intelligence; artificial neural network; fish; gene expression; generalized regression neural network; LC 50; multilayer perceptron neural network; nonhuman; Pimephales promelas; prediction; probabilistic neural network; radial basis function neura lnetwork; support vector machine; validation process; Acute aquatic toxicity; Artificial intelligence; Fish, diversity; Generalized regression neural network; Nonlinearity; Probabilistic neural network; Animals; Artificial Intelligence; Cyprinidae; Fishes; Lethal Dose 50; Models, Theoretical; Neural Networks (Computer); Organic Chemicals; Probability; Regression Analysis; Lepomis macrochirus; Pimephales promelas; Poecilia reticulata; Salmonidae",Article,Scopus,2-s2.0-84880324511
"Tang B., Wu Y., Jiang M., Chen Y., Denny J.C., Xu H.","A hybrid system for temporal information extraction from clinical text",2013,"Journal of the American Medical Informatics Association",34,10.1136/amiajnl-2013-001635,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881139675&doi=10.1136%2famiajnl-2013-001635&partnerID=40&md5=58b53f208b166021845f67789df15d08","Objective: To develop a comprehensive temporal information extraction system that can identify events, temporal expressions, and their temporal relations in clinical text. This project was part of the 2012 i2b2 clinical natural language processing (NLP) challenge on temporal information extraction. Materials and methods: The 2012 i2b2 NLP challenge organizers manually annotated 310 clinic notes according to a defined annotation guideline: a training set of 190 notes and a test set of 120 notes. All participating systems were developed on the training set and evaluated on the test set. Our system consists of three modules: event extraction, temporal expression extraction, and temporal relation (also called Temporal Link, or 'TLink') extraction. The TLink extraction module contains three individual classifiers for TLinks: (1) between events and section times, (2) within a sentence, and (3) across different sentences. The performance of our system was evaluated using scripts provided by the i2b2 organizers. Primary measures were micro-averaged Precision, Recall, and F-measure. Results: Our system was among the top ranked. It achieved F-measures of 0.8659 for temporal expression extraction (ranked fourth), 0.6278 for end-to-end TLink track (ranked first), and 0.6932 for TLink-only track (ranked first) in the challenge. We subsequently investigated different strategies for TLink extraction, and were able to marginally improve performance with an F-measure of 0.6943 for TLink-only track.",,"article; extraction; human; information; natural language processing; task performance; temporal cortex; training; Clinic event extraction; Machine learning; Natural language processing; Temporal expression extraction; Temporal information extraction; Temporal relation extraction; Artificial Intelligence; Electronic Health Records; Humans; Natural Language Processing; Patient Discharge Summaries; Time; Translational Medical Research",Article,Scopus,2-s2.0-84881139675
"Pluhacek M., Senkerik R., Zelinka I., Davendra D.","Chaos PSO algorithm driven alternately by two different chaotic maps-An initial study",2013,"2013 IEEE Congress on Evolutionary Computation, CEC 2013",34,10.1109/CEC.2013.6557862,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881587992&doi=10.1109%2fCEC.2013.6557862&partnerID=40&md5=03b00c57ba87424c69570bbec33f0e60","In this paper, a new approach for chaos driven PSO algorithm is proposed. Two different chaotic maps are alternately used as pseudorandom number generators and switched over during the run of chaos driven PSO algorithm. The motivation for this research came from the previous successful experiments with PSO algorithm driven by different chaotic maps. Promising results of this innovative approach are presented in the results section and briefly analyzed. © 2013 IEEE.","chaos; Evolutionary algorithm; optimization; Particle swarm optimization; PSO; Swarm intelligence","Chaos-PSO; Chaotic map; Innovative approaches; New approaches; Pseudo random number generators; PSO; PSO algorithms; Swarm Intelligence; Algorithms; Artificial intelligence; Chaos theory; Chaotic systems; Evolutionary algorithms; Lyapunov methods; Optimization; Random number generation; Particle swarm optimization (PSO)",Conference Paper,Scopus,2-s2.0-84881587992
"Nunez-Iglesias J., Kennedy R., Parag T., Shi J., Chklovskii D.B.","Machine Learning of Hierarchical Clustering to Segment 2D and 3D Images",2013,"PLoS ONE",34,10.1371/journal.pone.0071715,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882631243&doi=10.1371%2fjournal.pone.0071715&partnerID=40&md5=b468790ad088fb5c988a4659bfa81935","We aim to improve segmentation through the use of machine learning tools during region agglomeration. We propose an active learning approach for performing hierarchical agglomerative segmentation from superpixels. Our method combines multiple features at all scales of the agglomerative process, works for data with an arbitrary number of dimensions, and scales to very large datasets. We advocate the use of variation of information to measure segmentation accuracy, particularly in 3D electron microscopy (EM) images of neural tissue, and using this metric demonstrate an improvement over competing algorithms in EM and natural images. © 2013 Nunez-Iglesias et al.",,"accuracy; algorithm; article; cluster analysis; electron microscopy; image analysis; image processing; image quality; image reconstruction; machine learning; mathematical model; nervous tissue; three dimensional imaging; validation process; Algorithms; Artificial Intelligence; Cluster Analysis; Imaging, Three-Dimensional; Microscopy, Electron; Probability",Article,Scopus,2-s2.0-84882631243
"Basavanhally A., Ganesan S., Feldman M., Shih N., Mies C., Tomaszewski J., Madabhushi A.","Multi-field-of-view framework for distinguishing tumor grade in ER+ breast cancer from entire histopathology slides",2013,"IEEE Transactions on Biomedical Engineering",34,10.1109/TBME.2013.2245129,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880902295&doi=10.1109%2fTBME.2013.2245129&partnerID=40&md5=d97db2aa222e9b335fa887bfdc37aad3","Modified Bloom-Richardson (mBR) grading is known to have prognostic value in breast cancer (BCa), yet its use in clinical practice has been limited by intra- and interobserver variability. The development of a computerized system to distinguish mBR grade from entire estrogen receptor-positive (ER+) BCa histopathology slides will help clinicians identify grading discrepancies and improve overall confidence in the diagnostic result. In this paper, we isolate salient image features characterizing tumor morphology and texture to differentiate entire hematoxylin and eosin (H and E) stained histopathology slides based on mBR grade. The features are used in conjunction with a novel multi-field-of-view (multi-FOV) classifier - a whole-slide classifier that extracts features from a multitude of FOVs of varying sizes - to identify important image features at different FOV sizes. Image features utilized include those related to the spatial arrangement of cancer nuclei (i. e. , nuclear architecture) and the textural patterns within nuclei (i. e. , nuclear texture). Using slides from 126 ER+ patients (46 low, 60 intermediate, and 20 high mBR grade), our grading system was able to distinguish low versus high, low versus intermediate, and intermediate versus high grade patients with area under curve values of 0. 93, 0. 72, and 0. 74, respectively. Our results suggest that the multi-FOV classifier is able to 1) successfully discriminate low, medium, and high mBR grade and 2) identify specific image features at different FOV sizes that are important for distinguishing mBR grade in H and E stained ER+ BCa histology slides. © 1964-2012 IEEE.","Breast cancer (BCa); digital pathology; image analysis; modified Bloom-Richardson (mBR) grade; multi-field-of-view (multi-FOV); nuclear architecture; nuclear texture","Breast Cancer; Digital pathologies; multi-field-of-view (multi-FOV); Nuclear architecture; Nuclear texture; Blooms (metal); Image analysis; Textures; Tumors; Diseases; article; cancer grading; cancer patient; classifier; clinical feature; comparative study; controlled study; estrogen receptor positive breast cancer; histopathology; human; human tissue; imaging and display; morphology; multifield of view framework; Algorithms; Artificial Intelligence; Breast Neoplasms; Female; Humans; Image Interpretation, Computer-Assisted; Microscopy; Neoplasm Grading; Pattern Recognition, Automated; Receptors, Estrogen; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84880902295
"Yudelson M.V., Koedinger K.R., Gordon G.J.","Individualized bayesian knowledge tracing models",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",34,10.1007/978-3-642-39112-5-18,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879998340&doi=10.1007%2f978-3-642-39112-5-18&partnerID=40&md5=a3562e89c0140d6ff36227215665a690","Bayesian Knowledge Tracing (BKT)[1] is a user modeling method extensively used in the area of Intelligent Tutoring Systems. In the standard BKT implementation, there are only skill-specific parameters. However, a large body of research strongly suggests that student-specific variability in the data, when accounted for, could enhance model accuracy [5,6,8]. In this work, we revisit the problem of introducing student-specific parameters into BKT on a larger scale. We show that student-specific parameters lead to a tangible improvement when predicting the data of unseen students, and that parameterizing students' speed of learning is more beneficial than parameterizing a priori knowledge. © 2013 Springer-Verlag Berlin Heidelberg.","Bayesian knowledge tracing; Model fitting; Model selection; Student-specific model parameters","Bayesian knowledge tracings; Intelligent tutoring system; Model accuracy; Model fitting; Model parameters; Model Selection; Parameterizing; Priori knowledge; Artificial intelligence; Computer aided instruction; Students",Conference Paper,Scopus,2-s2.0-84879998340
"Chen X., Qiu J.-D., Shi S.-P., Suo S.-B., Huang S.-Y., Liang R.-P.","Incorporating key position and amino acid residue features to identify general and species-specific Ubiquitin conjugation sites",2013,"Bioinformatics",34,10.1093/bioinformatics/btt196,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879895159&doi=10.1093%2fbioinformatics%2fbtt196&partnerID=40&md5=e43ace2479a8bbe217a7ad4c5c81c927","Motivation: Systematic dissection of the ubiquitylation proteome is emerging as an appealing but challenging research topic because of the significant roles ubiquitylation play not only in protein degradation but also in many other cellular functions. High-throughput experimental studies using mass spectrometry have identified many ubiquitylation sites, primarily from eukaryotes. However, the vast majority of ubiquitylation sites remain undiscovered, even in well-studied systems. Because mass spectrometry-based experimental approaches for identifying ubiquitylation events are costly, time-consuming and biased toward abundant proteins and proteotypic peptides, in silico prediction of ubiquitylation sites is a potentially useful alternative strategy for whole proteome annotation. Because of various limitations, current ubiquitylation site prediction tools were not well designed to comprehensively assess proteomes.Results: We present a novel tool known as UbiProber, specifically designed for large-scale predictions of both general and species-specific ubiquitylation sites. We collected proteomics data for ubiquitylation from multiple species from several reliable sources and used them to train prediction models by a comprehensive machine-learning approach that integrates the information from key positions and key amino acid residues. Cross-validation tests reveal that UbiProber achieves some improvement over existing tools in predicting species-specific ubiquitylation sites. Moreover, independent tests show that UbiProber improves the areas under receiver operating characteristic curves by ∼15% by using the Combined model. © The Author 2013.",,"amino acid; proteome; ubiquitin; ubiquitinated protein; amino acid; ubiquitinated protein; animal; artificial intelligence; chemistry; computer program; human; metabolism; mouse; procedures; proteomics; sequence analysis; species difference; ubiquitination; article; chemistry; methodology; sequence analysis; Amino Acids; Animals; Artificial Intelligence; Humans; Mice; Proteome; Proteomics; Sequence Analysis, Protein; Software; Species Specificity; Ubiquitin; Ubiquitinated Proteins; Ubiquitination; Amino Acids; Animals; Artificial Intelligence; Humans; Mice; Proteome; Proteomics; Sequence Analysis, Protein; Software; Species Specificity; Ubiquitin; Ubiquitinated Proteins; Ubiquitination",Conference Paper,Scopus,2-s2.0-84879895159
"Lu W.-L., Ting J.-A., Little J.J., Murphy K.P.","Learning to track and identify players from broadcast sports videos",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",34,10.1109/TPAMI.2012.242,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878136463&doi=10.1109%2fTPAMI.2012.242&partnerID=40&md5=a494375aa4c8de017a5e2d927099d5ba","Tracking and identifying players in sports videos filmed with a single pan-tilt-zoom camera has many applications, but it is also a challenging problem. This paper introduces a system that tackles this difficult task. The system possesses the ability to detect and track multiple players, estimates the homography between video frames and the court, and identifies the players. The identification system combines three weak visual cues, and exploits both temporal and mutual exclusion constraints in a Conditional Random Field (CRF). In addition, we propose a novel Linear Programming (LP) Relaxation algorithm for predicting the best player identification in a video clip. In order to reduce the number of labeled training data required to learn the identification system, we make use of weakly supervised learning with the assistance of play-by-play texts. Experiments show promising results in tracking, homography estimation, and identification. Moreover, weakly supervised learning with play-by-play texts greatly reduces the number of labeled training examples required. The identification system can achieve similar accuracies by using merely 200 labels in weakly supervised learning, while a strongly supervised approach needs a least 20,000 labels. © 1979-2012 IEEE.","identification; Sports video analysis; tracking; weakly supervised learning","Conditional random field; Homography estimations; Labeled training data; Linear programming relaxation; Mutual exclusions; Pan-tilt-zoom camera; Sports video analysis; Weakly supervised learning; Algorithms; Identification (control systems); SportS; Surface discharges; Video cameras; Supervised learning; artificial intelligence; athlete; automated pattern recognition; classification; human; image processing; procedures; sport; videorecording; article; athlete; automated pattern recognition; classification; image processing; methodology; videorecording; Artificial Intelligence; Athletes; Humans; Image Processing, Computer-Assisted; Pattern Recognition, Automated; Sports; Video Recording; Artificial Intelligence; Athletes; Humans; Image Processing, Computer-Assisted; Pattern Recognition, Automated; Sports; Video Recording",Article,Scopus,2-s2.0-84878136463
"Sussillo D., Barak O.","Opening the black box: Low-dimensional dynamics in high-dimensional recurrent neural networks",2013,"Neural Computation",34,10.1162/NECO_a_00409,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877827546&doi=10.1162%2fNECO_a_00409&partnerID=40&md5=07cf1915ffa483660777b94fb624e7b5","Recurrent neural networks (RNNs) are useful tools for learning nonlinear relationships between time-varying inputs and outputswith complex temporal dependencies. Recently developed algorithms have been successful at training RNNs to perform a wide variety of tasks, but the resulting networks have been treated as black boxes: their mechanism of operation remains unknown. Here we explore the hypothesis that fixed points, both stable and unstable, and the linearized dynamics around them, can reveal crucial aspects of how RNNs implement their computations. Further, we explore the utility of linearization in areas of phase space that are not true fixed points but merely points of very slow movement. We present a simple optimization technique that is applied to trained RNNs to find the fixed and slow points of their dynamics. Linearization around these slow regions can be used to explore, or reverse-engineer, the behavior of the RNN. We describe the technique, illustrate it using simple examples, and finally showcase it on three highdimensional RNN examples: a 3-bit flip-flop device, an input-dependent sine wave generator, and a two-point moving average. In all cases, the mechanisms of trained networks could be inferred from the sets of fixed and slow points and the linearized dynamics around them. © 2013 Massachusetts Institute of Technology.",,"article; artificial intelligence; artificial neural network; Artificial Intelligence; Neural Networks (Computer)",Article,Scopus,2-s2.0-84877827546
"Didegah F., Thelwall M.","Determinants of research citation impact in nanoscience and nanotechnology",2013,"Journal of the American Society for Information Science and Technology",34,10.1002/asi.22806,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876295889&doi=10.1002%2fasi.22806&partnerID=40&md5=d45d6c43a5e6155dd39f8f533dce755d","This study investigates a range of metrics available when a nanoscience and nanotechnology article is published to see which metrics correlate more with the number of citations to the article. It also introduces the degree of internationality of journals and references as new metrics for this purpose. The journal impact factor; the impact of references; the internationality of authors, journals, and references; and the number of authors, institutions, and references were all calculated for papers published in nanoscience and nanotechnology journals in the Web of Science from 2007 to 2009. Using a zero-inflated negative binomial regression model on the data set, the impact factor of the publishing journal and the citation impact of the cited references were found to be the most effective determinants of citation counts in all four time periods. In the entire 2007 to 2009 period, apart from journal internationality and author numbers and internationality, all other predictor variables had significant effects on citation counts. © 2013 ASIS&T.","bibliometrics; citation analysis","Bibliometrics; Citation analysis; Citation impact; Impact factor; Nanoscience and nanotechnologies; Negative binomial regression model; Predictor variables; Web of Science; Artificial intelligence; Software engineering; Nanoscience",Article,Scopus,2-s2.0-84876295889
"Veselinović A.M., Milosavljević J.B., Toropov A.A., Nikolić G.M.","SMILES-based QSAR model for arylpiperazines as high-affinity 5-HT1A receptor ligands using CORAL",2013,"European Journal of Pharmaceutical Sciences",34,10.1016/j.ejps.2012.12.021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872580195&doi=10.1016%2fj.ejps.2012.12.021&partnerID=40&md5=d1919fbf115eb476d1f869b63879f9de","A predictive quantitative structure - activity relationships model of arylpiperazines as high-affinity 5- HT1A receptor ligands was developed using CORAL software (http://www.insilico.eu/CORAL). Simplified molecular input-line entry system (SMILES) was used as representation of the molecular structure of the arylpiperazines. The balance of correlations was used in the Monte Carlo optimization aimed to build up optimal descriptors for one-variable models. The robustness of this model has been tested in four random splits into the sub-training, calibration, and test set. The obtained results reveal good predictive potential of the applied approach: correlation coefficients (r 2) for the test sets of the four random splits are 0.9459, 0.9249, 0.9473 and 0.9362. © 2012 Elsevier B.V. All rights reserved.","Arylpiperazines; CORAL software; QSAR; SMILES","piperazine derivative; serotonin 1A antagonist; antidepressant agent; HTR1A protein, human; ligand; piperazine derivative; serotonin 1 antagonist; serotonin 1A receptor; article; binding affinity; calibration; chemical structure; computer program; coral software; information system; ligand binding; Monte Carlo method; physical chemistry; predictor variable; priority journal; quantitative structure activity relation; simplified molecular input line entry system; artificial intelligence; chemical structure; chemistry; computer program; conformation; human; kinetics; metabolism; quantitative structure activity relation; validation study; Antidepressive Agents; Artificial Intelligence; Calibration; Humans; Kinetics; Ligands; Models, Molecular; Molecular Conformation; Monte Carlo Method; Piperazines; Quantitative Structure-Activity Relationship; Receptor, Serotonin, 5-HT1A; Serotonin 5-HT1 Receptor Antagonists; Software",Article,Scopus,2-s2.0-84872580195
"Korus P., Dziech A.","Efficient method for content reconstruction with self-embedding",2013,"IEEE Transactions on Image Processing",34,10.1109/TIP.2012.2227769,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873344889&doi=10.1109%2fTIP.2012.2227769&partnerID=40&md5=142dffa8210eb980162cfe5eec0679f2","This paper presents a new model of the content reconstruction problem in self-embedding systems, based on an erasure communication channel. We explain why such a model is a good fit for this problem, and how it can be practically implemented with the use of digital fountain codes. The proposed method is based on an alternative approach to spreading the reference information over the whole image, which has recently been shown to be of critical importance in the application at hand. Our paper presents a theoretical analysis of the inherent restoration trade-offs. We analytically derive formulas for the reconstruction success bounds, and validate them experimentally with Monte Carlo simulations and a reference image authentication system. We perform an exhaustive reconstruction quality assessment, where the presented reference scheme is compared to five state-of-the-art alternatives in a common evaluation scenario. Our paper leads to important insights on how self-embedding schemes should be constructed to achieve optimal performance. The reference authentication system designed according to the presented principles allows for high-quality reconstruction, regardless of the amount of the tampered content. The average reconstruction quality, measured on 10000 natural images is 37 dB, and is achievable even when 50% of the image area becomes tampered. © 2012 IEEE.","Image and video processing for watermarking and security","Alternative approach; Authentication systems; Digital fountain codes; High quality; Image and video processing; Monte Carlo Simulation; Natural images; Optimal performance; Reconstruction problems; Reconstruction quality; Reference image; Authentication; Monte Carlo methods; Video signal processing; Image watermarking; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; image enhancement; information retrieval; methodology; reproducibility; sensitivity and specificity; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Information Storage and Retrieval; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84873344889
"Liu J., Tai X.-C., Huang H., Huan Z.","A weighted dictionary learning model for denoising images corrupted by mixed noise",2013,"IEEE Transactions on Image Processing",34,10.1109/TIP.2012.2227766,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873374116&doi=10.1109%2fTIP.2012.2227766&partnerID=40&md5=7c3b688ec15008ab919f50bb0e94f65f","This paper proposes a general weighted $l^{2}-l0 norms energy minimization model to remove mixed noise such as Gaussian-Gaussian mixture, impulse noise, and Gaussian-impulse noise from the images. The approach is built upon maximum likelihood estimation framework and sparse representations over a trained dictionary. Rather than optimizing the likelihood functional derived from a mixture distribution, we present a new weighting data fidelity function, which has the same minimizer as the original likelihood functional but is much easier to optimize. The weighting function in the model can be determined by the algorithm itself, and it plays a role of noise detection in terms of the different estimated noise parameters. By incorporating the sparse regularization of small image patches, the proposed method can efficiently remove a variety of mixed or single noise while preserving the image textures well. In addition, a modified K-SVD algorithm is designed to address the weighted rank-one approximation. The experimental results demonstrate its better performance compared with some existing methods. © 2012 IEEE.","Image denoising; K-SVD; mixed noise; sparse representation; weighted norms","Data fidelity; De-noising; Dictionary learning; Energy minimization; Image patches; K-SVD; Mixed noise; Mixture distributions; Noise detection; Noise parameters; Sparse representation; Weighted norm; Weighting functions; Approximation algorithms; Gaussian distribution; Impulse noise; Maximum likelihood estimation; Optimization; Image denoising; algorithm; article; artifact; artificial intelligence; automated pattern recognition; book; computer assisted diagnosis; image enhancement; methodology; reproducibility; sensitivity and specificity; signal noise ratio; statistical analysis; Algorithms; Artifacts; Artificial Intelligence; Data Interpretation, Statistical; Dictionaries as Topic; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Signal-To-Noise Ratio",Article,Scopus,2-s2.0-84873374116
"Wang L., Fu Q.-L., Lee C.-G., Zeng Y.-R.","Model and algorithm of fuzzy joint replenishment problem under credibility measure on fuzzy goal",2013,"Knowledge-Based Systems",34,10.1016/j.knosys.2012.10.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871922439&doi=10.1016%2fj.knosys.2012.10.002&partnerID=40&md5=c1b7deba4fd298c582fc6742bc5b6e2b","The joint replenishment problem (JRP) has received considerable attention and all of the work on JRP is under explicit environment. In fact, the decision makers often have to face vague operational conditions. In this paper, a novel JRP model with fuzzy minor replenishment cost and fuzzy inventory holding cost is developed. More concisely, this model is a fuzzy dependent-chance programming (DCP) model. Subsequently, the technique of the traditional fuzzy simulation (FS) approach and differential evolution algorithm (DE) are integrated to design a hybrid intelligent algorithm named FSDE-I to solve this practical fuzzy JRP. Thirdly, another intelligent algorithm named FSDE-II using an improved FS approach is proposed to estimate the credibility more precisely. Finally, FSDE-I and FSDE-II are illustrated with numerical examples and the results show the effectiveness of FSDE-II. © 2012 Published by Elsevier B.V. All rights reserved.","Dependent-chance programming; Differential evolution algorithm; Fuzzy simulation; Hybrid intelligent algorithm; Joint replenishment","Dependent-chance programming; Differential evolution algorithms; Fuzzy simulation; Hybrid intelligent algorithms; Joint replenishment; Artificial intelligence; Software engineering; Algorithms",Article,Scopus,2-s2.0-84871922439
"Kebschull M., Guarnieri P., Demmer R.T., Boulesteix A.L., Pavlidis P., Papapanou P.N.","Molecular Differences between Chronic and Aggressive Periodontitis",2013,"Journal of Dental Research",34,10.1177/0022034513506011,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892871186&doi=10.1177%2f0022034513506011&partnerID=40&md5=9dd2e3a2b6fc6557496957b870b8f5f2","The 2 major forms of periodontitis, chronic (CP) and aggressive (AgP), do not display sufficiently distinct histopathological characteristics or microbiological/immunological features. We used molecular profiling to explore biological differences between CP and AgP and subsequently carried out supervised classification using machine-learning algorithms including an internal validation. We used whole-genome gene expression profiles from 310 ‘healthy’ or ‘diseased’ gingival tissue biopsies from 120 systemically healthy non-smokers, 65 with CP and 55 with AgP, each contributing with Ã¢â€°Â¥ 2 ‘diseased’ gingival papillae (n = 241; with bleeding-on-probing, probing depth Ã¢â€°Â¥ 4 mm, and clinical attachment loss Ã¢â€°Â¥ 3 mm), and, when available, a ‘healthy’ papilla (n = 69; no bleeding-on-probing, probing depth Ã¢â€°Â¤ 4 mm, and clinical attachment loss Ã¢â€°Â¤ 4 mm). Our analyses revealed limited differences between the gingival tissue transcriptional profiles of AgP and CP, with genes related to immune responses, apoptosis, and signal transduction overexpressed in AgP, and genes related to epithelial integrity and metabolism overexpressed in CP. Different classifying algorithms discriminated CP from AgP with an area under the curve ranging from 0.63 to 0.99. The small differences in gene expression and the highly variable classifier performance suggest limited dissimilarities between established AgP and CP lesions. Future analyses may facilitate the development of a novel, ‘intrinsic’ classification of periodontitis based on molecular profiling. © 2013, International & American Associations for Dental Research. All rights reserved.","classification; gene expression; machine learning; microarray analysis; pathogenesis; transcriptome","transcriptome; aggressive periodontitis; algorithm; apoptosis; area under the curve; article; artificial intelligence; chronic periodontitis; classification; epithelium; gene expression; gene expression profiling; genetic transcription; genetics; gingiva; human; immunology; machine learning; metabolism; methodology; microarray analysis; pathogenesis; pathology; periodontal disease; periodontal pocket; periodontics; receiver operating characteristic; sensitivity and specificity; signal transduction; aggressive periodontitis; chronic periodontitis; procedures; classification; gene expression; machine learning; microarray analysis; pathogenesis; transcriptome; Aggressive Periodontitis; Algorithms; Apoptosis; Area Under Curve; Artificial Intelligence; Chronic Periodontitis; Epithelium; Gene Expression Profiling; Gingiva; Humans; Microarray Analysis; Periodontal Attachment Loss; Periodontal Index; Periodontal Pocket; ROC Curve; Sensitivity and Specificity; Signal Transduction; Transcription, Genetic; Transcriptome; Aggressive Periodontitis; Algorithms; Apoptosis; Area Under Curve; Artificial Intelligence; Chronic Periodontitis; Epithelium; Gene Expression Profiling; Gingiva; Humans; Microarray Analysis; Periodontal Attachment Loss; Periodontal Index; Periodontal Pocket; ROC Curve; Sensitivity and Specificity; Signal Transduction; Transcription, Genetic; Transcriptome",Article,Scopus,2-s2.0-84892871186
"Wu S., Weinstein S.P., Conant E.F., Schnall M.D., Kontos D.","Automated chest wall line detection for whole-breast segmentation in sagittal breast MR images",2013,"Medical Physics",34,10.1118/1.4793255,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876173859&doi=10.1118%2f1.4793255&partnerID=40&md5=3d02e5076a726ad4fa02b3c2a8c5d823","Purpose: Breast magnetic resonance imaging (MRI) plays an important role in the clinical management of breast cancer. Computerized analysis is increasingly used to quantify breast MRI features in applications such as computer-aided lesion detection and fibroglandular tissue estimation for breast cancer risk assessment. Automated segmentation of the whole-breast as an organ from the other parts imaged is an important step in aiding lesion localization and fibroglandular tissue quantification. For this task, identifying the chest wall line (CWL) is most challenging due to image contrast variations, intensity discontinuity, and bias field. Methods: In this work, the authors develop and validate a fully automated image processing algorithm for accurate delineation of the CWL in sagittal breast MRI. The CWL detection is based on an integrated scheme of edge extraction and CWL candidate evaluation. The edge extraction consists of applying edge-enhancing filters and an edge linking algorithm. Increased accuracy is achieved by the synergistic use of multiple image inputs for edge extraction, where multiple CWL candidates are evaluated by the dynamic time warping algorithm coupled with the construction of a CWL reference. Their method is quantitatively validated by a dataset of 60 3D bilateral sagittal breast MRI scans (in total 3360 2D MR slices) that span the full American College of Radiology Breast Imaging Reporting and Data System (BI-RADS) breast density range. Agreement with manual segmentation obtained by an experienced breast imaging radiologist is assessed by both volumetric and boundary-based metrics, including four quantitative measures. Results: In terms of breast volume agreement with manual segmentation, the overlay percentage expressed by the Dices similarity coefficient is 95.0 and the difference percentage is 10.1. More specifically, for the segmentation accuracy of the CWL boundary, the CWL overlay percentage is 92.7 and averaged deviation distance is 2.3 mm. Their method requires ∼4.5 min for segmenting each 3D breast MRI scan (56 slices) in comparison to ∼35 min required for manual segmentation. Further analysis indicates that the segmentation performance of their method is relatively stable across the different BI-RADS density categories and breast volume, and also robust with respect to a varying range of the major parameters of the algorithm. Conclusions: Their fully automated method achieves high segmentation accuracy in a time-efficient manner. It could support large scale quantitative breast MRI analysis and holds the potential to become integrated into the clinical workflow for breast cancer clinical applications in the future. © 2013 American Association of Physicists in Medicine.","breast; chest wall line; edge extraction; magnetic resonance imaging (MRI); segmentation","adult; algorithm; article; breast cancer; controlled study; dynamics; female; human; image processing; major clinical study; nuclear magnetic resonance imaging; priority journal; radiologist; randomized controlled trial; risk assessment; thorax wall; validation study; algorithm; artificial intelligence; automated pattern recognition; breast; computer assisted diagnosis; histology; image enhancement; image subtraction; methodology; middle aged; reproducibility; sensitivity and specificity; Adult; Algorithms; Artificial Intelligence; Breast; Female; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Middle Aged; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique; Thoracic Wall",Article,Scopus,2-s2.0-84876173859
"Shawash J., Selviah D.R.","Real-time nonlinear parameter estimation using the levenberg-marquardt algorithm on field programmable gate arrays",2013,"IEEE Transactions on Industrial Electronics",34,10.1109/TIE.2012.2183833,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866296556&doi=10.1109%2fTIE.2012.2183833&partnerID=40&md5=098bdd8b3feba4ed5919e7e8958128d3","The Levenberg-Marquardt (LM) algorithm is a nonlinear parameter learning algorithm that converges accurately and quickly. This paper demonstrates for the first time to our knowledge, a real-time implementation of the LM algorithm on field programmable gate arrays (FPGAs). It was used to train neural networks to solve the eXclusive Or function (XOR), and for 3D-to-2D camera calibration parameter estimation. A Xilinx Virtex-5 ML506 was used to implement the LMA as a hardware-in-the-loop system. The XOR function was approximated in only 13 iterations from zero initial conditions, usually the same function is approximated in thousands of iterations using the error backpropagation algorithm. Also, this type of training not only reduced the number of iterations but also achieved a speed up in excess of 3 × 106 when compared to the software implementation. A real-time camera calibration and parameter estimation was performed successfully on FPGAs. Compared to the software implementation the FPGA implementation led to an increase in the mean squared error and standard deviation by only 17.94% and 8.04% respectively. The FPGA increased the calibration speed by a factor of 1.41 × 106. There are a wide range of systems problems solved via nonlinear parameter optimization, this study demonstrated that a hardware solution for systems such as automated optical inspection systems or systems dealing with projective geometry estimation and motion compensation systems in robotic vision systems is possible in real time. © 2012 IEEE.","Artificial intelligence; camera calibration; embedded; estimation; FPGA; hardware-in-the-loop; Levenberg-Marquardt; machine learning; mapping; neural networks; nonlinear; on-chip learning; online learning; optical inspection; projective geometry; supervised learning; systems; XOR","Artificial intelligence; Calibration; Cameras; Computer systems; E-learning; Embedded systems; Estimation; Field programmable gate arrays (FPGA); Hardware; Learning algorithms; Learning systems; Mapping; Motion compensation; Neural networks; Optical testing; Parameter estimation; Real time control; Supervised learning; Synthetic apertures; Camera calibration; embedded; Hard-ware-in-the-loop; Levenberg-Marquardt; nonlinear; On-chip learning; Online learning; Optical inspection; Projective geometry; XOR; Online systems",Article,Scopus,2-s2.0-84866296556
"Dai Q.","A competitive ensemble pruning approach based on cross-validation technique",2013,"Knowledge-Based Systems",34,10.1016/j.knosys.2012.08.024,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870065802&doi=10.1016%2fj.knosys.2012.08.024&partnerID=40&md5=ee073417fdc5de559e63047dfd84c0d3","Ensemble pruning is crucial for the consideration of both efficiency and predictive accuracy of an ensemble system. This paper proposes a new Competitive technique for Ensemble Pruning based on Cross-Validation (CEPCV). The data to be learnt by neural computing models are mostly drifting with time and environment, therefore a dynamic ensemble pruning method is indispensable for practical applications, while the proposed CEPCV method is just the kind of dynamic ensemble pruning method, which can realize on-line ensemble pruning and take full advantage of potentially valuable information. The algorithm naturally inherits the predominance of cross-validation technique, which implies that those networks regarded as winners in selective competitions and chosen into the pruned ensemble have the ""strongest"" generalization capability. It is essentially based on the strategy of ""divide and rule, collect the wisdom"", and might alleviate the local minima problem of many conventional ensemble pruning approaches only at the cost of a little greater computational cost, which is acceptable to most applications of ensemble learning. The comparative experiments among the four ensemble pruning algorithms, including: CEPCV and the state-of-the-art Directed Hill Climbing Ensemble Pruning (DHCEP) algorithm and two baseline methods, i.e. BSM, which chooses the Best Single Model in the initial ensemble based on their performances on the pruning set, and ALL, which reserves all network members of the initial ensemble, on ten benchmark classification tasks, demonstrate the effectiveness and validity of CEPCV. © 2012 Elsevier B.V. All rights reserved.","Competitive learning; Concept-drifting data; Cross-validation; Ensemble pruning; Neural networks ensemble","Baseline methods; Benchmark classification; Comparative experiments; Competitive learning; Computational costs; Concept-drifting data; Cross validation; Cross-validation technique; Dynamic ensemble; Ensemble learning; Ensemble pruning; Ensemble systems; Generalization capability; Hill climbing; Local minima problems; Neural computing; Predictive accuracy; Artificial intelligence; Software engineering; Algorithms",Article,Scopus,2-s2.0-84870065802
"Bennett C.C., Hauser K.","Artificial intelligence framework for simulating clinical decision-making: A Markov decision process approach",2013,"Artificial Intelligence in Medicine",34,10.1016/j.artmed.2012.12.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875271177&doi=10.1016%2fj.artmed.2012.12.003&partnerID=40&md5=c6aa5c4a7608cfa3442d2c88608be0ec","Objective: In the modern healthcare system, rapidly expanding costs/complexity, the growing myriad of treatment options, and exploding information streams that often do not effectively reach the front lines hinder the ability to choose optimal treatment decisions over time. The goal in this paper is to develop a general purpose (non-disease-specific) computational/artificial intelligence (AI) framework to address these challenges. This framework serves two potential functions: (1) a simulation environment for exploring various healthcare policies, payment methodologies, etc., and (2) the basis for clinical artificial intelligence - an AI that can "" think like a doctor"" Methods: This approach combines Markov decision processes and dynamic decision networks to learn from clinical data and develop complex plans via simulation of alternative sequential decision paths while capturing the sometimes conflicting, sometimes synergistic interactions of various components in the healthcare system. It can operate in partially observable environments (in the case of missing observations or data) by maintaining belief states about patient health status and functions as an online agent that plans and re-plans as actions are performed and new observations are obtained. This framework was evaluated using real patient data from an electronic health record. Results: The results demonstrate the feasibility of this approach; such an AI framework easily outperforms the current treatment-as-usual (TAU) case-rate/fee-for-service models of healthcare. The cost per unit of outcome change (CPUC) was $189 vs $497 for AI vs. TAU (where lower is considered optimal) - while at the same time the AI approach could obtain a 30-35% increase in patient outcomes. Tweaking certain AI model parameters could further enhance this advantage, obtaining approximately 50% more improvement (outcome change) for roughly half the costs. Conclusion: Given careful design and problem formulation, an AI simulation framework can approximate optimal decisions even in complex and uncertain environments. Future work is described that outlines potential lines of research and integration of machine learning algorithms for personalized medicine. © 2012 Elsevier B.V.","Chronic illness; Clinical artificial intelligence; Dynamic decision network; Markov decision process; Medical decision making; Multi-agent system","Chronic illness; Dynamic decision network; Markov Decision Processes; Medical decision making; Multi agent system (MAS); Complex networks; Decision making; Decision theory; Health care; Hospital data processing; Markov processes; Medicine; Multi agent systems; Optimization; Learning algorithms; article; artificial intelligence; clinical decision making; conceptual framework; electronic medical record; feasibility study; health care cost; health care policy; health care system; health status; human; machine learning; Markov Decision Process; medical fee; priority journal; simulation; Algorithms; Artificial Intelligence; Chronic Disease; Computer Simulation; Cost-Benefit Analysis; Decision Support Systems, Clinical; Decision Support Techniques; Decision Trees; Delivery of Health Care; Electronic Health Records; Feasibility Studies; Health Care Costs; Humans; Individualized Medicine; Markov Chains; Patient Selection",Article,Scopus,2-s2.0-84875271177
"Fernandez-Sanchez E.J., Diaz J., Ros E.","Background subtraction based on color and depth using active sensors.",2013,"Sensors (Basel, Switzerland)",33,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891463401&partnerID=40&md5=53991b76add993690e375d96dd9a4990","Depth information has been used in computer vision for a wide variety of tasks. Since active range sensors are currently available at low cost, high-quality depth maps can be used as relevant input for many applications. Background subtraction and video segmentation algorithms can be improved by fusing depth and color inputs, which are complementary and allow one to solve many classic color segmentation issues. In this paper, we describe one fusion method to combine color and depth based on an advanced color-based algorithm. This technique has been evaluated by means of a complete dataset recorded with Microsoft Kinect, which enables comparison with the original method. The proposed method outperforms the others in almost every test, showing more robustness to illumination changes, shadows, reflections and camouflage.",,"article; artificial intelligence; automated pattern recognition; color; colorimetry; computer assisted diagnosis; equipment; equipment design; equipment failure; methodology; three dimensional imaging; transducer; automated pattern recognition; colorimetry; computer assisted diagnosis; device failure analysis; devices; procedures; three dimensional imaging; Artificial Intelligence; Color; Colorimetry; Equipment Design; Equipment Failure Analysis; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Pattern Recognition, Automated; Transducers; Artificial Intelligence; Color; Colorimetry; Equipment Design; Equipment Failure Analysis; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Pattern Recognition, Automated; Transducers",Article,Scopus,2-s2.0-84891463401
"Bach S.H., Huang B., London B., Getoor L.","Hinge-loss Markov random fields: Convex inference for structured prediction",2013,"Uncertainty in Artificial Intelligence - Proceedings of the 29th Conference, UAI 2013",33,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888154242&partnerID=40&md5=9aaea8a6c1bac7edb572840eefe4f91d","Graphical models for structured domains are powerful tools, but the computational complexities of combinatorial prediction spaces can force restrictions on models, or require approximate inference in order to be tractable. Instead of working in a combinatorial space, we use hinge-loss Markov random fields (HL-MRFs), an expressive class of graphical models with log-concave density functions over continuous variables, which can represent confidences in discrete predictions. This paper demonstrates that HLMRFs are general tools for fast and accurate structured prediction. We introduce the first inference algorithm that is both scalable and applicable to the full class of HL-MRFs, and show how to train HL-MRFs with several learning algorithms. Our experiments show that HL-MRFs match or surpass the predictive performance of state-of-the-art methods, including discrete models, in four application domains.",,"Approximate inference; Continuous variables; Inference algorithm; Log-concave density function; Markov Random Fields; Predictive performance; State-of-the-art methods; Structured prediction; Artificial intelligence; Forecasting; Graphic methods; Inference engines; Tools; Markov processes",Conference Paper,Scopus,2-s2.0-84888154242
"Xie P., Xing E.P.","Integrating document clustering and topic modeling",2013,"Uncertainty in Artificial Intelligence - Proceedings of the 29th Conference, UAI 2013",33,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888158949&partnerID=40&md5=bdc5eb1602dea8f5d6dfcac51a78c58a","Document clustering and topic modeling are two closely related tasks which can mutually benefit each other. Topic modeling can project documents into a topic space which facilitates effective document clustering. Cluster labels discovered by document clustering can be incorporated into topic models to extract local topics specific to each cluster and global topics shared by all clusters. In this paper, we propose a multi-grain clustering topic model (MGCTM) which integrates document clustering and topic modeling into a unified framework and jointly performs the two tasks to achieve the overall best performance. Our model tightly couples two components: a mixture component used for discovering latent groups in document collection and a topic model component used for mining multi-grain topics including local topics specific to each cluster and global topics shared across clusters. We employ variational inference to approximate the posterior of hidden variables and learn model parameters. Experiments on two datasets demonstrate the effectiveness of our model.",,"Document Clustering; Document collection; Hidden variable; Mixture components; Model parameters; Project documents; Unified framework; Variational inference; Artificial intelligence; Information retrieval; Cluster analysis",Conference Paper,Scopus,2-s2.0-84888158949
"Peng X., Lin P., Zhang T., Wang J.","Extreme learning machine-based classification of ADHD using brain structural MRI data",2013,"PLoS ONE",33,10.1371/journal.pone.0079476,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894284759&doi=10.1371%2fjournal.pone.0079476&partnerID=40&md5=3ec56d7c74d1407f93b1d6e1872dc1f0","Background: Effective and accurate diagnosis of attention-deficit/ hyperactivity disorder (ADHD) is currently of significant interest. ADHD has been associated with multiple cortical features from structural MRI data. However, most existing learning algorithms for ADHD identification contain obvious defects, such as time-consuming training, parameters selection, etc. The aims of this study were as follows: (1) Propose an ADHD classification model using the extreme learning machine (ELM) algorithm for automatic, efficient and objective clinical ADHD diagnosis. (2) Assess the computational efficiency and the effect of sample size on both ELM and support vector machine (SVM) methods and analyze which brain segments are involved in ADHD. Methods: High-resolution three-dimensional MR images were acquired from 55 ADHD subjects and 55 healthy controls. Multiple brain measures (cortical thickness, etc.) were calculated using a fully automated procedure in the FreeSurfer software package. In total, 340 cortical features were automatically extracted from 68 brain segments with 5 basic cortical features. F-score and SFS methods were adopted to select the optimal features for ADHD classification. Both ELM and SVM were evaluated for classification accuracy using leave-one-out cross-validation. Results: We achieved ADHD prediction accuracies of 90.18% for ELM using eleven combined features, 84.73% for SVM-Linear and 86.55% for SVM-RBF. Our results show that ELM has better computational efficiency and is more robust as sample size changes than is SVM for ADHD classification. The most pronounced differences between ADHD and healthy subjects were observed in the frontal lobe, temporal lobe, occipital lobe and insular. Conclusion: Our ELM-based algorithm for ADHD diagnosis performs considerably better than the traditional SVM algorithm. This result suggests that ELM may be used for the clinical diagnosis of ADHD and the investigation of different brain diseases. © 2013 Peng et al.",,"Adolescent; Artificial Intelligence; Attention Deficit Disorder with Hyperactivity; Brain; Child; Female; Humans; Magnetic Resonance Imaging; Male; Software; Support Vector Machines",Article,Scopus,2-s2.0-84894284759
"Danandeh Mehr A., Kahya E., Olyaie E.","Streamflow prediction using linear genetic programming in comparison with a neuro-wavelet technique",2013,"Journal of Hydrology",33,10.1016/j.jhydrol.2013.10.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886301699&doi=10.1016%2fj.jhydrol.2013.10.003&partnerID=40&md5=f05e29672222446c4fdbb48b5db044e3","Accurate prediction of streamflow is an essential ingredient for both water quantity and quality management. In recent years, artificial intelligence (AI) techniques have been pronounced as a branch of computer science to model wide range of hydrological processes. A number of research works have been still comparing these techniques in order to find more efficient approach in terms of accuracy and applicability. In this study, two AI techniques, including hybrid wavelet-artificial neural network (WANN) and linear genetic programming (LGP) technique have been proposed to forecast monthly streamflow in a particular catchment and then performance of the proposed models were compared with each other in terms of root mean square error (RMSE) and Nash-Sutcliffe efficiency (NSE) measures. In this way, six different monthly streamflow scenarios based on records of two successive gauging stations have been modelled by a common three layer artificial neural network (ANN) method as the primary reference models. Then main time series of input(s) and output records were decomposed into sub-time series components using wavelet transform. In the next step, sub-time series of each model were imposed to ANN to develop WANN models as optimized version of the reference ANN models. The obtained results were compared with those that have been developed by LGP models. Our results showed the higher performance of LGP over WANN in all reference models. An explicit LGP model constructed by only basic arithmetic functions including one month-lagged records of both target and upstream stations revealed the best prediction model for the study catchment. © 2013 Elsevier B.V.","Data pre-processing; Feed forward neural networks; Hydrologic models; Linear genetic programming; Streamflow prediction; Wavelet transform","Accurate prediction; Arithmetic functions; Data preprocessing; Hydrologic models; Hydrological process; Linear genetic programming; Root mean square errors; Streamflow prediction; Catchments; Forecasting; Genetic programming; Mathematical models; Mean square error; Models; Neural networks; Number theory; Quality management; Runoff; Time series; Wavelet transforms; Stream flow; accuracy assessment; artificial intelligence; artificial neural network; catchment; data processing; hydrological modeling; linear programing; research work; streamflow; time series; water management; water quality; wavelet analysis",Article,Scopus,2-s2.0-84886301699
"Damarla S.R., Just M.A.","Decoding the representation of numerical values from brain activation patterns",2013,"Human Brain Mapping",33,10.1002/hbm.22087,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879231971&doi=10.1002%2fhbm.22087&partnerID=40&md5=8303cd901a80b6cc094d2bdb0bb2cf9c","Human neuroimaging studies have increasingly converged on the possibility that the neural representation of specific numbers may be decodable from brain activity, particularly in parietal cortex. Multivariate machine learning techniques have recently demonstrated that the neural representation of individual concrete nouns can be decoded from fMRI patterns, and that some patterns are general over people. Here we use these techniques to investigate whether the neural codes for quantities of objects can be accurately decoded. The pictorial mode (nonsymbolic) depicted a set of objects pictorially (e.g., a picture of three tomatoes), whereas the digit-object mode depicted quantities as combination of a digit (e.g., 3) with a picture of a single object. The study demonstrated that quantities of objects were decodable from neural activation patterns, in parietal regions. These brain activation patterns corresponding to a given quantity were common across objects and across participants in the pictorial mode. Other important findings included better identification of individual numbers in the pictorial mode, partial commonality of neural patterns across the two modes, and hemispheric asymmetry with pictorially-depicted numbers represented bilaterally and numbers in the digit-object mode represented primarily in the left parietal regions. The findings demonstrate the ability to identify individual quantities of objects based on neural patterns, indicating the presence of stable neural representations of numbers. Additionally, they indicate a predominance of neural representation of pictorially depicted numbers over the digit-object mode. © 2012 Wiley Periodicals, Inc.","FMRI multivoxel pattern analysis; Number representation; Parietal cortex","adult; article; brain asymmetry; brain depth stimulation; brain function; classification; female; functional magnetic resonance imaging; human; human experiment; machine learning; male; mathematical analysis; mathematical model; mathematical variable; normal human; parietal cortex; photography; priority journal; quantitative analysis; task performance; fMRI multivoxel pattern analysis; number representation; parietal cortex; Adult; Artificial Intelligence; Brain Mapping; Cerebrovascular Circulation; Concept Formation; Dominance, Cerebral; Echo-Planar Imaging; Female; Humans; Male; Mathematics; Neural Pathways; Parietal Lobe; Pattern Recognition, Automated; Photic Stimulation; Semantics; Symbolism; Young Adult",Article,Scopus,2-s2.0-84879231971
"Sanders P., Schulz C.","Think locally, act globally: Highly balanced graph partitioning",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",33,10.1007/978-3-642-38527-8_16,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884396431&doi=10.1007%2f978-3-642-38527-8_16&partnerID=40&md5=318c2d14b2a039c25f55f58b88b75c03","We present a novel local improvement scheme for graph partitions that allows to enforce strict balance constraints. Using negative cycle detection algorithms this scheme combines local searches that individually violate the balance constraint into a more global feasible improvement. We combine this technique with an algorithm to balance unbalanced solutions and integrate it into a parallel multi-level evolutionary algorithm, KaFFPaE, to tackle the problem. Overall, we obtain a system that is fast on the one hand and on the other hand is able to improve or reproduce many of the best known perfectly balanced partitioning results reported in the Walshaw benchmark. © 2013 Springer-Verlag.",,"Balanced graph partitioning; Cycle detection; Graph partition; Local search; Artificial intelligence; Computer science; Algorithms",Conference Paper,Scopus,2-s2.0-84884396431
"Gao W., Zhou Z.-H.","On the doubt about margin explanation of boosting",2013,"Artificial Intelligence",33,10.1016/j.artint.2013.07.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881447722&doi=10.1016%2fj.artint.2013.07.002&partnerID=40&md5=6cc790161de98408dd876263f8850408","Margin theory provides one of the most popular explanations to the success of AdaBoost, where the central point lies in the recognition that margin is the key for characterizing the performance of AdaBoost. This theory has been very influential, e.g., it has been used to argue that AdaBoost usually does not overfit since it tends to enlarge the margin even after the training error reaches zero. Previously the minimum margin bound was established for AdaBoost, however, Breiman (1999) [9] pointed out that maximizing the minimum margin does not necessarily lead to a better generalization. Later, Reyzin and Schapire (2006) [37] emphasized that the margin distribution rather than minimum margin is crucial to the performance of AdaBoost. In this paper, we first present the kth margin bound and further study on its relationship to previous work such as the minimum margin bound and Emargin bound. Then, we improve the previous empirical Bernstein bounds (Audibert et al. 2009; Maurer and Pontil, 2009) [2,30], and based on such findings, we defend the margin-based explanation against BreimanÊs doubts by proving a new generalization error bound that considers exactly the same factors as Schapire et al. (1998) [39] but is sharper than BreimanÊs (1999) [9] minimum margin bound. By incorporating factors such as average margin and variance, we present a generalization error bound that is heavily related to the whole margin distribution. We also provide margin distribution bounds for generalization error of voting classifiers in finite VC-dimension space. © 2013 Elsevier B.V.","Boosting; Classification; Ensemble methods; Margin theory","Boosting; Ensemble methods; Generalization Error; Generalization error bounds; Margin distributions; Margin theory; Training errors; Voting classifiers; Artificial intelligence; Classification (of information); Adaptive boosting",Article,Scopus,2-s2.0-84881447722
"Dascalu M., Dessus P., Trausan-Matu S., Bianco M., Nardy A.","Readerbench, an environment for analyzing text complexity and reading strategies",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",33,10.1007/978-3-642-39112-5-39,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880016425&doi=10.1007%2f978-3-642-39112-5-39&partnerID=40&md5=9153185018f899588ad41eb9940a8001","ReaderBench is a multi-purpose, multi-lingual and flexible environment that enables the assessment of a wide range of learners' productions and their manipulation by the teacher. ReaderBench allows the assessment of three main textual features: cohesion-based assessment, reading strategies identification and textual complexity evaluation, which have been subject to empirical validations. ReaderBench covers a complete cycle, from the initial complexity assessment of reading materials, the assignment of texts to learners, the capture of metacognitions reflected in one's textual verbalizations and comprehension evaluation, therefore fostering learner's self-regulation process. © 2013 Springer-Verlag Berlin Heidelberg.","Latent Dirichlet Allocation; Latent Semantic Analysis; Reading Strategies; Support Vector Machines; Text Cohesion; Textual Complexity","Latent Dirichlet allocation; Latent Semantic Analysis; Reading strategies; Text cohesion; Textual Complexity; Adhesion; Semantics; Statistics; Support vector machines; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84880016425
"Shui P.-L., Zhang W.-C.","Corner detection and classification using anisotropic directional derivative representations",2013,"IEEE Transactions on Image Processing",33,10.1109/TIP.2013.2259834,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879036823&doi=10.1109%2fTIP.2013.2259834&partnerID=40&md5=ea4320833013f8f6a88344f692a3debf","This paper proposes a corner detector and classifier using anisotropic directional derivative (ANDD) representations. The ANDD representation at a pixel is a function of the oriented angle and characterizes the local directional grayscale variation around the pixel. The proposed corner detector fuses the ideas of the contour- and intensity-based detection. It consists of three cascaded blocks. First, the edge map of an image is obtained by the Canny detector and from which contours are extracted and patched. Next, the ANDD representation at each pixel on contours is calculated and normalized by its maximal magnitude. The area surrounded by the normalized ANDD representation forms a new corner measure. Finally, the nonmaximum suppression and thresholding are operated on each contour to find corners in terms of the corner measure. Moreover, a corner classifier based on the peak number of the ANDD representation is given. Experiments are made to evaluate the proposed detector and classifier. The proposed detector is competitive with the two recent state-of-the-art corner detectors, the He & Yung detector and CPDA detector, in detection capability and attains higher repeatability under affine transforms. The proposed classifier can discriminate effectively simple corners, Y-type corners, and higher order corners. © 1992-2012 IEEE.","Anisotropic Gaussian directional derivative; corner detection","Canny detector; Corner detection; Corner detector; Detection capability; Directional derivative; Intensity-based; Non-maximum suppression; Oriented angle; Anisotropy; Edge detection; Pixels; Detectors; algorithm; anisotropy; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; image enhancement; methodology; reproducibility; sensitivity and specificity; automated pattern recognition; computer assisted diagnosis; image enhancement; procedures; Algorithms; Anisotropy; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Algorithms; Anisotropy; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84879036823
"Fazeli H., Soleimani R., Ahmadi M.-A., Badrnezhad R., Mohammadi A.H.","Experimental study and modeling of ultrafiltration of refinery effluents using a hybrid intelligent approach",2013,"Energy and Fuels",33,10.1021/ef400179b,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879386932&doi=10.1021%2fef400179b&partnerID=40&md5=a48e57db4c4309eaba1d039542792285","This study aims at indicating the capability of a state-of-the-art computational intelligence approach for predicting pseudosteady flux and pseudosteady fouling at different operating condition (temperature (T), transmembrane pressure (TMP), cross-flow velocity (CFV), and feed pH) as well as for permeate flux decline at the mentioned operational conditions with processing filtration time. To train and test these models, the experimental data collected during the polyacrylonitrile (PAN) UF process to treat the oily wastewater of a Tehran refinery have been used. The proposed method utilizes a least-squares support vector machine (LSSVM) to carry out nonlinear modeling. The genetic algorithm (GA) was employed to tune the optimal model parameters. GA-LSSVM has the competence of describing the nonlinear behavior. The accuracy of the proposed GA-LSSVM models is very satisfactory and quantified by statistical parameters. Finally, the results obtained by implementing various sensitivity analysis techniques portrayed that T and TMP have the most significant influence on pseudosteady flux and pseudosteady fouling, correspondingly, in comparison with other factors involved in the addressed treatment process. © 2013 American Chemical Society.",,"Different operating conditions; Hybrid intelligent approach; Least-squares support vector machines; Operational conditions; Polyacrylonitrile (PAN); Sensitivity analysis techniques; Statistical parameters; Transmembrane pressures; Artificial intelligence; Effluents; Thermomechanical pulping process; Ultrafiltration; Wastewater treatment; Refining",Article,Scopus,2-s2.0-84879386932
"Bonnici V., Giugno R., Pulvirenti A., Shasha D., Ferro A.","A subgraph isomorphism algorithm and its application to biochemical data",2013,"BMC Bioinformatics",33,10.1186/1471-2105-14-S7-S13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887032278&doi=10.1186%2f1471-2105-14-S7-S13&partnerID=40&md5=68fb2764b80eb87bbada63e40fe9082d","Background: Graphs can represent biological networks at the molecular, protein, or species level. An important query is to find all matches of a pattern graph to a target graph. Accomplishing this is inherently difficult (NP-complete) and the efficiency of heuristic algorithms for the problem may depend upon the input graphs. The common aim of existing algorithms is to eliminate unsuccessful mappings as early as and as inexpensively as possible.Results: We propose a new subgraph isomorphism algorithm which applies a search strategy to significantly reduce the search space without using any complex pruning rules or domain reduction procedures. We compare our method with the most recent and efficient subgraph isomorphism algorithms (VFlib, LAD, and our C++ implementation of FocusSearch which was originally distributed in Modula2) on synthetic, molecules, and interaction networks data. We show a significant reduction in the running time of our approach compared with these other excellent methods and show that our algorithm scales well as memory demands increase.Conclusions: Subgraph isomorphism algorithms are intensively used by biochemical tools. Our analysis gives a comprehensive comparison of different software approaches to subgraph isomorphism highlighting their weaknesses and strengths. This will help researchers make a rational choice among methods depending on their application. We also distribute an open-source package including our system and our own C++ implementation of FocusSearch together with all the used datasets (http://ferrolab.dmi.unict.it/ri.html). In future work, our findings may be extended to approximate subgraph isomorphism algorithms. © 2013 Bonnici et al.; licensee BioMed Central Ltd.","Algorithms comparisons and distributions; Biochemical graph data; Search strategies; Subgraph isomorphism algorithms","Biological networks; Comprehensive comparisons; Domain reduction procedure; Graph data; Interaction networks; Search strategies; Software approach; Subgraph isomorphism; Complex networks; Heuristic algorithms; Set theory; protein; protein; algorithm; article; artificial intelligence; computer program; metabolism; protein protein interaction; signal transduction; metabolism; Algorithms; Artificial Intelligence; Protein Interaction Maps; Proteins; Signal Transduction; Software; Algorithms; Artificial Intelligence; Protein Interaction Maps; Proteins; Signal Transduction; Software",Article,Scopus,2-s2.0-84887032278
"Choi S.G., Katz J., Kumaresan R., Cid C.","Multi-client non-interactive verifiable computation",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",33,10.1007/978-3-642-36594-2_28,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873978628&doi=10.1007%2f978-3-642-36594-2_28&partnerID=40&md5=7a2214ebedf7bfda6bc734125a32f315","Gennaro et al. (Crypto 2010) introduced the notion of non-interactive verifiable computation, which allows a computationally weak client to outsource the computation of a function f on a series of inputs x (1),... to a more powerful but untrusted server. Following a pre-processing phase (that is carried out only once), the client sends some representation of its current input x (i) to the server; the server returns an answer that allows the client to recover the correct result f(x (i)), accompanied by a proof of correctness that ensures the client does not accept an incorrect result. The crucial property is that the work done by the client in preparing its input and verifying the server's proof is less than the time required for the client to compute f on its own. We extend this notion to the multi-client setting, where n computationally weak clients wish to outsource to an untrusted server the computation of a function f over a series of joint inputs ,... without interacting with each other. We present a construction for this setting by combining the scheme of Gennaro et al. with a primitive called proxy oblivious transfer. © 2013 International Association for Cryptologic Research.",,"Current input; Oblivious transfer; Outsource; Pre-processing; Proof of correctness; Untrusted server; Artificial intelligence; Cryptography",Conference Paper,Scopus,2-s2.0-84873978628
"Wu Q., Zhu Q.","Transactional and QoS-aware dynamic service composition based on ant colony optimization",2013,"Future Generation Computer Systems",33,10.1016/j.future.2012.12.010,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873337043&doi=10.1016%2fj.future.2012.12.010&partnerID=40&md5=d037e93b5f185118361aef2bf00eb30e","Service composition facilitates seamless and flexible integration of applications from different providers. With the growing number of services that offer the same functionality but differ in non-functional properties published online, an efficient approach for dynamic service selection and composition is required. Traditionally, the problem is mostly addressed either from the quality of service (QoS) aspect or from the transaction aspect. In this paper, we first investigate the transactional properties of services and focus on how to compose individual services in a transactional manner, and then formulate the problem of transactional and QoS-aware dynamic service composition. By modeling the problem as a constrained directed acyclic graph, the ant colony optimization algorithm is utilized to seek a near-to-optimal solution efficiently. At last empirical studies are conducted and the experiments show that the proposed approach can approximate the optimal solution well while staying efficient. © 2013 Elsevier Ltd.","Ant colony optimization; Quality of service; Service composition; Transaction","Ant Colony Optimization (ACO); Ant Colony Optimization algorithms; Directed acyclic graph (DAG); Dynamic service composition; Dynamic services; Empirical studies; Flexible integration; Individual service; Non functional properties; Number of services; Optimal solutions; Service compositions; Transaction; Transactional properties; Algorithms; Artificial intelligence; Optimal systems; Quality of service",Article,Scopus,2-s2.0-84873337043
"Sasaki Y., Wang L.","Meet-in-the-middle technique for integral attacks against feistel ciphers",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",33,10.1007/978-3-642-35999-6_16,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872535419&doi=10.1007%2f978-3-642-35999-6_16&partnerID=40&md5=ea5e6a0a98503c3df35b97dc95d5bd1e","In this paper, an improvement for integral attacks against Feistel ciphers is discussed. The new technique can reduce the complexity of the key recovery phase. This possibly leads to an extension of the number of attacked rounds. In the integral attack, an attacker guesses a part of round keys and performs the partial decryption. The correctness of the guess is judged by examining whether the XOR sum of the results becomes 0 or not. In this paper, it is shown that the computation of the XOR sum of the partial decryptions can be divided into two independent parts if the analysis target adopts the Feistel network or its variant. Then, correct key candidates are efficiently obtained with the meet-in-the-middle approach. The effect of our technique is demonstrated for several Feistel ciphers. Improvements on integral attacks against LBlock, HIGHT, and CLEFIA are presented. Particularly, the number of attacked rounds with integral analysis is extended for LBlock. © 2013 Springer-Verlag Berlin Heidelberg.","CLEFIA; Feistel; HIGHT; Integral attack; LBlock; Meet-in-the-middle; Partial-sum","CLEFIA; Feistel; HIGHT; Integral attack; LBlock; Meet-in-the-middle; Partial-sum; Artificial intelligence; Cryptography",Conference Paper,Scopus,2-s2.0-84872535419
"Brits R., Engelbrecht A.P., Van Den Bergh F.","Scalability of niche PSO",2013,"2003 IEEE Swarm Intelligence Symposium, SIS 2003 - Proceedings",33,10.1109/SIS.2003.1202273,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942092313&doi=10.1109%2fSIS.2003.1202273&partnerID=40&md5=f072065cc666bb47052e1e3e563a6e52","In contrast to optimization techniques intended to find a single, global solution in a problem domain, niching (speciation) techniques have the ability to locate multiple solutions in multimodal domains. Numerous niching techniques have been proposed, broadly classified as temporal (locating solutions sequentially) and parallel (multiple solutions are found concurrently) techniques. Most research efforts to date have considered niching solutions through the eyes of genetic algorithms (GA), studying simple multimodal problems. Little attention has been given to the possibilities associated with emergent swarm intelligence techniques. Particle swarm optimization (PSO) utilizes properties of swarm behaviour not present in evolutionary algorithms such as GA, to rapidly solve optimization problems. This paper investigates the ability of two genetic algorithm niching techniques, sequential niching and deterministic crowding, to scale to higher dimensional domains with large numbers of solutions, and compare their performance to a PSO-based niching technique, Niche PSO. © 2003 IEEE.","Africa; Animals; Biological system modeling; Environmental factors; Equations; Evolutionary computation; Eyes; Genetic algorithms; Particle swarm optimization; Scalability","Algorithms; Animals; Artificial intelligence; Bioinformatics; Biological systems; Evolutionary algorithms; Genetic algorithms; Optimization; Problem solving; Scalability; Africa; Biological system modeling; Environmental factors; Equations; Eyes; Particle swarm optimization (PSO)",Conference Paper,Scopus,2-s2.0-84942092313
"Srinivasan D., Loo W.H., Cheu R.L.","Traffic incident detection using particle swarm optimization",2013,"2003 IEEE Swarm Intelligence Symposium, SIS 2003 - Proceedings",33,10.1109/SIS.2003.1202260,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942093884&doi=10.1109%2fSIS.2003.1202260&partnerID=40&md5=0087349ed94e006ebd75b8479c466dc0","This paper proposes a new approach to automatic incident detection on traffic highways using particle swarm optimization (PSO). The rampant growth in traffic incidents, which is high cost incurring, has led to significant interest in the development of effective incident detection techniques in recent years. Various techniques have been proposed to effectively address this problem, the most promising of which are artificial neural networks (ANN) based methods. Backpropagation (BP) has proven to be one of the best methods to train weights of ANN for incident detection. However it has several limitations including slow convergence, heuristic determination of parameters and possibility of getting stuck in a local minima. This paper overcomes these problems by using particle swarm optimization to train a neural network in place of BP. Actual data from a highway was used for training and testing of this method. Simulation results show that PSO performed better than the backpropagation algorithm. © 2003 IEEE.","Artificial neural networks; Automated highways; Backpropagation; Convergence; Costs; Neural networks; Particle swarm optimization; Road transportation; Telecommunication traffic; Testing","Artificial intelligence; Backpropagation; Backpropagation algorithms; Costs; Intelligent vehicle highway systems; Neural networks; Optimization; Telecommunication traffic; Testing; Transportation; Automatic incident detection; Convergence; Determination of parameters; Incident detection; Road transportation; Traffic incident detections; Traffic incidents; Training and testing; Particle swarm optimization (PSO)",Conference Paper,Scopus,2-s2.0-84942093884
"Gu Q., Han J.","Clustered support vector machines",2013,"Journal of Machine Learning Research",33,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954238583&partnerID=40&md5=768f1cf29aa7188642f30cb364aefec3","In many problems of machine learning, the data are distributed nonlinearly. One way to address this kind of data is training a nonlinear classifier such as kernel support vector machine (kernel SVM). However, the computational burden of kernel SVM limits its application to large scale datasets. In this paper, we propose a Clustered Support Vector Machine (CSVM), which tackles the data in a divide and conquer manner. More specifically, CSVM groups the data into several clusters, followed which it trains a linear support vector machine in each cluster to separate the data locally. Meanwhile, CSVM has an additional global regularization, which requires the weight vector of each local linear SVM aligning with a global weight vector. The global regularization leverages the information from one cluster to another, and avoids over-fitting in each cluster. We derive a data-dependent generalization error bound for CSVM, which explains the advantage of CSVM over linear SVM. Experiments on several benchmark datasets show that the proposed method outperforms linear SVM and some other related locally linear classifiers. It is also comparable to a fine-tuned kernel SVM in terms of prediction performance, while it is more efficient than kernel SVM. Copyright 2013 by the authors.",,"Artificial intelligence; Classification (of information); Learning systems; Vectors; Benchmark datasets; Computational burden; Generalization error bounds; Large-scale datasets; Linear classifiers; Linear Support Vector Machines; Nonlinear classifiers; Prediction performance; Support vector machines",Conference Paper,Scopus,2-s2.0-84954238583
"Augustinack J.C., Huber K.E., Stevens A.A., Roy M., Frosch M.P., van der Kouwe A.J.W., Wald L.L., Van Leemput K., McKee A.C., Fischl B.","Predicting the location of human perirhinal cortex, Brodmann's area 35, from MRI",2013,"NeuroImage",33,10.1016/j.neuroimage.2012.08.071,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867479773&doi=10.1016%2fj.neuroimage.2012.08.071&partnerID=40&md5=fcdfb0887fbac02ae5320f278b95b5ea","The perirhinal cortex (Brodmann's area 35) is a multimodal area that is important for normal memory function. Specifically, perirhinal cortex is involved in the detection of novel objects and manifests neurofibrillary tangles in Alzheimer's disease very early in disease progression. We scanned ex vivo brain hemispheres at standard resolution (1. mm × 1. mm × 1. mm) to construct pial/white matter surfaces in FreeSurfer and scanned again at high resolution (120 μm × 120 μm × 120 μm) to determine cortical architectural boundaries. After labeling perirhinal area 35 in the high resolution images, we mapped the high resolution labels to the surface models to localize area 35 in fourteen cases. We validated the area boundaries determined using histological Nissl staining. To test the accuracy of the probabilistic mapping, we measured the Hausdorff distance between the predicted and true labels and found that the median Hausdorff distance was 4.0. mm for the left hemispheres (n = 7) and 3.2. mm for the right hemispheres (n = 7) across subjects. To show the utility of perirhinal localization, we mapped our labels to a subset of the Alzheimer's Disease Neuroimaging Initiative dataset and found decreased cortical thickness measures in mild cognitive impairment and Alzheimer's disease compared to controls in the predicted perirhinal area 35. Our ex vivo probabilistic mapping of the perirhinal cortex provides histologically validated, automated and accurate labeling of architectonic regions in the medial temporal lobe, and facilitates the analysis of atrophic changes in a large dataset for earlier detection and diagnosis. © 2012 Elsevier Inc.","Alzheimer's disease; Localization; Mesocortex; Morphometry","aged; Alzheimer disease; article; brain mapping; controlled study; female; hemisphere; human; human tissue; image analysis; image processing; image quality; male; mild cognitive impairment; morphometrics; neuroimaging; nuclear magnetic resonance imaging; parahippocampal gyrus; perirhinal cortex; priority journal; temporal lobe; thickness; white matter; Aged; Algorithms; Artificial Intelligence; Cadaver; Female; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Male; Nerve Net; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Temporal Lobe",Article,Scopus,2-s2.0-84867479773
"Thida M., Eng H.-L., Remagnino P.","Laplacian eigenmap with temporal constraints for local abnormality detection in crowded scenes",2013,"IEEE Transactions on Cybernetics",32,10.1109/TCYB.2013.2242059,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884543420&doi=10.1109%2fTCYB.2013.2242059&partnerID=40&md5=73deedeeb5397e7603ef145afb284e47","This paper addresses the problem of detecting and localizing abnormal activities in crowded scenes. A spatiotemporal Laplacian eigenmap method is proposed to extract different crowd activities from videos. This is achieved by learning the spatial and temporal variations of local motions in an embedded space. We employ representatives of different activities to construct the model which characterizes the regular behavior of a crowd. This model of regular crowd behavior allows the detection of abnormal crowd activities both in local and global contexts and the localization of regions which show abnormal behavior. Experiments on the recently published data sets show that the proposed method achieves comparable results with the state-of-the-art methods without sacrificing computational simplicity. © 2013 IEEE.","Abnormality detection; Crowd analysis; Manifold embedding; Visual surveillance","Abnormality detection; Crowd analysis; Laplacian eigenmap; Manifold embedding; Spatial and temporal variation; State-of-the-art methods; Temporal constraints; Visual surveillance; Laplace transforms; Behavioral research; actimetry; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; computer simulation; crowding (area); decision support system; human; methodology; theoretical model; Actigraphy; Algorithms; Artificial Intelligence; Computer Simulation; Crowding; Decision Support Techniques; Humans; Image Interpretation, Computer-Assisted; Models, Theoretical; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84884543420
"Becker C., Rigamonti R., Lepetit V., Fua P.","Supervised feature learning for curvilinear structure segmentation.",2013,"Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention",32,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894617371&partnerID=40&md5=167582f042e88e83f427eb403215bde1","We present a novel, fully-discriminative method for curvilinear structure segmentation that simultaneously learns a classifier and the features it relies on. Our approach requires almost no parameter tuning and, in the case of 2D images, removes the requirement for hand-designed features, thus freeing the practitioner from the time-consuming tasks of parameter and feature selection. Our approach relies on the Gradient Boosting framework to learn discriminative convolutional filters in closed form at each stage, and can operate on raw image pixels as well as additional data sources, such as the output of other methods like the Optimally Oriented Flux. We will show that it outperforms state-of-the-art curvilinear segmentation methods on both 2D images and 3D image stacks.",,"algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; image enhancement; methodology; reproducibility; sensitivity and specificity; three dimensional imaging; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84894617371
"Liffiton M.H., Malik A.","Enumerating infeasibility: Finding multiple MUSes quickly",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",32,10.1007/978-3-642-38171-3_11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879924242&doi=10.1007%2f978-3-642-38171-3_11&partnerID=40&md5=40dce492b6697f05987463d899eafd6b","Methods for analyzing infeasible constraint sets have proliferated in the past decade, commonly focused on finding maximal satisfiable subsets (MSSes) or minimal unsatisfiable subsets (MUSes). Most common are methods for producing a single such subset (one MSS or one MUS), while a few algorithms have been presented for enumerating all of the interesting subsets of a constraint set. In the case of enumerating MUSes, the existing algorithms all fall short of the best methods for producing a single MUS; that is, none come close to the ideals of 1) producing the first output as quickly as a state-of-the-art single-MUS algorithm and 2) finding each successive MUS after a similar delay. In this work, we present a novel algorithm, applicable to any type of constraint system, that enumerates MUSes in this fashion. In fact, it is structured such that one can easily ""plug in"" any new single-MUS algorithm as a black box to immediately match advances in that area. We perform a detailed experimental analysis of the new algorithm's performance relative to existing MUS enumeration algorithms, and we show that it avoids some severe intractability issues encountered by the others while outperforming them in the task of quickly enumerating MUSes. © Springer-Verlag 2013.",,"Algorithm's performance; Black boxes; Constraint set; Constraint systems; Enumeration algorithms; Experimental analysis; Novel algorithm; Plug-ins; Artificial intelligence; Computer programming; Constraint theory; Discrete cosine transforms; Operations research; Set theory; Algorithms",Conference Paper,Scopus,2-s2.0-84879924242
"Costarelli D., Spigler R.","Multivariate neural network operators with sigmoidal activation functions",2013,"Neural Networks",32,10.1016/j.neunet.2013.07.009,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883048212&doi=10.1016%2fj.neunet.2013.07.009&partnerID=40&md5=ab69f6f3359cb2776269926cae044816","In this paper, we study pointwise and uniform convergence, as well as order of approximation, of a family of linear positive multivariate neural network (NN) operators with sigmoidal activation functions. The order of approximation is studied for functions belonging to suitable Lipschitz classes and using a moment-type approach. The special cases of NN operators, activated by logistic, hyperbolic tangent, and ramp sigmoidal functions are considered. Multivariate NNs approximation finds applications, typically, in neurocomputing processes. Our approach to NN operators allows us to extend previous convergence results and, in some cases, to improve the order of approximation. The case of multivariate quasi-interpolation operators constructed with sigmoidal functions is also considered. © 2013 Elsevier Ltd.","Lipschitz classes; Multivariate neural networks operators; Order of approximation; Sigmoidal functions; Uniform approximation","Lipschitz; Networks operators; Order of approximation; Sigmoidal functions; Uniform approximation; Artificial intelligence; Cognitive systems; Neural networks; article; cell activation; cell function; conceptual framework; mathematical computing; mathematical parameters; mathematical phenomena; multivariate analysis; nerve cell network; operator; priority journal; sigmoidal function; Lipschitz classes; Multivariate neural networks operators; Order of approximation; Sigmoidal functions; Uniform approximation; Algorithms; Computing Methodologies; Linear Models; Multivariate Analysis; Neural Networks (Computer); Programming, Linear",Article,Scopus,2-s2.0-84883048212
"Li S., Xue Y., Wang Z., Zhou G.","Active learning for cross-domain sentiment classification",2013,"IJCAI International Joint Conference on Artificial Intelligence",32,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063005&partnerID=40&md5=e513a7b9b2130efaea3796e30c617e96","In the literature, various approaches have been proposed to address the domain adaptation problem in sentiment classification (also called cross-domain sentiment classification). However, the adaptation performance normally much suffers when the data distributions in the source and target domains differ significantly. In this paper, we suggest to perform active learning for cross-domain sentiment classification by actively selecting a small amount of labeled data in the target domain. Accordingly, we propose an novel active learning approach for cross-domain sentiment classification. First, we train two individual classifiers, i.e., the source and target classifiers with the labeled data from the source and target respectively. Then, the two classifiers are employed to select informative samples with the selection strategy of Query By Committee (QBC). Third, the two classifier is combined to make the classification decision. Importantly, the two classifiers are trained by fully exploiting the unlabeled data in the target domain with the label propagation (LP) algorithm. Empirical studies demonstrate the effectiveness of our active learning approach for cross-domain sentiment classification over some strong baselines.",,"Classification decision; Data distribution; Domain adaptation; Empirical studies; Individual classifiers; Label propagation; Query by committees; Sentiment classification; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84896063005
"Li S., Fu Y.","Low-rank coding with b-matching constraint for semi-supervised classification",2013,"IJCAI International Joint Conference on Artificial Intelligence",32,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896061548&partnerID=40&md5=12351fe4fa03f499ca6e1dca23a9b2d2","Graph based semi-supervised learning (GSSL) plays an important role in machine learning systems. The most crucial step in GSSL is graph construction. Although several interesting graph construction methods have been proposed in recent years, how to construct an effective graph is still an open problem. In this paper, we develop a novel approach to constructing graph, which is based on low-rank coding and b-matching constraint. By virtue of recent advances in low-rank subspace recovery theory, compact encoding using low-rank representation coefficients allows us to obtain a robust similarity metric between all pairs of samples. Meanwhile, the b-matching constraint helps in obtaining a sparse and balanced graph, which benefits label propagation in GSSL. We build a joint optimization model to learn lowrank codes and balanced graph simultaneously. After using a graph re-weighting strategy, we present a semi-supervised learning algorithm by incorporating our sparse and balanced graph with Gaussian harmonic function (GHF). Experimental results on the Extended YaleB, PIE, ORL and USPS databases demonstrate that our graph outperforms several state-of-the-art graphs, especially when the labeled samples are very scarce.",,"Graph construction; Graph-construction method; Joint optimization; Low-rank representations; Semi-supervised classification; Semi-supervised learning; Similarity metrics; Subspace recoveries; Artificial intelligence; Harmonic functions; Learning algorithms; Supervised learning; Graph theory",Conference Paper,Scopus,2-s2.0-84896061548
"Fang J., Yang R., Gao L., Zhou D., Yang S., Liu A.-L., Du G.-H.","Predictions of buche inhibitors using support vector machine and naive bayesian classification techniques in drug discovery",2013,"Journal of Chemical Information and Modeling",32,10.1021/ci400331p,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888584457&doi=10.1021%2fci400331p&partnerID=40&md5=ebf9c3b93d522e8fb3d614912d521cda","Butyrylcholinesterase (BuChE, EC 3.1.1.8) is an important pharmacological target for Alzheimer's disease (AD) treatment. However, the currently available BuChE inhibitor screening assays are expensive, labor-intensive, and compound-dependent. It is necessary to develop robust in silico methods to predict the activities of BuChE inhibitors for the lead identification. In this investigation, support vector machine (SVM) models and naive Bayesian models were built to discriminate BuChE inhibitors (BuChEIs) from the noninhibitors. Each molecule was initially represented in 1870 structural descriptors (1235 from ADRIANA.Code, 334 from MOE, and 301 from Discovery studio). Correlation analysis and stepwise variable selection method were applied to figure out activity-related descriptors for prediction models. Additionally, structural fingerprint descriptors were added to improve the predictive ability of models, which were measured by cross-validation, a test set validation with 1001 compounds and an external test set validation with 317 diverse chemicals. The best two models gave Matthews correlation coefficient of 0.9551 and 0.9550 for the test set and 0.9132 and 0.9221 for the external test set. To demonstrate the practical applicability of the models in virtual screening, we screened an in-house data set with 3601 compounds, and 30 compounds were selected for further bioactivity assay. The assay results showed that 10 out of 30 compounds exerted significant BuChE inhibitory activities with IC50 values ranging from 0.32 to 22.22 μM, at which three new scaffolds as BuChE inhibitors were identified for the first time. To our best knowledge, this is the first report on BuChE inhibitors using machine learning approaches. The models generated from SVM and naive Bayesian approaches successfully predicted BuChE inhibitors. The study proved the feasibility of a new method for predicting bioactivities of ligands and discovering novel lead compounds. © 2013 American Chemical Society.",,"Butyrylcholinesterase; Correlation analysis; Correlation coefficient; Machine learning approaches; Naive Bayesian approach; Naive Bayesian classification; Structural descriptors; Variable selection methods; Assays; Bayesian networks; Bioactivity; Drug products; Multivariable control systems; Scaffolds; Support vector machines; cholinesterase; cholinesterase inhibitor; ligand; nootropic agent; article; artificial intelligence; Bayes theorem; chemical database; chemistry; computer interface; drug development; high throughput screening; human; structure activity relation; support vector machine; Artificial Intelligence; Bayes Theorem; Butyrylcholinesterase; Cholinesterase Inhibitors; Databases, Chemical; Drug Discovery; High-Throughput Screening Assays; Humans; Ligands; Nootropic Agents; Structure-Activity Relationship; Support Vector Machines; User-Computer Interface",Article,Scopus,2-s2.0-84888584457
"Hutter M., Schwabe P.","NaCl on 8-bit AVR microcontrollers",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",32,10.1007/978-3-642-38553-7-9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884827958&doi=10.1007%2f978-3-642-38553-7-9&partnerID=40&md5=dfe1bf3636e63b1da95c6299610b4b29","This paper presents first results of the Networking and Cryptography library (NaCl) on the 8-bit AVR family of microcontrollers. We show that NaCl, which has so far been optimized mainly for different desktop and server platforms, is feasible on resource-constrained devices while being very fast and memory efficient. Our implementation shows that encryption using Salsa20 requires 268 cycles/byte, authentication using Poly1305 needs 195 cycles/byte, a Curve25519 scalar multiplication needs 22 791 579 cycles, signing of data using Ed25519 needs 23 216 241 cycles, and verification can be done within 32 634 713 cycles. All implemented primitives provide at least 128-bit security, run in constant time, do not use secret-data-dependent branch conditions, and are open to the public domain (no usage restrictions). © 2013 Springer-Verlag Berlin Heidelberg.","ATmega; AVR; Curve25519; Ed25519; Edwards curves; Elliptic-curve cryptography; Poly1305; Salsa20","ATmega; AVR; Curve25519; Ed25519; Edwards curves; Poly1305; Salsa20; Artificial intelligence; Computer science; Cryptography",Conference Paper,Scopus,2-s2.0-84884827958
"Kowal M., Filipczuk P., Obuchowicz A., Korbicz J., Monczak R.","Computer-aided diagnosis of breast cancer based on fine needle biopsy microscopic images",2013,"Computers in Biology and Medicine",32,10.1016/j.compbiomed.2013.08.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883314367&doi=10.1016%2fj.compbiomed.2013.08.003&partnerID=40&md5=488b9caa4a61e429d115bbb61b08dba5","Prompt and widely available diagnostics of breast cancer is crucial for the prognosis of patients. One of the diagnostic methods is the analysis of cytological material from the breast. This examination requires extensive knowledge and experience of the cytologist. Computer-aided diagnosis can speed up the diagnostic process and allow for large-scale screening. One of the largest challenges in the automatic analysis of cytological images is the segmentation of nuclei. In this study, four different clustering algorithms are tested and compared in the task of fast nuclei segmentation. K-means, fuzzy C-means, competitive learning neural networks and Gaussian mixture models were incorporated for clustering in the color space along with adaptive thresholding in grayscale. These methods were applied in a medical decision support system for breast cancer diagnosis, where the cases were classified as either benign or malignant. In the segmented nuclei, 42 morphological, topological and texture features were extracted. Then, these features were used in a classification procedure with three different classifiers. The system was tested for classification accuracy by means of microscopic images of fine needle breast biopsies. In cooperation with the Regional Hospital in Zielona Góra, 500 real case medical images from 50 patients were collected. The acquired classification accuracy was approximately 96-100%, which is very promising and shows that the presented method ensures accurate and objective data acquisition that could be used to facilitate breast cancer diagnosis. © 2013 Elsevier Ltd.","Breast cancer; Computer-aided diagnosis; Image segmentation; Machine learning","Adaptive thresholding; Breast Cancer; Breast cancer diagnosis; Classification accuracy; Classification procedure; Gaussian Mixture Model; Knowledge and experience; Medical decision support system; Artificial intelligence; Biopsy; Clustering algorithms; Computer aided diagnosis; Decision support systems; Diseases; Image segmentation; Learning systems; Medical imaging; article; breast biopsy; breast cancer; classification algorithm; clinical article; computer assisted diagnosis; decision support system; diagnostic accuracy; diagnostic test accuracy study; fine needle aspiration biopsy; human; human tissue; image analysis; priority journal; Breast cancer; Computer-aided diagnosis; Image segmentation; Machine learning; Algorithms; Biopsy, Fine-Needle; Breast Neoplasms; Cell Nucleus; Cluster Analysis; Diagnosis, Computer-Assisted; Female; Fuzzy Logic; Humans; Microscopy; Neural Networks (Computer); Reproducibility of Results",Article,Scopus,2-s2.0-84883314367
"Carneiro G., Nascimento J.C.","Combining multiple dynamic models and deep learning architectures for tracking the left ventricle endocardium in ultrasound data",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",32,10.1109/TPAMI.2013.96,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884546164&doi=10.1109%2fTPAMI.2013.96&partnerID=40&md5=52d25ba8fbde88e9c98a796dea795c66","We present a new statistical pattern recognition approach for the problem of left ventricle endocardium tracking in ultrasound data. The problem is formulated as a sequential importance resampling algorithm such that the expected segmentation of the current time step is estimated based on the appearance, shape, and motion models that take into account all previous and current images and previous segmentation contours produced by the method. The new appearance and shape models decouple the affine and nonrigid segmentations of the left ventricle to reduce the running time complexity. The proposed motion model combines the systole and diastole motion patterns and an observation distribution built by a deep neural network. The functionality of our approach is evaluated using a dataset of diseased cases containing 16 sequences and another dataset of normal cases comprised of four sequences, where both sets present long axis views of the left ventricle. Using a training set comprised of diseased and healthy cases, we show that our approach produces more accurate results than current state-of-the-art endocardium tracking methods in two test sequences from healthy subjects. Using three test sequences containing different types of cardiopathies, we show that our method correlates well with interuser statistics produced by four cardiologists. © 1979-2012 IEEE.","deep belief networks; discriminative classifiers; dynamical model; Left ventricle segmentation; particle filters","Deep belief networks; Discriminative classifiers; Dynamical model; Left ventricles; Particle filter; Ultrasonics; Image segmentation; algorithm; artificial intelligence; automated pattern recognition; biological model; computer assisted diagnosis; computer simulation; echocardiography; echography; endocardium; heart left ventricle function; heart ventricle; human; procedures; article; automated pattern recognition; computer assisted diagnosis; echocardiography; heart left ventricle function; heart ventricle; methodology; Algorithms; Artificial Intelligence; Computer Simulation; Echocardiography; Endocardium; Heart Ventricles; Humans; Image Interpretation, Computer-Assisted; Models, Cardiovascular; Pattern Recognition, Automated; Ventricular Dysfunction, Left; Algorithms; Artificial Intelligence; Computer Simulation; Echocardiography; Endocardium; Heart Ventricles; Humans; Image Interpretation, Computer-Assisted; Models, Cardiovascular; Pattern Recognition, Automated; Ventricular Dysfunction, Left",Article,Scopus,2-s2.0-84884546164
"Deja M., Siemiatkowski M.S.","Feature-based generation of machining process plans for optimised parts manufacture",2013,"Journal of Intelligent Manufacturing",32,10.1007/s10845-012-0633-x,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880315541&doi=10.1007%2fs10845-012-0633-x&partnerID=40&md5=11945c5d92e4d86eacf27d960fa7266e","Efficacious integration of such CAx technologies as CAD, CAM and CAPP still remains a problem in engineering practice which constantly attracts research attention. Design by feature model is assumed as a main factor in the integration effort in various engineering and manufacturing domains. It refers principally to feature clustering and consequently operation sequencing in elaborated process plan designs. The focus of this paper is on CAPP for parts manufacture in systems of definite processing capabilities, involving multi-axis machining centres. A methodical approach is proposed to optimally solve for process planning problems, which consists in the identification of process alternatives and sequencing adequate working steps. The approach involves the use of the branch and bound concept from the field of artificial intelligence. A conceptual scheme for generation of alternative process plans in the form of a network is developed. It is based on part design data modelling in terms of machining features. A relevant algorithm is proposed for creating such a network and searching for the optimal process plan solution from the viewpoint of its operational performance, under formulated process constraints. The feasibility of the approach and the algorithm are illustrated by a numerical case with regard to a real application and diverse machine tools with relevant tooling. Generated process alternatives for complex machining with given systems, are studied using models programmed in the environment of Matlab® software. © 2012 The Author(s).","CAPP; Dynamic programming; Generative methods; Machining features; Optimal design; Process plan selection","CAPP; Generative methods; Machining feature; Optimal design; Process plan; Algorithms; Artificial intelligence; Dynamic programming; Linear programming; Machine tools; MATLAB; Processing; Process planning",Article,Scopus,2-s2.0-84880315541
"Maleszka M., Mianowska B., Nguyen N.T.","A method for collaborative recommendation using knowledge integration tools and hierarchical structure of user profiles",2013,"Knowledge-Based Systems",32,10.1016/j.knosys.2013.02.016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878014446&doi=10.1016%2fj.knosys.2013.02.016&partnerID=40&md5=4d76255b7ab0d4934510f8b3f14dd63b","This paper proposes a new approach to collaborative profile recommendation using a hierarchical structure for user modeling. In an information retrieval system a hierarchical user profile, used to personalize the document retrieval process, is being recommended to a new user based on profiles of other, similar users. Using methodology from the Knowledge Integration domain, four criteria are defined and analyzed to complete the aim of recommendation: Reliability is required for maintaining the correct structure of the profile, O1 and O2 Optimality postulates are required to calculate the best output profile by minimizing distances to other profiles, and Conflict Solution is used to better represent situations inherent to profile recommendation. Based on those criteria, four algorithms are proposed: O1 and O2 algorithms and modified O1 and O2 algorithms. These algorithms are further analyzed to check if they provide good recommendation. © 2013 Elsevier B.V. All rights reserved.","Collaborative recommendation; Hierarchical user profile; Knowledge integration; User profile integration","Collaborative recommendation; Document Retrieval; Hierarchical structures; Knowledge integration; New approaches; Optimality; User Modeling; User profile; Artificial intelligence; Software engineering; Algorithms",Article,Scopus,2-s2.0-84878014446
"Stoean R., Stoean C.","Modeling medical decision making by support vector machines, explaining by rules of evolutionary algorithms with feature selection",2013,"Expert Systems with Applications",32,10.1016/j.eswa.2012.11.007,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873202886&doi=10.1016%2fj.eswa.2012.11.007&partnerID=40&md5=68b6b0a18a4cc32743909adf41db7f75","Machine learning support for medical decision making is truly helpful only when it meets two conditions: high prediction accuracy and a good explanation of how the diagnosis was reached. Support vector machines (SVMs) successfully achieve the first target due to a kernel-based engine; evolutionary algorithms (EAs) can greatly accomplish the second owing to their adaptable nature. In this context, the current paper puts forward a two-step hybridized methodology, where learning is accurately performed by the SVMs and a comprehensible emulation of the resulting decision model is generated by EAs in the form of propositional rules, while referring only those indicators that highly influence the class separation. An individual highlighting of the medical attributes that trigger a specific diagnosis for a current patient record is additionally obtained; this feature thus increases the confidence of the physician in the resulting automated diagnosis. Without loss of generality, we aim to model three breast cancer instances, for reasons of both high incidence of the disease and the large application of state of the art artificial intelligence methods for this medical task. As such, the prediction of a benign/malignant condition as well as the recurrence/nonrecurrence of a cancer event are studied on the Wisconsin corresponding data sets from the UCI Machine Learning Repository. The proposed hybridization reached its goals. Rule prototypes evolve against a SVM consistent training data, while diversity among the different classes is implicitly preserved. Feature selection eventually leads to a resulting rule set where only the significant medical indicators together with the discriminating threshold values are referred, while individual relevance of attributes can be additionally obtained for each patient. The gain is thus dual: the EA benefits from a noise-free SVM preprocessed data and the resulting SVM model is able to output rules in a comprehensible, concise format for the physician. © 2012 Elsevier B.V. All rights reserved.","Breast cancer diagnosis; Cooperative coevolution; Evolutionary algorithms; Feature selection; Rule extraction; Support vector machines","Artificial intelligence methods; Automated diagnosis; Breast Cancer; Breast cancer diagnosis; Class separation; Cooperative co-evolution; Data sets; Decision models; Evolutionary algorithms (EAs); High incidence; Medical decision making; Patient record; Pre-processed data; Prediction accuracy; Rule extraction; Rule set; Specific diagnosis; State of the art; SVM model; Training data; UCI machine learning repository; WISCONSIN; Artificial intelligence; Decision making; Diagnosis; Diseases; Evolutionary algorithms; Feature extraction; Support vector machines",Article,Scopus,2-s2.0-84873202886
"Liu S., Yang J.Y., Zhang X.Y., Drury C.F., Reynolds W.D., Hoogenboom G.","Modelling crop yield, soil water content and soil temperature for a soybean-maize rotation under conventional and conservation tillage systems in Northeast China",2013,"Agricultural Water Management",32,10.1016/j.agwat.2013.03.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876301077&doi=10.1016%2fj.agwat.2013.03.001&partnerID=40&md5=b3a17c008011608cec9913496a9292e1","Soil-crop simulation models can be a valuable tool in evaluating conservation tillage practices which are viable both economically and environmentally. The objective of this study was to evaluate the ability of the DSSAT (Decision Support Systems for Agro-technology Transfer) Cropping System Model (CSM) with the CSM-CROPGRO-Soybean and CSM-CERES-Maize modules to predict crop yields and root zone soil water and temperature dynamics for a soybean (Glycine max)-maize (Zea mays) rotation under conventional tillage (CT), reduced conventional tillage (RT) and no-tillage (NT) on a cool, semi-arid ""Black soil"" (Mollisol) in Northeastern China. Crop yield, soil water content and soil temperature data collected from a field experiment at Hailun Experimental Station (47°26'N, 126°38'E) during 2004-2011 were used for model calibration and evaluation. The soybean and maize cultivar coefficients were calibrated using the CT yield data, and evaluated using the RT and NT yield data. ""Good"" agreement between simulated and measured yields was achieved for model calibration (normalized Residual Mean Square Error, nRMSE = 9-15%), and ""good"" to ""moderate"" agreement was achieved for model evaluation (nRMSE = 12-17%). Simulated volumetric soil water content in the top 20. cm of CT, RT and NT were in ""moderate"" to ""good"" agreement with measurements (index of agreement, d=0.81-0.91; nRMSE = 15.3-20.0%), provided that non-destructive in situ measurements of water content were used. Overall agreement between measured and simulated soil temperature varied from ""poor"" to ""excellent"" depending on year and tillage; and the measured soil temperatures were consistently overestimated (mean error, E=3.2-6.2), possibly due to lack of accounting in DSSAT for the insulating effects of accumulated surface residues, and the shading effects of standing crops. Refinement of the soil temperature algorithm in DSSAT is recommended. © 2013 Elsevier B.V.","Conservation tillage; Crop yield; CSM-CERES-Maize; CSM-CROPGRO-Soybean; Soil temperature; Soil water content","Conservation tillage; Crop yield; CSM-CERES-Maize; CSM-CROPGRO-Soybean; Soil temperature; Soil water content; Agricultural machinery; Amino acids; Artificial intelligence; Calibration; Crops; Cultivation; Decision support systems; Grain (agricultural product); Nitrogen fixation; Oilseeds; Soil conservation; Soil moisture; Technology transfer; Temperature; Computer simulation; agricultural modeling; algorithm; calibration; crop yield; cultivar; decision support system; maize; Mollisol; soil temperature; soil water; soybean; tillage; water content; China; Glycine max; Zea mays",Article,Scopus,2-s2.0-84876301077
"Yao W., Kumar A.","CONFlexFlow: Integrating Flexible clinical pathways into clinical decision support systems using context and rules",2013,"Decision Support Systems",32,10.1016/j.dss.2012.10.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878254061&doi=10.1016%2fj.dss.2012.10.008&partnerID=40&md5=4a2bd6e5f4230a38e07f816399af375b","We propose CONFlexFlow (Clinical cONtext based Flexible workFlow) as a novel approach for integrating clinical pathways into Clinical Decision Support Systems (CDSS). It recognizes that clinical pathways involve frequent deviations and require considerable flexibility. Thus, integration of flexible pathways is critical for the success of CDSS. Further, it is based on a better understanding of clinical context through ontologies, and bringing them to bear in deciding the right rules for a certain activity. We also describe an approach for dynamically realizing context dependent medical activities in a clinical pathway based on the needs of a specific case. To illustrate the feasibility of our approach, we propose an implementation framework and present a proof of concept prototype using multiple open source tools. The role of semantic web technologies in realizing flexible clinical pathways and integrating them into CDSS is highlighted. Preliminary results from our initial implementation are discussed. © 2012 Elsevier B.V.","Ad hoc subprocess; CDSS; Clinical pathway; Context; Ontologies; Rules","Ad hoc subprocess; CDSS; Clinical pathways; Context; Rules; Artificial intelligence; Ontology; Decision support systems",Article,Scopus,2-s2.0-84878254061
"Hoseini P., Shayesteh M.G.","Efficient contrast enhancement of images using hybrid ant colony optimisation, genetic algorithm, and simulated annealing",2013,"Digital Signal Processing: A Review Journal",32,10.1016/j.dsp.2012.12.011,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875216504&doi=10.1016%2fj.dsp.2012.12.011&partnerID=40&md5=c3c4c2ef552a4f488453bbfa0b676d4f","In this paper, we propose a hybrid algorithm including Genetic Algorithm (GA), Ant Colony Optimisation (ACO), and Simulated Annealing (SA) metaheuristics for increasing the contrast of images. In this way, contrast enhancement is obtained by global transformation of the input intensities. Ant colony optimisation is used to generate the transfer functions which map the input intensities to the output intensities. Simulated annealing as a local search method is utilised to modify the transfer functions generated by ant colony optimisation. And genetic algorithm has the responsibility of evolutionary process of antsE characteristics. The employed fitness function operates automatically and tends to provide a balance between contrast and naturalness of images. The results indicate that the new method achieves images with higher contrast than the previously presented methods from the subjective and objective viewpoints. Further, the proposed algorithm preserves the natural look of input images. © 2012 Elsevier Inc.","Ant Colony Optimisation (ACO); Contrast enhancement; Genetic Algorithm (GA); Hybrid metaheuristics; Image processing; Simulated Annealing (SA)","Ant colony optimisation; Contrast Enhancement; Evolutionary process; Fitness functions; Global transformation; Hybrid algorithms; Hybrid metaheuristics; Local search method; Ant colony optimization; Artificial intelligence; Heuristic algorithms; Image processing; Simulated annealing; Transfer functions; Genetic algorithms",Article,Scopus,2-s2.0-84875216504
"Magalhães-Mendes J.","A comparative study of crossover operators for genetic algorithms to solve the job shop scheduling problem",2013,"WSEAS Transactions on Computers",32,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879905690&partnerID=40&md5=5f1e588d54511777b82ea21a830e2c5e","Genetic algorithms (GA) are wide class of global optimization methods. Many genetic algorithms have been applied to solve combinatorial optimization problems. One of the problems in using genetic algorithms is the choice of crossover operator. The aim of this paper is to show the influence of genetic crossover operators on the performance of a genetic algorithm. The GA is applied to the job shop scheduling problem (JSSP). To achieve this aim an experimental study of a set of crossover operators is presented. The experimental study is based on a decision support system (DSS). To compare the abilities of different crossover operators, the DSS was designed giving all the operators the same opportunities. The genetic crossover operators are tested on a set of standard instances taken from the literature. The makespan is the measure used to evaluate the genetic crossover operators. The main conclusion is that there is a crossover operator having the best average performance on a specific set of solved instances.","Crossover Operators; Genetic Algorithms; JSSP; Operations Research; Optimization; Scheduling","Combinatorial optimization problems; Comparative studies; Crossover operator; Decision support system (dss); Experimental studies; Global optimization method; Job shop scheduling problems; JSSP; Artificial intelligence; Decision support systems; Global optimization; Operations research; Optimization; Scheduling; Scheduling algorithms; Genetic algorithms",Article,Scopus,2-s2.0-84879905690
"Gavrilovska L., Atanasovski V., Macaluso I., Dasilva L.A.","Learning and reasoning in cognitive radio networks",2013,"IEEE Communications Surveys and Tutorials",32,10.1109/SURV.2013.030713.00113,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888387564&doi=10.1109%2fSURV.2013.030713.00113&partnerID=40&md5=7f7a0cd75789b12a0e35884bc4d3481f","Cognitive radio networks challenge the traditional wireless networking paradigm by introducing concepts firmly stemmed into the Artificial Intelligence (AI) field, i.e., learning and reasoning. This fosters optimal resource usage and management allowing a plethora of potential applications such as secondary spectrum access, cognitive wireless backbones, cognitive machine-to-machine etc. The majority of overview works in the field of cognitive radio networks deal with the notions of observation and adaptations, which are not a distinguished cognitive radio networking aspect. Therefore, this paper provides insight into the mechanisms for obtaining and inferring knowledge that clearly set apart the cognitive radio networks from other wireless solutions. © 1998-2012 IEEE.","Game theory; Knowledge; Learning; Policy based reasoning; Reasoning; Reinforcement learning","Cognitive radio network; Cognitive radio networkings; Knowledge; Learning; Policy-based; Reasoning; Secondary spectrum access; Wireless networking; Artificial intelligence; Cognitive radio; Game theory; Radio systems; Reinforcement learning; Wireless networks",Article,Scopus,2-s2.0-84888387564
"Huang C., Li Y., Nevatia R.","Multiple target tracking by learning-based hierarchical association of detection responses",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",32,10.1109/TPAMI.2012.159,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874532468&doi=10.1109%2fTPAMI.2012.159&partnerID=40&md5=51ac18a149aba90f3a0785cfe3744512","We propose a hierarchical association approach to multiple target tracking from a single camera by progressively linking detection responses into longer track fragments (i.e., tracklets). Given frame-by-frame detection results, a conservative dual-threshold method that only links very similar detection responses between consecutive frames is adopted to generate initial tracklets with minimum identity switches. Further association of these highly fragmented tracklets at each level of the hierarchy is formulated as a Maximum A Posteriori (MAP) problem that considers initialization, termination, and transition of tracklets as well as the possibility of them being false alarms, which can be efficiently computed by the Hungarian algorithm. The tracklet affinity model, which measures the likelihood of two tracklets belonging to the same target, is a linear combination of automatically learned weak nonparametric models upon various features, which is distinct from most of previous work that relies on heuristic selection of parametric models and manual tuning of their parameters. For this purpose, we develop a novel bag ranking method and train the crucial tracklet affinity models by the boosting algorithm. This bag ranking method utilizes the soft max function to relax the oversufficient objective function used by the conventional instance ranking method. It provides a tighter upper bound of empirical errors in distinguishing correct associations from the incorrect ones, and thus yields more accurate tracklet affinity models for the tracklet association problem. We apply this approach to the challenging multiple pedestrian tracking task. Systematic experiments conducted on two real-life datasets show that the proposed approach outperforms previous state-of-the-art algorithms in terms of tracking accuracy, in particular, considerably reducing fragmentations and identity switches. © 1979-2012 IEEE.","AdaBoost; bag ranking; hierarchical association; Multiple target tracking","bag ranking; Boosting algorithm; Empirical errors; False alarms; Heuristic selections; Hierarchical association; Hungarian algorithm; Instance ranking; Linear combinations; Manual tuning; Max functions; Maximum a posteriori; Multiple target tracking; Non-parametric model; Objective functions; Parametric models; Pedestrian tracking; Ranking methods; Real life datasets; Single cameras; State-of-the-art algorithms; Systematic experiment; Tracking accuracy; Tracklet associations; Tracklets; Upper Bound; Adaptive boosting; Errors; Models; Algorithms; algorithm; article; artificial intelligence; automated pattern recognition; classification; factual database; human; human activities; image processing; methodology; movement (physiology); receiver operating characteristic; theoretical model; videorecording; Algorithms; Artificial Intelligence; Databases, Factual; Human Activities; Humans; Image Processing, Computer-Assisted; Models, Theoretical; Movement; Pattern Recognition, Automated; ROC Curve; Video Recording",Article,Scopus,2-s2.0-84874532468
"Peng H., Wang J., Pérez-Jiménez M.J., Shi P.","A novel image thresholding method based on membrane computing and fuzzy entropy",2013,"Journal of Intelligent and Fuzzy Systems",32,10.3233/IFS-2012-0549,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873580341&doi=10.3233%2fIFS-2012-0549&partnerID=40&md5=2b02ac33d00ce65f2a8c0cd1f6b7a651","Multi-level thresholding methods are a class of most popular image segmentation techniques, however, they are not computationally efficient since they exhaustively search the optimal thresholds to optimize the objective function. In order to eliminate the shortcoming, a novel multi-level thresholding method for image segmentation based on tissue P systems is proposed in this paper. The fuzzy entropy is used as the evaluation criterion to find optimal segmentation thresholds. The presented method can effectively search the optimal thresholds for multi-level thresholding based on fuzzy entropy due to parallel computing ability and particular mechanism of tissue P systems. Experimental results of both qualitative and quantitative comparisons for the proposed method and several existing methods illustrate its applicability and effectiveness. © 2013-IOS Press and the authors. All rights reserved.","fuzzy entropy; Image segmentation; membrane computing; thresholding method; tissue P systems","Computationally efficient; Evaluation criteria; Fuzzy entropy; Image thresholding; Membrane computing; Multilevel thresholding; Objective functions; Optimal segmentation; Optimal threshold; Parallel com- puting; Quantitative comparison; Segmentation techniques; Thresholding methods; Tissue P systems; Artificial intelligence; Entropy; Image segmentation; Optimization; Parallel architectures; Tissue; Bioinformatics",Article,Scopus,2-s2.0-84873580341
"Vasant P.","Hybrid LS-SA-PS methods for solving fuzzy non-linear programming problems",2013,"Mathematical and Computer Modelling",32,10.1016/j.mcm.2011.08.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866174817&doi=10.1016%2fj.mcm.2011.08.002&partnerID=40&md5=25c5b70ea0ab2ca46e6c45274f4475d2","The fuzzy optimization problem is one of the prominent topics in the broad area of artificial intelligence. It is applicable in the field of non-linear fuzzy programming. Its application as well as practical realization can been seen in all the real world problems. In this paper a large scale non-linear fuzzy programming problem was solved by hybrid optimization techniques like Line Search (LS), Simulated Annealing (SA) and Pattern Search (PS). An industrial production planning problem with a cubic objective function, eight decision variables and 29 constraints was solved successfully using the LS-SA-PS hybrid optimization techniques. The computational results for the objective function with respect to vagueness factor and level of satisfaction has been provided in the form of 2D and 3D plots. The outcome is very promising and strongly suggests that the hybrid LS-SA-PS algorithm is very efficient and productive in solving the large scale non-linear fuzzy programming problem. © 2011 Elsevier Ltd.","Level of satisfaction; Line search; Nonlinear fuzzy programming; Pattern search; Simulated annealing; Vagueness factor","Fuzzy programming; Level of satisfaction; Line searches; Pattern search; Vagueness factor; Artificial intelligence; Simulated annealing; Fuzzy systems",Article,Scopus,2-s2.0-84866174817
"Ahn J.-W., Brusilovsky P.","Adaptive visualization for exploratory information retrieval",2013,"Information Processing and Management",32,10.1016/j.ipm.2013.01.007,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892988054&doi=10.1016%2fj.ipm.2013.01.007&partnerID=40&md5=dac6f3bc6e7552aa69ea9166916ab24d","As the volume and breadth of online information is rapidly increasing, ad hoc search systems become less and less efficient to answer information needs of modern users. To support the growing complexity of search tasks, researchers in the field of information developed and explored a range of approaches that extend the traditional ad hoc retrieval paradigm. Among these approaches, personalized search systems and exploratory search systems attracted many followers. Personalized search explored the power of artificial intelligence techniques to provide tailored search results according to different user interests, contexts, and tasks. In contrast, exploratory search capitalized on the power of human intelligence by providing users with more powerful interfaces to support the search process. As these approaches are not contradictory, we believe that they can re-enforce each other. We argue that the effectiveness of personalized search systems may be increased by allowing users to interact with the system and learn/investigate the problem in order to reach the final goal. We also suggest that an interactive visualization approach could offer a good ground to combine the strong sides of personalized and exploratory search approaches. This paper proposes a specific way to integrate interactive visualization and personalized search and introduces an adaptive visualization based search system Adaptive VIBE that implements it. We tested the effectiveness of Adaptive VIBE and investigated its strengths and weaknesses by conducting a full-scale user study. The results show that Adaptive VIBE can improve the precision and the productivity of the personalized search system while helping users to discover more diverse sets of information. © 2013 Elsevier Ltd. All rights reserved.","Adaptive visualization; Exploratory information retrieval; Human-computer interaction; Open user model; Personalization; User study","Human computer interaction; Information retrieval; Online systems; Safety devices; Visualization; Adaptive visualization; Artificial intelligence techniques; Interactive visualizations; On-line information; Open user models; Personalizations; Personalized search; User study; Search engines",Article,Scopus,2-s2.0-84892988054
"Schaul T., Zhang S., LeCun Y.","No more pesky learning rates",2013,"30th International Conference on Machine Learning, ICML 2013",32,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897487847&partnerID=40&md5=4826d8cb5f16da4cf785f82207cbf14d","The performance of stochastic gradient descent (SGD) depends critically on how learning rates are tuned and decreased over time. We propose a method to automatically adjust multiple learning rates so as to minimize the expected error at any one time. The method relies on local gradient variations across samples. In our approach, learning rates can increase as well as decrease, making it suitable for non-stationary problems. Using a number of convex and non-convex learning tasks, we show that the resulting algorithm matches the performance of SGD or other adaptive approaches with their best settings obtained through systematic search, and effectively removes the need for learning rate tuning. Copyright 2013 by the author(s).",,"Artificial intelligence; Software engineering; Adaptive approach; Learning rates; Learning tasks; Local gradients; Non-stationary problems; One-time; Stochastic gradient descent; Systematic searches; Learning systems",Conference Paper,Scopus,2-s2.0-84897487847
"He J., Shen W., Divakaruni P., Wynter L., Lawrence R.","Improving traffic prediction with tweet semantics",2013,"IJCAI International Joint Conference on Artificial Intelligence",31,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896061078&partnerID=40&md5=8ed8dc270ff8ad711d8c42c32401b083","Road traffic prediction is a critical component in modern smart transportation systems. It provides the basis for traffic management agencies to generate proactive traffic operation strategies for alleviating congestion. Existing work on near-term traffic prediction (forecasting horizons in the range of 5 minutes to 1 hour) relies on the past and current traffic conditions. However, once the forecasting horizon is beyond 1 hour, i.e., in longer-term traffic prediction, these techniques do not work well since additional factors other than the past and current traffic conditions start to play important roles. To address this problem, in this paper, for the first time, we examine whether it is possible to use the rich information in online social media to improve longer-term traffic prediction. To this end, we first analyze the correlation between traffic volume and tweet counts with various granularities. Then we propose an optimization framework to extract traffic indicators based on tweet semantics using a transformation matrix, and incorporate them into traffic prediction via linear regression. Experimental results using traffic and Twitter data originated from the San Francisco Bay area of California demonstrate the effectiveness of our proposed framework.",,"Critical component; Online social medias; Optimization framework; Traffic conditions; Traffic management; Traffic prediction; Transformation matrices; Transportation system; Artificial intelligence; Forecasting; Linear transformations; Semantics; Social networking (online); Traffic congestion",Conference Paper,Scopus,2-s2.0-84896061078
"Poria S., Gelbukh A., Agarwal B., Cambria E., Howard N.","Common sense knowledge based personality recognition from text",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",31,10.1007/978-3-642-45111-9_42,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893799597&doi=10.1007%2f978-3-642-45111-9_42&partnerID=40&md5=718267c80c75837a15986594d6904234","Past works on personality detection has shown that psycho-linguistic features, frequency based analysis at lexical level, emotive words and other lexical clues such as number of first person or second person words carry major role to identify personality associated with the text. In this work, we propose a new architecture for the same task using common sense knowledge with associated sentiment polarity and affective labels. To extract the common sense knowledge with sentiment polarity scores and affective labels we used Senticnet which is one of the most useful resources for opinion mining and sentiment analysis. In particular, we combined common sense knowledge based features with phycho-linguistic features and frequency based features and later the features were employed in supervised classifiers. We designed five SMO based supervised classifiers for five personality traits. We observe that the use of common sense knowledge with affective and sentiment information enhances the accuracy of the existing frameworks which use only psycho-linguistic features and frequency based analysis at lexical level. © Springer-Verlag 2013.","Affective and sentiment information; Common sense knowledge; Personality detection","Affective and sentiment information; Commonsense knowledge; Opinion mining; Personality detections; Personality recognition; Personality traits; Sentiment analysis; Supervised classifiers; Artificial intelligence; Data mining; Linguistics; Soft computing; Knowledge based systems",Conference Paper,Scopus,2-s2.0-84893799597
"Chen Z., Mukherjee A., Liu B., Hsu M., Castellanos M., Ghosh R.","Leveraging multi-domain prior knowledge in topic models",2013,"IJCAI International Joint Conference on Artificial Intelligence",31,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896058560&partnerID=40&md5=5c1e908c9f8f1887fbf4b8369aab9202","Topic models have been widely used to identify topics in text corpora. It is also known that purely unsupervised models often result in topics that are not comprehensible in applications. In recent years, a number of knowledge-based models have been proposed, which allow the user to input prior knowledge of the domain to produce more coherent and meaningful topics. In this paper, we go one step further to study how the prior knowledge from other domains can be exploited to help topic modeling in the new domain. This problem setting is important from both the application and the learning perspectives because knowledge is inherently accumulative. We human beings gain knowledge gradually and use the old knowledge to help solve new problems. To achieve this objective, existing models have some major difficulties. In this paper, we propose a novel knowledge-based model, called MDK-LDA, which is capable of using prior knowledge from multiple domains. Our evaluation results will demonstrate its effectiveness.",,"Evaluation results; Human being; Knowledge-based model; Multi domains; Multiple domains; Prior knowledge; Text corpora; Topic Modeling; Artificial intelligence; Knowledge based systems; Models; Knowledge management",Conference Paper,Scopus,2-s2.0-84896058560
"Lasecki W.S., Wesley R., Nichols J., Kulkarni A., Allen J.F., Bigham J.P.","Chorus: A crowd-powered conversational assistant",2013,"UIST 2013 - Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology",31,10.1145/2501988.2502057,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887568821&doi=10.1145%2f2501988.2502057&partnerID=40&md5=0a3a9c7c080d2b683cb5592d61bda354","Despite decades of research attempting to establish conversational interaction between humans and computers, the capabilities of automated conversational systems are still limited. In this paper, we introduce Chorus, a crowd-powered conversational assistant. When using Chorus, end users converse continuously with what appears to be a single conversational partner. Behind the scenes, Chorus leverages multiple crowd workers to propose and vote on responses. A shared memory space helps the dynamic crowd workforce maintain consistency, and a game-theoretic incentive mechanism helps to balance their efforts between proposing and voting. Studies with 12 end users and 100 crowd workers demonstrate that Chorus can provide accurate, topical responses, answering nearly 93% of user queries appropriately, and staying on-topic in over 95% of responses. We also observed that Chorus has advantages over pairing an end user with a single crowd worker and end users completing their own tasks in terms of speed, quality, and breadth of assistance. Chorus demonstrates a new future in which conversational assistants are made usable in the real world by combining human and machine intelligence, and may enable a useful new way of interacting with the crowds powering other systems. Copyright © 2013 ACM.","Conversational assistants; Crowd-powered systems; Crowdsourcing; Dialog systems; Human computation","Conversational assistants; Conversational interaction; Conversational systems; Crowdsourcing; Dialog systems; Human computation; Incentive mechanism; Machine intelligence; Artificial intelligence; User interfaces; Human computer interaction",Conference Paper,Scopus,2-s2.0-84887568821
"Jurio A., Bustince H., Pagola M., Pradera A., Yager R.R.","Some properties of overlap and grouping functions and their application to image thresholding",2013,"Fuzzy Sets and Systems",31,10.1016/j.fss.2012.12.009,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881479706&doi=10.1016%2fj.fss.2012.12.009&partnerID=40&md5=eac7871d8c9f3882c819445df209f1e1","In this paper we study under which conditions overlap and grouping functions satisfy some commonly demanded properties such as migrativity or homogeneity. We also recall that the convex combination of overlap (grouping) functions is a new overlap (grouping) function. This property allows us to achieve a consensus between different methods that solve certain problems by means of these functions. We also show one application of this property in image processing. © 2013 Elsevier B.V. All rights reserved.","Convex combination; Grouping function; Ignorance function; Migrativity; Overlap function","Convex combinations; Ignorance functions; Image thresholding; Migrativity; Overlap functions; Artificial intelligence; Fuzzy sets; Image processing",Article,Scopus,2-s2.0-84881479706
"Sander J.D., Ramirez C.L., Linder S.J., Pattanayak V., Shoresh N., Ku M., Foden J.A., Reyon D., Bernstein B.E., Liu D.R., Keith Joung J.","In silico abstraction of zinc finger nuclease cleavage profiles reveals an expanded landscape of off-target sites",2013,"Nucleic Acids Research",31,10.1093/nar/gkt716,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885979507&doi=10.1093%2fnar%2fgkt716&partnerID=40&md5=cc0866cbc1df8fc2cc4c85d3051d3dad","Gene-editing nucleases enable targeted modification of DNA sequences in living cells, thereby facilitating efficient knockout and precise editing of endogenous loci. Engineered nucleases also have the potential to introduce mutations at off-target sites of action. Such unintended alterations can confound interpretation of experiments and can have implications for development of therapeutic applications. Recently, two improved methods for identifying the off-target effects of zinc finger nucleases (ZFNs) were described-one using an in vitro cleavage site selection method and the other exploiting the insertion of integration-defective lentiviruses into nuclease-induced double-stranded DNA breaks. However, application of these two methods to a ZFN pair targeted to the human CCR5 gene led to identification of largely non-overlapping off-target sites, raising the possibility that additional off-target sites might exist. Here, we show that in silico abstraction of ZFN cleavage profiles obtained from in vitro cleavage site selections can greatly enhance the ability to identify potential off-target sites in human cells. Our improved method should enable more comprehensive profiling of ZFN specificities. © 2013 The Author(s) 2013.",,"chemokine receptor CCR5; vasculotropin; zinc finger nuclease; article; base pairing; CCR5 gene; computer model; controlled study; DNA cleavage; double stranded DNA break; gene; gene targeting; human; human cell; priority journal; protein engineering; site directed mutagenesis; VEGF gene; Artificial Intelligence; Base Sequence; Computer Simulation; Deoxyribonucleases; DNA; DNA Cleavage; High-Throughput Nucleotide Sequencing; Humans; Receptors, CCR5; Sequence Analysis, DNA; Vascular Endothelial Growth Factor A; Zinc Fingers",Article,Scopus,2-s2.0-84885979507
"Konukoglu E., Glocker B., Zikic D., Criminisi A.","Neighbourhood approximation using randomized forests",2013,"Medical Image Analysis",31,10.1016/j.media.2013.04.013,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879879457&doi=10.1016%2fj.media.2013.04.013&partnerID=40&md5=37e72c1cef46d34c3f5706e00f04c3c0","Leveraging available annotated data is an essential component of many modern methods for medical image analysis. In particular, approaches making use of the ""neighbourhood"" structure between images for this purpose have shown significant potential. Such techniques achieve high accuracy in analysing an image by propagating information from its immediate ""neighbours"" within an annotated database. Despite their success in certain applications, wide use of these methods is limited due to the challenging task of determining the neighbours for an out-of-sample image. This task is either computationally expensive due to large database sizes and costly distance evaluations, or infeasible due to distance definitions over semantic information, such as ground truth annotations, which is not available for out-of-sample images.This article introduces Neighbourhood Approximation Forests (NAFs), a supervised learning algorithm providing a general and efficient approach for the task of approximate nearest neighbour retrieval for arbitrary distances. Starting from an image training database and a user-defined distance between images, the algorithm learns to use appearance-based features to cluster images approximating the neighbourhood structured induced by the distance. NAF is able to efficiently infer nearest neighbours of an out-of-sample image, even when the original distance is based on semantic information. We perform experimental evaluation in two different scenarios: (i) age prediction from brain MRI and (ii) patch-based segmentation of unregistered, arbitrary field of view CT images. The results demonstrate the performance, computational benefits, and potential of NAF for different image analysis applications. © 2013 Elsevier B.V.","Approximate nearest neighbours; Image-based regression; Patch-based segmentation; Random decision forests; Supervised neighbour search","Annotated database; Appearance based; Decision forest; Experimental evaluation; Image-based; Nearest neighbour; Semantic information; Supervised neighbour search; Computerized tomography; Database systems; Decision trees; Image analysis; Magnetic resonance imaging; Semantics; Forestry; adult; age distribution; aged; article; data base; diagnostic imaging; human; image analysis; learning algorithm; neighborhood approximation forest; neuroimaging; nuclear magnetic resonance imaging; predictive value; priority journal; semantics; Approximate nearest neighbours; Image-based regression; Patch-based segmentation; Random decision forests; Supervised neighbour search; Algorithms; Artificial Intelligence; Computer Simulation; Data Interpretation, Statistical; Image Enhancement; Image Interpretation, Computer-Assisted; Models, Statistical; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique; Data Bases; Decision Making; Forests; Image Analysis; Magnetic Resonance; Regression Analysis",Article,Scopus,2-s2.0-84879879457
"Su H., Yin Z., Huh S., Kanade T.","Cell segmentation in phase contrast microscopy images via semi-supervised classification over optics-related features",2013,"Medical Image Analysis",31,10.1016/j.media.2013.04.004,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879890356&doi=10.1016%2fj.media.2013.04.004&partnerID=40&md5=7b4ce73b5c12db7d688bafef1aab6929","Phase-contrast microscopy is one of the most common and convenient imaging modalities to observe long-term multi-cellular processes, which generates images by the interference of lights passing through transparent specimens and background medium with different retarded phases. Despite many years of study, computer-aided phase contrast microscopy analysis on cell behavior is challenged by image qualities and artifacts caused by phase contrast optics. Addressing the unsolved challenges, the authors propose (1) a phase contrast microscopy image restoration method that produces phase retardation features, which are intrinsic features of phase contrast microscopy, and (2) a semi-supervised learning based algorithm for cell segmentation, which is a fundamental task for various cell behavior analysis. Specifically, the image formation process of phase contrast microscopy images is first computationally modeled with a dictionary of diffraction patterns; as a result, each pixel of a phase contrast microscopy image is represented by a linear combination of the bases, which we call phase retardation features. Images are then partitioned into phase-homogeneous atoms by clustering neighboring pixels with similar phase retardation features. Consequently, cell segmentation is performed via a semi-supervised classification technique over the phase-homogeneous atoms. Experiments demonstrate that the proposed approach produces quality segmentation of individual cells and outperforms previous approaches. © 2013 Elsevier B.V.","Cell segmentation; Phase contrast microscopy image; Phase retardation feature; Semi-supervised classification; Sparse representation","Cell segmentation; Phase retardation; Phase-contrast microscopy images; Semi-supervised classification; Sparse representation; Computer aided analysis; Image reconstruction; Pixels; Supervised learning; algorithm; article; cell function; cell membrane; cell segmentation; cellular parameters; computer aided design; human; image analysis; image quality; mitosis; phase contrast microscopy; priority journal; Cell segmentation; Phase contrast microscopy image; Phase retardation feature; Semi-supervised classification; Sparse representation; Algorithms; Artifacts; Artificial Intelligence; Cell Tracking; Image Enhancement; Image Interpretation, Computer-Assisted; Microscopy, Phase-Contrast; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84879890356
"Yang Y., Han D., Han C.","Discounted combination of unreliable evidence using degree of disagreement",2013,"International Journal of Approximate Reasoning",31,10.1016/j.ijar.2013.04.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879602287&doi=10.1016%2fj.ijar.2013.04.002&partnerID=40&md5=4dc0ca9a3c4e48c1ce1b9b0b4c4e6448","When Dempster's rule is used to implement a combination of evidence, all sources are considered equally reliable. However, in many real applications, all the sources of evidence may not have the same reliability. To resolve this problem, a number of methods for discounting unreliable sources of evidence have been proposed in which the estimation of the discounting (weighting) factors is crucial, especially when prior knowledge is unavailable. In this paper, we propose a new degree of disagreement through which discounting factors can be generated for discounting combinations of unreliable evidence. The new degree of disagreement is established using distance of evidence. It can be experimentally verified that our degree of disagreement describes the disagreements or differences among bodies of evidence well and that it can be effectively used in discounting combinations of unreliable evidence. © 2013 Elsevier Inc. All rights reserved.","Belief function; Discounting; Distance of evidence; Evidence theory","Belief function; Dempster's rule; Discounting; Distance of evidence; Evidence theories; Number of methods; Prior knowledge; Real applications; Artificial intelligence; Software engineering; Uncertainty analysis",Article,Scopus,2-s2.0-84879602287
"Dawadi P.N., Cook D.J., Schmitter-Edgecombe M., Parsey C.","Automated assessment of cognitive health using smart home technologies",2013,"Technology and Health Care",31,10.3233/THC-130734,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884367510&doi=10.3233%2fTHC-130734&partnerID=40&md5=9cf00f20a9f238ab6036e21dcd4ca4b8","BACKGROUND: The goal of this work is to develop intelligent systems to monitor the wellbeing of individuals in their home environments. OBJECTIVE: This paper introduces a machine learning-based method to automatically predict activity quality in smart homes and automatically assess cognitive health based on activity quality. METHODS: This paper describes an automated framework to extract set of features from smart home sensors data that reflects the activity performance or ability of an individual to complete an activity which can be input to machine learning algorithms. Output from learning algorithms including principal component analysis, support vector machine, and logistic regression algorithms are used to quantify activity quality for a complex set of smart home activities and predict cognitive health of participants. RESULTS: Smart home activity data was gathered from volunteer participants (n=263) who performed a complex set of activities in our smart home testbed. We compare our automated activity quality prediction and cognitive health prediction with direct observation scores and health assessment obtained from neuropsychologists. With all samples included, we obtained statistically significant correlation (r=0.54) between direct observation scores and predicted activity quality. Similarly, using a support vector machine classifier, we obtained reasonable classification accuracy (area under the ROC curve=0.80, g-mean=0.73) in classifying participants into two different cognitive classes, dementia and cognitive healthy. CONCLUSIONS: The results suggest that it is possible to automatically quantify the task quality of smart home activities and perform limited assessment of the cognitive health of individual if smart home activities are properly chosen and learning algorithms are appropriately trained. © 2013 - IOS Press and the authors. All rights reserved.","(medical) cognitive assessment; (Methodological) machine learning; behavior modeling; dementia; MCI; smart environments","adult; aged; algorithm; ambulatory monitoring; artificial intelligence; clinical trial; cognition; Cognition Disorders; daily life activity; dementia; family size; female; human; male; middle aged; mild cognitive impairment; procedures; psychology; telemetry; article; cognitive defect; daily life activity; methodology; psychological aspect; Activities of Daily Living; Adult; Aged; Algorithms; Artificial Intelligence; Cognition; Cognition Disorders; Dementia; Family Characteristics; Female; Humans; Male; Middle Aged; Mild Cognitive Impairment; Monitoring, Ambulatory; Telemetry; Activities of Daily Living; Adult; Aged; Algorithms; Artificial Intelligence; Cognition; Cognition Disorders; Dementia; Family Characteristics; Female; Humans; Male; Middle Aged; Mild Cognitive Impairment; Monitoring, Ambulatory; Telemetry",Article,Scopus,2-s2.0-84884367510
"Min D., Lu J., Do M.N.","Joint histogram-based cost aggregation for stereo matching",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",31,10.1109/TPAMI.2013.15,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883146246&doi=10.1109%2fTPAMI.2013.15&partnerID=40&md5=70674c7ce5b962db44b7d62892d82bd1","This paper presents a novel method for performing efficient cost aggregation in stereo matching. The cost aggregation problem is reformulated from the perspective of a histogram, giving us the potential to reduce the complexity of the cost aggregation in stereo matching significantly. Differently from previous methods which have tried to reduce the complexity in terms of the size of an image and a matching window, our approach focuses on reducing the computational redundancy that exists among the search range, caused by a repeated filtering for all the hypotheses. Moreover, we also reduce the complexity of the window-based filtering through an efficient sampling scheme inside the matching window. The tradeoff between accuracy and complexity is extensively investigated by varying the parameters used in the proposed method. Experimental results show that the proposed method provides high-quality disparity maps with low complexity and outperforms existing local methods. This paper also provides new insights into complexity-constrained stereo-matching algorithm design. © 1979-2012 IEEE.","Cost aggregation; disparity hypotheses; joint histogram; stereo matching","Algorithm design; Cost aggregations; disparity hypotheses; Disparity map; Efficient costs; Efficient sampling schemes; Joint histograms; Stereo matching; Communication channels (information theory); Graphic methods; Costs; algorithm; artificial intelligence; automated pattern recognition; computer assisted diagnosis; image enhancement; image subtraction; mathematical computing; procedures; reproducibility; sensitivity and specificity; three dimensional imaging; article; automated pattern recognition; computer assisted diagnosis; methodology; three dimensional imaging; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Numerical Analysis, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Numerical Analysis, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-84883146246
"Bagstad K.J., Semmens D.J., Winthrop R.","Comparing approaches to spatially explicit ecosystem service modeling: A case study from the San Pedro River, Arizona",2013,"Ecosystem Services",31,10.1016/j.ecoser.2013.07.007,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884590260&doi=10.1016%2fj.ecoser.2013.07.007&partnerID=40&md5=3af115da9c66fd055568f3b6f39ebf78","Although the number of ecosystem service modeling tools has grown in recent years, quantitative comparative studies of these tools have been lacking. In this study, we applied two leading open-source, spatially explicit ecosystem services modeling tools - Artificial Intelligence for Ecosystem Services (ARIES) and Integrated Valuation of Ecosystem Services and Tradeoffs (InVEST) - to the San Pedro River watershed in southeast Arizona, USA, and northern Sonora, Mexico. We modeled locally important services that both modeling systems could address - carbon, water, and scenic viewsheds. We then applied managerially relevant scenarios for urban growth and mesquite management to quantify ecosystem service changes. InVEST and ARIES use different modeling approaches and ecosystem services metrics; for carbon, metrics were more similar and results were more easily comparable than for viewsheds or water. However, findings demonstrate similar gains and losses of ecosystem services and conclusions when comparing effects across our scenarios. Results were more closely aligned for landscape-scale urban-growth scenarios and more divergent for a site-scale mesquite-management scenario. Follow-up studies, including testing in different geographic contexts, can improve our understanding of the strengths and weaknesses of these and other ecosystem services modeling tools as they move closer to readiness for supporting day-to-day resource management. © 2013.","Artificial Intelligence for Ecosystem Services (ARIES); Ecosystem services; Integrated Valuation of Ecosystem Services and Tradeoffs (InVEST); Modeling; Riparian; Semiarid","Prosopis",Article,Scopus,2-s2.0-84884590260
"Marcos M., Maldonado J.A., Martínez-Salvador B., Boscá D., Robles M.","Interoperability of clinical decision-support systems and electronic health records using archetypes: A case study in clinical trial eligibility",2013,"Journal of Biomedical Informatics",31,10.1016/j.jbi.2013.05.004,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880324881&doi=10.1016%2fj.jbi.2013.05.004&partnerID=40&md5=f71defe411917f57fb568c74197234db","Clinical decision-support systems (CDSSs) comprise systems as diverse as sophisticated platforms to store and manage clinical data, tools to alert clinicians of problematic situations, or decision-making tools to assist clinicians. Irrespective of the kind of decision-support task CDSSs should be smoothly integrated within the clinical information system, interacting with other components, in particular with the electronic health record (EHR). However, despite decades of developments, most CDSSs lack interoperability features.We deal with the interoperability problem of CDSSs and EHRs by exploiting the dual-model methodology. This methodology distinguishes a reference model and archetypes. A reference model is represented by a stable and small object-oriented model that describes the generic properties of health record information. For their part, archetypes are reusable and domain-specific definitions of clinical concepts in the form of structured and constrained combinations of the entities of the reference model. We rely on archetypes to make the CDSS compatible with EHRs from different institutions. Concretely, we use archetypes for modelling the clinical concepts that the CDSS requires, in conjunction with a series of knowledge-intensive mappings relating the archetypes to the data sources (EHR and/or other archetypes) they depend on.We introduce a comprehensive approach, including a set of tools as well as methodological guidelines, to deal with the interoperability of CDSSs and EHRs based on archetypes. Archetypes are used to build a conceptual layer of the kind of a virtual health record (VHR) over the EHR whose contents need to be integrated and used in the CDSS, associating them with structural and terminology-based semantics. Subsequently, the archetypes are mapped to the EHR by means of an expressive mapping language and specific-purpose tools. We also describe a case study where the tools and methodology have been employed in a CDSS to support patient recruitment in the framework of a clinical trial for colorectal cancer screening.The utilisation of archetypes not only has proved satisfactory to achieve interoperability between CDSSs and EHRs but also offers various advantages, in particular from a data model perspective. First, the VHR/data models we work with are of a high level of abstraction and can incorporate semantic descriptions. Second, archetypes can potentially deal with different EHR architectures, due to their deliberate independence of the reference model. Third, the archetype instances we obtain are valid instances of the underlying reference model, which would enable e.g. feeding back the EHR with data derived by abstraction mechanisms. Lastly, the medical and technical validity of archetype models would be assured, since in principle clinicians should be the main actors in their development. © 2013 Elsevier Inc.","Clinical decision support systems; Clinical trials; Electronic health records; SNOMED CT; Systems integration; Terminology","Clinical decision support systems; Clinical trial; Electronic health record; SNOMED-CT; Systems integration; Artificial intelligence; Computerized tomography; Experiments; Information management; Interoperability; Medical applications; Medical information systems; Models; Query languages; Records management; Semantics; Terminology; Tools; Decision support systems; article; blood pressure measurement; Charlson Comorbidity Index; clinical trial (topic); colorectal cancer; decision support system; electronic medical record; hospital planning; human; medical documentation; medical information system; occult blood test; practice guideline; priority journal; Systematized Nomenclature of Medicine; Clinical decision support systems; Clinical trials; Electronic health records; SNOMED CT; Systems integration; Terminology; Clinical Trials as Topic; Colorectal Neoplasms; Decision Support Systems, Clinical; Electronic Health Records; Humans; Medical Record Linkage",Article,Scopus,2-s2.0-84880324881
"Demesouka O.E., Vavatsikos A.P., Anagnostopoulos K.P.","Suitability analysis for siting MSW landfills and its multicriteria spatial decision support system: Method, implementation and case study",2013,"Waste Management",31,10.1016/j.wasman.2013.01.030,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877614126&doi=10.1016%2fj.wasman.2013.01.030&partnerID=40&md5=70db2015622badb3c3246254bda2e08f","Multicriteria spatial decision support systems (MC-SDSS) have emerged as an integration of geographical information systems (GIS) and multiple criteria decision analysis (MCDA) methods for incorporating conflicting objectives and decision makers' (DMs') preferences into spatial decision models. This article presents a raster-based MC-SDSS that combines the analytic hierarchy process (AHP) and compromise programming methods, such as TOPSIS (technique for order preference by similarity to the ideal solution) and Ideal Point Methods. To the best of our knowledge it is the first time that a synergy of AHP and compromise programming methods is implemented in raster-driven GIS-based landfill suitability analysis. This procedure is supported by a spatial decision support system (SDSS) that was developed within a widely used commercial GIS software package. A real case study in the Thrace region in northeast Greece serves as a guide on how to conduct a suitability analysis for a MSW landfill site with the proposed MC-SDSS. Moreover, the procedure for identifying MSW disposal sites is accomplished by performing four computational models for synthesizing the DMs per criterion preferential system. Based on the case study results, a comparison analysis is performed according to suitability index estimations. According to them Euclidean distance metric and TOPSIS present strong similarities. When compared with Euclidean distance metric, TOPSIS seems to generate results closer to that derived by Manhattan distance metric. The comparison of Chebychev distance metric with all the other approaches revealed the greatest deviations. © 2013 Elsevier Ltd.","Analytic hierarchy process (AHP); Compromise programming methods; Geographical information systems (GIS); MC-SDSS; Municipal solid waste (MSW) landfills; Site selection","Analytic hierarchy process (ahp); Compromise programming; Conflicting objectives; MC-SDSS; Multi-criteria Spatial Decision Support Systems; Multiple criteria decision analysis; Municipal solid waste landfills; Spatial decision support systems; Analytic hierarchy process; Artificial intelligence; Decision support systems; Geographic information systems; Hierarchical systems; Operations research; Research; Site selection; Urban planning; Land fill; analytical hierarchy process; decision support system; GIS; landfill; multicriteria analysis; municipal solid waste; software; spatial analysis; waste disposal; waste management; article; calculation; decision support system; environmental factor; environmental monitoring; environmental protection; environmental temperature; geography; Greece; land use; landfill; mathematical computing; municipal solid waste; priority journal; socioeconomics; soil texture; water supply; wetland; Decision Support Systems, Management; Expert Systems; Geographic Information Systems; Greece; Refuse Disposal; Solid Waste; Greece; Thrace",Article,Scopus,2-s2.0-84877614126
"Savitha R., Suresh S., Sundararajan N.","Projection-based fast learning fully complex-valued relaxation neural network",2013,"IEEE Transactions on Neural Networks and Learning Systems",31,10.1109/TNNLS.2012.2235460,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875891295&doi=10.1109%2fTNNLS.2012.2235460&partnerID=40&md5=a5ee70d69f904fcb8d1952d9420d0607","This paper presents a fully complex-valued relaxation network (FCRN) with its projection-based learning algorithm. The FCRN is a single hidden layer network with a Gaussian-like sech activation function in the hidden layer and an exponential activation function in the output layer. For a given number of hidden neurons, the input weights are assigned randomly and the output weights are estimated by minimizing a nonlinear logarithmic function (called as an energy function) which explicitly contains both the magnitude and phase errors. A projection-based learning algorithm determines the optimal output weights corresponding to the minima of the energy function by converting the nonlinear programming problem into that of solving a set of simultaneous linear algebraic equations. The resultant FCRN approximates the desired output more accurately with a lower computational effort. The classification ability of FCRN is evaluated using a set of real-valued benchmark classification problems from the University of California, Irvine machine learning repository. Here, a circular transformation is used to transform the real-valued input features to the complex domain. Next, the FCRN is used to solve three practical problems: a quadrature amplitude modulation channel equalization, an adaptive beamforming, and a mammogram classification. Performance results from this paper clearly indicate the superior classification/approximation performance of the FCRN. © 2012 IEEE.","Adaptive beamforming; classification; complex-valued neural network; energy function; quadrature amplitude modulation (QAM)","Adaptive Beamforming; Complex-valued neural networks; Energy functions; Fully complex-valued relaxation networks; Machine learning repository; Mammogram classifications; Nonlinear programming problem; Quadrature-amplitude modulations (QAM); Beamforming; Classification (of information); Complex networks; Learning algorithms; Linear equations; Network layers; Neural networks; Quadrature amplitude modulation; Problem solving; algorithm; artificial intelligence; artificial neural network; time; trends; Algorithms; Artificial Intelligence; Neural Networks (Computer); Time Factors",Article,Scopus,2-s2.0-84875891295
"Liu X., Wang L., Yin J., Zhu E., Zhang J.","An efficient approach to integrating radius information into multiple kernel learning",2013,"IEEE Transactions on Cybernetics",31,10.1109/TSMCB.2012.2212243,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890433386&doi=10.1109%2fTSMCB.2012.2212243&partnerID=40&md5=270e33d1af50764df28615342cdc0143","Integrating radius information has been demonstrated by recent work on multiple kernel learning (MKL) as a promising way to improve kernel learning performance. Directly integrating the radius of the minimum enclosing ball (MEB) into MKL as it is, however, not only incurs significant computational overhead but also possibly adversely affects the kernel learning performance due to the notorious sensitivity of this radius to outliers. Inspired by the relationship between the radius of the MEB and the trace of total data scattering matrix, this paper proposes to incorporate the latter into MKL to improve the situation. In particular, in order to well justify the incorporation of radius information, we strictly comply with the radius-margin bound of support vector machines (SVMs) and thus focus on the ℓ2-norm soft-margin SVM classifier. Detailed theoretical analysis is conducted to show how the proposed approach effectively preserves the merits of incorporating the radius of the MEB and how the resulting optimization is efficiently solved. Moreover, the proposed approach achieves the following advantages over its counterparts: 1) more robust in the presence of outliers or noisy training samples; 2) more computationally efficient by avoiding the quadratic optimization for computing the radius at each iteration; and 3) readily solvable by the existing off-the-shelf MKL packages. Comprehensive experiments are conducted on University of California, Irvine, protein subcellular localization, and Caltech-101 data sets, and the results well demonstrate the effectiveness and efficiency of our approach. © 2012 IEEE.","Class separability measure; Enclosing minimum ball; Kernel methods; Multiple kernel learning (MKL); Radius-margin bound; Support vector machines (SVMs)","Class separability measure; Enclosing minimum ball; Kernel methods; Multiple Kernel Learning; Radius-margin bound; Support vector machine (SVMs); Iterative methods; Optimization; Scattering parameters; Statistics; Support vector machines; algorithm; artificial intelligence; machine learning; protein database; support vector machine; theoretical model; Algorithms; Artificial Intelligence; Databases, Protein; Machine Learning; Models, Theoretical; Support Vector Machine",Article,Scopus,2-s2.0-84890433386
"Toews M., Wells W.M.","Efficient and robust model-to-image alignment using 3D scale-invariant features",2013,"Medical Image Analysis",31,10.1016/j.media.2012.11.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875259656&doi=10.1016%2fj.media.2012.11.002&partnerID=40&md5=c3dd1af5cf9998628605244ef2e43936","This paper presents feature-based alignment (FBA), a general method for efficient and robust model-to-image alignment. Volumetric images, e.g. CT scans of the human body, are modeled probabilistically as a collage of 3D scale-invariant image features within a normalized reference space. Features are incorporated as a latent random variable and marginalized out in computing a maximum a posteriori alignment solution. The model is learned from features extracted in pre-aligned training images, then fit to features extracted from a new image to identify a globally optimal locally linear alignment solution. Novel techniques are presented for determining local feature orientation and efficiently encoding feature intensity in 3D. Experiments involving difficult magnetic resonance (MR) images of the human brain demonstrate FBA achieves alignment accuracy similar to widely-used registration methods, while requiring a fraction of the memory and computation resources and offering a more robust, globally optimal solution. Experiments on CT human body scans demonstrate FBA as an effective system for automatic human body alignment where other alignment methods break down. © 2012 Elsevier B.V.","3D scale-invariant feature; Feature descriptor; Image alignment; Orientation assignment; Probabilistic model","Alignment accuracy; Computation resources; Feature descriptors; Image alignment; Maximum a posteriori; Probabilistic models; Registration methods; Scale-invariant; Alignment; Computerized tomography; Experiments; Magnetic resonance; Three dimensional computer graphics; Three dimensional; accuracy; article; brain; feature based alignment; human; image analysis; image reconstruction; model to image alignment; nuclear magnetic resonance imaging; priority journal; three dimensional imaging; volumetry; Algorithms; Artificial Intelligence; Computer Simulation; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Models, Anatomic; Models, Biological; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-84875259656
"Taher S.A., Bagherpour R.","A new approach for optimal capacitor placement and sizing in unbalanced distorted distribution systems using hybrid honey bee colony algorithm",2013,"International Journal of Electrical Power and Energy Systems",31,10.1016/j.ijepes.2013.02.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875129273&doi=10.1016%2fj.ijepes.2013.02.003&partnerID=40&md5=7ca46903b0ca9ae033657512babcc41f","Despite increase in application of distributed generations (DGs) in power systems in recent years and their positive impacts, DG's are likely to amplify the harmonic distortion level. Moreover, placing shunt capacitors in distribution systems without considering harmonics may increase harmonic pollution of distribution systems due to imposed resonance between shunt capacitors and inductive elements of the power system. In addition, distribution systems are inherently unbalanced due to their different loads. Hence, in case shunt capacitors are not optimally placed, system unbalance and harmonic level may increase. In this article, an attempt was made to achieve optimal capacitor placement using a hybrid honey bee colony (HHBC) optimization algorithm aiming to minimize power system losses and unbalances and maximize the ensuing net saving while maintaining voltage and total harmonic distortion (THD) of buses in an acceptable range according to IEEE standards. To include the presence of harmonics, the method was integrated with a three-phase harmonic power flow algorithm. Having defined a special fitness function, effects of power system unbalances and harmonics, were taken into account. The proposed method were tested on two unbalanced distorted radial distribution test systems (including an IEEE 25-bus and a modified IEEE 37-bus) with and without the presence of DGs. Simulation results have been compared with those from standard artificial bee colony (ABC) algorithm and other such artificial intelligence techniques as genetic algorithm (GA), particle swarm optimization (PSO), and imperialist competitive algorithm (ICA). These results suggest that the proposed methodology offer acceptable numerical accuracy, robustness and efficiency and demonstrate that the proposed approach may outperform other artificial intelligence techniques. © 2013 Elsevier Ltd. All rights reserved.","Bee colony; Harmonic unbalanced distorted distribution system; Hybrid honey bee colony algorithm; Optimal location; Shunt capacitors","Bee colony; Distribution systems; Honey bee; Optimal locations; Shunt capacitors; Artificial intelligence; Capacitors; Distributed power generation; Food products; Genetic algorithms; Local area networks; Particle swarm optimization (PSO); Harmonic analysis",Article,Scopus,2-s2.0-84875129273
"Bauer A., Jaulmes E., Prouff E., Wild J.","Horizontal and vertical side-channel attacks against secure RSA implementations",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",31,10.1007/978-3-642-36095-4_1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874302268&doi=10.1007%2f978-3-642-36095-4_1&partnerID=40&md5=63b33b3a81aab139d2f375c838d71ee4","Since the introduction of side-channel attacks in the nineties, RSA implementations have been a privileged target. A wide variety of countermeasures have been proposed and most of practical attacks are nowadays efficiently defeated by them. However, in a recent work published at ICICS 2010, Clavier et al.have pointed out that almost all the existing countermeasures were ineffective if the attacks are performed with a modus operandi called Horizontal. Such attacks, originally introduced by Colin Walter at CHES 2001, involve a single observation trace contrary to the classical attacks where several ones are required. To defeat Horizontal attacks, the authors of the ICICS paper have proposed a set of new countermeasures. In this paper, we introduce a general framework enabling to model both Horizontal and classical attacks (called Vertical) in a simple way. This framework enables to enlighten the similarities and the differences of those attack types. From this formalism, we show that even if Clavier et al.'s countermeasures thwart existing attacks, they do not fully solve the leakage issue. Actually, flaws are exhibited in this paper and efficient attacks are devised. We eventually propose a new countermeasure. © 2013 Springer-Verlag.",,"Modus operandi; Side channel attack; Artificial intelligence; Cryptography",Conference Paper,Scopus,2-s2.0-84874302268
"Merrifield M.S., McClintock W., Burt C., Fox E., Serpa P., Steinback C., Gleason M.","MarineMap: A web-based platform for collaborative marine protected area planning",2013,"Ocean and Coastal Management",31,10.1016/j.ocecoaman.2012.06.011,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875242430&doi=10.1016%2fj.ocecoaman.2012.06.011&partnerID=40&md5=9eb5ef2220e956b6802b08f5588fb9b7","The creation of a spatial decision support system that enabled stakeholder participation in designing marine protected areas (MPAs) was a necessary component of the planning process for California's Marine Life Protection Act (MLPA) Initiative. Implementation of the MLPA required stakeholders to understand and use a complicated set of spatial and scientific guidelines for MPA design that relied heavily on geographic information. Tools for the delivery and visualization of geographic information have changed radically in the seven years of planning during the MLPA Initiative. We collaborated to create a state-of-the-art spatial decision support system called MarineMap to facilitate the design and evaluation of MPA proposals. MarineMap provided an intuitive user experience that simplified complicated spatial concepts, delivered critical information immediately to allow users to iterate through scenarios rapidly. This tool provided transparency in the process, and moved spatial analysis away from the domain of GIS experts to a broader group of stakeholders. We think the lessons from this experience can contribute to the further development of tools and approaches for coastal and marine spatial planning more broadly. © 2012 Elsevier Ltd.",,"Critical information; Design and evaluations; Geographic information; Marine life protection acts; Marine protected area; Marine Spatial Planning; Spatial decision support systems; Stakeholder participation; Artificial intelligence; Conservation; Decision support systems; Geographic information systems; Regional planning; Environmental protection; decision making; GIS; implementation process; marine park; planning process; spatial analysis; spatial planning; stakeholder; visualization; California; United States",Article,Scopus,2-s2.0-84875242430
"Roggen D., Tröster G., Lukowicz P., Ferscha A., Del R. Millán J., Chavarriaga R.","Opportunistic human activity and context recognition",2013,"Computer",31,10.1109/MC.2012.393,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873646486&doi=10.1109%2fMC.2012.393&partnerID=40&md5=58adb9baf08cfadb90c9435b1d92ddcc","Achieving true ambient intelligence calls for a new opportunistic activity recognition paradigm in which, instead of deploying information sources for a specific goal, the recognition methods themselves dynamically adapt to available sensor data. The Web extra at http://youtu.be/dMN3Hoq5QfU is a video recording of opportunistic sensor configurations in a highly rich sensor environment that shows how a large number of activity instances occur during early morning activities in an apartment. © 1970-2012 IEEE.","ambient intelligence; context awareness; Internet of Things; IoT; opportunistic activity recognition; Opportunity Framework; pattern recognition; pervasive computing; ubiquitous computing; wearable AI; wireless sensor networks","Activity recognition; Ambient intelligence; Context- awareness; Internet of Things (IOT); IoT; Opportunity Framework; Artificial intelligence; Pattern recognition; Sensors; Video recording; Wireless sensor networks; Ubiquitous computing",Article,Scopus,2-s2.0-84873646486
"Liu P., Wang Y., Huang D., Zhang Z., Chen L.","Learning the spherical harmonic features for 3-D face recognition",2013,"IEEE Transactions on Image Processing",31,10.1109/TIP.2012.2222897,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873305344&doi=10.1109%2fTIP.2012.2222897&partnerID=40&md5=6a8c7bc9c7d33565666458b4bc989bd5","In this paper, a competitive method for 3-D face recognition (FR) using spherical harmonic features (SHF) is proposed. With this solution, 3-D face models are characterized by the energies contained in spherical harmonics with different frequencies, thereby enabling the capture of both gross shape and fine surface details of a 3-D facial surface. This is in clear contrast to most 3-D FR techniques which are either holistic or feature based, using local features extracted from distinctive points. First, 3-D face models are represented in a canonical representation, namely, spherical depth map, by which SHF can be calculated. Then, considering the predictive contribution of each SHF feature, especially in the presence of facial expression and occlusion, feature selection methods are used to improve the predictive performance and provide faster and more cost-effective predictors. Experiments have been carried out on three public 3-D face datasets, SHREC2007, FRGC v2.0, and Bosphorus, with increasing difficulties in terms of facial expression, pose, and occlusion, and which demonstrate the effectiveness of the proposed method. © 1992-2012 IEEE.","3-D face recognition; feature selection; spherical depth map; spherical harmonics","3D face recognition; 3D faces; Canonical representations; Data sets; Depth Map; Different frequency; Facial Expressions; Facial surfaces; Feature selection methods; Feature-based; Local feature; Predictive performance; Spherical harmonics; Surface details; Feature extraction; Harmonic analysis; Three dimensional; Face recognition; algorithm; article; artificial intelligence; automated pattern recognition; biometry; computer assisted diagnosis; face; histology; human; image enhancement; image subtraction; methodology; photography; reproducibility; sensitivity and specificity; Algorithms; Artificial Intelligence; Biometry; Face; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Photography; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-84873305344
"Xu D., Zhang Y.","Toward optimal fragment generations for ab initio protein structure assembly",2013,"Proteins: Structure, Function and Bioinformatics",31,10.1002/prot.24179,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872600524&doi=10.1002%2fprot.24179&partnerID=40&md5=e00e68d22e67f21d8b20ce9241fdb576","Fragment assembly using structural motifs excised from other solved proteins has shown to be an efficient method for ab initio protein-structure prediction. However, how to construct accurate fragments, how to derive optimal restraints from fragments, and what the best fragment length is are the basic issues yet to be systematically examined. In this work, we developed a gapless-threading method to generate position-specific structure fragments. Distance profiles and torsion angle pairs are then derived from the fragments by statistical consistency analysis, which achieved comparable accuracy with the machine-learning-based methods although the fragments were taken from unrelated proteins. When measured by both accuracies of the derived distance profiles and torsion angle pairs, we come to a consistent conclusion that the optimal fragment length for structural assembly is around 10, and at least 100 fragments at each location are needed to achieve optimal structure assembly. The distant profiles and torsion angle pairs as derived by the fragments have been successfully used in QUARK for ab initio protein structure assembly and are provided by the QUARK online server at http://zhanglab.ccmb. med.umich.edu/QUARK/. © 2012 Wiley Periodicals, Inc.","ab initio folding; Contact prediction; Secondary structure prediction; Structural fragments","ab initio calculation; accuracy; article; machine learning; priority journal; protein structure; restriction fragment; torque; Artificial Intelligence; Caspase 9; Databases, Protein; Models, Molecular; Peptide Fragments; Protein Conformation; Protein Folding; Proteins",Article,Scopus,2-s2.0-84872600524
"Fukushima K.","Artificial vision by multi-layered neural networks: Neocognitron and its advances",2013,"Neural Networks",31,10.1016/j.neunet.2012.09.016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870443413&doi=10.1016%2fj.neunet.2012.09.016&partnerID=40&md5=80aca28d381dd42c7d7b70a0e6ec0dc1","The neocognitron is a neural network model proposed by. Fukushima (1980). Its architecture was suggested by neurophysiological findings on the visual systems of mammals. It is a hierarchical multi-layered network. It acquires the ability to robustly recognize visual patterns through learning. Although the neocognitron has a long history, modifications of the network to improve its performance are still going on. For example, a recent neocognitron uses a new learning rule, named add-if-silent, which makes the learning process much simpler and more stable. Nevertheless, a high recognition rate can be kept with a smaller scale of the network. Referring to the history of the neocognitron, this paper discusses recent advances in the neocognitron. We also show that various new functions can be realized by, for example, introducing top-down connections to the neocognitron: mechanism of selective attention, recognition and completion of partly occluded patterns, restoring occluded contours, and so on. © 2012 Elsevier Ltd.","Artificial vision; Bottom-up and top-down; Hierarchical network; Modeling neural networks; Neocognitron","Bottom-up and top-down; Hierarchical network; ITS architecture; Learning process; Learning rules; Multi-layered; Multilayered neural networks; Neocognitron; Neural network model; Recognition rates; Selective attention; Topdown; Visual pattern; Visual systems; Computer vision; Network layers; Vision; Feedforward neural networks; article; cognition; controlled study; learning; mental performance; nerve cell network; neurocognition; neuromodulation; neurophysiology; priority journal; recognition; selective attention; visual discrimination; artificial neural network; intermethod comparison; learning algorithm; neocognitron; pattern recognition; visual information; visual orientation; visual system; animal; artificial intelligence; attention; human; mammal; pattern recognition; physiology; vision; visual system; Animals; Artificial Intelligence; Attention; Cognition; Humans; Learning; Mammals; Neural Networks (Computer); Pattern Recognition, Visual; Vision, Ocular; Visual Pathways; Visual Perception",Article,Scopus,2-s2.0-84870443413
"Semet Y., Lutton E., Collet P.","Ant colony optimisation for E-learning: Observing the emergence of pedagogic suggestions",2013,"2003 IEEE Swarm Intelligence Symposium, SIS 2003 - Proceedings",31,10.1109/SIS.2003.1202246,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942092842&doi=10.1109%2fSIS.2003.1202246&partnerID=40&md5=6abe2f8ed49afd76431147760484bfda","An attempt is made to apply ant colony optimization (ACO) heuristics to an E-learning problem: the pedagogic material of an online teaching Web site for high school students is modelled as a navigation graph where nodes are exercises or lessons and arcs are hypertext links. The arcs' valuation, representing the pedagogic structure and conditioning the Web site's presentation, is gradually modified through the release and evaporation of virtual pheromones that reflect the successes and failures of students roaming around the graph. A compromise is expected to emerge between the pedagogic structure as originally dictated by professors, the collective experience of the whole pool of students and the particularities of each individual. The purpose of this study conducted for Paraschool, the leading French E-learning company is twofold: enhancing the Web site by making its presentation intelligently dynamic and providing the pedagogical team with a refined auditing tool that could help it identify the strengths and weaknesses of its pedagogic choices. © 2003 IEEE.","Ant colony optimization; Artificial intelligence; Cost accounting; Education; Educational institutions; Electronic learning; Evolutionary computation; Fractals; Navigation; Software algorithms","Algorithms; Artificial intelligence; Cost accounting; E-learning; Education; Evolutionary algorithms; Fractals; Hypertext systems; Navigation; Optimization; Social networking (online); Students; Teaching; Websites; Ant colony optimisation; Ant Colony Optimization (ACO); Educational institutions; Electronic learning; High school students; Navigation graphs; Software algorithms; Virtual pheromones; Ant colony optimization",Conference Paper,Scopus,2-s2.0-84942092842
"Lua C.A., Altenburg K., Nygard K.E.","Synchronized multi-point attack by autonomous reactive vehicles with simple local communication",2013,"2003 IEEE Swarm Intelligence Symposium, SIS 2003 - Proceedings",31,10.1109/SIS.2003.1202253,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942133100&doi=10.1109%2fSIS.2003.1202253&partnerID=40&md5=c770b67afa4963af89b9518af0d2e493","We present a model consisting of a swarm of unmanned, autonomous flying munitions to conduct a synchronized multi-point attack on a target. The unpiloted air vehicles (UAV) lack global communication or extensive battlefield intelligence, instead, relying on passive short-range sensors and simple, inter-agent communication. The multi-point synchronized attack is successfully demonstrated in a simulated battlefield environment. The simulation results indicate that the reactive, synchronized, multi-point attack is effective, robust and scalable. It is especially well suited for numerous, small, inexpensive, and expendable UAV. © 2003 IEEE.","Communication system control; Global communication; Intelligent sensors; Intelligent vehicles; Mobile robots; Remotely operated vehicles; Robot sensing systems; Synchronization; Unmanned aerial vehicles; Weapons","Aircraft control; Amphibious vehicles; Artificial intelligence; Fighter aircraft; Intelligent robots; Intelligent vehicle highway systems; Mobile robots; Ordnance; Synchronization; Unmanned aerial vehicles (UAV); Vehicles; Battlefield environments; Communication system control; Global communication; Intelligent sensors; Inter-agent communications; Local communications; Robot sensing system; Synchronized attacks; Remotely operated vehicles",Conference Paper,Scopus,2-s2.0-84942133100
"Zang Q., Rotroff D.M., Judson R.S.","Binary classification of a large collection of environmental chemicals from estrogen receptor assays by quantitative structure-activity relationship and machine learning methods",2013,"Journal of Chemical Information and Modeling",30,10.1021/ci400527b,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896504827&doi=10.1021%2fci400527b&partnerID=40&md5=6e68170f655954c7238b4c398f1cede4","There are thousands of environmental chemicals subject to regulatory decisions for endocrine disrupting potential. The ToxCast and Tox21 programs have tested ∼8200 chemicals in a broad screening panel of in vitro high-throughput screening (HTS) assays for estrogen receptor (ER) agonist and antagonist activity. The present work uses this large data set to develop in silico quantitative structure-activity relationship (QSAR) models using machine learning (ML) methods and a novel approach to manage the imbalanced data distribution. Training compounds from the ToxCast project were categorized as active or inactive (binding or nonbinding) classes based on a composite ER Interaction Score derived from a collection of 13 ER in vitro assays. A total of 1537 chemicals from ToxCast were used to derive and optimize the binary classification models while 5073 additional chemicals from the Tox21 project, evaluated in 2 of the 13 in vitro assays, were used to externally validate the model performance. In order to handle the imbalanced distribution of active and inactive chemicals, we developed a cluster-selection strategy to minimize information loss and increase predictive performance and compared this strategy to three currently popular techniques: cost-sensitive learning, oversampling of the minority class, and undersampling of the majority class. QSAR classification models were built to relate the molecular structures of chemicals to their ER activities using linear discriminant analysis (LDA), classification and regression trees (CART), and support vector machines (SVM) with 51 molecular descriptors from QikProp and 4328 bits of structural fingerprints as explanatory variables. A random forest (RF) feature selection method was employed to extract the structural features most relevant to the ER activity. The best model was obtained using SVM in combination with a subset of descriptors identified from a large set via the RF algorithm, which recognized the active and inactive compounds at the accuracies of 76.1% and 82.8% with a total accuracy of 81.6% on the internal test set and 70.8% on the external test set. These results demonstrate that a combination of high-quality experimental data and ML methods can lead to robust models that achieve excellent predictive accuracy, which are potentially useful for facilitating the virtual screening of chemicals for environmental risk assessment. © 2013 American Chemical Society.",,"Computational chemistry; Decision trees; Mathematical models; Organic pollutants; Risk assessment; Support vector machines; Toxic materials; Classification and regression tree; Environmental risk assessment; Feature selection methods; High-throughput screening; Linear discriminant analysis; Machine learning methods; Quantitative structure activity relationship; Quantitative structure-activity relationships; Chemicals; endocrine disruptor; estrogen receptor; algorithm; article; artificial intelligence; classification; discriminant analysis; drug antagonism; drug potentiation; environmental monitoring; high throughput screening; human; metabolism; quantitative structure activity relation; risk assessment; water pollutant; Algorithms; Artificial Intelligence; Discriminant Analysis; Endocrine Disruptors; Environmental Monitoring; High-Throughput Screening Assays; Humans; Quantitative Structure-Activity Relationship; Receptors, Estrogen; Risk Assessment; Water Pollutants, Chemical",Article,Scopus,2-s2.0-84896504827
"Elahi M., Braunhofer M., Ricci F., Tkalcic M.","Personality-based active learning for collaborative filtering recommender systems",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",30,10.1007/978-3-319-03524-6_31,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892755296&doi=10.1007%2f978-3-319-03524-6_31&partnerID=40&md5=8f74ef5c7a853afa9ea1d23a8631d000","Recommender systems (RSs) suffer from the cold-start or new user/item problem, i.e., the impossibility to provide a new user with accurate recommendations or to recommend new items. Active learning (AL) addresses this problem by actively selecting items to be presented to the user in order to acquire her ratings and hence improve the output of the RS. In this paper, we propose a novel AL approach that exploits the user's personality - using the Five Factor Model (FFM) - in order to identify the items that the user is requested to rate. We have evaluated our approach in a user study by integrating it into a mobile, context-aware RS that provides users with recommendations for places of interest (POIs). We show that the proposed AL approach significantly increases the number of ratings acquired from the user and the recommendation accuracy. © Springer International Publishing Switzerland 2013.",,"Active Learning; Cold-start; Collaborative filtering recommender systems; Context-Aware; Five-Factor Model; Recommendation accuracy; User study; Artificial intelligence; Recommender systems",Conference Paper,Scopus,2-s2.0-84892755296
"Quan D.M., Ogliari E., Grimaccia F., Leva S., Mussetta M.","Hybrid model for hourly forecast of photovoltaic and wind power",2013,"IEEE International Conference on Fuzzy Systems",30,10.1109/FUZZ-IEEE.2013.6622453,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887842350&doi=10.1109%2fFUZZ-IEEE.2013.6622453&partnerID=40&md5=e134e457f2e6f5f40f43a0a3db5442be","High penetration of solar and wind power in the electricity system provides a number of challenges to the grid such as grid stability and security, system operation, and market economics. Ones of the considerable problems of solar and wind systems, they depend on the weather, as compared to the conventional generation. As we know, the balance in managing load and generated power in energy system is very important. If the power which is supplied from solar and wind perfectly predictable, the extra cost of operating power system with a large penetration of renewable energy will be reduced. Since, the accurate and reliable forecasting system for renewable sources represents an important topic as a major contribution for increasing non-programmable renewable on over the world. The target of this research is to describes the advanced hybrid evolutionary techniques of computational intelligence applied for PV as well as wind power forecast. The evaluation of this investigation is obtained by comparing different definitions of the forecasting error. Moreover, the meaning of NWP (numerical weather prediction) values based on meteorological information on solar and wind power forecasting at Italy has been highlighted in this research. © 2013 IEEE.","Fuzzy; Hybrid techniques; Neural networks; Pv forecasting; Wind power","Conventional generation; Evolutionary techniques; Forecasting system; Fuzzy; Hybrid techniques; Meteorological information; Numerical weather prediction; Wind power forecast; Artificial intelligence; Electric power transmission networks; Fuzzy systems; Neural networks; Wind power; Weather forecasting",Conference Paper,Scopus,2-s2.0-84887842350
"Bedregal B., Dimuro G.P., Bustince H., Barrenechea E.","New results on overlap and grouping functions",2013,"Information Sciences",30,10.1016/j.ins.2013.05.004,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883178390&doi=10.1016%2fj.ins.2013.05.004&partnerID=40&md5=85022593b78357981d6d148af3cc60b9","Overlap functions and grouping functions are special kinds of aggregation operators that have been recently proposed for applications in classification problems, like, e.g., imaging processing. Overlap and grouping functions can also be applied in decision making based on fuzzy preference relations, where the associativity property is not strongly required and the use of t-norms or t-conorms as the combination/separation operators is not necessary. The concepts of indifference and incomparability defined in terms of overlap and grouping functions may allow the application in several different contexts. This paper introduces new interesting results related to overlap and grouping functions, investigating important properties, such as migrativity, homogeneity, idempotency and the existence of generators. De Morgan triples are introduced in order to study the relationship between those dual concepts. In particular, we introduce important results related to the action of automorphisms on overlap and grouping functions, analyzing the preservation of those properties and also the Lipschitzianity condition. © 2013 Elsevier Inc. All rights reserved.","Grouping function; Homogeneity; Idempotency Automorphism; Migrativity; Overlap function","Aggregation operator; Automorphisms; De Morgan triple; Fuzzy preference relations; Homogeneity; Imaging processing; Migrativity; Overlap functions; Artificial intelligence; Software engineering; Mathematical operators",Article,Scopus,2-s2.0-84883178390
"Convertino V.A., Grudic G., Mulligan J., Moulton S.","Estimation of individual-specific progression to impending cardiovascular instability using arterial waveforms",2013,"Journal of Applied Physiology",30,10.1152/japplphysiol.00668.2013,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886070780&doi=10.1152%2fjapplphysiol.00668.2013&partnerID=40&md5=97e3ff62fe4e8806d8e68bd95f0e390a","Trauma patients with ""compensated"" internal hemorrhage may not be identified with standard medical monitors until signs of shock appear, at which point it may be difficult or too late to pursue life-saving interventions. We tested the hypothesis that a novel machine-learning model called the compensatory reserve index (CRI) could differentiate tolerance to acute volume loss of individuals well in advance of changes in stroke volume (SV) or standard vital signs. Two hundred one healthy humans underwent progressive lower body negative pressure (LBNP) until the onset of hemodynamic instability (decompensation). Continuously measured photoplethysmogram signals were used to estimate SV and develop a model for estimating CRI. Validation of the CRI was tested on 101 subjects who were classified into two groups: low tolerance (LT; n = 33) and high tolerance (HT; n = 68) to LBNP (mean LBNP time: LT = 16.23 min vs. HT = 25.86 min). On an arbitrary scale of 1 to 0, the LT group CRI reached 0.6 at an average time of 5.27 ± 1.18 (95% confidence interval) min followed by 0.3 at 11.39 ± 1.14 min. In comparison, the HT group reached CRI of 0.6 at 7.62 ± 0.94 min followed by 0.3 at 15.35 ± 1.03 min. Changes in heart rate, blood pressure, and SV did not differentiate HT from LT groups. Machine modeling of the photoplethysmogram response to reduced central blood volume can accurately trend individual-specific progression to hemodynamic decompensation. These findings foretell early identification of blood loss, anticipating hemodynamic instability, and timely application of life-saving interventions. Copyright © 2013 the American Physiological Society.",,"adult; algorithm; arterial pressure; artery; article; artificial intelligence; biological model; bleeding; blood pressure; blood volume; disease course; early diagnosis; evaluation study; female; finger; heart rate; heart stroke volume; hemodynamics; hemorrhagic shock; human; lower body negative pressure; male; methodology; model; pathophysiology; photoelectric plethysmography; physiologic monitoring; predictive value; prognosis; reproducibility; shock; signal processing; time; vascularization; vital sign; blood pressure; hemorrhage; modeling; shock; stroke volume; Adult; Algorithms; Arterial Pressure; Arteries; Artificial Intelligence; Blood Volume; Disease Progression; Early Diagnosis; Female; Fingers; Heart Rate; Hemodynamics; Humans; Lower Body Negative Pressure; Male; Models, Cardiovascular; Monitoring, Physiologic; Photoplethysmography; Predictive Value of Tests; Prognosis; Reproducibility of Results; Shock, Hemorrhagic; Signal Processing, Computer-Assisted; Stroke Volume; Time Factors; Vital Signs",Article,Scopus,2-s2.0-84886070780
"Tavenas S.","Improved bounds for reduction to depth 4 and depth 3",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",30,10.1007/978-3-642-40313-2_71,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885196280&doi=10.1007%2f978-3-642-40313-2_71&partnerID=40&md5=463967c99b963b345744d9cc452e8159","Koiran [7] showed that if an n-variate polynomial of degree d (with d = nO(1)) is computed by a circuit of size s, then it is also computed by a homogeneous circuit of depth four and of size 2 O(√d log(d) log(s)). Using this result, Gupta, Kamath, Kayal and Saptharishi [6] gave an exp (O(√d log(d) log(n) log(s))) upper bound for the size of the smallest depth three circuit computing an n-variate polynomial of degree d = nO(1) given by a circuit of size s. We improve here Koiran's bound. Indeed, we show that if we reduce an arithmetic circuit to depth four, then the size becomes exp (O(√d log(ds) log(n))). Mimicking the proof in [6], it also implies the same upper bound for depth three circuits. This new bound is not far from optimal in the sense that Gupta, Kamath, Kayal and Saptharishi [5] also showed a 2Ω(√d) lower bound for the size of homogeneous depth four circuits such that gates at the bottom have fan-in at most √d. Finally, we show that this last lower bound also holds if the fan-in is at least √d. © 2013 Springer-Verlag.",,"Arithmetic circuit; Depth 3; Depth 4; Depth three circuits; Lower bounds; Upper Bound; Artificial intelligence; Computer science",Conference Paper,Scopus,2-s2.0-84885196280
"Prokopowicz P.","Flexible and simple methods of calculations on fuzzy numbers with the ordered fuzzy numbers model",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",30,10.1007/978-3-642-38658-9_33,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884385104&doi=10.1007%2f978-3-642-38658-9_33&partnerID=40&md5=7e9bf78704360fc95c7893cdde6f6f1d","The publication shows the way of implementing arithmetic operations on fuzzy numbers based on Ordered Fuzzy Numbers calculation model [12], [13], [14]. This model allows to perform calculations on fuzzy numbers in a way that the outcomes meet the same criteria as the outcomes of calculations on real numbers. In this text, to the four basic operations with Ordered Fuzzy Numbers, a logarithm and exponentiation was added. Several examples of the calculations are included, the results of which are obvious and typical of real numbers but not achievable with the use of conventional computational methods for fuzzy numbers. From these examples one can see that the use of Ordered Fuzzy Numbers allows to obtain outcomes for real numbers in spite of using the fuzzy values. © 2013 Springer-Verlag.","arithmetic calculations on fuzzy numbers; exponent of fuzzy numbers; fuzzy number; logarithm of fuzzy numbers; Ordered Fuzzy Numbers","Arithmetic operations; Basic operation; Calculation models; Exponentiations; Fuzzy numbers; Ordered fuzzy number; Real number; SIMPLE method; Artificial intelligence; Soft computing; Fuzzy rules",Conference Paper,Scopus,2-s2.0-84884385104
"Veyrat-Charvillon N., Gérard B., Standaert F.-X.","Security evaluations beyond computing power: How to analyze side-channel attacks you cannot mount?",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",30,10.1007/978-3-642-38348-9_8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883417626&doi=10.1007%2f978-3-642-38348-9_8&partnerID=40&md5=5b5c39081bf47cf88212f287fe252c82","Current key sizes for symmetric cryptography are usually required to be at least 80-bit long for short-term protection, and 128-bit long for long-term protection. However, current tools for security evaluations against side-channel attacks do not provide a precise estimation of the remaining key strength after some leakage has been observed, e.g. in terms of number of candidates to test. This leads to an uncomfortable situation, where the security of an implementation can be anywhere between enumerable values (i.e. 2 10-250 key candidates to test) and the full key size (i.e. 260-2128 key candidates to test). In this paper, we propose a solution to this issue, and describe a key rank estimation algorithm that provides tight bounds for the security level of leaking cryptographic devices. As a result and for the first time, we are able to analyze the full complexity of ""standard"" (i.e. divide-and-conquer) side-channel attacks, in terms of their tradeoff between time, data and memory complexity. © 2013 International Association for Cryptologic Research.",,"Computing power; Cryptographic devices; Divide and conquer; Memory complexity; Rank estimations; Security evaluation; Side channel attack; Symmetric cryptography; Artificial intelligence; Computer science; Cryptography",Conference Paper,Scopus,2-s2.0-84883417626
"Stavropoulos T.G., Gottis K., Vrakas D., Vlahavas I.","AWESoME: A web service middleware for ambient intelligence",2013,"Expert Systems with Applications",30,10.1016/j.eswa.2013.01.061,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876033697&doi=10.1016%2fj.eswa.2013.01.061&partnerID=40&md5=17d7a27c93bbbfa63cf91821c12b7045","This work presents a Web Service Middleware infrastructure for Ambient Intelligence environments, named aWESoME. aWESoME is a vital part of the Smart IHU project, a large-scale Smart University deployment. The purpose of the proposed middleware within the project is twofold: for one, to ensure universal, homogeneous access to the system's functions and secondly, to fulfill functional and non-functional requirements of the system. Namely, the infrastructure itself should consume significantly low power (as it is meant for energy savings in addition to automations), without compromising reliability and fast response time. The infrastructure should enable fast and direct discovery, invocation and execution of services. Finally, on hardware level, the wireless sensor and actuator network should be optimally configured for speed and reliability as well. The proposed solution employs widely used web open standards for description and discovery to expose hardware and software functions and ensure interoperability, even outside the borders of this university deployment. It proposes a straightforward method to integrate low-cost and resource-constrained heterogeneous devices found in the market and a large-scale placement of servers and wireless sensor networks. Different server hardware installations have been evaluated to find the optimum trade-off between response time and power consumption. Finally, a range of client applications that exploit the middleware on different platforms are demonstrated, to prove its usability and effectiveness in enabling, in this scenario, energy monitoring and savings. © 2013 Elsevier Ltd. All rights reserved.","Real-time and embedded systems; Ubiquitous computing; Web services; Wireless sensor networks","Ambient intelligence; Hardware and software; Hardware installation; Heterogeneous devices; Non-functional requirements; Real-time and embedded systems; Straight-forward method; Wireless sensor and actuator networks; Artificial intelligence; Hardware; Middleware; Response time (computer systems); Ubiquitous computing; Web services; Websites; Wireless sensor networks; Computer hardware",Article,Scopus,2-s2.0-84876033697
"Gosens J., Lu Y.","From lagging to leading? Technological innovation systems in emerging economies and the case of Chinese wind power",2013,"Energy Policy",30,10.1016/j.enpol.2013.05.027,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880327168&doi=10.1016%2fj.enpol.2013.05.027&partnerID=40&md5=e4a38b69b8116874c328033de8677de6","There is increasing attention for the contribution of emerging economies to global innovation, including innovation of renewable energy technologies. The Technological Innovation Systems (TIS) framework presents a list of system functions for the analysis of the development of a technology. It has often been applied to renewable energy technologies, but with a strong focus on advanced economies. In this paper, we elaborate on emerging economy innovation system formation, structured according to TIS functions. Rather than analysing development of the technology, we analyse the development of the Chinese TIS vis-à-vis the global forefront. Key to this notion is that TIS, especially for clean-tech, are transnational phenomena. Lagging TIS depend on the global TIS, whereas leading TIS contribute to its formation. It is concluded that China has reduced its dependence on foreign knowledge and investment, but the outward contribution remains limited. The main challenge to foreign market expansion lies in reforming the domestic TIS to focus on turbine quality rather than cost reduction. Demonstration projects are needed, especially for large capacity and offshore turbine models, in order to build up operational history and get quality certification. © 2013 Elsevier Ltd.","China; Renewable energy; Technological Innovation System","Advanced economies; China; Demonstration project; Emerging economies; Quality certification; Renewable energies; Renewable energy technologies; Technological innovation systems; Economics; Innovation; Technology; Turbines; Wind power; Artificial intelligence; alternative energy; energy market; innovation; investment; offshore application; power generation; project management; renewable resource; technological development; wind farm; wind power; China",Article,Scopus,2-s2.0-84880327168
"Castelli M., Vanneschi L., Silva S.","Prediction of high performance concrete strength using Genetic Programming with geometric semantic genetic operators",2013,"Expert Systems with Applications",30,10.1016/j.eswa.2013.06.037,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880346886&doi=10.1016%2fj.eswa.2013.06.037&partnerID=40&md5=618165cadd613ec510cf32d3fc7c4b84","Concrete is a composite construction material made primarily with aggregate, cement, and water. In addition to the basic ingredients used in conventional concrete, high-performance concrete incorporates supplementary cementitious materials, such as fly ash and blast furnace slag, and chemical admixture, such as superplasticizer. Hence, high-performance concrete is a highly complex material and modeling its behavior represents a difficult task. In this paper, we propose an intelligent system based on Genetic Programming for the prediction of high-performance concrete strength. The system we propose is called Geometric Semantic Genetic Programming, and it is based on recently defined geometric semantic genetic operators for Genetic Programming. Experimental results show the suitability of the proposed system for the prediction of concrete strength. In particular, the new method provides significantly better results than the ones produced by standard Genetic Programming and other machine learning methods, both on training and on out-of-sample data. © 2013 Elsevier Ltd. All rights reserved.","Artificial intelligence; Genetic Programming; Geometric operators; High performance concrete; Semantics; Strength prediction","Chemical admixture; Complex materials; Composite construction; Conventional concrete; Geometric semantics; Machine learning methods; Strength prediction; Supplementary cementitious material; Artificial intelligence; Blast furnaces; Concrete slabs; Fly ash; Forecasting; Geometry; High performance concrete; Intelligent systems; Learning systems; Semantics; Slags; Genetic programming",Article,Scopus,2-s2.0-84880346886
"Chen S.-M., Manalu G.M.T., Pan J.-S., Liu H.-C.","Fuzzy forecasting based on two-factors second-order fuzzy-trend logical relationship groups and particle swarm optimization techniques",2013,"IEEE Transactions on Cybernetics",30,10.1109/TSMCB.2012.2223815,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890427180&doi=10.1109%2fTSMCB.2012.2223815&partnerID=40&md5=793f93d19144a5c780fd2e597a75418d","In this paper, we present a new method for fuzzy forecasting based on two-factors second-order fuzzy-trend logical relationship groups and particle swarm optimization (PSO) techniques. First, we fuzzify the historical training data of the main factor and the secondary factor, respectively, to form two-factors second-order fuzzy logical relationships. Then, we group the two-factors second-order fuzzy logical relationships into two-factors second-order fuzzy-trend logical relationship groups. Then, we obtain the optimal weighting vector for each fuzzy-trend logical relationship group by using PSO techniques to perform the forecasting. We also apply the proposed method to forecast the Taiwan Stock Exchange Capitalization Weighted Stock Index and the NTD/USD exchange rates. The experimental results show that the proposed method gets better forecasting performance than the existing methods. © 2012 IEEE.","Fuzzy forecasting; Fuzzy time series; Particle swarm optimization (PSO) techniques; Two-factors second-order fuzzy-trend logical relationship groups","Forecasting performance; Fuzzy forecasting; Fuzzy logical relationships; Fuzzy time series; Fuzzy trends; Optimal weighting vector; Particle swarm optimization technique; Taiwan stock exchanges; Finance; Particle swarm optimization (PSO); Forecasting; algorithm; artificial intelligence; automated pattern recognition; computer simulation; forecasting; procedures; statistical model; article; automated pattern recognition; methodology; Algorithms; Artificial Intelligence; Computer Simulation; Forecasting; Logistic Models; Pattern Recognition, Automated; Algorithms; Artificial Intelligence; Computer Simulation; Forecasting; Logistic Models; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84890427180
"Huang K., Liu X., Li X., Liang J., He S.","An improved artificial immune system for seeking the Pareto front of land-use allocation problem in large areas",2013,"International Journal of Geographical Information Science",30,10.1080/13658816.2012.730147,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878123032&doi=10.1080%2f13658816.2012.730147&partnerID=40&md5=7f4db7625d6ecf0b027fe75023864e3b","The Pareto front can provide valuable information on land-use planning decision by revealing the possible trade-offs among multiple, conflicting objectives. However, seeking the Pareto front of land-use allocation is much more difficult than finding a unique optimal solution, especially when dealing with large-area regions. This article proposes an improved artificial immune system for multi-objective land-use allocation (AIS-MOLA) to tackle this challenging task. The proposed AIS is equipped with three modified operators, namely (1) a heuristic hypermutation based on compromise programming, (2) a non-dominated neighbour-based proportional cloning and (3) a novel crossover operator that preserves connected patches. To validate the proposed algorithm, it was applied in a hypothetical land-use allocation problem. Compared with the Pareto Simulated Annealing (PSA) method, AIS-MOLA can generate solutions more approximate to the Pareto front, with computation time amounting to only 5.1% of PSA. In addition, AIS-MOLA was also applied in the case study of Panyu, Guangdong, PR China, a large area with cells. Experimental results indicate that this algorithm, even dealing with large-area land-use allocation problems, is capable of generating optimal alternative solutions approximate to the true Pareto front. Moreover, the distribution of these solutions can quantitatively demonstrate the complex trade-offs between the spatial suitability and the compactness in the study area. Software and supplementary materials are available at http://www.geosimulation.cn/AIS-MOLA/. © 2013 Copyright Taylor and Francis Group, LLC.","artificial immune system; land-use allocation problem; multi-objective optimization; Pareto front","algorithm; artificial intelligence; decision making; heuristics; land use planning; optimization; simulated annealing; trade-off; China; Guangdong; Panyu",Article,Scopus,2-s2.0-84878123032
"Tsai H.-C., Tyan Y.-Y., Wu Y.-W., Lin Y.-H.","Gravitational particle swarm",2013,"Applied Mathematics and Computation",30,10.1016/j.amc.2013.03.098,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876536667&doi=10.1016%2fj.amc.2013.03.098&partnerID=40&md5=9fbe2fc94339ab96761a3c0f7c996207","Particle swarm optimization (PSO) is inspired by social behavior of bird flocking, gravitational search algorithm (GSA) is based on the law of gravity, and both of them are related to swarm intelligence (SI). Gravitational particle swarm (GPS) is proposed where a GPS agent has attributes of GSA and PSO. GPS agents update their respective positions with PSO velocity and GSA acceleration. GPS agents, therefore, are able to exhibit PSO bird social and cognitive behaviors and motion in flight, while also reflecting the law of gravity of GSA. From results of 23 benchmark functions, GPS does significantly improve PSO and GSA, with noticeably marked improvements. This paper proposes GPS for hybridizing PSO and GSA due to the outstanding performance and interesting concepts embodied in the GPS. © 2013 Elsevier Inc.All rights reserved.","Gravitational particle swarm; Gravitational search algorithm; Optimization; Particle swarm optimization","Benchmark functions; Cognitive behavior; Gravitational search algorithm (GSA); Gravitational search algorithms; Law of gravities; Particle swarm; Social behavior; Swarm Intelligence; Artificial intelligence; Learning algorithms; Optimization; Particle swarm optimization (PSO)",Article,Scopus,2-s2.0-84876536667
"Hazir Ö., Dolgui A.","Assembly line balancing under uncertainty: Robust optimization models and exact solution method",2013,"Computers and Industrial Engineering",30,10.1016/j.cie.2013.03.004,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876104666&doi=10.1016%2fj.cie.2013.03.004&partnerID=40&md5=21e0be006b48704bd3baf2b5211c4a40","This research deals with line balancing under uncertainty and presents two robust optimization models. Interval uncertainty for operation times was assumed. The methods proposed generate line designs that are protected against this type of disruptions. A decomposition based algorithm was developed and combined with enhancement strategies to solve optimally large scale instances. The efficiency of this algorithm was tested and the experimental results were presented. The theoretical contribution of this paper lies in the novel models proposed and the decomposition based exact algorithm developed. Moreover, it is of practical interest since the production rate of the assembly lines designed with our algorithm will be more reliable as uncertainty is incorporated. Furthermore, this is a pioneering work on robust assembly line balancing and should serve as the basis for a decision support system on this subject. © 2013 Published by Elsevier Ltd.","Assembly line balancing; Combinatorial optimization; Exact algorithms; Robust optimization; Uncertainty","Assembly line balancing; Exact algorithms; Interval uncertainty; Production rates; Robust assembly; Robust optimization; Robust optimization models; Uncertainty; Artificial intelligence; Combinatorial optimization; Decision support systems; Mathematical models; Algorithms",Article,Scopus,2-s2.0-84876104666
"Tang B., Cao H., Wu Y., Jiang M., Xu H.","Recognizing clinical entities in hospital discharge summaries using Structural Support Vector Machines with word representation features",2013,"BMC Medical Informatics and Decision Making",30,10.1186/1472-6947-13-S1-S1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875945878&doi=10.1186%2f1472-6947-13-S1-S1&partnerID=40&md5=376931a8afe8e0986c59adfe72fcaefe","Background: Named entity recognition (NER) is an important task in clinical natural language processing (NLP) research. Machine learning (ML) based NER methods have shown good performance in recognizing entities in clinical text. Algorithms and features are two important factors that largely affect the performance of ML-based NER systems. Conditional Random Fields (CRFs), a sequential labelling algorithm, and Support Vector Machines (SVMs), which is based on large margin theory, are two typical machine learning algorithms that have been widely applied to clinical NER tasks. For features, syntactic and semantic information of context words has often been used in clinical NER systems. However, Structural Support Vector Machines (SSVMs), an algorithm that combines the advantages of both CRFs and SVMs, and word representation features, which contain word-level back-off information over large unlabelled corpus by unsupervised algorithms, have not been extensively investigated for clinical text processing. Therefore, the primary goal of this study is to evaluate the use of SSVMs and word representation features in clinical NER tasks. Methods. In this study, we developed SSVMs-based NER systems to recognize clinical entities in hospital discharge summaries, using the data set from the concept extration task in the 2010 i2b2 NLP challenge. We compared the performance of CRFs and SSVMs-based NER classifiers with the same feature sets. Furthermore, we extracted two different types of word representation features (clustering-based representation features and distributional representation features) and integrated them with the SSVMs-based clinical NER system. We then reported the performance of SSVM-based NER systems with different types of word representation features. Results and discussion. Using the same training (N = 27,837) and test (N = 45,009) sets in the challenge, our evaluation showed that the SSVMs-based NER systems achieved better performance than the CRFs-based systems for clinical entity recognition, when same features were used. Both types of word representation features (clustering-based and distributional representations) improved the performance of ML-based NER systems. By combining two different types of word representation features together with SSVMs, our system achieved a highest F-measure of 85.82%, which outperformed the best system reported in the challenge by 0.6%. Our results show that SSVMs is a great potential algorithm for clinical NLP research, and both types of unsupervised word representation features are beneficial to clinical NER tasks. © 2013 Tang et al.; licensee BioMed Central Ltd.",,"algorithm; article; artificial intelligence; automated pattern recognition; clinical pathway; cluster analysis; comparative study; electronic medical record; hospital discharge; human; natural language processing; probability; reproducibility; semantics; statistics; United States; Algorithms; Artificial Intelligence; Cluster Analysis; Critical Pathways; Humans; Markov Chains; Medical Records Systems, Computerized; Natural Language Processing; Patient Discharge; Pattern Recognition, Automated; Pennsylvania; Reproducibility of Results; Semantics",Article,Scopus,2-s2.0-84875945878
"Li M., Yang S., Liu X., Shen R.","A comparative study on evolutionary algorithms for many-objective optimization",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",30,10.1007/978-3-642-37140-0_22,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875501452&doi=10.1007%2f978-3-642-37140-0_22&partnerID=40&md5=e5fe4f5167341a23d4e119787594e31e","Many-objective optimization has been gaining increasing attention in the evolutionary multiobjective optimization community, and various approaches have been developed to solve many-objective problems in recent years. However, the existing empirically comparative studies are often restricted to only a few approaches on a handful of test problems. This paper provides a systematic comparison of eight representative approaches from the six angles to solve many-objective problems. The compared approaches are tested on four groups of well-defined continuous and combinatorial test functions, by three performance metrics as well as a visual observation in the decision space. We conclude that none of the approaches has a clear advantage over the others, although some of them are competitive on most of the problems. In addition, different search abilities of these approaches on the problems with different characteristics suggest a careful choice of approaches for solving a many-objective problem in hand. © 2013 Springer-Verlag.",,"Comparative studies; Decision space; Evolutionary multiobjective optimization; Many-objective optimizations; Performance metrics; Test functions; Test problem; Visual observations; Artificial intelligence; Multiobjective optimization",Conference Paper,Scopus,2-s2.0-84875501452
"Czaja W., Ehler M.","Schroedinger eigenmaps for the analysis of biomedical data",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",30,10.1109/TPAMI.2012.270,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875418338&doi=10.1109%2fTPAMI.2012.270&partnerID=40&md5=0f997df9bd579d2007c47736b50c19cf","We introduce Schroedinger Eigenmaps (SE), a new semi-supervised manifold learning and recovery technique. This method is based on an implementation of graph Schroedinger operators with appropriately constructed barrier potentials as carriers of labeled information. We use our approach for the analysis of standard biomedical datasets and new multispectral retinal images. © 1979-2012 IEEE.","barrier potential; dimension reduction; Laplacian Eigenmaps; manifold learning; Schroedinger Eigenmaps; Schroedinger operator on a graph","Barrier potential; Dimension reduction; Laplacian eigenmaps; Manifold learning; Schroedinger Eigenmaps; Schroedinger operator on a graph; Artificial intelligence; Computer vision; algorithm; article; artificial intelligence; automated pattern recognition; breast tumor; classification; data mining; factual database; female; heart disease; human; medical research; methodology; pathology; retina disease; statistical analysis; Algorithms; Artificial Intelligence; Biomedical Research; Breast Neoplasms; Data Interpretation, Statistical; Data Mining; Databases, Factual; Female; Heart Diseases; Humans; Pattern Recognition, Automated; Retinal Diseases",Article,Scopus,2-s2.0-84875418338
"Damgård I., Zakarias S.","Constant-overhead secure computation of Boolean circuits using preprocessing",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",30,10.1007/978-3-642-36594-2_35,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873966735&doi=10.1007%2f978-3-642-36594-2_35&partnerID=40&md5=d21fff6a08142d4e34d42b381ea62831","We present a protocol for securely computing a Boolean circuit C in presence of a dishonest and malicious majority. The protocol is unconditionally secure, assuming a preprocessing functionality that is not given the inputs. For a large number of players the work for each player is the same as computing the circuit in the clear, up to a constant factor. Our protocol is the first to obtain these properties for Boolean circuits. On the technical side, we develop new homomorphic authentication schemes based on asymptotically good codes with an additional multiplication property. We also show a new algorithm for verifying the product of Boolean matrices in quadratic time with exponentially small error probability, where previous methods only achieved constant error. © 2013 International Association for Cryptologic Research.",,"Authentication scheme; Boolean circuit; Boolean matrices; Constant factors; Error probabilities; Good code; Not given; Quadratic time; Secure computation; Unconditionally secure; Artificial intelligence; Cryptography",Conference Paper,Scopus,2-s2.0-84873966735
"Sahbi H., Ballan L., Serra G., Del Bimbo A.","Context-dependent logo matching and recognition",2013,"IEEE Transactions on Image Processing",30,10.1109/TIP.2012.2226046,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873292183&doi=10.1109%2fTIP.2012.2226046&partnerID=40&md5=2bf235019526f315cfae610817d39015","We contribute, through this paper, to the design of a novel variational framework able to match and recognize multiple instances of multiple reference logos in image archives. Reference logos and test images are seen as constellations of local features (interest points, regions, etc.) and matched by minimizing an energy function mixing: 1) a fidelity term that measures the quality of feature matching, 2) a neighborhood criterion that captures feature co-occurrence/geometry, and 3) a regularization term that controls the smoothness of the matching solution. We also introduce a detection/recognition procedure and study its theoretical consistency. Finally, we show the validity of our method through extensive experiments on the challenging MICC-Logos dataset. Our method overtakes, by 20%, baseline as well as state-of-the-art matching/recognition procedures. © 1992-2012 IEEE.","Context-dependent kernel; logo detection; logo recognition","Co-occurrence; Context dependent; Data sets; Energy functions; Feature matching; Fidelity term; Image archives; Local feature; Logo recognition; Multiple instances; Multiple references; Test images; Variational framework; Mathematical models; Image processing; algorithm; article; artificial intelligence; automated pattern recognition; classification; computer assisted diagnosis; documentation; history; image enhancement; image subtraction; information retrieval; methodology; reproducibility; sensitivity and specificity; Algorithms; Artificial Intelligence; Documentation; Emblems and Insignia; Image Enhancement; Image Interpretation, Computer-Assisted; Information Storage and Retrieval; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-84873292183
"Torresani L., Kolmogorov V., Rother C.","A dual decomposition approach to feature correspondence",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",30,10.1109/TPAMI.2012.105,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871731539&doi=10.1109%2fTPAMI.2012.105&partnerID=40&md5=159945ce6e3e84101ecfaecc38cf6490","In this paper, we present a new approach for establishing correspondences between sparse image features related by an unknown nonrigid mapping and corrupted by clutter and occlusion, such as points extracted from images of different instances of the same object category. We formulate this matching task as an energy minimization problem by defining an elaborate objective function of the appearance and the spatial arrangement of the features. Optimization of this energy is an instance of graph matching, which is in general an NP-hard problem. We describe a novel graph matching optimization technique, which we refer to as dual decomposition (DD), and demonstrate on a variety of examples that this method outperforms existing graph matching algorithms. In the majority of our examples, DD is able to find the global minimum within a minute. The ability to globally optimize the objective allows us to accurately learn the parameters of our matching model from training examples. We show on several matching tasks that our learned model yields results superior to those of state-of-the-art methods. © 2012 IEEE.","dual decomposition; feature correspondence; Graph matching","Dual decomposition; Energy minimization problem; Feature correspondence; Global minima; Graph matchings; Graph-matching algorithms; Matching models; Model yields; Object categories; Objective functions; Sparse images; Spatial arrangements; State-of-the-art methods; Training example; Computational complexity; Optimization; Pattern matching; Image matching; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; image enhancement; image subtraction; information retrieval; methodology; reproducibility; sensitivity and specificity; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Information Storage and Retrieval; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-84871731539
"Kulich M., Chudoba J., Kosnar K., Krajnik T., Faigl J., Preucil L.","SyRoTek-Distance teaching of mobile robotics",2013,"IEEE Transactions on Education",30,10.1109/TE.2012.2224867,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873406105&doi=10.1109%2fTE.2012.2224867&partnerID=40&md5=a0abc4cc19bd19481aa301dce8492151","E-learning is a modern and effective approach for training in various areas and at different levels of education. This paper gives an overview of SyRoTek, an e-learning platform for mobile robotics, artificial intelligence, control engineering, and related domains. SyRoTek provides remote access to a set of fully autonomous mobile robots placed in a restricted area with dynamically reconfigurable obstacles, which enables solving a huge variety of problems. A user is able to control the robots in real time by their own developed algorithms as well as being able to analyze gathered data and observe activity of the robots by provided interfaces. The system is currently used for education at the Czech Technical University in Prague, Prague, Czech Republic, and at the University of Buenos Aires, Buenos, Aires, Argentina, and it is freely accessible to other institutions. In addition to the system overview, this paper presents the experience gained from the actual deployment of the system in teaching activities. © 2012 IEEE.","Educational technology; mobile robots; robot programming; telerobotics; user interfaces","Argentina; Autonomous Mobile Robot; Buenos Aires; Control engineering; E-learning platforms; Educational technology; Mobile robotic; Prague , Czech Republic; Real time; Remote access; Technical universities; Tele-robotics; Artificial intelligence; E-learning; Mobile robots; Robot programming; User interfaces; Robotics",Article,Scopus,2-s2.0-84873406105
"Victer Paul P., Rajaguru D., Saravanan N., Baskaran R., Dhavachelvan P.","Efficient service cache management in mobile P2P networks",2013,"Future Generation Computer Systems",30,10.1016/j.future.2012.12.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880929509&doi=10.1016%2fj.future.2012.12.001&partnerID=40&md5=9a6a59cf0ef5338c19fa16e77ce5436a","Service oriented computing is growing as the prominent technology both in the technical and business perspectives. Due to the rapid growth in the volume of service repositories and services, obtaining the appropriate service is becoming tedious in the current scenario. Though advanced techniques are proposed to handle these challenges in conventional environments, still it is in its critical stage in the resource constrained scenarios, particularly in wireless mobile environments. The work presented in this paper proposes a Service Cache Management (SCM) for Mobile Peer-to-Peer (MP2P) networks to facilitate the efficient retrieval of services. We used a communication structure called Distributed Spanning Tree (DST), in which nodes of MP2P are made to form a forest of spanning trees in a distributed fashion. Using the DST structure, fast service cache and retrieval can be achieved within MP2P. Further to enhance the effectiveness of the proposed system, the DST network is optimized with Ant Colony Optimization (ACO), to identify the simplest path between nodes of each layer on the DST. The merits of the proposed techniques are demonstrated in terms of reduced time requirement for cached service search, increased availability, enhanced consistency, scalability and improved hit ratios. © 2012 Elsevier B.V. All rights reserved.","Ant colony optimization; Consistency; Distributed spanning tree; Hit ratio; Mobile peer to peer; Scalability; Service cache; Service computing","Ant colony optimization; Artificial intelligence; Peer to peer networks; Scalability; Consistency; Distributed spanning trees; Hit ratio; Mobile peer to peers; Service cache; Service computing; Distributed computer systems",Article,Scopus,2-s2.0-84880929509
"Todorovic N., Petrovic S.","Bee colony optimization algorithm for nurse rostering",2013,"IEEE Transactions on Systems, Man, and Cybernetics Part A:Systems and Humans",30,10.1109/TSMCA.2012.2210404,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878937604&doi=10.1109%2fTSMCA.2012.2210404&partnerID=40&md5=a24fd92d85875e2e601a5f33945967b6","In this paper, we propose a novel bee colony optimization approach to the nurse rostering problem. The bee colony optimization algorithm is motivated by the foraging habits of honey bees. In iterations, artificial bees collectively improve their solutions. The developed algorithm alternates constructive and local search phases. In the constructive phase, unscheduled shifts are assigned to available nurses, while the aim of local search phase is to improve the quality of the solution. The algorithm incorporates a novel intelligent discarding of portions of large neighborhoods for which it is predicted that they will not lead to the improvement of the objective function. Performance of the algorithm was evaluated on real-world data from hospitals in Belgium. The results show that the bee colony optimization is able to efficiently find solutions that are competitive compared to the solutions produced by other algorithms reported in the literature. © 2012 IEEE.","Bee colony optimization; Metaheuristics; Nurse rostering; Swarm intelligence","Artificial bees; Bee colony optimizations; Large neighborhood; Local search; Meta heuristics; Nurse rostering; Objective functions; Swarm Intelligence; Artificial intelligence; Nursing; Optimization; Algorithms",Article,Scopus,2-s2.0-84878937604
"Jacoban R., Reynolds R.G., Brewster J.","Cultural swarms: Assessing the impact of culture on social interaction and problem solving",2013,"2003 IEEE Swarm Intelligence Symposium, SIS 2003 - Proceedings",30,10.1109/SIS.2003.1202271,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942095850&doi=10.1109%2fSIS.2003.1202271&partnerID=40&md5=cbfa56418975ee3470e0f06da997b411","In this paper we investigate how diverse knowledge sources interact to direct individuals in a swarm population. We identify three basic phases of problem solving that are generated by the swarm population in the solution of real valued function optimization problems. The question that we are interested in answering is how these phases derive from the interaction of various sources of cultural knowledge present in the belief space of a population. We map the central tendency of the subset of individuals that are influenced by each knowledge source over time at the meta-level. The resultant patterns suggest the presence of ""cultural swarms"" where various knowledge sources take turns at leading and following in the exploration and exploitation of the problem space. This implies that the social interaction of individuals coupled with their interaction with a culture within which they are embedded provides a powerful vehicle for the solution of complex problems. © 2003 IEEE.","Computer science; Cultural differences; Java; Problem-solving; Testing; Vehicles","Amphibious vehicles; Artificial intelligence; Computer science; Economic and social effects; Optimization; Social networking (online); Social sciences; Testing; Vehicles; Central tendencies; Cultural difference; Cultural knowledge; Exploration and exploitation; Java; Knowledge sources; Real-valued functions; Social interactions; Problem solving",Conference Paper,Scopus,2-s2.0-84942095850
"Sarkka S., Solin A., Hartikainen J.","Spatiotemporal learning via infinite-dimensional bayesian filtering and smoothing: A look at gaussian process regression through kalman filtering",2013,"IEEE Signal Processing Magazine",30,10.1109/MSP.2013.2246292,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032750844&doi=10.1109%2fMSP.2013.2246292&partnerID=40&md5=5d56d93a4063ff181e50b216f38c9822","Gaussian process-based machine learning is a powerful Bayesian paradigm for nonparametric nonlinear regression and classification. In this article, we discuss connections of Gaussian process regression with Kalman filtering and present methods for converting spatiotemporal Gaussian process regression problems into infinite-dimensional state-space models. This formulation allows for use of computationally efficient infinite-dimensional Kalman filtering and smoothing methods, or more general Bayesian filtering and smoothing methods, which reduces the problematic cubic complexity of Gaussian process regression in the number of time steps into linear time complexity. The implication of this is that the use of machine-learning models in signal processing becomes computationally feasible, and it opens the possibility to combine machine-learning techniques with signal processing methods. © 1991-2012 IEEE.",,"Artificial intelligence; Gaussian noise (electronic); Kalman filters; Learning systems; Processing; Regression analysis; Signal processing; State space methods; Computationally efficient; Gaussian process regression; Linear time complexity; Machine learning models; Machine learning techniques; Non-linear regression; Spatio-temporal learning; State - space models; Gaussian distribution",Article,Scopus,2-s2.0-85032750844
"Dell'Aglio D., Calbimonte J.-P., Balduini M., Corcho O., Della Valle E.","On correctness in RDF stream processor benchmarking",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",29,10.1007/978-3-642-41338-4_21,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891940910&doi=10.1007%2f978-3-642-41338-4_21&partnerID=40&md5=942f77e8f9962c5d6b08b1eec67162db","Two complementary benchmarks have been proposed so far for the evaluation and continuous improvement of RDF stream processors: SRBench and LSBench. They put a special focus on different features of the evaluated systems, including coverage of the streaming extensions of SPARQL supported by each processor, query processing throughput, and an early analysis of query evaluation correctness, based on comparing the results obtained by different processors for a set of queries. However, none of them has analysed the operational semantics of these processors in order to assess the correctness of query evaluation results. In this paper, we propose a characterization of the operational semantics of RDF stream processors, adapting well-known models used in the stream processing engine community: CQL and SECRET. Through this formalization, we address correctness in RDF stream processor benchmarks, allowing to determine the multiple answers that systems should provide. Finally, we present CSRBench, an extension of SRBench to address query result correctness verification using an automatic method. © 2013 Springer-Verlag.",,"Automatic method; Continuous improvements; Correctness verifications; Operational semantics; Query evaluation; Query results; Stream processing engines; Stream processor; Artificial intelligence; Computer science; Computers; Computer programming languages",Conference Paper,Scopus,2-s2.0-84891940910
"Liu X., Tosun D., Weiner M.W., Schuff N.","Locally linear embedding (LLE) for MRI based Alzheimer's disease classification",2013,"NeuroImage",29,10.1016/j.neuroimage.2013.06.033,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881226895&doi=10.1016%2fj.neuroimage.2013.06.033&partnerID=40&md5=5b8853c7bf49a9359c09d23f0a7bdf46","Modern machine learning algorithms are increasingly being used in neuroimaging studies, such as the prediction of Alzheimer's disease (AD) from structural MRI. However, finding a good representation for multivariate brain MRI features in which their essential structure is revealed and easily extractable has been difficult. We report a successful application of a machine learning framework that significantly improved the use of brain MRI for predictions. Specifically, we used the unsupervised learning algorithm of local linear embedding (LLE) to transform multivariate MRI data of regional brain volume and cortical thickness to a locally linear space with fewer dimensions, while also utilizing the global nonlinear data structure. The embedded brain features were then used to train a classifier for predicting future conversion to AD based on a baseline MRI. We tested the approach on 413 individuals from the Alzheimer's Disease Neuroimaging Initiative (ADNI) who had baseline MRI scans and complete clinical follow-ups over 3. years with the following diagnoses: cognitive normal (CN; n. = 137), stable mild cognitive impairment (s-MCI; n. = 93), MCI converters to AD (c-MCI, n. = 97), and AD (n. = 86). We found that classifications using embedded MRI features generally outperformed (p. <. 0.05) classifications using the original features directly. Moreover, the improvement from LLE was not limited to a particular classifier but worked equally well for regularized logistic regressions, support vector machines, and linear discriminant analysis. Most strikingly, using LLE significantly improved (p. = 0.007) predictions of MCI subjects who converted to AD and those who remained stable (accuracy/sensitivity/specificity: = 0.68/0.80/0.56). In contrast, predictions using the original features performed not better than by chance (accuracy/sensitivity/specificity: = 0.56/0.65/0.46). In conclusion, LLE is a very effective tool for classification studies of AD using multivariate MRI data. The improvement in predicting conversion to AD in MCI could have important implications for health management and for powering therapeutic trials by targeting non-demented subjects who later convert to AD. © 2013 Elsevier Inc.","Alzheimer's disease; Classification of AD; Locally linear embedding; MRI; Statistical learning","aged; Alzheimer disease; article; brain size; classifier; controlled study; cortical thickness (brain); disease classification; female; human; learning algorithm; local linear embedding; major clinical study; male; mild cognitive impairment; nuclear magnetic resonance imaging; parietal lobe; priority journal; temporal lobe; algorithm; Alzheimer disease; artificial intelligence; automated pattern recognition; brain; computer assisted diagnosis; computer simulation; image enhancement; middle aged; nuclear magnetic resonance imaging; pathology; procedures; reproducibility; sensitivity and specificity; statistical model; Aged; Algorithms; Alzheimer Disease; Artificial Intelligence; Brain; Computer Simulation; Female; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Linear Models; Magnetic Resonance Imaging; Male; Middle Aged; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84881226895
"Zhai D., Chang H., Zhen Y., Liu X., Chen X., Gao W.","Parametric local multimodal hashing for cross-view similarity search",2013,"IJCAI International Joint Conference on Artificial Intelligence",29,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062612&partnerID=40&md5=a1b1ac9c24bb3d9065cb115525877978","Recent years have witnessed the growing popularity of hashing for efficient large-scale similarity search. It has been shown that the hashing quality could be boosted by hash function learning (HFL). In this paper, we study HFL in the context of multimodal data for cross-view similarity search. We present a novel multimodal HFL method, called Parametric Local Multimodal Hashing (PLMH), which learns a set of hash functions to locally adapt to the data structure of each modality. To balance locality and computational efficiency, the hashing projection matrix of each instance is parameterized, with guaranteed approximation error bound, as a linear combination of basis hashing projections of a small set of anchor points. A local optimal conjugate gradient algorithm is designed to learn the hash functions for each bit, and the overall hash codes are learned in a sequential manner to progressively minimize the bias. Experimental evaluations on cross-media retrieval tasks demonstrate that PLMH performs competitively against the state-of-the-art methods.",,"Approximation errors; Conjugate gradient algorithms; Cross-media retrieval; Experimental evaluation; Linear combinations; Projection matrix; Sequential manners; State-of-the-art methods; Artificial intelligence; Hash functions",Conference Paper,Scopus,2-s2.0-84896062612
"Jiang Y.-G., Wang Y., Feng R., Xue X., Zheng Y., Yang H.","Understanding and predicting interestingness of videos",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",29,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893344269&partnerID=40&md5=cdd418f297e7f01281c0fbb9f3267922","The amount of videos available on the Web is growing explosively. While some videos are very interesting and receive high rating from viewers, many of them are less interesting or even boring. This paper conducts a pilot study on the understanding of human perception of video interestingness, and demonstrates a simple computational method to identify more interesting videos. To this end we first construct two datasets of Flickr and YouTube videos respectively. Human judgements of interestingness are collected and used as the groundtruth for training computational models. We evaluate several off-the-shelf visual and audio features that are potentially useful for predicting interestingness on both datasets. Results indicate that audio and visual features are equally important and the combination of both modalities shows very promising results. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Audio features; Computational model; Human perception; Interestingness; Pilot studies; Visual feature; YouTube; Artificial intelligence; Computational methods; Data processing",Conference Paper,Scopus,2-s2.0-84893344269
"Churchill D., Buro M.","Portfolio greedy search and simulation for large-scale combat in starcraft",2013,"IEEE Conference on Computatonal Intelligence and Games, CIG",29,10.1109/CIG.2013.6633643,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892424004&doi=10.1109%2fCIG.2013.6633643&partnerID=40&md5=f06ba754928871c459770493119ba87b","Real-time strategy video games have proven to be a very challenging area for applications of artificial intelligence research. With their vast state and action spaces and real-time constraints, existing AI solutions have been shown to be too slow, or only able to be applied to small problem sets, while human players still dominate RTS AI systems. This paper makes three contributions to advancing the state of AI for popular commercial RTS game combat, which can consist of battles of dozens of units. First, we present an efficient system for modelling abstract RTS combat called SparCraft, which can perform millions of unit actions per second and visualize them. We then present a modification of the UCT algorithm capable of performing search in games with simultaneous and durative actions. Finally, a novel greedy search algorithm called Portfolio Greedy Search is presented which uses hill climbing and accurate playout-based evaluations to efficiently search even the largest combat scenarios. We demonstrate that Portfolio Greedy Search outperforms state of the art Alpha-Beta and UCT search methods for large StarCraft combat scenarios of up to 50 vs. 50 units under real-time search constraints of 40 ms per search episode. © 2013 IEEE.",,"Artificial intelligence research; Greedy search algorithms; Hill climbing; Real time constraints; Real time strategies; Real-time searches; Search method; State of the art; Algorithms; Artificial intelligence; Human computer interaction; Interactive computer graphics; Real time systems; Computer software",Conference Paper,Scopus,2-s2.0-84892424004
"Huffman B., Kunčar O.","Lifting and transfer: A modular design for quotients in Isabelle/HOL",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",29,10.1007/978-3-319-03545-1_9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893109614&doi=10.1007%2f978-3-319-03545-1_9&partnerID=40&md5=6fd19582367a2c36c4c3a135de67cee4","Quotients, subtypes, and other forms of type abstraction are ubiquitous in formal reasoning with higher-order logic. Typically, users want to build a library of operations and theorems about an abstract type, but they want to write definitions and proofs in terms of a more concrete representation type, or ""raw"" type. Earlier work on the Isabelle Quotient package has yielded great progress in automation, but it still has many technical limitations. We present an improved, modular design centered around two new packages: the Transfer package for proving theorems, and the Lifting package for defining constants. Our new design is simpler, applicable in more situations, and has more user-friendly automation. © Springer International Publishing 2013.",,"Abstract types; Formal reasoning; Higher order logic; Isabelle; Isabelle/HOl; Modular designs; Representation type; Technical limitations; Artificial intelligence; Computer science; Computers; Design",Conference Paper,Scopus,2-s2.0-84893109614
"Li G., Wen C., Li Z.G., Zhang A., Yang F., Mao K.","Model-based online learning with kernels",2013,"IEEE Transactions on Neural Networks and Learning Systems",29,10.1109/TNNLS.2012.2229293,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879125836&doi=10.1109%2fTNNLS.2012.2229293&partnerID=40&md5=c3c5ee674c7d2abb7d346e94a5ab8985","New optimization models and algorithms for online learning with Kernels (OLK) in classification, regression, and novelty detection are proposed in a reproducing Kernel Hilbert space. Unlike the stochastic gradient descent algorithm, called the naive online Reg minimization algorithm (NORMA), OLK algorithms are obtained by solving a constrained optimization problem based on the proposed models. By exploiting the techniques of the Lagrange dual problem like Vapnik's support vector machine (SVM), the solution of the optimization problem can be obtained iteratively and the iteration process is similar to that of the NORMA. This further strengthens the foundation of OLK and enriches the research area of SVM. We also apply the obtained OLK algorithms to problems in classification, regression, and novelty detection, including real time background substraction, to show their effectiveness. It is illustrated that, based on the experimental results of both classification and regression, the accuracy of OLK algorithms is comparable with traditional SVM-based algorithms, such as SVM and least square SVM (LS-SVM), and with the state-of-the-art algorithms, such as Kernel recursive least square (KRLS) method and projectron method, while it is slightly higher than that of NORMA. On the other hand, the computational cost of the OLK algorithm is comparable with or slightly lower than existing online methods, such as above mentioned NORMA, KRLS, and projectron methods, but much lower than that of SVM-based algorithms. In addition, different from SVM and LS-SVM, it is possible for OLK algorithms to be applied to non-stationary problems. Also, the applicability of OLK in novelty detection is illustrated by simulation results. © 2013 IEEE.","Classification; Kernels; Novelty detection; Online learning; Regression; Reproducing kernel hilbert space","Kernels; Novelty detection; Online learning; Regression; Reproducing Kernel Hilbert spaces; Algorithms; Classification (of information); Constrained optimization; E-learning; Least squares approximations; Regression analysis; Support vector machines; Iterative methods; artificial intelligence; artificial neural network; computer simulation; human; theoretical model; trends; Artificial Intelligence; Computer Simulation; Humans; Models, Theoretical; Neural Networks (Computer)",Article,Scopus,2-s2.0-84879125836
"Rosch E.","Principles of Categorization",2013,"Readings in Cognitive Science: A Perspective from Psychology and Artificial Intelligence",29,10.1016/B978-1-4832-1446-7.50028-5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944075245&doi=10.1016%2fB978-1-4832-1446-7.50028-5&partnerID=40&md5=e349236d9615a10e7573fee56512b704",[No abstract available],,"Artificial intelligence",Book Chapter,Scopus,2-s2.0-84944075245
"Barmpoutis A.","Tensor body: Real-time reconstruction of the human body and avatar synthesis from RGB-D",2013,"IEEE Transactions on Cybernetics",29,10.1109/TCYB.2013.2276430,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890411897&doi=10.1109%2fTCYB.2013.2276430&partnerID=40&md5=17ea704a7aaa0be1f6bd4e1d93c37a1c","Real-time 3-D reconstruction of the human body has many applications in anthropometry, telecommunications, gaming, fashion, and other areas of human-computer interaction. In this paper, a novel framework is presented for reconstructing the 3-D model of the human body from a sequence of RGB-D frames. The reconstruction is performed in real time while the human subject moves arbitrarily in front of the camera. The method employs a novel parameterization of cylindrical-type objects using Cartesian tensor and b-spline bases along the radial and longitudinal dimension respectively. The proposed model, dubbed tensor body, is fitted to the input data using a multistep framework that involves segmentation of the different body regions, robust filtering of the data via a dynamic histogram, and energy-based optimization with positive-definite constraints. A Riemannian metric on the space of positive-definite tensor splines is analytically defined and employed in this framework. The efficacy of the presented methods is demonstrated in several real-data experiments using the Microsoft Kinect sensor. © 2013 IEEE.","3-D reconstruction; Avatar; b-spline; Kinect; Positive-definite constraints; Tensor basis","3D reconstruction; Avatar; B-spline; Kinect; Positive-definite constraints; Tensor basis; Interpolation; Tensors; Three dimensional computer graphics; Three dimensional; algorithm; art; artificial intelligence; automated pattern recognition; biological model; biomimetics; computer; computer assisted diagnosis; computer simulation; computer system; devices; human; procedures; recreation; three dimensional imaging; transducer; whole body imaging; article; automated pattern recognition; biomimetics; computer assisted diagnosis; equipment; methodology; three dimensional imaging; whole body imaging; Algorithms; Artificial Intelligence; Biomimetics; Cartoons as Topic; Computer Peripherals; Computer Simulation; Computer Systems; Humans; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Models, Biological; Pattern Recognition, Automated; Transducers; Video Games; Whole Body Imaging; Algorithms; Artificial Intelligence; Biomimetics; Cartoons as Topic; Computer Peripherals; Computer Simulation; Computer Systems; Humans; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Models, Biological; Pattern Recognition, Automated; Transducers; Video Games; Whole Body Imaging",Article,Scopus,2-s2.0-84890411897
"Zhang Y., Zhang Y., Swears E., Larios N., Wang Z., Ji Q.","Modeling temporal interactions with interval temporal bayesian networks for complex activity recognition",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",29,10.1109/TPAMI.2013.33,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883137316&doi=10.1109%2fTPAMI.2013.33&partnerID=40&md5=8607683d7058bda6ecafd596a95ecf1a","Complex activities typically consist of multiple primitive events happening in parallel or sequentially over a period of time. Understanding such activities requires recognizing not only each individual event but, more importantly, capturing their spatiotemporal dependencies over different time intervals. Most of the current graphical model-based approaches have several limitations. First, time - sliced graphical models such as hidden Markov models (HMMs) and dynamic Bayesian networks are typically based on points of time and they hence can only capture three temporal relations: precedes, follows, and equals. Second, HMMs are probabilistic finite-state machines that grow exponentially as the number of parallel events increases. Third, other approaches such as syntactic and description-based methods, while rich in modeling temporal relationships, do not have the expressive power to capture uncertainties. To address these issues, we introduce the interval temporal Bayesian network (ITBN), a novel graphical model that combines the Bayesian Network with the interval algebra to explicitly model the temporal dependencies over time intervals. Advanced machine learning methods are introduced to learn the ITBN model structure and parameters. Experimental results show that by reasoning with spatiotemporal dependencies, the proposed model leads to a significantly improved performance when modeling and recognizing complex activities involving both parallel and sequential events. © 1979-2012 IEEE.","Activity recognition; Bayesian networks; interval temporal Bayesian networks; temporal reasoning","Activity recognition; Dynamic Bayesian networks; Hidden markov models (HMMs); Machine learning methods; Model based approach; Spatio-temporal dependencies; Temporal reasoning; Temporal relationships; Graphic methods; Hidden Markov models; Learning systems; Speech recognition; Uncertainty analysis; Bayesian networks; algorithm; article; artificial intelligence; automated pattern recognition; Bayes theorem; computer simulation; mathematical computing; methodology; statistical model; automated pattern recognition; procedures; Algorithms; Artificial Intelligence; Bayes Theorem; Computer Simulation; Models, Statistical; Numerical Analysis, Computer-Assisted; Pattern Recognition, Automated; Algorithms; Artificial Intelligence; Bayes Theorem; Computer Simulation; Models, Statistical; Numerical Analysis, Computer-Assisted; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84883137316
"Ding K., Zheng S., Tan Y.","A GPU-based parallel fireworks algorithm for optimization",2013,"GECCO 2013 - Proceedings of the 2013 Genetic and Evolutionary Computation Conference",29,10.1145/2463372.2463377,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883103945&doi=10.1145%2f2463372.2463377&partnerID=40&md5=02b7f0a17f50ff652ef15f3cb719d915","Swarm intelligence algorithms have been widely used to solve difficult real world problems in both academic and engineering domains. Thanks to the inherent parallelism, various parallelized swarm intelligence algorithms have been proposed to speed up the optimization process, especially on the massively parallel processing architecture GPUs. However, conventional swarm intelligence algorithms are usually not designed specifically for the GPU architecture. They neither can fully exploit the tremendous computational power of GPUs nor can extend effectively as the problem scales go large. To address this shortcoming, a novel GPU-based Fireworks Algorithm (GPU-FWA) is proposed in this paper. In order to fully leverage GPUs' high performance, GPU-FWA modified the original FWA so that it is more suitable for the GPU architecture. An implementation of GPU-FWA on the CUDA platform is presented and then tested on a suite of well-known benchmark optimization problems. We extensively evaluated GPU-FWA and compared it with FWA and PSO, with respect to both running time and solution quality, on a state-of-the-art commodity Fermi GPU. Experimental results demonstrate that GPU-FWA generally outperforms both FWA and PSO, and enjoys a significant speedup as high as 200x, compared to the sequential version of FWA and PSO running on an up-to-date CPU. GPU-FWA also enjoys the advantages of being easy to implement and scalable. Copyright © 2013 ACM.","CUDA; Fireworks algorithm; GPU computing; Parallel computing; Swarm intelligence","CUDA; Fireworks algorithms; GPU computing; Inherent parallelism; Massively parallel processing; Optimization problems; Swarm Intelligence; Swarm intelligence algorithms; Artificial intelligence; Explosives; Optimization; Parallel architectures; Parallel processing systems; Program processors; Algorithms",Conference Paper,Scopus,2-s2.0-84883103945
"Mwangi B., Hasan K.M., Soares J.C.","Prediction of individual subject's age across the human lifespan using diffusion tensor imaging: A machine learning approach",2013,"NeuroImage",29,10.1016/j.neuroimage.2013.02.055,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875660261&doi=10.1016%2fj.neuroimage.2013.02.055&partnerID=40&md5=f9c43e1210e9ba28dc86f18f801ef4da","Diffusion tensor imaging has the potential to be used as a neuroimaging marker of natural ageing and assist in elucidating trajectories of cerebral maturation and ageing. In this study, we applied a multivariate technique relevance vector regression (RVR) to predict individual subject's age using whole brain fractional anisotropy (FA), mean diffusivity (MD), axial diffusivity (AD) and radial diffusivity (RD) from a cohort of 188 subjects aged 4-85. years. High prediction accuracy as derived from Pearson correlation coefficient of actual versus predicted age (FA - r= 0.870 p. <. 0.0001; MD - r= 0.896 p. <. 0.0001; AD - r= 0.895 p. <. 0.0001; RD - r= 0.899 p. <. 0.0001) was achieved. Cerebral white-matter regions that contributed to these predictions include; corpus callosum, cingulum bundles, posterior longitudinal fasciculus and the cerebral peduncle. A post-hoc analysis of these regions showed that FA follows a nonlinear rational-quadratic trajectory across the lifespan peaking at approximately 21.8. years. The MD, RD and AD volumes were particularly useful for making predictions using grey matter cerebral regions. These results suggest that diffusion tensor imaging measurements can reliably predict individual subject's age and demonstrate that FA cerebral maturation and ageing patterns follow a non-linear trajectory with a noteworthy peaking age. These data will contribute to the understanding of neurobiology of cerebral maturation and ageing. Most notably, from a neuropsychiatric perspective our results may allow differentiation of cerebral changes that may occur due to natural maturation and ageing, and those due to developmental or neuropsychiatric disorders. •Machine-learning is used to predict age using whole-brain diffusion tensor images.•A cross-validation approach is used to separate training and testing datasets.•White matter follows a rational-quadratic trajectory peaking at 21.8years.•Diffusivity in grey-matter tissue increases with maturation and ageing. © 2013 Elsevier Inc.","Cerebral ageing; Cerebral maturation; Diffusion tensor imaging; Machine learning; Multivariate; Neuroimaging; Relevance vector regression","accuracy; adult; age; aged; aging; article; axial diffusivity; brain maturation; cerebral peduncle; child; cingulum bundle; controlled study; corpus callosum; correlation coefficient; diffusion tensor imaging; female; fractional anisotropy; human; lifespan; machine learning; male; mean diffusivity; neuroimaging; nonlinear system; normal human; posterior longitudinal fasciculus; prediction; preschool child; priority journal; radial diffusivity; radiological parameters; relevance vector regression; white matter; Adolescent; Adult; Aged; Aged, 80 and over; Aging; Artificial Intelligence; Brain; Child; Child, Preschool; Diffusion Tensor Imaging; Female; Humans; Image Interpretation, Computer-Assisted; Infant; Infant, Newborn; Male; Middle Aged; Young Adult",Article,Scopus,2-s2.0-84875660261
"Marqués A.I., García V., Sánchez J.S.","On the suitability of resampling techniques for the class imbalance problem in credit scoring",2013,"Journal of the Operational Research Society",29,10.1057/jors.2012.120,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878929285&doi=10.1057%2fjors.2012.120&partnerID=40&md5=34e76e109e4dbe4b0f7ee98b1623c9c9","In real-life credit scoring applications, the case in which the class of defaulters is under-represented in comparison with the class of non-defaulters is a very common situation, but it has still received little attention. The present paper investigates the suitability and performance of several resampling techniques when applied in conjunction with statistical and artificial intelligence prediction models over five real-world credit data sets, which have artificially been modified to derive different imbalance ratios (proportion of defaulters and non-defaulters examples). Experimental results demonstrate that the use of resampling methods consistently improves the performance given by the original imbalanced data. Besides, it is also important to note that in general, over-sampling techniques perform better than any under-sampling approach. © 2013 Operational Research Society Ltd.","class imbalance; credit scoring; logistic regression; resampling; support vector machine","Class imbalance; Class imbalance problems; Credit scoring; Logistic regressions; Resampling; Resampling method; Resampling technique; Under-represented; Artificial intelligence; Logistics; Support vector machines; Importance sampling",Article,Scopus,2-s2.0-84878929285
"Pitchipoo P., Venkumar P., Rajakarunakaran S.","Fuzzy hybrid decision model for supplier evaluation and selection",2013,"International Journal of Production Research",29,10.1080/00207543.2012.756592,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880507260&doi=10.1080%2f00207543.2012.756592&partnerID=40&md5=f73837a5b0c36a2afae537610f54f80c","This paper proposes a structured, integrated decision model for evaluating suppliers by combining the fuzzy analytical hierarchy process (FAHP) and grey relational analysis (GRA). The qualitative and partially-known information is incorporated in this decision model using the fuzzy set theory. In this proposed methodology, the weights of the evaluation criteria are calculated by using FAHP, then the ranking of the suppliers is determined by using GRA. Finally to show the robustness of the model, a sensitivity analysis is also performed. In this study, the supplier selection problem of an electroplating industry in the southern part of India was investigated, demonstrating the effectiveness of this developed integrated model. This model can help in solving the complex decision in supplier selection practice. The results generated from the model are properly validated and finally a systematic solution with decision support is provided for decision makers. This model can be integrated with other decision support systems of similar kinds of industries. © 2013 Copyright Taylor and Francis Group, LLC.","fuzzy analytic hierarchy process; grey relational analysis; hybrid model; multi-criteria decision making; supplier selection","Fuzzy analytic hierarchy process; Grey relational analysis; Hybrid model; Multi-criteria decision making; Supplier selection; Artificial intelligence; Decision making; Decision support systems; Electroplating shops; Models",Article,Scopus,2-s2.0-84880507260
"Liu H.-C., Liu L., Lin Q.-L., Liu N.","Knowledge acquisition and representation using fuzzy evidential reasoning and dynamic adaptive fuzzy petri nets",2013,"IEEE Transactions on Cybernetics",29,10.1109/TSMCB.2012.2223671,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888212025&doi=10.1109%2fTSMCB.2012.2223671&partnerID=40&md5=b431aa938cca18c6e4f0090823d1ed1c","The two most important issues of expert systems are the acquisition of domain experts' professional knowledge and the representation and reasoning of the knowledge rules that have been identified. First, during expert knowledge acquisition processes, the domain expert panel often demonstrates different experience and knowledge from one another and produces different types of knowledge information such as complete and incomplete, precise and imprecise, and known and unknown because of its cross-functional and multidisciplinary nature. Second, as a promising tool for knowledge representation and reasoning, fuzzy Petri nets (FPNs) still suffer a couple of deficiencies. The parameters in current FPN models could not accurately represent the increasingly complex knowledge-based systems, and the rules in most existing knowledge inference frameworks could not be dynamically adjustable according to propositions' variation as human cognition and thinking. In this paper, we present a knowledge acquisition and representation approach using the fuzzy evidential reasoning approach and dynamic adaptive FPNs to solve the problems mentioned above. As is illustrated by the numerical example, the proposed approach can well capture experts' diversity experience, enhance the knowledge representation power, and reason the rule-based knowledge more intelligently. © 2012 IEEE.","Evidential reasoning (ER) approach; Expert systems; Fuzzy petri nets (FPNs); Knowledge acquisition","Evidential reasoning approaches; Fuzzy evidential reasoning; Fuzzy Petri nets; Knowledge acquisition and representations; Knowledge information; Knowledge representation and reasoning; Professional knowledge; Rule-based knowledge; Expert systems; Fuzzy systems; Knowledge representation; Petri nets; Knowledge acquisition; algorithm; artificial intelligence; artificial neural network; automated pattern recognition; decision support system; fuzzy logic; procedures; article; automated pattern recognition; methodology; Algorithms; Artificial Intelligence; Decision Support Techniques; Fuzzy Logic; Neural Networks (Computer); Pattern Recognition, Automated; Algorithms; Artificial Intelligence; Decision Support Techniques; Fuzzy Logic; Neural Networks (Computer); Pattern Recognition, Automated",Article,Scopus,2-s2.0-84888212025
"Rodriguez F.J., Lozano M., García-Martínez C., González-Barrera J.D.","An artificial bee colony algorithm for the maximally diverse grouping problem",2013,"Information Sciences",29,10.1016/j.ins.2012.12.020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874115377&doi=10.1016%2fj.ins.2012.12.020&partnerID=40&md5=c58e3e3e9a23172b7f22b9b1b25f9de2","In this paper, an artificial bee colony algorithm is proposed to solve the maximally diverse grouping problem. This complex optimisation problem consists of forming maximally diverse groups with restricted sizes from a given set of elements. The artificial bee colony algorithm is a new swarm intelligence technique based on the intelligent foraging behaviour of honeybees. The behaviour of this algorithm is determined by two search strategies: an initialisation scheme employed to construct initial solutions and a method for generating neighbouring solutions. More specifically, the proposed approach employs a greedy constructive method to accomplish the initialisation task and also employs different neighbourhood operators inspired by the iterated greedy algorithm. In addition, it incorporates an improvement procedure to enhance the intensification capability. Through an analysis of the experimental results, the highly effective performance of the proposed algorithm is shown in comparison to the current state-of-the-art algorithms which address the problem. © 2013 Elsevier Inc. All rights reserved.","Artificial bee colony algorithm; Iterated greedy; Maximally diverse grouping problem","Artificial bee colony algorithms; Constructive methods; Effective performance; Foraging behaviours; Grouping problem; Initial solution; Iterated greedy; Iterated greedy algorithm; Neighbourhood; Optimisation problems; Search strategies; State-of-the-art algorithms; Swarm intelligence techniques; Artificial intelligence; Algorithms",Article,Scopus,2-s2.0-84874115377
"Lu X., Wang Y., Yuan Y.","Sparse coding from a bayesian perspective",2013,"IEEE Transactions on Neural Networks and Learning Systems",29,10.1109/TNNLS.2013.2245914,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876133796&doi=10.1109%2fTNNLS.2013.2245914&partnerID=40&md5=08a29ecba1cb2c886adb466309c0ed91","Sparse coding is a promising theme in computer vision. Most of the existing sparse coding methods are based on either ℓ0 or ℓ1 penalty, which often leads to unstable solution or biased estimation. This is because of the nonconvexity and discontinuity of the ℓ0 penalty and the over-penalization on the true large coefficients of the ℓ1 penalty. In this paper, sparse coding is interpreted from a novel Bayesian perspective, which results in a new objective function through maximum a posteriori estimation. The obtained solution of the objective function can generate more stable results than the ℓ0 penalty and smaller reconstruction errors than the ℓ1 penalty. In addition, the convergence property of the proposed algorithm for sparse coding is also established. The experiments on applications in single image super-resolution and visual tracking demonstrate that the proposed method is more effective than other state-of-the-art methods. © 2012 IEEE.","Bayesian; compressive sensing (CS); computer vision; maximum a posteriori (MAP); sparse coding","Bayesian; Compressive sensing; Convergence properties; Maximum a posteriori; Maximum a posteriori estimation; Reconstruction error; Sparse coding; State-of-the-art methods; Artificial intelligence; Computer vision; Computer networks",Article,Scopus,2-s2.0-84876133796
"Jones J.A., Swan III J.E., Bolas M.","Peripheral stimulation and its effect on perceived spatial scale in virtual environments",2013,"IEEE Transactions on Visualization and Computer Graphics",29,10.1109/TVCG.2013.37,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883481070&doi=10.1109%2fTVCG.2013.37&partnerID=40&md5=39af15f5776213fc05317dd2e483a2e4","The following series of experiments explore the effect of static peripheral stimulation on the perception of distance and spatial scale in a typical head-mounted virtual environment. It was found that applying constant white light in an observer's far periphery enabled the observer to more accurately judge distances using blind walking. An effect of similar magnitude was also found when observers estimated the size of a virtual space using a visual scale task. The presence of the effect across multiple psychophysical tasks provided confidence that a perceptual change was, in fact, being invoked by the addition of the peripheral stimulation. These results were also compared to observer performance in a very large field of view virtual environment and in the real world. The subsequent findings raise the possibility that distance judgments in virtual environments might be considerably more similar to those in the real world than previous work has suggested. © 2013 IEEE.","Distance judgments; Field of view; Periphery; Spatial perception; Virtual environments","Distance judgments; Field of views; Observer performance; Periphery; Psychophysical; Spatial perception; Very large field of view; Virtual spaces; Computer graphics; Software engineering; Virtual reality; adult; algorithm; article; artificial intelligence; association; computer assisted diagnosis; computer interface; female; human; image enhancement; methodology; photostimulation; physiology; three dimensional imaging; vision; Adult; Algorithms; Artificial Intelligence; Cues; Female; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Photic Stimulation; User-Computer Interface; Visual Perception",Article,Scopus,2-s2.0-84883481070
"Mehrabi M., Sharifpur M., Meyer J.P.","Viscosity of nanofluids based on an artificial intelligence model",2013,"International Communications in Heat and Mass Transfer",29,10.1016/j.icheatmasstransfer.2013.02.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875813215&doi=10.1016%2fj.icheatmasstransfer.2013.02.008&partnerID=40&md5=ee78c90b797d9055bb25263dea8433be","By using an FCM-based Adaptive neuro-fuzzy inference system (FCM-ANFIS) and a set of experimental data, models were developed to predict the effective viscosity of nanofluids. The effective viscosity was selected as the target parameter, and the volume concentration, temperature and size of the nanoparticles were considered as the input (design) parameters. To model the viscosity, experimental data from literature were divided into two sets: a train and a test data set. The model was instructed by the train set and the results were compared with the experimental data set. The predicted viscosities were compared with experimental data for four nanofluids, which were Al2O3, CuO, TiO2 and SiO2, and with water as base fluid. The viscosities were also compared with several of the most cited correlations in literature. The results, which were obtained by the proposed FCM-ANFIS model, in general compared very well with the experimental measurement. © 2013 Elsevier Ltd.","Effective viscosity; FCM-based adaptive neuro-fuzzy inference system (FCM-ANFIS); Nanofluid; Particle size; Temperature; Volume concentration","Adaptive neuro-fuzzy inference system; Effective viscosity; Experimental datum; Experimental measurements; Nanofluids; Target parameter; Test data; Volume concentration; Artificial intelligence; Fuzzy systems; Particle size; Temperature; Tracking (position); Viscosity; Nanofluidics",Article,Scopus,2-s2.0-84875813215
"Li P., Li H., Wu M.","Multi-label ensemble based on variable pairwise constraint projection",2013,"Information Sciences",29,10.1016/j.ins.2012.07.066,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870054699&doi=10.1016%2fj.ins.2012.07.066&partnerID=40&md5=48c1ce72319feb6508658fa7e3fbe34f","Multi-label classification has attracted an increasing amount of attention in recent years. To this end, many algorithms have been developed to classify multi-label data in an effective manner. However, they usually do not consider the pairwise relations indicated by sample labels, which actually play important roles in multi-label classification. Inspired by this, we naturally extend the traditional pairwise constraints to the multi-label scenario via a flexible thresholding scheme. Moreover, to improve the generalization ability of the classifier, we adopt a boosting-like strategy to construct a multi-label ensemble from a group of base classifiers. To achieve these goals, this paper presents a novel multi-label classification framework named Variable Pairwise Constraint projection for Multi-label Ensemble (VPCME). Specifically, we take advantage of the variable pairwise constraint projection to learn a lower-dimensional data representation, which preserves the correlations between samples and labels. Thereafter, the base classifiers are trained in the new data space. For the boosting-like strategy, we employ both the variable pairwise constraints and the bootstrap steps to diversify the base classifiers. Empirical studies have shown the superiority of the proposed method in comparison with other approaches. © 2012 Elsevier Inc. All rights reserved.","Boosting; Constraint projection; Ensemble learning; Multi-label classification; Variable pairwise constraints","Boosting; Constraint projection; Ensemble learning; Multi-label; Pairwise constraints; Artificial intelligence; Software engineering",Article,Scopus,2-s2.0-84870054699
"Shabani M.O., Mazahery A.","Application of GA to optimize the process conditions of Al Matrix nano-composites",2013,"Composites Part B: Engineering",29,10.1016/j.compositesb.2012.07.045,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869507853&doi=10.1016%2fj.compositesb.2012.07.045&partnerID=40&md5=88332dc22d9b30e7bed3f7745fd01bac","In this study, an effective approach based on genetic algorithm (GA), swarm intelligence optimization and finite element method (FEM) was implemented in order to model and optimize the process conditions of Al Matrix nano-composites. The nano-ceramic particles were added into the aluminum alloy to experimentally investigate the microstructure and mechanical behavior of metal matrix nano-composites (MMNCs). Inspired by the idea of breeding swarms, this paper proposes a GA/PSO hybrid algorithm, which combines the standard velocity and position update rules of PSO with the ideas of selection, crossover and mutation from GA. The experimental results of this project were compared with the modeled ones indicating the efficiency of the proposed model to estimate the optimal process conditions in fabrication of the nano-composite via casting. © 2012 Elsevier Ltd. All rights reserved.","A. Metal matrix composites (MMCs); C. Computational modeling; E. Casting","Computational modeling; Crossover and mutation; E. Casting; Finite element method FEM; Hybrid algorithms; Mechanical behavior; Metal matrices; Metal matrix composites; Nano ceramics; Optimal process; Process condition; Swarm intelligence optimization; Artificial intelligence; Finite element method; Metallic matrix composites; Nanocomposites; Optimization; Genetic algorithms",Article,Scopus,2-s2.0-84869507853
"Barisal A.K.","Dynamic search space squeezing strategy based intelligent algorithm solutions to economic dispatch with multiple fuels",2013,"International Journal of Electrical Power and Energy Systems",29,10.1016/j.ijepes.2012.08.049,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866979316&doi=10.1016%2fj.ijepes.2012.08.049&partnerID=40&md5=b70865908a97d46243dfd04c0520d886","This paper presents a new approach to the solution of optimal power generation for economic dispatch (ED) using improved particle swarm optimization (IPSO) technique. In this paper an improved PSO technique is suggested that deals with equality and inequality constraints in ED problems. A constraint treatment mechanism called dynamic search space squeezing strategy is devised to accelerate the optimization process and simultaneously the dynamic process inherent in the conventional PSO algorithm is preserved. The application and statistical performance of various intelligent algorithms such as differential evolution (DE), particle swarm optimization (PSO) and improved particle swarm optimization (IPSO) are considered on economic dispatch problems with non-smooth cost functions considering valve point effects and multiple fuel options. To determine the efficiency and effectiveness of various intelligent algorithms, three experiments are conducted considering only multiple fuel options, considering both valve-point and multiple fuel options and also taking into account the valve point loadings, ramp rate limits and prohibited operating zones. The simulation results reveal that the proposed IPSO has provided the better solution with a very high probability to demonstrate its robustness over other intelligent techniques such as DE, PSO and improved genetic algorithm with multiplier updating (IGA-MU), ant colony optimization (ACO), artificial bee colony algorithm (ABC), hybrid swarm intelligent based harmony search algorithm (HHS) and fuzzy adaptive chaotic ant swarm optimization (FCASO). The proposed IPSO ensures convergence within least execution time and provides quality solutions as compared to earlier reported best results. © 2012 Elsevier Ltd. All rights reserved.","Differential evolution; Dynamic search space squeezing strategy; Economic dispatch; Particle swarm optimization","Ant Colony Optimization (ACO); Artificial bee colony algorithms; Chaotic ant swarm; Differential Evolution; Dynamic process; Dynamic search; Economic Dispatch; Economic dispatch problems; Execution time; Fuzzy adaptive; Harmony search algorithms; High probability; Improved PSO; Inequality constraint; Intelligent Algorithms; Intelligent techniques; Multiple fuel options; Multiple fuels; Multiplier updating; Non-smooth cost functions; Optimization process; Prohibited operating zone; PSO algorithms; Ramp rate limits; Statistical performance; Swarm intelligent; Valve point effects; Valve point loading; Artificial intelligence; Constraint theory; Particle swarm optimization (PSO); Reactive power; Scheduling; Algorithms",Article,Scopus,2-s2.0-84866979316
"Beliakov G., James S.","On extending generalized Bonferroni means to Atanassov orthopairs in decision making contexts",2013,"Fuzzy Sets and Systems",29,10.1016/j.fss.2012.03.018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867900567&doi=10.1016%2fj.fss.2012.03.018&partnerID=40&md5=b3052d5f6910f7e0e16ef63987185f71","Extensions of aggregation functions to Atanassov orthopairs (often referred to as intuitionistic fuzzy sets or AIFS) usually involve replacing the standard arithmetic operations with those defined for the membership and non-membership orthopairs. One problem with such constructions is that the usual choice of operations has led to formulas which do not generalize the aggregation of ordinary fuzzy sets (where the membership and non-membership values add to 1). Previous extensions of the weighted arithmetic mean and ordered weighted averaging operator also have the absorbent element 〈1,0〉, which becomes particularly problematic in the case of the Bonferroni mean, whose generalizations are useful for modeling mandatory requirements. As well as considering the consistency and interpretability of the operations used for their construction, we hold that it is also important for aggregation functions over higher order fuzzy sets to exhibit analogous behavior to their standard definitions. After highlighting the main drawbacks of existing Bonferroni means defined for Atanassov orthopairs and interval data, we present two alternative methods for extending the generalized Bonferroni mean. Both lead to functions with properties more consistent with the original Bonferroni mean, and which coincide in the case of ordinary fuzzy values. © 2012 Elsevier B.V.","Aggregation operators; Atanassov intuitionistic fuzzy sets; Interval valued fuzzy sets; Means","Aggregation functions; Aggregation operator; Alternative methods; Arithmetic mean; Arithmetic operations; Interpretability; Interval data; Interval-valued fuzzy sets; Intuitionistic fuzzy sets; Mandatory requirement; Means; Ordered weighted averaging operator; Standard definitions; Artificial intelligence; Fuzzy sets",Article,Scopus,2-s2.0-84867900567
"Salama K.M., Abdelbar A.M., Otero F.E.B., Freitas A.A.","Utilizing multiple pheromones in an ant-based algorithm for continuous-attribute classification rule discovery",2013,"Applied Soft Computing Journal",29,10.1016/j.asoc.2012.07.026,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869412276&doi=10.1016%2fj.asoc.2012.07.026&partnerID=40&md5=1bcdd08ea5ce9711c48f08880b4344e1","The cAnt-Miner algorithm is an Ant Colony Optimization (ACO) based technique for classification rule discovery in problem domains which include continuous attributes. In this paper, we propose several extensions to cAnt-Miner. The main extension is based on the use of multiple pheromone types, one for each class value to be predicted. In the proposed μcAnt-Miner algorithm, an ant first selects a class value to be the consequent of a rule and the terms in the antecedent are selected based on the pheromone levels of the selected class value; pheromone update occurs on the corresponding pheromone type of the class value. The pre-selection of a class value also allows the use of more precise measures for the heuristic function and the dynamic discretization of continuous attributes, and further allows for the use of a rule quality measure that directly takes into account the confidence of the rule. Experimental results on 20 benchmark datasets show that our proposed extension improves classification accuracy to a statistically significant extent compared to cAnt-Miner, and has classification accuracy similar to the well-known Ripper and PART rule induction algorithms. © 2012 Elsevier B.V. All rights reserved.","Ant Colony Optimization; Biologically inspired computing; Classification rule discovery","Ant Colony Optimization (ACO); Benchmark datasets; Biologically inspired; Classification accuracy; Classification Rule Discovery; Continuous attribute; Discretizations; Heuristic functions; Pheromone levels; Pre-selection; Problem domain; Quality measures; Rule induction algorithms; Artificial intelligence; Heuristic algorithms; Miners; Classification (of information)",Article,Scopus,2-s2.0-84869412276
"Basha T., Moses Y., Kiryati N.","Multi-view scene flow estimation: A view centered variational approach",2013,"International Journal of Computer Vision",29,10.1007/s11263-012-0542-7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873191031&doi=10.1007%2fs11263-012-0542-7&partnerID=40&md5=9ebd9d92e71e1b72d89a0ab3568c2565","We present a novel method for recovering the 3D structure and scene flow from calibrated multi-view sequences. We propose a 3D point cloud parametrization of the 3D structure and scene flow that allows us to directly estimate the desired unknowns. A unified global energy functional is proposed to incorporate the information from the available sequences and simultaneously recover both depth and scene flow. The functional enforces multi-view geometric consistency and imposes brightness constancy and piecewise smoothness assumptions directly on the 3D unknowns. It inherently handles the challenges of discontinuities, occlusions, and large displacements. The main contribution of this work is the fusion of a 3D representation and an advanced variational framework that directly uses the available multi-view information. This formulation allows us to advantageously bind the 3D unknowns in time and space. Different from optical flow and disparity, the proposed method results in a nonlinear mapping between the images' coordinates, thus giving rise to additional challenges in the optimization process. Our experiments on real and synthetic data demonstrate that the proposed method successfully recovers the 3D structure and scene flow despite the complicated nonconvex optimization problem. © 2012 Springer Science+Business Media, LLC.","3D structure; Multiple view; Scene flow","3D point cloud; 3D Structure; Brightness constancies; Global energy; Large displacements; Multi-views; Multiple views; Nonconvex optimization problem; Nonlinear mappings; Optimization process; Parametrizations; Piecewise smoothness; Scene flow; Synthetic data; Variational approaches; Variational framework; Artificial intelligence; Software engineering; Three dimensional computer graphics",Article,Scopus,2-s2.0-84873191031
"Bai J., Yang G.-K., Chen Y.-W., Hu L.-S., Pan C.-C.","A model induced max-min ant colony optimization for asymmetric traveling salesman problem",2013,"Applied Soft Computing Journal",29,10.1016/j.asoc.2012.04.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881664722&doi=10.1016%2fj.asoc.2012.04.008&partnerID=40&md5=2ce69d4b65f2e67cb93887a9d76e40d6","A large number of hybrid metaheuristics for asymmetric traveling salesman problem (ATSP) have been proposed in the past decades which produced better solutions by exploiting the complementary characteristics of different optimization strategies. However, most of the hybridizations are criticized due to lacking of sufficient analytical basis. In this paper, a model induced max-min ant colony optimization (MIMM-ACO) is proposed to bridge the gap between hybridizations and theoretical analysis. The proposed method exploits analytical knowledge from both the ATSP model and the dynamics of ACO guiding the behavior of ants which forms the theoretical basis for the hybridization. The contribution of this paper mainly includes three supporting propositions that lead to two improvements in comparison with classical max-min ACO optimization (MM-ACO): (1) Adjusted transition probabilities are developed by replacing the static biased weighting factors with the dynamic ones which are determined by the partial solution that ant has constructed. As a byproduct, nonoptimal arcs will be indentified and excluded from further consideration based on the dual information derived from solving the associated assignment problem (AP). (2) A terminal condition is determined analytically based on the state of pheromone matrix structure rather than intuitively as in most traditional hybrid metaheuristics. Apart from the theoretical analysis, we experimentally show that the proposed algorithm exhibits more powerful searching ability than classical MM-ACO and outperforms state of art hybrid metaheuristics. © 2012 Elsevier B.V. All rights reserved.","Ant colony optimization; ATSP; Model induced","Ant colony optimization; Artificial intelligence; Combinatorial optimization; Heuristic algorithms; Optimization; Analytical knowledge; Assignment problems; Asymmetric traveling salesman problem; ATSP; Complementary characteristics; Hybrid metaheuristics; Optimization strategy; Transition probabilities; Traveling salesman problem",Article,Scopus,2-s2.0-84881664722
"Cobanoglu M.C., Liu C., Hu F., Oltvai Z.N., Bahar I.","Predicting drug-target interactions using probabilistic matrix factorization",2013,"Journal of Chemical Information and Modeling",28,10.1021/ci400219z,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896508914&doi=10.1021%2fci400219z&partnerID=40&md5=3c5fd3822eeef332b3e2bd5572b9b133","Quantitative analysis of known drug-target interactions emerged in recent years as a useful approach for drug repurposing and assessing side effects. In the present study, we present a method that uses probabilistic matrix factorization (PMF) for this purpose, which is particularly useful for analyzing large interaction networks. DrugBank drugs clustered based on PMF latent variables show phenotypic similarity even in the absence of 3D shape similarity. Benchmarking computations show that the method outperforms those recently introduced provided that the input data set of known interactions is sufficiently large - which is the case for enzymes and ion channels, but not for G-protein coupled receptors (GPCRs) and nuclear receptors. Runs performed on DrugBank after hiding 70% of known interactions show that, on average, 88 of the top 100 predictions hit the hidden interactions. De novo predictions permit us to identify new potential interactions. Drug-target pairs implicated in neurobiological disorders are overrepresented among de novo predictions. © 2013 American Chemical Society.",,"Forecasting; 3D shape similarities; Drug-target interactions; G-protein coupled receptors; Interaction networks; Latent variable; Nuclear receptors; Probabilistic matrix factorizations; Side effect; Drug interactions; cell receptor; G protein coupled receptor; ion channel; prescription drug; algorithm; article; artificial intelligence; binding site; chemistry; cluster analysis; drug antagonism; drug database; drug development; drug potentiation; drug repositioning; human; probability; protein database; Algorithms; Artificial Intelligence; Binding Sites; Cluster Analysis; Databases, Pharmaceutical; Databases, Protein; Drug Discovery; Drug Repositioning; Humans; Ion Channels; Prescription Drugs; Probability; Receptors, Cytoplasmic and Nuclear; Receptors, G-Protein-Coupled",Article,Scopus,2-s2.0-84896508914
"Zhang B., Huang Y., McDermott J.E., Posey R.H., Xu H., Zhao Z.","Interdisciplinary dialogue for education, collaboration, and innovation: Intelligent Biology and Medicine in and beyond 2013",2013,"BMC Genomics",28,10.1186/1471-2164-14-S8-S1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889671824&doi=10.1186%2f1471-2164-14-S8-S1&partnerID=40&md5=f012e3a00bfdc3c7ca4ab6fafb1989a9","The 2013 International Conference on Intelligent Biology and Medicine (ICIBM 2013) was held on August 11-13, 2013 in Nashville, Tennessee, USA. The conference included six scientific sessions, two tutorial sessions, one workshop, two poster sessions, and four keynote presentations that covered cutting-edge research topics in bioinformatics, systems biology, computational medicine, and intelligent computing. Here, we present a summary of the conference and an editorial report of the supplements to BMC Genomics and BMC Systems Biology that include 19 research papers selected from ICIBM 2013. © 2013 Zhang et al.; licensee BioMed Central Ltd.",,"article; bioinformatics; camel; cancer genetics; cancer research; computational medicine; data analysis; drug response; genetic procedures; genetic variability; genome analysis; human; intelligent computing; interdisciplinary research; international cooperation; medical informatics; medicine; molecular biology; next generation sequencing; nonhuman; pharmacogenomics; proteomics; quantitative trait locus mapping; signal transduction; single nucleotide polymorphism; systems biology; United States; Artificial Intelligence; Computational Biology; Genomics; Humans; Medicine; Systems Biology",Article,Scopus,2-s2.0-84889671824
"Marcolino L.S., Jiang A.X., Tambe M.","Multi-agent team formation: Diversity beats strength?",2013,"IJCAI International Joint Conference on Artificial Intelligence",28,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063202&partnerID=40&md5=63cb9157a62b32b7135f838eea306320","Team formation is a critical step in deploying a multi-agent team. In some scenarios, agents coordinate by voting continuously. When forming such teams, should we focus on the diversity of the team or on the strength of each member? Can a team of diverse (and weak) agents outperform a uniform team of strong agents? We propose a new model to address these questions. Our key contributions include: (i) we show that a diverse team can overcome a uniform team and we give the necessary conditions for it to happen; (ii) we present optimal voting rules for a diverse team; (iii) we perform synthetic experiments that demonstrate that both diversity and strength contribute to the performance of a team; (iv) we show experiments that demonstrate the usefulness of our model in one of the most difficult challenges for Artificial Intelligence: Computer Go.",,"Computer Go; Critical steps; Multiagent teams; Synthetic experiments; Team formation; Voting rules; Experiments; Intelligent agents; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84896063202
"Gebser M., Kaufmann B., Otero R., Romero J., Schaub T., Wanko P.","Domain-specific heuristics in answer set programming",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",28,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893348717&partnerID=40&md5=6128455c7647199cf3ea26db2c799aa3","We introduce a general declarative framework for incorporating domain-specific heuristics into ASP solving. We accomplish this by extending the first-order modeling language of ASP by a distinguished heuristic predicate. The resulting heuristic information is processed as an equitable part of the logic program and subsequently exploited by the solver when it comes to non-deterministically assigning a truth value to an atom. We implemented our approach as a dedicated heuristic in the ASP solver clasp and show its great prospect by an empirical evaluation. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Answer set programming; Domain specific; Empirical evaluations; First-order models; Heuristic information; Logic programs; Truth values; Artificial intelligence; Computer simulation languages; Logic programming; Heuristic programming",Conference Paper,Scopus,2-s2.0-84893348717
"Juang C.-F., Chen C.-Y.","Data-driven interval type-2 neural fuzzy system with high learning accuracy and improved model interpretability",2013,"IEEE Transactions on Cybernetics",28,10.1109/TSMCB.2012.2230253,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890064120&doi=10.1109%2fTSMCB.2012.2230253&partnerID=40&md5=650d09979ea33a4fc4d8d86614ac5a5a","Current studies of type-2 neural fuzzy systems (FSs) (NFSs) primarily focus on building a fuzzy model with high accuracy and disregard the interpretability of fuzzy rules. This paper proposes a data-driven interval type-2 (IT2) NFS with improved model interpretability (DIT2NFS-IP). The DIT2NFS-IP uses IT2 fuzzy sets in its antecedent part and intervals in its zero-order Takagi-Sugeno-Kang-type consequent part for rule form simplicity. The initial rule base is generated by a self-splitting clustering algorithm in the input-output space. The DIT2NFS-IP uses a two-phase parameter-learning algorithm to design an accurate model with improved rule interpretability. In the first phase, a new cost function that considers both accuracy and transparent fuzzy set partition is defined. The antecedent and consequent parameters are learned through gradient descent and rule-ordered recursive least squares algorithms, respectively, to achieve cost function minimization. The second phase performs a fuzzy set reduction, followed by consequent parameter learning to improve accuracy. Comparisons with different type-1 and type-2 FSs in five databased modeling and prediction problems verify the performance of the DIT2NFS-IP in both model accuracy and interpretability. © 2013 IEEE.","Fuzzy neural networks (FNNs); interpretable fuzzy systems (FSs); sequence prediction; type-2 FSs","Fuzzy neural network (FNNs); Input-output spaces; interpretable fuzzy systems (FSs); Neural fuzzy systems; Parameter learning; Recursive least squares algorithms; Sequence prediction; type-2 FSs; Clustering algorithms; Fuzzy neural networks; Fuzzy sets; Learning algorithms; Fuzzy systems; algorithm; article; artificial intelligence; automated pattern recognition; computer simulation; data mining; factual database; fuzzy logic; methodology; statistical model; Algorithms; Artificial Intelligence; Computer Simulation; Data Mining; Databases, Factual; Fuzzy Logic; Models, Statistical; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84890064120
"Bienvenu M., Lutz C., Wolter F.","First-order rewritability of atomic queries in Horn description logics",2013,"IJCAI International Joint Conference on Artificial Intelligence",28,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896061561&partnerID=40&md5=6ee5a3a269a467a1e572ee2027e1b916","One of the most advanced approaches to querying data in the presence of ontologies is to make use of relational database systems, rewriting the original query and the ontology into a new query that is formulated in SQL or, equivalently, in first-order logic (FO). For ontologies written in many standard description logics (DLs), however, such FO-rewritings are not guaranteed to exist. We study FO-rewritings and their existence for a basic class of queries and for ontologies formulated in Horn DLs such as Horn-SHI and EL. Our results include characterizations of the existence of FO-rewritings, tight complexity bounds for deciding whether an FO-rewriting exists (EXPTIME and PSPACE), and tight bounds on the (worst-case) size of FO-rewritings, when presented as a union of conjunctive queries.",,"Complexity bounds; Conjunctive queries; Description logic; Exptime; First-order; First-order logic; Tight bound; Artificial intelligence; Formal languages; Relational database systems; Data description",Conference Paper,Scopus,2-s2.0-84896061561
"Maddalena L., Petrosino A.","Stopped object detection by learning foreground model in videos",2013,"IEEE Transactions on Neural Networks and Learning Systems",28,10.1109/TNNLS.2013.2242092,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884969879&doi=10.1109%2fTNNLS.2013.2242092&partnerID=40&md5=576d8bebea722fcbb682e9e07956a6d2","The automatic detection of objects that are abandoned or removed in a video scene is an interesting area of computer vision, with key applications in video surveillance. Forgotten or stolen luggage in train and airport stations and irregularly parked vehicles are examples that concern significant issues, such as the fight against terrorism and crime, and public safety. Both issues involve the basic task of detecting static regions in the scene. We address this problem by introducing a model-based framework to segment static foreground objects against moving foreground objects in single view sequences taken from stationary cameras. An image sequence model, obtained by learning in a self-organizing neural network image sequence variations, seen as trajectories of pixels in time, is adopted within the model-based framework. Experimental results on real video sequences and comparisons with existing approaches show the accuracy of the proposed stopped object detection approach. © 2013 IEEE.","Artificial neural network; Image sequence modeling; Stopped foreground detection; Video surveillance","Automatic Detection; Foreground detection; Foreground modeling; Foreground objects; Image sequence modeling; Real video sequences; Self-organizing neural network; Video surveillance; Computer vision; Neural networks; Object recognition; Security systems; artificial intelligence; automated pattern recognition; human; image enhancement; image subtraction; nonlinear system; procedures; signal processing; videorecording; Artificial Intelligence; Humans; Image Enhancement; Nonlinear Dynamics; Pattern Recognition, Automated; Signal Processing, Computer-Assisted; Subtraction Technique; Video Recording",Article,Scopus,2-s2.0-84884969879
"Liu J., Zheng S., Tan Y.","The improvement on controlling exploration and exploitation of firework algorithm",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",28,10.1007/978-3-642-38703-6_2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884827482&doi=10.1007%2f978-3-642-38703-6_2&partnerID=40&md5=02aa92c955d371689f590e92f127de5e","Firework algorithm (FWA) is a new Swarm Intelligence (SI) based optimization technique, which presents a different search manner and simulates the explosion of fireworks to search the optimal solution of problem. Since it was proposed, fireworks algorithm has shown its significance and superiority in dealing with the optimization problems. However, the calculation of number of explosion spark and amplitude of firework explosion of FWA should dynamically control the exploration and exploitation of searching space with iteration. The mutation operator of FWA needs to generate the search diversity. This paper provides a kind of new method to calculate the number of explosion spark and amplitude of firework explosion. By designing a transfer function, the rank number of firework is mapped to scale of the calculation of scope and spark number of firework explosion. A parameter is used to dynamically control the exploration and exploitation of FWA with iteration going on. In addition, this paper uses a new random mutation operator to control the diversity of FWA search. The modified FWA have improved the performance of original FWA. By experiment conducted by the standard benchmark functions, the performance of improved FWA can match with that of particle swarm optimization (PSO). © 2013 Springer-Verlag Berlin Heidelberg.","Exploration and Exploitation; Firework Algorithm; PSO; Swarm Intelligence Algorithm","Benchmark functions; Exploration and exploitation; Fireworks algorithms; Optimization problems; Optimization techniques; PSO; Swarm Intelligence; Swarm intelligence algorithms; Algorithms; Artificial intelligence; Benchmarking; Electric sparks; Explosions; Explosives; Particle swarm optimization (PSO); Iterative methods",Conference Paper,Scopus,2-s2.0-84884827482
"Beloufa F., Chikh M.A.","Design of fuzzy classifier for diabetes disease using Modified Artificial Bee Colony algorithm",2013,"Computer Methods and Programs in Biomedicine",28,10.1016/j.cmpb.2013.07.009,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883746011&doi=10.1016%2fj.cmpb.2013.07.009&partnerID=40&md5=0dd8a875bfe7251b92989e4d6ae012a4","In this study, diagnosis of diabetes disease, which is one of the most important diseases, is conducted with artificial intelligence techniques. We have proposed a novel Artificial Bee Colony (ABC) algorithm in which a mutation operator is added to an Artificial Bee Colony for improving its performance. When the current best solution cannot be updated, a blended crossover operator (BLX-α) of genetic algorithm is applied, in order to enhance the diversity of ABC, without compromising with the solution quality. This modified version of ABC is used as a new tool to create and optimize automatically the membership functions and rules base directly from data. We take the diabetes dataset used in our work from the UCI machine learning repository. The performances of the proposed method are evaluated through classification rate, sensitivity and specificity values using 10-fold cross-validation method. The obtained classification rate of our method is 84.21% and it is very promising when compared with the previous research in the literature for the same problem. © 2013 Elsevier Ireland Ltd.","Artificial Bee Colony; Diabetes disease; Fuzzy rules; Interpretable classification","10-fold cross-validation; Artificial bee colonies; Artificial bee colony algorithms; Artificial bee colony algorithms (ABC); Artificial intelligence techniques; Classification rates; Sensitivity and specificity; UCI machine learning repository; Artificial intelligence; Evolutionary algorithms; Fuzzy rules; Fuzzy sets; Diagnosis; article; Artificial Bee Colony algorithm; artificial intelligence; diabetes mellitus; fuzzy system; genetic algorithm; machine learning; principal component analysis; sensitivity and specificity; validation study; Artificial Bee Colony; Diabetes disease; Fuzzy rules; Interpretable classification; Algorithms; Artificial Intelligence; Databases, Factual; Diabetes Mellitus; Diagnosis, Computer-Assisted; Fuzzy Logic; Humans; Software Design",Article,Scopus,2-s2.0-84883746011
"Gabryel M., Nowicki R.K., Woźniak M., Kempa W.M.","Genetic cost optimization of the GI/M/1/N finite-buffer queue with a single vacation policy",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",28,10.1007/978-3-642-38610-7_2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884400501&doi=10.1007%2f978-3-642-38610-7_2&partnerID=40&md5=1abca3db05b918c5dc4d247bdbf701d6","In the artice, problem of the cost optimization of the GI/M/1/N-type queue with finite buffer and a single vacation policy is analyzed. Basing on the explicit representation for the joint transform of the first busy period, first idle time and the number of packets transmitted during the first busy period and fixed values of unit costs of the server's functioning an optimal set of system parameters is found for exponentially distributed vacation period and 2-Erlang distribution of inter arrival times. The problem of optimization is solved using genetic algorithm. Different variants of the load of the system are considered as well. © 2013 Springer-Verlag.","Busy period; finite-buffer queue; genetic algorithm; idle time; optimization; single vacation","Busy period; Cost optimization; Explicit representation; Finite buffer queues; Idle time; Inter-arrival time; Single vacation; Vacation period; Artificial intelligence; Costs; Optimization; Queueing theory; Soft computing; Genetic algorithms",Conference Paper,Scopus,2-s2.0-84884400501
"Bos J.W., Costello C., Hisil H., Lauter K.","Fast cryptography in genus 2",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",28,10.1007/978-3-642-38348-9_12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883437294&doi=10.1007%2f978-3-642-38348-9_12&partnerID=40&md5=7756ff44d281194612ad384b2bff083d","In this paper we highlight the benefits of using genus 2 curves in public-key cryptography. Compared to the standardized genus 1 curves, or elliptic curves, arithmetic on genus 2 curves is typically more involved but allows us to work with moduli of half the size. We give a taxonomy of the best known techniques to realize genus 2 based cryptography, which includes fast formulas on the Kummer surface and efficient 4-dimensional GLV decompositions. By studying different modular arithmetic approaches on these curves, we present a range of genus 2 implementations. On a single core of an Intel Core i7-3520M (Ivy Bridge), our implementation on the Kummer surface breaks the 120 thousand cycle barrier which sets a new software speed record at the 128-bit security level for constant-time scalar multiplications compared to all previous genus 1 and genus 2 implementations. © 2013 International Association for Cryptologic Research.",,"Elliptic curve; Genus 2 curves; Intel core i7; Kummer surface; Modular arithmetic; Scalar multiplication; Security level; Artificial intelligence; Computer science; Public key cryptography",Conference Paper,Scopus,2-s2.0-84883437294
"Cayrol C., Lagasquie-Schiex M.-C.","Bipolarity in argumentation graphs: Towards a better understanding",2013,"International Journal of Approximate Reasoning",28,10.1016/j.ijar.2013.03.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878538205&doi=10.1016%2fj.ijar.2013.03.001&partnerID=40&md5=5cdc76ce024ea1709c656cec56413767","Different abstract argumentation frameworks have been used for various applications within multi-agents systems. Among them, bipolar frameworks make use of both attack and support relations between arguments. However, there is no single interpretation of the support, and the handling of bipolarity cannot avoid a deeper analysis of the notion of support. In this paper we consider three recent proposals for specializing the support relation in abstract argumentation: the deductive support, the necessary support and the evidential support. These proposals have been developed independently within different frameworks. We restate these proposals in a common setting, which enables us to undertake a comparative study of the modellings obtained for the three variants of the support. We highlight relationships and differences between these variants, namely a kind of duality between the deductive and the necessary interpretations of the support. © 2013 Elsevier Inc. All rights reserved.","Argumentation theory; Bipolarity in argumentation","Abstract argumentation; Argumentation theory; Bipolarity in argumentation; Comparative studies; Multi-agents systems; Support relations; Artificial intelligence; Software engineering; Problem solving",Conference Paper,Scopus,2-s2.0-84878538205
"Takáč Z.","Inclusion and subsethood measure for interval-valued fuzzy sets and for continuous type-2 fuzzy sets",2013,"Fuzzy Sets and Systems",28,10.1016/j.fss.2013.01.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877697288&doi=10.1016%2fj.fss.2013.01.002&partnerID=40&md5=c5a6dce02ffce31a94726cb98cd343ad","The main aim of this paper is to propose new subsethood measures for continuous, general type-2 fuzzy sets. For this purpose, we introduce inclusions and subsethood measures for interval-valued fuzzy sets first. Then, using an α-plane representation for type-2 fuzzy sets, we extend these inclusions and subsethood measures to general type-2 fuzzy sets. Subsethood measures for interval-valued fuzzy sets (hence, also for type-2 fuzzy sets) rely on already known subsethood measures for ordinary fuzzy sets. We focus on a special subsethood measure for ordinary fuzzy sets, based on α-cut representation, and show, how to compute subsethood measures for continuous type-2 fuzzy sets with no need for discretizing of universe. This is a very interesting and useful property of proposed subsethood measures, which is one of the reason, why our approach has less computational demand than the others. © 2013 Elsevier B.V.","Alpha plane representation; Inclusion; Inclusion indicator; Interval-valued fuzzy sets; Subsethood; Subsethood measure; Type-2 fuzzy sets","Alpha plane; Interval-valued fuzzy sets; Subsethood; Subsethood measures; Type-2 fuzzy set; Artificial intelligence; Inclusions; Fuzzy sets",Article,Scopus,2-s2.0-84877697288
"Li F., Lu X.","Complete synchronization of temporal Boolean networks",2013,"Neural Networks",28,10.1016/j.neunet.2013.03.009,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876334688&doi=10.1016%2fj.neunet.2013.03.009&partnerID=40&md5=45bb5da73940a4b661abdf53e543f513","This letter studies complete synchronization of two temporal Boolean networks coupled in the drive-response configuration. Necessary and sufficient conditions are provided based on the algebraic representation of Boolean networks. Moreover, the upper bound to check the criterion is given. Finally, an illustrative example shows the efficiency of the proposed results. © 2013 Elsevier Ltd.","Complete synchronization; Semi-tensor product; Temporal Boolean network; Time delay","Algebraic representations; Boolean Networks; Complete synchronization; Semi-tensor product; Sufficient conditions; Upper Bound; Artificial intelligence; Cognitive systems; Time delay; Boolean algebra; article; conceptual framework; cortical synchronization; evoked response; executive function; logic; mathematics; nerve cell network; priority journal; temporal Boolean network; Neural Networks (Computer); Time Factors",Article,Scopus,2-s2.0-84876334688
"Goel A.K.","A 30-year case study and 15 principles: Implications of an artificial intelligence methodology for functional modeling",2013,"Artificial Intelligence for Engineering Design, Analysis and Manufacturing: AIEDAM",28,10.1017/S0890060413000218,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880805443&doi=10.1017%2fS0890060413000218&partnerID=40&md5=40e07aebe6f23db3df054efddf4aa3c1","Research on design and analysis of complex systems has led to many functional representations with several meanings of function. This work on conceptual design uses a family of representations called structure-behavior- function (SBF) models. The SBF family ranges from behavior-function models of abstract design patterns to drawing-shape-SBF models that couple SBF models with visuospatial knowledge of technological systems. Development of SBF modeling is an instance of cognitively oriented artificial intelligence research that seeks to understand human cognition and build intelligent agents for addressing complex tasks such as design. This paper first traces the development of SBF modeling as our perspective on design evolved from that of problem solving to that of memory and learning. Next, the development of SBF modeling as a case study is used to abstract some of the core principles of an artificial intelligence methodology for functional modeling. Finally, some implications of the artificial intelligence methodology for different meanings of function are examined. Copyright © Cambridge University Press 2013.","Conceptual design; Functional modeling; Functional reasoning; Functional representation; Structure-behavior-function models; Systems thinking","Artificial intelligence research; Design and analysis; Functional modeling; Functional reasoning; Functional representation; Human cognition; Systems thinking; Technological system; Artificial intelligence; Conceptual design; Intelligent agents; Research; Structural design",Article,Scopus,2-s2.0-84880805443
"Liu Y., Li Z., Xiong H., Gao X., Wu J., Wu S.","Understanding and enhancement of internal clustering validation measures",2013,"IEEE Transactions on Cybernetics",28,10.1109/TSMCB.2012.2220543,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890434201&doi=10.1109%2fTSMCB.2012.2220543&partnerID=40&md5=ff7262a781c26988630236b7e7da78ab","Clustering validation has long been recognized as one of the vital issues essential to the success of clustering applications. In general, clustering validation can be categorized into two classes, external clustering validation and internal clustering validation. In this paper, we focus on internal clustering validation and present a study of 11 widely used internal clustering validation measures for crisp clustering. The results of this study indicate that these existing measures have certain limitations in different application scenarios. As an alternative choice, we propose a new internal clustering validation measure, named clustering validation index based on nearest neighbors (CVNN), which is based on the notion of nearest neighbors. This measure can dynamically select multiple objects as representatives for different clusters in different situations. Experimental results show that CVNN outperforms the existing measures on both synthetic data and real-world data in different application scenarios. © 2012 IEEE.","Clustering validation index based on nearest neighbors (CVNN); Internal clustering validation measure; k-nearest neighbor (kNN)","Application scenario; Clustering applications; Internal clustering validation measure; K nearest neighbor (KNN); Multiple objects; Nearest neighbors; Synthetic data; Validation index; algorithm; artificial intelligence; automated pattern recognition; cluster analysis; computer simulation; procedures; statistical analysis; statistical model; validation study; article; automated pattern recognition; methodology; Algorithms; Artificial Intelligence; Cluster Analysis; Computer Simulation; Data Interpretation, Statistical; Models, Statistical; Pattern Recognition, Automated; Algorithms; Artificial Intelligence; Cluster Analysis; Computer Simulation; Data Interpretation, Statistical; Models, Statistical; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84890434201
"Dornaika F., Bosaghzadeh A.","Exponential local discriminant embedding and its application to face recognition",2013,"IEEE Transactions on Cybernetics",28,10.1109/TSMCB.2012.2218234,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880936263&doi=10.1109%2fTSMCB.2012.2218234&partnerID=40&md5=fe8de2d87ef6b5574fb739af90781757","Local discriminant embedding (LDE) has been recently proposed to overcome some limitations of the global linear discriminant analysis method. In the case of a small training data set, however, LDE cannot directly be applied to high-dimensional data. This case is the so-called small-sample-size (SSS) problem. The classical solution to this problem was applying dimensionality reduction on the raw data (e.g., using principal component analysis). In this paper, we introduce a novel discriminant technique called ""exponential LDE"" (ELDE). The proposed ELDE can be seen as an extension of LDE framework in two directions. First, the proposed framework overcomes the SSS problem without discarding the discriminant information that was contained in the null space of the locality preserving scatter matrices associated with LDE. Second, the proposed ELDE is equivalent to transforming original data into a new space by distance diffusion mapping (similar to kernel-based nonlinear mapping), and then, LDE is applied in such a new space. As a result of diffusion mapping, the margin between samples belonging to different classes is enlarged, which is helpful in improving classification accuracy. The experiments are conducted on five public face databases: Yale, Extended Yale, PF01, Pose, Illumination, and Expression (PIE), and Facial Recognition Technology (FERET). The results show that the performances of the proposed ELDE are better than those of LDE andmany state-of-the-art discriminant analysis techniques. © 2012 IEEE.","Discriminant analysis; Face recognition; Feature extraction; Graph-based embedding; Local discriminant embedding (LDE); Small-sample-size (SSS) problem","Classification accuracy; Dimensionality reduction; Discriminant informations; Graph-based; High dimensional data; Linear discriminant analysis; Local discriminant embedding; Small-sample-size (SSS) problem; Discriminant analysis; Feature extraction; Mapping; Metadata; Principal component analysis; Face recognition; algorithm; anatomy and histology; artificial intelligence; automated pattern recognition; biometry; computer assisted diagnosis; discriminant analysis; face; human; image subtraction; procedures; statistical analysis; article; automated pattern recognition; biometry; computer assisted diagnosis; face; histology; methodology; Algorithms; Artificial Intelligence; Biometry; Data Interpretation, Statistical; Discriminant Analysis; Face; Humans; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Subtraction Technique; Algorithms; Artificial Intelligence; Biometry; Data Interpretation, Statistical; Discriminant Analysis; Face; Humans; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Subtraction Technique",Article,Scopus,2-s2.0-84880936263
"Wilk S., Michalowski W., Michalowski M., Farion K., Hing M.M., Mohapatra S.","Mitigation of adverse interactions in pairs of clinical practice guidelines using constraint logic programming",2013,"Journal of Biomedical Informatics",28,10.1016/j.jbi.2013.01.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875608723&doi=10.1016%2fj.jbi.2013.01.002&partnerID=40&md5=8a412e34ba78a765c0dcdea4daac3934","We propose a new method to mitigate (identify and address) adverse interactions (drug-drug or drug-disease) that occur when a patient with comorbid diseases is managed according to two concurrently applied clinical practice guidelines (CPGs). A lack of methods to facilitate the concurrent application of CPGs severely limits their use in clinical practice and the development of such methods is one of the grand challenges for clinical decision support. The proposed method responds to this challenge.We introduce and formally define logical models of CPGs and other related concepts, and develop the mitigation algorithm that operates on these concepts. In the algorithm we combine domain knowledge encoded as interaction and revision operators using the constraint logic programming (CLP) paradigm. The operators characterize adverse interactions and describe revisions to logical models required to address these interactions, while CLP allows us to efficiently solve the logical models - a solution represents a feasible therapy that may be safely applied to a patient.The mitigation algorithm accepts two CPGs and available (likely incomplete) patient information. It reports whether mitigation has been successful or not, and on success it gives a feasible therapy and points at identified interactions (if any) together with the revisions that address them. Thus, we consider the mitigation algorithm as an alerting tool to support a physician in the concurrent application of CPGs that can be implemented as a component of a clinical decision support system. We illustrate our method in the context of two clinical scenarios involving a patient with duodenal ulcer who experiences an episode of transient ischemic attack. © 2013 Elsevier Inc.","Clinical decision support; Clinical practice guideline; Comorbid diseases; Constraint logic programming; Domain knowledge","Clinical decision support; Clinical decision support systems; Clinical practice guidelines; Clinical practices; Constraint Logic Programming; Domain knowledge; Patient information; Transient ischemic attacks; Algorithms; Artificial intelligence; Decision support systems; Patient treatment; Logic programming; algorithm; article; clinical practice; constraint logic programming; drug safety; duodenum ulcer; feasibility study; human; information processing; knowledge; patient safety; practice guideline; priority journal; transient ischemic attack; Acute Disease; Algorithms; Chronic Disease; Comorbidity; Decision Support Systems, Clinical; Drug Interactions; Drug-Related Side Effects and Adverse Reactions; Humans; Models, Biological; Practice Guidelines as Topic",Article,Scopus,2-s2.0-84875608723
"González A.-B., Rodríguez Ma.-J., Olmos S., Borham M., García F.","Experimental evaluation of the impact of b-learning methodologies on engineering students in Spain",2013,"Computers in Human Behavior",28,10.1016/j.chb.2012.02.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857727695&doi=10.1016%2fj.chb.2012.02.003&partnerID=40&md5=c70b053b20f852750a1c22f85ab041e2","The aim of this article is to highlight the importance of an active learning methodology in engineering degrees in Spain. We present the outcomes of an experimental study carried out during the academic years 2007/2008 and 2008/2009 with engineering students at the University of Salamanca (Spain). In the present research, as we have done in previous ones, we have selected a subject which is common to the four degrees under consideration: Computer Science. This study explores in greater depth the validity of experimental designs coming from educational research and the impact of innovative teaching methodologies. The hypothesis that impulses this research is formulated to ascertain that the learning level and the satisfaction of students will be higher after the implementation of new teaching methodologies (based on constructive learning, collaborative work, and blended learning resources), than in more traditional teaching contexts. The obtained results partially confirm this hypothesis. The ultimate purpose of this work is that of providing evidence that contributes to the improvement of education and teaching methods for a better performance of students in engineering. © 2012 Elsevier Ltd. All rights reserved.","Active learning methodology; Blended-learning; Competence assessment; Experimental design in education; Formative processes in engineering; Learning assessment","Artificial intelligence; Design of experiments; Education; Engineering research; Statistics; Students; Active Learning; Blended learning; Competence assessments; Constructive learning; Educational research; Experimental evaluation; Innovative teaching; Learning assessment; Engineering education; experimental design; experimental study; human; human experiment; learning; satisfaction; Spain; teaching; university; validity",Article,Scopus,2-s2.0-84857727695
"Makonin S., Bartram L., Popowich F.","A smarter smart home: Case studies of ambient intelligence",2013,"IEEE Pervasive Computing",28,10.1109/MPRV.2012.58,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873820910&doi=10.1109%2fMPRV.2012.58&partnerID=40&md5=1df78de1d01338f31a3fd66afaff67e0","Technological support for sustainable home use lies in more subtle and contextually appropriate interventions that integrate informative models of occupant behavior, provide hybrid levels of automated control, and use ambient sensing for localized decisions. © 2002-2012 IEEE.","adaptive HVAC; adaptive lighting; ambient intelligence; pervasive computing; smart home","adaptive HVAC; Adaptive lighting; Ambient intelligence; Ambient sensing; Automated control; Occupant behavior; Smart homes; Sustainable homes; Technological supports; Artificial intelligence; Automation; Ubiquitous computing; Intelligent buildings",Article,Scopus,2-s2.0-84873820910
"Li H., Huang X., He L.","Object matching using a locally affine invariant and linear programming techniques",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",28,10.1109/TPAMI.2012.99,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871758809&doi=10.1109%2fTPAMI.2012.99&partnerID=40&md5=0cdd5479b775c740a3e30cbb2d3b7453","In this paper, we introduce a new matching method based on a novel locally affine-invariant geometric constraint and linear programming techniques. To model and solve the matching problem in a linear programming formulation, all geometric constraints should be able to be exactly or approximately reformulated into a linear form. This is a major difficulty for this kind of matching algorithm. We propose a novel locally affine-invariant constraint which can be exactly linearized and requires a lot fewer auxiliary variables than other linear programming-based methods do. The key idea behind it is that each point in the template point set can be exactly represented by an affine combination of its neighboring points, whose weights can be solved easily by least squares. Errors of reconstructing each matched point using such weights are used to penalize the disagreement of geometric relationships between the template points and the matched points. The resulting overall objective function can be solved efficiently by linear programming techniques. Our experimental results on both rigid and nonrigid object matching show the effectiveness of the proposed algorithm. © 2012 IEEE.","Feature matching; linear programming; locally affine invariant; object matching","Affine invariant; Auxiliary variables; Feature matching; Geometric constraint; Geometric relationships; Least Square; Linear programming formulation; Linear programming techniques; Matched points; Matching algorithm; Matching methods; Matching problems; Neighboring point; Non-rigid objects; Object matching; Objective functions; Point set; Algorithms; Linear programming; Geometry; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; image enhancement; image subtraction; methodology; reproducibility; sensitivity and specificity; signal processing; system analysis; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Programming, Linear; Reproducibility of Results; Sensitivity and Specificity; Signal Processing, Computer-Assisted; Subtraction Technique",Article,Scopus,2-s2.0-84871758809
"Zhou K., Zha H., Song L.","Learning social infectivity in sparse low-rank networks using multi-dimensional hawkes processes",2013,"Journal of Machine Learning Research",28,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954223626&partnerID=40&md5=7a7cd75572712dd7fb19b08f2b24f71e","How will the behaviors of individuals in a social network be influenced by their neighbors, the authorities and the communities in a quantitative way? Such critical and valuable knowledge is unfortunately not readily accessible and we tend to only observe its manifestation in the form of recurrent and time-stamped events occurring at the individuals involved in the social network. It is an important yet challenging problem to infer the underlying network of social inference based on the temporal patterns of those historical events that we can observe. In this paper, we propose a convex optimization approach to discover the hidden network of social influence by modeling the recurrent events at different individuals as multidimensional Hawkes processes, emphasizing the mutual-excitation nature of the dynamics of event occurrence. Furthermore, our estimation procedure, using nuclear and l1 norm regularization simultaneously on the parameters, is able to take into account the prior knowledge of the presence of neighbor interaction, authority influence, and community coordination in the social network. To efficiently solve the resulting optimization problem, we also design an algorithm ADM4 which combines techniques of alternating direction method of multipliers and majorization minimization. We experimented with both synthetic and real world data sets, and showed that the proposed method can discover the hidden network more accurately and produce a better predictive model than several baselines. Copyright 2013 by the authors.",,"Artificial intelligence; Convex optimization; Optimization; Social networking (online); Virtual reality; Alternating direction method of multipliers; Estimation procedures; Multi dimensional; Mutual excitation; Optimization approach; Optimization problems; Predictive modeling; Underlying networks; Economic and social effects",Conference Paper,Scopus,2-s2.0-84954223626
"Ponz-Tienda J.L., Yepes V., Pellicer E., Moreno-Flores J.","The Resource Leveling Problem with multiple resources using an adaptive genetic algorithm",2013,"Automation in Construction",28,10.1016/j.autcon.2012.10.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867895768&doi=10.1016%2fj.autcon.2012.10.003&partnerID=40&md5=4be73907b19233bb58f21b39a660abdc","Resource management ensures that a project is completed on time and at cost, and that its quality is as previously defined; nevertheless, resources are scarce and their use in the activities of the project leads to conflicts in the schedule. Resource leveling problems consider how to make the resource consumption as efficient as possible. This paper presents an Adaptive Genetic Algorithm for the Resource Leveling Problem, and its novelty lies in using the Weibull distribution to establish an estimation of the global optimum as a termination condition. The extension of the project deadline with a penalty is allowed, avoiding the increase in the project criticality. The algorithm is tested with the Project Scheduling Problem Library PSPLIB. The proposed algorithm is implemented using VBA for Excel 2010 to provide a flexible and powerful decision support system that enables practitioners to choose between different feasible solutions to a problem in realistic environments. © 2012 Elsevier B.V. All rights reserved.","Benchmarking; Genetic algorithms; Project scheduling; Resource leveling","Adaptive genetic algorithms; Excel; Feasible solution; Global optimum; Multiple resources; Project scheduling; Project scheduling problem; Realistic environments; Resource consumption; Resource leveling; Resource management; Termination condition; Artificial intelligence; Benchmarking; Decision support systems; Genetic algorithms; Scheduling; Weibull distribution; Algorithms",Article,Scopus,2-s2.0-84867895768
"Sarwate A.D., Chaudhuri K.","Signal processing and machine learning with differential privacy: Algorithms and challenges for continuous data",2013,"IEEE Signal Processing Magazine",28,10.1109/MSP.2013.2259911,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032751978&doi=10.1109%2fMSP.2013.2259911&partnerID=40&md5=032a02044b39ac7693e5280e4c5494d5","Private companies, government entities, and institutions such as hospitals routinely gather vast amounts of digitized personal information about the individuals who are their customers, clients, or patients. Much of this information is private or sensitive, and a key technological challenge for the future is how to design systems and processing techniques for drawing inferences from this large-scale data while maintaining the privacy and security of the data and individual identities. Individuals are often willing to share data, especially for purposes such as public health, but they expect that their identity or the fact of their participation will not be disclosed. In recent years, there have been a number of privacy models and privacy-preserving data analysis algorithms to answer these challenges. In this article, we will describe the progress made on differentially private machine learning and signal processing. © 1991-2012 IEEE.",,"Artificial intelligence; Data reduction; Learning systems; Security of data; Signal processing; Data analysis algorithms; Differential privacies; Government entities; Personal information; Privacy and security; Privacy preserving; Processing technique; Technological challenges; Data privacy",Article,Scopus,2-s2.0-85032751978
"Damianou A.C., Lawrence N.D.","Deep Gaussian processes",2013,"Journal of Machine Learning Research",28,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937432577&partnerID=40&md5=605d4659d3d7b539729b68bf8a2142f6","In this paper we introduce deep Gaussian process (GP) models. Deep GPS are a deep belief network based on Gaussian process mappings. The data is modeled as the output of a multivariate GP. The inputs to that Gaussian process are then governed by another GP. A single layer model is equivalent to a standard GP or the GP latent variable model (GP-LVM). We perform inference in the model by approximate variational marginalization. This results in a strict lower bound on the marginal likelihood of the model which we use for model selection (number of layers and nodes per layer). Deep belief networks are typically applied to relatively large data sets using stochastic gradient descent for optimization. Our fully Bayesian treatment allows for the application of deep models even when data is scarce. Model selection by our variational bound shows that a five layer hierarchy is justified even when modelling a digit data set containing only 150 examples. Copyright 2013 by the authors.",,"Artificial intelligence; Bayesian networks; Gaussian noise (electronic); Stochastic systems; Deep belief networks; Gaussian Processes; Latent variable modeling; Marginal likelihood; Number of layers; Single-layer models; Stochastic gradient descent; Variational bounds; Gaussian distribution",Conference Paper,Scopus,2-s2.0-84937432577
"Chakraborty S., Samanta S., Biswas D., Dey N., Chaudhuri S.S.","Particle Swarm Optimization based parameter optimization technique in medical information hiding",2013,"2013 IEEE International Conference on Computational Intelligence and Computing Research, IEEE ICCIC 2013",28,10.1109/ICCIC.2013.6724173,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894278706&doi=10.1109%2fICCIC.2013.6724173&partnerID=40&md5=c48905af73b27a2479dc94cb101a1294","In this era of globalization, use of technology has influenced medical science as well. Now-a-days, exchanging medical information using communication technologies like network devices or telecommunication to provide health care services for medical case studies amongst various diagnostic centers or hospitals is a very common practice. In this paper, a Discrete Wavelet Transformation (DWT) based method is proposed for embedding a Hospital Logo or Electronic Patient Record (EPR), where the embedding factors/scaling factors are optimized by Particle Swarm Optimization (PSO). © 2013 IEEE.","Biomedical image; Discrete Wavelet Transformation; Electronic Patient Record; Particle Swarm Optimization; Watermarking","Artificial intelligence; Bioinformatics; Digital watermarking; Discrete wavelet transforms; Hospitals; Particle swarm optimization (PSO); Research; Biomedical images; Communication technologies; Discrete wavelet transformation; Electronic patient record; Healthcare services; Medical information; Network devices; Parameter optimization techniques; Image watermarking",Conference Paper,Scopus,2-s2.0-84894278706
"Zhao L., Pan S.J., Xiang E.W., Zhong E., Lu Z., Yang Q.","Active transfer learning for cross-system recommendation",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",27,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893422716&partnerID=40&md5=b5b700022031d0f7c2d8bf5bc01b37e2","Recommender systems, especially the newly launched ones, have to deal with the data-sparsity issue, where little existing rating information is available. Recently, transfer learning has been proposed to address this problem by leveraging the knowledge from related recommender systems where rich collaborative data are available. However, most previous transfer learning models assume that entity-correspondences across different systems are given as input, which means that for any entity (e.g., a user or an item) in a target system, its corresponding entity in a source system is known. This assumption can hardly be satisfied in real-world scenarios where entity-correspondences across systems are usually unknown, and the cost of identifying them can be expensive. For example, it is extremely difficult to identify whether a user A from Facebook and a user B from Twitter are the same person. In this paper, we propose a framework to construct entity correspondence with limited budget by using active learning to facilitate knowledge transfer across recommender systems. Specifically, for the purpose of maximizing knowledge transfer, we first iteratively select entities in the target system based on our proposed criterion to query their correspondences in the source system. We then plug the actively constructed entity-correspondence mapping into a general transferred collaborative-filtering model to improve recommendation quality.We perform extensive experiments on real world datasets to verify the effectiveness of our proposed framework for this crosssystem recommendation problem. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Active Learning; Knowledge transfer; Rating information; Real-world datasets; Real-world scenario; Source systems; Target systems; Transfer learning; Artificial intelligence; Knowledge management; Recommender systems; Social networking (online); Collaborative filtering",Conference Paper,Scopus,2-s2.0-84893422716
"Beyeler M., Dutt N.D., Krichmar J.L.","Categorization and decision-making in a neurobiologically plausible spiking network using a STDP-like learning rule",2013,"Neural Networks",27,10.1016/j.neunet.2013.07.012,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883397026&doi=10.1016%2fj.neunet.2013.07.012&partnerID=40&md5=6841f374b328d92bf881e75cc365d6e7","Understanding how the human brain is able to efficiently perceive and understand a visual scene is still a field of ongoing research. Although many studies have focused on the design and optimization of neural networks to solve visual recognition tasks, most of them either lack neurobiologically plausible learning rules or decision-making processes. Here we present a large-scale model of a hierarchical spiking neural network (SNN) that integrates a low-level memory encoding mechanism with a higher-level decision process to perform a visual classification task in real-time. The model consists of Izhikevich neurons and conductance-based synapses for realistic approximation of neuronal dynamics, a spike-timing-dependent plasticity (STDP) synaptic learning rule with additional synaptic dynamics for memory encoding, and an accumulator model for memory retrieval and categorization. The full network, which comprised 71,026 neurons and approximately 133 million synapses, ran in real-time on a single off-the-shelf graphics processing unit (GPU). The network was constructed on a publicly available SNN simulator that supports general-purpose neuromorphic computer chips. The network achieved 92% correct classifications on MNIST in 100 rounds of random sub-sampling, which is comparable to other SNN approaches and provides a conservative and reliable performance metric. Additionally, the model correctly predicted reaction times from psychophysical experiments. Because of the scalability of the approach and its neurobiological fidelity, the current model can be extended to an efficient neuromorphic implementation that supports more generalized object recognition and decision-making architectures found in the brain. © 2013 Elsevier Ltd.","Decision-making; Object recognition; Spiking neural network; STDP; Supervised learning; Synaptic dynamics","Decision making process; Graphics Processing Unit; Psychophysical experiments; Spike-timing-dependent plasticity; Spiking neural network(SNN); Spiking neural networks; STDP; Synaptic dynamics; Computer graphics; Computer graphics equipment; Decision making; Dynamics; Encoding (symbols); Neurons; Object recognition; Program processors; Supervised learning; Neural networks; article; brain function; categorization; classification; computer graphics; decision making; learning; memory consolidation; model; nerve cell network; nerve cell plasticity; nervous system conductance; neurobiology; pattern recognition; priority journal; psychomotor activity; response time; simulator; spike timing dependent plasticity; spike wave; spiking neural network; synapse; visual discrimination; Decision-making; Object recognition; Spiking neural network; STDP; Supervised learning; Synaptic dynamics; Algorithms; Animals; Artificial Intelligence; Brain; Calcium Signaling; Computer Simulation; Computer Systems; Databases, Factual; Decision Theory; Haplorhini; Humans; Memory; Microcomputers; Models, Statistical; Neural Networks (Computer); Neuronal Plasticity; Neurons; Reproducibility of Results; Stochastic Processes; Synapses; Visual Perception",Article,Scopus,2-s2.0-84883397026
"Gonçalves E.C., Plastino A., Freitas A.A.","A genetic algorithm for optimizing the label ordering in multi-label classifier chains",2013,"Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI",27,10.1109/ICTAI.2013.76,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897716580&doi=10.1109%2fICTAI.2013.76&partnerID=40&md5=6fd72a38e263810a31ae364829afcdb9","First proposed in 2009, the classifier chains model (CC) has become one of the most influential algorithms for multi-label classification. It is distinguished by its simple and effective approach to exploit label dependencies. The CC method involves the training of q single-label binary classifiers, where each one is solely responsible for classifying a specific label in l1,..., lq. These q classifiers are linked in a chain, such that each binary classifier is able to consider the labels predicted by the previous ones as additional information at classification time. The label ordering has a strong effect on predictive accuracy, however it is decided at random and/or combining random orders via an ensemble. A disadvantage of the ensemble approach consists of the fact that it is not suitable when the goal is to generate interpretable classifiers. To tackle this problem, in this work we propose a genetic algorithm for optimizing the label ordering in classifier chains. Experiments on diverse benchmark datasets, followed by the Wilcoxon test for assessing statistical significance, indicate that the proposed strategy produces more accurate classifiers. © 2013 IEEE.","Classifier chains; Genetic algorithm; Multi-label classification","Classification time; Classifier chains; Effective approaches; Ensemble approaches; Most influential algorithms; Multi-label classifications; Predictive accuracy; Statistical significance; Artificial intelligence; Classification (of information); Genetic algorithms; Optimization; Tools; Chains",Conference Paper,Scopus,2-s2.0-84897716580
"Rienstra T., Thimm M., Oren N.","Opponent models with uncertainty for strategic argumentation",2013,"IJCAI International Joint Conference on Artificial Intelligence",27,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062988&partnerID=40&md5=240193d8a374902f5a4c3968ef3026f9","This paper deals with the issue of strategic argumentation in the setting of Dung-style abstract argumentation theory. Such reasoning takes place through the use of opponent models-recursive representations of an agent's knowledge and beliefs regarding the opponent's knowledge. Using such models, we present three approaches to reasoning. The first directly utilises the opponent model to identify the best move to advance in a dialogue. The second extends our basic approach through the use of quantitative uncertainty over the opponent's model. The final extension introduces virtual arguments into the opponent's reasoning process. Such arguments are unknown to the agent, but presumed to exist and interact with known arguments. They are therefore used to add a primitive notion of risk to the agent's reasoning. We have implemented our models and we have performed an empirical analysis that shows that this added expressivity improves the performance of an agent in a dialogue.",,"Abstract argumentation; Empirical analysis; Opponent modeling; Opponent models; Reasoning process; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84896062988
"Van Moffaert K., Drugan M.M., Nowe A.","Scalarized multi-objective reinforcement learning: Novel design techniques",2013,"IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning, ADPRL",27,10.1109/ADPRL.2013.6615007,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891506696&doi=10.1109%2fADPRL.2013.6615007&partnerID=40&md5=451d8990532eccc66ba55df7a61e23dc","In multi-objective problems, it is key to find compromising solutions that balance different objectives. The linear scalarization function is often utilized to translate the multi-objective nature of a problem into a standard, single-objective problem. Generally, it is noted that such as linear combination can only find solutions in convex areas of the Pareto front, therefore making the method inapplicable in situations where the shape of the front is not known beforehand, as is often the case. We propose a non-linear scalarization function, called the Chebyshev scalarization function, as a basis for action selection strategies in multi-objective reinforcement learning. The Chebyshev scalarization method overcomes the flaws of the linear scalarization function as it can (i) discover Pareto optimal solutions regardless of the shape of the front, i.e. convex as well as non-convex , (ii) obtain a better spread amongst the set of Pareto optimal solutions and (iii) is not particularly dependent on the actual weights used. © 2013 IEEE.",,"Action selection; Compromising solutions; Linear combinations; Multi objective; Multi-objective problem; Pareto optimal solutions; Scalarization; Scalarization method; Artificial intelligence; Dynamic programming; Optimal systems; Pareto principle; Reinforcement learning",Conference Paper,Scopus,2-s2.0-84891506696
"Zhou G., Liu Y., Liu F., Zeng D., Zhao J.","Improving question retrieval in community question answering using world knowledge",2013,"IJCAI International Joint Conference on Artificial Intelligence",27,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063888&partnerID=40&md5=3a1996c617a2d783597b433bd963432d","Community question answering (cQA), which provides a platform for people with diverse background to share information and knowledge, has become an increasingly popular research topic. In this paper, we focus on the task of question retrieval. The key problem of question retrieval is to measure the similarity between the queried questions and the historical questions which have been solved by other users. The traditional methods measure the similarity based on the bag-of-words (BOWs) representation. This representation neither captures dependencies between related words, nor hand les synonyms or polysemous words. In this work, we first propose a way to build a concept thesaurus based on the semantic relations extracted from the world knowledge of Wikipedia. Then, we develop a unified framework to leverage these semantic relations in order to enhance the question similarity in the concept space. Experiments conducted on a real cQA data set show that with the help of Wikipedia thesaurus, the performance of question retrieval is improved as compared to the traditional methods.",,"Bag-of-words (BoWs); Community question answering; Concept space; Polysemous word; Research topics; Semantic relations; Unified framework; World knowledge; Artificial intelligence; Thesauri; Semantics",Conference Paper,Scopus,2-s2.0-84896063888
"Sugiyama M., Kanamori T., Suzuki T., Plessis M.C.D., Liu S., Takeuchi I.","Density-difference estimation",2013,"Neural Computation",27,10.1162/NECO_a_00492,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877702876&doi=10.1162%2fNECO_a_00492&partnerID=40&md5=991c5798048ee0bd7c27ad5c8df550d4","We address the problem of estimating the difference between two probability densities. A naive approach is a two-step procedure of first estimating two densities separately and then computing their difference. However, this procedure does not necessarily work well because the first step is performed without regard to the second step, and thus a small estimation error incurred in the first stage can cause a big error in the second stage. In this letter, we propose a single-shot procedure for directly estimating the density difference without separately estimating two densities. We derive a nonparametric finite-sample error bound for the proposed single-shot density-difference estimator and show that it achieves the optimal convergence rate. We then show how the proposed density-difference estimator can be used in L2-distance approximation. Finally, we experimentally demonstrate the usefulness of the proposed method in robust distribution comparison such as class-prior estimation and change-point detection. © 2013 Massachusetts Institute of Technology.",,"algorithm; article; artificial intelligence; Australia; computer program; data mining; diabetes mellitus; factual database; Germany; human; statistical model; statistics; Algorithms; Artificial Intelligence; Australia; Data Mining; Databases, Factual; Diabetes Mellitus; Germany; Humans; Logistic Models; Software",Article,Scopus,2-s2.0-84877702876
"Siirtola P., Roning J.","Ready-to-use activity recognition for smartphones",2013,"Proceedings of the 2013 IEEE Symposium on Computational Intelligence and Data Mining, CIDM 2013 - 2013 IEEE Symposium Series on Computational Intelligence, SSCI 2013",27,10.1109/CIDM.2013.6597218,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885588005&doi=10.1109%2fCIDM.2013.6597218&partnerID=40&md5=4f309f495dd4fb4ad6cdf8a9f8d96728","In this study, every day activities are recognized from data collected using smartphones accelerometer sensors. Offline experiments are made to show that the presented method is user- and body position-independent. In addition, it is shown that the features used in the classification are not dependent on the calibration of the phone. The recognition models trained using the offline data are also tested online. A mobile application running these models is built for two operating systems: Symbian̂3 and Android. Real-time experiments using these applications are made to show that the presented method can be implemented to any operating system and hardware variations do not affect recognition results. High recognition accuracies are obtained, in the offline study, the average recognition rate is almost 99% and, also, in the online study, the average recognition accuracy is over 90%. © 2013 IEEE.","Accelerometer; activity recognition; machine learning; mobile phones","Accelerometer sensor; Activity recognition; Affect recognition; Mobile applications; Off-line studies; Real-time experiment; Recognition accuracy; Recognition models; Accelerometers; Data mining; Learning systems; Mobile phones; Pattern recognition; Smartphones; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84885588005
"Zahiri J., Yaghoubi O., Mohammad-Noori M., Ebrahimpour R., Masoudi-Nejad A.","PPIevo: Protein-protein interaction prediction from PSSM based evolutionary information",2013,"Genomics",27,10.1016/j.ygeno.2013.05.006,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886313848&doi=10.1016%2fj.ygeno.2013.05.006&partnerID=40&md5=c2858077e8bb6918b9bf9cac78d1ab41","Protein-protein interactions regulate a variety of cellular processes. There is a great need for computational methods as a complement to experimental methods with which to predict protein interactions due to the existence of many limitations involved in experimental techniques. Here, we introduce a novel evolutionary based feature extraction algorithm for protein-protein interaction (PPI) prediction. The algorithm is called PPIevo and extracts the evolutionary feature from Position-Specific Scoring Matrix (PSSM) of protein with known sequence. The algorithm does not depend on the protein annotations, and the features are based on the evolutionary history of the proteins. This enables the algorithm to have more power for predicting protein-protein interaction than many sequence based algorithms. Results on the HPRD database show better performance and robustness of the proposed method. They also reveal that the negative dataset selection could lead to an acute performance overestimation which is the principal drawback of the available methods. © 2013 Elsevier Inc.","Computational intelligence; Machine learning; Position-specific scoring matrix; Protein interaction networks; Protein-protein interaction map","accuracy; amino acid sequence; area under the curve; article; calculation; classification algorithm; controlled study; evolutionary algorithm; learning algorithm; mathematical computing; mathematical model; position weight matrix; priority journal; protein protein interaction; receiver operating characteristic; sequence analysis; validation process; Computational intelligence; Machine learning; Position-specific scoring matrix; Protein interaction networks; Protein-protein interaction map; Algorithms; Amino Acid Sequence; Artificial Intelligence; Computational Biology; Databases, Protein; Evolution, Molecular; Humans; Phylogeny; Position-Specific Scoring Matrices; Protein Interaction Maps; Proteins",Article,Scopus,2-s2.0-84886313848
"Zalasiński M., Cpałka K.","Novel algorithm for the on-line signature verification using selected discretization points groups",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",27,10.1007/978-3-642-38658-9_44,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884391159&doi=10.1007%2f978-3-642-38658-9_44&partnerID=40&md5=d3ee0c9280a26fcc6e99b92717be7840","Identity verification based on on-line signature is a commonly known biometric task. Some methods based on the on-line signature biometric attribute used for identity verification use information from partitions of the signature. Efficiency of these methods is relatively high. In this paper we would like to present a new approach to signature trajectories partitioning, based on selection of the discretization points groups. The new method was compared to other methods, with use of the SVC2004 public on-line signature database. © 2013 Springer-Verlag.",,"Discretizations; Identity verification; New approaches; Novel algorithm; On-line signature verification; Online signature; Artificial intelligence; Biometrics; Soft computing; Algorithms",Conference Paper,Scopus,2-s2.0-84884391159
"Frederiksen T.K., Jakobsen T.P., Nielsen J.B., Nordholt P.S., Orlandi C.","MiniLEGO: Efficient secure two-party computation from general assumptions",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",27,10.1007/978-3-642-38348-9_32,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883362608&doi=10.1007%2f978-3-642-38348-9_32&partnerID=40&md5=4554a553e2df2be17ec6b77110fc1128","One of the main tools to construct secure two-party computation protocols are Yao garbled circuits. Using the cut-and-choose technique, one can get reasonably efficient Yao-based protocols with security against malicious adversaries. At TCC 2009, Nielsen and Orlandi [28] suggested to apply cut-and-choose at the gate level, while previously cut-and-choose was applied on the circuit as a whole. This idea allows for a speed up with practical significance (in the order of the logarithm of the size of the circuit) and has become known as the ""LEGO"" construction. Unfortunately the construction in [28] is based on a specific number-theoretic assumption and requires public-key operations per gate of the circuit. The main technical contribution of this work is a new XOR-homomorphic commitment scheme based on oblivious transfer, that we use to cope with the problem of connecting the gates in the LEGO construction. Our new protocol has the following advantages: 1 It maintains the efficiency of the LEGO cut-and-choose. 2 After a number of seed oblivious transfers linear in the security parameter, the construction uses only primitives from Minicrypt (i.e., private-key cryptography) per gate in the circuit (hence the name MiniLEGO). 3 MiniLEGO is compatible with all known optimization for Yao garbled gates (row reduction, free-XORs, point-and-permute). © 2013 International Association for Cryptologic Research.",,"Commitment scheme; Garbled circuits; Malicious adversaries; Oblivious transfer; Row reductions; Secure two-party computations; Security parameters; Technical contribution; Artificial intelligence; Computer science; Cryptography",Conference Paper,Scopus,2-s2.0-84883362608
"Van Looy A., De Backer M., Poels G., Snoeck M.","Choosing the right business process maturity model",2013,"Information and Management",27,10.1016/j.im.2013.06.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883311910&doi=10.1016%2fj.im.2013.06.002&partnerID=40&md5=888a281845bb74599213f27f869327b3","We have built and tested a decision tool which will help organisations properly select one business process maturity model (BPMM) over another. This prototype consists of a novel questionnaire with decision criteria for BPMM selection, linked to a unique data set of 69 BPMMs. Fourteen criteria (questions) were elicited from an international Delphi study, and weighed by the analytical hierarchy process. Case studies have shown (non-)profit and academic applications. Our purpose was to describe criteria that enable an informed BPMM choice (conform to decision-making theories, rather than ad hoc). Moreover, we propose a design process for building BPMM decision tools. © 2013 Elsevier B.V.","Analytical hierarchy process; Business process; Decision support system; Delphi technique; Maturity model; Process improvement","Analytical Hierarchy Process; Business Process; Delphi technique; Maturity model; Process Improvement; Artificial intelligence; Decision support systems; Tools",Article,Scopus,2-s2.0-84883311910
"Baraka K., Ghobril M., Malek S., Kanj R., Kayssi A.","Low cost arduino/android-based energy-efficient home automation system with smart task scheduling",2013,"Proceedings - 5th International Conference on Computational Intelligence, Communication Systems, and Networks, CICSyN 2013",27,10.1109/CICSYN.2013.47,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883323850&doi=10.1109%2fCICSYN.2013.47&partnerID=40&md5=2ee4a70bc6ee489d785723a1cf5a628e","In this paper, we make use of Home Automation techniques to design and implement a remotely controlled, energy-efficient and highly scalable Smart Home with basic features that safeguard the residents' comfort and security. Our system consists of a house network (sensors and appliance actuators to respectively get information from and control the house environment). As a central controller, we used an Arduino microcontroller that communicates with an Android application, our user interface. Our house network brings together both wireless Zigbee and wired X10 technologies, thus making it a cost-efficient hybrid system. Events can be programmed to be triggered under specific conditions, and this can have a great role in reducing the total energy consumed by some appliances. On the other hand, the system can suggest smart task scheduling. The scheduling algorithm we present is a heuristic for the Resource-constrained-scheduling problem (RCPSP) with hybrid objective function merging both resource-leveling and weighted completion time considerations. © 2013 IEEE.","Android; Arduino; Energy management; Home automation; Smart scheduling","Android; Android applications; Arduino; Design and implements; Home automation; Home automation systems; Hybrid objective; Weighted completion time; Artificial intelligence; Automation; Communication systems; Energy efficiency; Energy management; Hybrid systems; Intelligent buildings; Multitasking; Scheduling; Scheduling algorithms; User interfaces; Houses",Conference Paper,Scopus,2-s2.0-84883323850
"Ghosh A., Subudhi B.N., Bruzzone L.","Integration of Gibbs Markov random field and hopfield-type neural networks for unsupervised change detection in remotely sensed multitemporal images",2013,"IEEE Transactions on Image Processing",27,10.1109/TIP.2013.2259833,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878499480&doi=10.1109%2fTIP.2013.2259833&partnerID=40&md5=5c7d5bbc439d9543610102fc4281b306","In this paper, a spatiocontextual unsupervised change detection technique for multitemporal, multispectral remote sensing images is proposed. The technique uses a Gibbs Markov random field (GMRF) to model the spatial regularity between the neighboring pixels of the multitemporal difference image. The difference image is generated by change vector analysis applied to images acquired on the same geographical area at different times. The change detection problem is solved using the maximum a posteriori probability (MAP) estimation principle. The MAP estimator of the GMRF used to model the difference image is exponential in nature, thus a modified Hopfield type neural network (HTNN) is exploited for estimating the MAP. In the considered Hopfield type network, a single neuron is assigned to each pixel of the difference image and is assumed to be connected only to its neighbors. Initial values of the neurons are set by histogram thresholding. An expectation-maximization algorithm is used to estimate the GMRF model parameters. Experiments are carried out on three-multispectral and multitemporal remote sensing images. Results of the proposed change detection scheme are compared with those of the manual-trial-and-error technique, automatic change detection scheme based on GMRF model and iterated conditional mode algorithm, a context sensitive change detection scheme based on HTNN, the GMRF model, and a graph-cut algorithm. A comparison points out that the proposed method provides more accurate change detection maps than other methods. © 2012 IEEE.","Change detection; hopfield neural network; markov random field (MRF); maximum a posteriori probability (MAP) estimation; multitemporal images; remote sensing","Change detection; Expectation-maximization algorithms; Hopfield type neural networks; Markov Random Fields; Maximum a posteriori probability estimations (MAP); Multi-temporal image; Multi-temporal remote sensing; Multispectral remote sensing image; Algorithms; Estimation; Gaussian distribution; Graphic methods; Hopfield neural networks; Image reconstruction; Image segmentation; Markov processes; Pixels; Remote sensing; Signal detection; algorithm; article; artificial intelligence; artificial neural network; automated pattern recognition; computer assisted diagnosis; image enhancement; image subtraction; methodology; probability; remote sensing; reproducibility; sensitivity and specificity; statistical analysis; system analysis; automated pattern recognition; computer assisted diagnosis; procedures; remote sensing; Algorithms; Artificial Intelligence; Data Interpretation, Statistical; Image Enhancement; Image Interpretation, Computer-Assisted; Markov Chains; Neural Networks (Computer); Pattern Recognition, Automated; Remote Sensing Technology; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique; Systems Integration; Algorithms; Artificial Intelligence; Data Interpretation, Statistical; Image Enhancement; Image Interpretation, Computer-Assisted; Markov Chains; Neural Networks (Computer); Pattern Recognition, Automated; Remote Sensing Technology; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique; Systems Integration",Article,Scopus,2-s2.0-84878499480
"Kaya Y., Uyar M.","A hybrid decision support system based on rough set and extreme learning machine for diagnosis of hepatitis disease",2013,"Applied Soft Computing Journal",27,10.1016/j.asoc.2013.03.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878164388&doi=10.1016%2fj.asoc.2013.03.008&partnerID=40&md5=3b2b85dae50e6e0345e63a66b5e8e303","Hepatitis is a disease which is seen at all levels of age. Hepatitis disease solely does not have a lethal effect, but the early diagnosis and treatment of hepatitis is crucial as it triggers other diseases. In this study, a new hybrid medical decision support system based on rough set (RS) and extreme learning machine (ELM) has been proposed for the diagnosis of hepatitis disease. RS-ELM consists of two stages. In the first one, redundant features have been removed from the data set through RS approach. In the second one, classification process has been implemented through ELM by using remaining features. Hepatitis data set, taken from UCI machine learning repository has been used to test the proposed hybrid model. A major part of the data set (48.3%) includes missing values. As removal of missing values from the data set leads to data loss, feature selection has been done in the first stage without deleting missing values. In the second stage, the classification process has been performed through ELM after the removal of missing values from sub-featured data sets that were reduced in different dimensions. The results showed that the highest 100.00% classification accuracy has been achieved through RS-ELM and it has been observed that RS-ELM model has been considerably successful compared to the other methods in the literature. Furthermore in this study, the most significant features have been determined for the diagnosis of the hepatitis. It is considered that proposed method is to be useful in similar medical applications. © 2013 Elsevier B.V. All rights reserved.","Dimensionality reduction; Extreme learning machine; Hepatitis disease; Rough set","Classification accuracy; Classification process; Dimensionality reduction; Extreme learning machine; Hybrid decision support systems; Medical decision support system; Redundant features; UCI machine learning repository; Artificial intelligence; Classification (of information); Decision support systems; Knowledge acquisition; Learning systems; Medical applications; Rough set theory; Statistical tests; Diagnosis",Article,Scopus,2-s2.0-84878164388
"Baldassarre G., Mannella F., Fiore V.G., Redgrave P., Gurney K., Mirolli M.","Intrinsically motivated action-outcome learning and goal-based action recall: A system-level bio-constrained computational model",2013,"Neural Networks",27,10.1016/j.neunet.2012.09.015,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875892345&doi=10.1016%2fj.neunet.2012.09.015&partnerID=40&md5=3286c1ea62fb5e825cbcd95be476c392","Reinforcement (trial-and-error) learning in animals is driven by a multitude of processes. Most animals have evolved several sophisticated systems of 'extrinsic motivations' (EMs) that guide them to acquire behaviours allowing them to maintain their bodies, defend against threat, and reproduce. Animals have also evolved various systems of 'intrinsic motivations' (IMs) that allow them to acquire actions in the absence of extrinsic rewards. These actions are used later to pursue such rewards when they become available. Intrinsic motivations have been studied in Psychology for many decades and their biological substrates are now being elucidated by neuroscientists. In the last two decades, investigators in computational modelling, robotics and machine learning have proposed various mechanisms that capture certain aspects of IMs. However, we still lack models of IMs that attempt to integrate all key aspects of intrinsically motivated learning and behaviour while taking into account the relevant neurobiological constraints. This paper proposes a bio-constrained system-level model that contributes a major step towards this integration. The model focusses on three processes related to IMs and on the neural mechanisms underlying them: (a) the acquisition of action-outcome associations (internal models of the agent-environment interaction) driven by phasic dopamine signals caused by sudden, unexpected changes in the environment; (b) the transient focussing of visual gaze and actions on salient portions of the environment; (c) the subsequent recall of actions to pursue extrinsic rewards based on goal-directed reactivation of the representations of their outcomes. The tests of the model, including a series of selective lesions, show how the focussing processes lead to a faster learning of action-outcome associations, and how these associations can be recruited for accomplishing goal-directed behaviours. The model, together with the background knowledge reviewed in the paper, represents a framework that can be used to guide the design and interpretation of empirical experiments on IMs, and to computationally validate and further develop theories on them. © 2012 Elsevier Ltd.","Attention; Basal ganglia selection; Dopamine; Intrinsic motivations; Parietal, premotor, prefrontal cortex; Repetition bias; Striato-cortical loops; Superior colliculus; Trial-and-error learning","Attention; Basal ganglia; Dopamine; Intrinsic motivation; Prefrontal cortex; Repetition bias; Striato-cortical loops; Superior colliculus; Trial-and-error learning; Amines; Animals; C (programming language); Motivation; Neurophysiology; Association reactions; dopamine; article; calculation; gaze; learning; mathematical analysis; mathematical computing; mathematical model; motivation; priority journal; process development; Animals; Artificial Intelligence; Attention; Child; Corpus Striatum; Dopamine; Feedback; Goals; Haplorhini; Humans; Mental Recall; Models, Neurological; Motivation; Motor Cortex; Neural Networks (Computer); Parietal Lobe; Prefrontal Cortex; Problem-Based Learning; Reinforcement (Psychology); Superior Colliculi",Article,Scopus,2-s2.0-84875892345
"Pasman H., Rogers W.","Bayesian networks make LOPA more effective, QRA more transparent and flexible, and thus safety more definable!",2013,"Journal of Loss Prevention in the Process Industries",27,10.1016/j.jlp.2012.07.016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876743028&doi=10.1016%2fj.jlp.2012.07.016&partnerID=40&md5=5a435284c5666ec3eac8e799b8c02b14","Quantitative risk analysis is in principle an ideal method to map one's risks, but it has limitations due to the complexity of models, scarcity of data, remaining uncertainties, and above all because effort, cost, and time requirements are heavy. Also, software is not cheap, the calculations are not quite transparent, and the flexibility to look at various scenarios and at preventive and protective options is limited. So, the method is considered as a last resort for determination of risks. Simpler methods such as LOPA that focus on a particular scenario and assessment of protection for a defined initiating event are more popular. LOPA may however not cover the whole range of credible scenarios, and calamitous surprises may emerge. In the past few decades, Artificial Intelligence university groups, such as the Decision Systems Laboratory of the University of Pittsburgh, have developed Bayesian approaches to support decision making in situations where one has to weigh gains and costs versus risks. This paper will describe details of such an approach and will provide some examples of both discrete random variables, such as the probability values in a LOPA, and continuous distributions, which can better reflect the uncertainty in data. © 2012 Elsevier Ltd.","Bayesian networks; Cost-benefit; Process safety; Risk analysis; Software tools","Bayesian approaches; Continuous distribution; Cost-benefit; Discrete random variables; Initiating events; Process safety; Quantitative risk analysis; University of Pittsburgh; Artificial intelligence; Bayesian networks; Computer aided software engineering; Probability distributions; Risk analysis",Article,Scopus,2-s2.0-84876743028
"Liu S., Leat M., Moizer J., Megicks P., Kasturiratne D.","A decision-focused knowledge management framework to support collaborative decision making for lean supply chain management",2013,"International Journal of Production Research",27,10.1080/00207543.2012.709646,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873456162&doi=10.1080%2f00207543.2012.709646&partnerID=40&md5=a8ca9711cf35c7402a0541120ed07865","Lean supply chain management is a relatively new concept resulting from the integration of lean philosophy into supply chain management. Decision making in a lean supply chain context is challenging because of the complexity, dynamics, and uncertainty inherent to both supply networks and the types of waste (defined as any processes, including use of resources, which do not add value to customers). Efficient knowledge management has been identified as one of the key requirements to achieve integrated support for lean supply chain decisions. This paper proposes a decision-focused knowledge framework including a multi-layer knowledge model (to capture the know-why and know-with together with the know-what and know-how), a knowledge matrix for knowledge elicitation, and a decision tree for the design of the knowledge base. A knowledge system for lean supply chain management (KSLSCM) has been developed using artificial intelligence system shells VisiRule and Flex. The KSLSCM has five core components: a supply chain decision network manager, a waste elimination knowledge base, a knowledge refinement module, an inference engine, and a decision justifier. The knowledge framework and the KSLSCM have been evaluated through an industrial decision case. It has been demonstrated through the KSLSCM that the decision-focused knowledge framework can provide efficient and effective support for collaborative decision making in supply chain waste elimination. © 2013 Taylor & Francis Group, LLC.","collaborative decision making; decision-focused knowledge framework; Flex; knowledge-based decision support system; lean supply chain management; multi-layer knowledge model; VisiRule","Collaborative decision making; Flex; Knowledge based decision support systems; Knowledge frameworks; Knowledge model; VisiRule; Artificial intelligence; Decision support systems; Decision trees; Knowledge based systems; Knowledge management; Supply chain management; Technology transfer; Decision making",Article,Scopus,2-s2.0-84873456162
"Mohd Fauzi F., Koutsoukas A., Lowe R., Joshi K., Fan T.-P., Glen R.C., Bender A.","Chemogenomics approaches to rationalizing the mode-of-action of traditional chinese and ayurvedic medicines",2013,"Journal of Chemical Information and Modeling",27,10.1021/ci3005513,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875462770&doi=10.1021%2fci3005513&partnerID=40&md5=08cf0e542c62006f634b62d897fe5a36","Traditional Chinese medicine (TCM) and Ayurveda have been used in humans for thousands of years. While the link to a particular indication has been established in man, the mode-of-action (MOA) of the formulations often remains unknown. In this study, we aim to understand the MOA of formulations used in traditional medicine using an in silico target prediction algorithm, which aims to predict protein targets (and hence MOAs), given the chemical structure of a compound. Following this approach we were able to establish several links between suggested MOAs and experimental evidence. In particular, compounds from the 'tonifying and replenishing medicinal' class from TCM exhibit a hypoglycemic effect which can be related to activity of the ingredients against the Sodium-Glucose Transporters (SGLT) 1 and 2 as well as Protein Tyrosine Phosphatase (PTP). Similar results were obtained for Ayurvedic anticancer drugs. Here, both primary anticancer targets (those directly involved in cancer pathogenesis) such as steroid-5-alpha-reductase 1 and 2 were predicted as well as targets which act synergistically with the primary target, such as the efflux pump P-glycoprotein (P-gp). In addition, we were able to elucidate some targets which may point us to novel MOAs as well as explain side effects. Most notably, GPBAR1, which was predicted as a target for both 'tonifying and replenishing medicinal' and anticancer classes, suggests an influence of the compounds on metabolism. Understanding the MOA of these compounds is beneficial as it provides a resource for NMEs with possibly higher efficacy in the clinic than those identified by single-target biochemical assays. © 2013 American Chemical Society.",,"Anticancer targets; Ayurvedic medicine; Biochemical assay; Cancer pathogenesis; Experimental evidence; Hypoglycemic effect; Protein-tyrosine phosphatase; Traditional Chinese Medicine; Algorithms; Amino acids; antidiabetic agent; antineoplastic agent; G protein coupled receptor; GPBAR1 protein, human; herbaceous agent; multidrug resistance protein; protein tyrosine phosphatase; SLC5A1 protein, human; SLC5A2 protein, human; sodium glucose cotransporter 1; sodium glucose cotransporter 2; algorithm; article; artificial intelligence; Ayurveda; chemistry; Chinese medicine; computer simulation; drug effect; genetic database; genetics; human; medicinal plant; Algorithms; Antineoplastic Agents; Artificial Intelligence; Computer Simulation; Databases, Genetic; Drugs, Chinese Herbal; Humans; Hypoglycemic Agents; Medicine, Ayurvedic; Medicine, Chinese Traditional; P-Glycoprotein; Plants, Medicinal; Protein Tyrosine Phosphatases; Receptors, G-Protein-Coupled; Sodium-Glucose Transporter 1; Sodium-Glucose Transporter 2",Article,Scopus,2-s2.0-84875462770
"Faundez-Zanuy M., Hussain A., Mekyska J., Sesa-Nogueras E., Monte-Moreno E., Esposito A., Chetouani M., Garre-Olmo J., Abel A., Smekal Z., Lopez-de-Ipiña K.","Biometric Applications Related to Human Beings: There Is Life beyond Security",2013,"Cognitive Computation",27,10.1007/s12559-012-9169-9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874556693&doi=10.1007%2fs12559-012-9169-9&partnerID=40&md5=a2edee0bfd813934c0c8d58b41f956f7","The use of biometrics has been successfully applied to security applications for some time. However, the extension of other potential applications with the use of biometric information is a very recent development. This paper summarizes the field of biometrics and investigates the potential of utilizing biometrics beyond the presently limited field of security applications. There are some synergies that can be established within security-related applications. These can also be relevant in other fields such as health and ambient intelligence. This paper describes these synergies. Overall, this paper highlights some interesting and exciting research areas as well as possible synergies between different applications using biometric information. © 2012 Springer Science+Business Media, LLC.","Ambient intelligence; Biometrics; Healthcare; Security","Ambient intelligence; Biometric applications; Biometric informations; Human being; Potential applications; Security; Security application; Artificial intelligence; Health care; Information use; Biometrics",Article,Scopus,2-s2.0-84874556693
"Wu D., Lance B.J., Parsons T.D.","Collaborative Filtering for Brain-Computer Interaction Using Transfer Learning and Active Class Selection",2013,"PLoS ONE",27,10.1371/journal.pone.0056624,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874336966&doi=10.1371%2fjournal.pone.0056624&partnerID=40&md5=2b57979175c9f4015498a6fe90468244","Brain-computer interaction (BCI) and physiological computing are terms that refer to using processed neural or physiological signals to influence human interaction with computers, environment, and each other. A major challenge in developing these systems arises from the large individual differences typically seen in the neural/physiological responses. As a result, many researchers use individually-trained recognition algorithms to process this data. In order to minimize time, cost, and barriers to use, there is a need to minimize the amount of individual training data required, or equivalently, to increase the recognition accuracy without increasing the number of user-specific training samples. One promising method for achieving this is collaborative filtering, which combines training data from the individual subject with additional training data from other, similar subjects. This paper describes a successful application of a collaborative filtering approach intended for a BCI system. This approach is based on transfer learning (TL), active class selection (ACS), and a mean squared difference user-similarity heuristic. The resulting BCI system uses neural and physiological signals for automatic task difficulty recognition. TL improves the learning performance by combining a small number of user-specific training samples with a large number of auxiliary training samples from other similar subjects. ACS optimally selects the classes to generate user-specific training samples. Experimental results on 18 subjects, using both k nearest neighbors and support vector machine classifiers, demonstrate that the proposed approach can significantly reduce the number of user-specific training data samples. This collaborative filtering approach will also be generalizable to handling individual differences in many other applications that involve human neural or physiological data, such as affective computing.",,"accuracy; active class selection; article; automated pattern recognition; brain computer interface; collaborative filtering; electroencephalogram; human; human computer interaction; human experiment; information processing; k nearest neighbor; learning algorithm; learning transfer; machine learning; process optimization; stimulus response; Stroop test; support vector machine; task performance; virtual reality; Algorithms; Artificial Intelligence; Brain; Brain-Computer Interfaces; Computer Systems; Cooperative Behavior; Electroencephalography; Humans; Learning; Male",Article,Scopus,2-s2.0-84874336966
"Chen F., Yu H., Hu R.","Shape sparse representation for joint object classification and segmentation",2013,"IEEE Transactions on Image Processing",27,10.1109/TIP.2012.2226044,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873289474&doi=10.1109%2fTIP.2012.2226044&partnerID=40&md5=d04e8f88aef1e9a524d7d769ad07cdbe","In this paper, a novel variational model based on prior shapes for simultaneous object classification and segmentation is proposed. Given a set of training shapes of multiple object classes, a sparse linear combination of training shapes in a low-dimensional representation is used to regularize the target shape in variational image segmentation. By minimizing the proposed variational functional, the model is able to automatically select the reference shapes that best represent the object by sparse recovery and accurately segment the image, taking into account both the image information and the shape priors. For some applications under an appropriate size of training set, the proposed model allows artificial enlargement of the training set by including a certain number of transformed shapes for transformation invariance, and then the model remains jointly convex and can handle the case of overlapping or multiple objects presented in an image within a small range. Numerical experiments show promising results and the potential of the method for object classification and segmentation. © 1992-2012 IEEE.","Image segmentation; shape priors; sparse representation; variational formulations","Image information; Linear combinations; Low-dimensional representation; Multiple objects; Numerical experiments; Object classification; Prior shapes; Shape priors; Sparse recovery; Sparse representation; Target shape; Training sets; Transformation invariance; Variational formulation; Variational functional; Variational models; Numerical methods; Object recognition; Image segmentation; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; image enhancement; image subtraction; methodology; reproducibility; sensitivity and specificity; three dimensional imaging; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-84873289474
"Guerrero Bote V.P., Olmeda-Gõmez C., De Moya-Anegõn F.","Quantifying the benefits of international scientific collaboration",2013,"Journal of the American Society for Information Science and Technology",27,10.1002/asi.22754,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872894820&doi=10.1002%2fasi.22754&partnerID=40&md5=870e834c2e72f88c1bed103febc2f261","We analyze the benefits in terms of scientific impact deriving from international collaboration, examining both those for a country when it collaborates and also those for the other countries when they are collaborating with the former. The data show the more countries there are involved in the collaboration, the greater the gain in impact. Contrary to what we expected, the scientific impact of a country does not significantly influence the benefit it derives from collaboration, but does seem to positively influence the benefit obtained by the other countries collaborating with it. Although there was a weak correlation between these two classes of benefit, the countries with the highest impact were clear outliers from this correlation, tending to provide proportionally more benefit to their collaborating countries than they themselves obtained. Two surprising findings were the null benefit resulting from collaboration with Iran, and the small benefit resulting from collaboration with the United States despite its high impact. © 2012 ASIS&;T.","citation analysis; scientometrics","Citation analysis; High impact; International collaborations; Scientific collaboration; Scientometrics; Weak correlation; Artificial intelligence; Software engineering; International cooperation",Article,Scopus,2-s2.0-84872894820
"Ferrández O., South B.R., Shen S., Jeffrey Friedlin F., Samore M.H., Meystre S.M.","BoB, a best-of-breed automated text de-identification system for VHA clinical documents",2013,"Journal of the American Medical Informatics Association",27,10.1136/amiajnl-2012-001020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871853864&doi=10.1136%2famiajnl-2012-001020&partnerID=40&md5=f04db50bd6d599c8159a110debdb3e3e","Objective: De-identification allows faster and more collaborative clinical research while protecting patient confidentiality. Clinical narrative de-identification is a tedious process that can be alleviated by automated natural language processing methods. The goal of this research is the development of an automated text deidentification system for Veterans Health Administration (VHA) clinical documents. Materials and methods: We devised a novel stepwise hybrid approach designed to improve the current strategies used for text de-identification. The proposed system is based on a previous study on the best deidentification methods for VHA documents. This best-ofbreed automated clinical text de-identification system (aka BoB) tackles the problem as two separate tasks: (1) maximize patient confidentiality by redacting as much protected health information (PHI) as possible; and (2) leave de-identified documents in a usable state preserving as much clinical information as possible. Results: We evaluated BoB with a manually annotated corpus of a variety of VHA clinical notes, as well as with the 2006 i2b2 de-identification challenge corpus. We present evaluations at the instance- and token-level, with detailed results for BoB's main components. Moreover, an existing text de-identification system was also included in our evaluation. Discussion: BoB's design efficiently takes advantage of the methods implemented in its pipeline, resulting in high sensitivity values (especially for sensitive PHI categories) and a limited number of false positives. Conclusions: Our system successfully addressed VHA clinical document de-identification, and its hybrid stepwise design demonstrates robustness and efficiency, prioritizing patient confidentiality while leaving most clinical information intact.",,"article; automation; best of breed automated text de identification system; clinical evaluation; confidentiality; information processing; medical documentation; medical information; methodology; sensitivity analysis; veterans health; Artificial Intelligence; Confidentiality; Data Mining; Electronic Health Records; Humans; Information Dissemination; Natural Language Processing; Technology Assessment, Biomedical; United States; United States Department of Veterans Affairs",Article,Scopus,2-s2.0-84871853864
"Wang Z., Lemon O.","A simple and generic belief tracking mechanism for the dialog state tracking challenge: On the believability of observed information",2013,"SIGDIAL 2013 - 14th Annual Meeting of the Special Interest Group on Discourse and Dialogue, Proceedings of the Conference",27,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987916141&partnerID=40&md5=c42f9ed7b271a24d9e58b186a5185313","This paper presents a generic dialogue state tracker that maintains beliefs over user goals based on a few simple domainindependent rules, using basic probability operations. The rules apply to observed system actions and partially observable user acts, without using any knowledge obtained from external resources (i.e. without requiring training data). The core insight is to maximise the amount of information directly gainable from an errorprone dialogue itself, so as to better lowerbound one's expectations on the performance of more advanced statistical techniques for the task. The proposed method is evaluated in the Dialog State Tracking Challenge, where it achieves comparable performance in hypothesis accuracy to machine learning based systems. Consequently, with respect to different scenarios for the belief tracking problem, the potential superiority and weakness of machine learning approaches in general are investigated. © 2013 Association for Computational Linguistics.",,"Artificial intelligence; Amount of information; External resources; Machine learning approaches; Observed systems; State tracking; Statistical techniques; Tracking mechanism; Tracking problem; Learning systems",Conference Paper,Scopus,2-s2.0-84987916141
"Guo Y.","Convex subspace representation learning from multi-view data",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",26,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893405763&partnerID=40&md5=bf147e22ea6352f7ce621fc54a4543f7","Learning from multi-view data is important in many applications. In this paper, we propose a novel convex subspace representation learning method for unsupervised multi-view clustering. We first formulate the subspace learning with multiple views as a joint optimization problem with a common subspace representation matrix and a group sparsity inducing norm. By exploiting the properties of dual norms, we then show a convex min-max dual formulation with a sparsity inducing trace norm can be obtained.We develop a proximal bundle optimization algorithm to globally solve the minmax optimization problem. Our empirical study shows the proposed subspace representation learning method can effectively facilitate multi-view clustering and induce superior clustering results than alternative multiview clustering methods. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Clustering results; Dual formulations; Joint optimization; Min-max optimization; Multi-view clustering; Optimization algorithms; Subspace learning; Subspace representation; Artificial intelligence; Clustering algorithms; Learning systems; Optimization",Conference Paper,Scopus,2-s2.0-84893405763
"Xu B., Bu J., Lin Y., Chen C., He X., Cai D.","Harmonious Hashing",2013,"IJCAI International Joint Conference on Artificial Intelligence",26,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062187&partnerID=40&md5=17195fec73ab3372e8acbd002562576d","Hashing-based fast nearest neighbor search technique has attracted great attention in both research and industry areas recently. Many existing hashing approaches encode data with projection-based hash functions and represent each projected dimension by 1-bit. However, the dimensions with high variance hold large energy or information of data but treated equivalently as dimensions with low variance, which leads to a serious information loss. In this paper, we introduce a novel hashing algorithm called Harmonious Hashing which aims at learning hash functions with low information loss. Specifically, we learn a set of optimized projections to preserve the maximum cumulative energy and meet the constraint of equivalent variance on each dimension as much as possible. In this way, we could minimize the information loss after binarization. Despite the extreme simplicity, our method outperforms superiorly to many state-of-the-art hashing methods in large-scale and high-dimensional nearest neighbor search experiments.",,"Binarizations; Cumulative energy; Fast nearest neighbor search; Hashing algorithms; Hashing method; High-dimensional; Information loss; Nearest Neighbor search; Hash functions; Industrial research; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84896062187
"Zhai X., Peng Y., Xiao J.","Heterogeneous metric learning with joint graph regularization for cross-media retrieval",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",26,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893369645&partnerID=40&md5=8b382644e97dc42d467662d3628aeb41","As the major component of big data, unstructured heterogeneous multimedia content such as text, image, audio, video and 3D increasing rapidly on the Internet. User demand a new type of cross-media retrieval where user can search results across various media by submitting query of any media. Since the query and the retrieved results can be of different media, how to learn a heterogeneous metric is the key challenge. Most existing metric learning algorithms only focus on a single media where all of the media objects share the same data representation. In this paper, we propose a joint graph regularized heterogeneous metric learning (JGRHML) algorithm, which integrates the structure of different media into a joint graph regularization. In JGRHML, different media are complementary to each other and optimizing them simultaneously can make the solution smoother for both media and further improve the accuracy of the final metric. Based on the heterogeneous metric, we further learn a high-level semantic metric through label propagation. JGRHML is effective to explore the semantic relationship hidden across different modalities. The experimental results on two datasets with up to five media types show the effectiveness of our proposed approach. © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Cross-media retrieval; Data representations; High level semantics; Label propagation; Media objects; Metric learning; Multimedia contents; Semantic relationships; Artificial intelligence; Semantics",Conference Paper,Scopus,2-s2.0-84893369645
"Kosek A.M., Costanzo G.T., Bindner H.W., Gehrke O.","An overview of demand side management control schemes for buildings in smart grids",2013,"IEEE International Conference on Smart Energy Grid Engineering, SEGE 2013",26,10.1109/SEGE.2013.6707934,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893776004&doi=10.1109%2fSEGE.2013.6707934&partnerID=40&md5=92018dc9fef9a3897e50f3f203ed3747","The increasing share of distributed energy resources and renewable energy in power systems results in a highly variable and less controllable energy production. Therefore, in order to ensure stability and to reduce the infrastructure and operation cost of the power grid, flexible and controllable demand is needed. The research area of demand side management is still very much in flux and several options are being presented which can all be used to manage loads in order to achieve a flexible and more responsive demand. These different control schemes are developed with different organization of the power sector in mind and thus can differ significantly in their architecture, their integration into the various markets, their integration into distribution network operation and several other aspects. This paper proposes a classification of load control policies for demand side management in smart buildings, based on external behavior: direct, indirect, transactional and autonomous control; internal operation: decision support system scope, control strategy, failure handling and architecture. This classification assists in providing an overview of the control schemes as well as different ways of representing a building. © 2013 IEEE.",,"Autonomous control; Control strategies; Demand side managements; Distributed Energy Resources; Distribution network operation; Energy productions; Internal operations; Renewable energies; Artificial intelligence; Decision support systems; Energy resources; Network architecture; Smart power grids",Conference Paper,Scopus,2-s2.0-84893776004
"Kosínski W., Prokopowicz P., Rosa A.","Defuzzification functionals of ordered fuzzy numbers",2013,"IEEE Transactions on Fuzzy Systems",26,10.1109/TFUZZ.2013.2243456,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897711366&doi=10.1109%2fTFUZZ.2013.2243456&partnerID=40&md5=ab177abb4c25c8883f96fe09f3c5cb83","Defuzzification functionals, which play the main role when dealing with fuzzy controllers and fuzzy inference systems, for convex as well for ordered fuzzy numbers, are discussed. Three characteristic conditions for them are formulated. It is shown that most of the known defuzzification functionals satisfy them. Motivations for introducing the extended class of convex fuzzy numbers are presented, together with operations on them.© 2013 IEEE.","Convex fuzzy numbers (CFNs); Defuzzification functionals; Homogeneous functions; Ordered fuzzy numbers (OFNs)","Defuzzifications; Functionals; Fuzzy controllers; Fuzzy inference systems; Fuzzy numbers; Homogeneous functions; Ordered fuzzy number; Artificial intelligence; Fuzzy sets; Fuzzy rules",Article,Scopus,2-s2.0-84897711366
"Li C.-M., Fang Z., Xu K.","Combining MaxSAT reasoning and incremental upper bound for the maximum clique problem",2013,"Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI",26,10.1109/ICTAI.2013.143,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897696787&doi=10.1109%2fICTAI.2013.143&partnerID=40&md5=4aaf6f14dbf023e76ee736d5812a8357","Recently, MaxSAT reasoning has been shown tobe powerful in computing upper bounds for the cardinalityof a maximum clique of a graph. However, existing upperbounds based on MaxSAT reasoning have two drawbacks: (1)at every node of the search tree, MaxSAT reasoning has to beperformed from scratch to compute an upper bound and istime-consuming, (2) due to the NP-hardness of the MaxSATproblem, MaxSAT reasoning generally cannot be complete at anode of a search tree, and may not give an upper bound tightenough for pruning search space. In this paper, we proposean incremental upper bound and combine it with MaxSATreasoning to remedy the two drawbacks. The new approach isused to develop an efficient branch-and-bound algorithm forMaxClique, called IncMaxCLQ. We conduct experiments toshow the complementarity of the incremental upper bound andMaxSAT reasoning and to compare IncMaxCLQ with severalstate-of-the-art algorithms for MaxClique. © 2013 IEEE.","Incremental Upper Bound; MaxClique; MaxSAT","Branch-and-bound algorithms; Max-SAT; MaxClique; Maximum clique; Maximum clique problems; New approaches; Search spaces; Upper Bound; Algorithms; Artificial intelligence; Tools; Forestry; Algorithms; Forestry; Tools",Conference Paper,Scopus,2-s2.0-84897696787
"Kupcsik A.G., Deisenroth M.P., Peters J., Neumann G.","Data-efficient generalization of robot skills with contextual policy search",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",26,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893352715&partnerID=40&md5=e1d7b6c135d41913570df907fe53f486","In robotics, controllers make the robot solve a task within a specific context. The context can describe the objectives of the robot or physical properties of the environment and is always specified before task execution. To generalize the controller to multiple contexts, we follow a hierarchical approach for policy learning: A lower-level policy controls the robot for a given context and an upper-level policy generalizes among contexts. Current approaches for learning such upper-level policies are based on model-free policy search, which require an excessive number of interactions of the robot with its environment. More data-efficient policy search approaches are model based but, thus far, without the capability of learning hierarchical policies. We propose a new model-based policy search approach that can also learn contextual upper-level policies. Our approach is based on learning probabilistic forward models for long-term predictions. Using these predictions, we use information-theoretic insights to improve the upper-level policy. Our method achieves a substantial improvement in learning speed compared to existing methods on simulated and real robotic tasks. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Hierarchical approach; Hierarchical policy; Long-term prediction; Model-based OPC; Multiple contexts; Policy control; Policy learning; Task executions; Artificial intelligence; Forecasting; Robotics; Robots",Conference Paper,Scopus,2-s2.0-84893352715
"Wang S., Liu D., Zhang Z.","Nonconvex relaxation approaches to robust matrix recovery",2013,"IJCAI International Joint Conference on Artificial Intelligence",26,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062135&partnerID=40&md5=e3c299fa87bad0fa715289f9104ed714","Motivated by the recent developments of nonconvex penalties in sparsity modeling, we propose a nonconvex optimization model for handing the low-rank matrix recovery problem. Different from the famous robust principal component analysis (RPCA), we suggest recovering low-rank and sparse matrices via a nonconvex loss function and a nonconvex penalty. The advantage of the nonconvex approach lies in its stronger robustness. To solve the model, we devise a majorization-minimization augmented Lagrange multiplier (MM-ALM) algorithm which finds the local optimal solutions of the proposed nonconvex model. We also provide an efficient strategy to speedup MM-ALM, which makes the running time comparable with the state-of-the-art algorithm of solving RPCA. Finally, empirical results demonstrate the superiority of our nonconvex approach over RPCA in terms of matrix recovery accuracy.",,"Augmented lagrange multipliers; Local optimal solution; Low-rank matrix recoveries; Non-convex loss function; Nonconvex optimization; Nonconvex penalties; Robust principal component analysis; State-of-the-art algorithms; Algorithms; Artificial intelligence; Lagrange multipliers; Principal component analysis; Recovery",Conference Paper,Scopus,2-s2.0-84896062135
"Balduini M., Della Valle E., Dell'Aglio D., Tsytsarau M., Palpanas T., Confalonieri C.","Social listening of city scale events using the streaming linked data framework",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",26,10.1007/978-3-642-41338-4_1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891942595&doi=10.1007%2f978-3-642-41338-4_1&partnerID=40&md5=ab2cc0608e6245146b2df1041bf9d13b","City-scale events may easily attract half a million of visitors in hundreds of venues over just a few days. Which are the most attended venues? What do visitors think about them? How do they feel before, during and after the event? These are few of the questions a city-scale event manger would like to see answered in real-time. In this paper, we report on our experience in social listening of two city-scale events (London Olympic Games 2012, and Milano Design Week 2013) using the Streaming Linked Data Framework. © 2013 Springer-Verlag.",,"City scale; Linked datum; Olympic games; Artificial intelligence; Computer science; Computers; Data handling",Conference Paper,Scopus,2-s2.0-84891942595
"Caluwaerts K., D'Haene M., Verstraeten D., Schrauwen B.","Locomotion without a brain: physical reservoir computing in tensegrity structures.",2013,"Artificial life",26,10.1162/ARTL_a_00080,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875640620&doi=10.1162%2fARTL_a_00080&partnerID=40&md5=bb836a616836770e982baa0ca4765bf7","Embodiment has led to a revolution in robotics by not thinking of the robot body and its controller as two separate units, but taking into account the interaction of the body with its environment. By investigating the effect of the body on the overall control computation, it has been suggested that the body is effectively performing computations, leading to the term morphological computation. Recent work has linked this to the field of reservoir computing, allowing one to endow morphologies with a theory of universal computation. In this work, we study a family of highly dynamic body structures, called tensegrity structures, controlled by one of the simplest kinds of ""brains."" These structures can be used to model biomechanical systems at different scales. By analyzing this extreme instantiation of compliant structures, we demonstrate the existence of a spectrum of choices of how to implement control in the body-brain composite. We show that tensegrity structures can maintain complex gaits with linear feedback control and that external feedback can intrinsically be integrated in the control loop. The various linear learning rules we consider differ in biological plausibility, and no specific assumptions are made on how to implement the feedback in a physical system.",,"algorithm; article; artificial intelligence; biomechanics; computer simulation; feedback system; gait; human; learning; locomotion; man machine interaction; methodology; motion; oscillometry; regression analysis; robotics; tensile strength; Algorithms; Artificial Intelligence; Biomechanics; Computer Simulation; Feedback; Gait; Humans; Learning; Least-Squares Analysis; Locomotion; Man-Machine Systems; Motion; Oscillometry; Robotics; Tensile Strength",Article,Scopus,2-s2.0-84875640620
"Zhang R., Lan Y., Huang G.-B., Xu Z.-B., Soh Y.C.","Dynamic extreme learning machine and its approximation capability",2013,"IEEE Transactions on Cybernetics",26,10.1109/TCYB.2013.2239987,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890110727&doi=10.1109%2fTCYB.2013.2239987&partnerID=40&md5=2bacd922a496ff4d633c15961d171453","Extreme learning machines (ELMs) have been proposed for generalized single-hidden-layer feedforward networks which need not be neuron alike and perform well in both regression and classification applications. The problem of determining the suitable network architectures is recognized to be crucial in the successful application of ELMs. This paper first proposes a dynamic ELM (D-ELM) where the hidden nodes can be recruited or deleted dynamically according to their significance to network performance, so that not only the parameters can be adjusted but also the architecture can be self-adapted simultaneously. Then, this paper proves in theory that such D-ELM using Lebesgue p-integrable hidden activation functions can approximate any Lebesgue p-integrable function on a compact input set. Simulation results obtained over various test problems demonstrate and verify that the proposed D-ELM does a good job reducing the network size while preserving good generalization performance. © 2013 IEEE.","Dynamic learning; extreme learning machine (ELM); feedforward neural networks; universal approximation","Activation functions; Approximation capabilities; Dynamic learning; Extreme learning machine; Feed-forward network; Generalization performance; Network size; Universal approximation; Feedforward neural networks; Learning systems; Network architecture; Network layers; Knowledge acquisition; algorithm; article; artificial intelligence; artificial neural network; automated pattern recognition; computer simulation; decision support system; methodology; theoretical model; Algorithms; Artificial Intelligence; Computer Simulation; Decision Support Techniques; Models, Theoretical; Neural Networks (Computer); Pattern Recognition, Automated",Article,Scopus,2-s2.0-84890110727
"Liu Z.-H., Zhang J., Zhou S.-W., Li X.-H., Liu K.","Coevolutionary particle swarm optimization using AIS and its application in multiparameter estimation of PMSM",2013,"IEEE Transactions on Cybernetics",26,10.1109/TSMCB.2012.2235828,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890024304&doi=10.1109%2fTSMCB.2012.2235828&partnerID=40&md5=45396b6a16059c7c4607495ff90538cc","In this paper, a coevolutionary particle-swarm-optimization (PSO) algorithm associating with the artificial immune principle is proposed. In the proposed algorithm, the whole population is divided into two kinds of subpopulations consisting of one elite subpopulation and several normal subpopulations. The best individual of each normal subpopulation will be memorized into the elite subpopulation during the evolution process. A hybrid method, which creates new individuals by using three different operators, is presented to ensure the diversity of all the subpopulations. Furthermore, a simple adaptive wavelet learning operator is utilized for accelerating the convergence speed of the pbest particles. The improved immune-clonal-selection operator is employed for optimizing the elite subpopulation, while the migration scheme is employed for the information exchange between elite subpopulation and normal subpopulations. The performance of the proposed algorithm is verified by testing on a suite of standard benchmark functions, which shows faster convergence and global search ability. Its performance is further evaluated by its application to multiparameter estimation of permanent-magnet synchronous machines, which shows that its performance significantly outperforms existing PSOs. The proposed algorithm can estimate the machine dq-axis inductances, stator winding resistance, and rotor flux linkage simultaneously. © 2013 IEEE.","Artificial immune system (AIS); Coevolution; Elite population; Global search; Migration scheme; Parameter estimation; Particle swarm optimization (PSO); Permanent-magnet synchronous machines (PMSMs)","Artificial Immune System; Co-evolution; Elite populations; Global search; Migration scheme; Permanent magnet synchronous machines; Ability testing; Algorithms; Benchmarking; Electric resistance; Estimation; Parameter estimation; Synchronous machinery; Particle swarm optimization (PSO); algorithm; animal; article; artificial intelligence; automated pattern recognition; biomimetics; evolution; human; immunology; innate immunity; methodology; wavelet analysis; Algorithms; Animals; Artificial Intelligence; Biological Evolution; Biomimetics; Humans; Immunity, Innate; Pattern Recognition, Automated; Wavelet Analysis",Article,Scopus,2-s2.0-84890024304
"Bui H.H., Huynh T.N., Riedel S.","Automorphism groups of graphical models and lifted variational inference",2013,"Uncertainty in Artificial Intelligence - Proceedings of the 29th Conference, UAI 2013",26,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888166143&partnerID=40&md5=619c042ed3dafb5b132dc7b39febef16","Using the theory of group action, we first introduce the concept of the automorphism group of an exponential family or a graphical model, thus formalizing the general notion of symmetry of a probabilistic model. This automorphism group provides a precise mathematical framework for lifted inference in the general exponential family. Its group action partitions the set of random variables and feature functions into equivalent classes (called orbits) having identical marginals and expectations. Then the inference problem is effectively reduced to that of computing marginals or expectations for each class, thus avoiding the need to deal with each individual variable or feature. We demonstrate the usefulness of this general framework in lifting two classes of variational approximation for maximum a posteriori (MAP) inference: local linear programming (LP) relaxation and local LP relaxation with cycle constraints; the latter yields the first lifted variational inference algorithm that operates on a bound tighter than the local constraints.",,"Automorphism groups; Exponential family; Local constraints; Mathematical frameworks; Maximum a posteriori; Probabilistic modeling; Variational approximation; Variational inference; Approximation algorithms; Artificial intelligence; Equivalence classes; Graphic methods; Inference engines; Speech recognition; Group theory",Conference Paper,Scopus,2-s2.0-84888166143
"Schreuder M., Riccio A., Risetti M., Dähne S., Ramsay A., Williamson J., Mattia D., Tangermann M.","User-centered design in brain-computer interfaces-A case study",2013,"Artificial Intelligence in Medicine",26,10.1016/j.artmed.2013.07.005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886784445&doi=10.1016%2fj.artmed.2013.07.005&partnerID=40&md5=1dcab6e9a52d8b12029befcb0e59be21","Objective: The array of available brain-computer interface (BCI) paradigms has continued to grow, and so has the corresponding set of machine learning methods which are at the core of BCI systems. The latter have evolved to provide more robust data analysis solutions, and as a consequence the proportion of healthy BCI users who can use a BCI successfully is growing. With this development the chances have increased that the needs and abilities of specific patients, the end-users, can be covered by an existing BCI approach. However, most end-users who have experienced the use of a BCI system at all have encountered a single paradigm only. This paradigm is typically the one that is being tested in the study that the end-user happens to be enrolled in, along with other end-users. Though this corresponds to the preferred study arrangement for basic research, it does not ensure that the end-user experiences a working BCI. In this study, a different approach was taken; that of a user-centered design. It is the prevailing process in traditional assistive technology. Given an individual user with a particular clinical profile, several available BCI approaches are tested and - if necessary - adapted to him/her until a suitable BCI system is found. Methods: Described is the case of a 48-year-old woman who suffered from an ischemic brain stem stroke, leading to a severe motor- and communication deficit. She was enrolled in studies with two different BCI systems before a suitable system was found. The first was an auditory event-related potential (ERP) paradigm and the second a visual ERP paradigm, both of which are established in literature. Results: The auditory paradigm did not work successfully, despite favorable preconditions. The visual paradigm worked flawlessly, as found over several sessions. This discrepancy in performance can possibly be explained by the user's clinical deficit in several key neuropsychological indicators, such as attention and working memory. While the auditory paradigm relies on both categories, the visual paradigm could be used with lower cognitive workload. Besides attention and working memory, several other neurophysiological and -psychological indicators - and the role they play in the BCIs at hand - are discussed. Conclusion: The user's performance on the first BCI paradigm would typically have excluded her from further ERP-based BCI studies. However, this study clearly shows that, with the numerous paradigms now at our disposal, the pursuit for a functioning BCI system should not be stopped after an initial failed attempt. © 2013 The Authors.","Assistive technology; Auditory evoked potentials; Brain-computer interface; Event-related potentials; Linear discriminant analysis; Locked-in syndrome; Stroke; Traumatic brain injury; User-centered design","Assistive technology; Event related potentials; Linear discriminant analysis; Locked-in syndrome; Stroke; Traumatic Brain Injuries; User centered designs; Bioelectric potentials; Brain; Design; Interfaces (computer); Learning systems; Brain computer interface; adult; article; attention; brain computer interface; brain stem; case report; cerebrovascular accident; cognition; communication disorder; computer; evoked auditory response; female; human; ischemic brain stem stroke; motor dysfunction; neurophysiology; neuropsychology; priority journal; working memory; Assistive technology; Auditory evoked potentials; Brain-computer interface; Event-related potentials; Linear discriminant analysis; Locked-in syndrome; Stroke; Traumatic brain injury; User-centered design; Artificial Intelligence; Brain-Computer Interfaces; Electroencephalography; Female; Humans; Middle Aged; Neuropsychological Tests; Questionnaires; Stroke",Article,Scopus,2-s2.0-84886784445
"Liu W., Zhang Y., Tang S., Tang J., Hong R., Li J.","Accurate estimation of human body orientation from RGB-D sensors",2013,"IEEE Transactions on Cybernetics",26,10.1109/TCYB.2013.2272636,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890349723&doi=10.1109%2fTCYB.2013.2272636&partnerID=40&md5=22bee2ef90cc353c1c8e5a44bb54179b","Accurate estimation of human body orientation can significantly enhance the analysis of human behavior, which is a fundamental task in the field of computer vision. However, existing orientation estimation methods cannot handle the various body poses and appearances. In this paper, we propose an innovative RGB-D-based orientation estimation method to address these challenges. By utilizing the RGB-D information, which can be real time acquired by RGB-D sensors, our method is robust to cluttered environment, illumination change and partial occlusions. Specifically, efficient static and motion cue extraction methods are proposed based on the RGB-D superpixels to reduce the noise of depth data. Since it is hard to discriminate all the 360° orientation using static cues or motion cues independently, we propose to utilize a dynamic Bayesian network system (DBNS) to effectively employ the complementary nature of both static and motion cues. In order to verify our proposed method, we build a RGB-D-based human body orientation dataset that covers a wide diversity of poses and appearances. Our intensive experimental evaluations on this dataset demonstrate the effectiveness and efficiency of the proposed method. © 2013 IEEE.","DBNS; Human body orientation estimation; RGB-D; Superpixel","Cluttered environments; DBNS; Dynamic Bayesian networks; Effectiveness and efficiencies; Experimental evaluation; Human bodies; RGB-D; Super pixels; Sensors; Estimation; actimetry; algorithm; artificial intelligence; automated pattern recognition; body posture; computer; computer simulation; devices; human; image enhancement; orientation; physiology; procedures; recreation; three dimensional imaging; transducer; whole body imaging; article; automated pattern recognition; body posture; equipment; methodology; orientation; physiology; three dimensional imaging; whole body imaging; Actigraphy; Algorithms; Artificial Intelligence; Computer Peripherals; Computer Simulation; Humans; Image Enhancement; Imaging, Three-Dimensional; Orientation; Pattern Recognition, Automated; Posture; Transducers; Video Games; Whole Body Imaging; Actigraphy; Algorithms; Artificial Intelligence; Computer Peripherals; Computer Simulation; Humans; Image Enhancement; Imaging, Three-Dimensional; Orientation; Pattern Recognition, Automated; Posture; Transducers; Video Games; Whole Body Imaging",Article,Scopus,2-s2.0-84890349723
"Jiang J., Wu Y., Huang M., Yang W., Chen W., Feng Q.","3D brain tumor segmentation in multimodal MR images based on learning population- and patient-specific feature sets",2013,"Computerized Medical Imaging and Graphics",26,10.1016/j.compmedimag.2013.05.007,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888050597&doi=10.1016%2fj.compmedimag.2013.05.007&partnerID=40&md5=36ad501ddc32c02b2a67b3d9df9eba38","Brain tumor segmentation is a clinical requirement for brain tumor diagnosis and radiotherapy planning. Automating this process is a challenging task due to the high diversity in appearance of tumor tissue among different patients and the ambiguous boundaries of lesions. In this paper, we propose a method to construct a graph by learning the population- and patient-specific feature sets of multimodal magnetic resonance (MR) images and by utilizing the graph-cut to achieve a final segmentation. The probabilities of each pixel that belongs to the foreground (tumor) and the background are estimated by global and custom classifiers that are trained through learning population- and patient-specific feature sets, respectively. The proposed method is evaluated using 23 glioma image sequences, and the segmentation results are compared with other approaches. The encouraging evaluation results obtained, i.e., DSC (84.5%), Jaccard (74.1%), sensitivity (87.2%), and specificity (83.1%), show that the proposed method can effectively make use of both population- and patient-specific information. © 2013.","Brain tumor; Graph-cut; Multimodal; Segmentation","Brain tumor segmentation; Brain tumors; Evaluation results; Graph-cut; Multi-modal; Patient specific; Radiotherapy planning; Segmentation results; Brain; Diagnosis; Graphic methods; Magnetic resonance; Magnetic resonance imaging; Tumors; Image segmentation; gadolinium; article; brain tumor; distance metric learning; glioma; human; image analysis; learning; learning algorithm; neuroimaging; nuclear magnetic resonance imaging; priority journal; probability; sensitivity and specificity; Brain tumor; Graph-cut; Multimodal; Segmentation; Algorithms; Artificial Intelligence; Brain Neoplasms; Glioma; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Multimodal Imaging; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-84888050597
"Badrinarayanan V., Budvytis I., Cipolla R.","Semi-supervised video segmentation using tree structured graphical models",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",26,10.1109/TPAMI.2013.54,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884565372&doi=10.1109%2fTPAMI.2013.54&partnerID=40&md5=bc32b6064aaf06cf20cea20db4229f20","We present a novel patch-based probabilistic graphical model for semi-supervised video segmentation. At the heart of our model is a temporal tree structure that links patches in adjacent frames through the video sequence. This permits exact inference of pixel labels without resorting to traditional short time window-based video processing or instantaneous decision making. The input to our algorithm is labeled key frame(s) of a video sequence and the output is pixel-wise labels along with their confidences. We propose an efficient inference scheme that performs exact inference over the temporal tree, and optionally a per frame label smoothing step using loopy BP, to estimate pixel-wise labels and their posteriors. These posteriors are used to learn pixel unaries by training a Random Decision Forest in a semi-supervised manner. These unaries are used in a second iteration of label inference to improve the segmentation quality. We demonstrate the efficacy of our proposed algorithm using several qualitative and quantitative tests on both foreground/background and multiclass video segmentation problems using publicly available and our own datasets. © 1979-2012 IEEE.","label propagation; mixture of trees graphical model; Semi-supervised video segmentation; structured variational inference; tree-structured video models","GraphicaL model; Label propagation; Variational inference; Video model; Video segmentation; Algorithms; Graphic methods; Image segmentation; Iterative methods; Pixels; Speech recognition; Video recording; Video signal processing; Forestry; Algorithms; Forestry; Image Analysis; Mathematical Models; Problem Solving; Signals; algorithm; artificial intelligence; automated pattern recognition; computer assisted diagnosis; computer simulation; image enhancement; image subtraction; photography; procedures; signal processing; theoretical model; videorecording; article; automated pattern recognition; computer assisted diagnosis; methodology; photography; videorecording; Algorithms; Artificial Intelligence; Computer Simulation; Image Enhancement; Image Interpretation, Computer-Assisted; Models, Theoretical; Pattern Recognition, Automated; Photography; Signal Processing, Computer-Assisted; Subtraction Technique; Video Recording; Algorithms; Artificial Intelligence; Computer Simulation; Image Enhancement; Image Interpretation, Computer-Assisted; Models, Theoretical; Pattern Recognition, Automated; Photography; Signal Processing, Computer-Assisted; Subtraction Technique; Video Recording",Article,Scopus,2-s2.0-84884565372
"Raczaszek-Leonardi J., Nomikou I., Rohlfing K.J.","Young children's dialogical actions: The beginnings of purposeful intersubjectivity",2013,"IEEE Transactions on Autonomous Mental Development",26,10.1109/TAMD.2013.2273258,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884516289&doi=10.1109%2fTAMD.2013.2273258&partnerID=40&md5=5eee36713981c738939bf8033e363410","Are higher-level cognitive processes the only way that purposefulness can be introduced into the human interaction In this paper, we provide a microanalysis of early mother-child interactions and argue that the beginnings of joint intentionality can be traced to the practice of embedding the child's actions into culturally shaped episodes. As action becomes coaction, an infant's perception becomes tuned to interaction affordances. © 2013 IEEE.","Cognitive development; intentions; inter subjectivity; social interaction","Cognitive development; Cognitive process; Human interactions; Intentionality; intentions; inter subjectivity; Social interactions; Young children; Software engineering; Artificial intelligence",Article,Scopus,2-s2.0-84884516289
"Athan T., Boley H., Governatori G., Palmirani M., Paschke A., Wyner A.","OASIS LegalRuleML",2013,"Proceedings of the International Conference on Artificial Intelligence and Law",26,10.1145/2514601.2514603,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883524875&doi=10.1145%2f2514601.2514603&partnerID=40&md5=87bac9e1012d079a6c1ebe85ab2bcb57","In this paper we present the motivation, use cases, design principles, abstract syntax, and initial core of LegalRuleML. The LegalRuleML-core is sufficiently rich for expressing legal sources, time, defeasibil-ity, and deontic operators. An example is provided. LegalRuleMLis compared to related work. Copyright 2013 ACM.","LegalRuleML","Abstract syntax; Deontic operators; Design Principles; LegalRuleML; Related works; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84883524875
"Li Y.","Design of a key establishment protocol for smart home energy management system",2013,"Proceedings - 5th International Conference on Computational Intelligence, Communication Systems, and Networks, CICSyN 2013",26,10.1109/CICSYN.2013.42,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883438554&doi=10.1109%2fCICSYN.2013.42&partnerID=40&md5=aeee2fe540b43f86118a5d9ec58dc0e7","With the fast development of Wireless Sensor Networks (WSNs) and RFID technology, many Internet of Things (IOT) applications have been deployed in recent years. Smart home energy management systems form part of the smart grid program and are a fast developing smart home application area. Inadequate security is a big issue in smart home energy management systems. Most security protocols widely used for computer network and internet security cannot be implemented in smart home energy management systems as they are computational expensive for the wireless sensor nodes used in smart home applications. The major issue in the security of smart home energy management systems is the establishment of the initial session key between the wireless nodes and control center. In this paper, we propose a lightweight key establishment protocol for smart home energy management systems and present the implementation details of the protocol. © 2013 IEEE.","key establishment; smart home energy management system; wireless sensor network","Internet of Things (IOT); Internet security; Key establishment protocol; Key establishments; Security protocols; Smart home energy management systems; Wireless sensor network (WSNs); Wireless sensor node; Artificial intelligence; Automation; Communication systems; Energy management; Internet; Network security; Sensor nodes; Wireless sensor networks; Intelligent buildings",Conference Paper,Scopus,2-s2.0-84883438554
"Tai Y.-W., Chen X., Kim S., Kim S.J., Li F., Yang J., Yu J., Matsushita Y., Brown M.S.","Nonlinear camera response functions and image deblurring: Theoretical analysis and practice",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",26,10.1109/TPAMI.2013.40,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883144504&doi=10.1109%2fTPAMI.2013.40&partnerID=40&md5=da308d1aa8ba07775326034eec6b7550","This paper investigates the role that nonlinear camera response functions (CRFs) have on image deblurring. We present a comprehensive study to analyze the effects of CRFs on motion deblurring. In particular, we show how nonlinear CRFs can cause a spatially invariant blur to behave as a spatially varying blur. We prove that such nonlinearity can cause large errors around edges when directly applying deconvolution to a motion blurred image without CRF correction. These errors are inevitable even with a known point spread function (PSF) and with state-of-the-art regularization-based deconvolution algorithms. In addition, we show how CRFs can adversely affect PSF estimation algorithms in the case of blind deconvolution. To help counter these effects, we introduce two methods to estimate the CRF directly from one or more blurred images when the PSF is known or unknown. Our experimental results on synthetic and real images validate our analysis and demonstrate the robustness and accuracy of our approaches. © 1979-2012 IEEE.","CRF estimation; motion deblurring; Nonlinear camera response functions (CRFs)","Blind deconvolution; Camera response functions; Deconvolution algorithm; Image deblurring; Motion blurred image; Motion deblurring; Point-spread functions; Spatially invariants; Cameras; Convolution; Deconvolution; Errors; Estimation; Image enhancement; Optical transfer function; Nonlinear analysis; algorithm; article; artifact; artificial intelligence; automated pattern recognition; computer assisted diagnosis; image enhancement; methodology; nonlinear system; reproducibility; sensitivity and specificity; automated pattern recognition; computer assisted diagnosis; image enhancement; procedures; Algorithms; Artifacts; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Nonlinear Dynamics; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Algorithms; Artifacts; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Nonlinear Dynamics; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84883144504
"Yeo L., Romero R.","Fetal Intelligent Navigation Echocardiography (FINE): A novel method for rapid, simple, and automatic examination of the fetal heart",2013,"Ultrasound in Obstetrics and Gynecology",26,10.1002/uog.12563,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883499638&doi=10.1002%2fuog.12563&partnerID=40&md5=37728c638eee7560fae95ff2d05fa458","Objective: To describe a novel method (Fetal Intelligent Navigation Echocardiography (FINE)) for visualization of standard fetal echocardiography views from volume datasets obtained with spatiotemporal image correlation (STIC) and application of 'intelligent navigation' technology. Methods: We developed a method to: 1) demonstrate nine cardiac diagnostic planes; and 2) spontaneously navigate the anatomy surrounding each of the nine cardiac diagnostic planes (Virtual Intelligent Sonographer Assistance (VIS-Assistance®)). The method consists of marking seven anatomical structures of the fetal heart. The following echocardiography views are then automatically generated: 1) four chamber; 2) five chamber; 3) left ventricular outflow tract; 4) short-axis view of great vessels/right ventricular outflow tract; 5) three vessels and trachea; 6) abdomen/stomach; 7) ductal arch; 8) aortic arch; and 9) superior and inferior vena cava. The FINE method was tested in a separate set of 50 STIC volumes of normal hearts (18.6-37.2 weeks of gestation), and visualization rates for fetal echocardiography views using diagnostic planes and/or VIS-Assistance® were calculated. To examine the feasibility of identifying abnormal cardiac anatomy, we tested the method in four cases with proven congenital heart defects (coarctation of aorta, tetralogy of Fallot, transposition of great vessels and pulmonary atresia with intact ventricular septum). Results: In normal cases, the FINE method was able to generate nine fetal echocardiography views using: 1) diagnostic planes in 78-100% of cases; 2) VIS-Assistance® in 98-100% of cases; and 3) a combination of diagnostic planes and/or VIS-Assistance® in 98-100% of cases. In all four abnormal cases, the FINE method demonstrated evidence of abnormal fetal cardiac anatomy. Conclusions The FINE method can be used to visualize nine standard fetal echocardiography views in normal hearts by applying 'intelligent navigation' technology to STIC volume datasets. This method can simplify examination of the fetal heart and reduce operator dependency. The observation of abnormal echocardiography views in the diagnostic planes and/or VIS-Assistance® should raise the index of suspicion for congenital heart disease. Copyright © 2013 ISUOG. Published by John Wiley & Sons Ltd.","4D; cardiac; congenital heart disease; fetal heart; prenatal diagnosis; spatiotemporal image correlation; STIC; ultrasound; Virtual Intelligent Sonographer Assistance; VIS-Assistance®","aorta coarctation; artificial intelligence; congenital heart malformation; echography; Fallot tetralogy; female; fetus disease; fetus echography; fetus heart; great vessels transposition; human; image processing; pregnancy; procedures; pulmonary valve atresia; three dimensional echocardiography; 4D; article; cardiac; congenital heart disease; congenital heart malformation; congenital malformation; fetus disease; fetus echography; fetus heart; methodology; physiology; prenatal diagnosis; pulmonary atresia with intact ventricular septum; spatiotemporal image correlation; STIC; three dimensional echocardiography; ultrasound; Virtual Intelligent Sonographer Assistance; VIS-Assistance®; Aortic Coarctation; Artificial Intelligence; Echocardiography, Three-Dimensional; Female; Fetal Diseases; Fetal Heart; Heart Defects, Congenital; Humans; Image Processing, Computer-Assisted; Pregnancy; Pulmonary Atresia; Tetralogy of Fallot; Transposition of Great Vessels; Ultrasonography, Prenatal; 4D; cardiac; congenital heart disease; fetal heart; prenatal diagnosis; spatiotemporal image correlation; STIC; ultrasound; Virtual Intelligent Sonographer Assistance; VIS-Assistance®; Aortic Coarctation; Artificial Intelligence; Echocardiography, Three-Dimensional; Female; Fetal Diseases; Fetal Heart; Heart Defects, Congenital; Humans; Image Processing, Computer-Assisted; Pregnancy; Pulmonary Atresia; Tetralogy of Fallot; Transposition of Great Vessels; Ultrasonography, Prenatal",Article,Scopus,2-s2.0-84883499638
"Ge Y., Li X., Huang C., Nan Z.","A Decision Support System for irrigation water allocation along the middle reaches of the Heihe River Basin, Northwest China",2013,"Environmental Modelling and Software",26,10.1016/j.envsoft.2013.05.010,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879548064&doi=10.1016%2fj.envsoft.2013.05.010&partnerID=40&md5=ac731795db2e1dee21ff50472cb5af7f","To improve the water resource management of the inland river basins of northwestern China, a Decision Support System (DSS) is developed to provide an operative computer platform for decision makers. The DSS is designed according to actual water resource management problems and is seamlessly integrated into a user-friendly interface implemented in the Visual C# programming language. The DSS comprises an information management system that performs data collection, verification, management, and visualization, and models estimated crop water demand and water allocation for different levels of water use units. The objective of this study is to aid in the decision-making process related to water allocation scheme planning and implementation and to aid real-time responses to changes in water supply, allowing a new water allocation scheme to be developed based on the actual relationship between the supply and demand for water. The system is tested to allocate water to different levels of water use units as a standard decision support tool by means of the actual total available water from rivers, reservoirs, and groundwater. More than 60 water decision makers use the system at more than 40 locations along the middle reaches of the Heihe River Basin. © 2013 Elsevier Ltd.","Decision Support System; GIS; Irrigation management; Irrigation water allocation; Water resource management","Decision making process; Decision support system (dss); Decision support tools; Information management systems; Irrigation management; Irrigation water allocation; User friendly interface; Waterresource management; Artificial intelligence; Decision making; Decision support systems; Economics; Geographic information systems; Groundwater; Irrigation; Reservoirs (water); Water management; Water supply; Watersheds; Information management; basin management; decision making; GIS; groundwater resource; irrigation system; real time; reservoir characterization; China; Gansu; Heihe Basin",Article,Scopus,2-s2.0-84879548064
"Kalayci C.B., Gupta S.M.","Ant colony optimization for sequence-dependent disassembly line balancing problem",2013,"Journal of Manufacturing Technology Management",26,10.1108/17410381311318909,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878666440&doi=10.1108%2f17410381311318909&partnerID=40&md5=2bb521e3e1eb7b6c0fd70f789ed796c5","Purpose - The purpose of this paper is to introduce sequence-dependent disassembly line balancing problem (SDDLBP) to the literature and propose an efficient metaheuristic solution methodology to this NP-complete problem. Design/methodology/approach - This manuscript utilizes a well-proven metaheuristics solution methodology, namely, ant colony optimization, to address the problem. Findings - Since SDDLBP is NP-complete, finding an optimal balance becomes computationally prohibitive due to exponential growth of the solution space with the increase in the number of parts. The proposed methodology is very fast, generates (near) optimal solutions, preserves precedence requirements and is easy to implement. Practical implications - Since development of cost effective and profitable disassembly systems is an important issue in end-of-life product treatment, every step towards improving disassembly line balancing brings us closer to cost savings and compelling practicality. Originality/value - This paper introduces a new problem (SDDLBP) and an efficient solution to the literature. © Emerald Group Publishing Limited.","Ant colony optimization; Combinatorial optimization; Disassembly; Metaheuristics; Optimization techniques; Product recovery; Production planning and control; Sequence-dependent disassembly line balancing","Disassembly; Disassembly line balancing; Meta heuristics; Optimization techniques; Product recovery; Production planning and control; Ant colony optimization; Combinatorial optimization; Computational complexity; Heuristic algorithms; Production control; Profitability; Artificial intelligence",Article,Scopus,2-s2.0-84878666440
"Manwani N., Sastry P.S.","Noise tolerance under risk minimization",2013,"IEEE Transactions on Cybernetics",26,10.1109/TSMCB.2012.2223460,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890431307&doi=10.1109%2fTSMCB.2012.2223460&partnerID=40&md5=3f347c9e64ed575ef3c63eb2c18ddd54","In this paper, we explore noise-tolerant learning of classifiers. We formulate the problem as follows. We assume that there is an unobservable training set that is noise free. The actual training set given to the learning algorithm is obtained from this ideal data set by corrupting the class label of each example. The probability that the class label of an example is corrupted is a function of the feature vector of the example. This would account for most kinds of noisy data one encounters in practice. We say that a learning method is noise tolerant if the classifiers learnt with noise-free data and with noisy data, both have the same classification accuracy on the noise-free data. In this paper, we analyze the noise-tolerance properties of risk minimization (under different loss functions). We show that risk minimization under 0-1 loss function has impressive noise-tolerance properties and that under squared error loss is tolerant only to uniform noise; risk minimization under other loss functions is not noise tolerant. We conclude this paper with some discussion on the implications of these theoretical results. © 2012 IEEE.","Label noise; Loss functions; Noise tolerance; Risk minimization","Classification accuracy; Feature vectors; Learning methods; Loss functions; Noise tolerance; Noise-tolerant learning; Risk minimization; Squared error loss; Learning algorithms; Risks; algorithm; artificial intelligence; automated pattern recognition; computer simulation; letter; methodology; risk reduction; signal noise ratio; statistical model; automated pattern recognition; procedures; Algorithms; Artificial Intelligence; Computer Simulation; Models, Statistical; Pattern Recognition, Automated; Risk Reduction Behavior; Signal-To-Noise Ratio; Algorithms; Artificial Intelligence; Computer Simulation; Models, Statistical; Pattern Recognition, Automated; Risk Reduction Behavior; Signal-To-Noise Ratio",Article,Scopus,2-s2.0-84890431307
"Chen H., Zhang Z.","A Semi-Supervised Method for Drug-Target Interaction Prediction with Consistency in Networks",2013,"PLoS ONE",26,10.1371/journal.pone.0062975,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877131233&doi=10.1371%2fjournal.pone.0062975&partnerID=40&md5=27c4b0d0ab6217f9724396f186e42466","Computational prediction of interactions between drugs and their target proteins is of great importance for drug discovery and design. The difficulties of developing computational methods for the prediction of such potential interactions lie in the rarity of known drug-protein interactions and no experimentally verified negative drug-target interaction sample. Furthermore, target proteins need also to be predicted for some new drugs without any known target interaction information. In this paper, a semi-supervised learning method NetCBP is presented to address this problem by using labeled and unlabeled interaction information. Assuming coherent interactions between the drugs ranked by their relevance to a query drug, and the target proteins ranked by their relevance to the hidden target proteins of the query drug, we formulate a learning framework maximizing the rank coherence with respect to the known drug-target interactions. When applied to four classes of important drug-target interaction networks, our method improves previous methods in terms of cross-validation and some strongly predicted interactions are confirmed by the publicly accessible drug target databases, which indicates the usefulness of our method. Finally, a comprehensive prediction of drug-target interactions enables us to suggest many new potential drug-target interactions for further studies. © 2013 Chen, Zhang.",,"article; calculation; controlled study; drug interaction; drug targeting; intermethod comparison; mathematical computing; mathematical parameters; Network Consistency based Prediction Method; prediction; protein targeting; Artificial Intelligence; Clozapine; Computational Biology; Drug Discovery; Humans; Pharmaceutical Preparations; Protein Binding; Proteins",Article,Scopus,2-s2.0-84877131233
"Kaldeli E., Warriach E.U., Lazovik A., Aiello M.","Coordinating the web of services for a smart home",2013,"ACM Transactions on the Web",26,10.1145/2460383.2460389,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879928714&doi=10.1145%2f2460383.2460389&partnerID=40&md5=2e7fd86ee6220d89118855407480fd3c","Domotics, concerned with the realization of intelligent home environments, is a novel field which can highly benefit from solutions inspired by service-oriented principles to enhance the convenience and security of modern home residents. In this work, we present an architecture for a smart home, starting from the lower device interconnectivity level up to the higher application layers that undertake the load of complex functionalities and provide a number of services to end-users. We claim that in order for smart homes to exhibit a genuinely intelligent behavior, the ability to compute compositions of individual devices automatically and dynamically is paramount. To this end, we incorporate into the architecture a composition component that employs artificial intelligence domain-independent planning to generate compositions at runtime, in a constantly evolving environment. We have implemented a fully working prototype that realizes such an architecture, and have evaluated it both in terms of performance as well as from the end-user point of view. The results of the evaluation show that the service-oriented architectural design and the support for dynamic compositions is quite efficient from the technical point of view, and that the system succeeds in satisfying the expectations and objectives of the users. © 2013 ACM.","Internet of things; Service composition; Service-oriented architecture","Application layers; Domain-independent planning; Dynamic composition; Individual devices; Intelligent behavior; Internet of Things (IOT); Number of services; Service compositions; Architecture; Artificial intelligence; Automation; Information services; Service oriented architecture (SOA); Web services; Intelligent buildings",Article,Scopus,2-s2.0-84879928714
"Bjelaković I., Boche H., Sommerfeld J.","Capacity results for arbitrarily varying wiretap channels",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",26,10.1007/978-3-642-36899-8-5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875981940&doi=10.1007%2f978-3-642-36899-8-5&partnerID=40&md5=9476777147b3800dabefaae35fbe5537","In this work the arbitrarily varying wiretap channel AVWC is studied. We derive a lower bound on the random code secrecy capacity for the average error criterion and the strong secrecy criterion in the case of a best channel to the eavesdropper by using Ahlswede's robustification technique for ordinary AVCs. We show that in the case of a non-symmetrisable channel to the legitimate receiver the deterministic code secrecy capacity equals the random code secrecy capacity, a result similar to Ahlswede's dichotomy result for ordinary AVCs. Using this we can derive that the lower bound is also valid for the deterministic code capacity of the AVWC. The proof of the dichotomy result is based on the elimination technique introduced by Ahlswede for ordinary AVCs. We further prove upper bounds on the deterministic code secrecy capacity in the general case, which results in a multi-letter expression for the secrecy capacity in the case of a best channel to the eavesdropper. Using techniques of Ahlswede, developed to guarantee the validity of a reliability criterion, the main contribution of this work is to integrate the strong secrecy criterion into these techniques. © Springer-Verlag Berlin Heidelberg 2013.","active wiretapper; arbitrarily varying wiretap channels; classes of attacks; jamming; strong secrecy","active wiretapper; classes of attacks; Dichotomy results; Elimination techniques; Reliability criterion; Robustification; Strong secrecies; Wire-tap channels; Artificial intelligence; Jamming; Communication channels (information theory)",Conference Paper,Scopus,2-s2.0-84875981940
"Sun A.","Enabling collaborative decision-making in watershed management using cloud-computing services",2013,"Environmental Modelling and Software",26,10.1016/j.envsoft.2012.11.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871669126&doi=10.1016%2fj.envsoft.2012.11.008&partnerID=40&md5=dd2e3b784065591282f3af3166e04d8f","Watershed management, in its very nature, represents a participatory process, requiring horizontal and vertical collaborations among multiple institutions and stakeholders. For watershed-scale management to be effective, a social-learning infrastructure needs to be in place to allow for the integration of diverse knowledge and interests related to watershed protection and restoration. Environmental decision support systems (EDSS) have long been used to support co-learning processes during watershed management. However, implementation and maintenance of EDSS in house often pose a significant burden to local watershed partnerships because of budgetary and technological constraints. Recent advances in service-oriented computing can help shift away the technical burden of EDSS implementation to service providers and enable watershed partnerships to focus primarily on decision-making activities. In this paper, I describe the migration of an EDSS module from the traditional client-server-based architecture to a client of cloud-computing services. Google Drive, which is behind the new version of the EDSS module, provides a number of basic visual analytics features that can be used to increase the collaborative decision-making experience while drastically reducing the cost of small-scale EDSS. More sophisticated EDSS may be implemented by leveraging the strengths of both client-server architectures and cloud-computing services. © 2012 Elsevier Ltd.","Cloud computing; Environmental decision-making; Total maximum daily load; Watershed management","Client-server architectures; Co-learning; Collaborative decision making; Environmental decision making; Environmental decision support systems; Participatory process; Service oriented computing; Service provider; Technological constraints; Total maximum daily load; Visual analytics; Watershed management; Watershed protection; Watershed-scale management; Artificial intelligence; Client server computer systems; Cloud computing; Decision making; Decision support systems; Landforms; Soil conservation; Water conservation; Water management; Watersheds; Environmental management; basin management; decision making; decision support system; Internet; participatory approach; stakeholder; watershed",Article,Scopus,2-s2.0-84871669126
"Li H.-X., Yang J.-L., Zhang G., Fan B.","Probabilistic support vector machines for classification of noise affected data",2013,"Information Sciences",26,10.1016/j.ins.2012.09.041,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868709802&doi=10.1016%2fj.ins.2012.09.041&partnerID=40&md5=c9190e24eade01db810b902c3d5dcf96","The support vector machines (SVMs) have gained visibility and been thoroughly studied in the machine learning community. However, the performance of these machines is sensitive to noisy data and the machine may not be effective when the level of noise is high. Since the noise makes the separating margin of SVM to be a stochastic variable, a probabilistic support vector machine (PSVM) is proposed to capture the probabilistic information of the separating margin and formulate the decision function within such a noisy environment. First, all data are clustered, upon which different subsets are formed by PCA-based sampling; then, a distributed SVM system is constructed to estimate the separating margin for each subset. Next, a quadratic optimization problem is being solved with the use of probabilistic information extracted from separating margins to determine the decision function. Using the weighted average of probability of cluster centers, the confidence of the decision can be estimated. An artificial dataset and four real-life datasets from a UCI machine learning database are used to demonstrate the effectiveness of the proposed probabilistic SVM. © 2012 Elsevier Inc. All rights reserved.","Classification; PCA based sampling; Probabilistic distribution; SVM","Artificial intelligence; Classification (of information); Learning systems; Optimization; Probability distributions; Quadratic programming; Separation; Stochastic systems; Decision functions; Machine learning communities; Machine-learning database; Probabilistic distribution; Probabilistic information; Quadratic optimization problems; Stochastic variable; Support vector machine (SVMs); Support vector machines",Article,Scopus,2-s2.0-84868709802
"Chen X., Kong Y., Fang X., Wu Q.","A fast two-stage ACO algorithm for robotic path planning",2013,"Neural Computing and Applications",26,10.1007/s00521-011-0682-7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872595752&doi=10.1007%2fs00521-011-0682-7&partnerID=40&md5=f48d8397476f96e73e54c718dc7464ae","Ant colony optimization (ACO) algorithms are often used in robotic path planning; however, the algorithms have two inherent problems. On one hand, the distance elicitation function and transfer function are usually used to improve the ACO algorithms, whereas, the two indexes often fail to balance between algorithm efficiency and optimization effect; On the other hand, the algorithms are heavily affected by environmental complexity. Based on the scent pervasion principle, a fast two-stage ACO algorithm is proposed in this paper, which overcomes the inherent problems of traditional ACO algorithms. The basic idea is to split the heuristic search into two stages: preprocess stage and path planning stage. In the preprocess stage, the scent information is broadcasted to the whole map and then ants do path planning under the direction of scent information. The algorithm is tested in maps of various complexities and compared with different algorithms. The results show the good performance and convergence speed of the proposed algorithm, even the high grid resolution does not affect the quality of the path found. © 2011 Springer-Verlag London Limited.","""Less-1"" search; Ant colony algorithm; Path planning; Scent broadcast","ACO algorithms; Algorithm efficiency; Ant colony algorithms; Ant Colony Optimization algorithms; Convergence speed; Environmental complexity; Grid resolution; Heuristic search; Less-1 search; Optimization effects; Planning stages; Preprocess; Scent broadcast; Artificial intelligence; Heuristic algorithms; Robotics; Motion planning",Article,Scopus,2-s2.0-84872595752
"Liao S., Gao Y., Oto A., Shen D.","Representation learning: a unified deep learning framework for automatic prostate MR segmentation.",2013,"Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention",26,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897576138&partnerID=40&md5=ead65d1c9660fe467161c84e6e009414","Image representation plays an important role in medical image analysis. The key to the success of different medical image analysis algorithms is heavily dependent on how we represent the input data, namely features used to characterize the input image. In the literature, feature engineering remains as an active research topic, and many novel hand-crafted features are designed such as Haar wavelet, histogram of oriented gradient, and local binary patterns. However, such features are not designed with the guidance of the underlying dataset at hand. To this end, we argue that the most effective features should be designed in a learning based manner, namely representation learning, which can be adapted to different patient datasets at hand. In this paper, we introduce a deep learning framework to achieve this goal. Specifically, a stacked independent subspace analysis (ISA) network is adopted to learn the most effective features in a hierarchical and unsupervised manner. The learnt features are adapted to the dataset at hand and encode high level semantic anatomical information. The proposed method is evaluated on the application of automatic prostate MR segmentation. Experimental results show that significant segmentation accuracy improvement can be achieved by the proposed deep learning method compared to other state-of-the-art segmentation approaches.",,"algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; human; image enhancement; male; methodology; nuclear magnetic resonance imaging; pathology; prostate tumor; reproducibility; sensitivity and specificity; Algorithms; Artificial Intelligence; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Male; Pattern Recognition, Automated; Prostatic Neoplasms; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84897576138
"Swartout W., Artstein R., Forbell E., Foutz S., Lane H.C., Lange B., Morie J., Noren D., Rizzo S., Traum D.","Virtual humans for learning",2013,"AI Magazine",26,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880670595&partnerID=40&md5=6eb5b1ea243d00ba0135755b89b6f71f","Virtual humans are computer-generated characters designed to look and behave like real people. Studies have shown that virtual humans can mimic many of the social effects that one finds in human-human interactions such as creating rapport, and people respond to virtual humans in ways that are similar to how they respond to real people. We believe that virtual humans represent a new metaphor for interacting with computers, one in which working with a computer becomes much like interacting with a person and this can bring social elements to the interaction that are not easily supported with conventional interfaces. We present two systems that embody these ideas. The first, the twins are virtual docents in the Museum of Science, Boston, designed to engage visitors and raise their awareness and knowledge of science. The second, Sim- Coach, uses an empathetic virtual human to provide veterans and their families with information about PTSD and depression. Copyright © 2013, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Artificial intelligence; Human-human interactions; Social elements; Virtual humans; Virtual reality",Article,Scopus,2-s2.0-84880670595
"Felita C., Suryanegara M.","5G key technologies: Identifying innovation opportunity",2013,"2013 International Conference on Quality in Research, QiR 2013 - In Conjunction with ICCS 2013: The 2nd International Conference on Civic Space",25,10.1109/QiR.2013.6632571,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890285996&doi=10.1109%2fQiR.2013.6632571&partnerID=40&md5=a1a992a8064c20a3d44b505cb09fd0c3","The 'not yet officially defined' 5G standard leads to the consequence that its relevant technological innovation is still widely open. For identifying innovations opportunity, we should discover the specific technical area to contribute to the technological development of 5G. This paper dicusses a framework answering the main question: In which technological area one may contribute to the innovation? The answer shall benefit countries, firms, universities and research institute which intends to contribute to the formulation of official 5G standard. First, we reviewed the key technologies of 5th generation mobile communication technology (5G). Ubiquitous and interoperability of the network are main technical focus. A flat IP-based network concept was reviewed, as well as cognitive radio technology to reach the terminal which have artificial intelligence. BDMA technology was proposed to help achieving system efficiency in terms of multiple access system. Second, we identified technological challenges, focusing on the issues related to security and problems to deal with limited frequency spectrum resources. Subsequently, we mapped the innovation opportunity based on technical area which is recently published in research article. We concluded that innovation opportunities lies on the research regarding security, network, technological implementation and applications issues. © 2013 IEEE.","5G; innovation; standard","5G; Cognitive radio technologies; Frequency spectrum resources; Mobile communication technology; Multiple access systems; Technological challenges; Technological development; Technological innovation; Artificial intelligence; Innovation; Research; Standards; Technology",Conference Paper,Scopus,2-s2.0-84890285996
"Li X., Guo Y.","Active learning with multi-label SVM classification",2013,"IJCAI International Joint Conference on Artificial Intelligence",25,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062998&partnerID=40&md5=9513e1ba9ab88d15c6e0cdaf99dcac7b","Multi-label classification, where each instance is assigned to multiple categories, is a prevalent problem in data analysis. However, annotations of multi-label instances are typically more timeconsuming or expensive to obtain than annotations of single-label instances. Though active learning has been widely studied on reducing labeling effort for single-label problems, current research on multi-label active learning remains in a preliminary state. In this paper, we first propose two novel multi-label active learning strategies, a max-margin prediction uncertainty strategy and a label cardinality inconsistency strategy, and then integrate them into an adaptive framework of multi-label active learning. Our empirical results on multiple multilabel data sets demonstrate the efficacy of the proposed active instance selection strategies and the integrated active learning approach.",,"Active Learning; Active learning strategies; Adaptive framework; Instance selection; Multi-label classifications; Multi-label instances; Prediction uncertainty; SVM classification; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84896062998
"Jaganathan P., Kuppuchamy R.","A threshold fuzzy entropy based feature selection for medical database classification",2013,"Computers in Biology and Medicine",25,10.1016/j.compbiomed.2013.10.016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887325021&doi=10.1016%2fj.compbiomed.2013.10.016&partnerID=40&md5=4a62d7a1eadcfb4027bb548fc3087748","Feature selection is one of the most common and critical tasks in database classification. It reduces the computational cost by removing insignificant features. Consequently, this makes the diagnosis process accurate and comprehensible. This paper presents the measurement of feature relevance based on fuzzy entropy, tested with a Radial Basis Function Network classifier for a medical database classification. Three feature selection strategies are devised to obtain the valuable subset of relevant features. Five benchmarked datasets, which are available in the UCI Machine Learning Repository, have been used in this work. The classification accuracy shows that the proposed method is capable of producing good results with fewer features than the original datasets. © 2013 Elsevier Ltd.","Classification; Feature selection; Fuzzy entropy; Medical database","Classification accuracy; Computational costs; Feature relevance; Fuzzy entropy; Medical database; Radial basis functions; Relevant features; UCI machine learning repository; Entropy; Feature extraction; Medical computing; Radial basis function networks; Classification (of information); article; breast cancer; classification algorithm; data base; data mining; diabetes mellitus; disease classification; entropy; feature selection; fuzzy entropy; fuzzy system; heart disease; hepatitis; human; machine learning; medical database; priority journal; radial based function; sensitivity and specificity; Classification; Feature selection; Fuzzy entropy; Medical database; Artificial Intelligence; Databases, Factual; Entropy; Fuzzy Logic; Humans",Article,Scopus,2-s2.0-84887325021
"Alur R., D'Antoni L., Gulwani S., Kini D., Viswanathan M.","Automated grading of DFA constructions",2013,"IJCAI International Joint Conference on Artificial Intelligence",25,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063349&partnerID=40&md5=f702bfbd442965222574ba4de5b6a826","One challenge in making online education more effective is to develop automatic grading software that can provide meaningful feedback. This paper provides a solution to automatic grading of the standard computation-theory problem that asks a student to construct a deterministic finite automaton (DFA) from the given description of its language. We focus on how to assign partial grades for incorrect answers. Each student's answer is compared to the correct DFA using a hybrid of three techniques devised to capture different classes of errors. First, in an attempt to catch syntactic mistakes, we compute the edit distance between the two DFA descriptions. Second, we consider the entropy of the symmetric difference of the languages of the two DFAs, and compute a score that estimates the fraction of the number of strings on which the student answer is wrong. Our third technique is aimed at capturing mistakes in reading of the problem description. For this purpose, we consider a description language MOSEL, which adds syntactic sugar to the classical Monadic Second Order Logic, and allows defining regular languages in a concise and natural way. We provide algorithms, along with optimizations, for transforming MOSEL descriptions into DFAs and vice-versa. These allow us to compute the syntactic edit distance of the incorrect answer from the correct one in terms of their logical representations. We report an experimental study that evaluates hundreds of answers submitted by (real) students by comparing grades/feedback computed by our tool with human graders. Our conclusion is that the tool is able to assign partial grades in a meaningful way, and should be preferred over the human graders for both scalability and consistency.",,"Automated grading; Description languages; Deterministic finite automata; Logical representations; Monadic second-order logic; On-line education; Problem description; Symmetric difference; Artificial intelligence; Computation theory; Distance education; Students; Syntactics; Tools; Grading",Conference Paper,Scopus,2-s2.0-84896063349
"Nguyen C.-T., Zhan D.-C., Zhou Z.-H.","Multi-modal image annotation with Multi-instance Multi-label LDA",2013,"IJCAI International Joint Conference on Artificial Intelligence",25,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063590&partnerID=40&md5=057497600e9dd9eb9f65a26f3e88bc8e","This paper studies the problem of image annotation in a multi-modal setting where both visual and textual information are available. We propose Multimodal Multi-instance Multi-label Latent Dirichlet Allocation (M3LDA), where the model consists of a visual-label part, a textual-label part and a labeltopic part. The basic idea is that the topic decided by the visual information and the topic decided by the textual information should be consistent, leading to the correct label assignment. Particularly, M3LDA is able to annotate image regions, thus provides a promising way to understand the relation between input patterns and output semantics. Experiments on Corel5K and ImageCLEF validate the effectiveness of the proposed method.",,"Image annotation; Image regions; Input patterns; Latent Dirichlet allocation; Multi-modal; Multi-modal image; Textual information; Visual information; Artificial intelligence; Semantics; Statistics; Image analysis",Conference Paper,Scopus,2-s2.0-84896063590
"Napoli C., Pappalardo G., Tramontana E.","A hybrid neuro-wavelet predictor for QoS control and stability",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",25,10.1007/978-3-319-03524-6_45,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892747401&doi=10.1007%2f978-3-319-03524-6_45&partnerID=40&md5=2702806fbd88a854ad0840e532d3ae92","For distributed systems to properly react to peaks of requests, their adaptation activities would benefit from the estimation of the amount of requests. This paper proposes a solution to produce a short-term forecast based on data characterising user behaviour of online services. We use wavelet analysis, providing compression and denoising on the observed time series of the amount of past user requests; and a recurrent neural network trained with observed data and designed so as to provide well-timed estimations of future requests. The said ensemble has the ability to predict the amount of future user requests with a root mean squared error below 0.06%. Thanks to prediction, advance resource provision can be performed for the duration of a request peak and for just the right amount of resources, hence avoiding over-provisioning and associated costs. Moreover, reliable provision lets users enjoy a level of availability of services unaffected by load variations. © Springer International Publishing Switzerland 2013.","Adaptive systems; Neural networks; QoS; Wavelet analysis","Associated costs; Distributed systems; Load variations; On-line service; Resource provisions; Root mean squared errors; Short-term forecasts; User behaviour; Adaptive systems; Artificial intelligence; Forecasting; Neural networks; Quality of service; Recurrent neural networks; Wavelet analysis; Behavioral research",Conference Paper,Scopus,2-s2.0-84892747401
"Akasiadis C., Chalkiadakis G.","Agent cooperatives for effective power consumption shifting",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",25,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893424436&partnerID=40&md5=5140a6ee2eb3285782d0f4eb9e047fb7","In this paper, we present a directly applicable scheme for electricity consumption shifting and effective demand curve flattening. The scheme can employ the services of either individual or cooperating consumer agents alike. Agents participating in the scheme, however, are motivated to form cooperatives, in order to reduce their electricity bills via lower group prices granted for sizable consumption shifting from high to low demand time intervals. The scheme takes into account individual costs, and uses a strictly proper scoring rule to reward contributors according to efficiency. Cooperative members, in particular, can attain variable reduced electricity price rates, given their different load shifting capabilities. This allows even agents with initially forbidding shifting costs to participate in the scheme, and is achieved by a weakly budget-balanced, truthful reward sharing mechanism. We provide four variants of this approach, and evaluate it experimentally. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Consumer agents; Effective demands; Electricity bill; Electricity prices; Electricity-consumption; Proper scoring rules; Sharing mechanism; Time interval; Artificial intelligence; Costs",Conference Paper,Scopus,2-s2.0-84893424436
"Rajagopal D., Cambria E., Olsher D., Kwok K.","A graph-based approach to commonsense concept extraction and semantic similarity detection",2013,"WWW 2013 Companion - Proceedings of the 22nd International Conference on World Wide Web",25,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890636931&partnerID=40&md5=72912eae0083e5997ddaac061ac667c3","Commonsense knowledge representation and reasoning support a wide variety of potential applications in fields such as document auto-categorization, Web search enhancement, topic gisting, social process modeling, and concept-level opinion and sentiment analysis. Solutions to these problems, however, demand robust knowledge bases capable of supporting exible, nuanced reasoning. Populating such knowledge bases is highly time-consuming, making it necessary to develop techniques for deconstructing natural language texts into commonsense concepts. In this work, we propose an approach for effective multi-word commonsense expression extraction from unrestricted English text, in addition to a semantic similarity detection technique allowing additional matches to be found for specific concepts not already present in knowledge bases.","Ai; Commonsense knowledge representation and reasoning; Natural language processing; Semantic similarity","Commonsense knowledge; Concept extraction; Knowledge basis; NAtural language processing; Natural language text; Semantic similarity; Sentiment analysis; Social process; Artificial intelligence; Knowledge representation; World Wide Web; Natural language processing systems",Conference Paper,Scopus,2-s2.0-84890636931
"Aung M.S.H., Thies S.B., Kenney L.P.J., Howard D., Selles R.W., Findlow A.H., Goulermas J.Y.","Automated detection of instantaneous gait events using time frequency analysis and manifold embedding",2013,"IEEE Transactions on Neural Systems and Rehabilitation Engineering",25,10.1109/TNSRE.2013.2239313,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888109321&doi=10.1109%2fTNSRE.2013.2239313&partnerID=40&md5=50a6da16d4ecada1cda52fe1ec30f15d","Accelerometry is a widely used sensing modality in human biomechanics due to its portability, non-invasiveness, and accuracy. However, difficulties lie in signal variability and interpretation in relation to biomechanical events. In walking, heel strike and toe off are primary gait events where robust and accurate detection is essential for gait-related applications. This paper describes a novel and generic event detection algorithm applicable to signals from tri-axial accelerometers placed on the foot, ankle, shank or waist. Data from healthy subjects undergoing multiple walking trials on flat and inclined, as well as smooth and tactile paving surfaces is acquired for experimentation. The benchmark timings at which heel strike and toe off occur, are determined using kinematic data recorded from a motion capture system. The algorithm extracts features from each of the acceleration signals using a continuous wavelet transform over a wide range of scales. A locality preserving embedding method is then applied to reduce the high dimensionality caused by the multiple scales while preserving salient features for classification. A simple Gaussian mixture model is then trained to classify each of the time samples into heel strike, toe off or no event categories. Results show good detection and temporal accuracies for different sensor locations and different walking terrains. © 2013 IEEE.","Accelerometry; classification; gait event detection; signal segmentation","Accelerometry; Continuous Wavelet Transform; Event detection; Event detection algorithm; Gaussian Mixture Model; Signal segmentation; Time frequency analysis; Triaxial accelerometer; Accelerometers; Biomechanics; Classification (of information); Walking aids; Signal detection; acceleration; algorithm; ambulatory monitoring; article; artificial intelligence; automated pattern recognition; equipment; equipment design; equipment failure; gait; human; methodology; microelectromechanical system; physiology; reproducibility; sensitivity and specificity; Acceleration; Algorithms; Artificial Intelligence; Equipment Design; Equipment Failure Analysis; Gait; Humans; Micro-Electrical-Mechanical Systems; Monitoring, Ambulatory; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84888109321
"Ciucci D., Dubois D.","A map of dependencies among three-valued logics",2013,"Information Sciences",25,10.1016/j.ins.2013.06.040,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883453197&doi=10.1016%2fj.ins.2013.06.040&partnerID=40&md5=70f56bf5d299f4f59f605d4c7a79aaab","Three-valued logics arise in several fields of computer science, both inspired by concrete problems (such as in the management of the null value in databases) and theoretical considerations. Several three-valued logics have been defined. They differ by their choice of basic connectives, hence also from a syntactic and proof-theoretic point of view. Different interpretations of the third truth value have also been suggested. They often carry an epistemic flavor. In this work, relationships between logical connectives on three-valued functions are explored. Existing theorems of functional completeness have laid bare some of these links, based on specific connectives. However we try to draw a map of such relationships between conjunctions, negations and implications that extend Boolean ones. It turns out that all reasonable connectives can be defined from a few of them and so all known three-valued logics appear as a fragment of only one logic. These results can be instrumental when choosing, for each application context, the appropriate fragment where the basic connectives make full sense, based on the appropriate meaning of the third truth-value. © 2013 Elsevier Inc. All rights reserved.","Functional completeness; MV-algebra; Three-valued logic; Truth-table","Application contexts; Functional completeness; Logical connectives; MV-algebras; Three-valued; Three-valued logic; Truth values; Truth-table; Artificial intelligence; Software engineering",Article,Scopus,2-s2.0-84883453197
"Bauer A., Küster J.-C., Vegliach G.","From propositional to first-order monitoring",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",25,10.1007/978-3-642-40787-1_4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887444663&doi=10.1007%2f978-3-642-40787-1_4&partnerID=40&md5=2f17dac52b11eff92b60240ca2b28819","The main purpose of this paper is to introduce a first-order temporal logic, LTLFO, and a corresponding monitor construction based on a new type of automaton, called spawning automaton. Specifically, we show that monitoring a specification in LTLFO boils down to an undecidable decision problem. The proof of this result revolves around specific ideas on what we consider a ""proper"" monitor. As these ideas are general, we outline them first in the setting of standard LTL, before lifting them to the setting of first-order logic and LTLFO. Although due to the above result one cannot hope to obtain a complete monitor for LTLFO, we prove the soundness of our automata-based construction and give experimental results from an implementation. These seem to substantiate our hypothesis that the automata-based construction leads to efficient runtime monitors whose size does not grow with increasing trace lengths (as is often observed in similar approaches). However, we also discuss formulae for which growth is unavoidable, irrespective of the chosen monitoring approach. © 2013 Springer-Verlag.",,"Decision problems; First order logic; First-order; First-order temporal logic; Monitoring approach; Runtime monitors; Trace length; Artificial intelligence; Computer science; Automata theory",Conference Paper,Scopus,2-s2.0-84887444663
"Casanova R., Hsu F.-C., Sink K.M., Rapp S.R., Williamson J.D., Resnick S.M., Espeland M.A.","Alzheimer's disease risk assessment using large-scale machine learning methods",2013,"PLoS ONE",25,10.1371/journal.pone.0077949,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892643220&doi=10.1371%2fjournal.pone.0077949&partnerID=40&md5=7ac12bedb0ad95b35dc5cb519e506d39","The goal of this work is to introduce new metrics to assess risk of Alzheimer's disease (AD) which we call AD Pattern Similarity (AD-PS) scores. These metrics are the conditional probabilities modeled by large-scale regularized logistic regression. The AD-PS scores derived from structural MRI and cognitive test data were tested across different situations using data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) study. The scores were computed across groups of participants stratified by cognitive status, age and functional status. Cox proportional hazards regression was used to evaluate associations with the distribution of conversion times from mild cognitive impairment to AD. The performances of classifiers developed using data from different types of brain tissue were systematically characterized across cognitive status groups. We also explored the performance of anatomical and cognitive-anatomical composite scores generated by combining the outputs of classifiers developed using different types of data. In addition, we provide the AD-PS scores performance relative to other metrics used in the field including the Spatial Pattern of Abnormalities for Recognition of Early AD (SPARE-AD) index and total hippocampal volume for the variables examined.",,"age; aged; Alzheimer disease; Alzheimer Disease Pattern Similarity score; article; brain size; classifier; cognition; controlled study; disease course; female; functional status; hippocampus; human; machine learning; major clinical study; male; mild cognitive impairment; neuropsychological test; nuclear magnetic resonance imaging; risk assessment; scoring system; algorithm; Alzheimer disease; artificial intelligence; neuroimaging; pathophysiology; procedures; radiography; statistical model; Aged; Algorithms; Alzheimer Disease; Artificial Intelligence; Female; Humans; Logistic Models; Magnetic Resonance Imaging; Male; Neuroimaging; Risk Assessment",Article,Scopus,2-s2.0-84892643220
"Wang S., Minku L.L., Yao X.","A learning framework for online class imbalance learning",2013,"Proceedings of the 2013 IEEE Symposium on Computational Intelligence and Ensemble Learning, CIEL 2013 - 2013 IEEE Symposium Series on Computational Intelligence, SSCI 2013",25,10.1109/CIEL.2013.6613138,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886779110&doi=10.1109%2fCIEL.2013.6613138&partnerID=40&md5=e5c7a9b94b4a3528c02a9f12c0bb387e","Online learning has been showing to be very useful for a large number of applications in which data arrive continuously and a timely response is required. In many online cases, the data stream can have very skewed class distributions, known as class imbalance, such as fault diagnosis of realtime control monitoring systems and intrusion detection in computer networks. Classifying imbalanced data streams poses new challenges, which have attracted very little attention so far. As the first work that formally addresses this problem, this paper looks into the underlying issues, clarifies the research questions, and proposes a framework for online class imbalance learning that decomposes the learning task into three modules. Within the framework, we use a time decay function to capture the imbalance rate dynamically. Then, we propose a class imbalance detection method, in order to decide the current imbalance status in data streams. According to this information, two resampling-based online learning algorithms are developed to tackle class imbalance in data streams. Three basic types of class imbalance change are discussed in our studies. The results suggest the usefulness of the learning framework. The proposed methods are shown to be effective on both minority-class accuracy and overall performance in all three cases we considered. © 2013 IEEE.",,"Current imbalances; Detection methods; Imbalanced data; Learning frameworks; Online learning; Online learning algorithms; Research questions; Skewed class distributions; Artificial intelligence; Data communication systems; Intrusion detection; E-learning",Conference Paper,Scopus,2-s2.0-84886779110
"Alsallakh B., Aigner W., Miksch S., Hauser H.","Radial sets: Interactive visual analysis of large overlapping sets",2013,"IEEE Transactions on Visualization and Computer Graphics",25,10.1109/TVCG.2013.184,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886712253&doi=10.1109%2fTVCG.2013.184&partnerID=40&md5=fab68ca5985fb99f18f3fa4bb63e308e","In many applications, data tables contain multi-valued attributes that often store the memberships of the table entities to multiple sets such as which languages a person masters, which skills an applicant documents, or which features a product comes with. With a growing number of entities, the resulting element-set membership matrix becomes very rich of information about how these sets overlap. Many analysis tasks targeted at set-typed data are concerned with these overlaps as salient features of such data. This paper presents Radial Sets, a novel visual technique to analyze set memberships for a large number of elements. Our technique uses frequency-based representations to enable quickly finding and analyzing different kinds of overlaps between the sets, and relating these overlaps to other attributes of the table entities. Furthermore, it enables various interactions to select elements of interest, find out if they are over-represented in specific sets or overlaps, and if they exhibit a different distribution for a specific attribute compared to the rest of the elements. These interactions allow formulating highly-expressive visual queries on the elements in terms of their set memberships and attribute values. As we demonstrate via two usage scenarios, Radial Sets enable revealing and analyzing a multitude of overlapping patterns between large sets, beyond the limits of state-of-the-art techniques. © 2013 IEEE.","Multi-valued attributes; overlapping sets; scalability; set-typed data; visualization technique","Different distributions; Interactive visual analysis; Multi-valued attribute; Overlapping patterns; overlapping sets; set-typed data; State-of-the-art techniques; Visualization technique; Computer graphics; Scalability; Software engineering; algorithm; artificial intelligence; computer graphics; computer interface; data base; factual database; image enhancement; information retrieval; multimodal imaging; procedures; reproducibility; sensitivity and specificity; article; image enhancement; information retrieval; methodology; multimodal imaging; Algorithms; Artificial Intelligence; Computer Graphics; Database Management Systems; Databases, Factual; Image Enhancement; Information Storage and Retrieval; Multimodal Imaging; Reproducibility of Results; Sensitivity and Specificity; User-Computer Interface; Algorithms; Artificial Intelligence; Computer Graphics; Database Management Systems; Databases, Factual; Image Enhancement; Information Storage and Retrieval; Multimodal Imaging; Reproducibility of Results; Sensitivity and Specificity; User-Computer Interface",Article,Scopus,2-s2.0-84886712253
"Wu H.-S., Zhang F.-M., Wu L.-S.","New swarm intelligence algorithm-wolf pack algorithm",2013,"Xi Tong Gong Cheng Yu Dian Zi Ji Shu/Systems Engineering and Electronics",25,10.3969/j.issn.1001-506X.2013.11.33,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891340866&doi=10.3969%2fj.issn.1001-506X.2013.11.33&partnerID=40&md5=76d35c906ea87bb74ccf19a0f0d29702","Based on swarm intelligence of a pach of wolves, inspiring by their hunting behaviors and distributive mode for prey, this paper abstracts three intelligent behaviors, scouting, summoning and beleaguering, and deduces a productive rule for leading wolf that the winner can dominate its all and a renewable mechaism, named survival of the stronger, for a pack of wolves. From the above, a new heuristic swarm intelligent method, wolf pack algorithm (WPA) is proposed. Moreover, this algorithm's global probability convergence is proved based on the theory of Markov chain. Then the proposed algorithm is applied to 15 typical complex function optimization problems, and compared with three classical intelligent algorithms, particle swarm optimization algorithm, artificial fish swarm algorithm and genetic algorithm. Simulation results show that WPA has better global convergence and computational robustness, and is especially suitable for solving high-dimension and multimodal function optimization problems.","Evolutionary computation; Function optimization; Swarm intelligence; Wolf pack algorithm (WPA)","Artificial fish swarm algorithms; Computational robustness; Function Optimization; Intelligent Algorithms; Multimodal function optimization; Particle swarm optimization algorithm; Swarm Intelligence; Wolf pack algorithm (WPA); Artificial intelligence; Evolutionary algorithms; Heuristic methods; Markov processes; Particle swarm optimization (PSO); Genetic algorithms",Article,Scopus,2-s2.0-84891340866
"Zhou C., Wang L., Zhang Q., Wei X.","Face recognition based on PCA image reconstruction and LDA",2013,"Optik",25,10.1016/j.ijleo.2013.04.108,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884592749&doi=10.1016%2fj.ijleo.2013.04.108&partnerID=40&md5=2ffc52d48c54f2d511f897263596b5ce","Face recognition has become a research hotspot in the field of pattern recognition and artificial intelligence. Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA) are two traditional methods in pattern recognition. In this paper, we propose a novel method based on PCA image reconstruction and LDA for face recognition. First, the inner-classes covariance matrix for feature extraction is used as generating matrix and then eigenvectors from each person is obtained, then we obtain the reconstructed images. Moreover, the residual images are computed by subtracting reconstructed images from original face images. Furthermore, the residual images are applied by LDA to obtain the coefficient matrices. Finally, the features are utilized to train and test SVMs for face recognition. The simulation experiments illustrate the effectivity of this method on the ORL face database. © 2013 Elsevier GmbH.","Face recognition; Image reconstruction; LDA; PCA; Residual images","Coefficient matrix; Face images; LDA; Linear discriminant analysis; ORL face database; PCA; Reconstructed image; Residual images; Artificial intelligence; Covariance matrix; Feature extraction; Image reconstruction; Principal component analysis; Face recognition",Article,Scopus,2-s2.0-84884592749
"Barto A.G.","Intrinsic motivation and reinforcement learning",2013,"Intrinsically Motivated Learning in Natural and Artificial Systems",25,10.1007/978-3-642-32375-1_2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929046579&doi=10.1007%2f978-3-642-32375-1_2&partnerID=40&md5=03d3398855e26333ced5d9b662f0cc20","Psychologists distinguish between extrinsically motivated behavior, which is behavior undertaken to achieve some externally supplied reward, such as a prize, a high grade, or a high-paying job, and intrinsically motivated behavior, which is behavior done for its own sake. Is an analogous distinction meaningful for machine learning systems? Can we say of a machine learning system that it is motivated to learn, and if so, is it possible to provide it with an analog of intrinsic motivation? Despite the fact that a formal distinction between extrinsic and intrinsic motivation is elusive, this chapter argues that the answer to both questions is assuredly yes and that the machine learning framework of reinforcement learning is particularly appropriate for bringing learning together with what in animals one would call motivation. Despite the common perception that a reinforcement learning agent's reward has to be extrinsic because the agent has a distinct input channel for reward signals, reinforcement learning provides a natural framework for incorporating principles of intrinsic motivation. © 2013 Springer-Verlag Berlin Heidelberg. All rights are reserved.",,"Artificial intelligence; Learning systems; Motivation; Reinforcement learning; High grades; Input channels; Intrinsic motivation; Reinforcement learning agent; Education",Book Chapter,Scopus,2-s2.0-84929046579
"Singal A.G., Mukherjee A., Joseph Elmunzer B., Higgins P.D.R., Lok A.S., Zhu J., Marrero J.A., Waljee A.K.","Machine learning algorithms outperform conventional regression models in predicting development of hepatocellular carcinoma",2013,"American Journal of Gastroenterology",25,10.1038/ajg.2013.332,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887246574&doi=10.1038%2fajg.2013.332&partnerID=40&md5=575c92967071f631e6bcdce85e8a0c46","OBJECTIVES:Predictive models for hepatocellular carcinoma (HCC) have been limited by modest accuracy and lack of validation. Machine-learning algorithms offer a novel methodology, which may improve HCC risk prognostication among patients with cirrhosis. Our study's aim was to develop and compare predictive models for HCC development among cirrhotic patients, using conventional regression analysis and machine-learning algorithms.METHODS:We enrolled 442 patients with Child A or B cirrhosis at the University of Michigan between January 2004 and September 2006 (UM cohort) and prospectively followed them until HCC development, liver transplantation, death, or study termination. Regression analysis and machine-learning algorithms were used to construct predictive models for HCC development, which were tested on an independent validation cohort from the Hepatitis C Antiviral Long-term Treatment against Cirrhosis (HALT-C) Trial. Both models were also compared with the previously published HALT-C model. Discrimination was assessed using receiver operating characteristic curve analysis, and diagnostic accuracy was assessed with net reclassification improvement and integrated discrimination improvement statistics.RESULTS:After a median follow-up of 3.5 years, 41 patients developed HCC. The UM regression model had a c-statistic of 0.61 (95% confidence interval (CI) 0.56-0.67), whereas the machine-learning algorithm had a c-statistic of 0.64 (95% CI 0.60-0.69) in the validation cohort. The HALT-C model had a c-statistic of 0.60 (95% CI 0.50-0.70) in the validation cohort and was outperformed by the machine-learning algorithm. The machine-learning algorithm had significantly better diagnostic accuracy as assessed by net reclassification improvement (P<0.001) and integrated discrimination improvement (P=0.04).CONCLUSIONS:Machine-learning algorithms improve the accuracy of risk stratifying patients with cirrhosis and can be used to accurately identify patients at high-risk for developing HCC.",,"antivirus agent; adult; aged; algorithm; article; cancer classification; cancer growth; cancer risk; cancer statistics; cohort analysis; controlled study; death; diagnostic accuracy; diagnostic test accuracy study; female; follow up; hepatitis C; human; human tissue; independent variable; liver cell carcinoma; liver cirrhosis; liver development; liver transplantation; long term care; machine learning; major clinical study; male; prediction; priority journal; prospective study; receiver operating characteristic; regression analysis; United States; university hospital; validation study; Algorithms; Artificial Intelligence; Carcinoma, Hepatocellular; Female; Hepatitis B, Chronic; Hepatitis C, Chronic; Humans; Liver Cirrhosis; Liver Neoplasms; Liver Transplantation; Male; Middle Aged; Models, Theoretical; Regression Analysis; Risk Factors",Article,Scopus,2-s2.0-84887246574
"Fergus P., Cheung P., Hussain A., Al-Jumeily D., Dobbins C., Iram S.","Prediction of Preterm Deliveries from EHG Signals Using Machine Learning",2013,"PLoS ONE",25,10.1371/journal.pone.0077154,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886615058&doi=10.1371%2fjournal.pone.0077154&partnerID=40&md5=5f13df602d8e41fb1696edfff1a4724b","There has been some improvement in the treatment of preterm infants, which has helped to increase their chance of survival. However, the rate of premature births is still globally increasing. As a result, this group of infants are most at risk of developing severe medical conditions that can affect the respiratory, gastrointestinal, immune, central nervous, auditory and visual systems. In extreme cases, this can also lead to long-term conditions, such as cerebral palsy, mental retardation, learning difficulties, including poor health and growth. In the US alone, the societal and economic cost of preterm births, in 2005, was estimated to be $26.2 billion, per annum. In the UK, this value was close to £2.95 billion, in 2009. Many believe that a better understanding of why preterm births occur, and a strategic focus on prevention, will help to improve the health of children and reduce healthcare costs. At present, most methods of preterm birth prediction are subjective. However, a strong body of evidence suggests the analysis of uterine electrical signals (Electrohysterography), could provide a viable way of diagnosing true labour and predict preterm deliveries. Most Electrohysterography studies focus on true labour detection during the final seven days, before labour. The challenge is to utilise Electrohysterography techniques to predict preterm delivery earlier in the pregnancy. This paper explores this idea further and presents a supervised machine learning approach that classifies term and preterm records, using an open source dataset containing 300 records (38 preterm and 262 term). The synthetic minority oversampling technique is used to oversample the minority preterm class, and cross validation techniques, are used to evaluate the dataset against other similar studies. Our approach shows an improvement on existing studies with 96% sensitivity, 90% specificity, and a 95% area under the curve value with 8% global error using the polynomial classifier. © 2013 Fergus et al.",,"article; controlled study; female; human; hysterography; machine learning; medical record; prediction; predictive value; premature labor; sensitivity and specificity; signal processing; systematic error; Area Under Curve; Artificial Intelligence; Databases, Factual; Electrophysiological Processes; Female; Health Care Costs; Humans; Infant, Newborn; Infant, Premature; Predictive Value of Tests; Pregnancy; Premature Birth; ROC Curve; Uterus",Article,Scopus,2-s2.0-84886615058
"Tian S., Sun H., Li Y., Pan P., Li D., Hou T.","Development and evaluation of an integrated virtual screening strategy by combining molecular docking and pharmacophore searching based on multiple protein structures",2013,"Journal of Chemical Information and Modeling",25,10.1021/ci400382r,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887042090&doi=10.1021%2fci400382r&partnerID=40&md5=701c4898afb320bb09eaea08fda207b8","In this study, we developed and evaluated a novel parallel virtual screening strategy by integrating molecular docking and complex-based pharmacophore searching based on multiple protein structures. First, the capacity of molecular docking or pharmacophore searching based on any single structure from nine crystallographic structures of Rho kinase 1 (ROCK1) to distinguish the known ROCK1 inhibitors from noninhibitors was evaluated systematically. Then, the naÏve Bayesian classification or recursive partitioning technique was employed to integrate the predictions from molecular docking and complex-based pharmacophore searching based on multiple crystallographic structures of ROCK1, and the integrated protocol yields much better performance than molecular docking or complex-based pharmacophore searching based on any single ROCK1 structure. Finally, the well-validated integrated virtual screening protocol was applied to identify potential inhibitors of ROCK1 from traditional chinese medicine (TCM). The obtained potential active compounds from TCM are structurally novel and diverse compared with the known inhibitors of ROCK1, and they may afford valuable clues for the development of potent ROCK1 inhibitors. © 2013 American Chemical Society.",,"Bayesian classification; Better performance; Crystallographic structure; Potential inhibitors; Protein structures; Recursive Partitioning; Traditional Chinese Medicine; Virtual screening strategies; Pharmacodynamics; Proteins; Regression analysis; Molecular modeling; herbaceous agent; ligand; protein kinase inhibitor; Rho kinase; ROCK1 protein, human; article; artificial intelligence; Bayes theorem; binding site; chemistry; computer interface; drug antagonism; drug development; high throughput screening; human; molecular docking; protein binding; protein conformation; structure activity relation; X ray crystallography; Artificial Intelligence; Bayes Theorem; Binding Sites; Crystallography, X-Ray; Drug Discovery; Drugs, Chinese Herbal; High-Throughput Screening Assays; Humans; Ligands; Molecular Docking Simulation; Protein Binding; Protein Conformation; Protein Kinase Inhibitors; rho-Associated Kinases; Structure-Activity Relationship; User-Computer Interface",Article,Scopus,2-s2.0-84887042090
"Bose R.P.J.C., Mans R.S., Van Der Aalst W.M.P.","Wanna improve process mining results?",2013,"Proceedings of the 2013 IEEE Symposium on Computational Intelligence and Data Mining, CIDM 2013 - 2013 IEEE Symposium Series on Computational Intelligence, SSCI 2013",25,10.1109/CIDM.2013.6597227,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883597918&doi=10.1109%2fCIDM.2013.6597227&partnerID=40&md5=7dd856e0ff5a3bc6bd6bc1ee77bb94c2","The growing interest in process mining is fueled by the increasing availability of event data. Process mining techniques use event logs to automatically discover process models, check conformance, identify bottlenecks and deviations, suggest improvements, and predict processing times. Lion's share of process mining research has been devoted to analysis techniques. However, the proper handling of problems and challenges arising in analyzing event logs used as input is critical for the success of any process mining effort. In this paper, we identify four categories of process characteristics issues that may manifest in an event log (e.g. process problems related to event granularity and case heterogeneity) and 27 classes of event log quality issues (e.g., problems related to timestamps in event logs, imprecise activity names, and missing events). The systematic identification and analysis of these issues calls for a consolidated effort from the process mining community. Five real-life event logs are analyzed to illustrate the omnipresence of process and event log issues. We hope that these findings will encourage systematic logging approaches (to prevent event log issues), repair techniques (to alleviate event log issues) and analysis techniques (to deal with the manifestation of process characteristics in event logs). © 2013 IEEE.","Data Cleansing; Data Quality; Event Log; Outliers; Preprocessing; Process Mining","Data cleansing; Data quality; Event Log; Outliers; Preprocessing; Process mining; Artificial intelligence; Repair; Well logging; Data mining",Conference Paper,Scopus,2-s2.0-84883597918
"Liu Y., Gall J., Stoll C., Dai Q., Seidel H.-P., Theobalt C.","Markerless motion capture of multiple characters using multiview image segmentation",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",25,10.1109/TPAMI.2013.47,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884557454&doi=10.1109%2fTPAMI.2013.47&partnerID=40&md5=8d3865c0e2f22ec0db417a3e9919b271","Capturing the skeleton motion and detailed time-varying surface geometry of multiple, closely interacting peoples is a very challenging task, even in a multicamera setup, due to frequent occlusions and ambiguities in feature-to-person assignments. To address this task, we propose a framework that exploits multiview image segmentation. To this end, a probabilistic shape and appearance model is employed to segment the input images and to assign each pixel uniquely to one person. Given the articulated template models of each person and the labeled pixels, a combined optimization scheme, which splits the skeleton pose optimization problem into a local one and a lower dimensional global one, is applied one by one to each individual, followed with surface estimation to capture detailed nonrigid deformations. We show on various sequences that our approach can capture the 3D motion of humans accurately even if they move rapidly, if they wear wide apparel, and if they are engaged in challenging multiperson motions, including dancing, wrestling, and hugging. © 1979-2012 IEEE.","image segmentation; Markerless motion capture; multiple characters; multiview video","Markerless motion capture; multiple characters; Multiview video; Non-rigid deformation; Optimization problems; Optimization scheme; Shape and appearance model; Surface estimation; Musculoskeletal system; Pixels; Image segmentation; algorithm; artificial intelligence; automated pattern recognition; computer assisted diagnosis; human; movement (physiology); physiology; procedures; three dimensional imaging; whole body imaging; article; automated pattern recognition; computer assisted diagnosis; methodology; movement (physiology); physiology; three dimensional imaging; whole body imaging; Algorithms; Artificial Intelligence; Humans; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Movement; Pattern Recognition, Automated; Whole Body Imaging; Algorithms; Artificial Intelligence; Humans; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Movement; Pattern Recognition, Automated; Whole Body Imaging",Article,Scopus,2-s2.0-84884557454
"Nikolić M., Teodorović D.","Empirical study of the Bee Colony Optimization (BCO) algorithm",2013,"Expert Systems with Applications",25,10.1016/j.eswa.2013.01.063,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876044342&doi=10.1016%2fj.eswa.2013.01.063&partnerID=40&md5=79bf495d82454055295fb88292062042","The Bee Colony Optimization (BCO) meta-heuristic deals with combinatorial optimization problems. It is biologically inspired method that explores collective intelligence applied by the honey bees during nectar collecting process. In this paper we perform empirical study of the BCO algorithm. We apply BCO to optimize numerous numerical test functions. The obtained results are compared with the results in the literature. The numerical experiments performed on well-known benchmark functions show that the BCO is competitive with other methods and it can generate high-quality solutions within negligible CPU times. © 2013 Elsevier Ltd. All rights reserved.","Bee Colony Optimization (BCO); Swarm intelligence","Bee colony optimizations; Benchmark functions; Biologically inspired methods; Collective intelligences; Combinatorial optimization problems; High-quality solutions; Numerical experiments; Swarm Intelligence; Algorithms; Artificial intelligence; Combinatorial optimization; Numerical methods; Optimization",Article,Scopus,2-s2.0-84876044342
"Grant J., Hunter A.","Distance-based measures of inconsistency",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",25,10.1007/978-3-642-39091-3-20,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880725975&doi=10.1007%2f978-3-642-39091-3-20&partnerID=40&md5=4c9d2a2d53f4c1602d4b4c2535d17e12","There have been a number of proposals for measuring inconsistency in a knowledgebase (i.e. a set of logical formulae). These include measures that consider the minimally inconsistent subsets of the knowledgebase, and measures that consider the paraconsistent models (3 or 4 valued models) of the knowledgebase. In this paper, we present a new approach that considers the amount each formula has to be weakened in order for the knowledgebase to be consistent. This approach is based on ideas of knowledge merging by Konienczny and Pino-Perez. We show that this approach gives us measures that are different from existing measures, that have desirable properties, and that can take the significance of inconsistencies into account. The latter is useful when we want to differentiate between inconsistencies that have minor significance from inconsistencies that have major significance. We also show how our measures are potentially useful in applications such as evaluating violations of integrity constraints in databases. © 2013 Springer-Verlag Berlin Heidelberg.",,"Distance-based; Integrity constraints; Knowledge base; Logical formulas; Measuring inconsistency; New approaches; Significance of inconsistency; Artificial intelligence; Computer science",Conference Paper,Scopus,2-s2.0-84880725975
"Al-Abbasi A., Al-Jasmi A., Goel H.K., Nasr H., Cullick A.S., Rodriguez J.A., Carvajal G.A., Velasquez G., Scott M., Bravo C.","New generation of petroleum workflow automation: Philosophy and practice",2013,"Society of Petroleum Engineers - SPE Digital Energy Conference and Exhibition 2013",25,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880461238&partnerID=40&md5=63e9073c08f7272ca8a394cb7174e6e6","Intelligent digital oilfield (iDOF) operations have gained momentum in the past few years, transformed from being merely a vision to real-world projects with quantifiable value. Challenges such as increased energy demand and diminishing new discoveries, coupled with a lack of specialist-domain expertise and trained personnel to efficiently operate assets, have forced operators to rethink the traditional way of asset management to increase productivity and operational efficiency. The amount of information that asset managers now have to make decisions has increased dramatically in the last few years. More data about a problem can lead to improved decisions, but it also increases the complexity of the decision-making process. Asset teams need tools and technologies to help them quickly and efficiently analyze and understand all this data so they make better, faster decisions. To help asset teams meet these challenges, a new generation of petroleum workflow automation integrates real-time data with asset models, helping team members to collaborate so they can better analyze data and more fully understand asset problems. We're calling this new generation of automated, intelligent workflows ""smart flows."" This approach is cutting-edge, but also more complex. The complexity is addressed with the use of artificial intelligence technology, such as proxy models and neural networks, coupled with a visualization engine to provide an effective visual data mining tool. The objective of this new generation of petroleum workflow automation is to provide integrated solutions to asset opportunities and guide the operations with instructions based on smart analysis and integrated visualization. This paper provides an overview of a workflow automation environment that is being implemented for a major operator. Copyright 2013, Society of Petroleum Engineers.",,"Amount of information; Artificial intelligence technologies; Decision making process; Integrated solutions; Operational efficiencies; Real world projects; Tools and technologies; Visualization engine; Artificial intelligence; Automation; Exhibitions; Human resource management; Oil fields; Tools; Petroleum analysis",Conference Paper,Scopus,2-s2.0-84880461238
"Guo J., Mei X., Tang K.","Automatic landmark annotation and dense correspondence registration for 3D human facial images",2013,"BMC Bioinformatics",25,10.1186/1471-2105-14-232,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880334955&doi=10.1186%2f1471-2105-14-232&partnerID=40&md5=3db7cf602c5df74b7d0cdc970a9aaebf","Background: Traditional anthropometric studies of human face rely on manual measurements of simple features, which are labor intensive and lack of full comprehensive inference. Dense surface registration of three-dimensional (3D) human facial images holds great potential for high throughput quantitative analyses of complex facial traits. However there is a lack of automatic high density registration method for 3D faical images. Furthermore, current approaches of landmark recognition require further improvement in accuracy to support anthropometric applications.Result: Here we describe a novel non-rigid registration method for fully automatic 3D facial image mapping. This method comprises two steps: first, seventeen facial landmarks are automatically annotated, mainly via PCA-based feature recognition following 3D-to-2D data transformation. Second, an efficient thin-plate spline (TPS) protocol is used to establish the dense anatomical correspondence between facial images, under the guidance of the predefined landmarks. We demonstrate that this method is highly accurate in landmark recognition, with an average RMS error of ~1.7 mm. The registration process is highly robust, even for different ethnicities.Conclusion: This method supports fully automatic registration of dense 3D facial images, with 17 landmarks annotated at greatly improved accuracy. A stand-alone software has been implemented to assist high-throughput high-content anthropometric analysis. © 2013 Guo et al.; licensee BioMed Central Ltd.","3D face; Dense correspondence; Facial morphology; Landmark localization; Registration","3D faces; Automatic registration; Dense correspondences; Landmark localization; Nonrigid registration method; Registration; Registration methods; Stand-alone software; Anthropometry; Face recognition; Three dimensional; article; artificial intelligence; biology; computer program; face; histology; human; methodology; three dimensional imaging; anatomy and histology; procedures; Artificial Intelligence; Computational Biology; Face; Humans; Imaging, Three-Dimensional; Software; Artificial Intelligence; Computational Biology; Face; Humans; Imaging, Three-Dimensional; Software",Article,Scopus,2-s2.0-84880334955
"Krouk G., Lingeman J., Colon A.M., Coruzzi G., Shasha D.","Gene regulatory networks in plants: Learning causality from time and perturbation",2013,"Genome Biology",25,10.1186/gb-2013-14-6-123,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883795281&doi=10.1186%2fgb-2013-14-6-123&partnerID=40&md5=3ac4e70bab93c5db4acb19d90bdb60e9","The goal of systems biology is to generate models for predicting how a system will react under untested conditions or in response to genetic perturbations. This paper discusses experimental and analytical approaches to deriving causal relationships in gene regulatory networks. © 2013 BioMed Central Ltd.","Gene regulatory networks; Network interference; Plant; Systems biology","gene regulatory network; learning; systems biology; artificial intelligence; biological model; gene expression regulation; genetics; plant; plant gene; plant genome; procedures; signal transduction; statistics and numerical data; systems biology; Artificial Intelligence; Gene Expression Regulation, Plant; Gene Regulatory Networks; Genes, Plant; Genome, Plant; Models, Genetic; Plants; Signal Transduction; Systems Biology",Article,Scopus,2-s2.0-84883795281
"Langsdale S., Beall A., Bourget E., Hagen E., Kudlas S., Palmer R., Tate D., Werick W.","Collaborative modeling for decision support in water resources: Principles and best practices",2013,"Journal of the American Water Resources Association",25,10.1111/jawr.12065,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878953007&doi=10.1111%2fjawr.12065&partnerID=40&md5=a21a041e5f3d4531d68d658b88f73abe","Collaborative Modeling for Decision Support integrates collaborative modeling with participatory processes to inform natural resources decisions. Practitioners and advocates claim that the approach will lead to better water management, balancing interests more effectively and reducing the likelihood of costly legal delays. These claims are easy to make, but the benefits will only be realized if the process is conducted effectively. To provide guidance for how to conduct an effective collaborative modeling process, a task committee cosponsored by the Environmental Water Resources Institute (EWRI) of the American Society of Civil Engineers and by the U.S. Army Corps of Engineers' Institute for Water Resources developed a set of Principles and Best Practices for anyone who might convene or conduct collaborative modeling processes. The guidance is intended for both conflict resolution professionals and modelers, and our goal is to integrate these two fields in a way that will improve water resources planning and decision making. Here, the set of eight principles is presented along with a selection of associated best practices, illustrated by two different case examples. The complete document is available at: http://www.computeraideddisputeresolution.us/bestpractices/. © 2013 American Water Resources Association.","Collaborative modeling; Conflict resolution; Decision support systems; Mediated Modeling; Participatory modeling; Planning; Public participation; Shared Vision Planning","Collaborative modeling; Conflict Resolution; Participatory modeling; Public participation; Shared vision; Artificial intelligence; Planning; Water management; Decision support systems; best management practice; conflict management; decision support system; participatory approach; water planning; water resource",Article,Scopus,2-s2.0-84878953007
"Hegenbart S., Uhl A., Vécsei A., Wimmer G.","Scale invariant texture descriptors for classifying celiac disease",2013,"Medical Image Analysis",25,10.1016/j.media.2013.02.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875853321&doi=10.1016%2fj.media.2013.02.001&partnerID=40&md5=93706eb4534f3393b4dbc71318a96b39","Scale invariant texture recognition methods are applied for the computer assisted diagnosis of celiac disease. In particular, emphasis is given to techniques enhancing the scale invariance of multi-scale and multi-orientation wavelet transforms and methods based on fractal analysis. After fine-tuning to specific properties of our celiac disease imagery database, which consists of endoscopic images of the duodenum, some scale invariant (and often even viewpoint invariant) methods provide classification results improving the current state of the art. However, not each of the investigated scale invariant methods is applicable successfully to our dataset. Therefore, the scale invariance of the employed approaches is explicitly assessed and it is found that many of the analyzed methods are not as scale invariant as they theoretically should be. Results imply that scale invariance is not a key-feature required for successful classification of our celiac disease dataset. © 2013 Elsevier B.V.","Celiac disease; Scale invariance; Texture recognition","Celiac disease; Classification results; Computer assisted diagnosis; Scale invariance; Specific properties; Texture descriptors; Texture recognition; Viewpoint invariant; Textures; Classification (of information); algorithm; analytic method; article; artificial neural network; celiac disease; computer assisted diagnosis; data base; discrete wavelet transform; dual tree complex wavelet transform; duodenum; Fourier transformation; fractal analysis; Gabor wavelet transform; gastrointestinal endoscopy; human; illumination; image analysis; insulin dependent diabetes mellitus; intestine biopsy; mathematical phenomena; priority journal; pulse coupled neural network; scale invariant feature transform; scale invariant texture recognition method; wavelet analysis; Algorithms; Artificial Intelligence; Celiac Disease; Duodenum; Endoscopy, Gastrointestinal; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84875853321
"Štěpnička M., Cortez P., Donate J.P., Štěpničková L.","Forecasting seasonal time series with computational intelligence: On recent methods and the potential of their combinations",2013,"Expert Systems with Applications",25,10.1016/j.eswa.2012.10.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872836165&doi=10.1016%2fj.eswa.2012.10.001&partnerID=40&md5=5557abbb65ebf2c78ae577fb2435e1b5","Accurate time series forecasting is a key issue to support individual and organizational decision making. In this paper, we introduce novel methods for multi-step seasonal time series forecasting. All the presented methods stem from computational intelligence techniques: evolutionary artificial neural networks, support vector machines and genuine linguistic fuzzy rules. Performance of the suggested methods is experimentally justified on seasonal time series from distinct domains on three forecasting horizons. The most important contribution is the introduction of a new hybrid combination using linguistic fuzzy rules and the other computational intelligence methods. This hybrid combination presents competitive forecasts, when compared with the popular ARIMA method. Moreover, such hybrid model is more easy to interpret by decision-makers when modeling trended series. © 2012 Elsevier Ltd. All rights reserved.","Computational intelligence; Fuzzy rules; Genetic algorithm; Neural networks; Support vector machine; Time series","Computational intelligence methods; Computational intelligence techniques; Decision makers; Evolutionary artificial neural networks; Hybrid model; Linguistic fuzzy rules; Multi-step; Novel methods; Organizational decision making; Seasonal time series; Time series forecasting; Artificial intelligence; Fuzzy rules; Genetic algorithms; Linguistics; Neural networks; Support vector machines; Time series; Forecasting",Article,Scopus,2-s2.0-84872836165
"Jaafar H.I., Mohamed Z., Abidin A.F.Z., Ghani Z.A.","PSO-tuned PID controller for a nonlinear gantry crane system",2013,"Proceedings - 2012 IEEE International Conference on Control System, Computing and Engineering, ICCSCE 2012",25,10.1109/ICCSCE.2012.6487200,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875998128&doi=10.1109%2fICCSCE.2012.6487200&partnerID=40&md5=f3e3355b7f39e866e7cc45afeeda06f5","This paper presents development of an optimal PID controller for control of a nonlinear gantry crane system. An improved PSO algorithm based on a priority-based fitness approach is implemented for finding optimal PID parameters. The system dynamic model is derived using Lagrange equation. A combination of PID and PD controllers are utilized for position and oscillation control of the system. System responses including trolley displacement and payload oscillation are observed and analyzed. Simulation is conducted within Matlab environment to verify the performance of the controller. It is demonstrated that the controller is effective to move the trolley as fast as possible to the desired position with low payload oscillation technique. © 2012 IEEE.","Computational Intelligence; Gantry crane; Particle Swarm Optimization; PID; Swarm Intelligence","Gantry crane systems; Improved pso algorithms; MATLAB environment; Oscillation control; Payload oscillation; PID; Swarm Intelligence; System dynamic models; Artificial intelligence; Control systems; Electric control equipment; Equations of motion; Gantry cranes; MATLAB; Particle swarm optimization (PSO); Controllers",Conference Paper,Scopus,2-s2.0-84875998128
"Lam P.V.N., Goldman R., Karagiannis K., Narsule T., Simonyan V., Soika V., Mazumder R.","Structure-based Comparative Analysis and Prediction of N-linked Glycosylation Sites in Evolutionarily Distant Eukaryotes",2013,"Genomics, Proteomics and Bioinformatics",25,10.1016/j.gpb.2012.11.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876796188&doi=10.1016%2fj.gpb.2012.11.003&partnerID=40&md5=bcd16c7da4f4cd48104f236cdd5b58e4","The asparagine-X-serine/threonine (NXS/T) motif, where X is any amino acid except proline, is the consensus motif for N-linked glycosylation. Significant numbers of high-resolution crystal structures of glycosylated proteins allow us to carry out structural analysis of the N-linked glycosylation sites (NGS). Our analysis shows that there is enough structural information from diverse glycoproteins to allow the development of rules which can be used to predict NGS. A Python-based tool was developed to investigate asparagines implicated in N-glycosylation in five species: Homo sapiens, Mus musculus, Drosophila melanogaster, Arabidopsis thaliana and Saccharomyces cerevisiae. Our analysis shows that 78% of all asparagines of NXS/T motif involved in N-glycosylation are localized in the loop/turn conformation in the human proteome. Similar distribution was revealed for all the other species examined. Comparative analysis of the occurrence of NXS/T motifs not known to be glycosylated and their reverse sequence (S/TXN) shows a similar distribution across the secondary structural elements, indicating that the NXS/T motif in itself is not biologically relevant. Based on our analysis, we have defined rules to determine NGS. Using machine learning methods based on these rules we can predict with 93% accuracy if a particular site will be glycosylated. If structural information is not available the tool uses structural prediction results resulting in 74% accuracy. The tool was used to identify glycosylation sites in 108 human proteins with structures and 2247 proteins without structures that have acquired NXS/T site/s due to non-synonymous variation. The tool, Structure Feature Analysis Tool (SFAT), is freely available to the public at http://hive.biochemistry.gwu.edu/tools/sfat. © 2013 .","Gain and loss of glycosylation; N-linked glycosylation; NsSNP; NsSNV; Variation","amino acid analysis; Arabidopsis; article; comparative study; eukaryote; glycosylation; human; mouse; n linked glycosylation site; nonhuman; prediction; protein analysis; protein structure; proteomics; Saccharomyces cerevisiae; Amino Acid Motifs; Amino Acids; Animals; Arabidopsis; Arabidopsis Proteins; Artificial Intelligence; Asparagine; Biological Evolution; Databases, Protein; Drosophila melanogaster; Drosophila Proteins; Eukaryota; Glycoproteins; Glycosylation; Humans; Mice; Polymorphism, Single Nucleotide; Protein Processing, Post-Translational; Proteome; Saccharomyces cerevisiae; Saccharomyces cerevisiae Proteins; Software; Arabidopsis thaliana; Drosophila melanogaster; Eukaryota; Homo sapiens; Mus musculus; Saccharomyces cerevisiae",Article,Scopus,2-s2.0-84876796188
"Noronha K., Acharya U.R., Nayak K.P., Kamath S., Bhandary S.V.","Decision support system for diabetic retinopathy using discrete wavelet transform",2013,"Proceedings of the Institution of Mechanical Engineers, Part H: Journal of Engineering in Medicine",25,10.1177/0954411912470240,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877817999&doi=10.1177%2f0954411912470240&partnerID=40&md5=f85f6920c14945d70770cf74f95e7ac9","Prolonged duration of the diabetes may affect the tiny blood vessels of the retina causing diabetic retinopathy. Routine eye screening of patients with diabetes helps to detect diabetic retinopathy at the early stage. It is very laborious and time-consuming for the doctors to go through many fundus images continuously. Therefore, decision support system for diabetic retinopathy detection can reduce the burden of the ophthalmologists. In this work, we have used discrete wavelet transform and support vector machine classifier for automated detection of normal and diabetic retinopathy classes. The wavelet-based decomposition was performed up to the second level, and eight energy features were extracted. Two energy features from the approximation coefficients of two levels and six energy values from the details in three orientations (horizontal, vertical and diagonal) were evaluated. These features were fed to the support vector machine classifier with various kernel functions (linear, radial basis function, polynomial of orders 2 and 3) to evaluate the highest classification accuracy. We obtained the highest average classification accuracy, sensitivity and specificity of more than 99% with support vector machine classifier (polynomial kernel of order 3) using three discrete wavelet transform features. We have also proposed an integrated index called Diabetic Retinopathy Risk Index using clinically significant wavelet energy features to identify normal and diabetic retinopathy classes using just one number. We believe that this (Diabetic Retinopathy Risk Index) can be used as an adjunct tool by the doctors during the eye screening to cross-check their diagnosis. © IMechE 2012.","Classifier; Diabetic retinopathy; Discrete wavelet transform; Eye; Support vector machine","Approximation coefficients; Classification accuracy; Diabetic retinopathy; Eye; Radial basis functions; Sensitivity and specificity; Support vector machine classifiers; Wavelet energy feature; Approximation coefficients; Classification accuracy; Diabetic retinopathy; Eye; Radial basis functions; Sensitivity and specificity; Support vector machine classifiers; Wavelet energy feature; Artificial intelligence; Blood vessels; Classifiers; Decision support systems; Discrete wavelet transforms; Radial basis function networks; Support vector machines; Wavelet decomposition; Artificial intelligence; Blood vessels; Classifiers; Decision support systems; Diagnosis; Discrete wavelet transforms; Radial basis function networks; Support vector machines; Vectors; Wavelet analysis; Wavelet decomposition; Wavelet transforms; Eye protection; Eye protection; adult; article; computer assisted diagnosis; diabetic retinopathy; eye fundus; human; methodology; middle aged; pathology; support vector machine; visual system examination; wavelet analysis; Adult; Diabetic Retinopathy; Diagnostic Techniques, Ophthalmological; Fundus Oculi; Humans; Image Interpretation, Computer-Assisted; Middle Aged; Support Vector Machines; Wavelet Analysis",Article,Scopus,2-s2.0-84877817999
"Trost B., Kusalik A.","Computational phosphorylation site prediction in plants using random forests and organism-specific instance weights",2013,"Bioinformatics",25,10.1093/bioinformatics/btt031,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875173596&doi=10.1093%2fbioinformatics%2fbtt031&partnerID=40&md5=5957eef569fab8f3ea6b659b6310494c","Motivation: Phosphorylation is the most important post-translational modification in eukaryotes. Although many computational phosphorylation site prediction tools exist for mammals, and a few were created specifically for Arabidopsis thaliana, none are currently available for other plants.Results: In this article, we propose a novel random forest-based method called PHOSFER (PHOsphorylation Site FindER) for applying phosphorylation data from other organisms to enhance the accuracy of predictions in a target organism. As a test case, PHOSFER is applied to phosphorylation sites in soybean, and we show that it more accurately predicts soybean sites than both the existing Arabidopsis-specific predictors, and a simpler machine-learning scheme that uses only known phosphorylation sites and non-phosphorylation sites from soybean. In addition to soybean, PHOSFER will be extended to other organisms in the near future. © The Author 2013.",,"vegetable protein; animal; article; artificial intelligence; biology; cattle; chemistry; computer program; human; metabolism; methodology; mouse; phosphorylation; protein processing; sequence alignment; sequence analysis; soybean; Animals; Artificial Intelligence; Cattle; Computational Biology; Humans; Mice; Phosphorylation; Plant Proteins; Protein Processing, Post-Translational; Sequence Alignment; Sequence Analysis, Protein; Software; Soybeans",Article,Scopus,2-s2.0-84875173596
"Yen S.-J., Wu Y.-C., Yang J.-C., Lee Y.-S., Lee C.-J., Liu J.-J.","A support vector machine-based context-ranking model for question answering",2013,"Information Sciences",25,10.1016/j.ins.2012.10.014,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871008442&doi=10.1016%2fj.ins.2012.10.014&partnerID=40&md5=8da19c4f139705f2b8e0db0efc9c553c","Modern information technologies and Internet services are suffering from the problem of selecting and managing a growing amount of textual information, to which access is often critical. Machine learning techniques have recently shown excellent performance and flexibility in many applications, such as artificial intelligence and pattern recognition. Question answering (QA) is a method of locating exact answer sentences from vast document collections. This paper presents a machine learning-based question-answering framework, which integrates a question classifier, simple document/passage retrievers, and the proposed context-ranking models. The question classifier is trained to categorize the answer type of the given question and instructs the context-ranking model to re-rank the passages retrieved from the initial retrievers. This method provides flexible features to learners, such as word forms, syntactic features, and semantic word features. The proposed context-ranking model, which is based on the sequential labeling of tasks, combines rich features to predict whether the input passage is relevant to the question type. We employ TREC-QA tracks and question classification benchmarks to evaluate the proposed method. The experimental results show that the question classifier achieves 85.60% accuracy without any additional semantic or syntactic taggers, and reached 88.60% after we employed the proposed term expansion techniques and a predefined related-word set. In the TREC-10 QA task, by using the gold TREC-provided relevant document set, the QA model achieves a 0.563 mean reciprocal rank (MRR) score, and a 0.342 MRR score is achieved after using the simple document and passage retrieval algorithms. © 2012 Elsevier Inc. All rights reserved.","Information retrieval; Passage retrieval; Question answering; Question classification; Support vector machines","Document collection; Internet services; Machine learning techniques; Mean reciprocal ranks; Modern information technologies; Passage retrieval; Question Answering; Question classification; Question type; Relevant documents; Support vector; Syntactic features; Textual information; Artificial intelligence; Information retrieval; Information technology; Pattern recognition; Semantics; Support vector machines; Syntactics; Learning algorithms",Article,Scopus,2-s2.0-84871008442
"Serrano E., Botia J.","Validating ambient intelligence based ubiquitous computing systems by means of artificial societies",2013,"Information Sciences",25,10.1016/j.ins.2010.11.012,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870055316&doi=10.1016%2fj.ins.2010.11.012&partnerID=40&md5=70b8987319898b27d9be02024b9da154","This paper introduces a new methodology based on the use of Multi-Agent Based Simulations (MABS) for testing and validation of Ambient Intelligence based Ubiquitous Computing (UbiCom) systems. An ambient intelligence based UbiCom is a pervasive system in which services have some intelligence in order to smoothly interact with users immersed in the environment. The motivation for this methodology is its application in UbiCom large-scale systems where large numbers of users are involved and in applications which deal with dangerous environments. In these cases, real tests are impractical and an artificial society is required. MABS allows building cheap and quick prototypes which can describe UbiCom systems. Analyzing these prototypes, if they are sufficiently descriptive, allows requisites violations in functionality of real UbiCom system designs to be discovered. MABSs and particularly the most descriptive ones can present very complex behaviors. Therefore, the MABS analysis obtained with the presented methodology is not trivial. Consequently, this paper also proposes two techniques for the analysis of general complex MABSs: forensic analysis and the use of simpler simulations. Moreover, the methodology proposes to inject elements of the actual UbiCom system in the simulated world to increase the confidence of the validation process. The proposal is illustrated with a detailed case study that considers a building on our campus and an AmI service for evacuation in case of fire. © 2012 Elsevier Inc. All rights reserved.","Ambient intelligence; Forensic analysis; Social simulations; Ubiquitous computing; Validation; Verification","Ambient intelligence; Artificial societies; Complex behavior; Forensic analysis; Multi-agent based simulations; Pervasive systems; Social simulations; Ubiquitous computing systems; Validation; Validation process; Artificial intelligence; Intelligent buildings; Systems analysis; Verification; Ubiquitous computing",Article,Scopus,2-s2.0-84870055316
"Sulis A., Sechi G.M.","Comparison of generic simulation models for water resource systems",2013,"Environmental Modelling and Software",25,10.1016/j.envsoft.2012.09.012,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871800972&doi=10.1016%2fj.envsoft.2012.09.012&partnerID=40&md5=7b69ce24e324b9638896351e3ddf8739","In water resource systems that frequently experience severe droughts, generic simulation models can provide useful information for developing drought mitigation measures. This paper is about modeling in practice rather than in theory. The emphasis is on the application of generic simulation models to a multi-reservoir and multi-use water system in Southern Italy where frequent droughts over the last two decades have necessitated the use of temporary and unsustainable user-supply restrictions. In particular, AQUATOOL (Valencia Polytechnic University), MODSIM (Colorado State University), RIBASIM (DELTARES), WARGI-SIM (University of Cagliari) and WEAP (Stockholm Environmental Institute) models are considered in a preliminary analysis, which considers series and parallel simple schemes and also evaluates the possibility of alternative plans and operating policies in complex real water system. Each model has its own characteristics and uses different approaches to define resources releases from reservoirs and allocation to demand centers. The proposed model comparison and application does not identify in detail all the features of each model, rather it provides insights as to how these generic simulation models implement and evaluate different operating rules. © 2012 Elsevier Ltd.","Decision support systems; Optimization; Simulation; Water resources management","Colorado state universities; Demand centers; Drought mitigation; Generic simulation; Model comparison; Operating policies; Operating rule; Preliminary analysis; Simulation; Southern Italy; Stockholm; Valencia; Water resource systems; Water resources management; Water system; Artificial intelligence; Computer simulation; Decision support systems; Drought; Optimization; Waterworks; Water resources; comparative study; numerical model; optimization; policy making; water management; water resource; Italy",Article,Scopus,2-s2.0-84871800972
"Santos I., Devesa J., Brezo F., Nieves J., Bringas P.G.","OPEM: A static-dynamic approach for machine-learning-based malware detection",2013,"Advances in Intelligent Systems and Computing",25,10.1007/978-3-642-33018-6_28,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868092598&doi=10.1007%2f978-3-642-33018-6_28&partnerID=40&md5=2fe04d1a12df59176d31cb012bdd61e7","Malware is any computer software potentially harmful to both computers and networks. The amount of malware is growing every year and poses a serious global security threat. Signature-based detection is the most extended method in commercial antivirus software, however, it consistently fails to detect new malware. Supervised machine learning has been adopted to solve this issue. There are two types of features that supervised malware detectors use: (i) static features and (ii) dynamic features. Static features are extracted without executing the sample whereas dynamic ones requires an execution. Both approaches have their advantages and disadvantages. In this paper, we propose for the first time, OPEM, an hybrid unknown malware detector which combines the frequency of occurrence of operational codes (statically obtained) with the information of the execution trace of an executable (dynamically obtained). We show that this hybrid approach enhances the performance of both approaches when run separately. © 2013 Springer-Verlag Berlin Heidelberg.","computer security; dynamic; hybrid; machine learning; malware; static","Artificial intelligence; Detectors; Dynamics; Learning systems; Security of data; Security systems; Soft computing; Antivirus softwares; Dynamic features; Execution trace; Global Security; hybrid; Hybrid approach; Malware detection; Malwares; static; Static features; Supervised machine learning; Computer crime",Conference Paper,Scopus,2-s2.0-84868092598
"Chen C.-W., Lin J., Chu Y.-W.","iStable: Off-the-shelf predictor integration for predicting protein stability changes",2013,"BMC Bioinformatics",25,10.1186/1471-2105-14-S2-S5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884189591&doi=10.1186%2f1471-2105-14-S2-S5&partnerID=40&md5=23e9dc0a6a0f011c1a2672c0269b78da","Background: Mutation of a single amino acid residue can cause changes in a protein, which could then lead to a loss of protein function. Predicting the protein stability changes can provide several possible candidates for the novel protein designing. Although many prediction tools are available, the conflicting prediction results from different tools could cause confusion to users. Results: We proposed an integrated predictor, iStable, with grid computing architecture constructed by using sequence information and prediction results from different element predictors. In the learning model, several machine learning methods were evaluated and adopted the support vector machine as an integrator, while not just choosing the majority answer given by element predictors. Furthermore, the role of the sequence information played was analyzed in our model, and an 11-window size was determined. On the other hand, iStable is available with two different input types: structural and sequential. After training and cross-validation, iStable has better performance than all of the element predictors on several datasets. Under different classifications and conditions for validation, this study has also shown better overall performance in different types of secondary structures, relative solvent accessibility circumstances, protein memberships in different superfamilies, and experimental conditions. Conclusions: The trained and validated version of iStable provides an accurate approach for prediction of protein stability changes. iStable is freely available online at: http://predictor.nchu.edu.tw/iStable. © 2013 Chen et al.",,"Amino acids; Artificial intelligence; Forecasting; Grid computing; Learning systems; Amino acid residues; Better performance; Experimental conditions; Grid computing architectures; Machine learning methods; Secondary structures; Sequence informations; Solvent accessibility; Proteins; amino acid; protein; article; chemistry; computer program; genetics; mutation; protein secondary structure; protein stability; support vector machine; Amino Acids; Mutation; Protein Stability; Protein Structure, Secondary; Proteins; Software; Support Vector Machines",Article,Scopus,2-s2.0-84884189591
"Hu B., Chen Y., Keogh E.","Time series classification under more realistic assumptions",2013,"Proceedings of the 2013 SIAM International Conference on Data Mining, SDM 2013",25,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960076541&partnerID=40&md5=a8b36e73eda819ec467897ba1749004a","Most literature on time series classification assumes that the beginning and ending points of the pattern of interest can be correctly identified, both during the training phase and later deployment. In this work, we argue that this assumption is unjustified, and this has in many cases led to unwarranted optimism about the performance of the proposed algorithms. As we shall show, the task of correctly extracting individual gait cycles, heartbeats, gestures, behaviors, etc., is generally much more difficult than the task of actually classifying those patterns. We propose to mitigate this problem by introducing an alignment-free time series classification framework. The framework requires only very weakly annotated data, such as ""in this ten minutes of data, we see mostly normal heartbeats..,"" and by generalizing the classic machine learning idea of data editing to streaming/continuous data, allows us to build robust, fast and accurate classifiers. We demonstrate on several diverse real-world problems that beyond removing unwarranted assumptions and requiring essentially no human intervention, our framework is both significantly faster and significantly more accurate than current state-of-the-art approaches. Copyright © SIAM.",,"Artificial intelligence; Data mining; Learning systems; Alignment-free; Data editing; Human intervention; Real-world problem; State-of-the-art approach; Time series classifications; Training phase; Weakly annotated data; Time series",Conference Paper,Scopus,2-s2.0-84960076541
"Vandenberghe R., Nelissen N., Salmon E., Ivanoiu A., Hasselbalch S., Andersen A., Korner A., Minthon L., Brooks D.J., Van Laere K., Dupont P.","Binary classification of 18F-flutemetamol PET using machine learning: Comparison with visual reads and structural MRI",2013,"NeuroImage",25,10.1016/j.neuroimage.2012.09.015,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867468075&doi=10.1016%2fj.neuroimage.2012.09.015&partnerID=40&md5=35c10cbfb28f5940e692e06a1ffb777f","18F-flutemetamol is a positron emission tomography (PET) tracer for in vivo amyloid imaging. The ability to classify amyloid scans in a binary manner as 'normal' versus 'Alzheimer-like', is of high clinical relevance. We evaluated whether a supervised machine learning technique, support vector machines (SVM), can replicate the assignments made by visual readers blind to the clinical diagnosis, which image components have highest diagnostic value according to SVM and how 18F-flutemetamol-based classification using SVM relates to structural MRI-based classification using SVM within the same subjects. By means of SVM with a linear kernel, we analyzed 18F-flutemetamol scans and volumetric MRI scans from 72 cases from the 18F-flutemetamol phase 2 study (27 clinically probable Alzheimer's disease (AD), 20 amnestic mild cognitive impairment (MCI), 25 controls). In a leave-one-out approach, we trained the 18F-flutemetamol based classifier by means of the visual reads and tested whether the classifier was able to reproduce the assignment based on visual reads and which voxels had the highest feature weights. The 18F-flutemetamol based classifier was able to replicate the assignments obtained by visual reads with 100% accuracy. The voxels with highest feature weights were in the striatum, precuneus, cingulate and middle frontal gyrus. Second, to determine concordance between the gray matter volume- and the 18F-flutemetamol-based classification, we trained the classifier with the clinical diagnosis as gold standard. Overall sensitivity of the 18F-flutemetamol- and the gray matter volume-based classifiers were identical (85.2%), albeit with discordant classification in three cases. Specificity of the 18F-flutemetamol based classifier was 92% compared to 68% for MRI. In the MCI group, the 18F-flutemetamol based classifier distinguished more reliably between converters and non-converters than the gray matter-based classifier. The visual read-based binary classification of 18F-flutemetamol scans can be replicated using SVM. In this sample the specificity of 18F-flutemetamol based SVM for distinguishing AD from controls is higher than that of gray matter volume-based SVM. © 2012 Elsevier Inc.","Alzheimer's disease; Amyloid imaging; Mild cognitive impairment; MRI volumetry","flutemetamol f 18; benzothiazole derivative; diagnostic agent; fluorine; N methyl 2 (4' methylaminophenyl) 6 hydroxybenzothiazole; N-methyl-2-(4'-methylaminophenyl)-6-hydroxybenzothiazole; adult; aged; Alzheimer disease; article; brain size; cingulate gyrus; clinical article; controlled study; corpus striatum; female; gray matter; human; image analysis; image processing; machine learning; male; middle frontal gyrus; mild cognitive impairment; nuclear magnetic resonance imaging; positron emission tomography; precuneus; priority journal; sensitivity and specificity; support vector machine; volumetry; algorithm; Alzheimer disease; artificial intelligence; automated pattern recognition; comparative study; computer assisted diagnosis; differential diagnosis; image enhancement; methodology; mild cognitive impairment; observer variation; reproducibility; Aged; Algorithms; Alzheimer Disease; Artificial Intelligence; Benzothiazoles; Diagnosis, Differential; Female; Fluorine Radioisotopes; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Male; Mild Cognitive Impairment; Observer Variation; Pattern Recognition, Automated; Positron-Emission Tomography; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84867468075
"Scherer S., Stratou G., Gratch J., Morency L.-P.","Investigating voice quality as a speaker-independent indicator of depression and PTSD",2013,"Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH",25,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905232283&partnerID=40&md5=0511bf000f576d12ac1b8dbfcec3dac1","We seek to investigate voice quality characteristics, in particular on a breathy to tense dimension, as an indicator for psychological distress, i.e. depression and post-traumatic stress disorder (PTSD), within semi-structured virtual human interviews. Our evaluation identifies significant differences between the voice quality of psychologically distressed participants and not-distressed participants within this limited corpus. We investigate the capability of automatic algorithms to classify psychologically distressed speech in speaker-independent experiments. Additionally, we examine the impact of the posed questions' affective polarity, as motivated by findings in the literature on positive stimulus attenuation and negative stimulus potentiation in emotional reactivity of psychologically distressed participants. The experiments yield promising results using standard machine learning algorithms and solely four distinct features capturing the tenseness of the speaker's voice. Copyright © 2013 ISCA.","Psychological distress classification; Virtual human interaction; Voice quality","Artificial intelligence; Experiments; Automatic algorithms; Posttraumatic stress disorder; Psychological distress; Semi-structured; Standard machines; Virtual humans; Voice quality; Learning algorithms",Conference Paper,Scopus,2-s2.0-84905232283
"Parkes A.","The effect of task-individual-technology fit on user attitude and performance: An experimental investigation",2013,"Decision Support Systems",25,10.1016/j.dss.2012.10.025,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871722601&doi=10.1016%2fj.dss.2012.10.025&partnerID=40&md5=14d1b2b8476360b7d2b73627eec6177c","Decision support research explores interactions between individuals, tasks, and technology. In this paper, I deconstruct the task-technology-individual fit model into three two-way interactions and ascertain how these interactions affect user attitude and performance. Performance is conceptualized as consisting of two dimensions, technology performance and task performance. The paper reports a controlled laboratory experiment involving 94 subjects using a purpose built decision support system. The results demonstrate several important principles. User attitude is affected by the fit between individual and technology whereas technology performance is affected by the fit between task and technology, and task and individual. Users of technology fitted to them as an individual can perceive it as more useful than it actually is, in terms of improving task performance. Finally, technology performance translates into task performance. Technology performance is a necessary but not sufficient precursor to task performance. © 2012 Elsevier B.V.","Decision support; Decisional guidance; Performance; Task complexity; Task-individual-technology fit; User attitude","Decision supports; Decisional guidance; Performance; Task complexity; Task-individual-technology fit; User attitudes; Artificial intelligence; Decision support systems; Technology",Article,Scopus,2-s2.0-84871722601
"Pennycook S.J., Hammond S.D., Wright S.A., Herdman J.A., Miller I., Jarvis S.A.","An investigation of the performance portability of OpenCL",2013,"Journal of Parallel and Distributed Computing",25,10.1016/j.jpdc.2012.07.005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884816817&doi=10.1016%2fj.jpdc.2012.07.005&partnerID=40&md5=698c4ba258a2c86861ebeb8a4350a070","This paper reports on the development of an MPI/OpenCL implementation of LU, an application-level benchmark from the NAS Parallel Benchmark Suite. An account of the design decisions addressed during the development of this code is presented, demonstrating the importance of memory arrangement and work-item/work-group distribution strategies when applications are deployed on different device types. The resulting platform-agnostic, single source application is benchmarked on a number of different architectures, and is shown to be 1.3-1.5× slower than native FORTRAN 77 or CUDA implementations on a single node and 1.3-3.1× slower on multiple nodes. We also explore the potential performance gains of OpenCL's device fissioning capability, demonstrating up to a 3× speed-up over our original OpenCL implementation. © 2013 Elsevier Ltd. All rights reserved.","GPU computing; High performance computing; Many-core computing; OpenCL; Optimisation","GPU computing; High performance computing; Many-core computing; OpenCL; Optimisations; Artificial intelligence; Computer programming; Benchmarking",Article,Scopus,2-s2.0-84884816817
"Gotovos A., Casati N., Hitz G., Krause A.","Active learning for level set estimation",2013,"IJCAI International Joint Conference on Artificial Intelligence",24,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063599&partnerID=40&md5=6583fb76ade0f0fa1a5a536706a2bc71","Many information gathering problems require determining the set of points, for which an unknown function takes value above or below some given threshold level. We formalize this task as a classification problem with sequential measurements, where the unknown function is modeled as a sample from a Gaussian process (GP). We propose LSE, an algorithm that guides both sampling and classification based on GP-derived confidence bounds, and provide theoretical guarantees about its sample complexity. Furthermore, we extend LSE and its theory to two more natural settings: (1) where the threshold level is implicitly defined as a percentage of the (unknown) maximum of the target function and (2) where samples are selected in batches. We evaluate the effectiveness of our proposed methods on two problems of practical interest, namely autonomous monitoring of algal populations in a lake environment and geolocating network latency.",,"Algal populations; Autonomous monitoring; Confidence bounds; Gaussian process; Information gathering; Network latencies; Sample complexity; Theoretical guarantees; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84896063599
"Weiss N., Rueckert D., Rao A.","Multiple sclerosis lesion segmentation using dictionary learning and sparse coding.",2013,"Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention",24,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894620284&partnerID=40&md5=70fa71897921e7b3bde6541d6049a5f7","The segmentation of lesions in the brain during the development of Multiple Sclerosis is part of the diagnostic assessment for this disease and gives information on its current severity. This laborious process is still carried out in a manual or semiautomatic fashion by clinicians because published automatic approaches have not been universal enough to be widely employed in clinical practice. Thus Multiple Sclerosis lesion segmentation remains an open problem. In this paper we present a new unsupervised approach addressing this problem with dictionary learning and sparse coding methods. We show its general applicability to the problem of lesion segmentation by evaluating our approach on synthetic and clinical image data and comparing it to state-of-the-art methods. Furthermore the potential of using dictionary learning and sparse coding for such segmentation tasks is investigated and various possibilities for further experiments are discussed.",,"algorithm; article; artificial intelligence; automated pattern recognition; brain; computer assisted diagnosis; human; image enhancement; information retrieval; methodology; multiple sclerosis; nuclear magnetic resonance imaging; pathology; reproducibility; sensitivity and specificity; Algorithms; Artificial Intelligence; Brain; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Information Storage and Retrieval; Magnetic Resonance Imaging; Multiple Sclerosis; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84894620284
"Cornaz D., Galand L., Spanjaard O.","Kemeny elections with bounded single-peaked or single-crossing width",2013,"IJCAI International Joint Conference on Artificial Intelligence",24,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896058965&partnerID=40&md5=4af1ac277b5d4f8976491e6193aae521","This paper is devoted to complexity results regarding specific measures of proximity to single-peakedness and single-crossingness, called ""single-peaked width"" [Cornaz et al., 2012] and ""single-crossing width"". Thanks to the use of the PQ-tree data structure [Booth and Lueker, 1976], we show that both problems are polynomial time solvable in the general case (while it was only known for single-peaked width and in the case of narcissistic preferences). Furthermore, we establish one of the first results (to our knowledge) concerning the effect of nearly single-peaked electorates on the complexity of an NP-hard voting system, namely we show the fixed-parameter tractability of Kemeny elections with respect to the parameters ""single-peaked width"" and ""single-crossing width"".",,"Complexity results; Fixed-parameter tractability; NP-hard; Polynomial-time; PQ-tree; Voting systems; Polynomial approximation; Voting machines; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84896058965
"Ketter W., Peters M., Collins J.","Autonomous agents in future energy markets: The 2012 Power Trading Agent Competition",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",24,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893401951&partnerID=40&md5=e1035ad3585d6544a35bfc45fe649fde","Sustainable energy systems of the future will need more than efficient, clean, and low-cost energy sources. They will also need efficient price signals that motivate sustainable energy consumption behaviors and a tight real-time alignment of energy demand with supply from renewable and traditional sources. The Power Trading Agent Competition (Power TAC) is a rich, competitive, open-source simulation platform for future retail power markets built on real-world data and state-of-the-art customer models. Its purpose is to help researchers understand the dynamics of customer and retailer decision-making as well as the robustness of proposed market designs. Power TAC invites researchers to develop autonomous electricity broker agents and to pit them against best-in-class strategies in global competitions, the first of which will be held at AAAI 2013. Power TAC competitions provide compelling, actionable information for policy makers and industry leaders. We describe the competition scenario, demonstrate the realism of the Power TAC platform, and analyze key characteristics of successful brokers in one of our 2012 pilot competitions between seven research groups from five different countries. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Future energies; Global competition; Key characteristics; Low-cost energy; Research groups; Simulation platform; Sustainable energy; Sustainable energy systems; Artificial intelligence; Autonomous agents; Computer simulation; Decision making; Energy conservation; Energy utilization; Research; Sales; Commerce",Conference Paper,Scopus,2-s2.0-84893401951
"Tawfeek M.A., El-Sisi A., Keshk A.E., Torkey F.A.","Cloud task scheduling based on ant colony optimization",2013,"Proceedings - 2013 8th International Conference on Computer Engineering and Systems, ICCES 2013",24,10.1109/ICCES.2013.6707172,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893671410&doi=10.1109%2fICCES.2013.6707172&partnerID=40&md5=774b449f140bafeaf642687a6c3255b1","Cloud computing is the development of distributed computing, parallel computing and grid computing, or defined as the commercial implementation of these computer science concepts. One of the fundamental issues in this environment is related to task scheduling. Cloud task scheduling is an NP-hard optimization problem, and many meta-heuristic algorithms have been proposed to solve it. A good task scheduler should adapt its scheduling strategy to the changing environment and the types of tasks. In this paper a cloud task scheduling policy based on ant colony optimization algorithm compared with different scheduling algorithms FCFS and round-robin, has been presented. The main goal of these algorithms is minimizing the makespan of a given tasks set. Ant colony optimization is random optimization search approach that will be used for allocating the incoming jobs to the virtual machines. Algorithms have been simulated using Cloudsim toolkit package. Experimental results showed that the ant colony optimization outperformed FCFS and round-robin algorithms. © 2013 IEEE.","ant colony optimization; Cloud computing; CloudSim; makespan; task scheduling","Ant Colony Optimization algorithms; Cloudsim; Commercial implementation; Makespan; Meta heuristic algorithm; Round-Robin algorithms; Scheduling strategies; Task-scheduling; Ant colony optimization; Artificial intelligence; Cloud computing; Grid computing; Heuristic algorithms; Parallel architectures; Scheduling algorithms; Multitasking",Conference Paper,Scopus,2-s2.0-84893671410
"Messina F., Pappalardo G., Rosaci D., Santoro C., Sarné G.M.L.","A distributed Agent-based approach for supporting group formation in P2P e-Learning",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",24,10.1007/978-3-319-03524-6_27,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892644762&doi=10.1007%2f978-3-319-03524-6_27&partnerID=40&md5=771149f423aeae654429d18b92accd95","Peer-to-Peer (P2P) technology can be effectively used to implement cooperative e-Learning systems, where the available knowledge for a student is not only from teachers, but also from other students having similar interests and preferences. In such a scenario, a central issue is to form groups of users having similar interests and satisfying personal user's constraints. In this paper we propose a novel approach, called HADEL (Hyperspace Agent-based E-Learning), based on an overlay network of software agents. Our approach preserves user's privacy, allowing to locally maintain sensitive user's data and inferring the properties necessary for determining the groups by using agents acting as personal assistants. The results obtained by some tests performed on simulated e-Learning environments show the efficiency of our approach, that suitably exploits the topology of the overlay network, which exhibits the typical properties of a small-world system. © Springer International Publishing Switzerland 2013.",,"Agent-based approach; Agent-based e-learning; Cooperative e-learning; E-learning environment; Peer-to-peer technologies; Personal assistants; Small-world systems; Typical properties; Artificial intelligence; Computer aided instruction; Overlay networks; E-learning",Conference Paper,Scopus,2-s2.0-84892644762
"Cambria E.","An introduction to concept-level sentiment analysis",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",24,10.1007/978-3-642-45111-9_41,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893774225&doi=10.1007%2f978-3-642-45111-9_41&partnerID=40&md5=b3e159fe528a396d571ce438220c396b","The ways people express their opinions and sentiments have radically changed in the past few years thanks to the advent of social networks, web communities, blogs, wikis, and other online collaborative media. The distillation of knowledge from the huge amount of unstructured information on the Web can be a key factor for marketers who want to create an image or identity in the minds of their customers for their product or brand. These online social data, however, remain hardly accessible to computers, as they are specifically meant for human consumption. The automatic analysis of online opinions, in fact, involves a deep understanding of natural language text by machines, from which we are still very far. To this end, concept-level sentiment analysis aims to go beyond a mere word-level analysis of text and provide novel approaches to opinion mining and sentiment analysis that enable a more efficient passage from (unstructured) textual information to (structured) machine-processable data, in potentially any domain. © Springer-Verlag 2013.","AI; Big social data analysis; Concept-level sentiment analysis; NLP","Automatic analysis; Human consumption; Natural language text; NLP; On-line opinions; Opinion mining; Sentiment analysis; Textual information; Artificial intelligence; Data mining; Distillation; Natural language processing systems; Soft computing; Social networking (online)",Conference Paper,Scopus,2-s2.0-84893774225
"Stefanoni G., Motik B., Horrocks I.","Introducing nominals to the combined query answering approaches for EL",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",24,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893407341&partnerID=40&md5=b0fa31fc392dd1283ead8cc0f62904ec","So-called combined approaches answer a conjunctive query over a description logic ontology in three steps: first, they materialise certain consequences of the ontology and the data; second, they evaluate the query over the data; and third, they filter the result of the second phase to eliminate unsound answers. Such approaches were developed for various members of the DL-Lite and the EL families of languages, but none of them can handle ontologies containing nominals. In our work, we bridge this gap and present a combined query answering approach for ELHO⊥r-a logic that contains all features of the OWL 2 EL standard apart from transitive roles and complex role inclusions. This extension is nontrivial because nominals require equality reasoning, which introduces complexity into the first and the third step. Our empirical evaluation suggests that our technique is suitable for practical application, and so it provides a practical basis for conjunctive query answering in a large fragment of OWL 2 EL Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Complex roles; Conjunctive queries; Description logic; Dl-lite; Empirical evaluations; Query answering; Second phase; Artificial intelligence; Data description; Query processing; Birds",Conference Paper,Scopus,2-s2.0-84893407341
"Li J., Lu Z.","Pathway-based drug repositioning using causal inference",2013,"BMC Bioinformatics",24,10.1186/1471-2105-14-S16-S3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886775358&doi=10.1186%2f1471-2105-14-S16-S3&partnerID=40&md5=f130c4b0c19d94a1511ee515ca442d7a","Background: Recent in vivo studies showed new hopes of drug repositioning through causality inference from drugs to disease. Inspired by their success, here we present an in silico method for building a causal network (CauseNet) between drugs and diseases, in an attempt to systematically identify new therapeutic uses of existing drugs.Methods: Unlike the traditional 'one drug-one target-one disease' causal model, we simultaneously consider all possible causal chains connecting drugs to diseases via target- and gene-involved pathways based on rich information in several expert-curated knowledge-bases. With statistical learning, our method estimates transition likelihood of each causal chain in the network based on known drug-disease treatment associations (e.g. bexarotene treats skin cancer).Results: To demonstrate its validity, our method showed high performance (AUC = 0.859) in cross validation. Moreover, our top scored prediction results are highly enriched in literature and clinical trials. As a showcase of its utility, we show several drugs for potential re-use in Crohn's Disease.Conclusions: We successfully developed a computational method for discovering new uses of existing drugs based on casual inference in a layered drug-target-pathway-gene- disease network. The results showed that our proposed method enables hypothesis generation from public accessible biological data for drug repositioning. © 2013 Li and Lu; licensee BioMed Central Ltd.",,"Biological data; Causal inferences; Causal network; Cross validation; Drug repositioning; Hypothesis generation; Statistical learning; Therapeutic use; Chains; Gene therapy; Diseases; article; artificial intelligence; biology; computer simulation; Crohn disease; drug repositioning; gene regulatory network; human; methodology; reproducibility; statistical model; Artificial Intelligence; Computational Biology; Computer Simulation; Crohn Disease; Drug Repositioning; Gene Regulatory Networks; Humans; Likelihood Functions; Reproducibility of Results",Article,Scopus,2-s2.0-84886775358
"Wang Y., Goh W., Wong L., Montana G.","Random forests on Hadoop for genome-wide association studies of multivariate neuroimaging phenotypes",2013,"BMC Bioinformatics",24,10.1186/1471-2105-14-S16-S6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886771360&doi=10.1186%2f1471-2105-14-S16-S6&partnerID=40&md5=1649538267a2c3c39d136562127649cf","Motivation: Multivariate quantitative traits arise naturally in recent neuroimaging genetics studies, in which both structural and functional variability of the human brain is measured non-invasively through techniques such as magnetic resonance imaging (MRI). There is growing interest in detecting genetic variants associated with such multivariate traits, especially in genome-wide studies. Random forests (RFs) classifiers, which are ensembles of decision trees, are amongst the best performing machine learning algorithms and have been successfully employed for the prioritisation of genetic variants in case-control studies. RFs can also be applied to produce gene rankings in association studies with multivariate quantitative traits, and to estimate genetic similarities measures that are predictive of the trait. However, in studies involving hundreds of thousands of SNPs and high-dimensional traits, a very large ensemble of trees must be inferred from the data in order to obtain reliable rankings, which makes the application of these algorithms computationally prohibitive.Results: We have developed a parallel version of the RF algorithm for regression and genetic similarity learning tasks in large-scale population genetic association studies involving multivariate traits, called PaRFR (Parallel Random Forest Regression). Our implementation takes advantage of the MapReduce programming model and is deployed on Hadoop, an open-source software framework that supports data-intensive distributed applications. Notable speed-ups are obtained by introducing a distance-based criterion for node splitting in the tree estimation process. PaRFR has been applied to a genome-wide association study on Alzheimer's disease (AD) in which the quantitative trait consists of a high-dimensional neuroimaging phenotype describing longitudinal changes in the human brain structure. PaRFR provides a ranking of SNPs associated to this trait, and produces pair-wise measures of genetic proximity that can be directly compared to pair-wise measures of phenotypic proximity. Several known AD-related variants have been identified, including APOE4 and TOMM40. We also present experimental evidence supporting the hypothesis of a linear relationship between the number of top-ranked mutated states, or frequent mutation patterns, and an indicator of disease severity.Availability: The Java codes are freely available at http://www2.imperial.ac.uk/~gmontana. © 2013 Wang et al.; licensee BioMed Central Ltd.",,"Distributed applications; Experimental evidence; Genetic similarities; Genome-wide association studies; Linear relationships; Map-reduce programming; Neuroimaging genetics; Open-source softwares; Brain; Decision trees; Forestry; Genes; Genetic programming; Learning algorithms; Magnetic resonance imaging; Population statistics; Trees (mathematics); Functional neuroimaging; Algorithms; Biological Populations; Decision Making; Forestry; Genes; Mathematics; Statistics; Trees; algorithm; Alzheimer disease; article; artificial intelligence; brain; computer program; computer simulation; genetic association; genetics; human; neuroimaging; nuclear magnetic resonance imaging; pathology; phenotype; quantitative trait locus; single nucleotide polymorphism; Algorithms; Alzheimer Disease; Artificial Intelligence; Brain; Computer Simulation; Genome-Wide Association Study; Humans; Magnetic Resonance Imaging; Neuroimaging; Phenotype; Polymorphism, Single Nucleotide; Quantitative Trait Loci; Software",Article,Scopus,2-s2.0-84886771360
"Di Ciccio C., Mecella M.","A two-step fast algorithm for the automated discovery of declarative workflows",2013,"Proceedings of the 2013 IEEE Symposium on Computational Intelligence and Data Mining, CIDM 2013 - 2013 IEEE Symposium Series on Computational Intelligence, SSCI 2013",24,10.1109/CIDM.2013.6597228,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885659827&doi=10.1109%2fCIDM.2013.6597228&partnerID=40&md5=83aadc22c574f1a8efcd1022f425cdd5","Declarative approaches are particularly suitable for modeling highly flexible processes. They especially apply to artful processes, i.e., rapid informal processes that are typically carried out by those people whose work is mental rather than physical (managers, professors, researchers, engineers, etc.), the so called 'knowledge workers'. This paper describes MINERful++, a two-step algorithm for an efficient discovery of constraints that constitute declarative workflow models. As a first step, a knowledge base is built, with information about temporal statistics gathered from execution traces. Then, the statistical support of constraints is computed, by querying that knowledge base. MINERful++ is fast, modular, independent of the specific formalism adopted for representing constraints, based on a probabilistic approach and capable of eliminating the redundancy of subsumed constraints. © 2013 IEEE.",,"Automated discovery; Declarative workflows; Fast algorithms; Flexible process; Knowledge workers; Probabilistic approaches; Two-step algorithms; Workflow models; Data mining; Knowledge based systems; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84885659827
"Bousmalis K., Zafeiriou S., Morency L.-P., Pantic M.","Infinite hidden conditional random fields for human behavior analysis",2013,"IEEE Transactions on Neural Networks and Learning Systems",24,10.1109/TNNLS.2012.2224882,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884943362&doi=10.1109%2fTNNLS.2012.2224882&partnerID=40&md5=af3e1b68c059dd42d689469f5074d100","Hidden conditional random fields (HCRFs) are discriminative latent variable models that have been shown to successfully learn the hidden structure of a given classification problem (provided an appropriate validation of the number of hidden states). In this brief, we present the infinite HCRF (iHCRF), which is a nonparametric model based on hierarchical Dirichlet processes and is capable of automatically learning the optimal number of hidden states for a classification task. We show how we learn the model hyperparameters with an effective Markov-chain Monte Carlo sampling technique, and we explain the process that underlines our iHCRF model with the Restaurant Franchise Rating Agencies analogy. We show that the iHCRF is able to converge to a correct number of represented hidden states, and outperforms the best finite HCRFs - chosen via cross-validation - for the difficult tasks of recognizing instances of agreement, disagreement, and pain. Moreover, the iHCRF manages to achieve this performance in significantly less total training, validation, and testing time. © 2012 IEEE.","Discriminative models; Hidden conditional random fields; Nonparametric bayesian learning","Classification tasks; Discriminative models; Hidden conditional random fields; Hierarchical Dirichlet process; Human behavior analysis; Latent variable models; Markov chain Monte Carlo; Non-parametric Bayesian; Monte Carlo methods; Random processes; algorithm; artificial intelligence; automated pattern recognition; behavior; computer simulation; human; Monte Carlo method; probability; Algorithms; Artificial Intelligence; Behavior; Computer Simulation; Humans; Markov Chains; Monte Carlo Method; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84884943362
"Mesiar R., Stupňanová A.","Decomposition integrals",2013,"International Journal of Approximate Reasoning",24,10.1016/j.ijar.2013.02.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879607193&doi=10.1016%2fj.ijar.2013.02.001&partnerID=40&md5=292a79ae026fde8ee26801c97838e717","Decomposition integrals recently proposed by Even and Lehrer are deeply studied and discussed. Characterization of integrals recognizing and distinguishing the underlying measures is given. As a by-product, a graded class of integrals varying from Shilkret integral to Choquet integral is proposed. © 2013 Elsevier Inc. All rights reserved.","Choquet integral; Lehrer integral; Monotone measure; Shilkret integral; Universal integral","Choquet integral; Lehrer integral; Monotone measures; Shilkret integral; Universal integrals; Artificial intelligence; Software engineering; Integral equations",Article,Scopus,2-s2.0-84879607193
"Ostrovsky R., Scafuro A., Visconti I., Wadia A.","Universally composable secure computation with (malicious) physically uncloneable functions",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",24,10.1007/978-3-642-38348-9_41,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883419777&doi=10.1007%2f978-3-642-38348-9_41&partnerID=40&md5=0fb356e7859b1e4ed8f526b16e8bbd0d","Physically Uncloneable Functions (PUFs) [28] are noisy physical sources of randomness. As such, they are naturally appealing for cryptographic applications, and have caught the interest of both theoreticians and practitioners. A major step towards understanding and securely using PUFs was recently taken in [Crypto 2011] where Brzuska, Fischlin, Schröder and Katzenbeisser model PUFs in the Universal Composition (UC) framework of Canetti [FOCS 2001]. A salient feature of their model is that it considers trusted PUFs only; that is, PUFs which have been produced via the prescribed manufacturing process and are guaranteed to be free of any adversarial influence. However, this does not accurately reflect real-life scenarios, where an adversary could be able to create and use malicious PUFs. The goal of this work is to extend the model proposed in [Crypto 2011] in order to capture such a real-world attack. The main contribution of this work is the study of the Malicious PUFs model. To this end, we first formalize the notion of ""malicious"" PUFs, and extend the UC formulation of Brzuska et al. to allow the adversary to create PUFs with arbitrary adversarial behaviour. Then, we provide positive results in this, more realistic, model. We show that, under computational assumptions, it is possible to UC-securely realize any functionality. © 2013 International Association for Cryptologic Research.",,"Computational assumptions; Cryptographic applications; Manufacturing process; Real-world attack; Salient features; Secure computation; Universally composable; Artificial intelligence; Computer science; Cryptography",Conference Paper,Scopus,2-s2.0-84883419777
"Rello L., Baeza-Yates R., Dempere-Marco L., Saggion H.","Frequent words improve readability and short words improve understandability for people with dyslexia",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",24,10.1007/978-3-642-40498-6_15,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883266397&doi=10.1007%2f978-3-642-40498-6_15&partnerID=40&md5=7f577673877fe25a9ed763f06bfb0a83","Around 10% of the population has dyslexia, a reading disability that negatively affects a person's ability to read and comprehend texts. Previous work has studied how to optimize the text layout, but adapting the text content has not received that much attention. In this paper, we present an eye-tracking study that investigates if people with dyslexia would benefit from content simplification. In an experiment with 46 people, 23 with dyslexia and 23 as a control group, we compare texts where words were substituted by shorter/longer and more/less frequent synonyms. Using more frequent words caused the participants with dyslexia to read significantly faster, while the use of shorter words caused them to understand the text better. Amongst the control group, no significant effects were found. These results provide evidence that people with dyslexia may benefit from interactive tools that perform lexical simplification. © 2013 Springer-Verlag.","dyslexia; eye-tracking; lexical simplification; readability; Textual accessibility; understandability; word frequency; word length","dyslexia; Eye-tracking; lexical simplification; readability; Textual accessibility; Understandability; Word frequencies; Word length; Artificial intelligence; Computer science; Human computer interaction",Conference Paper,Scopus,2-s2.0-84883266397
"Di Gaspero L., Rendl A., Urli T.","A hybrid ACO+CP for balancing bicycle sharing systems",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",24,10.1007/978-3-642-38516-2_16,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882960122&doi=10.1007%2f978-3-642-38516-2_16&partnerID=40&md5=7a9f6d36e60c0d2171756c03119d2961","Balancing bike sharing systems is an increasingly important problem, because of the rising popularity of this mean of transportation. Bike sharing systems need to be balanced so that bikes (and empty slots for returning bikes) are available to the customers, thus ensuring an adequate level of service. In this paper, we tackle the problem of balancing a real-world bike sharing system (BBSP) by means of a hybrid metaheuristic method. Our main contributions are: (i) a new Constraint Programming (CP) formulation for the problem, and (ii) a novel hybrid approach which combines CP techniques with Ant Colony Optimization (ACO). We validate our approach against real world instances from the Vienna Citybike system. © 2013 Springer-Verlag.",,"Ant Colony Optimization (ACO); Bicycle sharing; Constraint programming; Hybrid approach; Hybrid Meta-heuristic; Level of Service; Real-world; Sharing systems; Artificial intelligence; Computer programming; Constraint theory; Heuristic algorithms; Bicycles",Conference Paper,Scopus,2-s2.0-84882960122
"Pellegrino G., Cupertino F., Gerada C.","Barriers shapes and minimum set of rotor parameters in the automated design of Synchronous Reluctance machines",2013,"Proceedings of the 2013 IEEE International Electric Machines and Drives Conference, IEMDC 2013",24,10.1109/IEMDC.2013.6556286,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881648059&doi=10.1109%2fIEMDC.2013.6556286&partnerID=40&md5=b0c553529cb4ea64a4fc617f0dcff290","The rotor design of Synchronous Reluctance machines is considered in this paper, based on a multi-objective, genetic optimization algorithm and finite element analysis. Three different types of barrier geometries are compared, all described by a limited set of input variables. The aim of the paper is to investigate the relationships between the obtainable performance and the different barrier types. The two questions underlying this analysis are: which is the geometry that can potentially give the machine with the highest torque to volume ratio? Which is the geometry with the best compromise between number of input parameters (i.e. computational time) and performance? The results of the analysis show that Synchronous Reluctance machines can be designed using artificial intelligence in a reasonable time, obtaining adequate performances and rotor geometries consistent with the literature. © 2013 IEEE.","Design optimization; Pareto optimization; Rotor design; Synchronous reluctance machines","Automated design; Computational time; Design optimization; Genetic optimization algorithm; Pareto optimization; Rotor design; Rotor parameters; Synchronous reluctance machine; Algorithms; Artificial intelligence; Electric drives; Electric machinery; Geometry; Optimization; Synchronous machinery; Reluctance motors",Conference Paper,Scopus,2-s2.0-84881648059
"Akhmedova S., Semenkin E.","Co-operation of biology related algorithms",2013,"2013 IEEE Congress on Evolutionary Computation, CEC 2013",24,10.1109/CEC.2013.6557831,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881601922&doi=10.1109%2fCEC.2013.6557831&partnerID=40&md5=51432d84fc3a9448fad906d66712c99f","A new meta-heuristic algorithm, called Co-Operation of Biology Related Algorithms (COBRA), for solving real-parameter optimization problems is introduced and described. The algorithm is based on cooperation of biologically inspired algorithms such as Particle Swarm Optimization (PSO), Wolf Pack Search Algorithm (WPS), Firefly Algorithm (FFA), Cuckoo Search Algorithm (CSA) and Bat Algorithm (BA). The proposed algorithm performance is evaluated on given 28 test functions and its workability and usefulness is demonstrated. Ways of algorithm improvement are discussed. © 2013 IEEE.","cooperation; nature-inspired strategy; real-parameter black box optimization; self-tuning; swarm intelligence","Black-box optimization; cooperation; nature-inspired strategy; Selftuning; Swarm Intelligence; Artificial intelligence; Heuristic algorithms; Particle swarm optimization (PSO); Learning algorithms",Conference Paper,Scopus,2-s2.0-84881601922
"Bickmore T.W., Schulman D., Sidner C.","Automated interventions for multiple health behaviors using conversational agents",2013,"Patient Education and Counseling",24,10.1016/j.pec.2013.05.011,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880329815&doi=10.1016%2fj.pec.2013.05.011&partnerID=40&md5=f95f289aaba03c31e60fecb099ca2bc0","Objective: An automated health counselor agent was designed to promote both physical activity and fruit and vegetable consumption through a series of simulated conversations with users on their home computers. Methods: The agent was evaluated in a 4-arm randomized trial of a two-month daily contact intervention comparing: (a) physical activity; (b) fruit and vegetable consumption; (c) both interventions; and (d) a non-intervention control. Physical activity was assessed using daily pedometer steps. Daily servings of fruit and vegetables were assessed using the NIH/NCI self-report Fruit and Vegetable Scan. Results: Participants in the physical activity intervention increased their walking on average compared to the control group, while those in the fruit and vegetable intervention and combined intervention decreased walking. Participants in the fruit and vegetable intervention group consumed significantly more servings per day compared to those in the control group, and those in the combined intervention reported consuming more compared to those in the control group. Conclusion: Automated health intervention software designed for efficient re-use is effective at changing health behavior. Practice implications: Automated health behavior change interventions can be designed to facilitate translation and adaptation across multiple behaviors. © 2013 Elsevier Ireland Ltd.","Behavioral informatics; Dialog system; Diet promotion; Embodied conversational agent; Fruit and vegetable consumption promotion; Health behavior change intervention; Ontology; Physical activity promotion; Relational agent; Walking promotion","adult; article; artificial intelligence; automation; behavior change; computer; computer interface; computer program; controlled study; conversation; feeding behavior; female; food intake; fruit; Fruit and Vegetable Scan; health behavior; health education; health promotion; human; human experiment; International Physical Activity Questionnaire; male; named inventories, questionnaires and rating scales; normal human; pedometer; physical activity; portion size; priority journal; randomized controlled trial; self report; simulation; vegetable; walking; weight change; Behavioral informatics; Dialog system; Diet promotion; Embodied conversational agent; Fruit and vegetable consumption promotion; Health behavior change intervention; Ontology; Physical activity promotion; Relational agent; Walking promotion; Adult; Aged; Artificial Intelligence; Computer-Assisted Instruction; Female; Food Habits; Fruit; Health Behavior; Health Promotion; Humans; Intervention Studies; Interviews as Topic; Male; Motor Activity; Outcome and Process Assessment (Health Care); Qualitative Research; Questionnaires; Vegetables; Walking",Article,Scopus,2-s2.0-84880329815
"Abbasi M., Abduli M.A., Omidvar B., Baghvand A.","Forecasting municipal solid waste generation by hybrid support vector machine and partial least square model",2013,"International Journal of Environmental Research",24,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880671254&partnerID=40&md5=d9193accf5e711219fd9a7cc4da9b657","Forecasting of municipal waste generation is a critical challenge for decision making and planning, because proper planning and operation of a solid waste management system is intensively affected by municipal solid waste (MSW) streams analysis and accurate predictions of solid waste quantities generated. Due to dynamic and complexity of solid waste management system, models by artificial intelligence can be a useful solution of this problem. In this paper, a novel method of Forecasting MSW generation has been proposed. Here, support vector machine (SVM) as an intelligence tool combined with partial least square (PLS) as a feature selection tool was used to weekly prediction of MSW generated in Tehran, Iran. Weekly MSW generated in the period of 2008 to 2011 was used as input data for model learning. Moreover, Monte Carlo method was used to analyze uncertainty of the model results. Model performance evaluated and compared by statistical indices of Relative Mean Errors, Root Mean Squared Errors, Mean Absolute Relative Error and coefficient of determination. Comparison of SVM and PLS-SVM model showed PLS-SVM is superior to SVM model in predictive ability and calculation time saving. Also, results demonstrate which PLS could successfully identify the complex nonlinearity and correlations among input variables and minimize them. The uncertainty analysis also verified that the PLS-SVM model had more robustness than SVM and had a lower sensitivity to change of input variables.","Intelligent model; Municipal solid waste; Partial least square; Support vector machine","algorithm; artificial intelligence; forecasting method; least squares method; Monte Carlo analysis; municipal solid waste; statistical analysis; waste management; Iran; Tehran [Iran]",Article,Scopus,2-s2.0-84880671254
"Bondareva D., Conati C., Feyzi-Behnagh R., Harley J.M., Azevedo R., Bouchet F.","Inferring learning from gaze data during interaction with an environment to support self-regulated learning",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",24,10.1007/978-3-642-39112-5-24,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880003758&doi=10.1007%2f978-3-642-39112-5-24&partnerID=40&md5=0fff5a319c090cd53db52ad81a656a58","In this paper, we explore the potential of gaze data as a source of information to predict learning as students interact with MetaTutor, an ITS that scaffolds self-regulated learning. Using data from 47 college students, we show that a classifier using a variety of gaze features achieves considerable accuracy in predicting student learning after seeing gaze data from the complete interaction. We also show promising results on the classifier ability to detect learning in real-time during interaction. © 2013 Springer-Verlag Berlin Heidelberg.","Eye-tracking; Self-regulated learning; Student modeling","College students; Eye-tracking; Self-regulated learning; Student learning; Student Modeling; Artificial intelligence; Scaffolds; Students",Conference Paper,Scopus,2-s2.0-84880003758
"Kshirsagar M., Carbonell J., Klein-Seetharaman J.","Multitask learning for host-pathogen protein interactions",2013,"Bioinformatics",24,10.1093/bioinformatics/btt245,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879901847&doi=10.1093%2fbioinformatics%2fbtt245&partnerID=40&md5=ab8b45e3a9fecaaeeaf7d8d7de24ac59","Motivation: An important aspect of infectious disease research involves understanding the differences and commonalities in the infection mechanisms underlying various diseases. Systems biology-based approaches study infectious diseases by analyzing the interactions between the host species and the pathogen organisms. This work aims to combine the knowledge from experimental studies of host-pathogen interactions in several diseases to build stronger predictive models. Our approach is based on a formalism from machine learning called 'multitask learning', which considers the problem of building models across tasks that are related to each other. A 'task' in our scenario is the set of host-pathogen protein interactions involved in one disease. To integrate interactions from several tasks (i.e. diseases), our method exploits the similarity in the infection process across the diseases. In particular, we use the biological hypothesis that similar pathogens target the same critical biological processes in the host, in defining a common structure across the tasks.Results: Our current work on host-pathogen protein interaction prediction focuses on human as the host, and four bacterial species as pathogens. The multitask learning technique we develop uses a task-based regularization approach. We find that the resulting optimization problem is a difference of convex (DC) functions. To optimize, we implement a Convex-Concave procedure-based algorithm. We compare our integrative approach to baseline methods that build models on a single host-pathogen protein interaction dataset. Our results show that our approach outperforms the baselines on the training data. We further analyze the protein interaction predictions generated by the models, and find some interesting insights. © The Author 2013.",,"bacterial protein; bacterial protein; algorithm; artificial intelligence; host pathogen interaction; human; metabolism; procedures; protein analysis; article; metabolism; methodology; protein analysis; Algorithms; Artificial Intelligence; Bacterial Proteins; Host-Pathogen Interactions; Humans; Protein Interaction Mapping; Algorithms; Artificial Intelligence; Bacterial Proteins; Host-Pathogen Interactions; Humans; Protein Interaction Mapping",Conference Paper,Scopus,2-s2.0-84879901847
"Nascimento M.Z.D., Martins A.S., Neves L.A., Ramos R.P., Flores E.L., Carrijo G.A.","Classification of masses in mammographic image using wavelet domain features and polynomial classifier",2013,"Expert Systems with Applications",24,10.1016/j.eswa.2013.04.036,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879043579&doi=10.1016%2fj.eswa.2013.04.036&partnerID=40&md5=ff7fc8fdaacebd2f27f620ef76f0bc0b","Breast cancer is the most common cancer among women. In CAD systems, several studies have investigated the use of wavelet transform as a multiresolution analysis tool for texture analysis and could be interpreted as inputs to a classifier. In classification, polynomial classifier has been used due to the advantages of providing only one model for optimal separation of classes and to consider this as the solution of the problem. In this paper, a system is proposed for texture analysis and classification of lesions in mammographic images. Multiresolution analysis features were extracted from the region of interest of a given image. These features were computed based on three different wavelet functions, Daubechies 8, Symlet 8 and bi-orthogonal 3.7. For classification, we used the polynomial classification algorithm to define the mammogram images as normal or abnormal. We also made a comparison with other artificial intelligence algorithms (Decision Tree, SVM, K-NN). A Receiver Operating Characteristics (ROC) curve is used to evaluate the performance of the proposed system. Our system is evaluated using 360 digitized mammograms from DDSM database and the result shows that the algorithm has an area under the ROC curve Az of 0.98 ± 0.03. The performance of the polynomial classifier has proved to be better in comparison to other classification algorithms. © 2013 Elsevier Ltd. All rights reserved.","Mammography; Polynomial classifier; Texture analysis; Wavelet CAD","Area under the ROC curve; Artificial intelligence algorithms; Classification algorithm; Digitized mammograms; Polynomial classifier; Receiver operating characteristics curves (ROC); Texture analysis; Wavelet domain features; Algorithms; Artificial intelligence; Computer aided diagnosis; Decision trees; Discrete wavelet transforms; Diseases; Mammography; Multiresolution analysis; Orthogonal functions; Textures; X ray screens; Polynomials",Article,Scopus,2-s2.0-84879043579
"Li J., Wang Z., Zhang Y.","Provably secure certificate-based signature scheme without pairings",2013,"Information Sciences",24,10.1016/j.ins.2013.01.013,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875218600&doi=10.1016%2fj.ins.2013.01.013&partnerID=40&md5=045554fa131a0fca9012bac8900eacee","In order to simplify certificate management in traditional public key cryptography and solve the key escrow problem in identity-based cryptography, the notion of certificate-based cryptography was introduced. Recently, Ming and Wang proposed a certificate-based signature scheme without pairings. They claimed that the scheme was existentially unforgeable against adaptive chosen message and identity attacks in the random oracle. In this paper, we show that the scheme is insecure against a malicious certifier under existing security model. We also propose a new efficient certificate-based signature scheme without pairings, which is proven secure in the random oracle model. Compared with the existed certificate-based signature schemes without parings, our scheme enjoys shorter signature length and less operation cost. © 2013 Elsevier Inc. All rights reserved.","Certificate-based signature; Key replacement attack; Pairing; Random oracle model","Certificate management; Certificate-based; Certificate-based signatures; Identity based cryptography; Pairing; Provably secure; Random Oracle model; Replacement attacks; Artificial intelligence; Software engineering; Public key cryptography",Article,Scopus,2-s2.0-84875218600
"Fang R., Chen T., Sanelli P.C.","Towards robust deconvolution of low-dose perfusion CT: Sparse perfusion deconvolution using online dictionary learning",2013,"Medical Image Analysis",24,10.1016/j.media.2013.02.005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875794414&doi=10.1016%2fj.media.2013.02.005&partnerID=40&md5=c4deb0899d203bd6be10d70296f2f682","Computed tomography perfusion (CTP) is an important functional imaging modality in the evaluation of cerebrovascular diseases, particularly in acute stroke and vasospasm. However, the post-processed parametric maps of blood flow tend to be noisy, especially in low-dose CTP, due to the noisy contrast enhancement profile and the oscillatory nature of the results generated by the current computational methods. In this paper, we propose a robust sparse perfusion deconvolution method (SPD) to estimate cerebral blood flow in CTP performed at low radiation dose. We first build a dictionary from high-dose perfusion maps using online dictionary learning and then perform deconvolution-based hemodynamic parameters estimation on the low-dose CTP data. Our method is validated on clinical data of patients with normal and pathological CBF maps. The results show that we achieve superior performance than existing methods, and potentially improve the differentiation between normal and ischemic tissue in the brain. © 2013 Elsevier B.V.","Computed tomography perfusion; Deconvolution algorithm; Online dictionary learning; Radiation dosage; Sparse representation","Computed Tomography; Deconvolution algorithm; Online dictionary learning; Radiation dosage; Sparse representation; Computerized tomography; Deconvolution; Tissue; E-learning; adult; algorithm; article; brain blood flow; brain ischemia; brain mapping; brain perfusion; case report; computed tomography perfusion; computer assisted tomography; contrast enhancement; diagnostic accuracy; diagnostic test accuracy study; hemodynamics; human; human tissue; imaging system; male; neuropathology; online dictionary learning; online system; priority journal; radiation dose; sparse perfusion deconvolution; Artificial Intelligence; Brain; Cerebral Angiography; Cerebrovascular Circulation; Cerebrovascular Disorders; Databases, Factual; Dictionaries, Medical; Humans; Information Storage and Retrieval; Online Systems; Pattern Recognition, Automated; Perfusion Imaging; Radiation Dosage; Radiation Protection; Reproducibility of Results; Sensitivity and Specificity; Tomography, X-Ray Computed",Article,Scopus,2-s2.0-84875794414
"Gupta P., Inuiguchi M., Mehlawat M.K., Mittal G.","Multiobjective credibilistic portfolio selection model with fuzzy chance-constraints",2013,"Information Sciences",24,10.1016/j.ins.2012.12.011,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873307930&doi=10.1016%2fj.ins.2012.12.011&partnerID=40&md5=d5fbdc0dcd97f5cf2ada0edf2de885ac","In this paper, we propose a multiobjective credibilistic model with fuzzy chance constraints of the portfolio selection problem. The key financial criteria used are short-term return, long-term return, risk and liquidity. The model generates portfolios which are optimal to the extent of achieving the highest credibility values for the objective functions. The problem is solved using a hybrid intelligent algorithm that integrates fuzzy simulation with a real-coded genetic algorithm. The approach adopted here has advantage of handling the multiobjective portfolio selection problem where fuzzy parameters are characterized by general functional forms. Numerical examples are provided to demonstrate effectiveness of the solution approach and efficiency of the model. © 2012 Elsevier Inc. All rights reserved.","Chance-constrained programming; Credibility measure; Fuzzy portfolio selection; Multiobjective programming; Real-coded genetic algorithm","Chance-constrained programming; Credibility measure; Multiobjective programming; Portfolio selection; Real-coded genetic algorithm; Artificial intelligence; Software engineering; Computer programming",Article,Scopus,2-s2.0-84873307930
"Sugimoto C.R., Thelwall M.","Scholars on soap boxes: Science communication and dissemination in TED videos",2013,"Journal of the American Society for Information Science and Technology",24,10.1002/asi.22764,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876670464&doi=10.1002%2fasi.22764&partnerID=40&md5=3052107ba8c759203695633dfb315cda","Online videos provide a novel, and often interactive, platform for the popularization of science. One successful collection is hosted on the TED (Technology, Entertainment, Design) website. This study uses a range of bibliometric (citation) and webometric (usage and bookmarking) indicators to examine TED videos in order to provide insights into the type and scope of their impact. The results suggest that TED Talks impact primarily the public sphere, with about three-quarters of a billion total views, rather than the academic realm. Differences were found among broad disciplinary areas, with art and design videos having generally lower levels of impact but science and technology videos generating otherwise average impact for TED. Many of the metrics were only loosely related, but there was a general consensus about the most popular videos as measured through views or comments on YouTube and the TED site. Moreover, most videos were found in at least one online syllabus and videos in online syllabi tended to be more viewed, discussed, and blogged. Less-liked videos generated more discussion, although this may be because they are more controversial. Science and technology videos presented by academics were more liked than those by nonacademics, showing that academics are not disadvantaged in this new media environment. © 2013 ASIS&T.",,"Art and designs; Bibliometric; Bookmarking; New media; Online video; Science and Technology; Science communications; YouTube; Artificial intelligence; Software engineering; Websites",Article,Scopus,2-s2.0-84876670464
"Amini F., Ghaderi P.","Hybridization of harmony search and ant colony optimization for optimal locating of structural dampers",2013,"Applied Soft Computing Journal",24,10.1016/j.asoc.2013.02.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875169142&doi=10.1016%2fj.asoc.2013.02.001&partnerID=40&md5=e580bff8000067824d7230826beeba40","This paper proposes a hybrid algorithm to find optimal locations for dampers within a structural system. In this approach Harmony Search is augmented by a probability mass function which utilizes two concepts borrowed from Ant Colony Optimization algorithm: pheromone and heuristic values. The former is a dynamic weight factor assigned to each solution component and the latter is a constant one defined based on modal analysis of the structural system. Provided numerical examples show that this hybrid search scheme improves the convergence rate such that the algorithm finds qualified solutions in fewer number of iterations. © 2013 Elsevier B.V.","Ant Colony Optimization; Discrete optimization; Harmony Search; Structural control","Ant Colony Optimization algorithms; Discrete optimization; Harmony search; Number of iterations; Probability mass function; Solution components; Structural control; Structural dampers; Ant colony optimization; Artificial intelligence; Modal analysis; Structural dynamics; Algorithms",Article,Scopus,2-s2.0-84875169142
"Biehl M., Bunte K., Schneider P.","Analysis of Flow Cytometry Data by Matrix Relevance Learning Vector Quantization",2013,"PLoS ONE",24,10.1371/journal.pone.0059401,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875060415&doi=10.1371%2fjournal.pone.0059401&partnerID=40&md5=f32f358411c44afc5db4e8daf97da477","Flow cytometry is a widely used technique for the analysis of cell populations in the study and diagnosis of human diseases. It yields large amounts of high-dimensional data, the analysis of which would clearly benefit from efficient computational approaches aiming at automated diagnosis and decision support. This article presents our analysis of flow cytometry data in the framework of the DREAM6/FlowCAP2 Molecular Classification of Acute Myeloid Leukemia (AML) Challenge, 2011. In the challenge, example data was provided for a set of 179 subjects, comprising healthy donors and 23 cases of AML. The participants were asked to provide predictions with respect to the condition of 180 patients in a test set. We extracted feature vectors from the data in terms of single marker statistics, including characteristic moments, median and interquartile range of the observed values. Subsequently, we applied Generalized Matrix Relevance Learning Vector Quantization (GMLVQ), a machine learning technique which extends standard LVQ by an adaptive distance measure. Our method achieved the best possible performance with respect to the diagnoses of test set patients. The extraction of features from the flow cytometry data is outlined in detail, the machine learning approach is discussed and classification results are presented. In addition, we illustrate how GMLVQ can provide deeper insight into the problem by allowing to infer the relevance of specific markers and features for the diagnosis. © 2013 Biehl et al.",,"acute granulocytic leukemia; analytic method; article; controlled study; decision support system; flow cytometry; Generalized Matrix Relevance Learning Vector Quantization; human; machine learning; major clinical study; prediction; standardization; Artificial Intelligence; Biological Markers; Computational Biology; Decision Support Techniques; Diagnosis, Computer-Assisted; Flow Cytometry; Humans; Leukemia, Myeloid, Acute",Article,Scopus,2-s2.0-84875060415
"Skupin A., Biberstine J.R., Börner K.","Visualizing the Topical Structure of the Medical Sciences: A Self-Organizing Map Approach",2013,"PLoS ONE",24,10.1371/journal.pone.0058779,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874852081&doi=10.1371%2fjournal.pone.0058779&partnerID=40&md5=87e029e759220809c69662129ec1d292","Background: We implement a high-resolution visualization of the medical knowledge domain using the self-organizing map (SOM) method, based on a corpus of over two million publications. While self-organizing maps have been used for document visualization for some time, (1) little is known about how to deal with truly large document collections in conjunction with a large number of SOM neurons, (2) post-training geometric and semiotic transformations of the SOM tend to be limited, and (3) no user studies have been conducted with domain experts to validate the utility and readability of the resulting visualizations. Our study makes key contributions to all of these issues. Methodology: Documents extracted from Medline and Scopus are analyzed on the basis of indexer-assigned MeSH terms. Initial dimensionality is reduced to include only the top 10% most frequent terms and the resulting document vectors are then used to train a large SOM consisting of over 75,000 neurons. The resulting two-dimensional model of the high-dimensional input space is then transformed into a large-format map by using geographic information system (GIS) techniques and cartographic design principles. This map is then annotated and evaluated by ten experts stemming from the biomedical and other domains. Conclusions: Study results demonstrate that it is possible to transform a very large document corpus into a map that is visually engaging and conceptually stimulating to subject experts from both inside and outside of the particular knowledge domain. The challenges of dealing with a truly large corpus come to the fore and require embracing parallelization and use of supercomputing resources to solve otherwise intractable computational tasks. Among the envisaged future efforts are the creation of a highly interactive interface and the elaboration of the notion of this map of medicine acting as a base map, onto which other knowledge artifacts could be overlaid. © 2013 Skupin et al.",,"adult; article; artificial neural network; controlled study; data analysis; document examination; female; geographic information system; human; information dissemination; male; medical documentation; medical literature; Medical Subject Headings; self organizing map; validation process; Artificial Intelligence; Biomedical Research; Medical Informatics; Models, Theoretical; Publications",Article,Scopus,2-s2.0-84874852081
"Gudyś A., Szcześniak M.W., Sikora M., Makałowska I.","HuntMi: An efficient and taxon-specific approach in pre-miRNA identification",2013,"BMC Bioinformatics",24,10.1186/1471-2105-14-83,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874465120&doi=10.1186%2f1471-2105-14-83&partnerID=40&md5=327d4a4d088dd14b0f675cdd61df6b41","Background: Machine learning techniques are known to be a powerful way of distinguishing microRNA hairpins from pseudo hairpins and have been applied in a number of recognised miRNA search tools. However, many current methods based on machine learning suffer from some drawbacks, including not addressing the class imbalance problem properly. It may lead to overlearning the majority class and/or incorrect assessment of classification performance. Moreover, those tools are effective for a narrow range of species, usually the model ones. This study aims at improving performance of miRNA classification procedure, extending its usability and reducing computational time.Results: We present HuntMi, a stand-alone machine learning miRNA classification tool. We developed a novel method of dealing with the class imbalance problem called ROC-select, which is based on thresholding score function produced by traditional classifiers. We also introduced new features to the data representation. Several classification algorithms in combination with ROC-select were tested and random forest was selected for the best balance between sensitivity and specificity. Reliable assessment of classification performance is guaranteed by using large, strongly imbalanced, and taxon-specific datasets in 10-fold cross-validation procedure. As a result, HuntMi achieves a considerably better performance than any other miRNA classification tool and can be applied in miRNA search experiments in a wide range of species.Conclusions: Our results indicate that HuntMi represents an effective and flexible tool for identification of new microRNAs in animals, plants and viruses. ROC-select strategy proves to be superior to other methods of dealing with class imbalance problem and can possibly be used in other machine learning classification tasks. The HuntMi software as well as datasets used in the research are freely available at http://lemur.amu.edu.pl/share/HuntMi/. © 2013 Gudyśet al.; licensee BioMed Central Ltd.","Genome analysis; Imbalanced learning; MicroRNA; Random forest","Classification performance; Genome analysis; Imbalanced Learning; Machine learning classification; Machine learning techniques; MicroRNAs; Random forests; Sensitivity and specificity; Classification (of information); Data processing; Decision trees; Learning systems; Viruses; RNA; Animalia; microRNA; RNA precursor; microRNA; RNA precursor; algorithm; article; artificial intelligence; classification; computer program; classification; Algorithms; Artificial Intelligence; MicroRNAs; RNA Precursors; Software; Algorithms; Artificial Intelligence; MicroRNAs; RNA Precursors; Software",Article,Scopus,2-s2.0-84874465120
"Buccafurri F., Foti V.D., Lax G., Nocera A., Ursino D.","Bridge analysis in a social internetworking scenario",2013,"Information Sciences",24,10.1016/j.ins.2012.10.021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871015676&doi=10.1016%2fj.ins.2012.10.021&partnerID=40&md5=379594ab507c9604e2de10185acd4bae","The rapid development of the number and the size of Online Social Networks (OSNs) makes the analysis of Social Internetworking Scenarios (SISs) extremely challenging. In a SIS, a user can join multiple OSNs and two users can interact with each other even though they joined different OSNs and did not know each other. While OSNs have been extensively studied in the last years, the most peculiar aspects of Social Internetworking Scenarios have not been yet investigated, especially from the Social Network Analysis perspective. Our paper tries to give a first important contribution in this field by deeply studying the core elements of a SIS, i.e.; bridges. Bridges are those users who joined more OSNs and allow users of different OSNs to cooperate. We investigate the main features of this category of users by means of a Social Network Analysis campaign. In particular, we define several specific crawling strategies and extract several samples from a SIS by applying each of them. The experimental results define a clear ""identikit"" of bridges allowing us to state a number of non-trivial conclusions about their role in a SIS. © 2012 Elsevier Inc. All rights reserved.","Bridge users; Crawling strategies; Social Internetworking Scenario; Social Network; Social Network Analysis","Bridge analysis; Core elements; Crawling strategy; Internetworking; Non-trivial; Online social networks; Social Network Analysis; Social Networks; Artificial intelligence; Software engineering; Social networking (online)",Article,Scopus,2-s2.0-84871015676
"Chi E.C., Zhou H., Chen G.K., Del Vecchyo D.O., Lange K.","Genotype imputation via matrix completion",2013,"Genome Research",24,10.1101/gr.145821.112,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874592681&doi=10.1101%2fgr.145821.112&partnerID=40&md5=839358465b1a49ace47337aa144a23cd","Most current genotype imputation methods are model-based and computationally intensive, taking days to impute one chromosome pair on 1000 people. We describe an efficient genotype imputation method based on matrix completion. Our matrix completion method is implemented in MATLAB and tested on real data from HapMap 3, simulated pedigree data, and simulated low-coverage sequencing data derived from the 1000 Genomes Project. Compared with leading imputation programs, the matrix completion algorithm embodied in our program MENDEL-IMPUTE achieves comparable imputation accuracy while reducing run times significantly. Implementation in a lower-level language such as Fortran or C is apt to further improve computational efficiency. © 2013, Published by Cold Spring Harbor Laboratory Press.",,"accuracy; algorithm; article; gene frequency; gene linkage disequilibrium; gene mutation; gene sequence; genetic association; genotype; homozygosity; parsimony analysis; priority journal; single nucleotide polymorphism; Algorithms; Artificial Intelligence; Computer Simulation; Genome, Human; Genotype; HapMap Project; Humans; Microarray Analysis; Models, Genetic; Polymorphism, Single Nucleotide; Software",Article,Scopus,2-s2.0-84874592681
"Yang J., Xiong N., Vasilakos A.V.","Two-stage enhancement scheme for low-quality fingerprint images by learning from the images",2013,"IEEE Transactions on Human-Machine Systems",24,10.1109/TSMCC.2011.2174049,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893057405&doi=10.1109%2fTSMCC.2011.2174049&partnerID=40&md5=0087fbb2ac4d9308ab559b6cd5e35792","Fingerprint authentication for content protection in the human-machine systems, cybernetics, and computational intelligence is very popular. Because of the complex input contexts, low-quality input fingerprint images always exist with cracks and scars, dry skin, or poor ridges and valley contrast ridges. Usually, fingerprint images are enhanced by one stage in either the spatial or the frequency domain. However, the enhanced performances are not satisfactory because of the complicated ridge structures that are affected by unusual input contexts. In this paper, we propose a novel and effective two-stage enhancement scheme in both the spatial domain and the frequency domain by learning from the underlying images. To remedy the ridge areas and enhance the contrast of the local ridges, we first enhance the fingerprint image in the spatial domain with a spatial ridge-compensation filter by learning from the images.With the help of the first step, the secondstage filter, i.e., a frequency bandpass filter that is separable in the radial- and angular-frequency domains, is employed. It is noted that the parameters of the bandpass filters are learnt from both the original image and the first-stage enhanced image instead of acquiring from the original image solely. It enhances the fingerprint image significantly because of the fast and sharp attenuation of the filter in both the radial and the angular-frequency domains. Experimental results show that our proposed algorithm is able to handle various input image contexts and achieves better results compared with some state-of-the-art algorithms over public databases, and to improve the performances of fingerprint-authentication systems. © 2012 IEEE.","Fingerprint enhancement; Learning; Privacy in biometrics systems; Two-stage filtering","Content Protection; Fingerprint authentication; Fingerprint enhancement; Fingerprint images; Frequency domains; Human-machine systems; Learning; State-of-the-art algorithms; Algorithms; Artificial intelligence; Authentication; Bandpass filters; Frequency domain analysis; Biometrics",Article,Scopus,2-s2.0-84893057405
"Chen X., Liu Y., Ralescu D.A.","Uncertain stock model with periodic dividends",2013,"Fuzzy Optimization and Decision Making",24,10.1007/s10700-012-9141-x,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873992732&doi=10.1007%2fs10700-012-9141-x&partnerID=40&md5=a9f72d3514d2bfcb096da74f26da9912","This paper presents an uncertain stock model with periodic dividends based on uncertainty theory. Some option pricing formulae related to the proposed model are investigated and several numerical examples are discussed to illustrate the related formula. © 2012 Springer Science+Business Media New York.","Finance; Option pricing; Uncertainty theory","Numerical example; Option pricing; Stock models; Uncertainty theory; Artificial intelligence; Finance; Decision making",Article,Scopus,2-s2.0-84873992732
"Folkerts E., Alexandrov A., Sachs K., Iosup A., Markl V., Tosun C.","Benchmarking in the cloud: What it should, can, and cannot be",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",24,10.1007/978-3-642-36727-4_12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873950780&doi=10.1007%2f978-3-642-36727-4_12&partnerID=40&md5=25a650f1a8cb68600ce28a291f9d7a23","With the increasing adoption of Cloud Computing, we observe an increasing need for Cloud Benchmarks, in order to assess the performance of Cloud infrastructures and software stacks, to assist with provisioning decisions for Cloud users, and to compare Cloud offerings. We understand our paper as one of the first systematic approaches to the topic of Cloud Benchmarks. Our driving principle is that Cloud Benchmarks must consider end-to-end performance and pricing, taking into account that services are delivered over the Internet. This requirement yields new challenges for benchmarking and requires us to revisit existing benchmarking practices in order to adopt them to the Cloud. © 2013 Springer-Verlag.",,"Cloud infrastructures; End-to-end performance; Software stacks; Artificial intelligence; Benchmarking",Conference Paper,Scopus,2-s2.0-84873950780
"Dobbie M.J., Dail D.","Robustness and sensitivity of weighting and aggregation in constructing composite indices",2013,"Ecological Indicators",24,10.1016/j.ecolind.2012.12.025,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873278813&doi=10.1016%2fj.ecolind.2012.12.025&partnerID=40&md5=b5d4000a7ebe5364f8c48b0bcd3e0a93","A composite index is a combination of various sources of information known as indicators, measured in or of a system in order to provide a summary of the system that is itself not directly measurable. For the index to be useful and meaningful, its construction requires careful consideration of several important aspects of the potentially disparate and multiple indicators that help convey its meaning. In general there are five key steps that should be considered when constructing a composite index, one of which is how the indicators should be weighted and aggregated to form the index. This step is critical in index construction, yet there seems to be no documented evidence about making objective weighting and aggregation choices in constructing indices so that they are robust. This sort of evidence would be particularly helpful for deciding whether any of the existing indices for assessing stream health would suffice for assessing the health of a given set of stream data or whether developing a new index is warranted. Thus we designed a simulation study to test the robustness and sensitivity of the weighting and aggregation choices in four existing stream health indices. The four indices mainly differed in their choices of standardization and weighting and aggregation techniques. The three main general conclusions about these existing approaches are the recommendation to use bootstrapping to approximate the distribution of the stream health index; the standardization technique employed should use all of the available indicators; and the use of reference (or pristine) sites as a standardization tool is not essential. Since the study is based on artificial data, the findings should be applicable and relevant to indices in other fields of study such as economics, social sciences, finance and medicine. © 2013 Published by Elsevier Ltd.","Environmental index; Environmental indicator; Environmental report card; Indicator integration; Multi-metrics; Stream health","Environmental index; Environmental indicators; Environmental report; Multi-metrics; Stream health; Health; Occupational risks; Standardization; Economics; artificial intelligence; bootstrapping; data processing; environmental indicator; sensitivity analysis; simulation; standardization",Article,Scopus,2-s2.0-84873278813
"Bao B.-K., Zhu G., Shen J., Yan S.","Robust image analysis with sparse representation on quantized visual features",2013,"IEEE Transactions on Image Processing",24,10.1109/TIP.2012.2219543,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873312902&doi=10.1109%2fTIP.2012.2219543&partnerID=40&md5=634ec24456dcedb75a595cd78a2fcff0","Recent techniques based on sparse representation (SR) have demonstrated promising performance in high-level visual recognition, exemplified by the highly accurate face recognition under occlusion and other sparse corruptions. Most research in this area has focused on classification algorithms using raw image pixels, and very few have been proposed to utilize the quantized visual features, such as the popular bag-of-words feature abstraction. In such cases, besides the inherent quantization errors, ambiguity associated with visual word assignment and misdetection of feature points, due to factors such as visual occlusions and noises, constitutes the major cause of dense corruptions of the quantized representation. The dense corruptions can jeopardize the decision process by distorting the patterns of the sparse reconstruction coefficients. In this paper, we aim to eliminate the corruptions and achieve robust image analysis with SR. Toward this goal, we introduce two transfer processes (ambiguity transfer and mis-detection transfer) to account for the two major sources of corruption as discussed. By reasonably assuming the rarity of the two kinds of distortion processes, we augment the original SR-based reconstruction objective with ℓbf0-norm regularization on the transfer terms to encourage sparsity and, hence, discourage dense distortion/transfer. Computationally, we relax the nonconvex ℓ\bf0-norm optimization into a convex ℓ\bf1-norm optimization problem, and employ the accelerated proximal gradient method to optimize the convergence provable updating procedure. Extensive experiments on four benchmark datasets, Caltech-101, Caltech-256, Corel-5k, and CMU pose, illumination, and expression, manifest the necessity of removing the quantization corruptions and the various advantages of the proposed framework. © 1992-2012 IEEE.","Image classification; quantized visual feature; sparse representation","Bag of words; Benchmark datasets; Classification algorithm; Decision process; Feature abstraction; Nonconvex; Optimization problems; Quantization errors; Raw images; Sparse reconstruction; Sparse representation; Transfer process; Visual feature; Visual recognition; Visual word; Face recognition; Gradient methods; Image analysis; Image classification; Optimization; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; image enhancement; methodology; reproducibility; sensitivity and specificity; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84873312902
"Molina J.-L., Pulido-Velázquez D., García-Aróstegui J.L., Pulido-Velázquez M.","Dynamic Bayesian Networks as a Decision Support tool for assessing Climate Change impacts on highly stressed groundwater systems",2013,"Journal of Hydrology",24,10.1016/j.jhydrol.2012.11.038,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872272963&doi=10.1016%2fj.jhydrol.2012.11.038&partnerID=40&md5=bc23dd5bf53cf5ef711370d01731a290","Bayesian Networks (BNs) are powerful tools for assessing and predicting consequences of water management scenarios and uncertain drivers like climate change, integrating available scientific knowledge with the interests of the multiple stakeholders. However, among their major limitations, the non-transient treatment of the cause-effect relationship stands out. A Decision Support System (DSS) based on Dynamic Bayesian Networks (DBNs) is proposed here aimed to palliate that limitation through time slicing technique. The DSS comprises several classes (Object-Oriented BN networks), especially designed for future 5. years length time steps (time slices), covering a total control period of 30. years (2070-2100). The DSS has been developed for assessing impacts generated by different Climate Change (CC) scenarios (generated from several Regional Climatic Models (RCMs) under two emission scenarios, A1B and A2) in an aquifer system (Serral-Salinas) affected by intensive groundwater use over the last 30. years. A calibrated continuous water balance model was used to generate hydrological CC scenarios, and then a groundwater flow model (MODFLOW) was employed in order to analyze the aquifer behavior under CC conditions. Results obtained from both models were used as input for the DSS, considering rainfall, aquifer recharge, variation of piezometric levels and temporal evolution of aquifer storage as the main hydrological components of the aquifer system. Results show the evolution of the aquifer storage for each future time step under different climate change conditions and under controlled water management interventions. This type of applications would allow establishing potential adaptation strategies for aquifer systems as the CC comes into effect. © 2012 Elsevier B.V.","Aquifers management; Climate Change; Decision Support Systems; Dynamic Bayesian Networks; Groundwater intensive use; Impacts","Adaptation strategies; Aquifer recharge; Aquifer storage; Aquifer systems; Bayesian Networks (bns); Cause-effect relationships; Change conditions; Climate change impact; Climatic models; Control periods; Decision support tools; Dynamic Bayesian networks; Emission scenario; Groundwater Flow Model; Groundwater system; Groundwater use; Impacts; Management scenarios; MODFLOW; Multiple stakeholders; Object oriented; ON dynamics; Piezometric levels; Scientific knowledge; Temporal evolution; Time slice; Time slicing; Time step; Water balance models; Aquifers; Artificial intelligence; Bayesian networks; Climate change; Decision support systems; Groundwater flow; Hydrogeology; Water management; Groundwater resources; adaptive management; aquifer; Bayesian analysis; calibration; climate change; climate effect; decision support system; flow modeling; groundwater resource; hydrological modeling; hydrological response; piezometer; rainfall; recharge; regional climate; stakeholder; temporal evolution; water budget; water management; water storage; water use",Article,Scopus,2-s2.0-84872272963
"Das D.K., Ghosh M., Pal M., Maiti A.K., Chakraborty C.","Machine learning approach for automated screening of malaria parasite using light microscopic images",2013,"Micron",24,10.1016/j.micron.2012.11.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872497956&doi=10.1016%2fj.micron.2012.11.002&partnerID=40&md5=4e6e2b65b4b21df250e129a2a7010ee2","The aim of this paper is to address the development of computer assisted malaria parasite characterization and classification using machine learning approach based on light microscopic images of peripheral blood smears. In doing this, microscopic image acquisition from stained slides, illumination correction and noise reduction, erythrocyte segmentation, feature extraction, feature selection and finally classification of different stages of malaria (Plasmodium vivax and Plasmodium falciparum) have been investigated. The erythrocytes are segmented using marker controlled watershed transformation and subsequently total ninety six features describing shape-size and texture of erythrocytes are extracted in respect to the parasitemia infected versus non-infected cells. Ninety four features are found to be statistically significant in discriminating six classes. Here a feature selection-cum-classification scheme has been devised by combining F-statistic, statistical learning techniques i.e., Bayesian learning and support vector machine (SVM) in order to provide the higher classification accuracy using best set of discriminating features. Results show that Bayesian approach provides the highest accuracy i.e., 84% for malaria classification by selecting 19 most significant features while SVM provides highest accuracy i.e., 83.5% with 9 most significant features. Finally, the performance of these two classifiers under feature selection framework has been compared toward malaria parasite classification. © 2012 Elsevier Ltd.","Bayesian classifier; Erythrocyte; Machine learning; Malaria parasite; Texture","Automated screening; Bayesian approaches; Bayesian classifier; Bayesian learning; Classification accuracy; Computer assisted; Erythrocyte; Illumination correction; Learning approach; Malaria parasite; Microscopic image; Parasitemia; Peripheral blood smears; Plasmodium falciparum; Plasmodium vivax; Selection framework; Statistical learning techniques; Watershed transformations; Automation; Bayesian networks; Diseases; Feature extraction; Learning systems; Noise abatement; Support vector machines; Textures; Blood; article; artificial intelligence; automation; blood; diagnostic procedure; evaluation; human; malaria falciparum; mass screening; methodology; microscopy; parasitemia; parasitology; Plasmodium vivax malaria; Artificial Intelligence; Automation; Blood; Clinical Laboratory Techniques; Humans; Malaria, Falciparum; Malaria, Vivax; Mass Screening; Microscopy; Parasitemia; Parasitology; Plasmodium falciparum; Plasmodium vivax",Article,Scopus,2-s2.0-84872497956
"González-Ferrer A., ten Teije A., Fdez-Olivares J., Milian K.","Automated generation of patient-tailored electronic care pathways by translating computer-interpretable guidelines into hierarchical task networks",2013,"Artificial Intelligence in Medicine",24,10.1016/j.artmed.2012.08.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875094102&doi=10.1016%2fj.artmed.2012.08.008&partnerID=40&md5=6957a631278c72d8ac13324a7be1cdaa","Objective: This paper describes a methodology which enables computer-aided support for the planning, visualization and execution of personalized patient treatments in a specific healthcare process, taking into account complex temporal constraints and the allocation of institutional resources. To this end, a translation from a time-annotated computer-interpretable guideline (CIG) model of a clinical protocol into a temporal hierarchical task network (HTN) planning domain is presented. Materials and methods: The proposed method uses a knowledge-driven reasoning process to translate knowledge previously described in a CIG into a corresponding HTN Planning and Scheduling domain, taking advantage of HTNs known ability to (i) dynamically cope with temporal and resource constraints, and (ii) automatically generate customized plans. The proposed method, focusing on the representation of temporal knowledge and based on the identification of workflow and temporal patterns in a CIG, makes it possible to automatically generate time-annotated and resource-based care pathways tailored to the needs of any possible patient profile. Results: The proposed translation is illustrated through a case study based on a 70 pages long clinical protocol to manage Hodgkin's disease, developed by the Spanish Society of Pediatric Oncology. We show that an HTN planning domain can be generated from the corresponding specification of the protocol in the Asbru language, providing a running example of this translation. Furthermore, the correctness of the translation is checked and also the management of ten different types of temporal patterns represented in the protocol. By interpreting the automatically generated domain with a state-of-art HTN planner, a time-annotated care pathway is automatically obtained, customized for the patient's and institutional needs. The generated care pathway can then be used by clinicians to plan and manage the patients long-term care. Conclusion: The described methodology makes it possible to automatically generate patient-tailored care pathways, leveraging an incremental knowledge-driven engineering process that starts from the expert knowledge of medical professionals. The presented approach makes the most of the strengths inherent in both CIG languages and HTN planning and scheduling techniques: for the former, knowledge acquisition and representation of the original clinical protocol, and for the latter, knowledge reasoning capabilities and an ability to deal with complex temporal and resource constraints. Moreover, the proposed approach provides immediate access to technologies such as business process management (BPM) tools, which are increasingly being used to support healthcare processes. © 2012 Elsevier B.V.","Clinical decision support systems; Clinical pathways; Hodgkin disease; Patient care planning; Pediatric oncology; Planning and scheduling; Practice guideline","Clinical decision support systems; Clinical pathways; Patient care; Pediatric oncology; Planning and scheduling; Practice guideline; Artificial intelligence; Complex networks; Decision support systems; Enterprise resource management; Flow measurement; Health care; Knowledge acquisition; Knowledge management; Oncology; Pediatrics; Translation (languages); Medical computing; article; computer interpretable guideline; computer network; hierarchical task network; Hodgkin disease; practice guideline; priority journal; Antineoplastic Protocols; Artificial Intelligence; Critical Pathways; Decision Making, Computer-Assisted; Hodgkin Disease; Humans; Long-Term Care; Patient Care Planning; Pediatrics; Practice Guidelines as Topic; Workflow",Article,Scopus,2-s2.0-84875094102
"Huang R.-H., Yang C.-L., Cheng W.-C.","Flexible job shop scheduling with due window - A two-pheromone ant colony approach",2013,"International Journal of Production Economics",24,10.1016/j.ijpe.2012.10.011,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871717100&doi=10.1016%2fj.ijpe.2012.10.011&partnerID=40&md5=04beffbe4d33360393a5c608ded8e1ba","Recently, the companies reduce the manufacturing costs and increase capacity efficiency in the competitive environment. Therefore, to balance workstation loading, the hybrid production system is necessary, so that, the flexible job shop system is the most common production system, and there are parallel machines in each workstation. In this study, the due window and the sequential dependent setup time of jobs are considered. To satisfy the customers' requirement, and reduce the cost of the storage costs at the same time, the sum of the earliness and tardiness costs is the objective. In this study, to improve the traditional ant colony system, we developed the two pheromone ant colony optimization (2PH-ACO) to approach the flexible job shop scheduling problem. Computational results indicate that 2PH-ACO performs better than ACO in terms of sum of earliness and tardiness time. © 2012 Elsevier B.V.","Ant colony optimization; Flexible job shop scheduling; Parallel machine; Time window; Two-pheromone ant colony optimization","Ant colonies; Ant Colony Optimization (ACO); Ant colony systems; Competitive environment; Computational results; Due-window; Earliness and tardiness; Flexible job shops; Flexible job-shop scheduling; Flexible job-shop scheduling problem; Hybrid production; Manufacturing cost; Parallel machine; Production system; Set-up time; Storage costs; Time windows; Artificial intelligence; Costs; Customer satisfaction; Loading; Production engineering; Algorithms",Article,Scopus,2-s2.0-84871717100
"Mukherjee I., Schapire R.E.","A theory of multiclass boosting",2013,"Journal of Machine Learning Research",24,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875126775&partnerID=40&md5=d3061928e8b2eb0d12bb5ffef84ec500","Boosting combines weak classifiers to form highly accurate predictors. Although the case of binary classification is well understood, in the multiclass setting, the ""correct"" requirements on the weak classifier, or the notion of the most efficient boosting algorithms are missing. In this paper, we create a broad and general framework, within which we make precise and identify the optimal requirements on the weak-classifier, as well as design the most effective, in a certain sense, boosting algorithms that assume such requirements. © 2013 Indraneel Mukherjee and Robert E. Schapire.","Boosting; Drifting games; Multiclass; Weak learning condition","Binary classification; Boosting; Boosting algorithm; Drifting games; Efficient boosting algorithms; Multiclass; Weak classifiers; Weak learning condition; Artificial intelligence; Software engineering",Article,Scopus,2-s2.0-84875126775
"van Westen G.J.P., Hendriks A., Wegner J.K., IJzerman A.P., van Vlijmen H.W.T., Bender A.","Significantly Improved HIV Inhibitor Efficacy Prediction Employing Proteochemometric Models Generated From Antivirogram Data",2013,"PLoS Computational Biology",24,10.1371/journal.pcbi.1002899,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874779833&doi=10.1371%2fjournal.pcbi.1002899&partnerID=40&md5=90bee317ff0e5e63ced2558b659dbbfb","Infection with HIV cannot currently be cured; however it can be controlled by combination treatment with multiple anti-retroviral drugs. Given different viral genotypes for virtually each individual patient, the question now arises which drug combination to use to achieve effective treatment. With the availability of viral genotypic data and clinical phenotypic data, it has become possible to create computational models able to predict an optimal treatment regimen for an individual patient. Current models are based only on sequence data derived from viral genotyping; chemical similarity of drugs is not considered. To explore the added value of chemical similarity inclusion we applied proteochemometric models, combining chemical and protein target properties in a single bioactivity model. Our dataset was a large scale clinical database of genotypic and phenotypic information (in total ca. 300,000 drug-mutant bioactivity data points, 4 (NNRTI), 8 (NRTI) or 9 (PI) drugs, and 10,700 (NNRTI) 10,500 (NRTI) or 27,000 (PI) mutants). Our models achieved a prediction error below 0.5 Log Fold Change. Moreover, when directly compared with previously published sequence data, derived models PCM performed better in resistance classification and prediction of Log Fold Change (0.76 log units versus 0.91). Furthermore, we were able to successfully confirm both known and identify previously unpublished, resistance-conferring mutations of HIV Reverse Transcriptase (e.g. K102Y, T216M) and HIV Protease (e.g. Q18N, N88G) from our dataset. Finally, we applied our models prospectively to the public HIV resistance database from Stanford University obtaining a correct resistance prediction rate of 84% on the full set (compared to 80% in previous work on a high quality subset). We conclude that proteochemometric models are able to accurately predict the phenotypic resistance based on genotypic data even for novel mutants and mixtures. Furthermore, we add an applicability domain to the prediction, informing the user about the reliability of predictions. © 2013 van Westen et al.",,"antiretrovirus agent; atazanavir; darunavir; Human immunodeficiency virus proteinase; nonnucleoside reverse transcriptase inhibitor; proteinase inhibitor; RNA directed DNA polymerase; RNA directed DNA polymerase inhibitor; tipranavir; antiviral resistance; article; chemical analysis; chemical database; chemometric analysis; drug classification; drug efficacy; drug mechanism; genetic database; genotype; Human immunodeficiency virus; Human immunodeficiency virus infection; infection control; monotherapy; nonhuman; nucleotide sequence; phenotype; prediction; process model; prospective study; protein analysis; protein targeting; proteochemometric model; reliability; structure activity relation; validation process; virus inhibition; virus mutation; Anti-HIV Agents; Artificial Intelligence; Computational Biology; Databases, Genetic; Drug Discovery; HIV; Models, Biological; Mutation; Phenotype; Reproducibility of Results",Article,Scopus,2-s2.0-84874779833
"Halford J.J., Schalkoff R.J., Zhou J., Benbadis S.R., Tatum W.O., Turner R.P., Sinha S.R., Fountain N.B., Arain A., Pritchard P.B., Kutluay E., Martz G., Edwards J.C., Waters C., Dean B.C.","Standardized database development for EEG epileptiform transient detection: EEGnet scoring system and machine learning analysis",2013,"Journal of Neuroscience Methods",24,10.1016/j.jneumeth.2012.11.005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870769794&doi=10.1016%2fj.jneumeth.2012.11.005&partnerID=40&md5=2d98701b38b2bcb2b23551fe37897575","The routine scalp electroencephalogram (rsEEG) is the most common clinical neurophysiology procedure. The most important role of rsEEG is to detect evidence of epilepsy, in the form of epileptiform transients (ETs), also known as spike or sharp wave discharges. Due to the wide variety of morphologies of ETs and their similarity to artifacts and waves that are part of the normal background activity, the task of ET detection is difficult and mistakes are frequently made. The development of reliable computerized detection of ETs in the EEG could assist physicians in interpreting rsEEGs. We report progress in developing a standardized database for testing and training ET detection algorithms. We describe a new version of our EEGnet software system for collecting expert opinion on EEG datasets, a completely web-browser based system. We report results of EEG scoring from a group of 11 board-certified academic clinical neurophysiologists who annotated 30-s excepts from rsEEG recordings from 100 different patients. The scorers had moderate inter-scorer reliability and low to moderate intra-scorer reliability. In order to measure the optimal size of this standardized rsEEG database, we used machine learning models to classify paroxysmal EEG activity in our database into ET and non-ET classes. Based on our results, it appears that our database will need to be larger than its current size. Also, our non-parametric classifier, an artificial neural network, performed better than our parametric Bayesian classifier. Of our feature sets, the wavelet feature set proved most useful for classification. © 2012.","Automated interpretation; Computerized interpretation; Electroencephalogram (EEG); Electroencephalography; Epileptiform transient; Spike detection","algorithm; article; artificial neural network; Bayesian learning; computer program; EEGnet scoring system; electroencephalography; epileptic discharge; machine learning; priority journal; scoring system; sensitivity and specificity; waveform; web browser; Algorithms; Artificial Intelligence; Electroencephalography; Epilepsy; Humans; Signal Processing, Computer-Assisted; Software",Article,Scopus,2-s2.0-84870769794
"Depolli M., Trobec R., Filipič B.","Asynchronous master-slave parallelization of differential evolution for multi-objective optimization",2013,"Evolutionary Computation",24,10.1162/EVCO_a_00076,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882371750&doi=10.1162%2fEVCO_a_00076&partnerID=40&md5=04b7d7365ccc9057829ed48c01ff4a51","In this paper, we present AMS-DEMO, an asynchronous master-slave implementation of DEMO, an evolutionary algorithm for multi-objective optimization. AMS-DEMO was designed for solving time-intensive problems efficiently on both homogeneous and heterogeneous parallel computer architectures. The algorithm is used as a test case for the asynchronous master-slave parallelization of multi-objective optimization that has not yet been thoroughly investigated. Selection lag is identified as the key property of the parallelization method, which explains how its behavior depends on the type of computer architecture and the number of processors. It is arrived at analytically and from the empirical results. AMS-DEMO is tested on a benchmark problem and a time-intensive industrial optimization problem, on homogeneous and heterogeneous parallel setups, providing performance results for the algorithm and an insight into the parallelization method. A comparison is also performed between AMS-DEMO and generational master-slave DEMO to demonstrate how the asynchronous parallelization method enhances the algorithm and what benefits it brings compared to the synchronous method. © 2013 by the Massachusetts Institute of Technology.","Differential evolution; Distributed computing; Evolutionary algorithms; Multi-objective optimization; Parallelization; Selection lag; Speedup","Computer architecture; Distributed computer systems; Multiobjective optimization; Bench-mark problems; Differential Evolution; Industrial optimization; Master-slave; Parallel computer architecture; Parallelizations; Selection lag; Speedup; Evolutionary algorithms; algorithm; article; artificial intelligence; computer simulation; temperature; time; Algorithms; Artificial Intelligence; Computer Simulation; Temperature; Time Factors",Article,Scopus,2-s2.0-84882371750
"Ding Y., He Y., Jiang J.","Multi-robot cooperation method based on the ant algorithm",2013,"2003 IEEE Swarm Intelligence Symposium, SIS 2003 - Proceedings",24,10.1109/SIS.2003.1202241,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942120776&doi=10.1109%2fSIS.2003.1202241&partnerID=40&md5=c8b3b5344214a2863b3e07308d366cd3","The ant algorithm is an optimization algorithm that is gained by observing the real ant colonies, and it is very useful in solving difficult optimization problems and distributed control problems. The algorithm is modeled on a key concept called ""stigmergy"" of the ant societies, which is used in our algorithm to design the cooperation of multi-robots. In an unknown environment, one of the most important problems in the multi-robot system is to decide how many robots are needed to complete a task. With the concept ""stigmergy"", the number of the robots cooperating on a task is decided according to the difficulty of the task. Given the definition of the ""task deadlock"", an adaptive attenuation factor to eliminate task deadlock is introduced in the cooperation algorithm. Simulations show the effectiveness of the algorithm. © 2003 IEEE.","Algorithm design and analysis; Ant colony optimization; Attenuation; Cities and towns; Design optimization; Educational institutions; Helium; Multirobot systems; Robot kinematics; System recovery","Ant colony optimization; Artificial intelligence; Distributed parameter control systems; Helium; Industrial robots; Machine design; Multipurpose robots; Optimization; Problem solving; Robots; Algorithm design and analysis; Attenuation; Cities and towns; Design optimization; Educational institutions; Multi-robot systems; Robot kinematics; System recovery; Algorithms",Conference Paper,Scopus,2-s2.0-84942120776
"Kruppa J., Schwarz A., Arminger G., Ziegler A.","Consumer credit risk: Individual probability estimates using machine learning",2013,"Expert Systems with Applications",24,10.1016/j.eswa.2013.03.019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878300417&doi=10.1016%2fj.eswa.2013.03.019&partnerID=40&md5=ddc60efb9a7e4427876601ce8dc19dbd","Consumer credit scoring is often considered a classification task where clients receive either a good or a bad credit status. Default probabilities provide more detailed information about the creditworthiness of consumers, and they are usually estimated by logistic regression. Here, we present a general framework for estimating individual consumer credit risks by use of machine learning methods. Since a probability is an expected value, all nonparametric regression approaches which are consistent for the mean are consistent for the probability estimation problem. Among others, random forests (RF), k-nearest neighbors (kNN), and bagged k-nearest neighbors (bNN) belong to this class of consistent nonparametric regression approaches. We apply the machine learning methods and an optimized logistic regression to a large dataset of complete payment histories of short-termed installment credits. We demonstrate probability estimation in Random Jungle, an RF package written in C++ with a generalized framework for fast tree growing, probability estimation, and classification. We also describe an algorithm for tuning the terminal node size for probability estimation. We demonstrate that regression RF outperforms the optimized logistic regression model, kNN, and bNN on the test data of the short-term installment credits. © 2013 Elsevier Ltd. All rights reserved.","Credit scoring; Logistic regression; Machine learning; Probability estimation; Probability machines; Random forest","Artificial intelligence; C++ (programming language); Decision trees; Motion compensation; Nearest neighbor search; Probability; Regression analysis; Risk assessment; Risk perception; Credit scoring; K nearest neighbor (KNN); Logistic Regression modeling; Logistic regressions; Machine learning methods; Non-parametric regression; Probability estimation; Random forests; Learning systems",Article,Scopus,2-s2.0-84878300417
"Ciesielski K.C., Miranda P.A.V., Falcão A.X., Udupa J.K.","Joint graph cut and relative fuzzy connectedness image segmentation algorithm",2013,"Medical Image Analysis",23,10.1016/j.media.2013.06.006,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880580617&doi=10.1016%2fj.media.2013.06.006&partnerID=40&md5=cf1be905ce3782e4d347228c59faf863","We introduce an image segmentation algorithm, called GCsummax, which combines, in novel manner, the strengths of two popular algorithms: Relative Fuzzy Connectedness (RFC) and (standard) Graph Cut (GC). We show, both theoretically and experimentally, that GCsummax preserves robustness of RFC with respect to the seed choice (thus, avoiding ""shrinking problem"" of GC), while keeping GC's stronger control over the problem of ""leaking though poorly defined boundary segments."" The analysis of GCsummax is greatly facilitated by our recent theoretical results that RFC can be described within the framework of Generalized GC (GGC) segmentation algorithms. In our implementation of GCsummax we use, as a subroutine, a version of RFC algorithm (based on Image Forest Transform) that runs (provably) in linear time with respect to the image size. This results in GCsummax running in a time close to linear. Experimental comparison of GCsummax to GC, an iterative version of RFC (IRFC), and power watershed (PW), based on a variety medical and non-medical images, indicates superior accuracy performance of GCsummax over these other methods, resulting in a rank ordering of GCsummax>. PW. ~. IRFC. >. GC. © 2013 Elsevier B.V.","Fuzzy connectedness; Graph cut; Image segmentation; Robustness","Boundary segments; Experimental comparison; Fuzzy connectedness; Graph cut; Image segmentation algorithm; Rank ordering; Relative fuzzy connectedness; Segmentation algorithms; Algorithms; Fuzzy systems; Graphic methods; Iterative methods; Mathematical transformations; Robustness (control systems); Image segmentation; accuracy; algorithm; article; comparative study; fuzzy system; graph cut; image processing; priority journal; relative fuzzy connectedness; watershed; Fuzzy connectedness; Graph cut; Image segmentation; Robustness; Algorithms; Artificial Intelligence; Fuzzy Logic; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-84880580617
"Kharlamov E., Jiménez-Ruiz E., Zheleznyakov D., Bilidas D., Giese M., Haase P., Horrocks I., Kllapi H., Koubarakis M., Özçep Ö., Rodríguez-Muro M., Rosati R., Schmidt M., Schlatte R., Soylu A., Waaler A.","Optique: Towards OBDA systems for industry",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",23,10.1007/978-3-642-41242-4_11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892667646&doi=10.1007%2f978-3-642-41242-4_11&partnerID=40&md5=e394790f9c9825433e07df51a08207e4","The recently started EU FP7-funded project Optique will develop an end-to-end OBDA system providing scalable end-user access to industrial Big Data stores. This paper presents an initial architectural specification for the Optique system along with the individual system components. © Springer-Verlag 2013.","Big data; OBDA; Ontologies; OWL 2; System architecture","Architectural specifications; Big datum; End users; Individual systems; OBDA; OWL 2; System architectures; Computer science; Computers; Ontology; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84892667646
"Truong N.C., McInerney J., Tran-Thanh L., Costanza E., Ramchurn S.D.","Forecasting multi-appliance usage for smart home energy management",2013,"IJCAI International Joint Conference on Artificial Intelligence",23,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063470&partnerID=40&md5=62d2d5a6aae333deb29836f15de6793f","We address the problem of forecasting the usage of multiple electrical appliances by domestic users, with the aim of providing suggestions about the best time to run appliances in order to reduce carbon emissions and save money (assuming time-ofuse pricing), while minimising the impact on the users' daily habits. An important challenge related to this problem is the modelling the everyday routine of the consumers and of the inter-dependencies between the use of different appliances. Given this, we develop an important building block of future home energy management systems: a prediction algorithm, based on a graphical model, that captures the everyday habits and the inter-dependency between appliances by exploiting their periodic features. We demonstrate through extensive empirical evaluations on real-world data from a prominent database that our approach outperforms existing methods by up to 47%.",,"Building blockes; Carbon emissions; Electrical appliances; Empirical evaluations; GraphicaL model; Home energy management systems; Inter-dependencies; Prediction algorithms; Algorithms; Artificial intelligence; Automation; Energy management; Intelligent buildings; Domestic appliances",Conference Paper,Scopus,2-s2.0-84896063470
"Aranha D.F., Fuentes-Castañeda L., Knapp E., Menezes A., Rodríguez-Henríquez F.","Implementing pairings at the 192-bit security level",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",23,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893086003&partnerID=40&md5=da1b0703f84a48b2e4d34c1d97c08cf7","We implement asymmetric pairings derived from Kachisa- Schaefer-Scott (KSS), Barreto-Naehrig (BN), and Barreto-Lynn-Scott (BLS) elliptic curves at the 192-bit security level. Somewhat surprisingly, we find pairings derived from BLS curves with embedding degree 12 to be the fastest for our serial as well as our parallel implementations. Our serial implementations provide a factor-3 speedup over the previous state-of-the-art, demonstrating that pairing computation at the 192-bit security level is not as expensive as previously thought. We also present a general framework for deriving a Weil-type pairing that is well-suited for computing a single pairing on a multi-processor machine © Springer-Verlag Berlin Heidelberg 2013.",,"Elliptic curve; Multi-processors; Parallel implementations; Security level; Artificial intelligence; Computer science; Computers; Cryptography",Conference Paper,Scopus,2-s2.0-84893086003
"Ding Z., Qiu X., Zhang Q., Huang X.","Learning topical translation model for microblog hashtag suggestion",2013,"IJCAI International Joint Conference on Artificial Intelligence",23,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063112&partnerID=40&md5=a54d0c6331e5def0dc4dbcca05d2bfba","Hashtags can be viewed as an indication to the context of the tweet or as the core idea expressed in the tweet. They provide valuable information for many applications, such as information retrieval, opinion mining, text classification, and so on. However, only a small number of microblogs are manually tagged. To address this problem, in this work, we propose a topical translation model for microblog hashtag suggestion. We assume that the content and hashtags of the tweet are talking about the same themes but written in different languages. Under the assumption, hashtag suggestion is modeled as a translation process from content to hashtags. Moreover, in order to cover the topic of tweets, the proposed model regards the translation probability to be topic-specific. It uses topic-specific word trigger to bridge the vocabulary gap between the words in tweets and hashtags, and discovers the topics of tweets by a topic model designed for microblogs. Experimental results on the dataset crawled from real world microblogging service demonstrate that the proposed method outperforms state-of-the-art methods.",,"Micro-blogging services; Microblogs; Opinion mining; State-of-the-art methods; Text classification; Topic Modeling; Translation models; Translation process; Artificial intelligence; Bridges; Classification (of information); Text processing; World Wide Web",Conference Paper,Scopus,2-s2.0-84896063112
"Zhong S.-H., Liu Y., Ren F., Zhang J., Ren T.","Video saliency detection via dynamic consistent spatio-temporal attention modelling",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",23,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893378066&partnerID=40&md5=78e95b5cad352f78f7a124237d24cc97","Human vision system actively seeks salient regions and movements in video sequences to reduce the search effort. Modeling computational visual saliency map provides important information for semantic understanding in many real world applications. In this paper, we propose a novel video saliency detection model for detecting the attended regions that correspond to both interesting objects and dominant motions in video sequences. In spatial saliency map, we inherit the classical bottom-up spatial saliency map. In temporal saliency map, a novel optical flow model is proposed based on the dynamic consistency of motion. The spatial and the temporal saliency maps are constructed and further fused together to create a novel attention model. The proposed attention model is evaluated on three video datasets. Empirical validations demonstrate the salient regions detected by our dynamic consistent saliency map highlight the interesting objects effectively and efficiency. More importantly, the automatically video attended regions detected by proposed attention model are consistent with the ground truth saliency maps of eye movement data. © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Dynamic consistencies; Empirical validation; Eye movement datum; Human vision systems; Salient regions; Semantic understanding; Spatial saliencies; Video saliencies; Artificial intelligence; Computer vision; Eye movements; Object recognition; Semantics; Video recording; Image segmentation",Conference Paper,Scopus,2-s2.0-84893378066
"Bienvenu M., Ortiz M., Šimkus M., Xiao G.","Tractable queries for lightweight description logics",2013,"IJCAI International Joint Conference on Artificial Intelligence",23,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062717&partnerID=40&md5=6184e59b6491c9a70ea0b392ee725757","It is a classic result in database theory that conjunctive query (CQ) answering, which is NP-complete in general, is feasible in polynomial time when restricted to acyclic queries. Subsequent results identified more general structural properties of CQs (like bounded tree-width) which ensure tractable query evaluation. In this paper, we lift these tractability results to knowledge bases formulated in the lightweight description logics DL-Lite and ELH. The proof exploits known properties of query matches in these logics and involves a query-dependent modification of the data. To obtain a more practical approach, we propose a concrete polynomial-time algorithm for answering acyclic CQs based on rewriting queries into datalog programs. A preliminary evaluation suggests the interest of our approach for handling large acyclic CQs.",,"Conjunctive queries; Data base theory; Datalog programs; Description logic; Knowledge basis; Polynomial-time; Polynomial-time algorithms; Query evaluation; Algorithms; Artificial intelligence; Formal languages; Polynomial approximation; Query languages; Query processing; Data description",Conference Paper,Scopus,2-s2.0-84896062717
"Kim M., Wu G., Li W., Wang L., Son Y.-D., Cho Z.-H., Shen D.","Automatic hippocampus segmentation of 7.0Tesla MR images by combining multiple atlases and auto-context models",2013,"NeuroImage",23,10.1016/j.neuroimage.2013.06.006,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880956855&doi=10.1016%2fj.neuroimage.2013.06.006&partnerID=40&md5=2b140ca631cf1572402705243e5091cf","In many neuroscience and clinical studies, accurate measurement of hippocampus is very important to reveal the inter-subject anatomical differences or the subtle intra-subject longitudinal changes due to aging or dementia. Although many automatic segmentation methods have been developed, their performances are still challenged by the poor image contrast of hippocampus in the MR images acquired especially from 1.5 or 3.0Tesla (T) scanners. With the recent advance of imaging technology, 7.0T scanner provides much higher image contrast and resolution for hippocampus study. However, the previous methods developed for segmentation of hippocampus from 1.5T or 3.0T images do not work for the 7.0T images, due to different levels of imaging contrast and texture information. In this paper, we present a learning-based algorithm for automatic segmentation of hippocampi from 7.0T images, by taking advantages of the state-of-the-art multi-atlas framework and also the auto-context model (ACM). Specifically, ACM is performed in each atlas domain to iteratively construct sequences of location-adaptive classifiers by integrating both image appearance and local context features. Due to the plenty texture information in 7.0T images, more advanced texture features are also extracted and incorporated into the ACM during the training stage. Then, under the multi-atlas segmentation framework, multiple sequences of ACM-based classifiers are trained for all atlases to incorporate the anatomical variability. In the application stage, for a new image, its hippocampus segmentation can be achieved by fusing the labeling results from all atlases, each of which is obtained by applying the atlas-specific ACM-based classifiers. Experimental results on twenty 7.0T images with the voxel size of 0.35×0.35×0.35mm3 show very promising hippocampus segmentations (in terms of Dice overlap ratio 89.1±0.020), indicating high applicability for the future clinical and neuroscience studies. © 2013 Elsevier Inc.","7.0T MRI; Auto-context model; Automatic hippocampus segmentation; Label fusion; Multiple atlases based segmentation","adult; article; automatic segmentation algorithm; classifier; clinical article; contrast enhancement; controlled study; experimental study; female; hippocampus; human; image display; image processing; image quality; learning algorithm; male; neuroscience; nuclear magnetic resonance imaging; nuclear magnetic resonance scanner; priority journal; statistical model; algorithm; anatomy and histology; artificial intelligence; audiovisual equipment; automated pattern recognition; biological model; computer assisted diagnosis; computer simulation; hippocampus; image enhancement; image subtraction; nuclear magnetic resonance imaging; procedures; reproducibility; sensitivity and specificity; three dimensional imaging; Adult; Algorithms; Artificial Intelligence; Computer Simulation; Female; Hippocampus; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Male; Models, Anatomic; Models, Neurological; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-84880956855
"Gebser M., Kaufmann B., Schaub T.","Advanced conflict-driven disjunctive answer set solving",2013,"IJCAI International Joint Conference on Artificial Intelligence",23,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063922&partnerID=40&md5=356bb50cb197f36a30e7ef36b0129f5f","We introduce a new approach to disjunctive ASP solving that aims at an equitable interplay between ""generating"" and ""testing"" solver units. To this end, we develop novel characterizations of answer sets and unfounded sets allowing for a bidirectional dynamic information exchange between solver units for orthogonal tasks. This results in the new multithreaded disjunctive ASP solver claspD-2, greatly improving the performance of existing systems.",,"Answer set; Answer Set Solving; Dynamic information; Existing systems; Multithreaded; New approaches; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84896063922
"Hadjinikolis C., Siantos Y., Modgil S., Black E., McBurney P.","Opponent modelling in persuasion dialogues",2013,"IJCAI International Joint Conference on Artificial Intelligence",23,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063744&partnerID=40&md5=2f276ec5b0f30b4675518f406524bc6a","A strategy is used by a participant in a persuasion dialogue to select locutions most likely to achieve its objective of persuading its opponent. Such strategies often assume that the participant has a model of its opponents, which may be constructed on the basis of a participant's accumulated dialogue experience. However in most cases the fact that an agent's experience may encode additional information which if appropriately used could increase a strategy's efficiency, is neglected. In this work, we rely on an agent's experience to define a mechanism for augmenting an opponent model with information likely to be dialectally related to information already contained in it. Precise computation of this likelihood is exponential in the volume of related information. We thus describe and evaluate an approximate approach for computing these likelihoods based on Monte-Carlo simulation.",,"Monte-Carlo simulations; Most likely; Opponent modeling; Artificial intelligence; Intelligent systems",Conference Paper,Scopus,2-s2.0-84896063744
"Andrikopoulos V., Song Z., Leymann F.","Supporting the migration of applications to the cloud through a decision support system",2013,"IEEE International Conference on Cloud Computing, CLOUD",23,10.1109/CLOUD.2013.128,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897724135&doi=10.1109%2fCLOUD.2013.128&partnerID=40&md5=babbf371bf09f1964ab80f733cf94445","The motivation for this work is the necessity to be able to select an appropriate Cloud service provider offering for the migration of existing applications, based on cost minimization. While service providers offer pricing information publicly, and online tools allow for the calculation of cost for various Cloud offerings, the selection of which offering fits better the application requirements is left to application developers. For this purpose, this work proposes a migration decision support system that incorporates both offering matching and cost calculation, combining features from various approaches in the State of the Art. The proposed approach is then evaluated against existing tools. © 2013 IEEE.","Cloud migration; cost calculation; decision support; provider selection; usage patterns","Cloud migrations; Cost calculation; Decision supports; provider selection; Usage patterns; Artificial intelligence; Cloud computing; Decision support systems; Tools; Costs",Conference Paper,Scopus,2-s2.0-84897724135
"Xiang B., Liu Q., Chen E., Xiong H., Zheng Y., Yang Y.","PageRank with priors: An influence propagation perspective",2013,"IJCAI International Joint Conference on Artificial Intelligence",23,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896058955&partnerID=40&md5=d07cd740cf57d4bbddb75187985a0356","Recent years have witnessed increased interests in measuring authority and modelling influence in social networks. For a long time, PageRank has been widely used for authority computation and has also been adopted as a solid baseline for evaluating social influence related applications. However, the connection between authority measurement and influence modelling is not clearly established. To this end, in this paper, we provide a focused study on understanding of PageRank as well as the relationship between PageRank and social influence analysis. Along this line, we first propose a linear social influence model and reveal that this model is essentially PageRank with prior. Also, we show that the authority computation by PageRank can be enhanced with more generalized priors. Moreover, to deal with the computational challenge of PageRank with general priors, we provide an upper bound for top authoritative nodes identification. Finally, the experimental results on the scientific collaboration network validate the effectiveness of the proposed social influence model.",,"Computational challenges; PageRank; Scientific collaboration networks; Social influence; Social influence model; Upper Bound; Artificial intelligence; Economic and social effects",Conference Paper,Scopus,2-s2.0-84896058955
"Bachrach Y., Kohli P., Kolmogorov V., Zadimoghaddam M.","Optimal coalition structure generation in cooperative graph games",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",23,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893391177&partnerID=40&md5=d99208106de13cd510bb253e7e78a04c","Representation languages for coalitional games are a key research area in algorithmic game theory. There is an inherent tradeoff between how general a language is, allowing it to capture more elaborate games, and how hard it is computationally to optimize and solve such games. One prominent such language is the simple yet expressive Weighted Graph Games (WGGs) representation (Deng and Papadimitriou 1994), which maintains knowledge about synergies between agents in the form of an edge weighted graph. We consider the problem of finding the optimal coalition structure in WGGs. The agents in such games are vertices in a graph, and the value of a coalition is the sum of the weights of the edges present between coalition members. The optimal coalition structure is a partition of the agents to coalitions, that maximizes the sum of utilities obtained by the coalitions. We show that finding the optimal coalition structure is not only hard for general graphs, but is also intractable for restricted families such as planar graphs which are amenable for many other combinatorial problems. We then provide algorithms with constant factor approximations for planar, minorfree and bounded degree graphs. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Algorithmic Game Theory; Bounded degree graphs; Coalitional game; Combinatorial problem; Constant factor approximation; Edge-weighted graph; Optimal coalition; Representation languages; Algorithmic languages; Approximation algorithms; Artificial intelligence; Computer games; Graphic methods; Graph theory",Conference Paper,Scopus,2-s2.0-84893391177
"Akhtar N., Johri P., Khan S.","Enhancing the security and quality of lsb based image steganography",2013,"Proceedings - 5th International Conference on Computational Intelligence and Communication Networks, CICN 2013",23,10.1109/CICN.2013.85,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892666616&doi=10.1109%2fCICN.2013.85&partnerID=40&md5=41a2c0373000d254138bc33a265d605b","This work is concerned with implementing Steganography for images, with an improvement in both security and image quality. The one that is implemented here is a variation of plain LSB (Least Significant Bit) algorithm. The stego-image quality is improved by using bit-inversion technique. In this technique, certain least significant bits of cover image are inverted after LSB steganography that co-occur with some pattern of other bits and that reduces the number of modified LSBs. Thus, less number of least significant bits of cover image is altered in comparison to plain LSB method, improving the PSNR of stegoimage. By storing the bit patterns for which LSBs are inverted, message image can be obtained correctly. To improve the robustness of steganography, RC4 algorithm has been used to achieve the randomization in hiding message image bits into cover image pixels instead of storing them sequentially. This process randomly disperses the bits of the message in the cover image and thus, making it harder for unauthorized people to extract the original message. The proposed method shows good enhancement to Least Significant Bit technique in consideration to security as well as image quality. © 2013 IEEE.","image quality; LSB; RC4; security; steganography","Bit patterns; Image steganography; Least significant bits; LSB; LSB steganography; RC4; RC4 algorithm; security; Algorithms; Artificial intelligence; Image quality; Steganography",Conference Paper,Scopus,2-s2.0-84892666616
"Heizmann M., Hoenicke J., Leike J., Podelski A.","Linear ranking for linear lasso programs",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",23,10.1007/978-3-319-02444-8_26,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887481847&doi=10.1007%2f978-3-319-02444-8_26&partnerID=40&md5=3346d19e5c861d15b9f6676d7412fca8","The general setting of this work is the constraint-based synthesis of termination arguments. We consider a restricted class of programs called lasso programs. The termination argument for a lasso program is a pair of a ranking function and an invariant. We present the - to the best of our knowledge - first method to synthesize termination arguments for lasso programs that uses linear arithmetic.We prove a completeness theorem. The completeness theorem establishes that, even though we use only linear (as opposed to non-linear) constraint solving, we are able to compute termination arguments in several interesting cases. The key to our method lies in a constraint transformation that replaces a disjunction by a sum. © 2013 Springer International Publishing.",,"Completeness theorems; Constraint Solving; Constraint transformation; Constraint-based; Linear ranking; Ranking functions; Termination arguments; Artificial intelligence; Computer science",Conference Paper,Scopus,2-s2.0-84887481847
"Dobai R., Sekanina L.","Towards evolvable systems based on the Xilinx Zynq platform",2013,"Proceedings of the 2013 IEEE International Conference on Evolvable Systems, ICES 2013 - 2013 IEEE Symposium Series on Computational Intelligence, SSCI 2013",23,10.1109/ICES.2013.6613287,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885409678&doi=10.1109%2fICES.2013.6613287&partnerID=40&md5=d8c04ec891f50e3033a6c0bb10b3598e","Field programmable gate arrays (FPGAs) are considered as a good platform for digital evolvable hardware systems. Researchers introduced virtual reconfigurable circuits as the response to the insufficient support of partial reconfiguration in early FPGAs. Later, the features of FPGAs allowed the designers to develop evolvable systems fully exploiting native reconfiguration infrastructures. Xilinx recently introduced a new platform called Zynq-7000 all programmable (AP) system-on-chip (SoC) which has the potential to become the next revolutionary step in evolvable hardware design. The paper analyzes Zynq-7000 AP SoC from the perspective of an evolvable hardware designer. Several scenarios are described of how to implement evolvable systems on a developmental board equipped with this programmable SoC. These scenarios are evaluated in terms of area overhead, execution time, reconfiguration time and throughput. The resulting observations should be useful for those who are going to develop real-world evolvable systems on the Zynq-7000 AP SoC platform. © 2013 IEEE.",,"Area overhead; Evolvable hardware; Evolvable hardware system; Evolvable systems; Partial reconfiguration; Reconfiguration time; System-On-Chip; Virtual reconfigurable circuit; Application specific integrated circuits; Artificial intelligence; Hardware; Reconfigurable hardware; Field programmable gate arrays (FPGA)",Conference Paper,Scopus,2-s2.0-84885409678
"Tourassi G., Voisin S., Paquit V., Krupinski E.","Investigating the link between radiologists' gaze, diagnostic decision, and image content",2013,"Journal of the American Medical Informatics Association",23,10.1136/amiajnl-2012-001503,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886297620&doi=10.1136%2famiajnl-2012-001503&partnerID=40&md5=f2753fcedb2306b94c5e7c799a0608f4","Objective: To investigate machine learning for linking image content, human perception, cognition, and error in the diagnostic interpretation of mammograms. Methods: Gaze data and diagnostic decisions were collected from three breast imaging radiologists and three radiology residents who reviewed 20 screening mammograms while wearing a head-mounted eyetracker. Image analysis was performed in mammographic regions that attracted radiologists' attention and in all abnormal regions. Machine learning algorithms were investigated to develop predictive models that link: (i) image content with gaze, (ii) image content and gaze with cognition, and (iii) image content, gaze, and cognition with diagnostic error. Both group-based and individualized models were explored. Results: By pooling the data from all readers, machine learning produced highly accurate predictive models linking image content, gaze, and cognition. Potential linking of those with diagnostic error was also supported to some extent. Merging readers' gaze metrics and cognitive opinions with computer-extracted image features identified 59% of the readers' diagnostic errors while confirming 97.3% of their correct diagnoses. The readers' individual perceptual and cognitive behaviors could be adequately predicted by modeling the behavior of others. However, personalized tuning was in many cases beneficial for capturing more accurately individual behavior. Conclusions: There is clearly an interaction between radiologists' gaze, diagnostic decision, and image content which can be modeled with machine learning algorithms.",,"algorithm; article; breast tumor; clinical decision making; cognition; diagnostic accuracy; diagnostic error; eye tracking; female; human; image analysis; information processing device; machine learning; mammography; ophthalmological diagnostic device; radiologist; resident; eye-tracking; machine learning; mammography; user modeling; Artificial Intelligence; Breast Neoplasms; Cognition; Diagnostic Errors; Eye Movements; Humans; Image Processing, Computer-Assisted; Mammography; Pilot Projects; Radiology; Visual Perception",Article,Scopus,2-s2.0-84886297620
"Garg A., Tai K.","Selection of a robust experimental design for the effective modeling of nonlinear systems using Genetic Programming",2013,"Proceedings of the 2013 IEEE Symposium on Computational Intelligence and Data Mining, CIDM 2013 - 2013 IEEE Symposium Series on Computational Intelligence, SSCI 2013",23,10.1109/CIDM.2013.6597249,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885571970&doi=10.1109%2fCIDM.2013.6597249&partnerID=40&md5=00768f222ba208ecaf7464092ba002e6","The evolutionary approach of Genetic Programming (GP) has been applied extensively to model various non-linear systems. The distinct advantage of using GP is that prior assumptions for the selection of a model structure are not required. The GP automatically evolves the optimal model structure and its parameters that best describe the system characteristics. However, the evolution of an optimal model structure is highly dependent on the experimental designs used to sample the problem (system) domain and capture its characteristics. The literature reveals that very few researchers have studied the effect of various experimental designs on the performance of GP models and therefore the optimum choice of an experimental design is still unknown. This paper studies the effect of various experimental designs on the performance of GP models on two non-linear test functions. The objective of the paper is to identify the most robust (best) experimental design for effective modeling of non-linear test functions using GP. The analysis reveals that for the test function 1, the experimental design that gives best performance of GP models is response surface faced design and for test function 2, the best experimental design is 5-level full factorial design. Thus, the result concludes that the selection of the robust experimental design is a crucial preprocessing step for the effective modeling of non-linear systems using GP. © 2013 IEEE.","experimental designs; full factorial design; genetic programming; latin hypercube sampling; response surface design","Evolutionary approach; Full factorial design; Latin hypercube sampling; Optimal model structures; Pre-processing step; Response surface designs; Robust experimental designs; System characteristics; Artificial intelligence; Data mining; Genetic programming; Nonlinear systems; Statistics; Surface properties; Design of experiments",Conference Paper,Scopus,2-s2.0-84885571970
"Oliveira N., Cortez P., Areal N.","On the predictability of stock market behavior using StockTwits sentiment and posting volume",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",23,10.1007/978-3-642-40669-0_31,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884724722&doi=10.1007%2f978-3-642-40669-0_31&partnerID=40&md5=15dad7c04557eb76a4a2551f5a62cedd","In this study, we explored data from StockTwits, a microblogging platform exclusively dedicated to the stock market. We produced several indicators and analyzed their value when predicting three market variables: returns, volatility and trading volume. For six major stocks, we measured posting volume and sentiment indicators. We advance on the previous studies on this subject by considering a large time period, using a robust forecasting exercise and performing a statistical test of forecasting ability. In contrast with previous studies, we find no evidence of return predictability using sentiment indicators, and of information content of posting volume for forecasting volatility. However, there is evidence that posting volume can improve the forecasts of trading volume, which is useful for measuring stock liquidity (e.g. assets easily sold). © 2013 Springer-Verlag.","Microblogging Data; Regression; Returns; Trading Volume; Volatility","Microblogging; Regression; Returns; Trading volumes; Volatility; Artificial intelligence; Financial data processing; Forecasting; Commerce",Conference Paper,Scopus,2-s2.0-84884724722
"Woźniak M., Marszałek Z., Gabryel M., Nowicki R.K.","Modified merge sort algorithm for large scale data sets",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",23,10.1007/978-3-642-38610-7_56,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884398708&doi=10.1007%2f978-3-642-38610-7_56&partnerID=40&md5=5f8096a54a064b7579ac219166f08ccc","Sorting algorithms find their application in many fields. One of their main uses is to organize databases. Classical applications of sorting algorithms often can not cope satisfactorily with large data sets or with unfavorable poses of sorted strings. Typically, in such situations, we try to use other methods or apply sorting process to reshuffled input data. Unfortunately, this approach complicates sorting process and often results in significant prolongation of the time. In this paper, the authors examined an algorithm dedicated to the problem of sorting large scale data sets. In the literature, there are no studies of such examples. These studies will allow to describe the properties of sorting methods for large scale data sets. Performed tests have shown superior performance of the examined algorithm, especially for large scale data sets. Changes sped up sorting of data with any arrangement of the input elements. © 2013 Springer-Verlag.","analysis of computer algorithms; computer algorithm; data mining; data sorting","Data sorting; Input datas; Large datasets; Large scale data sets; Merge-sort algorithm; Sorting algorithm; Sorting method; Sorting process; Algorithms; Artificial intelligence; Data mining; Soft computing; Sorting",Conference Paper,Scopus,2-s2.0-84884398708
"Li L., Zhang D., Xie L., Yu B., Lin F.","A condition assessment method of power transformers based on association rules and variable weight coefficients",2013,"Zhongguo Dianji Gongcheng Xuebao/Proceedings of the Chinese Society of Electrical Engineering",23,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885085454&partnerID=40&md5=aebf6b5d3ab2dd33b21832c3f999e632","Condition assessment for power transformer requires not only integrating the known artificial intelligence technologies, but also processing the interrelation of the tested status parameters. According to the association rule of information data and the variable weight synthesizing theory of factor spaces, a condition assessment method of power transformer was proposed in this paper. Via analyzing the interrelation of the independent status parameters and transformer fault types, the set of synthetic status parameters can be built up. Association rule theory was used to calculate the constant weight coefficients of the independent status parameters. The variable weight synthesizing was used for the variable weight coefficients of the synthetic status parameters. Then, combining with the existing maintenance procedures, a preferable condition assessing system of power transformer was proposed. Operational example proved the condition assessing system may reflect the real running condition of power transformer. © 2013 Chin. Soc. for Elec. Eng.","Association rule; Condition assessment; Transformer; Variable weight","Artificial intelligence; Association rules; Artificial intelligence technologies; Association rule theory; Condition assessments; Maintenance procedures; Running conditions; Transformer; Transformer faults; Variable weight; Power transformers",Article,Scopus,2-s2.0-84885085454
"Dawson L., Stewart I.","Improving Ant Colony Optimization performance on the GPU using CUDA",2013,"2013 IEEE Congress on Evolutionary Computation, CEC 2013",23,10.1109/CEC.2013.6557791,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881594019&doi=10.1109%2fCEC.2013.6557791&partnerID=40&md5=e9d7124f648a265219a0ac9695845118","We solve the Travelling Salesman Problem (TSP) using a parallel implementation of the Ant System (AS) algorithm for execution on the Graphics Processing Unit (GPU) using NVIDIA CUDA. Extending some recent research, we implement both the tour construction and pheromone update stages of Ant Colony Optimization (ACO) on the GPU using a data parallel approach. In this recent work, roulette wheel selection is used during the tour contruction phase; however, we propose a new parallel implementation of roulette wheel selection called Double-Spin Roulette (DS-Roulette) which significantly reduces the running time of tour construction. We also develop a new implementation of the pheromone update stage. Our results show that compared to its sequential counterpart our new parallel implementation executes up to 82x faster whilst preserving the quality of the tours constructed, and up to 8.5x faster than the best existing parallel GPU implementation. © 2013 IEEE.",,"Ant Colony Optimization (ACO); Data parallel; GPU implementation; Graphics Processing Unit; Parallel implementations; Recent researches; Roulette wheel selection; Travelling salesman problem (TSP); Ant colony optimization; Artificial intelligence; Computer graphics; Evolutionary algorithms; Program processors; Traveling salesman problem; Wheels; Computer graphics equipment",Conference Paper,Scopus,2-s2.0-84881594019
"Duan H., Deng Y., Wang X., Xu C.","Small and Dim Target Detection via Lateral Inhibition Filtering and Artificial Bee Colony Based Selective Visual Attention",2013,"PLoS ONE",23,10.1371/journal.pone.0072035,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882679648&doi=10.1371%2fjournal.pone.0072035&partnerID=40&md5=d1cb66ed4c42568465b28bf7ffb97417","This paper proposed a novel bionic selective visual attention mechanism to quickly select regions that contain salient objects to reduce calculations. Firstly, lateral inhibition filtering, inspired by the limulus' ommateum, is applied to filter low-frequency noises. After the filtering operation, we use Artificial Bee Colony (ABC) algorithm based selective visual attention mechanism to obtain the interested object to carry through the following recognition operation. In order to eliminate the camera motion influence, this paper adopted ABC algorithm, a new optimization method inspired by swarm intelligence, to calculate the motion salience map to integrate with conventional visual attention. To prove the feasibility and effectiveness of our method, several experiments were conducted. First the filtering results of lateral inhibition filter were shown to illustrate its noise reducing effect, then we applied the ABC algorithm to obtain the motion features of the image sequence. The ABC algorithm is proved to be more robust and effective through the comparison between ABC algorithm and popular Particle Swarm Optimization (PSO) algorithm. Except for the above results, we also compared the classic visual attention mechanism and our ABC algorithm based visual attention mechanism, and the experimental results of which further verified the effectiveness of our method. © 2013 Duan et al.",,"algorithm; article; artificial bee colony; electrophysiology; image analysis; image processing; inhibition kinetics; insect swarm; lateral inhibition; organism colony; process optimization; selective attention; selective visual attention; signal noise ratio; Algorithms; Animals; Artificial Intelligence; Attention; Bees; Computer Simulation; Feasibility Studies; Humans; Image Processing, Computer-Assisted; Models, Biological; Motion; Pattern Recognition, Automated; Visual Perception",Article,Scopus,2-s2.0-84882679648
"Farshidianfar A., Soheili S.","Ant colony optimization of tuned mass dampers for earthquake oscillations of high-rise structures including soil-structure interaction",2013,"Soil Dynamics and Earthquake Engineering",23,10.1016/j.soildyn.2013.04.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877357537&doi=10.1016%2fj.soildyn.2013.04.002&partnerID=40&md5=5d362f8c69f965fe2c7182c24051c9f3","This paper investigates the optimized parameters for tuned mass dampers (TMDs) to decrease the earthquake vibrations of tall buildings; involving soil-structure interaction (SSI) effects. The time domain analysis based on Newmark method is employed in this study. To illustrate the results, Tabas and Kobe earthquakes data are applied to the model, and ant colony optimization (ACO) method is utilized to obtain the best parameters for TMD. The TMD mass, damping coefficient and spring stiffness are assumed as design variables, and the objective is to reduce both the maximum displacement and acceleration of stories. It is shown that how the ACO can be effectively applied to design the optimum TMD device. It is also indicated that the soil type greatly affects the TMD optimized parameters and the time response of structures. This study helps the researchers to better understanding of earthquake vibrations, and leads the designers to achieve the optimized TMD for high-rise buildings. © 2013 Elsevier Ltd.","Ant colony optimization; Earthquake oscillations; High-rise structures; Soil-structure interaction; Tuned mass dampers","Ant colony optimization methods; Damping coefficients; High rise building; High rise structures; Maximum displacement; Optimized parameter; Soil-Structure Interaction effects; Tuned mass dampers; Acoustic devices; Algorithms; Ant colony optimization; Artificial intelligence; Design; Structures (built objects); Tall buildings; Time domain analysis; Earthquakes; algorithm; damping; displacement; earthquake engineering; Kobe earthquake 1995; multistorey building; optimization; seismic design; soil-structure interaction; stiffness; vibration",Article,Scopus,2-s2.0-84877357537
"Castellano G., Paiva A., Kappas A., Aylett R., Hastie H., Barendregt W., Nabais F., Bull S.","Towards empathic virtual and robotic tutors",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",23,10.1007/978-3-642-39112-5-100,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879997749&doi=10.1007%2f978-3-642-39112-5-100&partnerID=40&md5=e0c7937782d01fb9535891fd69ff7df0","Building on existing work on artificial tutors with human-like capabilities, we describe the EMOTE project approach to harnessing benefits of an artificial embodied tutor in a shared physical space. Embodied in robotic platforms or through virtual agents, EMOTE aims to capture some of the empathic and human elements characterising a traditional teacher. As such, empathy and engagement, abilities key to influencing student learning, are at the core of the EMOTE approach. We present non-verbal and adaptive dialogue challenges for such embodied tutors as a foundation for researchers investigating the potential for empathic tutors that will be accepted by students and teachers. © 2013 Springer-Verlag Berlin Heidelberg.","Adaptive behaviour; Affect recognition; Virtual and robotic tutor","Adaptive behaviour; Affect recognition; Project approach; Robotic platforms; Student learning; Virtual agent; Virtual and robotic tutor; Artificial intelligence; Behavioral research; Robotics; Virtual reality; Teaching",Conference Paper,Scopus,2-s2.0-84879997749
"Yücel Z., Salah A.A., Meriçli Ç., Mericļi T., Valenti R., Gevers T.","Joint attention by gaze interpolation and saliency",2013,"IEEE Transactions on Cybernetics",23,10.1109/TSMCB.2012.2216979,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890437287&doi=10.1109%2fTSMCB.2012.2216979&partnerID=40&md5=ce180d7140583f361b25a093483a8ff9","Joint attention, which is the ability of coordination of a common point of reference with the communicating party, emerges as a key factor in various interaction scenarios. This paper presents an image-based method for establishing joint attention between an experimenter and a robot. The precise analysis of the experimenter's eye region requires stability and high-resolution image acquisition, which is not always available. We investigate regression-based interpolation of the gaze direction from the head pose of the experimenter, which is easier to track. Gaussian process regression and neural networks are contrasted to interpolate the gaze direction. Then, we combine gaze interpolation with image-based saliency to improve the target point estimates and test three different saliency schemes. We demonstrate the proposed method on a human-robot interaction scenario. Cross-subject evaluations, as well as experiments under adverse conditions (such as dimmed or artificial illumination or motion blur), show that our method generalizes well and achieves rapid gaze estimation for establishing joint attention. © 2012 IEEE.","Developmental robotics; Gaze following; Head pose estimation; Joint visual attention; Saliency; Selective attention","Developmental robotics; Gaze following; Head Pose Estimation; Saliency; Selective attention; Visual Attention; Image recognition; Interpolation; algorithm; artificial intelligence; attention; automated pattern recognition; biomimetics; eye fixation; human; interpersonal communication; man machine interaction; physiology; procedures; robotics; article; attention; automated pattern recognition; eye fixation; methodology; physiology; robotics; Algorithms; Artificial Intelligence; Attention; Biomimetics; Communication; Fixation, Ocular; Humans; Man-Machine Systems; Pattern Recognition, Automated; Robotics; Algorithms; Artificial Intelligence; Attention; Biomimetics; Communication; Fixation, Ocular; Humans; Man-Machine Systems; Pattern Recognition, Automated; Robotics",Article,Scopus,2-s2.0-84890437287
"Frolova D., Stern H., Berman S.","Most probable longest common subsequence for recognition of gesture character input",2013,"IEEE Transactions on Cybernetics",23,10.1109/TSMCB.2012.2217324,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890228520&doi=10.1109%2fTSMCB.2012.2217324&partnerID=40&md5=585f41d48e53016406c82957b1ef0143","This paper presents a technique for trajectory classification with applications to dynamic free-air hand gesture recognition. Such gestures are unencumbered and drawn in free air. Our approach is an extension to the longest common subsequence (LCS) classification algorithm. A learning preprocessing stage is performed to create a probabilistic 2-D template for each gesture, which allows taking into account different trajectory distortions with different probabilities. The modified LCS, termed the most probable LCS (MPLCS), is developed to measure the similarity between the probabilistic template and the hand gesture sample. The final decision is based on the length and probability of the extracted subsequence. Validation tests using a cohort of gesture digits from video-based capture show that the approach is promising with a recognition rate of more than 98% for video stream preisolated digits. The MPLCS algorithm can be integrated into a gesture recognition interface to facilitate gesture character input. This can greatly enhance the usability of such interfaces. © 2012 IEEE.","Classification; Dynamic gestures; Gesture recognition; Longest common subsequence (LCS)","Classification algorithm; Final decision; Hand gesture; Hand-gesture recognition; Longest common subsequences; Trajectory classification; Validation test; Video-based captures; Algorithms; Classification (of information); Video streaming; Gesture recognition; algorithm; anatomy and histology; artificial intelligence; automated pattern recognition; computer assisted diagnosis; gesture; hand; human; information retrieval; photography; physiology; procedures; videorecording; article; automated pattern recognition; hand; histology; methodology; photography; videorecording; Algorithms; Artificial Intelligence; Gestures; Hand; Humans; Image Interpretation, Computer-Assisted; Information Storage and Retrieval; Pattern Recognition, Automated; Photography; Video Recording; Algorithms; Artificial Intelligence; Gestures; Hand; Humans; Image Interpretation, Computer-Assisted; Information Storage and Retrieval; Pattern Recognition, Automated; Photography; Video Recording",Article,Scopus,2-s2.0-84890228520
"Daly I., Billinger M., Scherer R., Müller-Putz G.","On the automated removal of artifacts related to head movement from the EEG",2013,"IEEE Transactions on Neural Systems and Rehabilitation Engineering",23,10.1109/TNSRE.2013.2254724,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877842213&doi=10.1109%2fTNSRE.2013.2254724&partnerID=40&md5=dfe63ff35a4f81563495b92e7357380c","Contamination of the electroencephalogram (EEG) by artifacts related to head movement is a major cause of reduced signal quality. This is a problem in both neuroscience and other uses of the EEG. To attempt to reduce the influence, on the EEG, of artifacts related to head movement, an accelerometer is placed on the head and independent component analysis is applied to attempt to separate artifacts which are statistically related to head movements. To evaluate the method, EEG and accelerometer measurements are made from 14 individuals with Cerebral palsy attempting to control a sensorimotor rhythm based brain-computer interface. Results show that the approach significantly reduces the influence of head movement related artifacts in the EEG. © 2001-2011 IEEE.","Automated artifact removal; brain-computer interface (BCI); electroencephalogram; head movement; independent component analysis (ICA)","Accelerometer measurement; Artifact removal; Automated removal; Cerebral palsy; Electro-encephalogram (EEG); Head movements; Independent component analysis(ICA); Signal quality; Accelerometers; Brain computer interface; Independent component analysis; Electroencephalography; accelerometry; adult; algorithm; artifact; artificial intelligence; automated pattern recognition; brain; cerebral palsy; computer assisted diagnosis; electroencephalography; head movement; human; middle aged; pathophysiology; physiology; procedures; reproducibility; sensitivity and specificity; young adult; article; brain; cerebral palsy; computer assisted diagnosis; electroencephalography; head movement; methodology; pathophysiology; physiology; Accelerometry; Adult; Algorithms; Artifacts; Artificial Intelligence; Brain; Cerebral Palsy; Diagnosis, Computer-Assisted; Electroencephalography; Head Movements; Humans; Middle Aged; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Young Adult; Accelerometry; Adult; Algorithms; Artifacts; Artificial Intelligence; Brain; Cerebral Palsy; Diagnosis, Computer-Assisted; Electroencephalography; Head Movements; Humans; Middle Aged; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Young Adult",Article,Scopus,2-s2.0-84877842213
"Nourani V., Komasi M.","A geomorphology-based ANFIS model for multi-station modeling of rainfall-runoff process",2013,"Journal of Hydrology",23,10.1016/j.jhydrol.2013.03.024,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876822959&doi=10.1016%2fj.jhydrol.2013.03.024&partnerID=40&md5=fb352b67c7e6af024efaccd2e48e4237","This paper demonstrates the potential use of Artificial Intelligence (AI) techniques for predicting daily runoff at multiple gauging stations. Uncertainty and complexity of the rainfall-runoff process due to its variability in space and time in one hand and lack of historical data on the other hand, cause difficulties in the spatiotemporal modeling of the process. In this paper, an Integrated Geomorphological Adaptive Neuro-Fuzzy Inference System (IGANFIS) model conjugated with C-means clustering algorithm was used for rainfall-runoff modeling at multiple stations of the Eel River watershed, California. The proposed model could be used for predicting runoff in the stations with lack of data or any sub-basin within the watershed because of employing the spatial and temporal variables of the sub-basins as the model inputs. This ability of the integrated model for spatiotemporal modeling of the process was examined through the cross validation technique for a station. In this way, different ANFIS structures were trained using Sugeno algorithm in order to estimate daily discharge values at different stations. In order to improve the model efficiency, the input data were then classified into some clusters by the means of fuzzy C-means (FCMs) method. The goodness-of-fit measures support the gainful use of the IGANFIS and FCM methods in spatiotemporal modeling of hydrological processes. © 2013 Elsevier B.V.","Artificial intelligence; Black box model; Fuzzy clustering; Spatiotemporal modeling; The Eel River watershed","Adaptive neuro-fuzzy inference system; Black-box model; C-means clustering algorithm; Cross-validation technique; Rainfall-runoff modeling; River watersheds; Spatiotemporal modeling; Uncertainty and complexity; Artificial intelligence; Clustering algorithms; Fuzzy clustering; Fuzzy systems; Landforms; Runoff; Watersheds; Rain; algorithm; artificial intelligence; cluster analysis; fuzzy mathematics; geomorphology; prediction; rainfall-runoff modeling; spatial variation; spatiotemporal analysis; temporal variation; California; Eel Basin; United States",Article,Scopus,2-s2.0-84876822959
"Fannes T., Vandermarliere E., Schietgat L., Degroeve S., Martens L., Ramon J.","Predicting tryptic cleavage from proteomics data using decision tree ensembles",2013,"Journal of Proteome Research",23,10.1021/pr4001114,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877107079&doi=10.1021%2fpr4001114&partnerID=40&md5=79d9c131627de39b4f6414154fde69fa","Trypsin is the workhorse protease in mass spectrometry-based proteomics experiments and is used to digest proteins into more readily analyzable peptides. To identify these peptides after mass spectrometric analysis, the actual digestion has to be mimicked as faithfully as possible in silico. In this paper we introduce CP-DT (Cleavage Prediction with Decision Trees), an algorithm based on a decision tree ensemble that was learned on publicly available peptide identification data from the PRIDE repository. We demonstrate that CP-DT is able to accurately predict tryptic cleavage: tests on three independent data sets show that CP-DT significantly outperforms the Keil rules that are currently used to predict tryptic cleavage. Moreover, the trees generated by CP-DT can make predictions efficiently and are interpretable by domain experts. © 2013 American Chemical Society.","decision tree; machine learning; mass spectrometry; PRIDE; trypsin","proteinase; trypsin; accuracy; amino acid sequence; article; Cleavage Prediction with Decision Trees; computer model; decision tree; human; learning algorithm; machine learning; mass spectrometry; nonhuman; predictive value; priority journal; protein cleavage; protein determination; protein domain; proteomics; reproducibility; Algorithms; Amino Acid Sequence; Animals; Artificial Intelligence; Data Interpretation, Statistical; Decision Trees; Humans; Models, Biological; Proteolysis; Proteomics; Trypsin",Article,Scopus,2-s2.0-84877107079
"Caruana G., Li M., Liu Y.","An ontology enhanced parallel SVM for scalable spam filter training",2013,"Neurocomputing",23,10.1016/j.neucom.2012.12.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875111175&doi=10.1016%2fj.neucom.2012.12.001&partnerID=40&md5=2183e52c6f3123e44afdec979ec141c0","Spam, under a variety of shapes and forms, continues to inflict increased damage. Varying approaches including Support Vector Machine (SVM) techniques have been proposed for spam filter training and classification. However, SVM training is a computationally intensive process. This paper presents a MapReduce based parallel SVM algorithm for scalable spam filter training. By distributing, processing and optimizing the subsets of the training data across multiple participating computer nodes, the parallel SVM reduces the training time significantly. Ontology semantics are employed to minimize the impact of accuracy degradation when distributing the training data among a number of SVM classifiers. Experimental results show that ontology based augmentation improves the accuracy level of the parallel SVM beyond the original sequential counterpart. © 2012 Elsevier B.V.","Classification; MapReduce; Parallel computing; Spam filtering; Support vector machine","Accuracy level; Computer nodes; Map-reduce; Ontology semantics; Ontology-based; Spam filtering; Support vector machine techniques; SVM classifiers; Classification (of information); Internet; Parallel architectures; Parallel processing systems; Semantics; Support vector machines; accuracy; article; artificial intelligence; automation; classification; classification algorithm; classifier; e-mail; machine learning; priority journal; semantics; sequential minimal optimization algorithm; spam filtering training; support vector machine",Article,Scopus,2-s2.0-84875111175
"Liu Y.","Sustainable competitive advantage in turbulent business environments",2013,"International Journal of Production Research",23,10.1080/00207543.2012.720392,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876317225&doi=10.1080%2f00207543.2012.720392&partnerID=40&md5=d40ac975a2f84c631f7c436852a73e63","The future competitiveness of manufacturing operations under dynamic and complex business environments relies on forward-thinking strategies. The objective of this paper is to identify and develop the operational competitiveness in a sustainable manner by implementing a unique sustainable competitive advantage (SCA) for managing dynamic business situations. This paper develops a theoretical approach to integrating the core factors which affect operational competitiveness performance, that is to say manufacturing strategy and transformational leadership with technology level, into conceptual analytical models to evaluate overall competitiveness, and utilises sense and respond (S&R) for dynamic decision-making to optimise resource allocations and adjust strategies in order to develop competitiveness potential in a sustainable manner. From the empirical research, the adjustments in manufacturing strategy and transformation leadership by implementing SCA through fast strategy with proposed models are found to be effective and successful in managing turbulent business environments such as the economic crisis. Such proactive operations are proposed as the unique SCA with empirical research carried out in a global context, which provides both theoretical significance and also practical benefit to conclude the experience of managing operations in turbulent business environments. © 2013 Copyright Taylor and Francis Group, LLC.","Decision support systems; Operations strategy; Sustainable competitive advantage (SCA); Turbulent business environments","Business environments; Dynamic decision making; Manufacturing operations; Manufacturing strategy; Operations strategies; Sustainable competitive advantages; Theoretical approach; Transformational leadership; Artificial intelligence; Decision support systems; Industrial research; Manufacture; Sustainable development; Competition",Article,Scopus,2-s2.0-84876317225
"Manoj K., Monjezi M.","Prediction of flyrock in open pit blasting operation using machine learning method",2013,"International Journal of Mining Science and Technology",23,10.1016/j.ijmst.2013.05.005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880571852&doi=10.1016%2fj.ijmst.2013.05.005&partnerID=40&md5=51b24ede8c5b285e9326ee7aabadddcf","Flyrock is one of the most hazardous events in blasting operation of surface mines. There are several empirical methods to predict flyrock. Low performance of such models is due to the complexity of flyrock analysis. Existence of various effective parameters and their unknown relationships are the main reasons for inaccuracy of the empirical models. Presently, the application of new approaches such as artificial intelligence is highly recommended. In this paper, an attempt has been made to predict flyrock in blasting operations of Soungun Copper Mine, Iran incorporating rock properties and blast design parameters using support vector machine (SVM) method. To investigate the suitability of this approach, the predictions by SVM have been compared with multivariate regression analysis (MVRA), too. Coefficient of determination (CoD) and mean absolute error (MAE) were taken as performance measures. It was found that CoD between measured and predicted flyrock was 0.948 and 0.440 by SVM and MVRA, respectively, whereas MAE between measured and predicted flyrock was 3.11 and 7.74 by SVM and MVRA, respectively. © 2013 Published by Elsevier B.V. on behalf of China University of Mining & Technology.","Blasting; Copper; Flyrock; Mine; MVRA; Soungun; Support vector machine","Coefficient of determination; Effective parameters; Flyrock; Machine learning methods; Multivariate regression analysis; MVRA; Soungun; Soungun copper mines; Artificial intelligence; Blasting; Copper; Copper mines; Forecasting; Mines; Open pit mining; Regression analysis; Support vector machines",Article,Scopus,2-s2.0-84880571852
"Berber T., Alpkocak A., Balci P., Dicle O.","Breast mass contour segmentation algorithm in digital mammograms",2013,"Computer Methods and Programs in Biomedicine",23,10.1016/j.cmpb.2012.11.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875891311&doi=10.1016%2fj.cmpb.2012.11.003&partnerID=40&md5=9ee93303478f077e0c063142e8d7af66","Many computer aided diagnosis (CAD) systems help radiologist on difficult task of mass detection in a breast mammogram and, besides, they also provide interpretation about detected mass. One of the most crucial information of a mass is its shape and contour, since it provides valuable information about spread ability of a mass. However, accuracy of shape recognition of a mass highly related with the precision of detected mass contours. In this work, we introduce a new segmentation algorithm, breast mass contour segmentation, based on classical seed region growing algorithm to enhance contour of a mass from a given region of interest with ability to adjust threshold value adaptively. The new approach is evaluated over a dataset with 260 masses whose contours are manually annotated by expert radiologists. The performance of the method is evaluated with respect to a set of different evaluation metrics, such as specificity, sensitivity, balanced accuracy, Yassnoff and Hausdorrf error distances. The results obtained from experimentations shows that our method outperforms the other compared methods. All the findings and details of approach are presented in detail. © 2012 Elsevier Ireland Ltd.","Mammography; Region growing; Segmentation; Segmentation evaluation","Computer Aided Diagnosis(CAD); Contour segmentation; Contour segmentation algorithm; Digital mammograms; Region growing; Region growing algorithm; Segmentation algorithms; Segmentation evaluation; Computer aided diagnosis; Mammography; X ray screens; Image segmentation; algorithm; article; breast examination; breast masss contour segmentation algorithm; breast tumor; clinical evaluation; diagnostic accuracy; digital mammography; image processing; medical expert; radiologist; sensitivity and specificity; tumor volume; Algorithms; Artificial Intelligence; Breast; Breast Neoplasms; Diagnosis, Computer-Assisted; False Positive Reactions; Female; Humans; Image Processing, Computer-Assisted; Mammography; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Software",Article,Scopus,2-s2.0-84875891311
"Vatavu R.-D.","A comparative study of user-defined handheld vs. freehand gestures for home entertainment environments",2013,"Journal of Ambient Intelligence and Smart Environments",23,10.3233/AIS-130200,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876274780&doi=10.3233%2fAIS-130200&partnerID=40&md5=100bcff5299204e32c1c8624fb20bb32","Home entertainment systems have been continuously providing new functionalities to their users in a process in which they evolved from standalone electronic appliances to complex digital milieus superimposed on the home environment. Such complex entertainment environments are in need of appropriate interfaces with gestures standing as a viable option. However, designing gesture commands is a challenging task and today's designs do not always reflect users' natural gestures. For this reason, this work presents the results of a participatory study in which users were involved in the design and evaluation of gesture commands for 22 typical home entertainment tasks. This is the first study to deliver a comparison between user-defined gestures for two different capture technologies: using handheld devices to sense motion and employing freehand gestures and postures. Specifically, we found that: (1) consensus exists among participants in both scenarios; (2) when available, buttons are preferred to motion commands; (3) familiar point & click and drag & drop techniques are naturally reused for new environments; (4) one-hand gestures are preferred to bimanual gestures; (5) hand posture is important yet culture specific; and (6) body-referenced and parametrized gestures appeal to users. We report gesture sets for the handheld and freehand gesture scenarios in order to inform, assist, and inspire practitioners designing the gestural interfaces of our future home environments. © 2013-IOS Press and the authors. All rights reserved.","freehand; gestural interfaces; gesture elicitation study; gesture set; Gesture-based interaction; handheld; home entertainment; human-computer interaction; Kinect; participatory design; user-adapted interaction; Wii","freehand; Gestural interfaces; gesture elicitation study; gesture set; Gesture-based interaction; Handhelds; Home entertainment; Kinect; Participatory design; user-adapted interaction; Wii; Artificial intelligence; Human computer interaction; Software engineering",Article,Scopus,2-s2.0-84876274780
"Cui Y., Chang W., Nöll T., Stricker D.","KinectAvatar: Fully automatic body capture using a single kinect",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",23,10.1007/978-3-642-37484-5_12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875982193&doi=10.1007%2f978-3-642-37484-5_12&partnerID=40&md5=292227d93191065f89514100fb2c97f7","We present a novel scanning system for capturing a full 3D human body model using just a single depth camera and no auxiliary equipment. We claim that data captured from a single Kinect is sufficient to produce a good quality full 3D human model. In this setting, the challenges we face are the sensor's low resolution with random noise and the subject's non-rigid movement when capturing the data. To overcome these challenges, we develop an improved super-resolution algorithm that takes color constraints into account. We then align the super-resolved scans using a combination of automatic rigid and non-rigid registration. As the system is of low price and obtains impressive results in several minutes, full 3D human body scanning technology can now become more accessible to everyday users at home. © 2013 Springer-Verlag.",,"3D human body; 3D human models; Depth camera; Low resolution; Nonrigid registration; Random noise; Scanning systems; Super resolution algorithms; Artificial intelligence; Auxiliary equipment",Conference Paper,Scopus,2-s2.0-84875982193
"Medina J., Ojeda-Aciego M.","Dual multi-adjoint concept lattices",2013,"Information Sciences",23,10.1016/j.ins.2012.10.030,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871398272&doi=10.1016%2fj.ins.2012.10.030&partnerID=40&md5=2796dad7334e54b0e948b344e9c92fac","Several papers relate different alternative approaches to classical concept lattices: such as property-oriented and object-oriented concept lattices and the dual concept lattices. Whereas the usual approach to the latter is via a negation operator, this paper presents a fuzzy generalization of the dual concept lattice, the dual multi-adjoint concept lattice, in which the philosophy of the multi-adjoint paradigm is applied and no negation on the lattices is needed. © 2012 Elsevier Inc. All rights reserved.","Concept lattices; Dual concept lattices; Galois connection; Implication triples","Alternative approach; Concept Lattices; Fuzzy generalizations; Galois connection; Implication triples; Object-oriented concepts; Artificial intelligence; Software engineering; Information analysis",Article,Scopus,2-s2.0-84871398272
"Gao J.","Uncertain bimatrix game with applications",2013,"Fuzzy Optimization and Decision Making",23,10.1007/s10700-012-9145-6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873997994&doi=10.1007%2fs10700-012-9145-6&partnerID=40&md5=67f1026bdb0f6c9da326725da70d4ced","In real-world games, the players are often lack of the information about the other players' (or even his own) payoffs. Assuming that all entries of payoff matrices are uncertain variables, this paper introduces a concept of uncertain bimatrix game. Within the framework of uncertainty theory, three solution concepts of uncertain equilibrium strategies as well as their existence theorem are proposed. Furthermore, a sufficient and necessary condition is presented for finding the uncertain equilibrium strategies. Finally, an example is provided for illustrating the usefulness of the theory developed in this paper. © 2012 Springer Science+Business Media New York.","Nash equilibrium; Uncertain game; Uncertainty theory","Bimatrix games; Equilibrium strategy; Existence theorem; Nash equilibria; Real-world games; Sufficient and necessary condition; Three solutions; Uncertain game; Uncertain variables; Uncertainty theory; Artificial intelligence; Decision making; Game theory",Article,Scopus,2-s2.0-84873997994
"Mittal S.","Emergence in stigmergic and complex adaptive systems: A formal discrete event systems perspective",2013,"Cognitive Systems Research",23,10.1016/j.cogsys.2012.06.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868303966&doi=10.1016%2fj.cogsys.2012.06.003&partnerID=40&md5=b30b99db65e13213fe4fc9eebfa59ac9","Complex systems have been studied by researchers from every discipline: biology, chemistry, physics, sociology, mathematics and economics and more. Depending upon the discipline, complex systems theory has accrued many flavors. We are after a formal representation, a model that can predict the outcome of a complex adaptive system (CAS). In this article, we look at the nature of complexity, then provide a perspective based on discrete event systems (DEVS) theory. We pin down many of the shared features between CAS and artificial systems. We begin with an overview of network science showing how adaptive behavior in these scale-free networks can lead to emergence through stigmergy in CAS. We also address how both self-organization and emergence interplay in a CAS. We then build a case for the view that stigmergic systems are a special case of CAS. We then discuss DEVS levels of systems specifications and present the dynamic structure extensions of DEVS formalism that lends itself to a study of CAS and in turn, stigmergy. Finally, we address the shortcomings and the limitation of current DEVS extensions and propose the required augmentation to model stigmergy and CAS. © 2012 Elsevier B.V.","Artificial systems; Complex adaptive systems; DEVS; Dynamic structure; Emergence; Scale-free networks; Self-organization; Stigmergy","Artificial systems; Complex adaptive systems; DEVS; Dynamic structure; Emergence; Scale free networks; Self-organization; Stigmergy; Artificial intelligence; Cognitive systems; Adaptive systems; article; behavior; complex adaptive system; complex systems theory; discrete events systems theory; emergence; outcome assessment; prediction; priority journal; stigmergy; theoretical model; theory",Article,Scopus,2-s2.0-84868303966
"Labiosa W.B., Forney W.M., Esnard A.-M., Mitsova-Boneva D., Bernknopf R., Hearn P., Hogan D., Pearlstine L., Strong D., Gladwin H., Swain E.","An integrated multi-criteria scenario evaluation web tool for participatory land-use planning in urbanized areas: The Ecosystem Portfolio Model",2013,"Environmental Modelling and Software",23,10.1016/j.envsoft.2012.10.012,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873248492&doi=10.1016%2fj.envsoft.2012.10.012&partnerID=40&md5=b698cae30b4adf637a4447394f078a1b","Land-use land-cover change is one of the most important and direct drivers of changes in ecosystem functions and services. Given the complexity of the decision-making, there is a need for Internet-based decision support systems with scenario evaluation capabilities to help planners, resource managers and communities visualize, compare and consider trade-offs among the many values at stake in land use planning. This article presents details on an Ecosystem Portfolio Model (EPM) prototype that integrates ecological, socio-economic information and associated values of relevance to decision-makers and stakeholders. The EPM uses a multi-criteria scenario evaluation framework, Geographic Information Systems (GIS) analysis and spatially-explicit land-use/land-cover change-sensitive models to characterize changes in important land-cover related ecosystem values related to ecosystem services and functions, land parcel prices, and community quality-of-life (QoL) metrics. Parameters in the underlying models can be modified through the interface, allowing users in a facilitated group setting to explore simultaneously issues of scientific uncertainty and divergence in the preferences of stakeholders. One application of the South Florida EPM prototype reported in this article shows the modeled changes (which are significant) in aggregate ecological value, landscape patterns and fragmentation, biodiversity potential and ecological restoration potential for current land uses compared to the 2050 land-use scenario. Ongoing refinements to EPM, and future work especially in regard to modifiable sea level rise scenarios are also discussed. © 2012 .","Decision support; Ecological value; Ecosystem restoration; Land-use planning; Quality of life; Sea level rise mitigation; Sustainability","Decision supports; Ecological value; Ecosystem restoration; Land-use planning; Quality of life; Sea level rise; Artificial intelligence; Biodiversity; Decision making; Decision support systems; Economic and social effects; Economics; Ecosystems; Function evaluation; Geographic information systems; Land use; Sea level; Sustainable development; Quality control; biodiversity; decision making; decision support system; ecosystem modeling; ecosystem service; GIS; integrated approach; Internet; land cover; land use change; land use planning; multicriteria analysis; participatory approach; quality of life; restoration ecology; sea level change; sustainability; urban area; Florida [United States]; United States",Article,Scopus,2-s2.0-84873248492
"Weinkam P., Chen Y.C., Pons J., Sali A.","Impact of mutations on the allosteric conformational equilibrium",2013,"Journal of Molecular Biology",23,10.1016/j.jmb.2012.11.041,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872835595&doi=10.1016%2fj.jmb.2012.11.041&partnerID=40&md5=508933a33e5665a13434d9afa753fb31","Allostery in a protein involves effector binding at an allosteric site that changes the structure and/or dynamics at a distant, functional site. In addition to the chemical equilibrium of ligand binding, allostery involves a conformational equilibrium between one protein substate that binds the effector and a second substate that less strongly binds the effector. We run molecular dynamics simulations using simple, smooth energy landscapes to sample specific ligand-induced conformational transitions, as defined by the effector-bound and effector-unbound protein structures. These simulations can be performed using our web server (http://salilab.org/allosmod/). We then develop a set of features to analyze the simulations and capture the relevant thermodynamic properties of the allosteric conformational equilibrium. These features are based on molecular mechanics energy functions, stereochemical effects, and structural/dynamic coupling between sites. Using a machine-learning algorithm on a data set of 10 proteins and 179 mutations, we predict both the magnitude and the sign of the allosteric conformational equilibrium shift by the mutation; the impact of a large identifiable fraction of the mutations can be predicted with an average unsigned error of 1 kBT. With similar accuracy, we predict the mutation effects for an 11th protein that was omitted from the initial training and testing of the machine-learning algorithm. We also assess which calculated thermodynamic properties contribute most to the accuracy of the prediction. © 2012 Elsevier Ltd.","allostery; energy landscape; Keywords; machine learning; protein dynamics","beta lactamase; calmodulin; caspase 7; glucokinase; hemoglobin; maltose binding protein; phosphoinositide dependent protein kinase 1; protein tyrosine phosphatase; thrombin; accuracy; algorithm; allosterism; article; conformational transition; energy; equilibrium constant; machine learning; molecular mechanics; molecular model; mutation; priority journal; protein conformation; protein structure; stereochemistry; thermodynamics; Algorithms; Allosteric Regulation; Artificial Intelligence; Ligands; Models, Molecular; Molecular Dynamics Simulation; Mutation; Protein Conformation; Proteins",Article,Scopus,2-s2.0-84872835595
"Gao X., Gao Y.","CONnectedness index of uncertain graph",2013,"International Journal of Uncertainty, Fuzziness and Knowlege-Based Systems",23,10.1142/S0218488513500074,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874398761&doi=10.1142%2fS0218488513500074&partnerID=40&md5=f36dce340e3ccb8bfc34484ed2c3c065","In practical applications of graph theory, non-deterministic factors are frequently encountered. This paper employs uncertainty theory to deal with non-deterministic factors in problems of graph connectivity. The concepts of uncertain graph and connectedness index of uncertain graph are proposed in this paper. It presents two algorithms to calculate connectedness index of an uncertain graph. © World Scientific Publishing Company.","connectedness index; Uncertain graph; uncertainty theory","connectedness index; Graph connectivity; Uncertain graphs; Uncertainty theory; Artificial intelligence; Software engineering; Graph theory",Article,Scopus,2-s2.0-84874398761
"Wang F., Lee N., Hu J., Sun J., Ebadollahi S., Laine A.F.","A framework for mining signatures from event sequences and its applications in healthcare data",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",23,10.1109/TPAMI.2012.111,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871741964&doi=10.1109%2fTPAMI.2012.111&partnerID=40&md5=974a0e2bca3cc51c501b41f9c498b086","This paper proposes a novel temporal knowledge representation and learning framework to perform large-scale temporal signature mining of longitudinal heterogeneous event data. The framework enables the representation, extraction, and mining of high-order latent event structure and relationships within single and multiple event sequences. The proposed knowledge representation maps the heterogeneous event sequences to a geometric image by encoding events as a structured spatial-temporal shape process. We present a doubly constrained convolutional sparse coding framework that learns interpretable and shift-invariant latent temporal event signatures. We show how to cope with the sparsity in the data as well as in the latent factor model by inducing a double sparsity constraint on the β-divergence to learn an overcomplete sparse latent factor model. A novel stochastic optimization scheme performs large-scale incremental learning of group-specific temporal event signatures. We validate the framework on synthetic data and on an electronic health record dataset. © 2012 IEEE.","beta-divergence; dictionary learning; nonnegative matrix factorization; sparse coding; stochastic gradient descent; Temporal signature mining","beta-divergence; Dictionary learning; Nonnegative matrix factorization; Sparse coding; Stochastic gradient descent; Temporal signatures; Health care; Knowledge representation; Graphical user interfaces; algorithm; article; artificial intelligence; automated pattern recognition; data base; data mining; decision support system; electronic medical record; medical record; methodology; Algorithms; Artificial Intelligence; Data Mining; Database Management Systems; Decision Support Systems, Clinical; Decision Support Techniques; Electronic Health Records; Health Records, Personal; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84871741964
"Korniłowicz A.","On rewriting rules in Mizar",2013,"Journal of Automated Reasoning",23,10.1007/s10817-012-9261-6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878491114&doi=10.1007%2fs10817-012-9261-6&partnerID=40&md5=b4c198c1571d209ebb3927fbc6679aa2","This paper presents some tentative experiments in using a special case of rewriting rules in Mizar (Mizar homepage: http://www.mizar.org/): rewriting a term as its subterm. A similar technique, but based on another Mizar mechanism called functor identification (Korniłowicz 2009) was used by Caminati, in his paper on basic first-order model theory in Mizar (Caminati, J Form Reason 3(1):49-77, 2010, Form Math 19(3):157-169, 2011). However for this purpose he was obligated to introduce some artificial functors. The mechanism presented in the present paper looks promising and fits the Mizar paradigm. © 2012 The Author(s).","Computer algebra system; Mizar; Natural deduction; Proof assistant; Term rewriting","Computer algebra systems; Mizar; Natural deduction; Proof assistant; Term rewriting; Automata theory; Software engineering; Artificial intelligence",Article,Scopus,2-s2.0-84878491114
"Pa̧k K.","Methods of lemma extraction in natural deduction proofs",2013,"Journal of Automated Reasoning",23,10.1007/s10817-012-9267-0,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878512919&doi=10.1007%2fs10817-012-9267-0&partnerID=40&md5=7f172221e29abbdf381357c911216e95","The existing examples of natural deduction proofs, either declarative or procedural, indicate that often the legibility of proof scripts is of secondary importance to the authors. As soon as the computer accepts the proof script, many authors do not work on improving the parts that could be shortened and do not avoid repetitions of technical sub-deductions, which often could be replaced by a single lemma. This article presents selected properties of reasoning passages that may be used to determine if a reasoning passage can be extracted from a proof script, transformed into a lemma and replaced by a reference to the newly created lemma. Additionally, we present methods for improving the legibility of the reasoning that remains after the extraction of the lemmas. © 2012 The Author(s).","Deduction; Extraction; Legibility; Lemma; Natural","Deduction; Legibility; Lemma; Natural; Natural deduction proofs; Artificial intelligence; Automata theory; Software engineering; Extraction",Article,Scopus,2-s2.0-84878512919
"Huang C.-L., Huang W.-C., Chang H.-Y., Yeh Y.-C., Tsai C.-Y.","Hybridization strategies for continuous ant colony optimization and particle swarm optimization applied to data clustering",2013,"Applied Soft Computing Journal",23,10.1016/j.asoc.2013.05.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884412430&doi=10.1016%2fj.asoc.2013.05.003&partnerID=40&md5=a8f0550409336982ed9f8ec93a97b85c","Ant colony optimization (ACO) and particle swarm optimization (PSO) are two popular algorithms in swarm intelligence. Recently, a continuous ACO named ACOR was developed to solve the continuous optimization problems. This study incorporated ACOR with PSO to improve the search ability, investigating four types of hybridization as follows: (1) sequence approach, (2) parallel approach, (3) sequence approach with an enlarged pheromone-particle table, and (4) global best exchange. These hybrid systems were applied to data clustering. The experimental results utilizing public UCI datasets show that the performances of the proposed hybrid systems are superior compared to those of the K-mean, standalone PSO, and standalone ACOR. Among the four strategies of hybridization, the sequence approach with the enlarged pheromone table is superior to the other approaches because the enlarged pheromone table diversifies the generation of new solutions of ACOR and PSO, which prevents traps into the local optimum. © 2013 Elsevier B.V. All rights reserved.","Ant colony optimization; Data clustering; Hybrid systems; Particle swarm optimization; Swarm intelligence","Ant colony optimization; Artificial intelligence; Cluster analysis; Clustering algorithms; Hybrid systems; Optimization; Swarm intelligence; Ant Colony Optimization (ACO); Continuous optimization problems; Data clustering; K-means; Local optima; New solutions; Uci datasets; Particle swarm optimization (PSO)",Article,Scopus,2-s2.0-84884412430
"Snyder J.C., Rupp M., Hansen K., Blooston L., Müller K.-R., Burke K.","Orbital-free bond breaking via machine learning",2013,"Journal of Chemical Physics",22,10.1063/1.4834075,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903362304&doi=10.1063%2f1.4834075&partnerID=40&md5=2e83891e5160dad50b8394108b4e6207","Using a one-dimensional model, we explore the ability of machine learning to approximate the non-interacting kinetic energy density functional of diatomics. This nonlinear interpolation between Kohn-Sham reference calculations can (i) accurately dissociate a diatomic, (ii) be systematically improved with increased reference data and (iii) generate accurate self-consistent densities via a projection method that avoids directions with no data. With relatively few densities, the error due to the interpolation is smaller than typical errors in standard exchange-correlation functionals. © 2013 AIP Publishing LLC.",,"Bond-breaking; Exchange-correlation functionals; Kinetic energy density; Kohn shams; Nonlinear interpolation; One-dimensional model; Projection method; Reference data; Interpolation; Kinetics; Learning systems; algorithm; article; artificial intelligence; computer simulation; quantum theory; Algorithms; Artificial Intelligence; Computer Simulation; Quantum Theory",Article,Scopus,2-s2.0-84903362304
"Berkan Sesen M., Nicholson A.E., Banares-Alcantara R., Kadir T., Brady M.","Bayesian networks for clinical decision support in lung cancer care",2013,"PLoS ONE",22,10.1371/journal.pone.0082349,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891915402&doi=10.1371%2fjournal.pone.0082349&partnerID=40&md5=7da6e35bef52bdb1afe649dfd255a578","Survival prediction and treatment selection in lung cancer care are characterised by high levels of uncertainty. Bayesian Networks (BNs), which naturally reason with uncertain domain knowledge, can be applied to aid lung cancer experts by providing personalised survival estimates and treatment selection recommendations. Based on the English Lung Cancer Database (LUCADA), we evaluate the feasibility of BNs for these two tasks, while comparing the performances of various causal discovery approaches to uncover the most feasible network structure from expert knowledge and data. We show first that the BN structure elicited from clinicians achieves a disappointing area under the ROC curve of 0.75 (± 0.03), whereas a structure learned by the CAMML hybrid causal discovery algorithm, which adheres with the temporal restrictions, achieves 0.81 (± 0.03). Second, our causal intervention results reveal that BN treatment recommendations, based on prescribing the treatment plan that maximises survival, can only predict the recorded treatment plan 29% of the time. However, this percentage rises to 76% when partial matches are included. © 2013 Sesen et al.",,"area under the curve; article; Bayes theorem; cancer staging; causal attribution; chemoradiotherapy; clinical decision making; comparative study; data processing; human; learning algorithm; logistic regression analysis; lung cancer; medical expert; patient care; probability; receiver operating characteristic; survival prediction; survival rate; treatment planning; Algorithms; Area Under Curve; Artificial Intelligence; Bayes Theorem; Cluster Analysis; Databases, Factual; Decision Support Techniques; Delivery of Health Care; Humans; Lung Neoplasms; Neoplasm Staging; Prognosis; Reproducibility of Results",Article,Scopus,2-s2.0-84891915402
"Erdélyi G., Lackner M., Pfandler A.","Computational aspects of nearly single-peaked electorates",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",22,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893381862&partnerID=40&md5=ccd85488d7b76160f66bd72597fe938a","Manipulation, bribery, and control are well-studied ways of changing the outcome of an election. Many voting systems are, in the general case, computationally resistant to some of these manipulative actions. However when restricted to single-peaked electorates, these systems suddenly become easy to manipulate. Recently, Faliszewski, Hemaspaandra, and Hemaspaandra (2011b) studied the complexity of dishonest behavior in nearly single-peaked electorates. These are electorates that are not single-peaked but close to it according to some distance measure. In this paper we introduce several new distance measures regarding single-peakedness. We prove that determining whether a given profile is nearly single-peaked is NP-complete in many cases. For one case we present a polynomial-time algorithm. Furthermore, we explore the relations between several notions of nearly single-peakedness. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Computational aspects; Distance measure; NP Complete; Polynomial-time algorithms; Voting systems; Algorithms; Voting machines; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84893381862
"Choi A., Darwiche A.","Dynamic minimization of Sentential Decision Diagrams",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",22,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893415154&partnerID=40&md5=5b8b9ef721200b2a981e7b606d0ff75e","The Sentential Decision Diagram (SDD) is a recently proposed representation of Boolean functions, containing Ordered Binary Decision Diagrams (OBDDs) as a distinguished subclass. While OBDDs are characterized by total variable orders, SDDs are characterized more generally by vtrees. As both OBDDs and SDDs have canonical representations, searching for OBDDs and SDDs of minimal size simplifies to searching for variable orders and vtrees, respectively. For OBDDs, there are effective heuristics for dynamic reordering, based on locally swapping variables. In this paper, we propose an analogous approach for SDDs which navigates the space of vtrees via two operations: one based on tree rotations and a second based on swapping children in a vtree. We propose a particular heuristic for dynamically searching the space of vtrees, showing that it can find SDDs that are an order-of-magnitude more succinct than OBDDs found by dynamic reordering. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Canonical representations; Decision diagram; Dynamic reordering; Ordered binary decision diagrams; Tree rotations; Variable order; Artificial intelligence; Boolean functions; Decision theory",Conference Paper,Scopus,2-s2.0-84893415154
"Grau B.C., Motik B., Stoilos G., Horrocks I.","Computing datalog rewritings beyond Horn ontologies",2013,"IJCAI International Joint Conference on Artificial Intelligence",22,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896061429&partnerID=40&md5=7a19b92f49102fdd82aad3a96945742e","Rewriting-based approaches for answering queries over an OWL 2 DL ontology have so far been developed mainly for Horn fragments of OWL 2 DL. In this paper, we study the possibilities of answering queries over non-Horn ontologies using datalog rewritings. We prove that this is impossible in general even for very simple ontology languages, and even if PTIME = NP. Furthermore, we present a resolution-based procedure for SHI ontologies that, in case it terminates, produces a datalog rewriting of the ontology. We also show that our procedure necessarily terminates on DL-Lite boolH+ ontologies-an extension of OWL 2 QL with transitive roles and Boolean connectives.",,"Answering queries; Boolean connectives; Datalog; Dl-lite; Horn fragments; Ontology language; Artificial intelligence; Birds",Conference Paper,Scopus,2-s2.0-84896061429
"Huang J., Nie F., Huang H., Lei Y., Ding C.","Social trust prediction using rank-κ matrix recovery",2013,"IJCAI International Joint Conference on Artificial Intelligence",22,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063033&partnerID=40&md5=71f2f402c0bb7e2db8b1ba0d35146b51","Trust prediction, which explores the unobserved relationships between online community users, is an emerging and important research topic in social network analysis and many web applications. Similar to other social-based recommender systems, trust relationships between users can be also modeled in the form of matrices. Recent study shows users generally establish friendship due to a few latent factors, it is therefore reasonable to assume the trust matrices are of low-rank. As a result, many recommendation system strategies can be applied here. In particular, trace norm minimization, which uses matrix's trace norm to approximate its rank, is especially appealing. However, recent articles cast doubts on the validity of trace norm approximation. In this paper, instead of using trace norm minimization, we propose a new robust rank-k matrix completion method, which explicitly seeks a matrix with exact rank. Moreover, our method is robust to noise or corrupted observations. We optimize the new objective function in an alternative manner, based on a combination of ancillary variables and Augmented Lagrangian Multiplier (ALM) Method. We perform the experiments on three real-world data sets and all empirical results demonstrate the effectiveness of our method.",,"Augmented Lagrangians; Matrix completion; Matrix recovery; Objective functions; On-line communities; Trust predictions; Trust relationship; WEB application; Recommender systems; Social networking (online); Virtual reality; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84896063033
"Shiokawa H., Fujiwara Y., Onizuka M.","Fast algorithm for modularity-based graph clustering",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",22,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893426724&partnerID=40&md5=ce55f836bda409194e9e980aa80de220","In AI and Web communities, modularity-based graph clustering algorithms are being applied to various applications. However, existing algorithms are not applied to large graphs because they have to scan all vertices/edges iteratively. The goal of this paper is to efficiently compute clusters with high modularity from extremely large graphs with more than a few billion edges. The heart of our solution is to compute clusters by incrementally pruning unnecessary vertices/edges and optimizing the order of vertex selections. Our experiments show that our proposal outperforms all other modularity-based algorithms in terms of computation time, and it finds clusters with high modularity. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Computation time; Fast algorithms; Graph clustering; Graph clustering algorithms; Large graphs; Vertex selection; Web community; Artificial intelligence; Clustering algorithms; Iterative methods",Conference Paper,Scopus,2-s2.0-84893426724
"Peng H., Li B., Ji R., Hu W., Xiong W., Lang C.","Salient object detection via Low-rank and Structured sparse Matrix Decomposition",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",22,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893400122&partnerID=40&md5=f7a31ad4883e92891f4901d79b281ee6","Salient object detection provides an alternative solution to various image semantic understanding tasks such as object recognition, adaptive compression and image retrieval. Recently, low-rank matrix recovery (LR) theory has been introduced into saliency detection, and achieves impressed results. However, the existing LR-based models neglect the underlying structure of images, and inevitably degrade the associated performance. In this paper, we propose a Low-rank and Structured sparse Matrix Decomposition (LSMD) model for salient object detection. In the model, a tree-structured sparsity-inducing norm regularization is firstly introduced to provide a hierarchical description of the image structure to ensure the completeness of the extracted salient object. The similarity of saliency values within the salient object is then guaranteed by the ℓ∞-norm. Finally, high-level priors are integrated to guide the matrix decomposition and enhance the saliency detection. Experimental results on the largest public benchmark database show that our model outperforms existing LRbased approaches and other state-of-the-art methods, which verifies the effectiveness and robustness of the structure cues in our model. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Adaptive compression; Alternative solutions; Hierarchical description; Low-rank matrix recoveries; Matrix decomposition; Saliency detection; Salient object detection; State-of-the-art methods; Artificial intelligence; Image retrieval; Semantics; Object recognition",Conference Paper,Scopus,2-s2.0-84893400122
"Boots B., Gretton A., Gordon G.J.","Hilbert space embeddings of predictive state representations",2013,"Uncertainty in Artificial Intelligence - Proceedings of the 29th Conference, UAI 2013",22,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888177050&partnerID=40&md5=903207f610a6f4cf62d2a13dc8a7547e","Predictive State Representations (PSRs) are an expressive class of models for controlled stochastic processes. PSRs represent state as a set of predictions of future observable events. Because PSRs are defined entirely in terms of observable data, statistically consistent estimates of PSR parameters can be learned efficiently by manipulating moments of observed training data. Most learning algorithms for PSRs have assumed that actions and observations are finite with low cardinality. In this paper, we generalize PSRs to infinite sets of observations and actions, using the recent concept of Hilbert space embeddings of distributions. The essence is to represent the state as one or more nonparametric conditional embedding operators in a Reproducing Kernel Hilbert Space (RKHS) and leverage recent work in kernel methods to estimate, predict, and update the representation. We show that these Hilbert space embeddings of PSRs are able to gracefully handle continuous actions and observations, and that our learned models outperform competing system identification algorithms on several prediction benchmarks.",,"Cardinalities; Competing systems; Continuous actions; Controlled stochastic process; Kernel methods; Non-parametric; Predictive state representation; Reproducing Kernel Hilbert spaces; Artificial intelligence; Computation theory; Hilbert spaces; Learning algorithms; Random processes; Vector spaces",Conference Paper,Scopus,2-s2.0-84888177050
"Riniker S., Fechner N., Landrum G.A.","Heterogeneous classifier fusion for ligand-based virtual screening: Or, how decision making by committee can be a good thing",2013,"Journal of Chemical Information and Modeling",22,10.1021/ci400466r,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888627544&doi=10.1021%2fci400466r&partnerID=40&md5=0d5ae4b3b64bd5fc96ab21d2d9c33e95","The concept of data fusion - the combination of information from different sources describing the same object with the expectation to generate a more accurate representation - has found application in a very broad range of disciplines. In the context of ligand-based virtual screening (VS), data fusion has been applied to combine knowledge from either different active molecules or different fingerprints to improve similarity search performance. Machine-learning (ML) methods based on fusion of multiple homogeneous classifiers, in particular random forests, have also been widely applied in the ML literature. The heterogeneous version of classifier fusion - fusing the predictions from different model types - has been less explored. Here, we investigate heterogeneous classifier fusion for ligand-based VS using three different ML methods, RF, naÏve Bayes (NB), and logistic regression (LR), with four 2D fingerprints, atom pairs, topological torsions, RDKit fingerprint, and circular fingerprint. The methods are compared using a previously developed benchmarking platform for 2D fingerprints which is extended to ML methods in this article. The original data sets are filtered for difficulty, and a new set of challenging data sets from ChEMBL is added. Data sets were also generated for a second use case: starting from a small set of related actives instead of diverse actives. The final fused model consistently outperforms the other approaches across the broad variety of targets studied, indicating that heterogeneous classifier fusion is a very promising approach for ligand-based VS. The new data sets together with the adapted source code for ML methods are provided in the Supporting Information. © 2013 American Chemical Society.",,"Active molecules; Benchmarking platforms; Classifier fusion; Heterogeneous classifiers; Logistic regressions; Machine-learning; Similarity search; Virtual Screening; Data fusion; Decision trees; Ligands; ligand; protein; algorithm; article; artificial intelligence; Bayes theorem; chemical database; chemical structure; chemistry; computer interface; data mining; decision making; drug antagonism; drug potentiation; high throughput screening; quality control; statistical model; statistics; Algorithms; Artificial Intelligence; Bayes Theorem; Benchmarking; Data Mining; Databases, Chemical; Decision Making; High-Throughput Screening Assays; Ligands; Logistic Models; Models, Molecular; Proteins; User-Computer Interface",Article,Scopus,2-s2.0-84888627544
"Alizadegan A., Asady B., Ahmadpour M.","Two modified versions of artificial bee colony algorithm",2013,"Applied Mathematics and Computation",22,10.1016/j.amc.2013.09.012,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887087771&doi=10.1016%2fj.amc.2013.09.012&partnerID=40&md5=38d08d9fdcc1c273546ebdbe06b51b07","Artificial Bee Colony (ABC) algorithm is one of the most recently introduced swarm-based algorithms used in optimization problems. ABC simulates the intelligent foraging behavior of a honeybee swarm. In this paper, two aspects of ABC algorithm are modified and new configurations are used. The modified versions are tested on some well-known benchmark functions. Results show that the new changes have positive effects on the performance of ABC algorithm. © 2013 Published by Elsevier Inc.","Artificial bee colony algorithm; Numerical function optimization; Optimization algorithms; Swarm intelligence; Unconstrained optimization","Artificial bee colony algorithms; Numerical function optimization; Optimization algorithms; Swarm Intelligence; Unconstrained optimization; Artificial intelligence; Optimization; Evolutionary algorithms",Article,Scopus,2-s2.0-84887087771
"Boureanu I., Mitrokotsa A., Vaudenay S.","Secure and lightweight distance-bounding",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",22,10.1007/978-3-642-40392-7_8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886770905&doi=10.1007%2f978-3-642-40392-7_8&partnerID=40&md5=66e4c6f88e9c613a488528489a0f172d","Distance-bounding is a practical solution aiming to prevent relay attacks. The main challenge when designing such protocols is maintaining their inexpensive cryptographic nature, whilst being able to protect against as many, if not all, of the classical threats posed in their context. Moreover, in distance-bounding, some subtle security shortcomings related to the PRF (pseudorandom function) assumption and ingenious attack techniques based on observing verifiers' outputs have recently been put forward. Also, the recent terrorist-fraud by Hancke somehow recalls once more the need to account for noisy communications in the security analysis of distance-bounding. In this paper, we attempt to incorporate the lessons taught by these new developments in our distance-bounding protocol design. The result is a new class of protocols, with increasing levels of security, accommodating the latest advances; at the same time, we preserve the lightweight nature of the design throughout the whole class. © 2013 Springer-Verlag.",,"Distance-bounding protocols; Practical solutions; Pseudo-random functions; Relay attack; Security analysis; Artificial intelligence; Computer science; Terrorism",Conference Paper,Scopus,2-s2.0-84886770905
"Hullman J., Drucker S., Henry Riche N., Lee B., Fisher D., Adar E.","A deeper understanding of sequence in narrative visualization",2013,"IEEE Transactions on Visualization and Computer Graphics",22,10.1109/TVCG.2013.119,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885391881&doi=10.1109%2fTVCG.2013.119&partnerID=40&md5=c82fd5732367f9b5a07fcd368d7ea284","Conveying a narrative with visualizations often requires choosing an order in which to present visualizations. While evidence exists that narrative sequencing in traditional stories can affect comprehension and memory, little is known about how sequencing choices affect narrative visualization. We consider the forms and reactions to sequencing in narrative visualization presentations to provide a deeper understanding with a focus on linear, 'slideshow-style' presentations. We conduct a qualitative analysis of 42 professional narrative visualizations to gain empirical knowledge on the forms that structure and sequence take. Based on the results of this study we propose a graph-driven approach for automatically identifying effective sequences in a set of visualizations to be presented linearly. Our approach identifies possible transitions in a visualization set and prioritizes local (visualization-to- visualization) transitions based on an objective function that minimizes the cost of transitions from the audience perspective. We conduct two studies to validate this function. We also expand the approach with additional knowledge of user preferences for different types of local transitions and the effects of global sequencing strategies on memory, preference, and comprehension. Our results include a relative ranking of types of visualization transitions by the audience perspective and support for memory and subjective rating benefits of visualization sequences that use parallelism as a structural device. We discuss how these insights can guide the design of narrative visualization and systems that support optimization of visualization sequence. © 2013 IEEE.","Data storytelling; narrative structure; narrative visualization","Additional knowledge; Cost of transition; Data storytelling; Empirical knowledge; Narrative structures; Objective functions; Qualitative analysis; Support optimizations; Data visualization; Visualization; algorithm; article; artificial intelligence; comprehension; computer graphics; computer interface; human; methodology; multimodal imaging; pattern recognition; physiology; reproducibility; sensitivity and specificity; verbal communication; vision; comprehension; multimodal imaging; pattern recognition; physiology; procedures; vision; Algorithms; Artificial Intelligence; Comprehension; Computer Graphics; Humans; Multimodal Imaging; Narration; Pattern Recognition, Visual; Reproducibility of Results; Sensitivity and Specificity; User-Computer Interface; Visual Perception; Algorithms; Artificial Intelligence; Comprehension; Computer Graphics; Humans; Multimodal Imaging; Narration; Pattern Recognition, Visual; Reproducibility of Results; Sensitivity and Specificity; User-Computer Interface; Visual Perception",Article,Scopus,2-s2.0-84885391881
"Hu Y., Du J., Zhang X., Hao X., Ngai E.W.T., Fan M., Liu M.","An integrative framework for intelligent software project risk planning",2013,"Decision Support Systems",22,10.1016/j.dss.2012.12.029,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884208630&doi=10.1016%2fj.dss.2012.12.029&partnerID=40&md5=69ae0cf82c28442ed8a2eba7ee4c504e","Software projects have inherent uncertainties and risks. Social software projects suffer even more requirement changes and require more attention to risk management. Risk analysis and planning are complex, making it difficult to manage risks effectively through subjective judgment. At present, ample empirical research on intelligent decision-support models for risk analysis in software projects exists. However, to the best of our knowledge, empirical models for software project risk planning, or those related to integrative software risk analysis and planning are not available. Thus, the current study proposes an integrative framework for intelligent software project risk planning (IF-ISPRP) to help in minimizing the impacts of project risks and achieving a better foreseeable project outcome. IF-ISPRP includes two core components, namely, risk analysis module and risk planning module. The risk analysis module is to predict whether a project will be successful or not. The risk planning module is to produce a cost-minimal action set for risk control based on the risk analysis module. For integrative risk analysis and planning, we propose a novel many-to-many actionable knowledge discovery (MMAKD) method for complex risk planning. We also apply the framework on a social media platform project, Guangzhou Wireless City, and demonstrate how the model can generate a cost-minimal action set to mitigate the project risk. The risk-control actions found may help develop strategies on mitigating the risks of other social software projects. We hope that the proposed framework will provide an intelligent decision-support tool for project stakeholders to effectively control project risks by integrating risk analysis and planning. © 2013 Elsevier B.V.","Actionable knowledge discovery; Decision support systems; Social media platform project; Software project risk analysis and planning","Actionable knowledge discoveries; Decision support models; Decision support tools; Integrative framework; Intelligent software; Project stakeholders; Social media platforms; Software project risks; Artificial intelligence; Decision support systems; File editors; Project management; Risk management; Risk analysis",Article,Scopus,2-s2.0-84884208630
"Gollub T., Potthast M., Beyer A., Busse M., Rangel F., Rosso P., Stamatatos E., Stein B.","Recent trends in digital text forensics and its evaluation: Plagiarism detection, author identification, and author profiling",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",22,10.1007/978-3-642-40802-1_28,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886396350&doi=10.1007%2f978-3-642-40802-1_28&partnerID=40&md5=81b59f66503ed66b9ca6979f806a92fe","This paper outlines the concepts and achievements of our evaluation lab on digital text forensics, PAN 13, which called for original research and development on plagiarism detection, author identification, and author profiling. We present a standardized evaluation framework for each of the three tasks and discuss the evaluation results of the altogether 58 submitted contributions. For the first time, instead of accepting the output of software runs, we collected the softwares themselves and run them on a computer cluster at our site. As evaluation and experimentation platform we use TIRA, which is being developed at the Webis Group in Weimar. TIRA can handle large-scale software submissions by means of virtualization, sandboxed execution, tailored unit testing, and staged submission. In addition to the achieved evaluation results, a major achievement of our lab is that we now have the largest collection of state-of-the-art approaches with regard to the mentioned tasks for further analysis at our disposal. © 2013 Springer-Verlag.",,"Author identification; Computer clusters; Evaluation framework; Evaluation results; Experimentation platforms; Plagiarism detection; Research and development; State-of-the-art approach; Artificial intelligence; Computer science; Intellectual property",Conference Paper,Scopus,2-s2.0-84886396350
"Garg A., Sriram S., Tai K.","Empirical analysis of model selection criteria for genetic programming in modeling of time series system",2013,"Proceedings of the 2013 IEEE Conference on Computational Intelligence for Financial Engineering and Economics, CIFEr 2013 - 2013 IEEE Symposium Series on Computational Intelligence, SSCI 2013",22,10.1109/CIFEr.2013.6611702,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886052287&doi=10.1109%2fCIFEr.2013.6611702&partnerID=40&md5=f0f5b3b4bad574278ac6314bf6c7b18e","Genetic programming (GP) and its variants have been extensively applied for modeling of the stock markets. To improve the generalization ability of the model, GP have been hybridized with its own variants (gene expression programming (GEP), multi expression programming (MEP)) or with the other methods such as neural networks and boosting. The generalization ability of the GP model can also be improved by an appropriate choice of model selection criterion. In the past, several model selection criteria have been applied. In addition, data transformations have significant impact on the performance of the GP models. The literature reveals that few researchers have paid attention to model selection criterion and data transformation while modeling stock markets using GP. The objective of this paper is to identify the most appropriate model selection criterion and transformation that gives better generalized GP models. Therefore, the present work will conduct an empirical analysis to study the effect of three model selection criteria across two data transformations on the performance of GP while modeling the stock indexed in the New York Stock Exchange (NYSE). It was found that FPE criteria have shown a better fit for the GP model on both data transformations as compared to other model selection criteria. © 2013 IEEE.","fitness function; genetic programming; model selection; stock market","Fitness functions; Gene expression programming; Generalization ability; Model Selection; Model selection criteria; Multi expression programming; New York Stock Exchange; Stock market; Artificial intelligence; Commerce; Finance; Genetic programming; Economics",Conference Paper,Scopus,2-s2.0-84886052287
"De Almeida Falbo R., Barcellos M.P., Nardi J.C., Guizzardi G.","Organizing ontology design patterns as ontology pattern languages",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",22,10.1007/978-3-642-38288-8-5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884999315&doi=10.1007%2f978-3-642-38288-8-5&partnerID=40&md5=c25842ad1eafac944ced7e066114af2e","Ontology design patterns have been pointed out as a promising approach for ontology engineering. The goal of this paper is twofold. Firstly, based on well-established works in Software Engineering, we revisit the notion of ontology patterns in Ontology Engineering to introduce the notion of ontology pattern language as a way to organize related ontology patterns. Secondly, we present an overview of a software process ontology pattern language. © 2013 Springer-Verlag Berlin Heidelberg.","ontology design patterns; ontology pattern language; software process ontology","Ontology design; Ontology engineering; Ontology patterns; Software process; Artificial intelligence; Computer science; Software design",Conference Paper,Scopus,2-s2.0-84884999315
"Dubno J.R., Eckert M.A., Lee F.-S., Matthews L.J., Schmiedt R.A.","Classifying human audiometric phenotypes of age-related hearing loss from animal models",2013,"JARO - Journal of the Association for Research in Otolaryngology",22,10.1007/s10162-013-0396-x,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884287238&doi=10.1007%2fs10162-013-0396-x&partnerID=40&md5=b8e26b25d6ccef37f5bf0b00f2b3f238","Age-related hearing loss (presbyacusis) has a complex etiology. Results from animal models detailing the effects of specific cochlear injuries on audiometric profiles may be used to understand the mechanisms underlying hearing loss in older humans and predict cochlear pathologies associated with certain audiometric configurations (""audiometric phenotypes""). Patterns of hearing loss associated with cochlear pathology in animal models were used to define schematic boundaries of human audiograms. Pathologies included evidence for metabolic, sensory, and a mixed metabolic + sensory phenotype; an older normal phenotype without threshold elevation was also defined. Audiograms from a large sample of older adults were then searched by a human expert for ""exemplars"" (best examples) of these phenotypes, without knowledge of the human subject demographic information. Mean thresholds and slopes of higher frequency thresholds of the audiograms assigned to the four phenotypes were consistent with the predefined schematic boundaries and differed significantly from each other. Significant differences in age, gender, and noise exposure history provided external validity for the four phenotypes. Three supervised machine learning classifiers were then used to assess reliability of the exemplar training set to estimate the probability that newly obtained audiograms exhibited one of the four phenotypes. These procedures classified the exemplars with a high degree of accuracy; classifications of the remaining cases were consistent with the exemplars with respect to average thresholds and demographic information. These results suggest that animal models of age-related hearing loss can be used to predict human cochlear pathology by classifying audiograms into phenotypic classifications that reflect probable etiologies for hearing loss in older humans. © 2013 Association for Research in Otolaryngology.","animal models; audiogram classification; endocochlear potential; metabolic presbyacusis; sensory presbyacusis; supervised machine learning classifiers","adult; age distribution; aged; article; audiometry; classifier; cochlea; disease classification; female; hearing impairment; human; machine learning; major clinical study; male; noise; nonhuman; phenotype; presbyacusis; priority journal; Aged; Aged, 80 and over; Animals; Artificial Intelligence; Audiometry, Pure-Tone; Auditory Threshold; Databases, Factual; Disease Models, Animal; Female; Hearing Loss, Noise-Induced; Humans; Male; Middle Aged; Phenotype; Presbycusis; Reproducibility of Results",Article,Scopus,2-s2.0-84884287238
"Sun C., Zhang T., Bao B.-K., Xu C., Mei T.","Discriminative exemplar coding for sign language recognition with kinect",2013,"IEEE Transactions on Cybernetics",22,10.1109/TCYB.2013.2265337,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890337900&doi=10.1109%2fTCYB.2013.2265337&partnerID=40&md5=ee7a4af18875fa75464827ba315ee21a","Sign language recognition is a growing research area in the field of computer vision. A challenge within it is to model various signs, varying with time resolution, visual manual appearance, and so on. In this paper, we propose a discriminative exemplar coding (DEC) approach, as well as utilizing Kinect sensor, to model various signs. The proposed DEC method can be summarized as three steps. First, a quantity of class-specific candidate exemplars are learned from sign language videos in each sign category by considering their discrimination. Then, every video of all signs is described as a set of similarities between frames within it and the candidate exemplars. Instead of simply using a heuristic distance measure, the similarities are decided by a set of exemplar-based classifiers through the multiple instance learning, in which a positive (or negative) video is treated as a positive (or negative) bag and those frames similar to the given exemplar in Euclidean space as instances. Finally, we formulate the selection of the most discriminative exemplars into a framework and simultaneously produce a sign video classifier to recognize sign. To evaluate our method, we collect an American sign language dataset, which includes approximately 2000 phrases, while each phrase is captured by Kinect sensor with color, depth, and skeleton information. Experimental results on our dataset demonstrate the feasibility and effectiveness of the proposed approach for sign language recognition. © 2013 IEEE.","Discriminative exemplar coding; Kinect sensor; Sign language recognition","American sign language; Discriminative exemplar coding; Distance measure; Euclidean spaces; Kinect sensors; Multiple instance learning; Sign Language recognition; Time resolution; Sensors; Data processing; actimetry; algorithm; article; artificial intelligence; automated pattern recognition; computer; computer simulation; computer system; equipment; human; image enhancement; methodology; recreation; sign language; three dimensional imaging; transducer; whole body imaging; automated pattern recognition; devices; procedures; three dimensional imaging; whole body imaging; Actigraphy; Algorithms; Artificial Intelligence; Computer Peripherals; Computer Simulation; Computer Systems; Humans; Image Enhancement; Imaging, Three-Dimensional; Pattern Recognition, Automated; Sign Language; Transducers; Video Games; Whole Body Imaging; Actigraphy; Algorithms; Artificial Intelligence; Computer Peripherals; Computer Simulation; Computer Systems; Humans; Image Enhancement; Imaging, Three-Dimensional; Pattern Recognition, Automated; Sign Language; Transducers; Video Games; Whole Body Imaging",Article,Scopus,2-s2.0-84890337900
"Legg P.A., Rosin P.L., Marshall D., Morgan J.E.","Improving accuracy and efficiency of mutual information for multi-modal retinal image registration using adaptive probability density estimation",2013,"Computerized Medical Imaging and Graphics",22,10.1016/j.compmedimag.2013.08.004,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888031500&doi=10.1016%2fj.compmedimag.2013.08.004&partnerID=40&md5=f8c213e48a43df8d2b4c00e6d65c7046","Mutual information (MI) is a popular similarity measure for performing image registration between different modalities. MI makes a statistical comparison between two images by computing the entropy from the probability distribution of the data. Therefore, to obtain an accurate registration it is important to have an accurate estimation of the true underlying probability distribution. Within the statistics literature, many methods have been proposed for finding the 'optimal' probability density, with the aim of improving the estimation by means of optimal histogram bin size selection. This provokes the common question of how many bins should actually be used when constructing a histogram. There is no definitive answer to this. This question itself has received little attention in the MI literature, and yet this issue is critical to the effectiveness of the algorithm. The purpose of this paper is to highlight this fundamental element of the MI algorithm. We present a comprehensive study that introduces methods from statistics literature and incorporates these for image registration. We demonstrate this work for registration of multi-modal retinal images: colour fundus photographs and scanning laser ophthalmoscope images. The registration of these modalities offers significant enhancement to early glaucoma detection, however traditional registration techniques fail to perform sufficiently well. We find that adaptive probability density estimation heavily impacts on registration accuracy and runtime, improving over traditional binning techniques. © 2013 Elsevier Ltd.","Histogramming; Image registration; Mutual information; Probability estimation","Adaptive probabilities; Histogramming; Mutual informations; Probability estimation; Registration techniques; Retinal image registrations; Scanning laser ophthalmoscope; Statistical comparisons; Algorithms; Estimation; Graphic methods; Image registration; Ophthalmology; Optimization; Probability distributions; Probability density function; algorithm; article; eye fundus; histogram; image processing; measurement accuracy; medical information system; mutual information; photography; priority journal; probability; retina image; scanning laser ophthalmoscope; Histogramming; Image registration; Mutual information; Probability estimation; Algorithms; Artificial Intelligence; Computer Simulation; Data Interpretation, Statistical; Glaucoma; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Models, Biological; Models, Statistical; Multimodal Imaging; Pattern Recognition, Automated; Reproducibility of Results; Retinoscopy; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-84888031500
"Mirsharif Q., Tajeripour F., Pourreza H.","Automated characterization of blood vessels as arteries and veins in retinal images",2013,"Computerized Medical Imaging and Graphics",22,10.1016/j.compmedimag.2013.06.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888026521&doi=10.1016%2fj.compmedimag.2013.06.003&partnerID=40&md5=2058df5638110f91e1b88b033c1cff32","In recent years researchers have found that alternations in arterial or venular tree of the retinal vasculature are associated with several public health problems such as diabetic retinopathy which is also the leading cause of blindness in the world. A prerequisite for automated assessment of subtle changes in arteries and veins, is to accurately separate those vessels from each other. This is a difficult task due to high similarity between arteries and veins in addition to variation of color and non-uniform illumination inter and intra retinal images. In this paper a novel structural and automated method is presented for artery/vein classification of blood vessels in retinal images. The proposed method consists of three main steps. In the first step, several image enhancement techniques are employed to improve the images. Then a specific feature extraction process is applied to separate major arteries from veins. Indeed, vessels are divided to smaller segments and feature extraction and vessel classification are applied to each small vessel segment instead of each vessel point. Finally, a post processing step is added to improve the results obtained from the previous step using structural characteristics of the retinal vascular network. In the last stage, vessel features at intersection and bifurcation points are processed for detection of arterial and venular sub trees. Ultimately vessel labels are revised by publishing the dominant label through each identified connected tree of arteries or veins. Evaluation of the proposed approach against two different datasets of retinal images including DRIVE database demonstrates the good performance and robustness of the method. The proposed method may be used for determination of arteriolar to venular diameter ratio in retinal images. Also the proposed method potentially allows for further investigation of labels of thinner arteries and veins which might be found by tracing them back to the major vessels. © 2013 Elsevier Ltd.","Artery/vein classification; Blood vessels; Feature extraction; Retinal images; Retinex image enhancement; Structural features","Arteriolar-to-venular diameter ratios; Automated assessment; Non-uniform illumination; Retinal image; Retinex; Structural characteristics; Structural feature; Vessel classification; Automation; Eye protection; Feature extraction; Forestry; Image enhancement; Ophthalmology; Blood vessels; algorithm; article; automation; blood vessel diameter; color; controlled study; data base; diabetic retinopathy; histogram; illumination; image analysis; image enhancement; image processing; ophthalmologist; priority journal; public health problem; publishing; retina artery; retina image; retina vein; task performance; Automation; Biometrics; Forestry; Image Analysis; Artery/vein classification; Blood vessels; Feature extraction; Retinal images; Retinex image enhancement; Structural features; Algorithms; Artificial Intelligence; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Retinal Artery; Retinal Vein; Retinoscopy; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-84888026521
"Holinski B.J., Everaert D.G., Mushahwar V.K., Stein R.B.","Real-time control of walking using recordings from dorsal root ganglia",2013,"Journal of Neural Engineering",22,10.1088/1741-2560/10/5/056008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885444348&doi=10.1088%2f1741-2560%2f10%2f5%2f056008&partnerID=40&md5=296d1a6fa3d7d042dca442fad2718640","Objective. The goal of this study was to decode sensory information from the dorsal root ganglia (DRG) in real time, and to use this information to adapt the control of unilateral stepping with a state-based control algorithm consisting of both feed-forward and feedback components. Approach. In five anesthetized cats, hind limb stepping on a walkway or treadmill was produced by patterned electrical stimulation of the spinal cord through implanted microwire arrays, while neuronal activity was recorded from the DRG. Different parameters, including distance and tilt of the vector between hip and limb endpoint, integrated gyroscope and ground reaction force were modelled from recorded neural firing rates. These models were then used for closed-loop feedback. Main results. Overall, firing-rate-based predictions of kinematic sensors (limb endpoint, integrated gyroscope) were the most accurate with variance accounted for >60% on average. Force prediction had the lowest prediction accuracy (48 ± 13%) but produced the greatest percentage of successful rule activations (96.3%) for stepping under closed-loop feedback control. The prediction of all sensor modalities degraded over time, with the exception of tilt. Significance. Sensory feedback from moving limbs would be a desirable component of any neuroprosthetic device designed to restore walking in people after a spinal cord injury. This study provides a proof-of-principle that real-time feedback from the DRG is possible and could form part of a fully implantable neuroprosthetic device with further development. © 2013 IOP Publishing Ltd.",,"Closed-loop feedback; Closed-loop feedback control; Dorsal root ganglia (DRG); Electrical stimulations; Ground reaction forces; Neuronal activities; Prediction accuracy; Spinal cord injuries (SCI); Algorithms; Biophysics; Gyroscopes; Reaction rates; Real time control; Sensory feedback; Forecasting; accuracy; action potential; animal experiment; ankle; article; cat; electrostimulation; female; ground reaction force; hip; knee; limb; male; motoneuron; nonhuman; passive movement; priority journal; range of motion; sensory feedback; spinal cord; spinal cord injury; spinal ganglion; treadmill; walking; Adaptation, Physiological; Algorithms; Animals; Artifacts; Artificial Intelligence; Biomechanical Phenomena; Biosensing Techniques; Cats; Computer Systems; Electric Stimulation; Female; Ganglia, Spinal; Hindlimb; Male; Models, Neurological; Neural Prostheses; Neurons, Afferent; Signal Processing, Computer-Assisted; Walking",Article,Scopus,2-s2.0-84885444348
"Bradshaw J.M., Hoffman R.R., Woods D.D., Johnson M.","The seven deadly myths of 'autonomous systems'",2013,"IEEE Intelligent Systems",22,10.1109/MIS.2013.70,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884181510&doi=10.1109%2fMIS.2013.70&partnerID=40&md5=01d5781780b36085c3b346040ec1b20f","As designers conceive and implement what are commonly (but mistakenly) called autonomous systems, they adhere to certain myths of autonomy that are not only damaging in their own right, but also by their continued propagation. This article busts such myths and gives reasons why each of these myths should be called out and cast aside. © 2013 IEEE.","autonomous systems; human-centered computing; intelligent systems","Autonomous systems; Human-centered computing; Artificial intelligence; Intelligent systems",Article,Scopus,2-s2.0-84884181510
"Forbus K.D.","The Qualitative Process Engine",2013,"Readings in Qualitative Reasoning About Physical Systems",22,10.1016/B978-1-4832-1447-4.50017-1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944060300&doi=10.1016%2fB978-1-4832-1447-4.50017-1&partnerID=40&md5=36d7bd32405426d9008014c2411aef4a","Efficient qualitative simulators are crucial to continued progress in qualitative physics. Assumption-based truth maintenance systems (ATMS) were developed in part to simplify writing such programs. This paper identifies several general abstractions for organizing ATMS-based problem-solvers which are especially useful for envisioning. In particular, we describe the many-worlds database, which avoids complex temporal reference schemes; how to organize problem-solving into justify/assume/interpret cycles which successively construct and extend partial solutions; and closed-world tables, which provide a mechanism for making closed-world assumptions. We sketch the design of the Qualitative Process Engine, QPE, an implementation of Qualitative Process theory, to illustrate the utility of these abstractions. On the basis of our experience in developing QPE and analyzing its performance, we draw some general conclusions about the advantages and disadvantages of assumption-based truth maintenance systems. © 1990 Morgan Kaufmann Publishers, Inc. Published by Elsevier Inc. All rights reserved.",,"Abstracting; Artificial intelligence; Assumption-based truth maintenance systems; Closed world assumption; Problem solvers; Qualitative physics; Qualitative process; Engines",Book Chapter,Scopus,2-s2.0-84944060300
"Romano M., Kapelan Z., Savić D.A.","Geostatistical techniques for approximate location of pipe burst events in water distribution systems",2013,"Journal of Hydroinformatics",22,10.2166/hydro.2013.094,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882735696&doi=10.2166%2fhydro.2013.094&partnerID=40&md5=6de3a2c0e6d3af58fb9cc6401250b368","This paper focusses on the customisation and further enhancement of the recently developed data-driven methodology for the automated near real-time detection of pipe bursts and other (e.g. sensor faults) events at the district metered area (DMA) level. Assuming the availability of pressure/flow data from an increased number of sensors deployed in a DMA, the aim is to: (i) overcome the limitations of the probabilistic inference engine when dealing with the increased data availability; and (ii) exploit the event information resulting from the analysis of the larger number of DMA signals for determining the approximate location of the pipe burst events within the DMA. This is achieved by making use of a multivariate Gaussian mixtures-based graphical model and geostatistical techniques. The novel detection and location methodology is demonstrated and tested on a series of simulated pipe burst events that were performed by opening hydrants in a real-life DMA in the UK. The results obtained illustrate that the new methodology can successfully determine the approximate location of pipe bursts within a DMA (in addition to detecting them in a fast and reliable manner). The performance comparison of several geostatistical techniques shows that the Ordinary Cokriging technique outperforms all other techniques tested. © IWA Publishing 2013.","Artificial intelligence techniques; Burst detection and location; Geostatistical techniques",,Article,Scopus,2-s2.0-84882735696
"Zhou X.-D., Wang D.-H., Tian F., Liu C.-L., Nakagawa M.","Handwritten Chinese/Japanese text recognition using semi-Markov conditional random fields",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",22,10.1109/TPAMI.2013.49,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883165322&doi=10.1109%2fTPAMI.2013.49&partnerID=40&md5=f108c87299c1386d2f341a8bbee182b4","This paper proposes a method for handwritten Chinese/Japanese text (character string) recognition based on semi-Markov conditional random fields (semi-CRFs). The high-order semi-CRF model is defined on a lattice containing all possible segmentation-recognition hypotheses of a string to elegantly fuse the scores of candidate character recognition and the compatibilities of geometric and linguistic contexts by representing them in the feature functions. Based on given models of character recognition and compatibilities, the fusion parameters are optimized by minimizing the negative log-likelihood loss with a margin term on a training string sample set. A forward-backward lattice pruning algorithm is proposed to reduce the computation in training when trigram language models are used, and beam search techniques are investigated to accelerate the decoding speed. We evaluate the performance of the proposed method on unconstrained online handwritten text lines of three databases. On the test sets of databases CASIA-OLHWDB (Chinese) and TUAT Kondate (Japanese), the character level correct rates are 95.20 and 95.44 percent, and the accurate rates are 94.54 and 94.55 percent, respectively. On the test set (online handwritten texts) of ICDAR 2011 Chinese handwriting recognition competition, the proposed method outperforms the best system in competition. © 1979-2012 IEEE.","beam search; Character string recognition; lattice pruning; semi-Markov conditional random field","Beam search; Character strings; Chinese handwriting recognition; Conditional random field; Feature function; lattice pruning; On-line handwritten text; Pruning algorithms; Computational linguistics; Image segmentation; Random processes; Character recognition; algorithm; article; artificial intelligence; Asian; automated pattern recognition; computer assisted diagnosis; computer simulation; handwriting; human; image enhancement; image subtraction; language; methodology; probability; reproducibility; sensitivity and specificity; statistical model; Asian continental ancestry group; automated pattern recognition; computer assisted diagnosis; procedures; Algorithms; Artificial Intelligence; Asian Continental Ancestry Group; Computer Simulation; Handwriting; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Language; Markov Chains; Models, Statistical; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique; Algorithms; Artificial Intelligence; Asian Continental Ancestry Group; Computer Simulation; Handwriting; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Language; Markov Chains; Models, Statistical; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-84883165322
"Sharma H., Bansal J.C., Arya K.V.","Opposition based lévy flight artificial bee colony",2013,"Memetic Computing",22,10.1007/s12293-012-0104-0,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881614951&doi=10.1007%2fs12293-012-0104-0&partnerID=40&md5=5bfb1165023d25107af5761f53c0b5d8","Artificial Bee Colony (ABC) is a well known optimization approach to solve nonlinear and complex problems. It is relatively a simple and recent population based probabilistic approach for global optimization. Similar to other population based algorithms, ABC is also computationally expensive due to its slow nature of search process. The solution search equation of ABC is significantly influenced by a random quantity which helps in exploration at the cost of exploitation of the search space. In the solution search equation of ABC due to the large step size the chance of skipping the true solution is high. Therefore, in this paper, to balance the diversity and convergence capability of the ABC, Lévy Flight random walk based local search strategy is proposed and incorporated with ABC along with opposition based learning strategy. The proposed algorithm is named as Opposition Based Lévy Flight ABC. The experiments over 14 un-biased test problems of different complexities and five well known engineering optimization problems show that the proposed algorithm outperforms the basic ABC and its recent variants namely Gbest guided ABC, Best-So-Far ABC, and Modified ABC in most of the experiments. © 2012 Springer-Verlag Berlin Heidelberg.","Evolutionary computation; Lévy flight local search; Memetic algorithm; Swarm intelligence","Artificial bee colonies (ABC); Engineering optimization problems; Local search; Memetic algorithms; Opposition-based learning; Population-based algorithm; Probabilistic approaches; Swarm Intelligence; Artificial intelligence; Experiments; Global optimization; Optimization; Evolutionary algorithms",Article,Scopus,2-s2.0-84881614951
"Canny J., Zhao H.","Big data analytics with small footprint: Squaring the cloud",2013,"Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",22,10.1145/2487575.2487677,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013628326&doi=10.1145%2f2487575.2487677&partnerID=40&md5=05d5fce344b2431c80ef886f5566ec1b","This paper describes the BID Data Suite, a collection of hardware, software and design patterns that enable fast, large-scale data mining at very low cost. By co-designing all of these elements we achieve single-machine performance levels that equal or exceed reported cluster implementations for common benchmark problems. A key design criterion is rapid exploration of models, hence the system is interactive and primarily single-user. The elements of the suite are: (i) the data engine, a hardware design pattern that balances storage, CPU and GPU acceleration for typical data mining workloads, (ii) BIDMat, an interactive matrix library that integrates CPU and GPU acceleration and novel computa-Tional kernels (iii), BIDMach, a machine learning system that includes very efficient model optimizers, (iv) Butter- y mixing, a communication strategy that hides the latency of frequent model updates needed by fast optimizers and (v) Design patterns to improve performance of iterative up- date algorithms. We present several benchmark problems to show how the above elements combine to yield multiple orders-of-magnitude improvements for each problem. Copyright © 2013 ACM.","Cluster; Data mining; GPU; Machine learning; Toolkit","Artificial intelligence; Benchmarking; Data mining; Digital storage; Education; Graphics processing unit; Hardware; Iterative methods; Learning systems; Bench-mark problems; Cluster; Cluster implementation; Communication strategy; GPU accelerations; Improve performance; Single- machines; Toolkit; Big data",Conference Paper,Scopus,2-s2.0-85013628326
"Chen P., Li J., Wong L., Kuwahara H., Huang J.Z., Gao X.","Accurate prediction of hot spot residues through physicochemical characteristics of amino acid sequences",2013,"Proteins: Structure, Function and Bioinformatics",22,10.1002/prot.24278,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880667308&doi=10.1002%2fprot.24278&partnerID=40&md5=bf55a1bcd71e430c6e40fdcb05238ec2","Hot spot residues of proteins are fundamental interface residues that help proteins perform their functions. Detecting hot spots by experimental methods is costly and time-consuming. Sequential and structural information has been widely used in the computational prediction of hot spots. However, structural information is not always available. In this article, we investigated the problem of identifying hot spots using only physicochemical characteristics extracted from amino acid sequences. We first extracted 132 relatively independent physicochemical features from a set of the 544 properties in AAindex1, an amino acid index database. Each feature was utilized to train a classification model with a novel encoding schema for hot spot prediction by the IBk algorithm, an extension of the K-nearest neighbor algorithm. The combinations of the individual classifiers were explored and the classifiers that appeared frequently in the top performing combinations were selected. The hot spot predictor was built based on an ensemble of these classifiers and to work in a voting manner. Experimental results demonstrated that our method effectively exploited the feature space and allowed flexible weights of features for different queries. On the commonly used hot spot benchmark sets, our method significantly outperformed other machine learning algorithms and state-of-the-art hot spot predictors. The program is available at http://sfb.kaust.edu.sa/pages/software.aspx. © 2013 Wiley Periodicals, Inc.","Classification; Feature selection; Hot spot residue; Physicochemical characteristic; Protein-protein interaction","amino acid; accuracy; amino acid sequence; article; beta turn; classification algorithm; classifier; comparative study; controlled study; evaluation; hydrophobicity; k nearest neighbor; physical chemistry; prediction; priority journal; protein secondary structure; classification; feature selection; hot spot residue; physicochemical characteristic; protein-protein interaction; Algorithms; Amino Acid Sequence; Amino Acids; Animals; Artificial Intelligence; Databases, Protein; Drosophila; Drosophila Proteins; Humans; Juvenile Hormones; Models, Molecular; Protein Interaction Maps; Proteins; Receptors, Erythropoietin",Article,Scopus,2-s2.0-84880667308
"Salama K.M., Freitas A.A.","Learning Bayesian network classifiers using ant colony optimization",2013,"Swarm Intelligence",22,10.1007/s11721-013-0087-6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880778798&doi=10.1007%2fs11721-013-0087-6&partnerID=40&md5=2e9731d29ebc2b64a93309149667591a","Bayesian networks are knowledge representation tools that model the (in)dependency relationships among variables for probabilistic reasoning. Classification with Bayesian networks aims to compute the class with the highest probability given a case. This special kind is referred to as Bayesian network classifiers. Since learning the Bayesian network structure from a dataset can be viewed as an optimization problem, heuristic search algorithms may be applied to build high-quality networks in medium- or large-scale problems, as exhaustive search is often feasible only for small problems. In this paper, we present our new algorithm, ABC-Miner, and propose several extensions to it. ABC-Miner uses ant colony optimization for learning the structure of Bayesian network classifiers. We report extended computational results comparing the performance of our algorithm with eight other classification algorithms, namely six variations of well-known Bayesian network classifiers, cAnt-Miner for discovering classification rules and a support vector machine algorithm. © 2013 Springer Science+Business Media New York.","Ant colony optimization (ACO); Bayesian network classifiers; Classification; Data mining","Ant Colony Optimization (ACO); Bayesian network classifiers; Bayesian network structure; Classification algorithm; Heuristic search algorithms; Learning Bayesian networks; Probabilistic reasoning; Support vector machine algorithm; Ant colony optimization; Artificial intelligence; Classification (of information); Data mining; Heuristic algorithms; Knowledge representation; Miners; Bayesian networks",Article,Scopus,2-s2.0-84880778798
"Evans L., Lohse N., Summers M.","A fuzzy-decision-tree approach for manufacturing technology selection exploiting experience-based information",2013,"Expert Systems with Applications",22,10.1016/j.eswa.2013.05.047,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879493093&doi=10.1016%2fj.eswa.2013.05.047&partnerID=40&md5=b708ed88226986b084939e81cf36c1e8","Manufacturing technology selection is traditionally a human-driven approach where the trade-off of alternative manufacturing investments is steered by a group of experts. The problem is a semi-structured and subjective-based decision practice influenced by the experience and intuitive feeling of the decision-makers involved. This paper presents a distinct experience-based decision support system that uses factual information of historical decisions to calculate confidence factors for the successful adoption of potential technologies for a given set of requirements. A fuzzy-decision-tree algorithm is applied to provide a more objective approach given the evidence of previous manufacturing technology implementation cases. The model uses the information relationship of key technology decision variables, project requirements of an implemented technology case and the success outcome of a project to support decision problems. An empirical study was conducted at an aircraft manufacturer to support their technology decision for a typical medium complexity assembly investment project. The experimental analysis demonstrated encouraging results and practical viability of the approach. © 2013 Elsevier B.V. All rights reserved.","Data mining; Experience-based decision support; Fuzzy decision tree; Manufacturing technology selection","Aircraft manufacturers; Decision supports; Experimental analysis; Fuzzy decision trees; Manufacturing investment; Manufacturing technologies; Objective approaches; Potential technologies; Artificial intelligence; Data mining; Decision support systems; Decision theory; Forestry; Manufacture; Technology; Trees (mathematics); Decision making; Data Processing; Decision Making; Forestry; Manufacture; Mathematics; Technology; Trees",Article,Scopus,2-s2.0-84879493093
"Kumar A., Vembu S., Menon A.K., Elkan C.","Beam search algorithms for multilabel learning",2013,"Machine Learning",22,10.1007/s10994-013-5371-6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879304113&doi=10.1007%2fs10994-013-5371-6&partnerID=40&md5=28d469e318f00da7ee674bf170c9b90c","Multilabel learning is a machine learning task that is important for applications, but challenging. A recent method for multilabel learning called probabilistic classifier chains (PCCs) has several appealing properties. However, PCCs suffer from the computational issue that inference (i.e., predicting the label of an example) requires time exponential in the number of tags. Also, PCC accuracy is sensitive to the ordering of the tags while training. In this paper, we show how to use the classical technique of beam search to solve both these problems. Specifically, we show how to apply beam search to make inference tractable, and how to integrate beam search with training to determine a suitable tag ordering. Experimental results on a range of datasets show that the proposed improvements yield a state-of-the-art method for multilabel learning. © 2013 The Author(s).","Beam search; Multilabel classification; Probabilistic models; Structured prediction","Beam search; Beam search algorithms; Computational issues; Multi-label classifications; Probabilistic classifiers; Probabilistic models; State-of-the-art methods; Structured prediction; Artificial intelligence; Software engineering",Article,Scopus,2-s2.0-84879304113
"Bouchard M., Jousselme A.-L., Doré P.-E.","A proof for the positive definiteness of the Jaccard index matrix",2013,"International Journal of Approximate Reasoning",22,10.1016/j.ijar.2013.01.006,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875720103&doi=10.1016%2fj.ijar.2013.01.006&partnerID=40&md5=116402b26b9aaa4182a8b6c8babf6f2a","In this paper we provide a proof for the positive definiteness of the Jaccard index matrix used as a weighting matrix in the Euclidean distance between belief functions defined in Jousselme et al. [13]. The idea of this proof relies on the decomposition of the matrix into an infinite sum of positive semidefinite matrices. The proof is valid for any size of the frame of discernment but we provide an illustration for a frame of three elements. The Jaccard index matrix being positive definite guaranties that the associated Euclidean distance is a full metric and thus that a null distance between two belief functions implies that these belief functions are strictly identical. Crown Copyright © 2013 Published by Elsevier Inc. All rights reserved.","Belief functions; Definiteness; Distance; Evidence theory; Jaccard; Separability","Belief function; Definiteness; Distance; Evidence theories; Jaccard; Separability; Artificial intelligence; Software engineering; Uncertainty analysis",Article,Scopus,2-s2.0-84875720103
"Zheng S., Tang X., Song B., Lu S., Ye B.","Stable adaptive PI control for permanent magnet synchronous motor drive based on improved JITL technique",2013,"ISA Transactions",22,10.1016/j.isatra.2013.03.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879881819&doi=10.1016%2fj.isatra.2013.03.002&partnerID=40&md5=135ed8e6da8636b02377e82cec608ba1","In this paper, a stable adaptive PI control strategy based on the improved just-in-time learning (IJITL) technique is proposed for permanent magnet synchronous motor (PMSM) drive. Firstly, the traditional JITL technique is improved. The new IJITL technique has less computational burden and is more suitable for online identification of the PMSM drive system which is highly real-time compared to traditional JITL. In this way, the PMSM drive system is identified by IJITL technique, which provides information to an adaptive PI controller. Secondly, the adaptive PI controller is designed in discrete time domain which is composed of a PI controller and a supervisory controller. The PI controller is capable of automatically online tuning the control gains based on the gradient descent method and the supervisory controller is developed to eliminate the effect of the approximation error introduced by the PI controller upon the system stability in the Lyapunov sense. Finally, experimental results on the PMSM drive system show accurate identification and favorable tracking performance. © 2013 ISA. Published by Elsevier Ltd. All rights reserved.","Adaptive; Discrete time; Improved; JITL; PI control; PMSM","Adaptive; Discrete time; Improved; JITL; PI control; PMSM; Controllers; Electric drives; Synchronous motors; System stability; Two term control systems; Permanent magnets; algorithm; artificial intelligence; computer simulation; devices; feedback system; magnetism; theoretical model; transducer; article; equipment; magnetism; Algorithms; Artificial Intelligence; Computer Simulation; Feedback; Magnetics; Models, Theoretical; Transducers; Algorithms; Artificial Intelligence; Computer Simulation; Feedback; Magnetics; Models, Theoretical; Transducers",Article,Scopus,2-s2.0-84879881819
"Wörgötter F., Aksoy E.E., Krüger N., Piater J., Ude A., Tamosiunaite M.","A simple ontology of manipulation actions based on hand-object relations",2013,"IEEE Transactions on Autonomous Mental Development",22,10.1109/TAMD.2012.2232291,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879400409&doi=10.1109%2fTAMD.2012.2232291&partnerID=40&md5=0a5e71ffc82fdd80d6fb2ded75120ad0","Humans can perform a multitude of different actions with their hands (manipulations). In spite of this, so far there have been only a few attempts to represent manipulation types trying to understand the underlying principles. Here we first discuss how manipulation actions are structured in space and time. For this we use as temporal anchor points those moments where two objects (or hand and object) touch or un-touch each other during a manipulation. We show that by this one can define a relatively small tree-like manipulation ontology. We find less than 30 fundamental manipulations. The temporal anchors also provide us with information about when to pay attention to additional important information, for example when to consider trajectory shapes and relative poses between objects. As a consequence a highly condensed representation emerges by which different manipulations can be recognized and encoded. Examples of manipulations recognition and execution by a robot based on this representation are given at the end of this study. © 2013 IEEE.","Manipulation action; manipulation ontology; scene graph; semantic event chain","Condensed representations; Manipulation action; Relative pose; Scene graph; Semantic event; Space and time; Trajectory shapes; Underlying principles; Artificial intelligence; Software engineering; Semantics",Article,Scopus,2-s2.0-84879400409
"Boucher P., Atrash A., Kelouwani S., Honoré W., Nguyen H., Villemure J., Routhier F., Cohen P., Demers L., Forget R., Pineau J.","Design and validation of an intelligent wheelchair towards a clinically-functional outcome",2013,"Journal of NeuroEngineering and Rehabilitation",22,10.1186/1743-0003-10-58,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879045185&doi=10.1186%2f1743-0003-10-58&partnerID=40&md5=13314b4ad72290a718d33907e6c50ef3","Background: Many people with mobility impairments, who require the use of powered wheelchairs, have difficulty completing basic maneuvering tasks during their activities of daily living (ADL). In order to provide assistance to this population, robotic and intelligent system technologies have been used to design an intelligent powered wheelchair (IPW). This paper provides a comprehensive overview of the design and validation of the IPW. Methods. The main contributions of this work are three-fold. First, we present a software architecture for robot navigation and control in constrained spaces. Second, we describe a decision-theoretic approach for achieving robust speech-based control of the intelligent wheelchair. Third, we present an evaluation protocol motivated by a meaningful clinical outcome, in the form of the Robotic Wheelchair Skills Test (RWST). This allows us to perform a thorough characterization of the performance and safety of the system, involving 17 test subjects (8 non-PW users, 9 regular PW users), 32 complete RWST sessions, 25 total hours of testing, and 9 kilometers of total running distance. Results: User tests with the RWST show that the navigation architecture reduced collisions by more than 60% compared to other recent intelligent wheelchair platforms. On the tasks of the RWST, we measured an average decrease of 4% in performance score and 3% in safety score (not statistically significant), compared to the scores obtained with conventional driving model. This analysis was performed with regular users that had over 6 years of wheelchair driving experience, compared to approximately one half-hour of training with the autonomous mode. Conclusions: The platform tested in these experiments is among the most experimentally validated robotic wheelchairs in realistic contexts. The results establish that proficient powered wheelchair users can achieve the same level of performance with the intelligent command mode, as with the conventional command mode. © 2013 Boucher et al.; licensee BioMed Central Ltd.","Assistive robotics; Intelligent powered wheelchairs; Wheelchair skills test","adult; aged; artificial intelligence; computer interface; computer program; devices; disabled person; equipment design; female; human; male; middle aged; rehabilitation; robotics; treatment outcome; validation study; very elderly; wheelchair; article; disabled person; equipment; robotics; Adult; Aged; Aged, 80 and over; Artificial Intelligence; Disabled Persons; Equipment Design; Female; Humans; Male; Middle Aged; Robotics; Software; Treatment Outcome; User-Computer Interface; Wheelchairs; Adult; Aged; Aged, 80 and over; Artificial Intelligence; Disabled Persons; Equipment Design; Female; Humans; Male; Middle Aged; Robotics; Software; Treatment Outcome; User-Computer Interface; Wheelchairs",Article,Scopus,2-s2.0-84879045185
"Li H., Zhang H.","Ant colony optimization-based multi-mode scheduling under renewable and nonrenewable resource constraints",2013,"Automation in Construction",22,10.1016/j.autcon.2013.05.030,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884501489&doi=10.1016%2fj.autcon.2013.05.030&partnerID=40&md5=6d8bebd065a5a4cf9c63c3453b3f94ef","An ant colony optimization (ACO)-based methodology for solving the multi-mode resource-constrained project scheduling problem (MRCPSP) considering both renewable and nonrenewable resources is presented. With regard to the MRCPSP solution consisting of activity sequencing and mode selection, two levels of pheromones are proposed to guide search in the ACO algorithm. Correspondingly, two types of heuristic information and probabilities as well as related calculation algorithms are introduced. Nonrenewable resource-constraint and elitist-rank strategy are taken into account in updating the pheromones. The flowchart of the proposed ACO algorithm is described, where a serial schedule generation scheme is incorporated to transform an ACO solution into a feasible schedule. The parameter-selection and the resultant performance of the proposed ACO methodology are investigated through a series of computational experiments. It is expected to provide an effective alternative methodology for solving the MRCPSP by utilizing the ACO theory. © 2013 Elsevier B.V.","Ant colony optimization; Construction projects; Multi-mode scheduling; Renewable and nonrenewable resource constraints","Ant Colony Optimization (ACO); Calculation algorithms; Computational experiment; Construction projects; Multi-mode resource-constrained project scheduling problem; Multi-mode scheduling; Renewable and non-renewable resources; Serial schedule generation scheme; Ant colony optimization; Flow measurement; Heuristic algorithms; Hormones; Scheduling; Artificial intelligence",Article,Scopus,2-s2.0-84884501489
"Zhang X.-Y., Liu C.-L.","Writer adaptation with style transfer mapping",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",22,10.1109/TPAMI.2012.239,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876733279&doi=10.1109%2fTPAMI.2012.239&partnerID=40&md5=b385b6d15b7c017dbc0916871e54a4e8","Adapting a writer-independent classifier toward the unique handwriting style of a particular writer has the potential to significantly increase accuracy for personalized handwriting recognition. This paper proposes a novel framework of style transfer mapping (STM) for writer adaptation. The STM is a writer-specific class-independent feature transformation which has a closed-form solution. After style transfer mapping, the data of different writers are projected onto a style-free space, where the writer-independent classifier needs no change to classify the transformed data and can achieve significantly higher accuracy. The framework of STM can be combined with different types of classifiers for supervised, unsupervised, and semi-supervised adaptation, where writer-specific data can be either labeled or unlabeled and need not cover all classes. In this paper, we combine STM with the state-of-the-art classifiers for large-category Chinese handwriting recognition: learning vector quantization (LVQ) and modified quadratic discriminant function (MQDF). Experiments on the online Chinese handwriting database CASIA-OLHWDB demonstrate that STM-based adaptation is very efficient and effective in improving classification accuracy. Semi-supervised adaptation achieves the best performance, while unsupervised adaptation is even better than supervised adaptation. On handwritten text data, semi-supervised adaptation achieves error reduction rates 31.95 and 25.00 percent by LVQ and MQDF, respectively. © 1979-2012 IEEE.","handwriting recognition; style transfer mapping; Writer adaptation","Chinese handwriting recognition; Feature transformations; Handwriting recognition; Learning Vector Quantization; Modified quadratic discriminant functions; Online Chinese handwriting; Unsupervised adaptation; Writer adaptation; Vector quantization; Mapping; algorithm; artificial intelligence; automated pattern recognition; factual database; handwriting; human; image processing; procedures; article; automated pattern recognition; image processing; methodology; Algorithms; Artificial Intelligence; Databases, Factual; Handwriting; Humans; Image Processing, Computer-Assisted; Pattern Recognition, Automated; Algorithms; Artificial Intelligence; Databases, Factual; Handwriting; Humans; Image Processing, Computer-Assisted; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84876733279
"Klenk M., Molineaux M., Aha D.W.","Goal-driven autonomy for responding to unexpected events in strategy simulations",2013,"Computational Intelligence",22,10.1111/j.1467-8640.2012.00445.x,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877285929&doi=10.1111%2fj.1467-8640.2012.00445.x&partnerID=40&md5=5b55f114ff4c4006ae036bb516491b16","To operate autonomously in complex environments, an agent must monitor its environment and determine how to respond to new situations. To be considered intelligent, an agent should select actions in pursuit of its goals, and adapt accordingly when its goals need revision. However, most agents assume that their goals are given to them; they cannot recognize when their goals should change. Thus, they have difficulty coping with the complex environments of strategy simulations that are continuous, partially observable, dynamic, and open with respect to new objects. To increase intelligent agent autonomy, we are investigating a conceptual model for goal reasoning called Goal-Driven Autonomy (GDA), which allows agents to generate and reason about their goals in response to environment changes. Our hypothesis is that GDA enables an agent to respond more effectively to unexpected events in complex environments. We instantiate the GDA model in ARTUE (Autonomous Response to Unexpected Events), a domain-independent autonomous agent. We evaluate ARTUE on scenarios from two complex strategy simulations, and report on its comparative benefits and limitations. By employing goal reasoning, ARTUE outperforms an off-line planner and a discrepancy-based replanner on scenarios requiring reasoning about unobserved objects and facts and on scenarios presenting opportunities outside the scope of its current mission. © 2012 Wiley Periodicals, Inc.","autonomous agents; goal reasoning; goal-driven autonomy; on-line planning","Autonomous response; Complex environments; goal reasoning; Goal-driven; On-line planning; Response to environment; Strategy simulation; Unexpected events; Artificial intelligence; Computational methods; Autonomous agents",Article,Scopus,2-s2.0-84877285929
"Eichner M., Ferrari V.","Appearance sharing for collective human pose estimation",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",22,10.1007/978-3-642-37331-2_11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875876374&doi=10.1007%2f978-3-642-37331-2_11&partnerID=40&md5=80ea91ca4cc50aafcb2d8195d10803d8","While human pose estimation (HPE) techniques usually process each test image independently, in real applications images come in collections containing interdependent images. Often several images have similar backgrounds or show persons wearing similar clothing (foreground). We present a novel human pose estimation technique to exploit these dependencies by sharing appearance models between images. Our technique automatically determines which images in the collection should share appearance. We extend the state-of-the art HPE model of Yang and Ramanan to include our novel appearance sharing cues and demonstrate on the highly challenging Leeds Sports Poses dataset that they lead to better results than traditional single-image pose estimation. © 2013 Springer-Verlag.",,"Appearance models; Human pose estimations; Pose estimation; Real applications; State of the art; Test images; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84875876374
"Qiu Q., Wu Q., Bishop M., Pino R.E., Linderman R.W.","A parallel neuromorphic text recognition system and its implementation on a heterogeneous high-performance computing cluster",2013,"IEEE Transactions on Computers",22,10.1109/TC.2012.50,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875823054&doi=10.1109%2fTC.2012.50&partnerID=40&md5=a08dab9f20519d6212d48b690f6e4e28","Given the recent progress in the evolution of high-performance computing (HPC) technologies, the research in computational intelligence has entered a new era. In this paper, we present an HPC-based context-aware intelligent text recognition system (ITRS) that serves as the physical layer of machine reading. A parallel computing architecture is adopted that incorporates the HPC technologies with advances in neuromorphic computing models. The algorithm learns from what has been read and, based on the obtained knowledge, it forms anticipations of the word and sentence level context. The information processing flow of the ITRS imitates the function of the neocortex system. It incorporates large number of simple pattern detection modules with advanced information association layer to achieve perception and recognition. Such architecture provides robust performance to images with large noise. The implemented ITRS software is able to process about 16 to 20 scanned pages per second on the 500 trillion floating point operations per second (TFLOPS) Air Force Research Laboratory (AFRL)/Information Directorate (RI) Condor HPC after performance optimization. © 1968-2012 IEEE.","distributed architecture; Heterogeneous (hybrid) systems; machine learning; natural language interfaces","Air Force Research Laboratory; Distributed architecture; Floating point operations per seconds; Heterogeneous (hybrid) systems; High-performance computing clusters; Natural language interfaces; Parallel computing architecture; Perception and recognition; Artificial intelligence; Learning systems; Natural language processing systems; Network layers; Parallel architectures; Cluster computing",Article,Scopus,2-s2.0-84875823054
"Mónica Faria B., Vasconcelos S., Paulo Reis L., Lau N.","Evaluation of distinct input methods of an intelligent wheelchair in simulated and real environments: A performance and usability study",2013,"Assistive Technology",22,10.1080/10400435.2012.723297,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877932442&doi=10.1080%2f10400435.2012.723297&partnerID=40&md5=046f08dadeee9512915d4fe2eab8a056","This article focuses on evaluating the usability of an intelligent wheelchair (IW) in both real and simulated environments. The wheelchair is controlled at a high-level by a flexible multimodal interface, using voice commands, facial expressions, head movements and joystick as its main inputs. A quasi-experimental design was applied including a deterministic sample with a questionnaire that enabled to apply the System Usability Scale. The subjects were divided in two independent samples: 46 individuals performing the experiment with an IW in a simulated environment (28 using different commands in a sequential way and 18 with the liberty to choose the command); 12 individuals performing the experiment with a real IW. The main conclusion achieved by this study is that the usability of the IW in a real environment is higher than in the simulated environment. However, there were not statistical evidences to affirm that there are differences between the real and simulated wheelchairs in terms of safety and control. Also, most of users considered the multimodal way of driving the wheelchair very practical and satisfactory. Thus, it may be concluded that the multimodal interfaces enables very easy and safe control of the IW both in simulated and real environments. © 2013 Copyright 2013 RESNA.","adaptability; assistive robotics; assistive technology; human-machine interface; intelligent wheelchair; multimodal interface; powered wheelchairs","adaptability; Assistive robotics; Assistive technology; Human Machine Interface; Intelligent wheelchair; Multi-modal interfaces; Powered wheel chairs; Experiments; Human computer interaction; Intelligent robots; Interactive computer systems; Wheelchairs; Usability engineering; adult; article; artificial intelligence; clinical trial; comparative study; computer interface; computer simulation; environmental planning; equipment design; female; human; locomotion; male; microclimate; patient satisfaction; wheelchair; Adult; Artificial Intelligence; Computer Simulation; Environment Design; Environment, Controlled; Equipment Design; Female; Humans; Locomotion; Male; Patient Satisfaction; User-Computer Interface; Wheelchairs; Young Adult",Article,Scopus,2-s2.0-84877932442
"López A., Coello Coello C.A., Oyama A., Fujii K.","An alternative preference relation to deal with many-objective optimization problems",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",22,10.1007/978-3-642-37140-0_24,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875509932&doi=10.1007%2f978-3-642-37140-0_24&partnerID=40&md5=c04a109fcc748895a6fc7f6f2c56ba04","In this paper, we use an alternative preference relation that couples an achievement function and the ε-indicator in order to improve the scalability of a Multi-Objective Evolutionary Algorithm (moea) in many-objective optimization problems. The resulting algorithm was assessed using the Deb-Thiele-Laumanns-Zitzler (dtlz) and the Walking- Fish-Group (wfg) test suites. Our experimental results indicate that our proposed approach has a good performance even when using a high number of objectives. Regarding the dtlz test problems, their main difficulty was found to lie on the presence of dominance resistant solutions. In contrast, the hardness of wfg problems was not found to be significantly increased by adding more objectives. © 2013 Springer-Verlag.",,"Achievement functions; Many-objective optimizations; Multi objective evolutionary algorithms; Preference relation; Test problem; Artificial intelligence; Optimization",Conference Paper,Scopus,2-s2.0-84875509932
"Shen M., Chen W.-N., Zhang J., Chung H.S.-H., Kaynak O.","Optimal selection of parameters for nonuniform embedding of chaotic time series using ant colony optimization",2013,"IEEE Transactions on Cybernetics",22,10.1109/TSMCB.2012.2219859,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890432110&doi=10.1109%2fTSMCB.2012.2219859&partnerID=40&md5=3469ac69105a0ce988bee5bc69838254","The optimal selection of parameters for time-delay embedding is crucial to the analysis and the forecasting of chaotic time series. Although various parameter selection techniques have been developed for conventional uniform embedding methods, the study of parameter selection for nonuniform embedding is progressed at a slow pace. In nonuniform embedding, which enables different dimensions to have different time delays, the selection of time delays for different dimensions presents a difficult optimization problem with combinatorial explosion. To solve this problem efficiently, this paper proposes an ant colony optimization (ACO) approach. Taking advantage of the characteristic of incremental solution construction of the ACO, the proposed ACO for nonuniform embedding (ACO-NE) divides the solution construction procedure into two phases, i.e., selection of embedding dimension and selection of time delays. In this way, both the embedding dimension and the time delays can be optimized, along with the search process of the algorithm. To accelerate search speed, we extract useful information from the original time series to define heuristics to guide the search direction of ants. Three geometry- or model-based criteria are used to test the performance of the algorithm. The optimal embeddings found by the algorithm are also applied in time-series forecasting. Experimental results show that the ACO-NE is able to yield good embedding solutions from both the viewpoints of optimization performance and prediction accuracy. © 2012 IEEE.","Ant colony optimization (ACO); Attractor embedding; Nonuniform embedding; Phase space reconstruction; Time series","Ant Colony Optimization (ACO); Attractor embedding; Combinatorial explosion; Construction procedures; Incremental solutions; Nonuniform embedding; Phase space reconstruction; Time series forecasting; Ant colony optimization; Artificial intelligence; Phase space methods; Time delay; Time series; Algorithms",Article,Scopus,2-s2.0-84890432110
"Fouad S., Tino P., Raychaudhury S., Schneider P.","Incorporating privileged information through metric learning",2013,"IEEE Transactions on Neural Networks and Learning Systems",22,10.1109/TNNLS.2013.2251470,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877924976&doi=10.1109%2fTNNLS.2013.2251470&partnerID=40&md5=b1faf18c06deac380ba146a2fd0db7b9","In some pattern analysis problems, there exists expert knowledge, in addition to the original data involved in the classification process. The vast majority of existing approaches simply ignore such auxiliary (privileged) knowledge. Recently a new paradigm - learning using privileged information - was introduced in the framework of {\rm SVM}+. This approach is formulated for binary classification and, as typical for many kernel-based methods, can scale unfavorably with the number of training examples. While speeding up training methods and extensions of {\rm SVM}+ to multiclass problems are possible, in this paper we present a more direct novel methodology for incorporating valuable privileged knowledge in the model construction phase, primarily formulated in the framework of generalized matrix learning vector quantization. This is done by changing the global metric in the input space, based on distance relations revealed by the privileged information. Hence, unlike in {\rm SVM}+, any convenient classifier can be used after such metric modification, bringing more flexibility to the problem of incorporating privileged information during the training. Experiments demonstrate that the manipulation of an input space metric based on privileged data improves classification accuracy. Moreover, our methods can achieve competitive performance against the {\rm SVM}+ formulations. © 2012 IEEE.","Distance metric learning (DML); generalized matrix learning vector quantization (GMLVQ); information theoretic metric learning (ITML); learning using privileged information (LUPI)","Classification accuracy; Classification process; Competitive performance; Distance Metric Learning; Generalized Matrix Learning Vector Quantization (GMLVQ); Information Theoretic Metric Learning (ITML); learning using privileged information (LUPI); Learning Vector Quantization; Information theory; Information use; Vector quantization; Classification (of information); algorithm; artificial intelligence; biometry; factual database; human; information science; learning; Algorithms; Artificial Intelligence; Biometry; Databases, Factual; Humans; Information Theory; Learning",Article,Scopus,2-s2.0-84877924976
"Hoffman R.R., Johnson M., Bradshaw J.M., Underbrink A.","Trust in automation",2013,"IEEE Intelligent Systems",22,10.1109/MIS.2013.24,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874628136&doi=10.1109%2fMIS.2013.24&partnerID=40&md5=a784ef470142f05a35fb570af88b0268","This essay focuses on trust in the automation within macrocognitive work systems. The authors emphasize the dynamics of trust. They consider numerous different meanings or kinds of trust, and different modes of operation in which trust dynamics play a role. Their goal is to contribute to the development of a methodology for designing and analyzing collaborative human-centered work systems, a methodology that might promote both trust 'calibration' and appropriate reliance. The analysis suggests an ontology for what the authors call 'active exploration for trusting' (AET). © 2001-2011 IEEE.","active exploration; competence envelopes; dynamics; macrocognitive work systems; trust","Active explorations; competence envelopes; Dynamics of trusts; Modes of operation; trust; Trust dynamics; Work system; Artificial intelligence; Intelligent systems; Dynamics",Article,Scopus,2-s2.0-84874628136
"Caires L., Pérez J.A., Pfenning F., Toninho B.","Behavioral polymorphism and parametricity in session-based communication",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",22,10.1007/978-3-642-37036-6_19,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874399250&doi=10.1007%2f978-3-642-37036-6_19&partnerID=40&md5=de0370df02a17692982d39daa3f24121","We investigate a notion of behavioral genericity in the context of session type disciplines. To this end, we develop a logically motivated theory of parametric polymorphism, reminiscent of the Girard-Reynolds polymorphic λ-calculus, but casted in the setting of concurrent processes. In our theory, polymorphism accounts for the exchange of abstract communication protocols and dynamic instantiation of heterogeneous interfaces, as opposed to the exchange of data types and dynamic instantiation of individual message types. Our polymorphic session-typed process language satisfies strong forms of type preservation and global progress, is strongly normalizing, and enjoys a relational parametricity principle. Combined, our results confer strong correctness guarantees for communicating systems. In particular, parametricity is key to derive non-trivial results about internal protocol independence, a concurrent analogous of representation independence, and non-interference properties of modular, distributed systems. © 2013 Springer-Verlag.",,"Concurrent process; Data type; Distributed systems; Genericity; Heterogeneous interfaces; Internal protocols; Message types; Non interference; Non-trivial; Parametric polymorphism; Parametricity; Process languages; Relational parametricity; Session types; Strong form; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84874399250
"Zhou L.","Performance of corporate bankruptcy prediction models on imbalanced dataset: The effect of sampling methods",2013,"Knowledge-Based Systems",22,10.1016/j.knosys.2012.12.007,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873725663&doi=10.1016%2fj.knosys.2012.12.007&partnerID=40&md5=a52b5f4b71c3eca0592f48219119f456","Corporate bankruptcy prediction is very important for creditors and investors. Most literature improves performance of prediction models by developing and optimizing the quantitative methods. This paper investigates the effect of sampling methods on the performance of quantitative bankruptcy prediction models on real highly imbalanced dataset. Seven sampling methods and five quantitative models are tested on two real highly imbalanced datasets. A comparison of model performance tested on random paired sample set and real imbalanced sample set is also conducted. The experimental results suggest that the proper sampling method in developing prediction models is mainly dependent on the number of bankruptcies in the training sample set. © 2012 Elsevier B.V. All rights reserved.","Bankruptcy prediction; Classification; Imbalanced dataset; Oversampling; Undersampling","Bankruptcy prediction; Comparison of models; Imbalanced Data-sets; Imbalanced dataset; Over sampling; Paired sample; Prediction model; Quantitative method; Quantitative models; Sample sets; Sampling method; Training sample; Under-sampling; Artificial intelligence; Classification (of information); Software engineering; Mathematical models",Article,Scopus,2-s2.0-84873725663
"Liu S., Weng J., Zhao Y.","Efficient public key cryptosystem resilient to key leakage chosen ciphertext attacks",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",22,10.1007/978-3-642-36095-4_6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874322306&doi=10.1007%2f978-3-642-36095-4_6&partnerID=40&md5=f9e3fb141e59ae0a2a8722484f478508","Leakage-resilient public key encryption (PKE) schemes are designed to resist ""memory attacks"", i.e., the adversary recovers the cryptographic key in the memory adaptively, but subject to constraint that the total amount of leaked information about the key is bounded by some parameter λ. Among all the IND-CCA2 leakage-resilient PKE proposals, the leakage-resilient version of the Cramer-Shoup cryptosystem (CS-PKE), referred to as the KL-CS-PKE scheme proposed by Naor and Segev in Crypto09, is the most practical one. But, the key leakage parameter λ and plaintext length m of KL-CS-PKE are subject to λ + m ≤ log q - ω(log κ), where κ is security parameter and q is the prime order of the group on which the scheme is based. Such a dependence between λ and m is undesirable. For example, when λ (resp., m) approaches to log q, m (resp., λ) approaches to 0. In this paper, we designed a new variant of CS-PKE that is resilient to key leakage chosen ciphertext attacks. Our proposal is λ ≤ log q - ω(log κ) leakage-resilient, and the leakage parameter λ is independent of the plaintext space that has the constant size q (exactly the same as that in CS-PKE). The performance of our proposal is almost as efficient as the original CS-PKE. As far as we know, this is the first leakage-resilient CS-type cryptosystem whose plaintext length is independent of the key leakage parameter, and is also the most efficient IND-CCA2 PKE scheme resilient to up to log q - ω(log κ) leakage. © 2013 Springer-Verlag.",,"Chosen ciphertext attack; Constant sizes; Cryptographic key; Leakage parameters; Plaintext; Prime orders; Public key cryptosystems; Public-key encryption; Security parameters; Artificial intelligence; Public key cryptography",Conference Paper,Scopus,2-s2.0-84874322306
"Korol T.","Early warning models against bankruptcy risk for Central European and Latin American enterprises",2013,"Economic Modelling",22,10.1016/j.econmod.2012.11.017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871742115&doi=10.1016%2fj.econmod.2012.11.017&partnerID=40&md5=7824f6baa03e1399b41ce7d8372e730a","This article is devoted to the issue of forecasting the bankruptcy risk of enterprises in Latin America and Central Europe. The author has used statistical and soft computing methods to program the prediction models. It compares the effectiveness of twelve different early warning models for forecasting the bankruptcy risk of companies. In the research conducted, the author used data on 185 companies listed on the Warsaw Stock Exchange and 60 companies listed on Stock Exchange markets in Mexico, Argentina, Peru, Brazil and Chile. This population of firms was divided into learning and testing setdata. Each company was analyzed using the absolute values of 14 financial ratios and the dynamics of change of these ratios.The author's developed models are characterized by high efficiency. These studies are one of the world's first attempts at comparing differences in forecasting this phenomenon between the regions of Latin America and Central Europe. Additionally, a comparison of the effectiveness of discriminant analysis, decisional trees, and artificial neural networks models was made. © 2012 Elsevier B.V.","Artificial intelligence; Bankruptcy prediction; Early warning model; Financial crisis",,Article,Scopus,2-s2.0-84871742115
"Archambault D., Purchase H.C.","Mental map preservation helps user orientation in dynamic graphs",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",22,10.1007/978-3-642-36763-2_42,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874129042&doi=10.1007%2f978-3-642-36763-2_42&partnerID=40&md5=8e4025516735d9dde908485351000729","We present the results of a formal experiment that tests the ability of a participant to orient themselves in a dynamically evolving graph. Examples of these tasks include finding a specific location or route between two locations. We find that preserving the mental map for the tasks tested is significantly faster and produces fewer errors. As the number of targets increase, this result holds. © 2013 Springer-Verlag.",,"Dynamic graph; Evolving graphs; Formal Experiments; Mental maps; Specific location; User-orientation; Artificial intelligence; Drawing (graphics)",Conference Paper,Scopus,2-s2.0-84874129042
"Liu P.","The multi-attribute group decision making method based on the interval grey linguistic variables weighted aggregation operator",2013,"Journal of Intelligent and Fuzzy Systems",22,10.3233/IFS-2012-0572,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873572105&doi=10.3233%2fIFS-2012-0572&partnerID=40&md5=c1a9926a5266a7591c3ee90839f799d2","With respect to the multi-attribute group decision-making problems in which the attribute values and attribute weights take the form of the interval grey linguistic variables, the multi-attribute group decision making method based on the interval grey linguistic variables weighted aggregation operator is proposed. Firstly, the operation rules, the properties, and the comparing method of the interval grey linguistic variables are defined. Then some aggregation operators are defined, such as interval grey linguistic weighted aggregation (IGLWA) operator, interval grey linguistic ordered weighted aggregation (IGLOWA) operator, and interval grey linguistic hybrid weighted aggregation (IGLHWA) operator; the decision making methods based on these operators are proposed to solve the group decision making problems. Finally, a numerical example is used to illustrate the use of the proposed method. The result shows the approach is simple and effective. © 2013-IOS Press and the authors. All rights reserved.","Grey fuzzy number; interval grey linguistic variables; multi-attribute group decision making; the interval grey linguistic variables hybrid weighted aggregation operator","Aggregation operator; Attribute values; Attribute weight; Decision-making method; Fuzzy numbers; Group decision making problems; Interval grey linguistic variables; Multi-attribute group decision making; Numerical example; Operation rules; Ordered weighted aggregations; Artificial intelligence; Decision making; Fuzzy sets; Image processing; Mathematical operators; Linguistics",Article,Scopus,2-s2.0-84873572105
"Vink J.P., Leeuwen M.V., Deurzen C.V., Haan G.D.","Efficient nucleus detector in histopathology images",2013,"Journal of Microscopy",22,10.1111/jmi.12001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872779847&doi=10.1111%2fjmi.12001&partnerID=40&md5=5d0ee8bc1fd64d75f14cad06999af3f9","In traditional cancer diagnosis, (histo)pathological images of biopsy samples are visually analysed by pathologists. However, this judgment is subjective and leads to variability among pathologists. Digital scanners may enable automated objective assessment, improved quality and reduced throughput time. Nucleus detection is seen as the corner stone for a range of applications in automated assessment of (histo)pathological images. In this paper, we propose an efficient nucleus detector designed with machine learning. We applied colour deconvolution to reconstruct each applied stain. Next, we constructed a large feature set and modified AdaBoost to create two detectors, focused on different characteristics in appearance of nuclei. The proposed modification of AdaBoost enables inclusion of the computational cost of each feature during selection, thus improving the computational efficiency of the resulting detectors. The outputs of the two detectors are merged by a globally optimal active contour algorithm to refine the border of the detected nuclei. With a detection rate of 95% (on average 58 incorrectly found objects per field-of-view) based on 51 field-of-view images of Her2 immunohistochemistry stained breast tissue and a complete analysis in 1 s per field-of-view, our nucleus detector shows good performance and could enable a range of applications in automated assessment of (histo)pathological images. © 2012 The Authors Journal of Microscopy © 2012 Royal Microscopical Society.","Biomedical image processing; Feature extraction; Image segmentation; Machine learning; Nucleus segmentation; Object detection; Pathology","epidermal growth factor receptor 2; adaboost; algorithm; article; breast cancer; breast epithelium; cancer diagnosis; cell nucleus; color deconvolution; fibroblast; histopathology; human; image analysis; image processing; immunohistochemistry; lymphocyte; machine learning; priority journal; procedures; staining; workflow; Artificial Intelligence; Automation; Cell Nucleus; Histocytochemistry; Humans; Image Processing, Computer-Assisted; Immunohistochemistry; Microscopy; Neoplasms",Article,Scopus,2-s2.0-84872779847
"Li Y., Li-Byarlay H., Burns P., Borodovsky M., Robinson G.E., Ma J.","TrueSight: A new algorithm for splice junction detection using RNA-seq",2013,"Nucleic Acids Research",22,10.1093/nar/gks1311,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876388379&doi=10.1093%2fnar%2fgks1311&partnerID=40&md5=54fd794d142d20b62a14ea14eacc26cc","RNA-seq has proven to be a powerful technique for transcriptome profiling based on next-generation sequencing (NGS) technologies. However, due to the short length of NGS reads, it is challenging to accurately map RNA-seq reads to splice junctions (SJs), which is a critically important step in the analysis of alternative splicing (AS) and isoform construction. In this article, we describe a new method, called TrueSight, which for the first time combines RNA-seq read mapping quality and coding potential of genomic sequences into a unified model. The model is further utilized in a machine-learning approach to precisely identify SJs. Both simulations and real data evaluations showed that TrueSight achieved higher sensitivity and specificity than other methods. We applied TrueSight to new high coverage honey bee RNA-seq data to discover novel splice forms. We found that 60.3% of honey bee multi-exon genes are alternatively spliced. By utilizing gene models improved by TrueSight, we characterized AS types in honey bee transcriptome. We believe that TrueSight will be highly useful to comprehensively study the biology of alternative splicing. © 2013 The Author(s).",,"animal experiment; article; controlled study; gene mapping; genetic algorithm; genetic code; honeybee; intermethod comparison; machine learning; nonhuman; priority journal; RNA analysis; RNA sequence; sensitivity and specificity; sequence alignment; sequence analysis; splice junction; TrueSight algorithm; Algorithms; Alternative Splicing; Animals; Artificial Intelligence; Bees; Gene Expression Profiling; Genomics; High-Throughput Nucleotide Sequencing; Humans; Models, Genetic; RNA Splice Sites; Sequence Analysis, RNA; Apis mellifera",Article,Scopus,2-s2.0-84876388379
"Anghinolfi D., Paolucci M., Robba M., Taramasso A.C.","A dynamic optimization model for solid waste recycling",2013,"Waste Management",22,10.1016/j.wasman.2012.10.006,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874324212&doi=10.1016%2fj.wasman.2012.10.006&partnerID=40&md5=5570ff8ae19cbe996fbfbc28cedb2ba4","Recycling is an important part of waste management (that includes different kinds of issues: environmental, technological, economic, legislative, social, etc.). Differently from many works in literature, this paper is focused on recycling management and on the dynamic optimization of materials collection. The developed dynamic decision model is characterized by state variables, corresponding to the quantity of waste in each bin per each day, and control variables determining the quantity of material that is collected in the area each day and the routes for collecting vehicles. The objective function minimizes the sum of costs minus benefits. The developed decision model is integrated in a GIS-based Decision Support System (DSS). A case study related to the Cogoleto municipality is presented to show the effectiveness of the proposed model. From optimal results, it has been found that the net benefits of the optimized collection are about 2.5 times greater than the estimated current policy. © 2012 Elsevier Ltd.","Dynamic modelling; Environmental modelling; Optimization; Recycling; Waste management","Control variable; Decision models; Dynamic decision models; Dynamic optimization; Dynamic optimization models; Environmental modelling; Gis-based decision support systems; Materials collection; Objective functions; Optimal results; Recycling management; Solid waste recycling; State variables; Artificial intelligence; Decision support systems; Dynamic models; Environmental management; Geographic information systems; Models; Optimization; Solid wastes; Waste management; Recycling; decision support system; environmental monitoring; environmental policy; GIS; optimization; pollution control; recycling; solid waste; waste management; article; case study; cost benefit analysis; decision support system; dynamics; environmental planning; geographic distribution; pollution monitoring; priority journal; process optimization; quantitative analysis; recycling; solid waste management; statistical analysis; waste generation model; Decision Making; Geographic Information Systems; Italy; Models, Theoretical; Recycling; Solid Waste; Waste Management",Article,Scopus,2-s2.0-84874324212
"Schürer S.C., Muskal S.M.","Kinome-wide activity modeling from diverse public high-quality data sets",2013,"Journal of Chemical Information and Modeling",22,10.1021/ci300403k,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873024709&doi=10.1021%2fci300403k&partnerID=40&md5=32fccf5d4dfca438b761419d17fed05c","Large corpora of kinase small molecule inhibitor data are accessible to public sector research from thousands of journal article and patent publications. These data have been generated employing a wide variety of assay methodologies and experimental procedures by numerous laboratories. Here we ask the question how applicable these heterogeneous data sets are to predict kinase activities and which characteristics of the data sets contribute to their utility. We accessed almost 500 000 molecules from the Kinase Knowledge Base (KKB) and after rigorous aggregation and standardization generated over 180 distinct data sets covering all major groups of the human kinome. To assess the value of the data sets, we generated hundreds of classification and regression models. Their rigorous cross-validation and characterization demonstrated highly predictive classification and quantitative models for the majority of kinase targets if a minimum required number of active compounds or structure-activity data points were available. We then applied the best classifiers to compounds most recently profiled in the NIH Library of Integrated Network-based Cellular Signatures (LINCS) program and found good agreement of profiling results with predicted activities. Our results indicate that, although heterogeneous in nature, the publically accessible data sets are exceedingly valuable and well suited to develop highly accurate predictors for practical Kinome-wide virtual screening applications and to complement experimental kinase profiling. © 2012 American Chemical Society.",,"Active compounds; Cross validation; Data points; Data sets; Experimental procedure; Heterogeneous data; High quality; Journal articles; Kinase activity; Kinase targets; Knowledge base; Network-based; Public sector; Quantitative models; Regression model; Small molecule inhibitor; Structure-activity; Virtual Screening; Enzymes; Knowledge based systems; Molecules; Regression analysis; Classification (of information); protein kinase; article; artificial intelligence; genetics; genomics; human; human genome; metabolism; protein database; regression analysis; statistical model; Artificial Intelligence; Databases, Protein; Genome, Human; Genomics; Humans; Models, Statistical; Protein Kinases; Regression Analysis",Article,Scopus,2-s2.0-84873024709
"Lowd D., Rooshenas A.","Learning Markov networks with arithmetic circuits",2013,"Journal of Machine Learning Research",22,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897689061&partnerID=40&md5=4f281f59a83ea0a8224522c958888ce9","Markov networks are an effective way to represent complex probability distributions. However, learning their structure and parameters or using them to answer queries is typically intractable. One approach to making learning and inference tractable is to use approximations, such as pseudo-likelihood or approximate inference. An alternate approach is to use a restricted class of models where exact inference is always efficient. Previous work has explored low treewidth models, models with tree-structured features, and latent variable models. In this paper, we introduce ACMN, the first ever method for learning efficient Markov networks with arbitrary conjunctive features. The secret to ACMN's greater flexibility is its use of arithmetic circuits, a lineartime inference representation that can handle many high treewidth models by exploiting local structure. ACMN uses the size of the corresponding arithmetic circuit as a learning bias, allowing it to trade off accuracy and inference complexity. In experiments on 12 standard datasets, the tractable models learned by ACMN are more accurate than both tractable models learned by other algorithms and approximate inference in intractable models. Copyright 2013 by the authors.",,"Artificial intelligence; Complex networks; Economic and social effects; Inference engines; Integrating circuits; Markov processes; Networks (circuits); Probability distributions; Query processing; Reconfigurable hardware; Alternate approaches; Approximate inference; Arithmetic circuit; Exact inference; Latent variable models; Local structure; Markov networks; Pseudo-likelihood; Logic circuits",Conference Paper,Scopus,2-s2.0-84897689061
"Cruz-Roa A.A., Arevalo Ovalle J.E., Madabhushi A., González Osorio F.A.","A deep learning architecture for image representation, visual interpretability and automated basal-cell carcinoma cancer detection.",2013,"Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention",22,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897573990&partnerID=40&md5=dbd417adb7e951966e1e2b08e84b5bee","This paper presents and evaluates a deep learning architecture for automated basal cell carcinoma cancer detection that integrates (1) image representation learning, (2) image classification and (3) result interpretability. A novel characteristic of this approach is that it extends the deep learning architecture to also include an interpretable layer that highlights the visual patterns that contribute to discriminate between cancerous and normal tissues patterns, working akin to a digital staining which spotlights image regions important for diagnostic decisions. Experimental evaluation was performed on set of 1,417 images from 308 regions of interest of skin histopathology slides, where the presence of absence of basal cell carcinoma needs to be determined. Different image representation strategies, including bag of features (BOF), canonical (discrete cosine transform (DCT) and Haar-based wavelet transform (Haar)) and proposed learned-from-data representations, were evaluated for comparison. Experimental results show that the representation learned from a large histology image data set has the best overall performance (89.4% in F-measure and 91.4% in balanced accuracy), which represents an improvement of around 7% over canonical representations and 3% over the best equivalent BOF representation.",,"algorithm; article; artificial intelligence; automated pattern recognition; basal cell carcinoma; biopsy; computer assisted diagnosis; human; image enhancement; methodology; microscopy; pathology; reproducibility; sensitivity and specificity; skin tumor; Algorithms; Artificial Intelligence; Biopsy; Carcinoma, Basal Cell; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Microscopy; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Skin Neoplasms",Article,Scopus,2-s2.0-84897573990
"Barrett C., Deters M., De Moura L., Oliveras A., Stump A.","6 Years of SMT-COMP",2013,"Journal of Automated Reasoning",22,10.1007/s10817-012-9246-5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878563375&doi=10.1007%2fs10817-012-9246-5&partnerID=40&md5=6c3bb32da85cfa1987def416f0afe982","The annual Satisfiability Modulo Theories Competition (SMT-COMP) was initiated in 2005 in order to stimulate the advance of state-of-the-art techniques and tools developed by the Satisfiability Modulo Theories (SMT) community. This paper summarizes the first six editions of the competition. We present the evolution of the competition's organization and rules, show how the state of the art has improved over the course of the competition, and discuss the impact SMT-COMP has had on the SMT community and beyond. Additionally, we include an exhaustive list of all competitors, and present experimental results showing significant improvement in SMT solvers during these six years. Finally, we analyze to what extent the initial goals of the competition have been achieved, and sketch future directions for the competition. © 2012 Springer Science+Business Media B.V.","Competition; Experimental evaluation; SAT Modulo Theories","Exhaustive lists; Experimental evaluation; Sat modulo theories; Satisfiability modulo Theories; Smt solvers; State of the art; State-of-the-art techniques; Artificial intelligence; Automata theory; Competition; Software engineering; Formal logic",Article,Scopus,2-s2.0-84878563375
"El-Mounayri H., Dugla Z., Deng H.","Prediction of surface roughness in end milling using swarm intelligence",2013,"2003 IEEE Swarm Intelligence Symposium, SIS 2003 - Proceedings",22,10.1109/SIS.2003.1202272,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897703593&doi=10.1109%2fSIS.2003.1202272&partnerID=40&md5=fc3c76e17bbecaf90658b40406e24ac4","A new technique from EC (evolutionary computation), PSO (particle swarm optimization), is implemented to model the end milling process and predict the resulting surface roughness. Data is collected from CNC cutting experiments using DOE approach. The data is used for model calibration and validation. The inputs to the model consist of feed, speed and depth of cut while the output from the model is surface roughness. The model is validated through a comparison of the experimental values with their predicted counterparts. A good agreement is found. The proved technique opens the door for a new, simple and efficient approach that could be applied to the calibration of other empirical models of machining. © 2003 IEEE.","Calibration; Computer numerical control; Evolutionary computation; Feeds; Milling; Particle swarm optimization; Predictive models; Rough surfaces; Surface roughness; US Department of Energy","Artificial intelligence; Calibration; Computer control systems; Evolutionary algorithms; Feeding; Milling (machining); Particle swarm optimization (PSO); Computer numerical control; End-milling process; Experimental values; Model calibration and validation; Predictive models; PSO(particle swarm optimization); Rough surfaces; US Department of Energy; Surface roughness",Conference Paper,Scopus,2-s2.0-84897703593
"Liapis A., Yannakakis G.N., Togelius J.","Towards a generic method of evaluating game levels",2013,"Proceedings of the 9th AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, AIIDE 2013",22,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910061402&partnerID=40&md5=5e452f04ab8eadf313dc034803bd149e","This paper addresses the problem of evaluating the quality of game levels across different games and even genres, which is of key importance for making procedural content generation and assisted game design tools more generally applicable. Three game design patterns are identified for having high generality while being easily quantifiable: area control, exploration and balance. Formulas for measuring the extent to which a level includes these concepts are proposed, and evaluation functions are derived for levels in two different game genres: multiplayer strategy game maps and single-player roguelike dungeons. To illustrate the impact of these evaluation functions, and the similarity of impact across domains, sets of levels for each function are generated using a constrained genetic algorithm. The proposed measures can easily be extended to other game genres. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Artificial intelligence; Genetic algorithms; Area control; Evaluation function; Game design; Generic method; Multiplayers; Procedural content generations; Strategy games; Function evaluation",Conference Paper,Scopus,2-s2.0-84910061402
"Shaker M., Shaker N., Togelius J.","Evolving playable content for Cut the Rope through a simulation-based approach",2013,"Proceedings of the 9th AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, AIIDE 2013",22,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910062103&partnerID=40&md5=5fd113fc69c99fb991b95a861f41035c","In order to automatically generate high-quality game levels, one needs to be able to automatically verify that the levels are playable. The simulation-based approach to playability testing uses an artificial agent to play through the level, but building such an agent is not always an easy task and such an agent is not always readily available. We discuss this problem in the context of the physics-based puzzle game Cut the Rope, which features continuous time and state space, making several approaches such as exhaustive search and reactive agents inefficient. We show that a deliberative Prolog-based agent can be used to suggest all sensible moves at each state, which allows us to restrict the search space so that depth-first search for solutions become viable. This agent is successfully used to test playability in Ropossum, a level generator based on grammatical evolution. The method proposed in this paper is likely to be useful for a large variety of games with similar characteristics. Copyright c 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Artificial intelligence; Continuous time systems; Artificial agents; Depth first search; Grammatical evolution; High quality; Physics-based; Puzzle games; Reactive agent; Search spaces; Rope",Conference Paper,Scopus,2-s2.0-84910062103
"Du N., Song L., Woo H., Zha H.","Uncover topic-sensitive information diffusion networks",2013,"Journal of Machine Learning Research",22,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954202959&partnerID=40&md5=1799e69847b104a70996c6afd822e23e","Analyzing the spreading patterns of memes with respect to their topic distributions and the underlying diffusion network structures is an important task in social network analysis. This task in many cases becomes very challenging since the underlying diffusion networks are often hidden, and the topic specific transmission rates are unknown either. In this paper, we propose a continuous time model, TOPICCASCADE, for topicsensitive information diffusion networks, and infer the hidden diffusion networks and the topic dependent transmission rates from the observed time stamps and contents of cascades. One attractive property of the model is that its parameters can be estimated via a convex optimization which we solve with an efficient proximal gradient based block coordinate descent (BCD) algorithm. In both synthetic and real-world data, we show that our method significantly improves over the previous state-of-the-art models in terms of both recovering the hidden diffusion networks and predicting the transmission times of memes. Copyright 2013 by the authors.",,"Continuous time systems; Convex optimization; Block coordinate descents; Continuous time modeling; Diffusion networks; Information diffusion; Sensitive informations; State of the art; Topic distributions; Transmission rates; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84954202959
"Suk H.I., Shen D.","Deep learning-based feature representation for AD/MCI classification.",2013,"Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention",22,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897570840&partnerID=40&md5=5257b0c3b0b79d361ba4111cbbc46707","In recent years, there has been a great interest in computer-aided diagnosis of Alzheimer's Disease (AD) and its prodromal stage, Mild Cognitive Impairment (MCI). Unlike the previous methods that consider simple low-level features such as gray matter tissue volumes from MRI, mean signal intensities from PET, in this paper, we propose a deep learning-based feature representation with a stacked auto-encoder. We believe that there exist latent complicated patterns, e.g., non-linear relations, inherent in the low-level features. Combining latent information with the original low-level features helps build a robust model for AD/MCI classification with high diagnostic accuracy. Using the ADNI dataset, we conducted experiments showing that the proposed method is 95.9%, 85.0%, and 75.8% accurate for AD, MCI, and MCI-converter diagnosis, respectively.",,"algorithm; Alzheimer disease; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; human; image enhancement; methodology; mild cognitive impairment; multimodal imaging; reproducibility; sensitivity and specificity; Algorithms; Alzheimer Disease; Artificial Intelligence; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Mild Cognitive Impairment; Multimodal Imaging; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84897570840
"Ding W., Qin T., Zhang X.-D., Liu T.-Y.","Multi-armed bandit with budget constraint and variable costs",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",21,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893395808&partnerID=40&md5=a43707be0cf9abf4cf3270b13f057f90","We study the multi-armed bandit problems with budget constraint and variable costs (MAB-BV). In this setting, pulling an arm will receive a random reward together with a random cost, and the objective of an algorithm is to pull a sequence of arms in order to maximize the expected total reward with the costs of pulling those arms complying with a budget constraint. This new setting models many Internet applications (e.g., ad exchange, sponsored search, and cloud computing) in a more accurate manner than previous settings where the pulling of arms is either costless or with a fixed cost. We propose two UCB based algorithms for the new setting. The first algorithm needs prior knowledge about the lower bound of the expected costs when computing the exploration term. The second algorithm eliminates this need by estimating the minimal expected costs from empirical observations, and therefore can be applied to more real-world applications where prior knowledge is not available. We prove that both algorithms have nice learning abilities, with regret bounds of O(lnB). Furthermore, we show that when applying our proposed algorithms to a previous setting with fixed costs (which can be regarded as our special case), one can improve the previously obtained regret bound. Our simulation results on real-time bidding in ad exchange verify the effectiveness of the algorithms and are consistent with our theoretical analysis. © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Budget constraint; Expected costs; Internet application; Learning abilities; Multi armed bandit; Multi-armed bandit problem; Prior knowledge; Sponsored searches; Artificial intelligence; Budget control; Cost accounting; Costs; Algorithms",Conference Paper,Scopus,2-s2.0-84893395808
"Pfandler A., Rümmele S., Szeider S.","Backdoors to Abduction",2013,"IJCAI International Joint Conference on Artificial Intelligence",21,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063098&partnerID=40&md5=eafdd2f91ca9abdb8fdd1989284753b4","Abductive reasoning (or Abduction, for short) is among the most fundamental AI reasoning methods, with a broad range of applications, including fault diagnosis, belief revision, and automated planning. Unfortunately, Abduction is of high computational complexity; even propositional Abduction is Σ2P-complete and thus harder than NP and co-NP. This complexity barrier rules out the existence of a polynomial transformation to propositional satisfiability (SAT). In this work we use structural properties of the Abduction instance to break this complexity barrier. We utilize the problem structure in terms of small backdoor sets. We present fixed-parameter tractable transformations from Abduction to SAT, which make the power of today's SAT solvers available to Abduction.",,"Abductive reasoning; Automated planning; Backdoor sets; Belief revision; Polynomial transformations; Problem structure; Propositional satisfiability; Reasoning methods; Artificial intelligence; Automata theory",Conference Paper,Scopus,2-s2.0-84896063098
"Veit A., Xu Y., Zheng R., Chakraborty N., Sycara K.","Multiagent coordination for energy consumption scheduling in consumer cooperatives",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",21,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893346677&partnerID=40&md5=bd2efd46e2a7ea5a9ad9a50cc6ed93e6","A key challenge to create a sustainable and energy-efficient society is in making consumer demand adaptive to energy supply, especially renewable supply. In this paper, we propose a partially-centralized organization of consumers, namely, a consumer cooperative for purchasing electricity from the market. We propose a novel multiagent coordination algorithm to shape the energy consumption of the cooperative. In the cooperative, a central coordinator buys the electricity for the whole group and consumers make their own consumption decisions based on their private consumption constraints and preferences. To coordinate individual consumers under incomplete information, we propose an iterative algorithm in which a virtual price signal is sent by the coordinator to induce consumers to shift demand. We prove that our algorithm converges to the central optimal solution. Additionally we analyze the convergence rate of the algorithm via simulations on randomly generated instances. The results indicate scalability with respect to the number of agents and consumption slots. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Consumer demands; Convergence rates; Energy efficient; Energy supplies; Incomplete information; Iterative algorithm; Multi-agent coordinations; Optimal solutions; Artificial intelligence; Energy utilization; Algorithms",Conference Paper,Scopus,2-s2.0-84893346677
"Barrett S., Stone P., Kraus S., Rosenfeld A.","Teamwork with limited knowledge of teammates",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",21,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893367330&partnerID=40&md5=d8f94e2cedc9d1237240cc39086faff2","While great strides have been made in multiagent teamwork, existing approaches typically assume extensive information exists about teammates and how to coordinate actions. This paper addresses how robust teamwork can still be created even if limited or no information exists about a specific group of teammates, as in the ad hoc teamwork scenario. The main contribution of this paper is the first empirical evaluation of an agent cooperating with teammates not created by the authors, where the agent is not provided expert knowledge of its teammates. For this purpose, we develop a general purpose teammate modeling method and test the resulting ad hoc team agent's ability to collaborate with more than 40 unknown teams of agents to accomplish a benchmark task. These agents were designed by people other than the authors without these designers planning for the ad hoc teamwork setting. A secondary contribution of the paper is a new transfer learning algorithm, TwoStageTransfer, that can improve results when the ad hoc team agent does have some limited observations of its current teammates. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Ad-hoc teams; Empirical evaluations; Expert knowledge; Limited observations; Model method; Multi-agent teamwork; Artificial intelligence; Multi agent systems",Conference Paper,Scopus,2-s2.0-84893367330
"Brânzei S., Caragiannis I., Morgenstern J., Procaccia A.D.","How bad is selfish voting?",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",21,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893382035&partnerID=40&md5=bb670faf93929be5920da4120ee2351c","It is well known that strategic behavior in elections is essentially unavoidable; we therefore ask: how bad can the rational outcome be? We answer this question via the notion of the price of anarchy, using the scores of alternatives as a proxy for their quality and bounding the ratio between the score of the optimal alternative and the score of the winning alternative in Nash equilibrium. Specifically, we are interested in Nash equilibria that are obtained via sequences of rational strategic moves. Focusing on three common voting rules - plurality, veto, and Borda - we provide very positive results for plurality and very negative results for Borda, and place veto in the middle of this spectrum. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Nash equilibria; Optimal alternative; Price of anarchy; Strategic Behavior; Voting rules; Artificial intelligence; Telecommunication networks",Conference Paper,Scopus,2-s2.0-84893382035
"Bhateja V., Urooj S., Mehrotra R., Verma R., Lay-Ekuakille A., Verma V.D.","A composite wavelets and morphology approach for ECG noise filtering",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",21,10.1007/978-3-642-45062-4_49,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893391300&doi=10.1007%2f978-3-642-45062-4_49&partnerID=40&md5=e50e6e75c16b5508d66076cdd3728091","Noisy ECG signals contain variations in the amplitudes or in the time intervals which represents the abnormalities associated with the heart; thereby making visual diagnosis difficult for cardiovascular diseases. Hence, to facilitate proper analysis of ECG; this paper presents a combination of wavelets analysis and morphological filtering as an approach for noise removal in ECG signals. The proposed algorithm involves sub-band decomposition of ECG signal using bi-orthogonal wavelet family. The wavelet detail coefficients containing the noisy components are then processed by morphological operators using linear structuring elements. The morphological filter processes only the corrupted bands without affecting the signal parameters. Simulation results of the proposed algorithm show noteworthy suppression of noise in terms of higher signal-to-noise ratio preserving the ST segment and R wave of ECG. © Springer-Verlag 2013.","Bi-orthogonal wavelets; Detail coefficients; Morphological filtering","Biorthogonal wavelet; Cardio-vascular disease; Detail coefficients; Linear structuring elements; Morphological filtering; Morphological filters; Morphological operator; Subband decomposition; Algorithms; Artificial intelligence; Pattern recognition; Signal processing; Wavelet decomposition; Electrocardiography",Conference Paper,Scopus,2-s2.0-84893391300
"Wei S., Xu D., Li X., Zhao Y.","Joint optimization toward effective and efficient image search",2013,"IEEE Transactions on Cybernetics",21,10.1109/TCYB.2013.2245890,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890021527&doi=10.1109%2fTCYB.2013.2245890&partnerID=40&md5=0fa7463def998996acb8ce48ea0a7226","The bag-of-words (BoW) model has been known as an effective method for large-scale image search and indexing. Recent work shows that the performance of the model can be further improved by using the embedding method. While different variants of the BoW model and embedding method have been developed, less effort has been made to discover their underlying working mechanism. In this paper, we systematically investigate the image search performance variation with respect to a few factors of the BoW model, and study how to employ the embedding method to further improve the image search performance. Subsequently, we summarize several observations based on the experiments on descriptor matching. To validate these observations in a real image search, we propose an effective and efficient image search scheme, in which the BoW model and embedding method are jointly optimized in terms of effectiveness and efficiency by following these observations. Our comprehensive experiments demonstrate that it is beneficial to employ these observations to develop an image search algorithm, and the proposed image search scheme outperforms state-of-the-art methods in both effectiveness and efficiency. © 2013 IEEE.","Bag-of-words (BoW); Embedding method; High effectiveness; High efficiency; Large scale image search","Bag of words; Descriptor matching; Effectiveness and efficiencies; Embedding method; High effectiveness; Image search; Image search algorithms; State-of-the-art methods; Efficiency; Experiments; Information retrieval; Optimization; Search engines; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; decision support system; methodology; Algorithms; Artificial Intelligence; Decision Support Techniques; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84890021527
"Flores J.L., Inza I., Larrañaga P., Calvo B.","A new measure for gene expression biclustering based on non-parametric correlation",2013,"Computer Methods and Programs in Biomedicine",21,10.1016/j.cmpb.2013.07.025,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885418387&doi=10.1016%2fj.cmpb.2013.07.025&partnerID=40&md5=ad7b541ecb46e2077c9db949b9596ec8","Background: One of the emerging techniques for performing the analysis of the DNA microarray data known as biclustering is the search of subsets of genes and conditions which are coherently expressed. These subgroups provide clues about the main biological processes. Until now, different approaches to this problem have been proposed. Most of them use the mean squared residue as quality measure but relevant and interesting patterns can not be detected such as shifting, or scaling patterns. Furthermore, recent papers show that there exist new coherence patterns involved in different kinds of cancer and tumors such as inverse relationships between genes which can not be captured. Results: The proposed measure is called Spearman's biclustering measure (SBM) which performs an estimation of the quality of a bicluster based on the non-linear correlation among genes and conditions simultaneously. The search of biclusters is performed by using a evolutionary technique called estimation of distribution algorithms which uses the SBM measure as fitness function. This approach has been examined from different points of view by using artificial and real microarrays. The assessment process has involved the use of quality indexes, a set of bicluster patterns of reference including new patterns and a set of statistical tests. It has been also examined the performance using real microarrays and comparing to different algorithmic approaches such as Bimax, CC, OPSM, Plaid and xMotifs. Conclusions: SBM shows several advantages such as the ability to recognize more complex coherence patterns such as shifting, scaling and inversion and the capability to selectively marginalize genes and conditions depending on the statistical significance. © 2013 Elsevier Ireland Ltd.","Artificial intelligence; Biclustering; Biomedicine; Machine learning","Algorithmic approach; Bi-clustering; Biomedicine; Estimation of distribution algorithms; Evolutionary techniques; Inverse relationship; Non-linear correlations; Statistical significance; Artificial intelligence; Gene expression; Learning systems; Microarrays; algorithm; article; correlation analysis; data analysis; DNA determination; DNA microarray; gene expression; nonlinear system; nonparametric test; Artificial intelligence; Biclustering; Biomedicine; Machine learning; Algorithms; Cluster Analysis; Gene Expression",Article,Scopus,2-s2.0-84885418387
"Aucher G., Bolander T.","Undecidability in epistemic planning",2013,"IJCAI International Joint Conference on Artificial Intelligence",21,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062800&partnerID=40&md5=43192e825a0951aeb0e62f33bc29775f","Dynamic epistemic logic (DEL) provides a very expressive framework for multi-agent planning that can deal with nondeterminism, partial observability, sensing actions, and arbitrary nesting of beliefs about other agents' beliefs. However, as we show in this paper, this expressiveness comes at a price. The planning framework is undecidable, even if we allow only purely epistemic actions (actions that change only beliefs, not ontic facts). Undecidability holds already in the S5 setting with at least 2 agents, and even with 1 agent in S4. It shows that multi-agent planning is robustly undecidable if we assume that agents can reason with an arbitrary nesting of beliefs about beliefs. We also prove a corollary showing undecidability of the DEL model checking problem with the star operator on actions (iteration).",,"Arbitrary nesting; Dynamic epistemic logic; Epistemic actions; Epistemic planning; Model checking problem; Multi-agent planning; Partial observability; Planning framework; Model checking; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84896062800
"Venkata Narasimha K., Kivelevitch E., Sharma B., Kumar M.","An ant colony optimization technique for solving min-max Multi-Depot Vehicle Routing Problem",2013,"Swarm and Evolutionary Computation",21,10.1016/j.swevo.2013.05.005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889888857&doi=10.1016%2fj.swevo.2013.05.005&partnerID=40&md5=0dba6f7d865c586aae4253c351c2ed93","The Multi-Depot Vehicle Routing Problem (MDVRP) involves minimizing the total distance traveled by vehicles originating from multiple depots so that the vehicles together visit the specified customer locations (or cities) exactly once. This problem belongs to a class of Nondeterministic Polynomial Hard (NP Hard) problems and has been used in literature as a benchmark for development of optimization schemes. This article deals with a variant of MDVRP, called min-max MDVRP, where the objective is to minimize the tour-length of the vehicle traveling the longest distance in MDVRP. Markedly different from the traditional MDVRP, min-max MDVRP is of specific significance for time-critical applications such as emergency response, where one wants to minimize the time taken to attend any customer. This article presents an extension of an existing ant-colony technique for solving the Single Depot Vehicle Routing Problem (SDVRP) to solve the multiple depots and min-max variants of the problem. First, the article presents the algorithm that solves the min-max version of SDVRP. Then, the article extends the algorithm for min-max MDVRP using an equitable region partitioning approach aimed at assigning customer locations to depots so that MDVRP is reduced to multiple SDVRPs. The proposed method has been implemented in MATLAB for obtaining the solution for the min-max MDVRP with any number of vehicles and customer locations. A comparative study is carried out to evaluate the proposed algorithm's performance with respect to a currently available Linear Programming (LP) based algorithm in literature in terms of the optimality of solution. Based on simulation studies and statistical evaluations, it has been demonstrated that the ant colony optimization technique proposed in this article leads to more optimal results as compared to the existing LP based method. © 2013 Elsevier B.V.","Ant colony optimization; Combinatorial optimization problems; Min-max objective; Multi-Depot Vehicle Routing Problem","Algorithm's performance; Combinatorial optimization problems; Min-max; Multi-depot vehicle routing problems; Nondeterministic polynomial; Statistical evaluation; Time-critical applications; Vehicle Routing Problems; Ant colony optimization; Artificial intelligence; MATLAB; Network routing; Routing algorithms; Sales; Vehicles; Algorithms",Article,Scopus,2-s2.0-84889888857
"Lee J., Lifschitz V., Yang F.","Action language BC: Preliminary report",2013,"IJCAI International Joint Conference on Artificial Intelligence",21,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896061685&partnerID=40&md5=eacce43c3540058cd68a93de1e061154","The action description languages B and C have significant common core. Nevertheless, some expressive possibilities of B are difficult or impossible to simulate in C, and the other way around. The main advantage of B is that it allows the user to give Prolog-style recursive definitions, which is important in applications. On the other hand, B solves the frame problem by incorporating the commonsense law of inertia in its semantics, which makes it difficult to talk about fluents whose behavior is described by defaults other than inertia. In C and in its extension C+, the inertia assumption is expressed by axioms that the user is free to include or not to include, and other defaults can be postulated as well. This paper defines a new action description language, called BC, that combines the attractive features of B and C+. Examples of formalizing commonsense domains discussed in the paper illustrate the expressive capabilities of BC and the use of answer set solvers for the automation of reasoning about actions described in this language.",,"Action description languages; Action language; Answer set; Fluents; Frame problems; Reasoning about actions; Recursive definitions; Artificial intelligence; Semantics; C (programming language)",Conference Paper,Scopus,2-s2.0-84896061685
"Kim D., Wang H., Oh A.","Context-dependent conceptualization",2013,"IJCAI International Joint Conference on Artificial Intelligence",21,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063773&partnerID=40&md5=2befb565e3738819af6cd5b1e7f3b3e5","Conceptualization seeks to map a short text (i.e., a word or a phrase) to a set of concepts as a mechanism of understanding text. Most of prior research in conceptualization uses human-crafted knowledge bases that map instances to concepts. Such approaches to conceptualization have the limitation that the mappings are not context sensitive. To overcome this limitation, we propose a framework in which we harness the power of a probabilistic topic model which inherently captures the semantic relations between words. By combining latent Dirichlet allocation, a widely used topic model with Probase, a large-scale probabilistic knowledge base, we develop a corpus-based framework for context-dependent conceptualization. Through this simple but powerful framework, we improve conceptualization and enable a wide range of applications that rely on semantic understanding of short texts, including frame element prediction, word similarity in context, ad-query similarity, and query similarity.",,"Context dependent; Context sensitive; Latent Dirichlet allocation; Probabilistic knowledge; Probabilistic topic models; Query similarity; Semantic relations; Semantic understanding; Knowledge based systems; Semantics; Statistics; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84896063773
"Erdem E., Kisa D.G., Oztok U., Schüller P.","A general formal framework for pathfinding problems with multiple agents",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",21,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893343464&partnerID=40&md5=c0adf8aedaddf21607a82c5dadf2966d","Pathfinding for a single agent is the problem of planning a route from an initial location to a goal location in an environment, going around obstacles. Pathfinding for multiple agents also aims to plan such routes for each agent, subject to different constraints, such as restrictions on the length of each path or on the total length of paths, no self-intersecting paths, no intersection of paths/plans, no crossing/meeting each other. It also has variations for finding optimal solutions, e.g., with respect to the maximum path length, or the sum of plan lengths. These problems are important for many real-life applications, such as motion planning, vehicle routing, environmental monitoring, patrolling, computer games. Motivated by such applications, we introduce a formal framework that is general enough to address all these problems: we use the expressive high-level representation formalism and efficient solvers of the declarative programming paradigm Answer Set Programming. We also introduce heuristics to improve the computational efficiency and/or solution quality. We show the applicability and usefulness of our framework by experiments, with randomly generated problem instances on a grid, on a real-world road network, and on a real computer game terrain. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Answer set programming; Declarative Programming; Efficient solvers; Environmental Monitoring; Maximum path lengths; Problem instances; Real-life applications; Representation formalisms; Computer games; Logic programming; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84893343464
"Kirchmair J., Williamson M.J., Afzal A.M., Tyzack J.D., Choy A.P.K., Howlett A., Rydberg P., Glen R.C.","FAst MEtabolizer (FAME): A rapid and accurate predictor of sites of metabolism in multiple species by endogenous enzymes",2013,"Journal of Chemical Information and Modeling",21,10.1021/ci400503s,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888589061&doi=10.1021%2fci400503s&partnerID=40&md5=2b234e72ae195c275c7aa6c36ea59c73","FAst MEtabolizer (FAME) is a fast and accurate predictor of sites of metabolism (SoMs). It is based on a collection of random forest models trained on diverse chemical data sets of more than 20 000 molecules annotated with their experimentally determined SoMs. Using a comprehensive set of available data, FAME aims to assess metabolic processes from a holistic point of view. It is not limited to a specific enzyme family or species. Besides a global model, dedicated models are available for human, rat, and dog metabolism; specific prediction of phase I and II metabolism is also supported. FAME is able to identify at least one known SoM among the top-1, top-2, and top-3 highest ranked atom positions in up to 71%, 81%, and 87% of all cases tested, respectively. These prediction rates are comparable to or better than SoM predictors focused on specific enzyme families (such as cytochrome P450s), despite the fact that FAME uses only seven chemical descriptors. FAME covers a very broad chemical space, which together with its inter- and extrapolation power makes it applicable to a wide range of chemicals. Predictions take less than 2.5 s per molecule in batch mode on an Ultrabook. Results are visualized using Jmol, with the most likely SoMs highlighted. © 2013 American Chemical Society.",,"Chemical data set; Chemical descriptors; Chemical space; Endogenous enzyme; Global modeling; Metabolic process; Multiple species; Prediction rate; Decision trees; Enzymes; Indicators (chemical); Molecules; Physiology; Metabolism; cytochrome P450; diazepam; algorithm; animal; article; artificial intelligence; chemical model; chemistry; computer program; dog; drug detoxification; enzymology; eukaryotic cell; human; metabolism; quantum theory; rat; Algorithms; Animals; Artificial Intelligence; Cytochrome P-450 Enzyme System; Diazepam; Dogs; Eukaryotic Cells; Humans; Metabolic Detoxication, Drug; Metabolic Networks and Pathways; Models, Chemical; Quantum Theory; Rats; Software",Article,Scopus,2-s2.0-84888589061
"Smirnov A., Kashevnik A., Balandin S.I., Laizane S.","Intelligent mobile tourist guide: Context-based approach and implementation",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",21,10.1007/978-3-642-40316-3_9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885787894&doi=10.1007%2f978-3-642-40316-3_9&partnerID=40&md5=93f86019f8e5a7407d5bcd66353e5a1a","Nowadays there is a wide range of different mobile solutions that support travelers before, during and after the trip. However, majority of these solutions focus either on recommending tourist attractions or on providing of some tourist services, but there is a lack of studies for unified approach that combines both needs. This paper describes mobile tourist guide - a complex system that enables comprehensive up-to date information search along with personalized recommendations and services. The key principle of the developed solution is based on fact that to provide really relevant tourists information, it should be based on analysis of current situation. Prototype of the mobile tourist guide has been developed using Smart Space infrastructure to facilitate integration of services and internal processes in such complex system. This paper aims to describe context-based information implementation in the complex mobile tourist guide, developed using Smart Space infrastructure. © 2013 Springer-Verlag.","context management; e-tourism; IoT; Karelia ENPI; mobile tourist guide; Smart space; Smart-M3","Context management; eTourism; IoT; Karelia ENPI; Mobile tourist guide; Smart space; Smart-M3; Artificial intelligence; Computer science; Internet",Conference Paper,Scopus,2-s2.0-84885787894
"Portela F., Santos M.F., Machado J., Abelha A., Silva Á.","Pervasive and intelligent decision support in critical health care using ensembles",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",21,10.1007/978-3-642-40093-3_1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885207111&doi=10.1007%2f978-3-642-40093-3_1&partnerID=40&md5=5efa035b80b06da0c05d28fe4cbb2f44","Critical health care is one of the most difficult areas to make decisions. Every day new situations appear and doctors need to decide very quickly. Moreover, it is difficult to have an exact perception of the patient situation and a precise prediction on the future condition. The introduction of Intelligent Decision Support Systems (IDSS) in this area can help the doctors in the decision making process, giving them an important support based in new knowledge. Previous work has demonstrated that is possible to use data mining models to predict future situations of patients. Even so, two other problems arise: i) how fast; and ii) how accurate? To answer these questions, an ensemble strategy was experimented in the context of INTCare system, a pervasive IDSS to automatically predict the organ failure and the outcome of the patients throughout next 24 hours. This paper presents the results obtained combining real-time data processing with ensemble approach in the intensive care unit of the Centro Hospitalar do Porto, Porto, Portugal. © 2013 Springer-Verlag.",,"Data mining models; Decision making process; Ensemble approaches; Ensemble strategies; Intelligent decision support; Intelligent decision support systems; Organ failure; Real-time data processing; Artificial intelligence; Automobile drivers; Decision support systems; Forecasting; Health care; Information science; Intensive care units; Information technology",Conference Paper,Scopus,2-s2.0-84885207111
"Rizk-Allah R.M., Zaki E.M., El-Sawy A.A.","Hybridizing ant colony optimization with firefly algorithm for unconstrained optimization problems",2013,"Applied Mathematics and Computation",21,10.1016/j.amc.2013.07.092,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884878533&doi=10.1016%2fj.amc.2013.07.092&partnerID=40&md5=adb41bb1d3a1862b148581d4de8cfd5b","We propose a novel hybrid algorithm named ACO-FA, which integrate ant colony optimization (ACO) with firefly algorithm (FA) to solve unconstrained optimization problems. The proposed algorithm integrates the merits of both ACO and FA and it has two characteristic features. Firstly, the algorithm is initialized by a population of random ants that roam through the search space. During this roaming an evolution of these ants are performed by integrating ACO and FA, where FA works as a local search to refine the positions found by the ants. Secondly, the performance of FA is improved by reducing the randomization parameter so that it decreases gradually as the optima are approaching. Finally, the proposed algorithm ACO-FA is tested on several benchmark problems from the usual literature and the numerical results have demonstrated the superiority of the proposed algorithm for finding the global optimal solution. © 2013 Published by Elsevier Inc.","Ant colony optimization; Firefly algorithm; Unconstrained optimization","Ant Colony Optimization (ACO); Bench-mark problems; Firefly algorithms; Global optimal solutions; Hybrid algorithms; Numerical results; Unconstrained optimization; Unconstrained optimization problems; Ant colony optimization; Artificial intelligence; Bioluminescence; Algorithms",Article,Scopus,2-s2.0-84884878533
"Zhou W., Piramuthu S.","Remanufacturing with RFID item-level information: Optimization, waste reduction and quality improvement",2013,"International Journal of Production Economics",21,10.1016/j.ijpe.2013.05.019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883461226&doi=10.1016%2fj.ijpe.2013.05.019&partnerID=40&md5=e38976dcd6992447e79b63f3c5dc0415","We consider RFID tags and their applications from a recycling/ remanufacturing perspective and propose a novel framework to assist such process based on item-level information visibility and instantaneous tracking/tracing ability enabled by RFID. The incorporation of RFID in the reverse supply chain results in cost reduction, service and production quality improvement and pollution and waste reduction. With RFID in a reverse supply chain, we observe the power shift from waste-driven to market-driven system. Moreover, RFID's value increases with uncertainties in reverse operations as well as individual products and components. © 2013 Elsevier B.V. All rights reserved.","Closed-loop supply chain; Decision support system; Knowledge-based system; Manufacturing; RFID","Closed-loop supply chain; Information visibilities; Process-based; Production quality; Quality improvement; Reverse operations; Reverse supply chains; Waste reduction; Artificial intelligence; Decision support systems; Knowledge based systems; Manufacture; Radio frequency identification (RFID); Supply chains; Linear matrix inequalities",Article,Scopus,2-s2.0-84883461226
"Mozaffari A., Ramiar A., Fathi A.","Optimal design of classic Atkinson engine with dynamic specific heat using adaptive neuro-fuzzy inference system and mutable smart bee algorithm",2013,"Swarm and Evolutionary Computation",21,10.1016/j.swevo.2013.01.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887074792&doi=10.1016%2fj.swevo.2013.01.002&partnerID=40&md5=1ea61b3ef9cbbbe618a1b68875c1a52b","In this article, an improved version of Artificial Bee Colony (ABC) algorithm is developed to optimize a multi-modal thermodynamic power system with dynamic specific heat. Since original Karaboga's ABC for constraint problems does not consider the initial population to be feasible, a modified method called ""Mutable Smart Bee Algorithm"" (MSBA) is used which utilizes mutable heuristic agents. These mutable agents are also capable to maintain their historical memory for the location and quality of food sources. These features have been found as strong elements for mining data in constraint areas. In additions, our implementations reveal that MSBA is faster than Karaboga's method. To elaborate on authenticity of MSBA, several state-of-the-art techniques are used as rival methods to optimize well-known benchmark problems. Then, two main steps are made to optimize Atkinson engine. Firstly, an Adaptive Neuro-Fuzzy Inference System (ANFIS) is developed to identify the dynamic behavior of specific heat. Then, MSBA is hired to design the optimum features of the engine. It is observed that the proposed method is capable to successfully handle the real-life engineering problem as well as the numerical benchmark problems. © 2013 Elsevier B.V.","Atkinson cycle; Evolutionary algorithms; Mutable smart bee algorithm; Swarm intelligence","Adaptive neuro-fuzzy inference system; Artificial bee colony algorithms (ABC); Atkinson cycle; Bee Algorithm; Bench-mark problems; Engineering problems; State-of-the-art techniques; Swarm Intelligence; Artificial intelligence; Benchmarking; Engines; Evolutionary algorithms; Fuzzy systems; Heuristic methods; Optimization; Specific heat; Tracking (position); Numerical methods",Article,Scopus,2-s2.0-84887074792
"Shen J., Wang D., Li X.","Depth-aware image seam carving",2013,"IEEE Transactions on Cybernetics",21,10.1109/TCYB.2013.2273270,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890381828&doi=10.1109%2fTCYB.2013.2273270&partnerID=40&md5=9f539bcfc400e26c5bef483398567f6e","Image seam carving algorithm should preserve important and salient objects as much as possible when changing the image size, while not removing the secondary objects in the scene. However, it is still difficult to determine the important and salient objects that avoid the distortion of these objects after resizing the input image. In this paper, we develop a novel depthaware single image seam carving approach by taking advantage of the modern depth cameras such as the Kinect sensor, which captures the RGB color image and its corresponding depth map simultaneously. By considering both the depth information and the just noticeable difference (JND) model, we develop an efficient JND-based significant computation approach using the multiscale graph cut based energy optimization. Our method achieves the better seam carving performance by cutting the near objects less seams while removing distant objects more seams. To the best of our knowledge, our algorithm is the first work to use the true depth map captured by Kinect depth camera for single image seam carving. The experimental results demonstrate that the proposed approach produces better seam carving results than previous content-aware seam carving methods. © 2013 IEEE.","Energy optimization; Image retargeting; Kinect depth camera; Saliency; Seam carving","Depth camera; Energy optimization; Image retargeting; Saliency; Seam carving; Algorithms; Optimization; Cameras; actimetry; algorithm; article; artificial intelligence; automated pattern recognition; computer; computer simulation; equipment; image enhancement; image subtraction; methodology; recreation; three dimensional imaging; transducer; automated pattern recognition; devices; procedures; three dimensional imaging; Actigraphy; Algorithms; Artificial Intelligence; Computer Peripherals; Computer Simulation; Image Enhancement; Imaging, Three-Dimensional; Pattern Recognition, Automated; Subtraction Technique; Transducers; Video Games; Actigraphy; Algorithms; Artificial Intelligence; Computer Peripherals; Computer Simulation; Image Enhancement; Imaging, Three-Dimensional; Pattern Recognition, Automated; Subtraction Technique; Transducers; Video Games",Article,Scopus,2-s2.0-84890381828
"Gabryel M., Korytkowski M., Scherer R., Rutkowski L.","Object detection by simple fuzzy classifiers generated by boosting",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",21,10.1007/978-3-642-38658-9_49,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884362577&doi=10.1007%2f978-3-642-38658-9_49&partnerID=40&md5=710a1218364a8c7584f46ed72adf41a7","Finding key points based on SURF and SIFT and size of their vector reduction is a classical approach for object recognition systems. In this paper we present a new framework for object recognition based on generating simple fuzzy classifiers using key points and boosting meta learning to distinguish between one known class and other classes. We tested proposed approach on a known image dataset. © 2013 Springer-Verlag.",,"Classical approach; Fuzzy classifiers; Image datasets; Keypoints; Metalearning; Object Detection; Object recognition systems; Artificial intelligence; Fuzzy sets; Fuzzy systems; Soft computing; Object recognition",Conference Paper,Scopus,2-s2.0-84884362577
"Giancardo L., Sona D., Huang H., Sannino S., Managò F., Scheggia D., Papaleo F., Murino V.","Automatic Visual Tracking and Social Behaviour Analysis with Multiple Mice",2013,"PLoS ONE",21,10.1371/journal.pone.0074557,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884190541&doi=10.1371%2fjournal.pone.0074557&partnerID=40&md5=ba9564c366457fee392e637e078f1b51","Social interactions are made of complex behavioural actions that might be found in all mammalians, including humans and rodents. Recently, mouse models are increasingly being used in preclinical research to understand the biological basis of social-related pathologies or abnormalities. However, reliable and flexible automatic systems able to precisely quantify social behavioural interactions of multiple mice are still missing. Here, we present a system built on two components. A module able to accurately track the position of multiple interacting mice from videos, regardless of their fur colour or light settings, and a module that automatically characterise social and non-social behaviours. The behavioural analysis is obtained by deriving a new set of specialised spatio-temporal features from the tracker output. These features are further employed by a learning-by-example classifier, which predicts for each frame and for each mouse in the cage one of the behaviours learnt from the examples given by the experimenters. The system is validated on an extensive set of experimental trials involving multiple mice in an open arena. In a first evaluation we compare the classifier output with the independent evaluation of two human graders, obtaining comparable results. Then, we show the applicability of our technique to multiple mice settings, using up to four interacting mice. The system is also compared with a solution recently proposed in the literature that, similarly to us, addresses the problem with a learning-by-examples approach. Finally, we further validated our automatic system to differentiate between C57B/6J (a commonly used reference inbred strain) and BTBR T+tf/J (a mouse model for autism spectrum disorders). Overall, these data demonstrate the validity and effectiveness of this new machine learning system in the detection of social and non-social behaviours in multiple (>2) interacting mice, and its versatility to deal with different experimental settings and scenarios. © 2013 Giancardo et al.",,"animal behavior; animal experiment; article; automation; cage; classifier; eye tracking; illumination; inbred strain; learning; machine learning; male; mathematical analysis; mouse; mouse strain; nonhuman; open field behavior; prediction; skin color; social behavior; social interaction; spatiotemporal analysis; videorecording; Animals; Artificial Intelligence; Mice; Social Behavior; Video Recording",Article,Scopus,2-s2.0-84884190541
"Ornostay A., Cowie A.M., Hindle M., Baker C.J.O., Martyniuk C.J.","Classifying chemical mode of action using gene networks and machine learning: A case study with the herbicide linuron",2013,"Comparative Biochemistry and Physiology - Part D: Genomics and Proteomics",21,10.1016/j.cbd.2013.08.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883571256&doi=10.1016%2fj.cbd.2013.08.001&partnerID=40&md5=aeb139a2185da3a36e323af99ccbf3d8","The herbicide linuron (LIN) is an endocrine disruptor with an anti-androgenic mode of action. The objectives of this study were to (1) improve knowledge of androgen and anti-androgen signaling in the teleostean ovary and to (2) assess the ability of gene networks and machine learning to classify LIN as an anti-androgen using transcriptomic data. Ovarian explants from vitellogenic fathead minnows (FHMs) were exposed to three concentrations of either 5α-dihydrotestosterone (DHT), flutamide (FLUT), or LIN for 12 h. Ovaries exposed to DHT showed a significant increase in 17β-estradiol (E2) production while FLUT and LIN had no effect on E2. To improve understanding of androgen receptor signaling in the ovary, a reciprocal gene expression network was constructed for DHT and FLUT using pathway analysis and these data suggested that steroid metabolism, translation, and DNA replication are processes regulated through AR signaling in the ovary. Sub-network enrichment analysis revealed that FLUT and LIN shared more regulated gene networks in common compared to DHT. Using transcriptomic datasets from different fish species, machine learning algorithms classified LIN successfully with other anti-androgens. This study advances knowledge regarding molecular signaling cascades in the ovary that are responsive to androgens and anti-androgens and provides proof of concept that gene network analysis and machine learning can classify priority chemicals using experimental transcriptomic data collected from different fish species. © 2013 Elsevier Inc. All rights reserved.","Machine learning Sub-network enrichment analysis Wnt-frizzled pathway Herbicides Notch signaling","androgen; androgen receptor; androstanolone; antiandrogen; DNA; estradiol; flutamide; linuron; steroid; animal cell; animal experiment; animal tissue; article; case study; controlled study; DNA replication; drug effect; drug mechanism; explant; female; fish; gene expression; gene regulatory network; genetic algorithm; in vitro study; in vivo study; machine learning; nonhuman; nucleotide sequence; ovary; ovary explant; Pimephales promelas; priority journal; protein function; signal transduction; steroid metabolism; teleost; transcriptomics; translation regulation; vitellogenesis; 17-β estradiol; 17beta-trenbolone; 24-dehydrocholesterol reductase; 5α-dihydrotestosterone; androgen receptor; AR; axin 1; AXIN1; BCL2-like 1; BCL2L1; BMPR; bone morphogenetic protein receptor; C; casein kinase 1, epsilon; catenin (cadherin-associated protein), beta 1, 88kDa; CSNK1E; CTNNB1; CYP17A1; CYP1B1; cytochrome P450, family 1, subfamily B, polypeptide 1; cytochrome P450, family 17, subfamily A, polypeptide 1; DHCR24; DHT; disheveled, dsh homolog 1 (Drosophila); DVL1; E(2); EDCs; endocrine disrupting compounds; ER; estrogen receptor; FABP1; fathead minnow; fatty acid binding protein 1, liver; FDX1; ferredoxin 1; FGF8; FHM; fibroblast growth factor 8 (androgen-induced); fibronectin 1; FLUT; flutamide; FN1; follicle stimulating hormone, beta polypeptide; forkhead box O3; FOS; FOXO; FRAT1; frequently rearranged in advanced T-cell lymphomas; frizzled receptor; FSHB; FZD; Gene Ontology; gene set enrichment analysis; GH1; glycogen synthase kinase 3 beta; GnRH; GO; gonadotropin-releasing hormone receptor; growth hormone 1; GSEA; GSK3B; Herbicides; HNF1 homeobox B; HNF1B; ID1; IFT88; IL16; inhibitor of DNA binding 1, dominant negative helix-loop-helix protein; inositol polyphosphate-5-phosphatase, 145kDa; INPP5D; interleukin 16 (lymphocyte chemoattractant factor); intraflagellar transport 88 homolog (Chlamydomonas); JUN; jun oncogene; LDLR; left-right determination factor 2; LEFTY2; LIN; linuron; low density lipoprotein receptor; low density lipoprotein receptor-related protein 6; LRP6; Machine learning; matrix metallopeptidase 9 (gelatinase B,92kDa gelatinase,92kDa type IV collagenase); MMP9; MOA; mode of action; MYC; Notch signaling; NUCB2; nucleobindin 2; Parameter of cost; PCK2; PCR; PCSK2; phosphoenolpyruvate carboxykinase 2 (mitochondrial); polymerase chain reaction; POMC; proopiomelanocortin; proprotein convertase subtilisin/kexin type 2; Radial Basis Machine; RBM; sema domain, immunoglobulin domain (Ig), short basic domain, secreted, (semaphorin) 3A; SEMA3A; sex hormone-binding globulin; SHBG; signal transducer and activator of transcription; SLC5A1; SNEA; SOCS3; solute carrier family 5 (sodium/glucose cotransporter), member 1; STAT; Sub-network enrichment analysis; sub-network enrichment analysis; Support Vector Machine; suppressor of cytokine signaling 3; SVM; TB; TFE3; TGFBR; TH; transcription factor binding to IGHM enhancer 3; transforming growth factor, beta receptor 1; tyrosine hydroxylase; v-fos FBJ murine osteosarcoma viral oncogene homolog; v-myc myelocytomatosis viral oncogene homolog (avian); vitellogenin; Vtg; Wnt-frizzled pathway; Androgen Antagonists; Animals; Artificial Intelligence; Cyprinidae; Dihydrotestosterone; Endocrine Disruptors; Estradiol; Female; Flutamide; Gene Expression Profiling; Gene Regulatory Networks; Linuron; Ovary; Receptors, Androgen; Signal Transduction; Support Vector Machines; Water Pollutants, Chemical",Article,Scopus,2-s2.0-84883571256
"Butler M., Maamria I.","Practical theory extension in event-B",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",21,10.1007/978-3-642-39698-4_5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883236183&doi=10.1007%2f978-3-642-39698-4_5&partnerID=40&md5=e6d726d26343261a5254e83acd0d1b6a","The Rodin tool for Event-B supports formal modelling and proof using a mathematical language that is based on predicate logic and set theory. Although Rodin has in-built support for a rich set of operators and proof rules, for some application areas there may be a need to extend the set of operators and proof rules supported by the tool. This paper outlines a new feature of the Rodin tool, the theory component, that allows users to extend the mathematical language supported by the tool. Using theories, Rodin users may define new data types and polymorphic operators in a systematic and practical way. Theories also allow users to extend the proof capabilities of Rodin by defining new proof rules that get incorporated into the proof mechanisms. Soundness of new definitions and rules is provided through validity proof obligations. © 2013 Springer-Verlag.",,"Application area; Formal modelling; Mathematical languages; Practical theory; Predicate logic; Proof obligations; Proof rules; Rodin tool; Artificial intelligence; Computer science; Tools",Conference Paper,Scopus,2-s2.0-84883236183
"Xu R., Chen H., Li X.","A bi-objective scheduling problem on batch machines via a Pareto-based ant colony system",2013,"International Journal of Production Economics",21,10.1016/j.ijpe.2013.04.053,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880919174&doi=10.1016%2fj.ijpe.2013.04.053&partnerID=40&md5=dcdb7004f6edec0092eead16a3f552de","This research aims to minimize the bi-criteria of makespan and maximum tardiness on a set of identical batch-processing machines arranged in parallel. Each machine can process multiple jobs simultaneously as long as the machine capacity is not exceeded. Each job is defined by its processing time, ready time, due date, and size. The processing time and ready time of a batch are represented by the largest processing time and release time among all jobs in the batch, respectively. For this problem, a scheduling algorithm based on the framework of a multi-objective ant colony optimization (MOACO) approach called a Pareto-based ant colony system (PACS) was developed. Based on the constructive characteristics of PACS, a new mechanism of solution construction was introduced so that the proposed algorithm had the ability to explore the entire solution space. Moreover, corresponding to the new construction mechanism, a candidate list strategy and a form of dynamic heuristic information were developed to reduce the search space and direct the search toward the promising regions, respectively. Through extensive computational experiments with various problem instances, the effectiveness of the proposed algorithm was evaluated by measuring the computational efficiency and solution quality. The experiment results demonstrated that PACS had a superior performance compared to other benchmark algorithms, especially for large job instances. © 2013 Elsevier B.V.","Batch-processing machine; Makespan; Maximum tardiness; Multi-objective ant colony optimization; Scheduling","Batch processing machine; Bi-objective scheduling; Computational experiment; Heuristic information; Largest processing time; Makespan; Maximum tardiness; Multi objective; Ant colony optimization; Artificial intelligence; Benchmarking; Experiments; Scheduling; Scheduling algorithms",Conference Paper,Scopus,2-s2.0-84880919174
"Grinshpoun T., Grubshtein A., Zivan R., Netzer A., Meisels A.","Asymmetric distributed constraint optimization problems",2013,"Journal of Artificial Intelligence Research",21,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882336053&partnerID=40&md5=84c3e5412979b0d34bc150db1a43862f","Distributed Constraint Optimization (DCOP) is a powerful framework for representing and solving distributed combinatorial problems, where the variables of the problem are owned by different agents. Many multi-agent problems include constraints that produce different gains (or costs) for the participating agents. Asymmetric gains of constrained agents cannot be naturally represented by the standard DCOP model. The present paper proposes a general framework for Asymmetric DCOPs (ADCOPs). In ADCOPs different agents may have different valuations for constraints that they are involved in. The new framework bridges the gap between multi-agent problems which tend to have asymmetric structure and the standard symmetric DCOP model. The benefits of the proposed model over previous attempts to generalize the DCOP model are discussed and evaluated. Innovative algorithms that apply to the special properties of the proposed ADCOP model are presented in detail. These include complete algorithms that have a substantial advantage in terms of runtime and network load over existing algorithms (for standard DCOPs) which use alternative representations. Moreover, standard incomplete algorithms (i.e., local search algorithms) are inapplicable to the existing DCOP representations of asymmetric constraints and when they are applied to the new ADCOP framework they often fail to converge to a local optimum and yield poor results. The local search algorithms proposed in the present paper converge to high quality solutions. The experimental evidence that is presented reveals that the proposed local search algorithms for ADCOPs achieve high quality solutions while preserving a high level of privacy. © 2013 AI Access Foundation. All rights reserved.",,"Asymmetric structures; Combinatorial problem; Distributed constraint optimizations; Experimental evidence; High-quality solutions; Innovative algorithms; Local search algorithm; Multi-agent problems; Artificial intelligence; Learning algorithms",Article,Scopus,2-s2.0-84882336053
"Brockschmidt M., Cook B., Fuhs C.","Better termination proving through cooperation",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",21,10.1007/978-3-642-39799-8_28,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881156001&doi=10.1007%2f978-3-642-39799-8_28&partnerID=40&md5=05543da099b9e26017488fff889f65e3","One of the difficulties of proving program termination is managing the subtle interplay between the finding of a termination argument and the finding of the argument's supporting invariant. In this paper we propose a new mechanism that facilitates better cooperation between these two types of reasoning. In an experimental evaluation we find that our new method leads to dramatic performance improvements. © 2013 Springer-Verlag.",,"Experimental evaluation; New mechanisms; Performance improvements; Program termination; Termination arguments; Artificial intelligence; Computer science; Computer aided analysis",Conference Paper,Scopus,2-s2.0-84881156001
"Zhao P., Hoi S.C.H.","Cost-sensitive online active learning with application to malicious URL detection",2013,"Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",21,10.1145/2487575.2487647,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978060787&doi=10.1145%2f2487575.2487647&partnerID=40&md5=367fe6404d951ab95a0a36dfa2af774b","Malicious Uniform Resource Locator (URL) detection is an important problem in web search and mining, which plays a critical role in internet security. In literature, many existing studies have attempted to formulate the problem as a regular supervised binary classification task, which typically aims to optimize the prediction accuracy. However, in a real-world malicious URL detection task, the ratio between the number of malicious URLs and legitimate URLs is highly imbalanced, making it very inappropriate for simply optimizing the prediction accuracy. Besides, another key limitation of the existing work is to assume a large amount of training data is available, which is impractical as the human labeling cost could be potentially quite expensive. To solve these issues, in this paper, we present a novel framework of Cost-Sensitive Online Active Learning (CSOAL), which only queries a small fraction of training data for labeling and directly optimizes two cost-sensitive measures to address the class-imbalance issue. In particular, we propose two CSOAL algorithms and analyze their theoretical performance in terms of cost-sensitive bounds. We conduct an extensive set of experiments to examine the empirical performance of the proposed algorithms for a large-scale challenging malicious URL detection task, in which the encouraging results showed that the proposed technique by querying an extremely small-sized labeled data (about 0.5% out of 1- million instances) can achieve better or highly comparable classification performance in comparison to the state-of-Theart cost-insensitive and cost-sensitive online classification algorithms using a huge amount of labeled data. Copyright © 2013 ACM.","Active learning; Cost-sensitive learning; Malicious URL detection; Online learning","Artificial intelligence; Costs; Data mining; Education; Active Learning; Binary classification; Classification performance; Cost-sensitive learning; Empirical performance; On-line classification; Online learning; Theoretical performance; E-learning",Conference Paper,Scopus,2-s2.0-84978060787
"Boers A.M., Marquering H.A., Jochem J.J., Besselink N.J., Berkhemer O.A., Van Der Lugt A., Beenen L.F., Majoie C.B.","Automated cerebral infarct volume measurement in follow-up noncontrast CT scans of patients with acute ischemic stroke",2013,"American Journal of Neuroradiology",21,10.3174/ajnr.A3463,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883075581&doi=10.3174%2fajnr.A3463&partnerID=40&md5=1ab35c2a50ae5c583e5ad006e1b08a96","BACKGROUND AND PURPOSE: Cerebral infarct volume as observed in follow-up CT is an important radiologic outcome measure of the effectiveness of treatment of patients with acute ischemic stroke. However, manual measurement of CIV is time-consuming and operatordependent. The purpose of this study was to develop and evaluate a robust automated measurement of the CIV. MATERIALS AND METHODS: The CIV in early follow-up CT images of 34 consecutive patients with acute ischemic stroke was segmented with an automated intensity-based region-growing algorithm, which includes partial volume effect correction near the skull, midline determination, and ventricle and hemorrhage exclusion. Two observers manually delineated the CIV. Interobserver variability of the manual assessments and the accuracy of the automated method were evaluated by using the Pearson correlation, Bland-Altman analysis, and Dice coefficients. The accuracy was defined as the correlation with the manual assessment as a reference standard. RESULTS: The Pearson correlation for the automated method compared with the reference standard was similar to the manual correlation (R = 0.98). The accuracy of the automated method was excellent with a mean difference of 0.5 mL with limits of agreement of -38.0-39.1 mL, which were more consistent than the interobserver variability of the 2 observers (-40.9-44.1 mL). However, the Dice coefficients were higher for the manual delineation. CONCLUSIONS: The automated method showed a strong correlation and accuracy with the manual reference measurement. This approach has the potential to become the standard in assessing the infarct volume as a secondary outcome measure for evaluating the effectiveness of treatment.",,"accuracy; adult; algorithm; article; automation; brain infarction size; brain ischemia; clinical article; computer assisted tomography; controlled study; follow up; human; neuroimaging; Acute Disease; Algorithms; Artificial Intelligence; Brain Ischemia; Female; Follow-Up Studies; Humans; Imaging, Three-Dimensional; Male; Middle Aged; Netherlands; Pattern Recognition, Automated; Radiographic Image Enhancement; Radiographic Image Interpretation, Computer-Assisted; Reproducibility of Results; Sensitivity and Specificity; Stroke; Tomography, X-Ray Computed",Article,Scopus,2-s2.0-84883075581
"Azar A.T., El-Said S.A., Hassanien A.E.","Fuzzy and hard clustering analysis for thyroid disease",2013,"Computer Methods and Programs in Biomedicine",21,10.1016/j.cmpb.2013.01.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878509265&doi=10.1016%2fj.cmpb.2013.01.002&partnerID=40&md5=14a948c4d5740dcf319e7c6506cb52b8","Thyroid hormones produced by the thyroid gland help regulation of the body's metabolism. A variety of methods have been proposed in the literature for thyroid disease classification. As far as we know, clustering techniques have not been used in thyroid diseases data set so far. This paper proposes a comparison between hard and fuzzy clustering algorithms for thyroid diseases data set in order to find the optimal number of clusters. Different scalar validity measures are used in comparing the performances of the proposed clustering systems. To demonstrate the performance of each algorithm, the feature values that represent thyroid disease are used as input for the system. Several runs are carried out and recorded with a different number of clusters being specified for each run (between 2 and 11), so as to establish the optimum number of clusters. To find the optimal number of clusters, the so-called elbow criterion is applied. The experimental results revealed that for all algorithms, the elbow was located at c= 3. The clustering results for all algorithms are then visualized by the Sammon mapping method to find a low-dimensional (normally 2D or 3D) representation of a set of points distributed in a high dimensional pattern space. At the end of this study, some recommendations are formulated to improve determining the actual number of clusters present in the data set. © 2013 Elsevier Ireland Ltd.","Fuzzy C-means; Gath-Geva algorithm; Gustafson-Kessel algorithm; K-means clustering; K-medoids clustering; Thyroid disease","Fuzzy C mean; Gath-Geva algorithms; Gustafson-Kessel algorithm; K-means clustering; K-medoids clustering; Thyroid disease; Fuzzy clustering; Optimization; Clustering algorithms; algorithm; Alternative Dunn index; article; C mean algorithm; classification entropy; cluster analysis; computer assisted diagnosis; computer program; criterion variable; disease classification; Dunn index; fuzzy system; Gath Geva algorithm; Gustafsone Kessel algorithm; K mean algorithm; K medoid algorithm; mathematical analysis; Partition index; Separation index; thyroid disease; validity; Xie and Beni index; Algorithms; Artificial Intelligence; Cluster Analysis; Databases, Factual; Diagnosis, Computer-Assisted; Fuzzy Logic; Humans; Pattern Recognition, Automated; Software; Thyroid Diseases",Article,Scopus,2-s2.0-84878509265
"Chen H., Carlsson L., Eriksson M., Varkonyi P., Norinder U., Nilsson I.","Beyond the scope of free-wilson analysis: Building interpretable QSAR models with machine learning algorithms",2013,"Journal of Chemical Information and Modeling",21,10.1021/ci4001376,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879570665&doi=10.1021%2fci4001376&partnerID=40&md5=5a39ea9137fd238a525ca5423794a744","A novel methodology was developed to build Free-Wilson like local QSAR models by combining R-group signatures and the SVM algorithm. Unlike Free-Wilson analysis this method is able to make predictions for compounds with R-groups not present in a training set. Eleven public data sets were chosen as test cases for comparing the performance of our new method with several other traditional modeling strategies, including Free-Wilson analysis. Our results show that the R-group signature SVM models achieve better prediction accuracy compared with Free-Wilson analysis in general. Moreover, the predictions of R-group signature models are also comparable to the models using ECFP6 fingerprints and signatures for the whole compound. Most importantly, R-group contributions to the SVM model can be obtained by calculating the gradient for R-group signatures. For most of the studied data sets, a significant correlation with that of a corresponding Free-Wilson analysis is shown. These results suggest that the R-group contribution can be used to interpret bioactivity data and highlight that the R-group signature based SVM modeling method is as interpretable as Free-Wilson analysis. Hence the signature SVM model can be a useful modeling tool for any drug discovery project. © 2013 American Chemical Society.",,"Drug discovery; Modeling strategy; Modeling tool; Novel methodology; Prediction accuracy; Signature model; SVM algorithm; Training sets; Forecasting; Learning algorithms; Mathematical models; Molecular graphics; Computational chemistry; drug; algorithm; artificial intelligence; chemistry; drug development; pharmacology; quantitative structure activity relation; article; Algorithms; Artificial Intelligence; Drug Discovery; Pharmaceutical Preparations; Pharmacology; Quantitative Structure-Activity Relationship; Algorithms; Artificial Intelligence; Drug Discovery; Pharmaceutical Preparations; Pharmacology; Quantitative Structure-Activity Relationship",Article,Scopus,2-s2.0-84879570665
"Glass D.H.","Confirmation measures of association rule interestingness",2013,"Knowledge-Based Systems",21,10.1016/j.knosys.2013.01.021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875716538&doi=10.1016%2fj.knosys.2013.01.021&partnerID=40&md5=e34478a5788d595b7d84f01f3f9cfe10","This paper considers advantages of measures of confirmation or evidential support in the context of interestingness of association rules. In particular, it is argued that the way in which they characterize positive/negative association has advantages over other measures such as null-invariant measures. Several properties are reviewed and proposed as requirements for an adequate confirmation measure in a data mining context. While none of the well-known confirmation measures satisfy all of these requirements, two new measures are proposed which do and one of these is shown to have a further advantage. Some results suggest that these measures are relatively stable when the number of null transactions varies. © 2013 Elsevier B.V. All rights reserved.","Association rule; Confirmation; Evidential support; Interestingness; Probability","Confirmation; Interestingness; Measures of association; Positive/negative; Rule interestingness; Artificial intelligence; Probability; Software engineering; Association rules",Article,Scopus,2-s2.0-84875716538
"Niyogi P.","Manifold regularization and semi-supervised learning: Some theoretical analyses",2013,"Journal of Machine Learning Research",21,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878920314&partnerID=40&md5=f1c77487da3bbae507d8aa9cdc3c7bdd","Manifold regularization (Belkin et al., 2006) is a geometrically motivated framework for machine learning within which several semi-supervised algorithms have been constructed. Here we try to provide some theoretical understanding of this approach. Our main result is to expose the natural structure of a class of problems on which manifold regularization methods are helpful. We show that for such problems, no supervised learner can learn effectively. On the other hand, a manifold based learner (that knows the manifold or learns it from unlabeled examples) can learn with relatively few labeled examples. Our analysis follows a minimax style with an emphasis on finite sample results (in terms of n: the number of labeled examples). These results allow us to properly interpret manifold regularization and related spectral and geometric algorithms in terms of their potential use in semi-supervised learning. © 2013 Partha Niyogi.","Graph Laplacian; Manifold regularization; Minimax rates; Semi-supervised learning","Finite sample results; Geometric algorithm; Graph Laplacian; Manifold regularizations; Minimax rates; Natural structures; Semi-supervised algorithm; Semi-supervised learning; Artificial intelligence; Software engineering; Supervised learning",Article,Scopus,2-s2.0-84878920314
"Antoniou P., Pitsillides A., Blackwell T., Engelbrecht A., Michael L.","Congestion control in wireless sensor networks based on bird flocking behavior",2013,"Computer Networks",21,10.1016/j.comnet.2012.12.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875726335&doi=10.1016%2fj.comnet.2012.12.008&partnerID=40&md5=e403cc4588125cd5a33f04077bb42a6c","This paper proposes that the flocking behavior of birds can guide the design of a robust, scalable and self-adaptive congestion control protocol in the context of wireless sensor networks (WSNs). The proposed approach adopts a swarm intelligence paradigm inspired by the collective behavior of bird flocks. The main idea is to 'guide' packets (birds) to form flocks and flow towards the sink (global attractor), whilst trying to avoid congestion regions (obstacles). The direction of motion of a packet flock is influenced by repulsion and attraction forces between packets, as well as the field of view and the artificial magnetic field in the direction of the artificial magnetic pole (sink). The proposed approach is simple to implement at the individual node, involving minimal information exchange. In addition, it displays global self properties and emergent behavior, achieved collectively without explicitly programming these properties into individual packets. Performance evaluations show the effectiveness of the proposed Flock-based Congestion Control (Flock-CC) mechanism in dynamically balancing the offered load by effectively exploiting available network resources and moving packets to the sink. Furthermore, Flock-CC provides graceful performance degradation in terms of packet delivery ratio, packet loss, delay and energy tax under low, high and extreme traffic loads. In addition, the proposed approach achieves robustness against failing nodes, scalability in different network sizes and outperforms typical conventional approaches. © 2012 Elsevier B.V. All rights reserved.","Bird flocking behavior; Congestion control and avoidance; Wireless Sensor Networks (WSNs)","Collective behavior; Congestion control protocols; Conventional approach; Direction of motion; Graceful performance degradations; Minimal information; Packet delivery ratio; Wireless sensor network (WSNs); Artificial intelligence; Taxation; Sensor nodes",Article,Scopus,2-s2.0-84875726335
"Mao S., Zhao C., Zhou Z., Ye Y.","An improved fuzzy unequal clustering algorithm for wireless sensor network",2013,"Mobile Networks and Applications",21,10.1007/s11036-012-0356-4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876485601&doi=10.1007%2fs11036-012-0356-4&partnerID=40&md5=b7ae9d82fc7baade3f1208dea186e473","This paper introduces IFUC, which is an Improved Fuzzy Unequal Clustering scheme for large scale wireless sensor networks (WSNs).It aims to balance the energy consumption and prolong the network lifetime. Our approach focuses on energy efficient clustering scheme and inter-cluster routing protocol. On the one hand, considering each node's local information such as energy level, distance to base station and local density, we use fuzzy logic system to determine each node's chance of becoming cluster head and estimate the cluster head competence radius. On the other hand, we use Ant Colony Optimization (ACO) method to construct the energy-aware routing between cluster heads and base station. It reduces and balances the energy consumption of cluster heads and solves the hot spots problem that occurs in multi-hop WSN routing protocol to a large extent. The validation experiment results have indicated that the proposed clustering scheme performs much better than many other methods such as LEACH, CHEF and EEUC. © 2012 Springer Science+Business Media, LLC.","ant colony optimization; fuzzy logic; network lifetime; unequal clustering; wireless sensor network","Ant colony optimization methods; Energy Efficient clustering; Energy-aware routing; Inter-cluster routing; Large-scale wireless sensor networks; Network lifetime; Unequal clustering; Wsn routing protocols; Ant colony optimization; Artificial intelligence; Base stations; Energy efficiency; Energy utilization; Fuzzy logic; Routing protocols; Sensor nodes; Wireless sensor networks; Clustering algorithms",Conference Paper,Scopus,2-s2.0-84876485601
"Lenny Koh S.C., Genovese A., Acquaye A.A., Barratt P., Rana N., Kuylenstierna J., Gibbs D.","Decarbonising product supply chains: Design and development of an integrated evidence-based decision support system-the supply chain environmental analysis tool (SCEnAT)",2013,"International Journal of Production Research",21,10.1080/00207543.2012.705042,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873477519&doi=10.1080%2f00207543.2012.705042&partnerID=40&md5=1a84552db3270b5badb1ca7e3eb5be75","Based upon an increasing academic and business interest in greening the industrial supply chains, this paper establishes the need for a state-of-the-art decision support system (DSS) for carbon emissions accounting and management, mainly across the product supply chains by identifying methodological shortcomings in existing tools, and proposing a supply chain (SC) framework which provide businesses with a holistic understanding of their supply chains and ensuring partners within supply chain collaborative networks have a shared understanding of their emissions. It describes the design and development of a DSS now known as supply chain environmental analysis tool (SCEnAT) in detail, putting its unique and innovative features into a comparative perspective vis-à-vis existing tools and software of different types. The methodological framework used to design and develop SCEnAT integrates different individual techniques/methods of supply chain (SC) mapping, SC carbon accounting, SC interventions and SC interventions evaluation on a range of key performance indicators (KPIs). These individual methods have been used and applied innovatively to the challenge of designing SCEnAT within the desired framework. Finally, we demonstrate the application of SCEnAT, especially the advantage of using a robust carbon accounting methodology, to a SC case study. The SCEnAT framework pushes the theoretical boundary by addressing the problems of intra-organisational approach in decision making for lowering carbon along the supply chain; with an open innovation, cutting edge, hybridised framework that considers the supply chain as a whole in co-decision making for lowering carbon along the supply chain with the most robust methodology of hybrid life cycle analysis (LCA) that considers direct and indirect emissions and interventional performance evaluation for low carbon technology investment and business case building in order to adapt and mitigate climate change problems. This research has implications for future sustainability research in SC, decisions science, management theory, practice and policy. © 2013 Taylor & Francis Group, LLC.","decision support system; SC carbon accounting; SC decarbonisation; SC low carbon interventions; SC management; SC mapping","Business case; Carbon accounting; Carbon emissions; Collaborative network; Cutting edges; Decision supports; Design and Development; Environmental analysis; Industrial supply; Interventional; Key performance indicators; Life cycle analysis; Low carbon; Low-carbon technologies; Management theory; Methodological frameworks; Open innovation; Performance evaluation; Product supply chains; Shared understanding; Theoretical boundary; Artificial intelligence; Benchmarking; Carbon; Climate change; Decision making; Decision support systems; Product design; Research; Supply chains",Article,Scopus,2-s2.0-84873477519
"Vazou N., Rondon P.M., Jhala R.","Abstract refinement types",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",21,10.1007/978-3-642-37036-6_13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874401559&doi=10.1007%2f978-3-642-37036-6_13&partnerID=40&md5=11c9bc1a22f87642057057ad1034ad31","We present abstract refinement types which enable quantification over the refinements of data- and function-types. Our key insight is that we can avail of quantification while preserving SMT-based decidability, simply by encoding refinement parameters as uninterpreted propositions within the refinement logic. We illustrate how this mechanism yields a variety of sophisticated means for reasoning about programs, including: parametric refinements for reasoning with type classes, index-dependent refinements for reasoning about key-value maps, recursive refinements for reasoning about recursive data types, and inductive refinements for reasoning about higher-order traversal routines. We have implemented our approach in a refinement type checker for Haskell and present experiments using our tool to verify correctness invariants of various programs. © 2013 Springer-Verlag.",,"Data type; Haskell; Higher-order; Key values; Parametric refinements; Reasoning about programs; Type checker; Type class; Artificial intelligence; Computability and decidability",Conference Paper,Scopus,2-s2.0-84874401559
"Tavana M., Behzadian M., Pirdashti M., Pirdashti H.","A PROMETHEE-GDSS for oil and gas pipeline planning in the Caspian Sea basin",2013,"Energy Economics",21,10.1016/j.eneco.2012.11.023,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874296664&doi=10.1016%2fj.eneco.2012.11.023&partnerID=40&md5=1e91fa402aabf48627b17d0669db764d","The demand for oil and natural gas has severely challenged the world supply. The Caspian Sea basin holds large quantities of both oil and natural gas. Pipelines are needed to transport the oil and natural gas from this landlocked region over long distances within countries and across borders to meet this increasing demand. The evaluation of alternative export routes in the Caspian Sea basin is a complex multicriteria problem with conflicting objectives. We present a Group Decision Support System (GDSS) for the evaluation of alternative pipeline routes in this region. The proposed system decomposes the route selection process into manageable steps. The system combines Strength, Weakness, Opportunity and Threat (SWOT) analysis with the Delphi method to capture the decision makers' (DMs') beliefs. A group Preference Ranking Organization Method for Enrichment Evaluation (PROMETHEE) model is used to integrate these beliefs with subjective judgments and identify the most attractive pipeline route. The Geometrical Analysis for Interactive Assistance (GAIA) plane is used to further analyze the alternative routes and arrive at a group solution consistent with managerial goals and objectives. © 2012 Elsevier B.V.","Caspian Sea; Delphi; GDSS; Oil and gas pipeline; PROMETHEE; SWOT","Caspian sea; Delphi; GDSS; Oil-and-Gas pipelines; PROMETHEE; SWOT; Artificial intelligence; Decision making; Decision support systems; Gas pipelines; Transportation routes; Natural gas; decision making; Delphi analysis; gas pipeline; geometry; multicriteria analysis; numerical model; oil pipeline; Caspian Basin",Article,Scopus,2-s2.0-84874296664
"Grimm D., Hagmann J., Koenig D., Weigel D., Borgwardt K.","Accurate indel prediction using paired-end short reads",2013,"BMC Genomics",21,10.1186/1471-2164-14-132,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874223315&doi=10.1186%2f1471-2164-14-132&partnerID=40&md5=5d4cb6ffa76ea762239e579c9bb14312","Background: One of the major open challenges in next generation sequencing (NGS) is the accurate identification of structural variants such as insertions and deletions (indels). Current methods for indel calling assign scores to different types of evidence or counter-evidence for the presence of an indel, such as the number of split read alignments spanning the boundaries of a deletion candidate or reads that map within a putative deletion. Candidates with a score above a manually defined threshold are then predicted to be true indels. As a consequence, structural variants detected in this manner contain many false positives.Results: Here, we present a machine learning based method which is able to discover and distinguish true from false indel candidates in order to reduce the false positive rate. Our method identifies indel candidates using a discriminative classifier based on features of split read alignment profiles and trained on true and false indel candidates that were validated by Sanger sequencing. We demonstrate the usefulness of our method with paired-end Illumina reads from 80 genomes of the first phase of the 1001 Genomes Project (http://www.1001genomes.org) in Arabidopsis thaliana.Conclusion: In this work we show that indel classification is a necessary step to reduce the number of false positive candidates. We demonstrate that missing classification may lead to spurious biological interpretations. The software is available at: http://agkb.is.tuebingen.mpg.de/Forschung/SV-M/. © 2013 Grimm et al.; licensee BioMed Central Ltd.","Discriminative machine learning; Indel detection; Next generation sequencing; Paired-end short reads; Split-read mapping","accuracy; Arabidopsis; article; computer program; controlled study; discrimination learning; false positive result; gene deletion; gene insertion; gene sequence; indel classification; machine learning; next generation sequencing; nonhuman; prediction; support vector machine; validation study; Algorithms; Arabidopsis; Artificial Intelligence; Computational Biology; High-Throughput Nucleotide Sequencing; INDEL Mutation; Software; Arabidopsis",Article,Scopus,2-s2.0-84874223315
"Care M.A., Barrans S., Worrillow L., Jack A., Westhead D.R., Tooze R.M.","A Microarray Platform-Independent Classification Tool for Cell of Origin Class Allows Comparative Analysis of Gene Expression in Diffuse Large B-cell Lymphoma",2013,"PLoS ONE",21,10.1371/journal.pone.0055895,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873728254&doi=10.1371%2fjournal.pone.0055895&partnerID=40&md5=ecfa45b464e43d16fa999d8204ef178a","Cell of origin classification of diffuse large B-cell lymphoma (DLBCL) identifies subsets with biological and clinical significance. Despite the established nature of the classification existing studies display variability in classifier implementation, and a comparative analysis across multiple data sets is lacking. Here we describe the validation of a cell of origin classifier for DLBCL, based on balanced voting between 4 machine-learning tools: the DLBCL automatic classifier (DAC). This shows superior survival separation for assigned Activated B-cell (ABC) and Germinal Center B-cell (GCB) DLBCL classes relative to a range of other classifiers. DAC is effective on data derived from multiple microarray platforms and formalin fixed paraffin embedded samples and is parsimonious, using 20 classifier genes. We use DAC to perform a comparative analysis of gene expression in 10 data sets (2030 cases). We generate ranked meta-profiles of genes showing consistent class-association using ≥6 data sets as a cut-off: ABC (414 genes) and GCB (415 genes). The transcription factor ZBTB32 emerges as the most consistent and differentially expressed gene in ABC-DLBCL while other transcription factors such as ARID3A, BATF, and TCF4 are also amongst the 24 genes associated with this class in all datasets. Analysis of enrichment of 12323 gene signatures against meta-profiles and all data sets individually confirms consistent associations with signatures of molecular pathways, chromosomal cytobands, and transcription factor binding sites. We provide DAC as an open access Windows application, and the accompanying meta-analyses as a resource. © 2013 Care et al.",,"paraffin; transcription factor; transcription factor ARID3A; transcription factor BATF; transcription factor TCF4; transcription factor ZBTB32; unclassified drug; article; binding site; carcinogenesis; chromosome analysis; controlled study; gene expression; genetic association; germinal center; large cell lymphoma; machine learning; microarray analysis; nucleotide sequence; Artificial Intelligence; B-Lymphocytes; Cell Survival; Chromosomes, Human; Computational Biology; Focal Adhesions; Gene Expression Profiling; Humans; Lymphoma, Large B-Cell, Diffuse; Oligonucleotide Array Sequence Analysis; Transcription Factors",Article,Scopus,2-s2.0-84873728254
"Mohammadhassani M., Nezamabadi-Pour H., Jumaat M., Jameel M., Hakim S.J.S., Zargar M.","Application of the ANFIS model in deflection prediction of concrete deep beam",2013,"Structural Engineering and Mechanics",21,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875202220&partnerID=40&md5=b18461ae95198383aabcd0cf272780f1","With the ongoing development in the computer science areas of artificial intelligence and computational intelligence, researchers are able to apply them successfully in the construction industry. Given the complexities indeep beam behaviour and the difficulties in accurate evaluation of its deflection, the current study has employed the Adaptive Network-based Fuzzy Inference System (ANFIS) as one of the modelling tools to predict deflection for high strength self compacting concrete (HSSCC) deep beams. In this study, about 3668measured data on eight HSSCC deep beams are considered. Effective input data and the corresponding deflection as output data were recorded at all loading stages up to failure load for all tested deep beams. The results of ANFIS modelling and the classical linear regression were compared and concluded that the ANFIS results are highly accurate, precise and satisfactory.","ANFIS; Deep beams; Deflection; Fuzzy inference system; Linear regression","Adaptive network based fuzzy inference system; ANFIS; Concrete deep beams; Deep beam; Effective inputs; Fuzzy inference systems; Highly accurate; Modelling tools; Artificial intelligence; Construction industry; Linear regression; Loading; Self compacting concrete; Deflection (structures)",Article,Scopus,2-s2.0-84875202220
"Feulner J., Kevin Zhou S., Hammon M., Hornegger J., Comaniciu D.","Lymph node detection and segmentation in chest CT data using discriminative learning and a spatial prior",2013,"Medical Image Analysis",21,10.1016/j.media.2012.11.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883896690&doi=10.1016%2fj.media.2012.11.001&partnerID=40&md5=bee2e303495dd366c4922dc6d1536e7e","Lymph nodes have high clinical relevance and routinely need to be considered in clinical practice. Automatic detection is, however, challenging due to clutter and low contrast. In this paper, a method is presented that fully automatically detects and segments lymph nodes in 3-D computed tomography images of the chest. Lymph nodes can easily be confused with other structures, it is therefore vital to incorporate as much anatomical prior knowledge as possible in order to achieve a good detection performance. Here, a learned prior of the spatial distribution is used to model this knowledge. Different prior types with increasing complexity are proposed and compared to each other. This is combined with a powerful discriminative model that detects lymph nodes from their appearance. It first generates a number of candidates of possible lymph node center positions. Then, a segmentation method is initialized with a detected candidate. The graph cuts method is adapted to the problem of lymph nodes segmentation. We propose a setting that requires only a single positive seed and at the same time solves the small cut problem of graph cuts. Furthermore, we propose a feature set that is extracted from the segmentation. A classifier is trained on this feature set and used to reject false alarms. Cross-validation on 54 CT datasets showed that for a fixed number of four false alarms per volume image, the detection rate is well more than doubled when using the spatial prior. In total, our proposed method detects mediastinal lymph nodes with a true positive rate of 52.0% at the cost of only 3.1 false alarms per volume image and a true positive rate of 60.9% with 6.1 false alarms per volume image, which compares favorably to prior work on mediastinal lymph node detection. © 2012 Elsevier B.V.","Chest CT; Detection; Lymph nodes; Segmentation; Spatial prior","Chest CT; Computed tomography images; Detection performance; Discriminative learning; Lymph node; Lymph node detections; Mediastinal lymph nodes; Spatial priors; Alarm systems; Body fluids; Computerized tomography; Error detection; Errors; Graphic methods; Image segmentation; article; classifier; comparative study; computer assisted tomography; discrimination learning; human; knowledge base; lymph node detection; lymph node segmentation; lymphadenopathy; lymphoid tissue; mediastinum lymph node; priority journal; probability; radiation attenuation; radiation detection; radiological procedures; reference database; spatial prior probability; surface property; three dimensional imaging; watershed; algorithm; artificial intelligence; automated pattern recognition; computer assisted diagnosis; discriminant analysis; image quality; lymph node; lymph node metastasis; methodology; radiography; reproducibility; sensitivity and specificity; thorax radiography; Algorithms; Artificial Intelligence; Discriminant Analysis; Humans; Lymph Nodes; Lymphatic Metastasis; Pattern Recognition, Automated; Radiographic Image Enhancement; Radiographic Image Interpretation, Computer-Assisted; Radiography, Thoracic; Reproducibility of Results; Sensitivity and Specificity; Tomography, X-Ray Computed",Article,Scopus,2-s2.0-84883896690
"García J., Diaz O., Azanza M.","Model transformation co-evolution: A semi-automatic approach",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",21,10.1007/978-3-642-36089-3_9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872726686&doi=10.1007%2f978-3-642-36089-3_9&partnerID=40&md5=0d1f613ba91ae38c31b15116c5782d3b","Model transformations are precious and effortful outcomes of Model-Driven Engineering. As any other artifact, transformations are also subject to evolution forces. Not only are they affected by changes to transformation requirements, but also by the changes to the associated metamodels. Manual co-evolution of transformations after these metamodel changes is cumbersome and error-prone. In this setting, this paper introduces a semi-automatic process for the co-evolution of transformations after metamodel evolution. The process is divided in two main stages: at the detection stage, the changes to the metamodel are detected and classified, while the required actions for each type of change are performed at the co-evolution stage. The contributions of this paper include the automatic co-evolution of breaking and resolvable changes and the assistance to the transformation developer to aid in the co-evolution of breaking and unresolvable changes. The presented process is implemented for ATL in the CO-URE prototype. © 2013 Springer-Verlag Berlin Heidelberg.",,"Co-evolution; Detection stage; Error prones; Meta model; Model transformation; Model-driven Engineering; Semi-automatics; Artificial intelligence; Mathematical models",Conference Paper,Scopus,2-s2.0-84872726686
"Dos Santos J.A., Gosselin P.-H., Philipp-Foliguet S., Torres R.D.S., Falcão A.X.","Interactive multiscale classification of high-resolution remote sensing images",2013,"IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing",21,10.1109/JSTARS.2012.2237013,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881075584&doi=10.1109%2fJSTARS.2012.2237013&partnerID=40&md5=0c6ac73005634b444dc3605dc63ed843","The use of remote sensing images (RSIs) as a source of information in agribusiness applications is very common. In those applications, it is fundamental to identify and understand trends and patterns in space occupation. However, the identification and recognition of crop regions in remote sensing images are not trivial tasks yet. In high-resolution image analysis and recognition, many of the problems are related to the representation scale of the data, and to both the size and the representativeness of the training set. In this paper, we propose a method for interactive classification of remote sensing images considering multiscale segmentation. Our aim is to improve the selection of training samples using the features from the most appropriate scales of representation. We use a boosting-based active learning strategy to select regions at various scales for user's relevance feedback. The idea is to select the regions that are closer to the border that separates both target classes: relevant and non-relevant regions. Experimental results showed that the combination of scales produces better results than isolated scales in a relevance feedback process. Furthermore, the interactive method achieved good results with few user interactions. The proposed method needs only a small portion of the training set to build classifiers that are as strong as the ones generated by a supervised method that uses the whole training set. © 2008-2012 IEEE.","Active learning; boosting; interactive classification; multiscale classification; support vector machines","Active Learning; Active learning strategies; boosting; High resolution image; High-resolution remote sensing images; Interactive classification; Multiscale segmentation; User's relevance feedback; Classification (of information); Support vector machines; Image reconstruction; artificial intelligence; image analysis; multispectral image; remote sensing; segmentation",Article,Scopus,2-s2.0-84881075584
"Mondal S., Bours P.","Continuous authentication using mouse dynamics",2013,"BIOSIG 2013 - Proceedings of the 12th International Conference of the Biometrics Special Interest Group",21,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937109395&partnerID=40&md5=9d307d46cc7c1a1396248baf4f61723e","In this paper, we demonstrate a new way to perform continuous authentication using Mouse Dynamics as the behavioural biometric modality. In the proposed scheme, the user will be authenticated per mouse event performed on his/her system. We have used a publicly available mouse dynamics dataset and extracted per event features suitable for the proposed scheme. In this research, we have used the mouse dynamics data of 49 users and evaluated the system performance with 6 machine learning algorithms. In this approach, the genuine user has never been classified as an impostor throughout a full session whereas the average number of mouse actions an impostor could perform before detection is 94 from the best classification algorithm with a person based threshold.",,"Artificial intelligence; Biometrics; Dynamics; Learning algorithms; Learning systems; Mammals; Average numbers; Behavioural Biometric; Classification algorithm; Continuous authentications; Mouse dynamics; Authentication",Conference Paper,Scopus,2-s2.0-84937109395
"Chowdhury S.R., Imran M., Asghar M.R., Amer-Yahia S., Castillo C.","Tweet4act: Using incident-specific profiles for classifying crisis-related messages",2013,"ISCRAM 2013 Conference Proceedings - 10th International Conference on Information Systems for Crisis Response and Management",21,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905670750&partnerID=40&md5=9967de64cf3bc70b4df02f45dc1f684c","We present Tweet4act, a system to detect and classify crisis-related messages communicated over a microblogging platform. Our system relies on extracting content features from each message. These features and the use of an incident-specific dictionary allow us to determine the period type of an incident that each message belongs to. The period types are: Pre-incident (messages talking about prevention, mitigation, and preparedness), during-incident (messages sent while the incident is taking place), and post-incident (messages related to the response, recovery, and reconstruction). We show that our detection method can effectively identify incident-related messages with high precision and recall, and that our incident-period classification method outperforms standard machine learning classification methods.","Crisis informatics; Disaster management; Microblogging; Twitter data-analytics","Artificial intelligence; Disaster prevention; Classification methods; Crisis informatics; Disaster management; Micro-blogging platforms; Microblogging; Precision and recall; Standard machines; Twitter data-analytics; Information systems",Conference Paper,Scopus,2-s2.0-84905670750
"Giannarou S., Visentini-Scarzanella M., Yang G.-Z.","Probabilistic tracking of affine-invariant anisotropic regions",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",21,10.1109/TPAMI.2012.81,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870173714&doi=10.1109%2fTPAMI.2012.81&partnerID=40&md5=4cad36cd25f34acb800c6586f168b8c6","Despite a wide range of feature detectors developed in the computer vision community over the years, direct application of these techniques to surgical navigation has shown significant difficulties due to the paucity of reliable salient features coupled with free - form tissue deformation and changing visual appearance of surgical scenes. The aim of this paper is to propose a novel probabilistic framework to track affine-invariant anisotropic regions under contrastingly different visual appearances during Minimally Invasive Surgery (MIS). The theoretical background of the affine-invariant anisotropic feature detector is presented and a real-time implementation exploiting the computational power of the GPU is proposed. An Extended Kalman Filter (EKF) parameterization scheme is used to adaptively adjust the optimal templates of the detected regions, enabling accurate identification and matching of the tracked features. For effective tracking verification, spatial context and region similarity have also been incorporated. They are used to boost the prediction of the EKF and recover potential tracking failure due to drift or false positives. The proposed framework is compared to the existing methods and their respective performance is evaluated with in vivo video sequences recorded from robotic-assisted MIS procedures, as well as real-world scenes. © 1979-2012 IEEE.","feature point tracking; image-guided navigation; Salient feature extraction","Anisotropic features; Computational power; False positive; Feature detector; Feature point tracking; Image-guided; In-vivo; Minimally invasive surgery; Parameterization schemes; Probabilistic framework; Real-time implementations; Region similarity; Salient feature extraction; Salient features; Spatial context; Surgical navigation; Tissue deformations; Tracking failure; Video sequences; Vision communities; Visual appearance; Computer vision; Extended Kalman filters; Feature extraction; Image matching; Image segmentation; Real time control; Surgery; Surgical equipment; Three dimensional computer graphics; Visualization; Anisotropy; algorithm; anisotropy; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; computer assisted surgery; decision support system; image subtraction; methodology; Algorithms; Anisotropy; Artificial Intelligence; Decision Support Techniques; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Subtraction Technique; Surgery, Computer-Assisted",Article,Scopus,2-s2.0-84870173714
"Tansel Iç Y., Yurdakul M., Dengiz B.","Development of a decision support system for robot selection",2013,"Robotics and Computer-Integrated Manufacturing",21,10.1016/j.rcim.2012.11.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886790902&doi=10.1016%2fj.rcim.2012.11.008&partnerID=40&md5=b4b46202738499f63de7ca86b7b95d37","With the availability of more different robot types and models along with their separate specifications, selecting the most appropriate robot is becoming more difficult and complicated for companies. Furthermore, a common set of robot selection criteria is not available for the decision makers. In this study, a two-phase robot selection decision support system, namely ROBSEL, is developed to help the decision makers in their robot selection decisions. In development of ROBSEL, an independent set of criteria is obtained first and arranged in the Fuzzy Analytical Hierarchy Process (FAHP) decision hierarchy. In the first elimination phase of the decision support system, the user obtains the feasible set of robots by providing limited values for the 15 requirements. ROBSEL, then, uses FAHP decision hierarchy to rank the feasible robots in the second phase. ROBSEL is illustrated and tested and several critical issues in its practical usage are explored in the paper. The applications of ROBSEL show that ROBSEL is a useful, practical and easy to use robot selection tool and improves robot selection decisions in the companies. © 2012 Elsevier Ltd. All rights reserved.","Correlation test Fuzzy AHP; Industrial robot; Robot selection","Artificial intelligence; Decision making; Industrial robots; Robots; Critical issues; Decision makers; Feasible set; Fuzzy AHP; Fuzzy analytical hierarchy process; Independent set; Robot selection; Second phase; Decision support systems",Article,Scopus,2-s2.0-84886790902
"Ontañón S.","The combinatorial Multi-armed Bandit problem and its application to real-time strategy games",2013,"Proceedings of the 9th AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, AIIDE 2013",21,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890921020&partnerID=40&md5=676d59cd184e6b7afb1af0d29c288a8d","Game tree search in games with large branching factors is a notoriously hard problem. In this paper, we address this problem with a new sampling strategy for Monte Carlo Tree Search (MCTS) algorithms, called Naïve Sampling, based on a variant of the Multi-armed Bandit problem called the Combinatorial Multi-armed Bandit (CMAB) problem. We present a new MCTS algorithm based on Naïve Sampling called NaïveMCTS, and evaluate it in the context of real-time strategy (RTS) games. Our results show that as the branching factor grows, NaïveMCTS performs significantly better than other algorithms. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Artificial intelligence; Probability; Real time systems; Sodium; Statistics; Trees (mathematics); Branching factors; Game tree search; ITS applications; Monte Carlo tree search (MCTS); Multi armed bandit; Multi-armed bandit problem; Real-time strategy games; Sampling strategies; Computer software",Conference Paper,Scopus,2-s2.0-84890921020
"Engelbrecht A.P.","Particle swarm optimization: Global best or local best?",2013,"Proceedings - 1st BRICS Countries Congress on Computational Intelligence, BRICS-CCI 2013",21,10.1109/BRICS-CCI-CBIC.2013.31,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905374834&doi=10.1109%2fBRICS-CCI-CBIC.2013.31&partnerID=40&md5=e3d7470fcc33c2873b9927703a3cb0bf","A number of empirical studies have compared the two extreme neighborhood topologies used in particle swarm optimization (PSO) algorithms, namely the star and the ring topologies. Based on these empirical studies, and also based on intuitive understanding of these neighborhood topologies, there is a faction within the PSO research community that advocates the use of the local best (lbest) PSO due to its better exploration abilities, diminished susceptibility to being trapped in local minima, and because it does not suffer from premature convergence as is the case with the global best (gbest) PSO. However, the opinions that emanated from these studies were based on a very limited benchmark suite containing only a few benchmark functions. This paper conducts a very elaborate empirical comparison of the gbest and lbest PSO algorithms on a benchmark suite of 60 boundary constrained minimization problems of varying complexities. The statistical analysis conducted shows that the general statements made about premature convergence, exploration ability, and even solution accuracy are not correct, and shows that neither of the two algorithms can be considered outright as the best, not even for specific problem classes. © 2013 IEEE.","Global best; Local best; Neigbhorhood topologies; Particle swarm optimization","Algorithms; Artificial intelligence; Constrained optimization; Topology; Constrained minimization problem; Empirical - comparisons; Global best; Intuitive understanding; Local best; Neighborhood topology; Particle swarm optimization algorithm; Pre-mature convergences; Particle swarm optimization (PSO)",Conference Paper,Scopus,2-s2.0-84905374834
"Pears R., Koh Y.S., Dobbie G., Yeap W.","Weighted association rule mining via a graph based connectivity model",2013,"Information Sciences",21,10.1016/j.ins.2012.07.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867405323&doi=10.1016%2fj.ins.2012.07.001&partnerID=40&md5=4c2fdf066d17b17b071ec20b04508311","Association rule mining is an important data mining task that discovers relationships among items in a transaction database. Classical association rule mining approaches make the implicit assumption that an item's importance is determined by its support. In contrast, Weighted Association Rule Mining (WARM) attempts to provide a notion of importance, or weight to individual items that are not based solely on item support. Previous approaches to Weighted Association Rule Mining assign item weights in a subjective manner, based on a user's specialized knowledge of the underlying domain that is involved. Such approaches are infeasible when millions of items are present in a dataset, or when domain knowledge is unavailable. Furthermore, even when such domain information is available, a weight assignment based on subjective information constrains the knowledge discovered to fit with the weights assigned, thus inhibiting the discovery of new trends in the data. In this research we automate the process of weight assignment by formulating a linear model that captures relationships between items. This approach extends prior research based on the Valency model. We extend the Valency model by expanding the field of interaction beyond immediate neighborhoods and show that this leads to significant improvements in performance on a number of different metrics that we use. © 2012 Elsevier Inc. All rights reserved.","Association rule mining; Connectivity; Graph cliques; Item weights; Purity; Valency model","Connectivity; Graph cliques; Item weights; Purity; Valencies; Artificial intelligence; Software engineering; Data mining",Article,Scopus,2-s2.0-84867405323
"Ogawa K., Suzuki Y., Takeuchi I.","Safe screening of non-support vectors in pathwise SVM computation",2013,"30th International Conference on Machine Learning, ICML 2013",21,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897564278&partnerID=40&md5=c3439fc877890dcfcde96379cfefa5bb","In this paper, we claim that some of the non-support vectors (non-SVs) that have no influence on the SVM classifier can be screened out prior to the training phase in pathwise SVM computation scenario, in which one is asked to train a sequence (or path) of SVM classifiers for different regularization parameters. Based on a recently proposed framework so-called safe screening rule, we derive a rule for screening out non-SVs in advance, and discuss how we can exploit the advantage of the rule in pathwise SVM computation scenario. Experiments indicate that our approach often substantially reduce the total pathwise computation cost. Copyright 2013 by the author(s).",,"Artificial intelligence; Software engineering; Computation costs; Regularization parameters; SVM classifiers; Training phase; Learning systems",Conference Paper,Scopus,2-s2.0-84897564278
"Dhanda S.K., Vir P., Raghava G.P.S.","Designing of interferon-gamma inducing MHC class-II binders",2013,"Biology Direct",20,10.1186/1745-6150-8-30,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889053011&doi=10.1186%2f1745-6150-8-30&partnerID=40&md5=0f113e13c2d3606296d7460a46023c8b","Background: The generation of interferon-gamma (IFN-γ) by MHC class II activated CD4+ T helper cells play a substantial contribution in the control of infections such as caused by Mycobacterium tuberculosis. In the past, numerous methods have been developed for predicting MHC class II binders that can activate T-helper cells. Best of author's knowledge, no method has been developed so far that can predict the type of cytokine will be secreted by these MHC Class II binders or T-helper epitopes. In this study, an attempt has been made to predict the IFN-γ inducing peptides. The main dataset used in this study contains 3705 IFN-γ inducing and 6728 non-IFN-γ inducing MHC class II binders. Another dataset called IFNgOnly contains 4483 IFN-γ inducing epitopes and 2160 epitopes that induce other cytokine except IFN-γ. In addition we have alternate dataset that contains IFN-γ inducing and equal number of random peptides. Results: It was observed that the peptide length, positional conservation of residues and amino acid composition affects IFN-γ inducing capabilities of these peptides. We identified the motifs in IFN-γ inducing binders/peptides using MERCI software. Our analysis indicates that IFN-γ inducing and non-inducing peptides can be discriminated using above features. We developed models for predicting IFN-γ inducing peptides using various approaches like machine learning technique, motifs-based search, and hybrid approach. Our best model based on the hybrid approach achieved maximum prediction accuracy of 82.10% with MCC of 0.62 on main dataset. We also developed hybrid model on IFNgOnly dataset and achieved maximum accuracy of 81.39% with 0.57 MCC.Conclusion: Based on this study, we have developed a webserver for predicting i) IFN-γ inducing peptides, ii) virtual screening of peptide libraries and iii) identification of IFN-γ inducing regions in antigen (http://crdd.osdd.net/raghava/ifnepitope/).Reviewers: This article was reviewed by Prof Kurt Blaser, Prof Laurence Eisenlohr and Dr Manabu Sugai. © 2013 Dhanda et al.; licensee BioMed Central Ltd.",,"Mycobacterium tuberculosis; epitope; gamma interferon; HLA antigen class 2; subunit vaccine; animal; article; artificial intelligence; biology; chemical model; chemistry; computer simulation; evaluation study; human; methodology; Animals; Artificial Intelligence; Computational Biology; Computer Simulation; Epitopes, T-Lymphocyte; Histocompatibility Antigens Class II; Humans; Interferon-gamma; Models, Chemical; Vaccines, Subunit",Article,Scopus,2-s2.0-84889053011
"Mastrocinque E., Yuce B., Lambiase A., Packianather M.S.","A multi-objective optimization for supply chain network using the bees algorithm",2013,"International Journal of Engineering Business Management",20,10.5772/56754,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893366133&doi=10.5772%2f56754&partnerID=40&md5=0eefce0c925fdc67a4a4d8194356b942","A supply chain is a complex network which involves the products, services and information flows between suppliers and customers. A typical supply chain is composed of different levels, hence, there is a need to optimize the supply chain by finding the optimum configuration of the network in order to get a good compromise between the multi-objectives such as cost minimization and lead-time minimization. There are several multi-objective optimization methods which have been applied to find the optimum solutions set based on the Pareto front line. In this study, a swarm-based optimization method, namely, the bees algorithm is proposed in dealing with the multi-objective supply chain model to find the optimum configuration of a given supply chain problem which minimizes the total cost and the total lead-time. The supply chain problem utilized in this study is taken from literature and several experiments have been conducted in order to show the performance of the proposed model; in addition, the results have been compared to those achieved by the ant colony optimization method. The results show that the proposed bees algorithm is able to achieve better Pareto solutions forthe supply chain problem. © 2013 Mastrocinque et al.","Artificial intelligence; Multi-Objective optimization; Supply chain management; Swarm-based optimization; The bees algorithm",,Article,Scopus,2-s2.0-84893366133
"Moro A., Navigli R.","Integrating syntactic and semantic analysis into the open information extraction paradigm",2013,"IJCAI International Joint Conference on Artificial Intelligence",20,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896059864&partnerID=40&md5=a8933ccdb77ae0969641253b04a0d53a","In this paper we present an approach aimed at enriching the Open Information Extraction paradigm with semantic relation ontologization by integrating syntactic and semantic features into its workflow. To achieve this goal, we combine deep syntactic analysis and distributional semantics using a shortest path kernel method and soft clustering. The output of our system is a set of automatically discovered and ontologized semantic relations.",,"Distributional semantics; Kernel methods; Semantic analysis; Semantic features; Semantic relations; Shortest path; Soft clustering; Syntactic analysis; Artificial intelligence; Syntactics; Semantics",Conference Paper,Scopus,2-s2.0-84896059864
"Meng D., Xu Z., Zhang L., Zhao J.","A cyclic weighted median method for L1 low-rank matrix factorization with missing entries",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",20,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893361849&partnerID=40&md5=dc01df4a4761e86070aa8032a2c29746","A challenging problem in machine learning, information retrieval and computer vision research is how to recover a low-rank representation of the given data in the presence of outliers and missing entries. The L 1-norm low-rank matrix factorization (LRMF) has been a popular approach to solving this problem. However, L1-norm LRMF is difficult to achieve due to its non-convexity and non-smoothness, and existing methods are often inefficient and fail to converge to a desired solution. In this paper we propose a novel cyclic weighted median (CWM) method, which is intrinsically a coordinate decent algorithm, for L1-norm LRMF. The CWM method minimizes the objective by solving a sequence of scalar minimization sub-problems, each of which is convex and can be easily solved by the weighted median filter. The extensive experimental results validate that the CWM method outperforms state-of-the-arts in terms of both accuracy and computational efficiency. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Coordinate decent algorithms; Low-rank matrices; Low-rank representations; Non-smoothness; Nonconvexity; Sub-problems; Weighted median; Weighted median filter; Artificial intelligence; Problem solving",Conference Paper,Scopus,2-s2.0-84893361849
"Bourhis P., Morak M., Pieris A.","The impact of disjunction on query answering under guarded-based existential rules",2013,"IJCAI International Joint Conference on Artificial Intelligence",20,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063788&partnerID=40&md5=d68fe409f7132cba36876a6a1d0c023a","We study the complexity of conjunctive query answering under (weakly-)(frontier-)guarded disjunctive existential rules, i.e., existential rules extended with disjunction, and their main subclasses, linear rules and inclusion dependencies (IDs). Our main result states that conjunctive query answering under a fixed set of disjunctive IDs is 2EXPTIME-hard. This quite surprising result together with a 2EXPTIME upper bound for weakly-frontier- guarded disjunctive rules, obtained by exploiting recent results on guarded negation first-order logic, gives us a complete picture of the computational complexity of our problem. We also consider a natural subclass of disjunctive IDs, namely frontier-one (only one variable is propagated), for which the combined complexity decreases to EXPTIME. Finally, we show that frontier-guarded rules, combined with negative constraints, are strictly more expressive than DL-Litebool H, one of the most expressive languages of the DL-Lite family. We also show that query answering under DL-Litebool H is 2EXPTIMEcomplete in combined complexity.",,"Combined complexity; Conjunctive queries; First order logic; Inclusion dependencies; Linear rule; Negative constraints; Query answering; Upper Bound; Artificial intelligence; Query languages",Conference Paper,Scopus,2-s2.0-84896063788
"Yang S.-J., Jiang Y., Zhou Z.-H.","Multi-Instance Multi-Label learning with weak label",2013,"IJCAI International Joint Conference on Artificial Intelligence",20,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896061460&partnerID=40&md5=c27819bc552960a2f07f3687ce051596","Multi-Instance Multi-Label learning (MIML) deals with data objects that are represented by a bag of instances and associated with a set of class labels simultaneously. Previous studies typically assume that for every training example, all positive labels are tagged whereas the untagged labels are all negative. In many real applications such as image annotation, however, the learning problem often suffers from weak label; that is, users usually tag only a part of positive labels, and the untagged labels are not necessarily negative. In this paper, we propose the MIMLwel approach which works by assuming that highly relevant labels share some common instances, and the underlying class means of bags for each label are with a large margin. Experiments validate the effectiveness of MIMLwel in handling the weak label problem.",,"Class labels; Data objects; Image annotation; Large margins; Learning problem; Multi-instance multi-label learning; Real applications; Training example; Artificial intelligence; Learning systems",Conference Paper,Scopus,2-s2.0-84896061460
"Lécué F., Pan J.Z.","Predicting knowledge in an ontology stream",2013,"IJCAI International Joint Conference on Artificial Intelligence",20,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896061807&partnerID=40&md5=1c6b974add64ccd28a357d627d8f21bc","Recently, ontology stream reasoning has been introduced as a multidisciplinary approach, merging synergies from Artificial Intelligence, Database, World-Wide-Web to reason on semantic augmented data streams. Although knowledge evolution and real-time reasoning have been largely addressed in ontology streams, the challenge of predicting its future (or missing) knowledge remains open and yet unexplored. We tackle predictive reasoning as a correlation and interpretation of past semanticsaugmented data over exogenous ontology streams. Consistent predictions are constructed as Description Logics entailments by selecting and applying relevant cross-streams association rules. The experiments have shown accurate prediction with real and live stream data from Dublin City in Ireland.",,"Accurate prediction; Data stream; Description logic; Knowledge evolution; Multi-disciplinary approach; Real-time reasoning; Stream data; Stream reasonings; Data description; Semantic Web; Semantics; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84896061807
"Bessiere C., Coletta R., Hebrard E., Katsirelos G., Lazaar N., Narodytska N., Quimper C.-G., Walsh T.","Constraint acquisition via partial queries",2013,"IJCAI International Joint Conference on Artificial Intelligence",20,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896061412&partnerID=40&md5=041600d6ee691755701b04b40539fdbb","We learn constraint networks by asking the user partial queries. That is, we ask the user to classify assignments to subsets of the variables as positive or negative. We provide an algorithm that, given a negative example, focuses onto a constraint of the target network in a number of queries logarithmic in the size of the example. We give information theoretic lower bounds for learning some simple classes of constraint networks and show that our generic algorithm is optimal in some cases. Finally we evaluate our algorithm on some benchmarks.",,"Constraint networks; Generic algorithm; Information-theoretic lower bounds; Negative examples; Artificial intelligence; Computer programming; Algorithms",Conference Paper,Scopus,2-s2.0-84896061412
"Heath III F., Boaz D., Gupta M., Vaculín R., Sun Y., Hull R., Limonad L.","Barcelona: A design and runtime environment for declarative artifact-centric BPM",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",20,10.1007/978-3-642-45005-1_65,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892389819&doi=10.1007%2f978-3-642-45005-1_65&partnerID=40&md5=7ff14db97839386d112a1294d7d03957","A promising approach to managing business operations is based on business artifacts, a.k.a. business entities (with lifecycles) [8, 6]. These are key conceptual entities that are central to guiding the operations of a business, and whose content changes as they move through those operations. A business artifact type is modeled using (a) an information model, which is intended to hold all business-relevant data about entities of this type, and (b) a lifecycle model, which is intended to hold the possible ways that an entity of this type might progress through the business. In 2010 a declarative style of business artifact lifecycles, called Guard-Stage-Milestone (GSM), was introduced [4, 5]. GSM has since been adopted [7] to form the conceptual basis of the OMG Case Management Model and Notation (CMMN) standard [1]. The Barcelona component of the recently open-sourced [2] ArtiFact system supports both design-time and run-time environments for GSM. Both of these will be illustrated in the proposed demo. © 2013 Springer-Verlag.",,"Artifact-centric; Business Artifacts; Business entities; Business operation; Case management; Information Modeling; Life cycle model; Runtime environments; Computer science; Computers; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84892389819
"Azar A.T., El-Metwally S.M.","Decision tree classifiers for automated medical diagnosis",2013,"Neural Computing and Applications",20,10.1007/s00521-012-1196-7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887478929&doi=10.1007%2fs00521-012-1196-7&partnerID=40&md5=029c2d4f842b8ebf1a366f8a724b3d1b","Decision support systems help physicians and also play an important role in medical decision-making. They are based on different models, and the best of them are providing an explanation together with an accurate, reliable and quick response. This paper presents a decision support tool for the detection of breast cancer based on three types of decision tree classifiers. They are single decision tree (SDT), boosted decision tree (BDT) and decision tree forest (DTF). Decision tree classification provides a rapid and effective method of categorizing data sets. Decision-making is performed in two stages: training the classifiers with features from Wisconsin breast cancer data set, and then testing. The performance of the proposed structure is evaluated in terms of accuracy, sensitivity, specificity, confusion matrix and receiver operating characteristic (ROC) curves. The results showed that the overall accuracies of SDT and BDT in the training phase achieved 97.07 % with 429 correct classifications and 98.83 % with 437 correct classifications, respectively. BDT performed better than SDT for all performance indices than SDT. Value of ROC and Matthews correlation coefficient (MCC) for BDT in the training phase achieved 0.99971 and 0.9746, respectively, which was superior to SDT classifier. During validation phase, DTF achieved 97.51 %, which was superior to SDT (95.75 %) and BDT (97.07 %) classifiers. Value of ROC and MCC for DTF achieved 0.99382 and 0.9462, respectively. BDT showed the best performance in terms of sensitivity, and SDT was the best only considering speed. © 2012 Springer-Verlag London.","Boosted decision tree; Computer-aided diagnosis (CAD); Decision support systems (DSS); Decision tree classification; Decision tree forest; k-fold cross-validation; Single decision tree","Boosted decision trees; Decision support system (dss); Decision tree classification; K fold cross validations; Single decision; Artificial intelligence; Classification (of information); Computer aided diagnosis; Data mining; Decision support systems; Decision trees; Diseases; Forestry; Statistical tests; Decision making; Artificial Intelligence; Classification; Decision Making; Diseases; Forestry; Statistical Analysis",Article,Scopus,2-s2.0-84887478929
"Liu X., He J., Lang B.","Reciprocal hash tables for nearest neighbor search",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",20,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893360847&partnerID=40&md5=20ae8fca5640e67bb643d8ffc8b1cad5","Recent years have witnessed the success of hashing techniques in approximate nearest neighbor search. In practice, multiple hash tables are usually employed to retrieve more desired results from all hit buckets of each table. However, there are rare works studying the unified approach to constructing multiple informative hash tables except the widely used random way. In this paper, we regard the table construction as a selection problem over a set of candidate hash functions. With the graph representation of the function set, we propose an efficient solution that sequentially applies normalized dominant set to finding the most informative and independent hash functions for each table. To further reduce the redundancy between tables, we explore the reciprocal hash tables in a boosting manner, where the hash function graph is updated with high weights emphasized on the misclassified neighbor pairs of previous hash tables. The construction method is general and compatible with different types of hashing algorithms using different feature spaces and/or parameter settings. Extensive experiments on two large-scale benchmarks demonstrate that the proposed method outperforms both naive construction method and state-of-the-art hashing algorithms, with up to 65.93% accuracy gains. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Approximate Nearest Neighbor Search; Construction method; Graph representation; Hashing algorithms; Hashing techniques; Nearest Neighbor search; Parameter setting; Selection problems; Artificial intelligence; Hash functions",Conference Paper,Scopus,2-s2.0-84893360847
"Dibangoye J.S., Amato C., Buffet O., Charpillet F.","Optimally solving Dec-POMDPs as continuous-state MDPs",2013,"IJCAI International Joint Conference on Artificial Intelligence",20,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896064243&partnerID=40&md5=6c17b524a989cdbe7d4b7c1a9969e862","Optimally solving decentralized partially observable Markov decision processes (Dec-POMDPs) is a hard combinatorial problem. Current algorithms search through the space of full histories for each agent. Because of the doubly exponential growth in the number of policies in this space as the planning horizon increases, these methods quickly become intractable. However, in real world problems, computing policies over the full history space is often unnecessary. True histories experienced by the agents often lie near a structured, low-dimensional manifold embedded into the history space. We show that by transforming a Dec-POMDP into a continuous-state MDP, we are able to find and exploit these low-dimensional representations. Using this novel transformation, we can then apply powerful techniques for solving POMDPs and continuous-state MDPs. By combining a general search algorithm and dimension reduction based on feature selection, we introduce a novel approach to optimally solve problems with significantly longer planning horizons than previous methods.",,"Combinatorial problem; Computing policies; Dimension reduction; Exponential growth; Low-dimensional manifolds; Low-dimensional representation; Partially observable Markov decision process; Real-world problem; Artificial intelligence; Algorithms",Conference Paper,Scopus,2-s2.0-84896064243
"Magka D., Krötzsch M., Horrocks I.","Computing stable models for nonmonotonic existential rules",2013,"IJCAI International Joint Conference on Artificial Intelligence",20,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063267&partnerID=40&md5=5d994435c48bd21f0847a1ab25238e14","In this work, we consider function-free existential rules extended with nonmonotonic negation under a stable model semantics. We present new acyclicity and stratification conditions that identify a large class of rule sets having finite, unique stable models, and we show how the addition of constraints on the input facts can further extend this class. Checking these conditions is computationally feasible, and we provide tight complexity bounds. Finally, we demonstrate how these new methods allowed us to solve relevant reasoning problems over a real-world knowledge base from biochemistry using an off-the-shelf answer set programming engine.",,"Answer set programming; Complexity bounds; Knowledge base; Non-monotonic negation; Nonmonotonic; Reasoning problems; Stable model; Stable model semantics; Knowledge based systems; Logic programming; Semantics; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84896063267
"Cai X., Nie F., Huang H.","Exact top-k feature selection via ℓ2;0-norm constraint",2013,"IJCAI International Joint Conference on Artificial Intelligence",20,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896061418&partnerID=40&md5=774f761d5fc74d81f44745a8f4aea49b","In this paper, we propose a novel robust and pragmatic feature selection approach. Unlike those sparse learning based feature selection methods which tackle the approximate problem by imposing sparsity regularization in the objective function, the proposed method only has one ℓ2;0-norm loss term with an explicit ℓ2;0-Norm equality constraint. An efficient algorithm based on augmented Lagrangian method will be derived to solve the above constrained optimization problem to find out the stable local solution. Extensive experiments on four biological datasets show that although our proposed model is not a convex problem, it outperforms the approximate convex counterparts and state-ofart feature selection methods evaluated in terms of classification accuracy by two popular classifiers. What is more, since the regularization parameter of our method has the explicit meaning, i.e. The number of feature selected, it avoids the burden of tuning the parameter, making it a pragmatic feature selection method.",,"Augmented Lagrangian methods; Classification accuracy; Constrained optimi-zation problems; Equality constraints; Feature selection methods; Objective functions; Regularization parameters; Sparsity regularizations; Algorithms; Artificial intelligence; Constrained optimization; Lagrange multipliers; Classification (of information)",Conference Paper,Scopus,2-s2.0-84896061418
"Santos G., Proenca H.","Periocular biometrics: An emerging technology for unconstrained scenarios",2013,"IEEE Workshop on Computational Intelligence in Biometrics and Identity Management, CIBIM",20,10.1109/CIBIM.2013.6607908,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891527843&doi=10.1109%2fCIBIM.2013.6607908&partnerID=40&md5=fb2d92d4926ea64a5150e53b32bea3c2","The periocular region has recently emerged as a promising trait for unconstrained biometric recognition, specially on cases where neither the iris and a full facial picture can be obtained. Previous studies concluded that the regions in the vicinity of the human eye - the periocular region- have surprisingly high discriminating ability between individuals, are relatively permanent and easily acquired at large distances. Hence, growing attention has been paid to periocular recognition methods, on the performance levels they are able to achieve, and on the correlation of the responses given by other. This work overviews the most relevant research works in the scope of periocular recognition: summarizes the developed methods, and enumerates the current issues, providing a comparative overview. For contextualization, a brief overview of the biometric field is also given. © 2013 IEEE.",,"Biometric recognition; Contextualization; Discriminating abilities; Emerging technologies; Human eye; Performance level; Periocular; Periocular recognition; Artificial intelligence; Biometrics",Conference Paper,Scopus,2-s2.0-84891527843
"Ahirwal M.K., Kumar A., Singh G.K.","EEG/ERP adaptive noise canceller design with controlled search space (CSS) approach in cuckoo and other optimization algorithms",2013,"IEEE/ACM Transactions on Computational Biology and Bioinformatics",20,10.1109/TCBB.2013.119,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894598585&doi=10.1109%2fTCBB.2013.119&partnerID=40&md5=ab8ba2e4688bf07eac8691feb514e482","This paper explores the migration of adaptive filtering with swarm intelligence/evolutionary techniques employed in the field of electroencephalogram/event-related potential noise cancellation or extraction. A new approach is proposed in the form of controlled search space to stabilize the randomness of swarm intelligence techniques especially for the EEG signal. Swarm-based algorithms such as Particles Swarm Optimization, Artificial Bee Colony, and Cuckoo Optimization Algorithm with their variants are implemented to design optimized adaptive noise canceler. The proposed controlled search space technique is tested on each of the swarm intelligence techniques and is found to be more accurate and powerful. Adaptive noise canceler with traditional algorithms such as least-mean-square, normalized least-mean-square, and recursive least-mean-square algorithms are also implemented to compare the results. ERP signals such as simulated visual evoked potential, real visual evoked potential, and real sensorimotor evoked potential are used, due to their physiological importance in various EEG studies. Average computational time and shape measures of evolutionary techniques are observed 8.21E-01 sec and 1.73E-01, respectively. Though, traditional algorithms take negligible time consumption, but are unable to offer good shape preservation of ERP, noticed as average computational time and shape measure difference, 1.41E-02 sec and 2.60E+00, respectively. © 2013 IEEE.","ABC; Adaptive noise canceler; COA; EEG/ERP; Evolutionary techniques; PSO","algorithm; animal; article; artificial intelligence; computer simulation; electroencephalography; evoked response; human; methodology; regression analysis; reproducibility; signal processing; Algorithms; Animals; Artificial Intelligence; Computer Simulation; Electroencephalography; Evoked Potentials; Humans; Least-Squares Analysis; Reproducibility of Results; Signal Processing, Computer-Assisted",Article,Scopus,2-s2.0-84894598585
"Li N., Yi W., Bi Z., Kong H., Gong G.","An optimisation method for complex product design",2013,"Enterprise Information Systems",20,10.1080/17517575.2012.743682,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884350717&doi=10.1080%2f17517575.2012.743682&partnerID=40&md5=fc80b1d1485366a771a68dbe4257c1c3","Designing a complex product such as an aircraft usually requires both qualitative and quantitative data and reasoning. To assist the design process, a critical issue is how to represent qualitative data and utilise it in the optimisation. In this study, a new method is proposed for the optimal design of complex products: to make the full use of available data, information and knowledge, qualitative reasoning is integrated into the optimisation process. The transformation and fusion of qualitative and qualitative data are achieved via the fuzzy sets theory and a cloud model. To shorten the design process, parallel computing is implemented to solve the formulated optimisation problems. A parallel adaptive hybrid algorithm (PAHA) has been proposed. The performance of the new algorithm has been verified by a comparison with the results from PAHA and two other existing algorithms. Further, PAHA has been applied to determine the shape parameters of an aircraft model for aerodynamic optimisation purpose. © 2013 Copyright Taylor and Francis Group, LLC.","aircraft design; genetic algorithm; heuristic algorithm; parallel computing; qualitative and quantitative data; simulated annealing","Aerodynamic optimisation; Aircraft design; Fuzzy sets theory; Hybrid algorithms; Optimisation method; Optimisation problems; Qualitative reasoning; Quantitative data; Aerodynamics; Aircraft models; Artificial intelligence; Fuzzy sets; Genetic algorithms; Heuristic algorithms; Metadata; Parallel architectures; Parallel processing systems; Simulated annealing; Product design",Article,Scopus,2-s2.0-84884350717
"Yager R.R., Alajlan N.","Decision making with ordinal payoffs under Dempster-Shafer type uncertainty",2013,"International Journal of Intelligent Systems",20,10.1002/int.21615,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884284648&doi=10.1002%2fint.21615&partnerID=40&md5=467d588a9b02f5619434423723aa878f","Our focus is on decision making in uncertain environments. We first introduce the Dempster-Shafer framework to model the uncertainty associated with possible outcomes. We then describe an approach for decision making when our uncertainty is captured using the Dempster-Shafer model and where the payoffs are numeric values. An important part of this approach is the role of the decision attitude as well as the aggregation of the possible payoffs. We then look at the situation where the payoffs, rather than being numbers, are values drawn from an ordinal scale. This requires us to provide appropriate operations for combining payoffs drawn from an ordinal scale. © 2013 Wiley Periodicals, Inc.",,"Decision attitudes; Dempster-shafer; Dempster-Shafer model; Numeric values; Ordinal scale; Uncertain environments; Artificial intelligence; Software engineering; Decision making",Article,Scopus,2-s2.0-84884284648
"Becker C., Rigamonti R., Lepetit V., Fua P.","Supervised feature learning for curvilinear structure segmentation",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",20,10.1007/978-3-642-40811-3_66,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885782468&doi=10.1007%2f978-3-642-40811-3_66&partnerID=40&md5=ddb32b22d55cbf6976a0e3a2e2b1b132","We present a novel, fully-discriminative method for curvilinear structure segmentation that simultaneously learns a classifier and the features it relies on. Our approach requires almost no parameter tuning and, in the case of 2D images, removes the requirement for hand-designed features, thus freeing the practitioner from the time-consuming tasks of parameter and feature selection. Our approach relies on the Gradient Boosting framework to learn discriminative convolutional filters in closed form at each stage, and can operate on raw image pixels as well as additional data sources, such as the output of other methods like the Optimally Oriented Flux. We will show that it outperforms state-of-the-art curvilinear segmentation methods on both 2D images and 3D image stacks. © 2013 Springer-Verlag.",,"Additional datum; Closed form; Curvilinear structures; Feature learning; Gradient boosting; Parameter-tuning; Segmentation methods; Time-consuming tasks; Artificial intelligence; Computer science; Image segmentation",Conference Paper,Scopus,2-s2.0-84885782468
"Kindermans P.-J., Verschore H., Schrauwen B.","A unified probabilistic approach to improve spelling in an event-related potential-based brain-computer interface",2013,"IEEE Transactions on Biomedical Engineering",20,10.1109/TBME.2013.2262524,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884548142&doi=10.1109%2fTBME.2013.2262524&partnerID=40&md5=aa711381a467a81c3c209d64a914c9ac","In recent years, in an attempt to maximize performance, machine learning approaches for event-related potential (ERP) spelling have become more and more complex. In this paper, we have taken a step back as we wanted to improve the performance without building an overly complex model, that cannot be used by the community. Our research resulted in a unified probabilistic model for ERP spelling, which is based on only three assumptions and incorporates language information. On top of that, the probabilistic nature of our classifier yields a natural dynamic stopping strategy. Furthermore, our method uses the same parameters across 25 subjects from three different datasets. We show that our classifier, when enhanced with language models and dynamic stopping, improves the spelling speed and accuracy drastically. Additionally, we would like to point out that as our model is entirely probabilistic, it can easily be used as the foundation for complex systems in future work. All our experiments are executed on publicly available datasets to allow for future comparison with similar techniques. © 1964-2012 IEEE.","Brain-computer interface (BCI); Dynamic stopping; Event-related potential; Language models; Machine learning; P300","Event related potentials; Language informations; Language model; Machine learning approaches; Natural dynamics; P300; Probabilistic approaches; Probabilistic modeling; Computational linguistics; Learning systems; Brain computer interface; accuracy; article; brain computer interface; classifier; event related potential; language; machine learning; spelling; Algorithms; Artificial Intelligence; Brain-Computer Interfaces; Data Interpretation, Statistical; Electroencephalography; Evoked Potentials, Visual; Humans; Language; Visual Cortex; Writing",Article,Scopus,2-s2.0-84884548142
"Stojanova D., Ceci M., Malerba D., Dzeroski S.","Using PPI network autocorrelation in hierarchical multi-label classification trees for gene function prediction",2013,"BMC Bioinformatics",20,10.1186/1471-2105-14-285,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884553372&doi=10.1186%2f1471-2105-14-285&partnerID=40&md5=5732ac5488d08a0b4365a48e4a1753cd","Background: Ontologies and catalogs of gene functions, such as the Gene Ontology (GO) and MIPS-FUN, assume that functional classes are organized hierarchically, that is, general functions include more specific ones. This has recently motivated the development of several machine learning algorithms for gene function prediction that leverages on this hierarchical organization where instances may belong to multiple classes. In addition, it is possible to exploit relationships among examples, since it is plausible that related genes tend to share functional annotations. Although these relationships have been identified and extensively studied in the area of protein-protein interaction (PPI) networks, they have not received much attention in hierarchical and multi-class gene function prediction. Relations between genes introduce autocorrelation in functional annotations and violate the assumption that instances are independently and identically distributed (i.i.d.), which underlines most machine learning algorithms. Although the explicit consideration of these relations brings additional complexity to the learning process, we expect substantial benefits in predictive accuracy of learned classifiers.Results: This article demonstrates the benefits (in terms of predictive accuracy) of considering autocorrelation in multi-class gene function prediction. We develop a tree-based algorithm for considering network autocorrelation in the setting of Hierarchical Multi-label Classification (HMC). We empirically evaluate the proposed algorithm, called NHMC (Network Hierarchical Multi-label Classification), on 12 yeast datasets using each of the MIPS-FUN and GO annotation schemes and exploiting 2 different PPI networks. The results clearly show that taking autocorrelation into account improves the predictive performance of the learned models for predicting gene function.Conclusions: Our newly developed method for HMC takes into account network information in the learning phase: When used for gene function prediction in the context of PPI networks, the explicit consideration of network autocorrelation increases the predictive performance of the learned models. Overall, we found that this holds for different gene features/ descriptions, functional annotation schemes, and PPI networks: Best results are achieved when the PPI network is dense and contains a large proportion of function-relevant interactions. © 2013 Stojanova et al.; licensee BioMed Central Ltd.",,"Functional annotation; Gene function prediction; Hierarchical multi-label classifications; Hierarchical organizations; Independently and identically distributed; Predictive performance; Protein-protein interaction networks; Tree-based algorithms; Autocorrelation; Classification (of information); Data processing; Genes; Learning algorithms; Learning systems; Proteins; Complex networks; Saccharomyces cerevisiae protein; algorithm; article; artificial intelligence; biology; classification; gene ontology; genetic database; genetics; metabolism; methodology; molecular genetics; protein protein interaction; Algorithms; Artificial Intelligence; Computational Biology; Databases, Genetic; Gene Ontology; Molecular Sequence Annotation; Protein Interaction Maps; Saccharomyces cerevisiae Proteins",Article,Scopus,2-s2.0-84884553372
"Li B., Ray B.H., Leister K.J., Ryder A.G.","Performance monitoring of a mammalian cell based bioprocess using Raman spectroscopy",2013,"Analytica Chimica Acta",20,10.1016/j.aca.2013.07.058,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883555277&doi=10.1016%2fj.aca.2013.07.058&partnerID=40&md5=5b1231ca6624b3b359434b962b74d08d","Being able to predict the final product yield at all stages in long-running, industrial, mammalian cell culture processes is vital for both operational efficiency, process consistency, and the implementation of quality by design (QbD) practices. Here we used Raman spectroscopy to monitor (in terms of glycoprotein yield prediction) a fed-batch fermentation from start to finish. Raman data were collected from 12 different time points in a Chinese hamster ovary (CHO) based manufacturing process and across 37 separate production runs. The samples comprised of clarified bioprocess broths extracted from the CHO cell based process with varying amounts of fresh and spent cell culture media. Competitive adaptive reweighted sampling (CoAdReS) and ant colony optimization (ACO) variable selection methods were used to enhance the predictive ability of the chemometric models by removing unnecessary spectral information. Using CoAdReS accurate prediction models (relative error of predictions between 2.1% and 3.3%) were built for the final glycoprotein yield at every stage of the bioprocess from small scale up to the final 5000. L bioreactor. This result reinforces our previous studies which indicate that media quality is one of the most significant factors determining the efficiency of industrial CHO-cell processes. This Raman based approach could thus be used to manage production in terms of selecting which small scale batches are progressed to large-scale manufacture, thus improving process efficiency significantly. © 2013 Elsevier B.V.","Bioprocess; Chemometrics; CHO cell; Glycoprotein; Raman spectroscopy; Variable selection","Ant Colony Optimization (ACO); Bioprocesses; Chemometrics; CHO cell; Fed-batch fermentation; Operational efficiencies; Variable selection; Variable selection methods; Artificial intelligence; Cell culture; Efficiency; Fermentation; Glycoproteins; Manufacture; Raman spectroscopy; glycoprotein; accuracy; analytical error; animal cell; article; batch process; biological monitoring; bioprocess; bioreactor; chemometrics; CHO cell; culture medium; fed batch fermentation; mammal cell; nonhuman; prediction; priority journal; quality control; Raman spectrometry; sampling; statistical model; Cricetulus griseus; Mammalia; Bioprocess; Chemometrics; CHO cell; Glycoprotein; Raman spectroscopy; Variable selection; Animals; Bioreactors; Cell Culture Techniques; CHO Cells; Cricetinae; Cricetulus; Culture Media; Glycoproteins; Spectrum Analysis, Raman",Article,Scopus,2-s2.0-84883555277
"Vlek C., Prakken H., Renooij S., Verheij B.","Modeling crime scenarios in a Bayesian network",2013,"Proceedings of the International Conference on Artificial Intelligence and Law",20,10.1145/2514601.2514618,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883543424&doi=10.1145%2f2514601.2514618&partnerID=40&md5=ef0143b1e558180461a06dc0f2e2cbf6","Legal cases involve reasoning with evidence and with the development of a software support tool in mind, a formal foundation for evidential reasoning is required. Three approaches to evidential reasoning have been prominent in the literature: argumentation, narrative and probabilistic reasoning. In this paper a combination of the latter two is proposed. In recent research on Bayesian networks applied to legal cases, a number of legal idioms have been developed as recurring structures in legal Bayesian networks. A Bayesian network quantifies how various variables in a case interact. In the narrative approach, scenarios provide a context for the evidence in a case. A method that integrates the quantitative, numerical techniques of Bayesian networks with the qualitative, holistic approach of scenarios is lacking. In this paper, a method is proposed for modeling several scenarios in a single Bayesian network. The method is tested by doing a case study. Two new idioms are introduced: the scenario idiom and the merged scenarios idiom. The resulting network is meant to assist a judge or jury, helping to maintain a good overview of the interactions between relevant variables in a case and preventing tunnel vision by comparing various scenarios. Copyright 2013 ACM.",,"Evidential reasoning; Formal foundation; Holistic approach; Numerical techniques; Probabilistic reasoning; Recent researches; Software support; Tunnel vision; Artificial intelligence; Numerical methods; Bayesian networks",Conference Paper,Scopus,2-s2.0-84883543424
"De Nicola R., Ferrari G., Loreti M., Pugliese R.","A language-based approach to autonomic computing",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",20,10.1007/978-3-642-35887-6-2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883287136&doi=10.1007%2f978-3-642-35887-6-2&partnerID=40&md5=57fb54369b911147ea18bc99bd4ca6a8","SCEL is a new language specifically designed to model autonomic components and their interaction. It brings together various programming abstractions that permit to directly represent knowledge, behaviors and aggregations according to specific policies. It also supports naturally programming self-awareness, context-awareness, and adaptation. In this paper, we first present design principles, syntax and operational semantics of SCEL. Then, we show how a dialect can be defined by appropriately instantiating the features of the language we left open to deal with different application domains and use this dialect to model a simple, yet illustrative, example application. Finally, we demonstrate that adaptation can be naturally expressed in SCEL. © 2013 Springer-Verlag Berlin Heidelberg.",,"Autonomic Computing; Context-awareness; Design Principles; Operational semantics; Programming abstractions; Artificial intelligence; Computer science; Computer programming",Conference Paper,Scopus,2-s2.0-84883287136
"Sadowski Ł.","Non-destructive evaluation of the pull-off adhesion of concrete floor layers using RBF neural network",2013,"Journal of Civil Engineering and Management",20,10.3846/13923730.2013.790838,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887664161&doi=10.3846%2f13923730.2013.790838&partnerID=40&md5=a8b9105090c08cab3b65da006fa65448","The interlayer bond is one of the primary qualities assessed during an inspection of floor concrete workmanship. The measure of this bond is the value of pull-off adhesion fb determined in practice by the pull-off method. The drawback of this method is that the tested floor is damaged in each of the test points and then needs to be repaired. This drawback can be overcome by developing a way which will make it possible to test floors in any point without damaging them locally. In this paper it is proposed to evaluate the pull-off adhesion of the top layer to the base layer in concrete floors by means of the radial basis function (RBF) artificial neural network using the parameters evaluated by the non-destructive acoustic impulse response technique and the non-destructive optical laser triangulation method. Presented RBF neural network model is useful tool in the non-destructive evaluation of the pull-off adhesion of concrete floor layers without the need to damage the top layer fragment from the base. Copyright © 2013 Vilnius Gediminas Technical University (VGTU) Press.","Acoustic methods; Artificial intelligence; Concrete floor; Impulse response technique; Interlayer bond; Surface roughness","Acoustic impulse response; Acoustic method; Concrete floor; Interlayer bonds; Non destructive; Non destructive evaluation; Radial Basis Function(RBF); RBF Neural Network; Adhesion; Artificial intelligence; Concrete construction; Floors; Impulse response; Neural networks; Nondestructive examination; Radial basis function networks; Surface roughness; Concretes",Article,Scopus,2-s2.0-84887664161
"Liu Y., Zhang B., Wang L.-M., Wang N.","A self-trained semisupervised SVM approach to the remote sensing land cover classification",2013,"Computers and Geosciences",20,10.1016/j.cageo.2013.03.024,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880362774&doi=10.1016%2fj.cageo.2013.03.024&partnerID=40&md5=92a710da45ef9d3b610cff3752a99d68","Support vector machines (SVM) are nowadays receiving increasing attention in remote sensing applications although this technique is very sensitive to the parameters setting and training set definition. Self-training is an effective semisupervised method, which can reduce the effort needed to prepare the training set by training the model with a small number of labeled examples and an additional set of unlabeled examples. In this study, a novel semisupervised SVM model that uses self-training approach is proposed to address the problem of remote sensing land cover classification. The key characteristics of this approach are that (1) the self-adaptive mutation particle swarm optimization algorithm is introduced to get the optimum parameters that improve the generalization performance of the SVM classifier, and (2) the Gustafson-Kessel fuzzy clustering algorithm is proposed for the selection of unlabeled points to reduce the impact of ineffective labels. The effectiveness of the proposed technique is evaluated firstly with samples from remote sensing data and then by identifying different land cover regions in the remote sensing imagery. Experimental results show that accuracy level is increased by applying this learning scheme, which results in the smallest generalization error compared with the other schemes. © 2013.","Gustafson-Kessel fuzzy clustering; Land cover classification; Self-adaptive mutation particle swarm optimization; Self-training; Semisupervised support vector machines","Gustafson-Kessel; Land cover classification; Self-adaptive mutation; Self-training; Semi-supervised; Algorithms; Fuzzy clustering; Remote sensing; Support vector machines; algorithm; artificial intelligence; land classification; land cover; optimization; remote sensing",Article,Scopus,2-s2.0-84880362774
"Urbanowicz Dr. R.J., Andrew A.S., Karagas M.R., Moore J.H.","Role of genetic heterogeneity and epistasis in bladder cancer susceptibility and outcome: A learning classifier system approach",2013,"Journal of the American Medical Informatics Association",20,10.1136/amiajnl-2012-001574,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882384758&doi=10.1136%2famiajnl-2012-001574&partnerID=40&md5=c1f4a9b15381cc773cda335d054c8e77","Background and objective: Detecting complex patterns of association between genetic or environmental risk factors and disease risk has become an important target for epidemiological research. In particular, strategies that provide multifactor interactions or heterogeneous patterns of association can offer new insights into association studies for which traditional analytic tools have had limited success. Materials and methods: To concurrently examine these phenomena, previous work has successfully considered the application of learning classifier systems (LCSs), a flexible class of evolutionary algorithms that distributes learned associations over a population of rules. Subsequent work dealt with the inherent problems of knowledge discovery and interpretation within these algorithms, allowing for the characterization of heterogeneous patterns of association. Whereas these previous advancements were evaluated using complex simulation studies, this study applied these collective works to a 'real-world' genetic epidemiology study of bladder cancer susceptibility. Results and discussion: We replicated the identification of previously characterized factors that modify bladder cancer risk-namely, single nucleotide polymorphisms from a DNA repair gene, and smoking. Furthermore, we identified potentially heterogeneous groups of subjects characterized by distinct patterns of association. Cox proportional hazard models comparing clinical outcome variables between the cases of the two largest groups yielded a significant, meaningful difference in survival time in years (survivorship). A marginally significant difference in recurrence time was also noted. These results support the hypothesis that an LCS approach can offer greater insight into complex patterns of association. Conclusions: This methodology appears to be well suited to the dissection of disease heterogeneity, a key component in the advancement of personalized medicine.",,"DNA; article; bladder cancer; cancer recurrence; cancer risk; cancer staging; cancer susceptibility; comparative study; epistasis; evolutionary algorithm; genetic association; genetic epidemiology; genetic heterogeneity; genetic risk; human; outcome assessment; outcome variable; single nucleotide polymorphism; smoking; survival time; Bladder cancer; Epistasis; Heterogeneity; Learning Classifier System; Smoking; XPD; Algorithms; Artificial Intelligence; Classification; Computational Biology; Epistasis, Genetic; Genetic Heterogeneity; Genetic Predisposition to Disease; Humans; Individualized Medicine; Polymorphism, Single Nucleotide; Urinary Bladder Neoplasms",Article,Scopus,2-s2.0-84882384758
"Tetko I.V., Novotarskyi S., Sushko I., Ivanov V., Petrenko A.E., Dieden R., Lebon F., Mathieu B.","Development of dimethyl sulfoxide solubility models using 163 000 molecules: Using a domain applicability metric to select more reliable predictions",2013,"Journal of Chemical Information and Modeling",20,10.1021/ci400213d,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883238632&doi=10.1021%2fci400213d&partnerID=40&md5=7011612f3eaa2a9879b62ae43a98b859","The dimethyl sulfoxide (DMSO) solubility data from Enamine and two UCB pharma compound collections were analyzed using 8 different machine learning methods and 12 descriptor sets. The analyzed data sets were highly imbalanced with 1.7-5.8% nonsoluble compounds. The libraries' enrichment by soluble molecules from the set of 10% of the most reliable predictions was used to compare prediction performances of the methods. The highest accuracies were calculated using a C4.5 decision classification tree, random forest, and associative neural networks. The performances of the methods developed were estimated on individual data sets and their combinations. The developed models provided on average a 2-fold decrease of the number of nonsoluble compounds amid all compounds predicted as soluble in DMSO. However, a 4-9-fold enrichment was observed if only 10% of the most reliable predictions were considered. The structural features influencing compounds to be soluble or nonsoluble in DMSO were also determined. The best models developed with the publicly available Enamine data set are freely available online at http://ochem.eu/article/33409. © 2013 American Chemical Society.",,"Associative neural network; Compound collections; Decision classification tree; Dimethyl sulfoxide (DMSO); Machine learning methods; Prediction performance; Solubility models; Structural feature; Decision trees; Dimethyl sulfoxide; Forecasting; Learning systems; Molecules; Solubility; Organic solvents; dimethyl sulfoxide; dimethyl sulfoxide; article; artificial intelligence; artificial neural network; chemistry; drug database; information science; methodology; reproducibility; solubility; statistical model; support vector machine; chemistry; information science; procedures; Artificial Intelligence; Databases, Pharmaceutical; Dimethyl Sulfoxide; Informatics; Linear Models; Neural Networks (Computer); Reproducibility of Results; Solubility; Support Vector Machines; Artificial Intelligence; Databases, Pharmaceutical; Dimethyl Sulfoxide; Informatics; Linear Models; Neural Networks (Computer); Reproducibility of Results; Solubility; Support Vector Machines",Article,Scopus,2-s2.0-84883238632
"Buijs J.C.A.M., Van Dongen B.F., Van Der Aalst W.M.P.","Mining configurable process models from collections of event logs",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",20,10.1007/978-3-642-40176-3_5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881261626&doi=10.1007%2f978-3-642-40176-3_5&partnerID=40&md5=80b83bf2488c4881abef90f7b2834f15","Existing process mining techniques are able to discover a specific process model for a given event log. In this paper, we aim to discover a configurable process model from a collection of event logs, i.e., the model should describe a family of process variants rather than one specific process. Consider for example the handling of building permits in different municipalities. Instead of discovering a process model per municipality, we want to discover one configurable process model showing commonalities and differences among the different variants. Although there are various techniques that merge individual process models into a configurable process model, there are no techniques that construct a configurable process model based on a collection of event logs. By extending our ETM genetic algorithm, we propose and compare four novel approaches to learn configurable process models from collections of event logs. We evaluate these four approaches using both a running example and a collection of real event logs. © 2013 Springer-Verlag.",,"Building permits; Configurable process models; Event logs; Process mining; Process model; Process variants; Artificial intelligence; Computer science; Enterprise resource management",Conference Paper,Scopus,2-s2.0-84881261626
"Yousefi S., Qin J., Zhi Z., Wang R.K.","Label-free optical lymphangiography: Development of an automatic segmentation method applied to optical coherence tomography to visualize lymphatic vessels using Hessian filters",2013,"Journal of Biomedical Optics",20,10.1117/1.JBO.18.8.086004,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884544400&doi=10.1117%2f1.JBO.18.8.086004&partnerID=40&md5=d76a6ddf258bbd2aa8ec84c1ddd1bfc7","Lymphatic vessels are a part of the circulatory system that collect plasma and other substances that have leaked from the capillaries into interstitial fluid (lymph) and transport lymph back to the circulatory system. Since lymph is transparent, lymphatic vessels appear as dark hallow vessel-like regions in optical coherence tomography (OCT) cross sectional images. We propose an automatic method to segment lymphatic vessel lumen from OCT structural cross sections using eigenvalues of Hessian filters. Compared to the existing method based on intensity threshold, Hessian filters are more selective on vessel shape and less sensitive to intensity variations and noise. Using this segmentation technique along with optical micro-angiography allows label-free noninvasive simultaneous visualization of blood and lymphatic vessels in vivo. Lymphatic vessels play an important role in cancer, immune system response, inflammatory disease, wound healing and tissue regeneration. Development of imaging techniques and visualization tools for lymphatic vessels is valuable in understanding the mechanisms and studying therapeutic methods in related disease and tissue response. © 2013 Society of Photo-Optical Instrumentation Engineers.","automatic segmentation; lymphatic vessels; multi-scale Hessian filters; optical coherence tomography; optical microangiography","Automatic segmentations; Cross sectional image; Inflammatory disease; Intensity variations; Lymphatic vessels; Micro-angiography; Segmentation techniques; Structural cross-section; Angiography; Blood vessels; Body fluids; Diseases; Eigenvalues and eigenfunctions; Imaging techniques; Microcirculation; Visualization; Optical tomography; algorithm; animal; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; histology; image enhancement; lymph vessel; lymphography; male; methodology; mouse; nude mouse; optical coherence tomography; sensitivity and specificity; staining; anatomy and histology; automated pattern recognition; computer assisted diagnosis; lymph vessel; lymphography; optical coherence tomography; procedures; Algorithms; Animals; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Lymphatic Vessels; Lymphography; Male; Mice; Mice, Nude; Pattern Recognition, Automated; Sensitivity and Specificity; Staining and Labeling; Tomography, Optical Coherence; Algorithms; Animals; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Lymphatic Vessels; Lymphography; Male; Mice; Mice, Nude; Pattern Recognition, Automated; Sensitivity and Specificity; Staining and Labeling; Tomography, Optical Coherence",Article,Scopus,2-s2.0-84884544400
"Schulz P.J., Nakamoto K.","Patient behavior and the benefits of artificial intelligence: The perils of ""dangerous"" literacy and illusory patient empowerment",2013,"Patient Education and Counseling",20,10.1016/j.pec.2013.05.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880329001&doi=10.1016%2fj.pec.2013.05.002&partnerID=40&md5=c4da9666890f632a21bd73e7a9b4c69d","Objective: Artificial intelligence can provide important support of patient health. However, limits to realized benefits can arise as patients assume an active role in their health decisions. Methods: Distinguishing the concepts of health literacy and patient empowerment, we analyze conditions that bias patient use of the Internet and limit access to and impact of artificial intelligence. Results: Improving health literacy in the face of the Internet requires significant guidance. Patients must be directed toward the appropriate tools and also provided with key background knowledge enabling them to use the tools and capitalize on the artificial intelligence technology. Conclusion: Benefits of tools employing artificial intelligence to promote health cannot be realized without recognizing and addressing the patients' desires, expectations, and limitations that impact their Internet behavior. In order to benefit from artificial intelligence, patients need a substantial level of background knowledge and skill in information use-i.e., health literacy. Practice implications: It is critical that health professionals respond to patient search for information on the Internet, first by guiding their search to relevant, authoritative, and responsive sources, and second by educating patients about how to interpret the information they are likely to encounter. © 2013 Elsevier Ireland Ltd.","Health literacy; Internet; Patient education; Patient empowerment","access to information; article; artificial intelligence; decision support system; empowerment; expectation; health literacy; health promotion; human; information processing; Internet; medical information; patient attitude; patient decision making; patient education; priority journal; skill; Health literacy; Internet; Patient education; Patient empowerment; Artificial Intelligence; Choice Behavior; Decision Making; Health Communication; Health Literacy; Humans; Internet; Patient Education as Topic; Patient Participation; Power (Psychology)",Article,Scopus,2-s2.0-84880329001
"Ghose S., Oliver A., Mitra J., Martí R., Lladó X., Freixenet J., Sidibé D., Vilanova J.C., Comet J., Meriaudeau F.","A supervised learning framework of statistical shape and probability priors for automatic prostate segmentation in ultrasound images",2013,"Medical Image Analysis",20,10.1016/j.media.2013.04.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877798775&doi=10.1016%2fj.media.2013.04.001&partnerID=40&md5=c4d87a21befc596804ed310a8587668b","Prostate segmentation aids in prostate volume estimation, multi-modal image registration, and to create patient specific anatomical models for surgical planning and image guided biopsies. However, manual segmentation is time consuming and suffers from inter-and intra-observer variabilities. Low contrast images of trans rectal ultrasound and presence of imaging artifacts like speckle, micro-calcifications, and shadow regions hinder computer aided automatic or semi-automatic prostate segmentation. In this paper, we propose a prostate segmentation approach based on building multiple mean parametric models derived from principal component analysis of shape and posterior probabilities in a multi-resolution framework. The model parameters are then modified with the prior knowledge of the optimization space to achieve optimal prostate segmentation. In contrast to traditional statistical models of shape and intensity priors, we use posterior probabilities of the prostate region determined from random forest classification to build our appearance model, initialize and propagate our model. Furthermore, multiple mean models derived from spectral clustering of combined shape and appearance parameters are applied in parallel to improve segmentation accuracies. The proposed method achieves mean Dice similarity coefficient value of 0.91. ±. 0.09 for 126 images containing 40 images from the apex, 40 images from the base and 46 images from central regions in a leave-one-patient-out validation framework. The mean segmentation time of the procedure is 0.67. ±. 0.02. s. © 2013 Elsevier B.V.","Prostate segmentation; Random forest classification; Spectral clustering; Statistical shape and appearance model","Intra-observer variability; Multimodal image registration; Prostate segmentation; Random forest classification; Segmentation accuracy; Similarity coefficients; Spectral clustering; Statistical shapes; Decision trees; Face recognition; Optimization; Principal component analysis; Probability; Ultrasonics; Urology; Image segmentation; accuracy; article; human; image analysis; male; model; principal component analysis; priority journal; probability; prostate segmentation; prostate volume; random forest; ultrasound; validation process; Algorithms; Artificial Intelligence; Data Interpretation, Statistical; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Male; Pattern Recognition, Automated; Prostate; Prostatic Neoplasms; Reproducibility of Results; Sensitivity and Specificity; Ultrasonography",Article,Scopus,2-s2.0-84877798775
"Chen F., Wang H., Qi C., Xie Y.","An ant colony optimization routing algorithm for two order pickers with congestion consideration",2013,"Computers and Industrial Engineering",20,10.1016/j.cie.2013.06.013,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880557254&doi=10.1016%2fj.cie.2013.06.013&partnerID=40&md5=18399490dc00cd81ea7a92bfec17cefa","This paper develops a routing method to control the picker congestion that challenges the traditional assumption regarding the narrow-aisle order picking system. We proposes a new routing algorithm based on Ant Colony Optimization (ACO) for two order pickers (A-TOP) with congestion consideration. Using two extended dedicated heuristics with congestion consideration as reference group, a comprehensive simulation study is conducted to evaluate the effectiveness of A-TOP. The simulation proves that A-TOP achieves the shortest total picking time in most instances and performs well in dealing with the congestion. The impacts of warehouse layout, order size, and pick:walk-time ratio on A-TOP and system performance are analyzed as well. A-TOP can adapt to different warehouse configurations, meanwhile, it can be easily extended to the situation with more than two order pickers. © 2013 Elsevier Ltd. All rights reserved.","Ant colony optimization; Congestion; Order picking; Routing method; Warehouse management","Ant Colony Optimization (ACO); Congestion; Order picking; Order-picking systems; Routing methods; Simulation studies; Warehouse configurations; Warehouse management; Algorithms; Ant colony optimization; Routing algorithms; Warehouses; Artificial intelligence",Article,Scopus,2-s2.0-84880557254
"Beck J.E., Gong Y.","Wheel-spinning: Students who fail to master a skill",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",20,10.1007/978-3-642-39112-5-44,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880014270&doi=10.1007%2f978-3-642-39112-5-44&partnerID=40&md5=600f93885c9f72461b40a410e1ae980e","The concept of mastery learning is powerful: rather than a fixed number of practices, students continue to practice a skill until they have mastered it. However, an implicit assumption in this formulation is that students are capable of mastering the skill. Such an assumption is crucial in computer tutors, as their repertoire of teaching actions may not be as effective as commonly believed. What if a student lacks sufficient knowledge to solve problems involving the skill, and the computer tutor is not capable of providing sufficient instruction? This paper introduces the concept of wheel-spinning; that is, students who do not succeed in mastering a skill in a timely manner. We show that if a student does not master a skill in ASSISTments or the Cognitive Tutor quickly, the student is likely to struggle and will probably never master the skill. We discuss connections between such lack of learning and negative student behaviors such as gaming and disengagement, and discuss alterations to ITS design to overcome this issue. © 2013 Springer-Verlag Berlin Heidelberg.","Mastery learning; Student modeling; Wheel-spinning","Cognitive Tutors; Fixed numbers; Mastery learning; Student behavior; Student Modeling; Wheel-spinning; Artificial intelligence; Highway planning; Teaching; Wheels; Students",Conference Paper,Scopus,2-s2.0-84880014270
"Gueth P., Dauvergne D., Freud N., Létang J.M., Ray C., Testa E., Sarrut D.","Machine learning-based patient specific prompt-gamma dose monitoring in proton therapy",2013,"Physics in Medicine and Biology",20,10.1088/0031-9155/58/13/4563,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879495172&doi=10.1088%2f0031-9155%2f58%2f13%2f4563&partnerID=40&md5=88b0a4a3984ba70f88544ecad88f1775","Online dose monitoring in proton therapy is currently being investigated with prompt-gamma (PG) devices. PG emission was shown to be correlated with dose deposition. This relationship is mostly unknown under real conditions. We propose a machine learning approach based on simulations to create optimized treatment-specific classifiers that detect discrepancies between planned and delivered dose. Simulations were performed with the Monte-Carlo platform Gate/Geant4 for a spot-scanning proton therapy treatment and a PG camera prototype currently under investigation. The method first builds a learning set of perturbed situations corresponding to a range of patient translation. This set is then used to train a combined classifier using distal falloff and registered correlation measures. Classifier performances were evaluated using receiver operating characteristic curves and maximum associated specificity and sensitivity. A leave-one-out study showed that it is possible to detect discrepancies of 5 mm with specificity and sensitivity of 85% whereas using only distal falloff decreases the sensitivity down to 77% on the same data set. The proposed method could help to evaluate performance and to optimize the design of PG monitoring devices. It is generic: other learning sets of deviations, other measures and other types of classifiers could be studied to potentially reach better performance. At the moment, the main limitation lies in the computation time needed to perform the simulations. © 2013 Institute of Physics and Engineering in Medicine.",,"Better performance; Classifier performance; Combined classifiers; Correlation measures; Machine learning approaches; Monitoring device; Patient specific; Receiver operating characteristic curves; Learning systems; Optimization; Proton beams; proton; proton; artificial intelligence; computer assisted radiotherapy; computer assisted tomography; gamma radiation; human; image guided radiotherapy; male; megavoltage radiotherapy; Monte Carlo method; procedures; Prostatic Neoplasms; radiation dose; radiography; radiometry; reproducibility; sensitivity and specificity; article; computer assisted radiotherapy; image guided radiotherapy; megavoltage radiotherapy; methodology; prostate tumor; radiometry; Artificial Intelligence; Gamma Rays; Humans; Male; Monte Carlo Method; Prostatic Neoplasms; Protons; Radiometry; Radiotherapy Dosage; Radiotherapy Planning, Computer-Assisted; Radiotherapy, High-Energy; Radiotherapy, Image-Guided; Reproducibility of Results; Sensitivity and Specificity; Tomography, X-Ray Computed; Artificial Intelligence; Gamma Rays; Humans; Male; Monte Carlo Method; Prostatic Neoplasms; Protons; Radiometry; Radiotherapy Dosage; Radiotherapy Planning, Computer-Assisted; Radiotherapy, High-Energy; Radiotherapy, Image-Guided; Reproducibility of Results; Sensitivity and Specificity; Tomography, X-Ray Computed",Article,Scopus,2-s2.0-84879495172
"Dzotsi K.A., Basso B., Jones J.W.","Development, uncertainty and sensitivity analysis of the simple SALUS crop model in DSSAT",2013,"Ecological Modelling",20,10.1016/j.ecolmodel.2013.03.017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877322727&doi=10.1016%2fj.ecolmodel.2013.03.017&partnerID=40&md5=996bf8578f223c6ec9033eba524135ed","Simplified approaches to modeling crop growth and development have recently received more attention due to increased interest in applying crop models at large scales for various agricultural assessments. In this study, we integrated the simple version of SALUS (System Approach to Land Use Sustainability) crop model in the widely-used Decision Support System for Agrotechnology Transfer (DSSAT) to enhance the capability of DSSAT to simulate additional crops without requiring detailed parameterization. An uncertainty and sensitivity analysis was conducted using the integrated DSSAT-simple SALUS model to assess the variability in model outputs and crop parameter ranking in response to uncertainties associated with crop parameters required by the model. The influence of year, production level, and location on the effect of crop parameter uncertainty was also investigated.Parameter uncertainty resulted in a high variability in modeled outputs. Simulated potential aboveground biomass ranged from 1.2tha-1 to 38tha-1 for maize and 4tha-1 to 26.5tha-1 for peanut and cotton, all locations and years considered. The degree of variability was dependent upon the production level, the location, the year, and the crop. Ranking of crop parameters was not significantly affected by the year of study but was strongly related to the production level, location, and crop. The model was not sensitive to parameters related to prediction of the timing of germination and emergence. The most influential parameters were related to leaf area index growth, crop duration, and thermal time accumulation. Findings from this study contributed to understanding the effects of crop parameter uncertainty on the model's outputs under different environmental conditions. © 2013 Elsevier B.V.","Correlation; Cotton; Crop modeling; Latin hypercube; Maize; Peanut","Agrotechnology transfer; Crop growth and development; Crop modeling; Environmental conditions; Latin hypercube; Maize; Peanut; Uncertainty and sensitivity analysis; Artificial intelligence; Cotton; Decision support systems; Oilseeds; Optical correlation; Sensitivity analysis; Uncertainty analysis; Crops; aboveground biomass; agricultural modeling; correlation; cotton; crop production; decision support system; growth rate; legume; maize; sensitivity analysis; uncertainty analysis",Article,Scopus,2-s2.0-84877322727
"Namasivayam V., Hu Y., Balfer J., Bajorath J.","Classification of compounds with distinct or overlapping multi-target activities and diverse molecular mechanisms using emerging chemical patterns",2013,"Journal of Chemical Information and Modeling",20,10.1021/ci400186n,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879586122&doi=10.1021%2fci400186n&partnerID=40&md5=17212f26000f7cddd8ea41b1641991e8","The emerging chemical patterns (ECP) approach has been introduced for compound classification. Thus far, only very few ECP applications have been reported. Here, we further investigate the ECP methodology by studying complex classification problems. The analysis involves multi-target data sets with systematically organized subsets of compounds having distinct or overlapping target activities and, in addition, data sets containing classes of specifically active compounds with different mechanism-of-action. In systematic classification trials focusing on individual compound subsets or mechanistic classes, ECP calculations utilizing numerical descriptors achieve moderate to high sensitivity, dependent on the data set, and consistently high specificity. Accurate ECP predictions are already obtained on the basis of very small learning sets with only three positive training instances, which distinguishes the ECP approach from many other machine learning techniques. © 2013 American Chemical Society.",,"Active compounds; Chemical pattern; High sensitivity; High specificity; Learning sets; Machine learning techniques; Molecular mechanism; Target activity; Learning systems; Chemical compounds; drug; G protein coupled receptor; drug; agonists; antagonists and inhibitors; artificial intelligence; chemistry; classification; drug database; human; pharmacology; article; chemistry; classification; drug antagonism; drug potentiation; Artificial Intelligence; Databases, Pharmaceutical; Humans; Pharmaceutical Preparations; Pharmacology; Receptors, G-Protein-Coupled; Artificial Intelligence; Databases, Pharmaceutical; Humans; Pharmaceutical Preparations; Pharmacology; Receptors, G-Protein-Coupled",Article,Scopus,2-s2.0-84879586122
"Feng P.-M., Ding H., Chen W., Lin H.","Naïve bayes classifier with feature selection to identify phage virion proteins",2013,"Computational and Mathematical Methods in Medicine",20,10.1155/2013/530696,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878657414&doi=10.1155%2f2013%2f530696&partnerID=40&md5=f7cc027fa8193ba1fba0d2c5d18d79c1","Knowledge about the protein composition of phage virions is a key step to understand the functions of phage virion proteins. However, the experimental method to identify virion proteins is time consuming and expensive. Thus, it is highly desirable to develop novel computational methods for phage virion protein identification. In this study, a Naïve Bayes based method was proposed to predict phage virion proteins using amino acid composition and dipeptide composition. In order to remove redundant information, a novel feature selection technique was employed to single out optimized features. In the jackknife test, the proposed method achieved an accuracy of 79.15% for phage virion and nonvirion proteins classification, which are superior to that of other state-of-the-art classifiers. These results indicate that the proposed method could be as an effective and promising high-throughput method in phage proteomics research. © 2013 Peng-Mian Feng et al.",,"bacterial protein; virus protein; amino acid; virus protein; virus protein; article; bacteriophage; Bayesian learning; nonhuman; protein analysis; proteomics; algorithm; artificial intelligence; Bayes theorem; biology; chemistry; classification; comparative study; evaluation study; protein database; statistics and numerical data; virion; chemistry; classification; statistics; virion; Algorithms; Amino Acids; Artificial Intelligence; Bayes Theorem; Computational Biology; Databases, Protein; Proteomics; Viral Proteins; Virion; Algorithms; Amino Acids; Artificial Intelligence; Bayes Theorem; Computational Biology; Databases, Protein; Proteomics; Viral Proteins; Virion",Article,Scopus,2-s2.0-84878657414
"Wang H.-Q., Li J.-S., Zhang Y.-F., Suzuki M., Araki K.","Creating personalised clinical pathways by semantic interoperability with electronic health records",2013,"Artificial Intelligence in Medicine",20,10.1016/j.artmed.2013.02.005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878109502&doi=10.1016%2fj.artmed.2013.02.005&partnerID=40&md5=3224883054ad43c262e8792db5f50568","Objective: There is a growing realisation that clinical pathways (CPs) are vital for improving the treatment quality of healthcare organisations. However, treatment personalisation is one of the main challenges when implementing CPs, and the inadequate dynamic adaptability restricts the practicality of CPs. The purpose of this study is to improve the practicality of CPs using semantic interoperability between knowledge-based CPs and semantic electronic health records (EHRs). Methods: Simple protocol and resource description framework query language is used to gather patient information from semantic EHRs. The gathered patient information is entered into the CP ontology represented by web ontology language. Then, after reasoning over rules described by semantic web rule language in the Jena semantic framework, we adjust the standardised CPs to meet different patients' practical needs. Results: A CP for acute appendicitis is used as an example to illustrate how to achieve CP customisation based on the semantic interoperability between knowledge-based CPs and semantic EHRs. A personalised care plan is generated by comprehensively analysing the patient's personal allergy history and past medical history, which are stored in semantic EHRs. Additionally, by monitoring the patient's clinical information, an exception is recorded and handled during CP execution. According to execution results of the actual example, the solutions we present are shown to be technically feasible. Conclusion: This study contributes towards improving the clinical personalised practicality of standardised CPs. In addition, this study establishes the foundation for future work on the research and development of an independent CP system. © 2013 Elsevier B.V.","Clinical pathway; Electronic health record; Knowledge base; Semantic interoperability","Clinical pathways; Electronic health record; Electronic health record (EHRs); Knowledge base; Research and development; Resource description framework; Semantic interoperability; Semantic Web rule language (SWRL); Health care; Knowledge based systems; Query languages; Records management; Interoperability; acute appendicitis; appendectomy; article; clinical decision making; clinical pathway; electronic medical record; health care need; health care quality; human; knowledge base; knowledge management; markup language; medical history; medical information system; patient care; patient information; patient monitoring; personalized medicine; priority journal; Appendicitis; Artificial Intelligence; Critical Pathways; Data Mining; Electronic Health Records; Humans; Individualized Medicine; Knowledge Bases; Programming Languages; Quality Improvement; Semantics; Systems Integration; Terminology as Topic; Therapy, Computer-Assisted; User-Computer Interface",Article,Scopus,2-s2.0-84878109502
"Fekete T., Wilf M., Rubin D., Edelman S., Malach R., Mujica-Parodi L.R.","Combining Classification with fMRI-Derived Complex Network Measures for Potential Neurodiagnostics",2013,"PLoS ONE",20,10.1371/journal.pone.0062867,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877092317&doi=10.1371%2fjournal.pone.0062867&partnerID=40&md5=4e04fb7b639c148f7b0edd218f5ec866","Complex network analysis (CNA), a subset of graph theory, is an emerging approach to the analysis of functional connectivity in the brain, allowing quantitative assessment of network properties such as functional segregation, integration, resilience, and centrality. Here, we show how a classification framework complements complex network analysis by providing an efficient and objective means of selecting the best network model characterizing given functional connectivity data. We describe a novel kernel-sum learning approach, block diagonal optimization (BDopt), which can be applied to CNA features to single out graph-theoretic characteristics and/or anatomical regions of interest underlying discrimination, while mitigating problems of multiple comparisons. As a proof of concept for the method's applicability to future neurodiagnostics, we apply BDopt classification to two resting state fMRI data sets: a trait (between-subjects) classification of patients with schizophrenia vs. controls, and a state (within-subjects) classification of wake vs. sleep, demonstrating powerful discriminant accuracy for the proposed framework. © 2013 Fekete et al.",,"accuracy; adult; article; classifier; clinical article; complex network analysis; controlled study; functional magnetic resonance imaging; human; kernel method; learning algorithm; male; neuroimaging; post hoc analysis; schizophrenia; sensitivity and specificity; sleep; support vector machine; wakefulness; Adult; Algorithms; Artificial Intelligence; Brain; Brain Mapping; Case-Control Studies; Female; Humans; Magnetic Resonance Imaging; Male; Middle Aged; Models, Neurological; Nerve Net; Neural Pathways; Schizophrenia; Sleep; Young Adult",Article,Scopus,2-s2.0-84877092317
"Gijsberts A., Metta G.","Real-time model learning using Incremental Sparse Spectrum Gaussian Process Regression",2013,"Neural Networks",20,10.1016/j.neunet.2012.08.011,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875887187&doi=10.1016%2fj.neunet.2012.08.011&partnerID=40&md5=ac6ebc20c8bbb2a939ed78bacea9dd6c","Novel applications in unstructured and non-stationary human environments require robots that learn from experience and adapt autonomously to changing conditions. Predictive models therefore not only need to be accurate, but should also be updated incrementally in real-time and require minimal human intervention. Incremental Sparse Spectrum Gaussian Process Regression is an algorithm that is targeted specifically for use in this context. Rather than developing a novel algorithm from the ground up, the method is based on the thoroughly studied Gaussian Process Regression algorithm, therefore ensuring a solid theoretical foundation. Non-linearity and a bounded update complexity are achieved simultaneously by means of a finite dimensional random feature mapping that approximates a kernel function. As a result, the computational cost for each update remains constant over time. Finally, algorithmic simplicity and support for automated hyperparameter optimization ensures convenience when employed in practice. Empirical validation on a number of synthetic and real-life learning problems confirms that the performance of Incremental Sparse Spectrum Gaussian Process Regression is superior with respect to the popular Locally Weighted Projection Regression, while computational requirements are found to be significantly lower. The method is therefore particularly suited for learning with real-time constraints or when computational resources are limited. © 2012 Elsevier Ltd.","Function approximation; Incremental learning; Online learning; Real-time; Robotics","Function approximation; Gaussian process regression; Hyper-parameter optimizations; Incremental learning; Locally weighted projection regressions; Online learning; Real-time; Sparse spectrum gaussian process; Algorithms; Gaussian distribution; Gaussian noise (electronic); Robotics; Regression analysis; article; automation; Bayes theorem; data analysis; incremental sparse sepctrum Gaussian process regression; intermethod comparison; kernel method; learning algorithm; mathematical computing; mathematical model; nonlinear system; online analysis; priority journal; process development; process optimization; robotics; validation process; visuomotor coordination; Algorithms; Artificial Intelligence; Computer Systems; Humans; Models, Theoretical; Normal Distribution; Regression Analysis; Robotics",Article,Scopus,2-s2.0-84875887187
"Sowa J.-P., Heider D., Bechmann L.P., Gerken G., Hoffmann D., Canbay A.","Novel Algorithm for Non-Invasive Assessment of Fibrosis in NAFLD",2013,"PLoS ONE",20,10.1371/journal.pone.0062439,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876975846&doi=10.1371%2fjournal.pone.0062439&partnerID=40&md5=aa20943bc3199dcad8afdd83cb37f673","Introduction: Various conditions of liver disease and the downsides of liver biopsy call for a non-invasive option to assess liver fibrosis. A non-invasive score would be especially useful to identify patients with slow advancing fibrotic processes, as in Non-Alcoholic Fatty Liver Disease (NAFLD), which should undergo histological examination for fibrosis. Patients/Methods: Classic liver serum parameters, hyaluronic acid (HA) and cell death markers of 126 patients undergoing bariatric surgery for morbid obesity were analyzed by machine learning techniques (logistic regression, k-nearest neighbors, linear support vector machines, rule-based systems, decision trees and random forest (RF)). Specificity, sensitivity and accuracy of the evaluated datasets to predict fibrosis were assessed. Results: None of the single parameters (ALT, AST, M30, M60, HA) did differ significantly between patients with a fibrosis score 1 or 2. However, combining these parameters using RFs reached 79% accuracy in fibrosis prediction with a sensitivity of more than 60% and specificity of 77%. Moreover, RFs identified the cell death markers M30 and M65 as more important for the decision than the classic liver parameters. Conclusion: On the basis of serum parameters the generation of a fibrosis scoring system seems feasible, even when only marginally fibrotic tissue is available. Prospective evaluation of novel markers, i.e. cell death parameters, should be performed to identify an optimal set of fibrosis predictors. © 2013 Sowa et al.",,"alanine aminotransferase; aspartate aminotransferase; cell marker; cell protein; hyaluronic acid; protein M30; protein M60; unclassified drug; adult; article; bariatric surgery; cell death; clinical assessment; clinical evaluation; controlled study; decision tree; diagnostic accuracy; diagnostic test accuracy study; diagnostic value; female; human; k nearest neighbor; learning algorithm; liver fibrosis; logistic regression analysis; major clinical study; male; morbid obesity; nonalcoholic fatty liver; Nonalcoholic Fatty Liver Disease Activity Score; prognosis; random forest; sensitivity and specificity; support vector machine; algorithm; artificial intelligence; blood; complication; fatty liver; liver; liver cirrhosis; middle aged; nonalcoholic fatty liver; pathology; Adult; Algorithms; Artificial Intelligence; Decision Trees; Fatty Liver; Female; Humans; Liver; Liver Cirrhosis; Male; Middle Aged; Non-alcoholic Fatty Liver Disease; Prognosis",Article,Scopus,2-s2.0-84876975846
"Liu Y., Guo J., Hu G., Zhu H.","Gene prediction in metagenomic fragments based on the SVM algorithm",2013,"BMC Bioinformatics",20,10.1186/1471-2105-14-S5-S12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876113075&doi=10.1186%2f1471-2105-14-S5-S12&partnerID=40&md5=b1248b9800a5dc22492f18490301a789","Background: Metagenomic sequencing is becoming a powerful technology for exploring micro-ogranisms from various environments, such as human body, without isolation and cultivation. Accurately identifying genes from metagenomic fragments is one of the most fundamental issues.Results: In this article, we present a novel gene prediction method named MetaGUN for metagenomic fragments based on a machine learning approach of SVM. It implements in a three-stage strategy to predict genes. Firstly, it classifies input fragments into phylogenetic groups by a k-mer based sequence binning method. Then, protein-coding sequences are identified for each group independently with SVM classifiers that integrate entropy density profiles (EDP) of codon usage, translation initiation site (TIS) scores and open reading frame (ORF) length as input patterns. Finally, the TISs are adjusted by employing a modified version of MetaTISA. To identify protein-coding sequences, MetaGun builds the universal module and the novel module. The former is based on a set of representative species, while the latter is designed to find potential functionary DNA sequences with conserved domains.Conclusions: Comparisons on artificial shotgun fragments with multiple current metagenomic gene finders show that MetaGUN predicts better results on both 3' and 5' ends of genes with fragments of various lengths. Especially, it makes the most reliable predictions among these methods. As an application, MetaGUN was used to predict genes for two samples of human gut microbiome. It identifies thousands of additional genes with significant evidences. Further analysis indicates that MetaGUN tends to predict more potential novel genes than other current metagenomic gene finders. © 2013 Liu et al.; licensee BioMed Central Ltd.",,"Entropy density; Gene prediction; Machine learning approaches; Multiple currents; Open reading frame; Phylogenetic groups; SVM classifiers; Translation initiation site; Forecasting; Proteins; Genes; archaeal gene; artificial intelligence; bacterial gene; DNA sequence; gastrointestinal tract; human; metagenomics; microbiology; microflora; open reading frame; phylogeny; procedures; support vector machine; article; metagenomics; methodology; Artificial Intelligence; Gastrointestinal Tract; Genes, Archaeal; Genes, Bacterial; Humans; Metagenomics; Microbiota; Open Reading Frames; Phylogeny; Sequence Analysis, DNA; Support Vector Machines; Artificial Intelligence; Gastrointestinal Tract; Genes, Archaeal; Genes, Bacterial; Humans; Metagenomics; Microbiota; Open Reading Frames; Phylogeny; Sequence Analysis, DNA; Support Vector Machines",Article,Scopus,2-s2.0-84876113075
"Valdez F., Melin P., Castillo O.","Parallel particle swarm optimization with parameters adaptation using fuzzy logic",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",20,10.1007/978-3-642-37798-3_33,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875855464&doi=10.1007%2f978-3-642-37798-3_33&partnerID=40&md5=7d1d39fc2c96856f389eaeb37dc0c3f2","We describe in this paper a Parallel Particle Swarm Optimization (PPSO) method with dynamic parameter adaptation to optimize complex mathematical functions. Fuzzy Logic is used to adapt the parameters of the PSO in the best way possible. The PPSO is shown to be superior to the individual evolutionary methods on the set of benchmark functions. © 2013 Springer-Verlag.","FGA; GA; Parallel FPSO","Benchmark functions; Dynamic parameter adaptation; Evolutionary method; FGA; Mathematical functions; Parallel FPSO; Parallel particle swarm optimization; Artificial intelligence; Fuzzy logic; Gallium; Functions",Conference Paper,Scopus,2-s2.0-84875855464
"Ma A.J., Yuen P.C., Lai J.-H.","Linear dependency modeling for classifier fusion and feature combination",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",20,10.1109/TPAMI.2012.198,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875433320&doi=10.1109%2fTPAMI.2012.198&partnerID=40&md5=b2741f95386974f774a1060cd3b3ffbd","This paper addresses the independent assumption issue in fusion process. In the last decade, dependency modeling techniques were developed under a specific distribution of classifiers or by estimating the joint distribution of the posteriors. This paper proposes a new framework to model the dependency between features without any assumption on feature/classifier distribution, and overcomes the difficulty in estimating the high-dimensional joint density. In this paper, we prove that feature dependency can be modeled by a linear combination of the posterior probabilities under some mild assumptions. Based on the linear combination property, two methods, namely, Linear Classifier Dependency Modeling (LCDM) and Linear Feature Dependency Modeling (LFDM), are derived and developed for dependency modeling in classifier level and feature level, respectively. The optimal models for LCDM and LFDM are learned by maximizing the margin between the genuine and imposter posterior probabilities. Both synthetic data and real datasets are used for experiments. Experimental results show that LCDM and LFDM with dependency modeling outperform existing classifier level and feature level combination methods under nonnormal distributions and on four real databases, respectively. Comparing the classifier level and feature level fusion methods, LFDM gives the best performance. © 1979-2012 IEEE.","classifier level fusion; feature dependency; feature level fusion; Linear dependency modeling; multiple feature fusion","feature dependency; Feature level fusion; Level fusion; Linear dependency; Multiple feature fusion; Artificial intelligence; Computer vision; Classification (of information)",Article,Scopus,2-s2.0-84875433320
"Pajares Ferrando S., Onaindia E.","Context-Aware Multi-Agent Planning in intelligent environments",2013,"Information Sciences",20,10.1016/j.ins.2012.11.021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872303158&doi=10.1016%2fj.ins.2012.11.021&partnerID=40&md5=98000f6141e2fefe6df5101e47b015ef","A system is context-aware if it can extract, interpret and use context information and adapt its functionality to the current context of use. Multi-agent planning generalizes the problem of planning in domains where several agents plan and act together, and share resources, activities, and goals. This contribution presents a practical extension of a formal theoretical model for Context-Aware Multi-Agent Planning based upon an argumentation-based defeasible logic. Our framework, named CAMAP, is implemented on a platform for open multi-agent systems and has been experimentally tested, among others, in applications of ambient intelligence in the field of health-care. CAMAP is based on a multi-agent partial-order planning paradigm in which agents have diverse abilities, use an argumentation-based defeasible contextual reasoning to support their own beliefs and refute the beliefs of the others according to their context knowledge during the plan search process. CAMAP shows to be an adequate approach to tackle ambient intelligence problems as it gathers together in a single framework the ability of planning while it allows agents to put forward arguments that support or argue upon the accuracy, unambiguity and reliability of the context-aware information. © 2012 Elsevier Inc. All rights reserved.","Ambient intelligence; Context-aware reasoning; Defeasible argumentation; Multi-agent planning","Ambient intelligence; Context of use; Context-Aware; Contextual reasoning; Defeasible argumentation; Defeasible logic; Intelligent environment; Multi-agent planning; Open multi-agent system; Partial-order planning; Search process; Theoretical models; Use context; Software engineering; Artificial intelligence",Article,Scopus,2-s2.0-84872303158
"Senaras C., Ozay M., Yarman Vural F.T.","Building detection with decision fusion",2013,"IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing",20,10.1109/JSTARS.2013.2249498,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880303707&doi=10.1109%2fJSTARS.2013.2249498&partnerID=40&md5=df266d9e9faef211c99b3dba235499bf","A novel decision fusion approach to building detection problem in VHR optical satellite images is proposed. The method combines the detection results of multiple classifiers under a hierarchical architecture, called Fuzzy Stacked Generalization (FSG). After an initial segmentation and pre-processing step, a large variety of color, texture and shape features are extracted from each segment. Then, the segments, represented in K different feature spaces are classified by K different base-layer classifiers of the FSG architecture. The class membership values of the segments, which represent the decisions of different base-layer classifiers in a decision space, are aggregated to form a fusion space which is then fed to a meta-layer classifier of the FSG to label the vectors in the fusion space. The paper presents the performance results of the proposed decision fusion model by a comparison with the state of the art machine learning algorithms. The results show that fusing the decisions of multiple classifiers improves the performance, when they are ensembled under the suggested hierarchical learning architecture. © 2013 IEEE.","Building detection; decision fusion; ensemble learning; fuzzy κ-nearest neighbors classification; multi-layer classification; segmentation","Building detection; Decision fusion; Ensemble learning; Hierarchical architectures; Hierarchical learning; Nearest neighbors; Optical satellite images; Stacked generalization; Architecture; Image segmentation; Learning algorithms; Vector spaces; algorithm; artificial intelligence; building; classification; detection method; fuzzy mathematics; nearest neighbor analysis; satellite imagery; segmentation",Article,Scopus,2-s2.0-84880303707
"Shribman A., Hudzia B.","Pre-copy and post-copy VM live migration for memory intensive applications",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",20,10.1007/978-3-642-36949-0_63,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874444308&doi=10.1007%2f978-3-642-36949-0_63&partnerID=40&md5=18703470a469e0b83219fb9b2ce3ffd4","Virtualization technology provides a means for server consolidation, reducing the number of physical servers required for running a given workload. Virtual Machine (VM) live migration facilitates the transfer of a running (VM) between physical hosts while appearing transparent to the running application. Memory intensive applications tend to obstruct the original pre-copy live migration process and may result in the failure of the migration process due to its inability to transfer memory faster than memory is dirtied by the running application. The focus of this paper is to present several techniques that can be applied to both pre-copy live migration and post-copy live migration to better support migration of memory intensive applications. © 2013 Springer-Verlag.","Hypervisors; Linux/KVM; Live Migration; Operating Systems; Post-Copy; Pre-Copy; QEMU; RDMA; Virtual Machines","Hypervisors; Linux/KVM; Live migrations; Post-Copy; Pre-Copy; QEMU; RDMA; Virtual machines; Artificial intelligence; Computer operating systems; Computer simulation",Conference Paper,Scopus,2-s2.0-84874444308
"Khandelwal M., Monjezi M.","Prediction of backbreak in open-pit blasting operations using the machine learning method",2013,"Rock Mechanics and Rock Engineering",20,10.1007/s00603-012-0269-3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879413852&doi=10.1007%2fs00603-012-0269-3&partnerID=40&md5=098ff1c57fbcb846d96b6184e030a490","Backbreak is an undesirable phenomenon in blasting operations. It can cause instability of mine walls, falling down of machinery, improper fragmentation, reduced efficiency of drilling, etc. The existence of various effective parameters and their unknown relationships are the main reasons for inaccuracy of the empirical models. Presently, the application of new approaches such as artificial intelligence is highly recommended. In this paper, an attempt has been made to predict backbreak in blasting operations of Soungun iron mine, Iran, incorporating rock properties and blast design parameters using the support vector machine (SVM) method. To investigate the suitability of this approach, the predictions by SVM have been compared with multivariate regression analysis (MVRA). The coefficient of determination (CoD) and the mean absolute error (MAE) were taken as performance measures. It was found that the CoD between measured and predicted backbreak was 0.987 and 0.89 by SVM and MVRA, respectively, whereas the MAE was 0.29 and 1.07 by SVM and MVRA, respectively. © 2012 Springer-Verlag.","Backbreak; Blasting; MVRA; Soungun iron mine; Support vector machine","Backbreak; Coefficient of determination; Effective parameters; Machine learning methods; Mean absolute error; Multivariate regression analysis; MVRA; Performance measure; Artificial intelligence; Blasting; Iron mines; Machinery; Pile foundations; Regression analysis; Support vector machines; artificial intelligence; blasting; error analysis; machinery; numerical model; open pit mine; operations technology; prediction; rock property",Article,Scopus,2-s2.0-84879413852
"Sadowski L.","Non-destructive investigation of corrosion current density in steel reinforced concrete by artificial neural networks",2013,"Archives of Civil and Mechanical Engineering",20,10.1016/j.acme.2012.10.007,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875794081&doi=10.1016%2fj.acme.2012.10.007&partnerID=40&md5=e535ebc1933c6b2b09b6724d106437a4","Corrosion of the steel reinforcement in concrete is an important problem for the civil engineering. Inspection techniques are needed to assess the corrosion in order to protect and repair concrete structures. Many studies were performed to establish a series of corrosion rate assessment methods. A method of providing a direct evaluation of the corrosion rate by corrosion current density measurement is Linear Polarisation Resistance (LPR). The main drawback is that it requires a localised breakout of the concrete cover. The corrosion of the steel reinforcement is monitored by measuring the resistivity of the concrete. The purpose of this paper is to use the resistivity four-probe method and galvanostatic resistivity measurement together with neural networks to assess the corrosion rate of steel in concrete without a direct connection to the reinforcement. Three parameters determined by two non-destructive resistivity methods together with the air temperature were employed as input variables, and corrosion current density, predicted by the destructive LPR method, acted as the output variable. The results shows that it is possible to predict corrosion current density in steel reinforced concrete by using the model based on artificial neural networks on the basis of parameters determined by two non-destructive resistivity measurement techniques. © 2012 Politechnika WrocŁawska.","Artificial intelligence; Non-destructive testing; Polarisation; Resistivity; Steel reinforced concrete","Corrosion current densities; Inspection technique; Linear polarisation resistances; Non destructive testing; Resistivity measurement; Resistivity methods; Steel reinforced concrete; Steel reinforcements; Artificial intelligence; Civil engineering; Concrete reinforcements; Corrosion rate; Current density; Electric conductivity; Neural networks; Nondestructive examination; Polarization; Repair; Concretes",Article,Scopus,2-s2.0-84875794081
"Brzostowski K., Drapała J., Grzech A., Świa̧tek P.","Adaptive decision support system for automatic physical effort plan generation-data-driven approach",2013,"Cybernetics and Systems",20,10.1080/01969722.2013.762260,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875515815&doi=10.1080%2f01969722.2013.762260&partnerID=40&md5=85d8d55f16f84509205bb8be86a7dde9","Mathematical models delivered using both expert knowledge and experimental data improve understanding of dynamic properties of the system under consideration. This is useful for different purposes, such as prediction, diagnosis, decision making, and system control. A data-driven approach has been found to be particularly useful in designing adaptive decision support systems. We demonstrate the usefulness of data-driven models in a custom application designed for sport training management. We have developed a system that makes use of expert knowledge together with measurement data (heart rate, electromyography, and acceleration) as well as environmental (Global Positioning System) in order to generate an optimal training plan. The system performs such tasks as modeling of the athlete's cardiovascular system, estimation of the athlete's parameters, and adaptation of the model to the athlete. © 2013 Taylor & Francis Group, LLC.","adaptive control; e-health; Kalman filtering; optimization; ubiquitous computing","Adaptive Control; Adaptive decision support system; Data-driven approach; Data-driven model; Ehealth; Experimental datum; Kalman-filtering; Measurement data; Artificial intelligence; Cardiovascular system; Decision support systems; Electromyography; Mathematical models; Optimization; Ubiquitous computing; Digital storage",Article,Scopus,2-s2.0-84875515815
"Pereira C., Gonçalves L., Ferreira M.","Optic disc detection in color fundus images using ant colony optimization",2013,"Medical and Biological Engineering and Computing",20,10.1007/s11517-012-0994-5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885627549&doi=10.1007%2fs11517-012-0994-5&partnerID=40&md5=cf015880b94c4498f0ac0b8439b3214c","Diabetic retinopathy has been revealed as the most common cause of blindness among people of working age in developed countries. However, loss of vision could be prevented by an early detection of the disease and, therefore, by a regular screening program to detect retinopathy. Due to its characteristics, the digital color fundus photographs have been the easiest way to analyze the eye fundus. An important prerequisite for automation is the segmentation of the main anatomical features in the image, particularly the optic disc. Currently, there are many works reported in the literature with the purpose of detecting and segmenting this anatomical structure. Though, none of them performs as needed, especially when dealing with images presenting pathologies and a great variability. Ant colony optimization (ACO) is an optimization algorithm inspired by the foraging behavior of some ant species that has been applied in image processing with different purposes. In this paper, this algorithm preceded by anisotropic diffusion is used for optic disc detection in color fundus images. Experimental results demonstrate the good performance of the proposed approach as the optic disc was detected in most of all the images used, even in the images with great variability. © 2012 International Federation for Medical and Biological Engineering.","Anisotropic diffusion; Ant colony optimization; Diabetic retinopathy; Digital color fundus image; Medical image processing","Anatomical structures; Anisotropic Diffusion; Ant Colony Optimization (ACO); Developed countries; Diabetic retinopathy; Digital color fundus images; Optic disc detections; Optimization algorithms; Algorithms; Ant colony optimization; Artificial intelligence; Color; Diagnosis; Eye protection; Medical image processing; Optical anisotropy; Image segmentation; algorithm; anisotropic diffusion; Ant colony optimization; article; diabetic retinopathy; diffusion; eye fundus; fundus camera; image processing; microaneurysm; optic disk; proliferative retinopathy; retina blood vessel; retina image; anisotropy; biological model; factual database; human; pathology; procedures; visual system examination; image processing; methodology; Algorithms; Anisotropy; Databases, Factual; Diabetic Retinopathy; Diagnostic Techniques, Ophthalmological; Diffusion; Humans; Image Processing, Computer-Assisted; Models, Biological; Optic Disk; Algorithms; Anisotropy; Databases, Factual; Diabetic Retinopathy; Diagnostic Techniques, Ophthalmological; Diffusion; Humans; Image Processing, Computer-Assisted; Models, Biological; Optic Disk",Article,Scopus,2-s2.0-84885627549
"Sdao F., Lioi D.S., Pascale S., Caniani D., Mancini I.M.","Landslide susceptibility assessment by using a neuro-fuzzy model: A case study in the Rupestrian heritage rich area of Matera",2013,"Natural Hazards and Earth System Science",20,10.5194/nhess-13-395-2013,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874025822&doi=10.5194%2fnhess-13-395-2013&partnerID=40&md5=6fc59fa6d2610e473b789a06583edd6c","The complete assessment of landslide susceptibility needs uniformly distributed detailed information on the territory. This information, which is related to the temporal occurrence of landslide phenomena and their causes, is often fragmented and heterogeneous. The present study evaluates the landslide susceptibility map of the Natural Archaeological Park of Matera (Southern Italy) (Sassi and area Rupestrian Churches sites). The assessment of the degree of ""spatial hazard"" or ""susceptibility"" was carried out by the spatial prediction regardless of the return time of the events. The evaluation model for the susceptibility presented in this paper is very focused on the use of innovative techniques of artificial intelligence such as Neural Network, Fuzzy Logic and Neuro-fuzzy Network. The method described in this paper is a novel technique based on a neuro-fuzzy system. It is able to train data like neural network and it is able to shape and control uncertain and complex systems like a fuzzy system. This methodology allows us to derive susceptibility maps of the study area. These data are obtained from thematic maps representing the parameters responsible for the instability of the slopes. The parameters used in the analysis are: plan curvature, elevation (DEM), angle and aspect of the slope, lithology, fracture density, kinematic hazard index of planar and wedge sliding and toppling. Moreover, this method is characterized by the network training which uses a training matrix, consisting of input and output training data, which determine the landslide susceptibility. The neuro-fuzzy method was integrated to a sensitivity analysis in order to overcome the uncertainty linked to the used membership functions. The method was compared to the landslide inventory map and was validated by applying three methods: a ROC (Receiver Operating Characteristic) analysis, a confusion matrix and a SCAI method. The developed neuro-fuzzy method showed a good performance in the determination of the landslide susceptibility map. © 2013 Author(s).",,"artificial intelligence; artificial neural network; digital elevation model; fuzzy mathematics; hazard management; landslide; lithology; numerical model; sensitivity analysis; thematic mapping; Basilicata; Italy; Matera",Article,Scopus,2-s2.0-84874025822
"Khooban M.H., Soltanpour M.R.","Swarm optimization tuned fuzzy sliding mode control design for a class of nonlinear systems in presence of uncertainties",2013,"Journal of Intelligent and Fuzzy Systems",20,10.3233/IFS-2012-0569,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873608218&doi=10.3233%2fIFS-2012-0569&partnerID=40&md5=41768fb10b3024ba61fd7b1ebcdb6a3d","This paper provides an optimal controlling approach for a class of nonlinear systems with structured and unstructured uncertainties using fuzzy sliding mode control. First known dynamics of the system are eliminated through feedback linearization and then optimal fuzzy sliding mode controller is designed using an intelligent fuzzy controller based on Sugeno-Type structure. The proposed controller is optimized by a novel heuristic algorithm namely Particle Swarm Optimization with random inertia Weight (RNW-PSO). In order to handle, the uncertainties Lyapunov method is used. There are no signs of the undesired chattering phenomenon in the proposed method. The globally asymptotic stability of the closed-loop system is mathematically proved. Finally, this control method is applied to the inverted pendulum system as a case study. Simulation results show desirability of the system performance. © 2013-IOS Press and the authors. All rights reserved.","fuzzy; Nonlinear system; optimal; sliding mode control; uncertainties","Chattering phenomenon; Control methods; fuzzy; Fuzzy controllers; Fuzzy sliding mode control; Fuzzy sliding mode controller; Globally asymptotic stability; Inertia weight; Inverted pendulum system; optimal; Swarm optimization; uncertainties; Unstructured uncertainty; Artificial intelligence; Asymptotic stability; Feedback linearization; Fuzzy systems; Heuristic algorithms; Image processing; Lyapunov methods; Nonlinear systems; Particle swarm optimization (PSO); Pendulums; Sliding mode control",Article,Scopus,2-s2.0-84873608218
"Hawley M.S., Cunningham S.P., Green P.D., Enderby P., Palmer R., Sehgal S., O'Neill P.","A voice-input voice-output communication aid for people with severe speech impairment",2013,"IEEE Transactions on Neural Systems and Rehabilitation Engineering",20,10.1109/TNSRE.2012.2209678,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872129131&doi=10.1109%2fTNSRE.2012.2209678&partnerID=40&md5=6874096c303e6e1a0f5571085f38c55f","A new form of augmentative and alternative communication (AAC) device for people with severe speech impairment - the voice-input voice-output communication aid (VIVOCA) - is described. The VIVOCA recognizes the disordered speech of the user and builds messages, which are converted into synthetic speech. System development was carried out employing user-centered design and development methods, which identified and refined key requirements for the device. A novel methodology for building small vocabulary, speaker-dependent automatic speech recognizers with reduced amounts of training data, was applied. Experiments showed that this method is successful in generating good recognition performance (mean accuracy 96%) on highly disordered speech, even when recognition perplexity is increased. The selected message-building technique traded off various factors including speed of message construction and range of available message outputs. The VIVOCA was evaluated in a field trial by individuals with moderate to severe dysarthria and confirmed that they can make use of the device to produce intelligible speech output from disordered speech input. The trial highlighted some issues which limit the performance and usability of the device when applied in real usage situations, with mean recognition accuracy of 67% in these circumstances. These limitations will be addressed in future work. © 2001-2011 IEEE.","Augmentative and alternative communication; automatic speech recognition; dysarthria; voice output communication aid","Augmentative and alternative communication devices; Augmentative-and-alternative communication; Automatic speech recognition; Automatic speech recognizers; Communication aids; Dysarthria; Field trial; New forms; Novel methodology; Recognition accuracy; Recognition performance; Speech input; Speech output; Synthetic speech; System development; Training data; User centered designs; Human rehabilitation engineering; Speech synthesis; Speech recognition; adult; aged; article; artificial intelligence; automatic speech recognition; clinical trial; communication aid; equipment; equipment design; equipment failure; female; human; linguistics; male; middle aged; sound detection; speech analysis; speech disorder; treatment outcome; very elderly; Adult; Aged; Aged, 80 and over; Artificial Intelligence; Communication Aids for Disabled; Equipment Design; Equipment Failure Analysis; Female; Humans; Male; Middle Aged; Sound Spectrography; Speech Disorders; Speech Production Measurement; Speech Recognition Software; Treatment Outcome; Vocabulary, Controlled",Article,Scopus,2-s2.0-84872129131
"Chakraborty B., Laber E.B., Zhao Y.","Inference for Optimal Dynamic Treatment Regimes Using an Adaptive m-Out-of-n Bootstrap Scheme",2013,"Biometrics",20,10.1111/biom.12052,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901199092&doi=10.1111%2fbiom.12052&partnerID=40&md5=16a1509125afa41af92e1daa80812633","Summary: A dynamic treatment regime consists of a set of decision rules that dictate how to individualize treatment to patients based on available treatment and covariate history. A common method for estimating an optimal dynamic treatment regime from data is Q-learning which involves nonsmooth operations of the data. This nonsmoothness causes standard asymptotic approaches for inference like the bootstrap or Taylor series arguments to breakdown if applied without correction. Here, we consider the m-out-of-n bootstrap for constructing confidence intervals for the parameters indexing the optimal dynamic regime. We propose an adaptive choice of m and show that it produces asymptotically correct confidence sets under fixed alternatives. Furthermore, the proposed method has the advantage of being conceptually and computationally much simple than competing methods possessing this same theoretical property. We provide an extensive simulation study to compare the proposed method with currently available inference procedures. The results suggest that the proposed method delivers nominal coverage while being less conservative than alternatives. The proposed methods are implemented in the qLearn R-package and have been made available on the Comprehensive R-Archive Network (http://cran.r-project.org/). Analysis of the Sequenced Treatment Alternatives to Relieve Depression (STAR D) study is used as an illustrative example. © 2013, The International Biometric Society.","Dynamic Treatment Regime; m-Out-of-n Bootstrap; Nonregularity; Q-learning","biometry; bootstrapping; confidence interval; data set; disease treatment; numerical method; simulation; antidepressant agent; article; artificial intelligence; biometry; computer simulation; confidence interval; decision theory; Dynamic Treatment Regime; human; m-Out-of-n Bootstrap; major depression; methodology; Monte Carlo method; Nonregularity; Q-learning; randomized controlled trial (topic); statistical model; statistics; Dynamic Treatment Regime; m-Out-of-n Bootstrap; Nonregularity; Q-learning; Antidepressive Agents; Artificial Intelligence; Biometry; Computer Simulation; Confidence Intervals; Decision Theory; Depressive Disorder, Major; Humans; Linear Models; Logistic Models; Models, Statistical; Monte Carlo Method; Randomized Controlled Trials as Topic",Article,Scopus,2-s2.0-84901199092
"Yang N., Liu S., Li M., Zhou M., Yu N.","Word alignment modeling with context dependent Deep Neural Network",2013,"ACL 2013 - 51st Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference",20,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905247134&partnerID=40&md5=0554ec237674227de78b9903f4537fa3","In this paper, we explore a novel bilingual word alignment approach based on DNN (Deep Neural Network), which has been proven to be very effective in various machine learning tasks (Collobert et al., 2011). We describe in detail how we adapt and extend the CD-DNN-HMM (Dahl et al., 2012) method introduced in speech recognition to the HMM-based word alignment model, in which bilingual word embedding is discriminatively learnt to capture lexical translation information, and surrounding words are leveraged to model context information in bilingual sentences. While being capable to model the rich bilingual correspondence, our method generates a very compact model with much fewer parameters. Experiments on a large scale English-Chinese word alignment task show that the proposed method outperforms the HMM and IBM model 4 baselines by 2 points in F-score. © 2013 Association for Computational Linguistics.",,"Artificial intelligence; Computational linguistics; Speech recognition; Compact model; Context dependent; Deep neural networks; F-score; HMM-based; IBM Models; Model contexts; Word alignment; Alignment",Conference Paper,Scopus,2-s2.0-84905247134
"Miranda V., Alves R.","Differential Evolutionary Particle Swarm Optimization (DEEPSO): A successful hybrid",2013,"Proceedings - 1st BRICS Countries Congress on Computational Intelligence, BRICS-CCI 2013",20,10.1109/BRICS-CCI-CBIC.2013.68,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905388945&doi=10.1109%2fBRICS-CCI-CBIC.2013.68&partnerID=40&md5=4db18c59b771f409f4a12329223b3641","This paper explores, with numerical case studies, the performance of an optimization algorithm that is a variant of EPSO, the Evolutionary Particle Swarm Optimization method. EPSO is already a hybrid approach that may be seen as a PSO with self-Adaptive weights or an Evolutionary Programming approach with a self-Adaptive recombination operator. The new hybrid DEEPSO retains the self-Adaptive properties of EPSO but borrows the concept of rough gradient from Differential Evolution algorithms. The performance of DEEPSO is compared to a well-performing EPSO algorithm in the optimization of problems of the fixed cost type, showing consistently better results in the cases presented. © 2013 IEEE.","Differential Evolution; Evolutionary Particle Swarm Optimization; Fuzzy clustering; PAR location; Unit commitment","Algorithms; Artificial intelligence; Computer programming; Fuzzy clustering; Differential Evolution; Differential evolution algorithms; Differential evolutionary; Evolutionary particle swarm optimizations; Evolutionary programming approach; Optimization algorithms; Recombination operators; Unit commitment; Particle swarm optimization (PSO)",Conference Paper,Scopus,2-s2.0-84905388945
"Wu I.-C., Lin H.-H., Sun D.-J., Kao K.-Y., Lin P.-H., Chan Y.-C., Chen P.-T.","Job-level proof number search",2013,"IEEE Transactions on Computational Intelligence and AI in Games",20,10.1109/TCIAIG.2012.2224659,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897126099&doi=10.1109%2fTCIAIG.2012.2224659&partnerID=40&md5=50abd6f5aca451a720aea8b28290f574","This paper introduces an approach, called generic job-level search, to leverage the game-playing programs which are already written and encapsulated as jobs. Such an approach is well suited to a distributed computing environment, since these jobs are allowed to be run by remote processors independently. In this paper, we present and focus on a job-level proof number search (JL-PNS), a kind of generic job-level search for solving computer game search problems, and apply JL-PNS to solving automatically several Connect6 positions, including some difficult openings. This paper also proposes a method of postponed sibling generation to generate nodes smoothly, and some policies, such as virtual win, virtual loss, virtual equivalence, flagging, or hybrids of the above, to expand the nodes. Our experiment compared these policies, and the results showed that the virtual-equivalence policy, together with flagging, performed the best against other policies. In addition, the results also showed that the speedups for solving these positions are 8.58 on average on 16 cores. © 2013 IEEE.","Connect6; Desktop grids; Job-level proof number search (JL-PNS); Proof number search; Threat-space search","Artificial intelligence; Connect6; Desktop grid; Job-level proof number search (JL-PNS); Proof number search; Threat-space search; Software engineering",Article,Scopus,2-s2.0-84897126099
"Gastaldo P., Zunino R., Redi J.","Supporting visual quality assessment with machine learning",2013,"Eurasip Journal on Advances in Signal Processing",20,10.1186/1687-5281-2013-54,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911890032&doi=10.1186%2f1687-5281-2013-54&partnerID=40&md5=2215849d18ed7cb159d7d96b7e2fa158","Objective metrics for visual quality assessment often base their reliability on the explicit modeling of the highly non-linear behavior of human perception; as a result, they may be complex and computationally expensive. Conversely, machine learning (ML) paradigms allow to tackle the quality assessment task from a different perspective, as the eventual goal is to mimic quality perception instead of designing an explicit model the human visual system. Several studies already proved the ability of ML-based approaches to address visual quality assessment; nevertheless, these paradigms are highly prone to overfitting, and their overall reliability may be questionable. In fact, a prerequisite for successfully using ML in modeling perceptual mechanisms is a profound understanding of the advantages and limitations that characterize learning machines. This paper illustrates and exemplifies the good practices to be followed. © 2013 Gastaldo et al.",,"Artificial intelligence; Behavioral research; Explicit modeling; Human Visual System; Nonlinear behavior; Objective metrics; Perceptual mechanism; Quality assessment; Quality perceptions; Visual quality assessment; Learning systems",Article,Scopus,2-s2.0-84911890032
"Kougias I.P., Theodossiou N.P.","Multiobjective Pump Scheduling Optimization Using Harmony Search Algorithm (HSA) and Polyphonic HSA",2013,"Water Resources Management",20,10.1007/s11269-012-0236-5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874248549&doi=10.1007%2fs11269-012-0236-5&partnerID=40&md5=3245ef0a7e378edb8238e7ee620258f6","Harmony Search Algorithm (HSA) is a metaheuristic method that has attracted the scientific interest since its first presentation in 2001. It is a music inspired method, imitating the music creation process in order to find optimal solutions in complicated problems. HSA's successful application on single - objective optimization problems has resulted to an increasing interest in the implementation of HSA towards multiobjective optimization. The authors have adjusted HSA in order to deal successfully with multi-criteria water management problems. This adjustment has resulted to the creation of Multiobjective - HSA (MO-HSA). In addition, they have designed the multiobjective variant Polyphonic-HSA (Poly-HSA), which is inspired by the independent development of different voices in music and borrows elements from Swarm Intelligence and the single-objective variant Global-Best HSA. In the first part of this paper, both methods are presented in detail. Moreover, the performance of the proposed Algorithms is evaluated using standard multiobjective test - functions. ZDT and DTLZ multiobjective tests have been chosen and indicators such as Hypervolume, C - metric and diversity metric - Δ have been used to measure the convergence to the optimal front and the diversity of the solutions obtained by the proposed methods. In the second part, MO-HSA and Poly-HSA have been introduced towards the optimization of a pump scheduling problem. The objectives considered are water supply, pumping cost, electric power peak demand and pump maintenance cost. Both methods converged to non-dominated fronts and provided excellent results which are presented in 3d figures, indicating that these methods can be effectively used in multiobjective water management problems. © 2012 Springer Science+Business Media Dordrecht.","Harmony Search Algorithm; Multiobjective optimization; Pareto dominance; Pump scheduling; Water management","Diversity metrics; Electric power; Harmony search algorithms; Hypervolume; Maintenance cost; Management problems; Meta-heuristic methods; Multi objective; Multi-criteria; Multi-objective tests; Music creation; Optimal solutions; Pareto dominance; Peak demand; Pump scheduling; Pumping cost; Single- objective optimizations; Swarm Intelligence; Artificial intelligence; Learning algorithms; Pumps; Scheduling; Water management; Water supply; Multiobjective optimization; algorithm; implementation process; multiobjective programming; optimization; performance assessment; testing method; water management; water supply",Article,Scopus,2-s2.0-84874248549
"López-de-Ipiña K., Alonso J.B., Solé-Casals J., Barroso N., Henriquez P., Faundez-Zanuy M., Travieso C.M., Ecay-Torres M., Martínez-Lage P., Eguiraun H.","On Automatic Diagnosis of Alzheimer’s Disease Based on Spontaneous Speech Analysis and Emotional Temperature",2013,"Cognitive Computation",20,10.1007/s12559-013-9229-9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922835056&doi=10.1007%2fs12559-013-9229-9&partnerID=40&md5=7301404de10a280928a77d4a69dabe47","Alzheimer’s disease (AD) is the most prevalent form of progressive degenerative dementia; it has a high socioeconomic impact in Western countries. Therefore, it is one of the most active research areas today. Alzheimer’s disease is sometimes diagnosed by excluding other dementias, and definitive confirmation is only obtained through a postmortem study of the brain tissue of the patient. The work presented here is part of a larger study that aims to identify novel technologies and biomarkers for early AD detection, and it focuses on evaluating the suitability of a new approach for early diagnosis of AD by noninvasive methods. The purpose is to examine, in a pilot study, the potential of applying machine learning algorithms to speech features obtained from suspected Alzheimer’s disease sufferers in order to help diagnose this disease and determine its degree of severity. Two human capabilities relevant in communication have been analyzed for feature selection: spontaneous speech and emotional response. The experimental results obtained were very satisfactory and promising for the early diagnosis and classification of AD patients. © 2013, Springer Science+Business Media New York.","Alzheimer’s disease diagnosis; Emotion recognition; Spontaneous speech","Artificial intelligence; Computer aided diagnosis; Diagnosis; Learning algorithms; Learning systems; Noninvasive medical procedures; Speech communication; Automatic diagnosis; Disease diagnosis; Emotion recognition; Emotional response; Noninvasive methods; Socio-economic impacts; Spontaneous speech; Western countries; Speech recognition",Article,Scopus,2-s2.0-84922835056
"Ahmad F., Lee S., Thottethodi M., Vijaykumar T.N.","MapReduce with communication overlap (MaRCO)",2013,"Journal of Parallel and Distributed Computing",20,10.1016/j.jpdc.2012.12.012,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893670179&doi=10.1016%2fj.jpdc.2012.12.012&partnerID=40&md5=2ce3a5dd1164d7fa5e6aca6518d2d08e","MapReduce is a programming model from Google for cluster-based computing in domains such as search engines, machine learning, and data mining. MapReduce provides automatic data management and fault tolerance to improve programmability of clusters. MapReduce's execution model includes an all-mapto- all-reduce communication, called the shuffle, across the network bisection. Some MapReductions move large amounts of data (e.g., as much as the input data), stressing the bisection bandwidth and introducing significant runtime overhead. Optimizing such shuffle-heavy MapReductions is important because (1) they include key applications (e.g., inverted indexing for search engines and data clustering for machine learning) and (2) they run longer than shuffle-light MapReductions (e.g., 5x longer). In MapReduce, the asynchronous nature of the shuffle results in some overlap between the shuffle and map. Unfortunately, this overlap is insufficient in shuffle-heavy MapReductions. We propose MapReduce with communication overlap (MaRCO) to achieve nearly full overlap via the novel idea of including reduce in the overlap. While MapReduce lazily performs reduce computation only after receiving all the map data, MaRCO employs eager reduce to process partial data from some map tasks while overlapping with other map tasks' communication. MaRCO's approach of hiding the latency of the inevitably high shuffle volume of shuffle-heavy MapReductions is fundamental for achieving performance. We implement MaRCO in Hadoop's MapReduce and show that on a 128-node Amazon EC2 cluster, MaRCO achieves 23% average speed-up over Hadoop for shuffle-heavy MapReductions. © 2012 Elsevier Inc. All rights reserved.","Cloud computing; Distributed processing; Large-scale data processing; MapReduce; Parallel computing; Performance optimization","Artificial intelligence; Cloud computing; Cluster computing; Clustering algorithms; Data handling; Data mining; Distributed computer systems; Fault tolerance; Information management; Learning systems; Multiprocessing systems; Parallel processing systems; Web crawler; Bisection bandwidth; Communication overlap; Distributed processing; Large amounts of data; Large-scale data processing; Map-reduce; Performance optimizations; Programming models; Search engines",Article,Scopus,2-s2.0-84893670179
"Skowron P., Faliszewski P., Slinko A.","Fully proportional representation as resource allocation: Approximability results",2013,"IJCAI International Joint Conference on Artificial Intelligence",19,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896060687&partnerID=40&md5=18eb3ef8899d34a9908d006d5372a8f2","We study the complexity of (approximate) winner determination under Monroe's and Chamberlin- Courant's multiwinner voting rules, where we focus on the total (dis)satisfaction of the voters (the utilitarian case) or the (dis)satisfaction of the worstoff voter (the egalitarian case). We show good approximation algorithms for the satisfaction-based utilitarian cases, and inapproximability results for the remaining settings.",,"Approximability; Inapproximability; Voting rules; Winner determination; Approximation algorithms; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84896060687
"Zhang K., Zhang W., Zheng Y., Xue X.","Sparse reconstruction for weakly supervised semantic segmentation",2013,"IJCAI International Joint Conference on Artificial Intelligence",19,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063076&partnerID=40&md5=ef126541ffa1e1e4292fb7b77c7bc74b","We propose a novel approach to semantic segmentation using weakly supervised labels. In traditional fully supervised methods, superpixel labels are available for training; however, it is not easy to obtain enough labeled superpixels to learn a satisfying model for semantic segmentation. By contrast, only image-level labels are necessary in weakly supervised methods, which makes them more practical in real applications. In this paper we develop a new way of evaluating classification models for semantic segmentation given weekly supervised labels. For a certain category, provided the classification model parameter, we firstly learn the basis superpixels by sparse reconstruction, and then evaluate the parameters by measuring the reconstruction errors among negative and positive superpixels. Based on Gaussian Mixture Models, we use Iterative Merging Update (IMU) algorithm to obtain the best parameters for the classification models. Experimental results on two real-world datasets show that the proposed approach outperforms the existing weakly supervised methods, and it also competes with state-of-the-art fully supervised methods.",,"Classification models; Gaussian Mixture Model; Real applications; Real-world datasets; Reconstruction error; Semantic segmentation; Sparse reconstruction; Supervised methods; Artificial intelligence; Mathematical models; Iterative methods",Conference Paper,Scopus,2-s2.0-84896063076
"Cekmez U., Ozsiginan M., Sahingoz O.K.","Adapting the GA approach to solve Traveling Salesman Problems on CUDA architecture",2013,"CINTI 2013 - 14th IEEE International Symposium on Computational Intelligence and Informatics, Proceedings",19,10.1109/CINTI.2013.6705234,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893753273&doi=10.1109%2fCINTI.2013.6705234&partnerID=40&md5=3866566cf04614f319044c35388d5919","The vehicle routing problem (VRP) is one of the most challenging combinatorial optimization problems, which has been studied for several decades. The number of solutions for VRP increases exponentially while the number of points, which must be visited increases. There are 3.0×1064 different solutions for 50 visiting points in a direct solution, and it is practically impossible to try out all these permutations. Some approaches like evolutionary algorithms allow finding feasible solutions in an acceptable time. However, if the number of visiting points increases, these algorithms require high performance computing, and they remain insufficient for finding a feasible solution quickly. Graphics Processing Units (GPUs) have tremendous computational power by allowing parallel processing over lots of computing grids, and they can lead to significant performance gains compared with typical CPU implementations. In this paper, it is aimed to present efficient implementation of Genetic Algorithm, which is an evolutionary algorithm that is inspired by processes observed in the biological evolution of living organisms to find approximate solutions for optimization problems such as Traveling Salesman Problem, on GPU. A 1-Thread in 1-Position (1T1P) approach is developed to improve the performance through maximizing efficiency, which then yielded a significant acceleration by using GPUs. Performance of implemented system is measured with the different parameters and the corresponding CPU implementation. © 2013 IEEE.","1T1P; CUDA; GPU; High Performance; parallel GA; TSP","1T1P; CUDA; GPU; High Performance; Parallel GA; TSP; Artificial intelligence; Biology; Computer graphics; Genetic algorithms; Information science; Program processors; Traveling salesman problem",Conference Paper,Scopus,2-s2.0-84893753273
"Santos D.O., Xavier E.C.","Dynamic taxi and ridesharing: A framework and heuristics for the optimization problem",2013,"IJCAI International Joint Conference on Artificial Intelligence",19,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062153&partnerID=40&md5=399b023e05e85904f5987d1e4a2c8b2f","In this paper we study a dynamic problem of ridesharing and taxi sharing with time windows. We consider a scenario where people needing a taxi or interested in getting a ride use a phone app to designate their source and destination points in a city, as well others restrictions (such as maximum allowable time to be at the destination). On the other hand, we have taxis and people interested in giving a ride, with their current positions and also some constraints (vehicle capacity, destination, maximum time to destination). We want to maximize the number of shared trips: in the case of taxis, people going to close locations can share the costs of the trip, and in case of rides, the driver and passengers can share costs as well. This problem is dynamic since new calls for taxis or calls for rides arrive on demand. This gives rise to an optimization problem which we prove to be NP-Hard. We then propose heuristics to deal with it. We focus on the taxi sharing problem, but we show that our model is easily extendable to model the ridesharing situation or even a situation where there are both taxis and car owners. In addition, we present a framework that consists basically of a client application and a server. The last one processes all incoming information in order to match vehicles to passengers requests. The entire system can be used by taxi companies and riders in a way to reduce the traffic in the cities and to reduce the emission of greenhouse gases.",,"Client applications; Destination points; Dynamic problem; Entire system; Optimization problems; Ride-sharing; Time windows; Vehicle capacity; Artificial intelligence; Greenhouse gases; Optimization; Taxicabs",Conference Paper,Scopus,2-s2.0-84896062153
"Kotov V., Massacci F.","Anatomy of exploit kits: Preliminary analysis of exploit kits as software artefacts",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",19,10.1007/978-3-642-36563-8_13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893098033&doi=10.1007%2f978-3-642-36563-8_13&partnerID=40&md5=02e744fed0f22d91cc21ada545b4a816","In this paper we report a preliminary analysis of the source code of over 30 different exploit kits which are the main tool behind drive-by- download attacks. The analysis shows that exploit kits make use of a very limited number of vulnerabilities and in a rather unsophisticated fashion. Their key strength is rather their ability to support ""customers"" in avoiding detection, monitoring traffic, and managing exploits. © Springer-Verlag Berlin Heidelberg 2013.","Exploit kits; Malware analysis; Web threats","Exploit kits; Malware analysis; Preliminary analysis; Software artefacts; Source codes; Web threats; Computer science; Computers; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84893098033
"Bonet B.","An admissible heuristic for SAS+ planning obtained from the state equation",2013,"IJCAI International Joint Conference on Artificial Intelligence",19,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063322&partnerID=40&md5=2c3983bde368bd1417ca280bdd9c4000","Domain-independent optimal planning has seen important breakthroughs in recent years with the development of tractable and informative admissible heuristics, suitable for planners based on forward state-space search. These heuristics allow planners to optimally solve an important number of benchmark problems, including problems that are quite involved and difficult for the layman. In this paper we present a new admissible heuristic that is obtained from the state equation associated to the Petri-net representation of the planning problem. The new heuristic, that does not fall into one of the four stand ard classes, can be computed in polynomial time and is competitive with the current state of the art for optimal planning, as empirically demonstrated over a large number of problems, mainly because it often shows an improved quality-to-cost ratio. The new heuristic applies to SAS+ planning tasks with arbitrary non-negative action costs.",,"Bench-mark problems; Non negatives; Optimal planning; Planning problem; Planning tasks; Polynomial-time; State equations; State of the art; Artificial intelligence; Heuristic methods; Planning; Polynomial approximation; Equations of state",Conference Paper,Scopus,2-s2.0-84896063322
"Radanovic G., Faltings B.","A robust Bayesian truth serum for non-binary signals",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",19,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893347052&partnerID=40&md5=5bb64e937ecc501cb885def8599f1a35","Several mechanisms have been proposed for incentivizing truthful reports of a private signals owned by rational agents, among them the peer prediction method and the Bayesian truth serum. The robust Bayesian truth serum (RBTS) for small populations and binary signals is particularly interesting since it does not require a common prior to be known to the mechanism. We further analyze the problem of the common prior not known to the mechanism and give several results regarding the restrictions that need to be placed in order to have an incentive-compatible mechanism. Moreover, we construct a Bayes-Nash incentive-compatible scheme called multi-valued RBTS that generalizes RBTS to operate on both small populations and non-binary signals. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Bayesian; Binary signals; Multi-valued; Non-binary; Prediction methods; Rational agents; Small population; Artificial intelligence; Body fluids",Conference Paper,Scopus,2-s2.0-84893347052
"Yin Y., Liu M., Cheng T.C.E., Wu C.-C., Cheng S.-R.","Four single-machine scheduling problems involving due date determination decisions",2013,"Information Sciences",19,10.1016/j.ins.2013.06.035,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883232278&doi=10.1016%2fj.ins.2013.06.035&partnerID=40&md5=3a277cad1a89f85f3a0ce3d831515014","This paper considers single-machine scheduling problems with simultaneous consideration of due date assignment, past-sequence-dependent (p-s-d) delivery times, and position-dependent learning effects. By p-s-d delivery times, we mean that the delivery time of a job is proportional to the job's waiting time. Specifically, we study four variants of the problem: (i) the variant of total earliness and tardiness with common due date assignment (referred to as TETDC), (ii) the variant of total earliness and weighted number of tardy jobs with CON due date assignment (referred to as TEWNTDC), (iii) the variant of total earliness and weighted number of tardy jobs with slack due date assignment (referred to as TEWNTDS), and (iv) the variant of weighted number of tardy jobs with different due date assignment (referred to as WNTDD). We derive the structural properties of the optimal schedules and show that the variants TETDC, TEWNTDC and TEWNTDS are all polynomially solvable. Although the complexity status of the variant WNTDD is still open, we show that two special cases of it are polynomially solvable. © 2013 Elsevier Inc. All rights reserved.","Due date assignment; Past-sequence-dependent delivery time; Position-dependent learning effect; Scheduling","Common due date assignment; Delivery time; Due-date assignment; Earliness and tardiness; Learning effects; Polynomially solvable; Single-machine scheduling; Weighted number of tardy jobs; Artificial intelligence; Scheduling; Software engineering; Machinery",Article,Scopus,2-s2.0-84883232278
"Wassermann D., Makris N., Rathi Y., Shenton M., Kikinis R., Kubicki M., Westin C.F.","On describing human white matter anatomy: the white matter query language.",2013,"Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention",19,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894627429&partnerID=40&md5=83867a9ef4ade78569fb5937aa0b1971","The main contribution of this work is the careful syntactical definition of major white matter tracts in the human brain based on a neuroanatomist's expert knowledge. We present a technique to formally describe white matter tracts and to automatically extract them from diffusion MRI data. The framework is based on a novel query language with a near-to-English textual syntax. This query language allows us to construct a dictionary of anatomical definitions describing white matter tracts. The definitions include adjacent gray and white matter regions, and rules for spatial relations. This enables automated coherent labeling of white matter anatomy across subjects. We use our method to encode anatomical knowledge in human white matter describing 10 association and 8 projection tracts per hemisphere and 7 commissural tracts. The technique is shown to be comparable in accuracy to manual labeling. We present results applying this framework to create a white matter atlas from 77 healthy subjects, and we use this atlas in a proof-of-concept study to detect tract changes specific to schizophrenia.",,"article; artificial intelligence; audiovisual equipment; automated pattern recognition; biological model; brain; computer assisted diagnosis; computer interface; cytology; diffusion tensor imaging; human; image enhancement; information retrieval; methodology; myelinated nerve; natural language processing; reproducibility; sensitivity and specificity; ultrastructure; Artificial Intelligence; Brain; Diffusion Tensor Imaging; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Information Storage and Retrieval; Models, Anatomic; Models, Neurological; Natural Language Processing; Nerve Fibers, Myelinated; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; User-Computer Interface",Article,Scopus,2-s2.0-84894627429
"Kuznetsov S.O.","Scalable knowledge discovery in complex data with pattern structures",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",19,10.1007/978-3-642-45062-4_3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893369381&doi=10.1007%2f978-3-642-45062-4_3&partnerID=40&md5=7245b821973c676ddc92c2362502e0ea","Pattern structures propose a direct way to knowledge discovery in data with structure, such as logical formulas, graphs, strings, tuples of numerical intervals, etc., by defining closed descriptions and discovery tools build upon them: automatic construction of taxonomies, association rules and classifiers. A combination of lazy evaluation with projections of initial data, randomization and parallelization suggest efficient approach which is scalable to big data. © Springer-Verlag 2013.",,"Automatic construction; Big datum; Complex data; Knowledge discovery in data; Lazy evaluation; Logical formulas; Parallelizations; Pattern structure; Artificial intelligence; Pattern recognition; Textile printing",Conference Paper,Scopus,2-s2.0-84893369381
"Kisi O., Akbari N., Sanatipour M., Hashemi A., Teimourzadeh K., Shiri J.","Modeling of dissolved oxygen in river water using artificial intelligence techniques",2013,"Journal of Environmental Informatics",19,10.3808/jei.201300248,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891110067&doi=10.3808%2fjei.201300248&partnerID=40&md5=22ed696e712abdb40a08c4df52f72aa0","The accuracy of artificial neural networks (ANNs), adaptive neuro-fuzzy inference system (ANFIS) and gene expression programming (GEP) in modeling dissolved oxygen (DO) concentration was investigated in this study. Water temperature, specific conductance, pH, discharge and DO concentration data from South Platte River at Englewood, Colorado were used. Various input combinations of these data were tried as inputs to the ANN and ANFIS methods. The ANN and ANFIS models with the water temperature, specific conductance, pH and discharge input parameters performed the best. The optimal GEP model was obtained for the best input combination and compared with the ANN and ANFIS models with respect to correlation coefficient, root mean square error, mean absolute error and mean absolute relative error criteria. Results revealed that the GEP model performed better than the ANN and ANFIS models in modeling DO concentration. © 2013 ISEIS All rights reserved.","Dissolved oxygen; Gene expression programming; Modeling; Neural networks; Neuro-fuzzy","artificial intelligence; artificial neural network; discharge; dissolved oxygen; gene expression; modeling; pH; river water; water temperature; Colorado; South Platte River; United States",Article,Scopus,2-s2.0-84891110067
"Portoghese I., D'Agostino D., Giordano R., Scardigno A., Apollonio C., Vurro M.","An integrated modelling tool to evaluate the acceptability of irrigation constraint measures for groundwater protection",2013,"Environmental Modelling and Software",19,10.1016/j.envsoft.2013.03.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892646212&doi=10.1016%2fj.envsoft.2013.03.001&partnerID=40&md5=3aa48322a31f5fcb195b6ee937db6a8e","In many arid and semi-arid regions agriculture is the main user of GW, causing problems with the quantity and quality of water, but there are few institutional policies and regulations governing sustainable GW exploitation. The authors suggest an integrated methodology for enabling local GW management, capable of combining the need for GW protection with socio-economic and behavioural determinants of GW use. In the proposed tool, integration is reinforced by the inclusion of multiple stakeholders, and the use of Bayesian Belief Networks (BBN) to simulate and explore these stakeholders' attitude to GW exploitation and their responses to the introduction of new protection policies. BBNs and hydrological system properties are integrated in a GIS-based decision support system - GeSAP - which can elaborate and analyse scenarios concerning the pressure on GW due to exploitation for irrigation, and the effectiveness of protection policies, taking into account the level of consensus. In addition, the GIS interface makes it possible to spatialize the information and to investigate model results.The paper presents the results of an experimental application of the GeSAP tool to support GW planning and management in the Apulia Region (Southern Italy). To evaluate the actual usability of the GeSAP tool, case study applications were performed involving the main experts in GW protection and the regional decision-makers. Results showed that GeSAP can simulate farmers' behaviour concerning the selection of water sources for irrigation, allowing evaluation of the effectiveness of a wide range of strategies which impact water demand and consumption. •An integrated GIS-based tool was developed to support groundwater protection.•Different sources of knowledge were integrated, from models to lay knowledge.•BBN were implemented to model farmers' behaviour in irrigation management.•The groundwater pressure was assessed considering the impacts of farmers behaviour.•The policy effectiveness was evaluated considering the level of conflict. © 2013 Elsevier Ltd.","Bayesian Belief Networks; Conflict mitigation; Groundwater protection policy; Stakeholder involvement","Arid and semi-arid regions; Conflict mitigation; Experimental application; Gis-based decision support systems; Ground water protection; Institutional policies; Integrated methodology; Stakeholder involvement; Artificial intelligence; Bayesian networks; Decision support systems; Geographic information systems; Groundwater; Irrigation; Water quality; Tools; agricultural worker; decision support system; GIS; hydrological modeling; integrated approach; irrigation; numerical model; policy making; stakeholder; water management; water quality; Italy; Puglia",Article,Scopus,2-s2.0-84892646212
"Cano A., Zafra A., Ventura S.","Weighted data gravitation classification for standard and imbalanced data",2013,"IEEE Transactions on Cybernetics",19,10.1109/TSMCB.2012.2227470,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890059513&doi=10.1109%2fTSMCB.2012.2227470&partnerID=40&md5=994b085106db6166e30e7e468a8a1dbb","Gravitation is a fundamental interaction whose concept and effects applied to data classification become a novel data classification technique. The simple principle of data gravitation classification (DGC) is to classify data samples by comparing the gravitation between different classes. However, the calculation of gravitation is not a trivial problem due to the different relevance of data attributes for distance computation, the presence of noisy or irrelevant attributes, and the class imbalance problem. This paper presents a gravitation-based classification algorithm which improves previous gravitation models and overcomes some of their issues. The proposed algorithm, called DGC+, employs a matrix of weights to describe the importance of each attribute in the classification of each class, which is used to weight the distance between data samples. It improves the classification performance by considering both global and local data information, especially in decision boundaries. The proposal is evaluated and compared to other well-known instance-based classification techniques, on 35 standard and 44 imbalanced data sets. The results obtained from these experiments show the great performance of the proposed gravitation model, and they are validated using several nonparametric statistical tests. © 2013 IEEE.","Classification; covariance matrix adaptation evolution strategy (CMA-ES); data gravitation; evolutionary strategies; imbalanced data","Class imbalance problems; Classification algorithm; Classification performance; Covariance matrix adaptation evolution strategies; Data gravitations; Evolutionary strategies; Imbalanced data; Non-parametric statistical tests; Evolutionary algorithms; Gravitation; Classification (of information); algorithm; article; artificial intelligence; automated pattern recognition; computer simulation; data mining; information retrieval; methodology; statistical analysis; statistical model; Algorithms; Artificial Intelligence; Computer Simulation; Data Interpretation, Statistical; Data Mining; Information Storage and Retrieval; Models, Statistical; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84890059513
"Yu L., Chan H., Elkind E.","Multiwinner elections under preferences that are single-peaked on a tree",2013,"IJCAI International Joint Conference on Artificial Intelligence",19,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062271&partnerID=40&md5=defbdbaf312997ed9e4aca34e66205c5","We study the complexity of electing a committee under several variants of the Chamberlin-Courant rule when the voters' preferences are single-peaked on a tree. We first show that this problem is easy for the egalitarian, or ""minimax"" version of this problem, for arbitrary trees and misrepresentation functions. For the standard (utilitarian) version of this problem we provide an algorithm for an arbitrary misrepresentation function whose running time is polynomial in the input size as long as the number of leaves of the underlying tree is bounded by a constant. On the other hand, we prove that our problem remains computationally hard on trees that have bounded degree, diameter, algorithm to check whether an election is single-peaked on a tree whose number of leaves does not exceed a given parameter λ.",,"Bounded degree; Input size; Minimax; Running time; Algorithms; Artificial intelligence; Forestry; Trees (mathematics); Algorithms; Artificial Intelligence; Problem Solving",Conference Paper,Scopus,2-s2.0-84896062271
"Xie P., Xing E.P.","Multi-modal distance metric learning",2013,"IJCAI International Joint Conference on Artificial Intelligence",19,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062057&partnerID=40&md5=0cb5d66737334891b2b78af36b548240","Multi-modal data is dramatically increasing with the fast growth of social media. Learning a good distance measure for data with multiple modalities is of vital importance for many applications, including retrieval, clustering, classification and recommendation. In this paper, we propose an effective and scalable multi-modal distance metric learning framework. Based on the multi-wing harmonium model, our method provides a principled way to embed data of arbitrary modalities into a single latent space, of which an optimal distance metric can be learned under proper supervision, i.e., by minimizing the distance between similar pairs whereas maximizing the distance between dissimilar pairs. The parameters are learned by jointly optimizing the data likelihood under the latent space model and the loss induced by distance supervision, thereby our method seeks a balance between explaining the data and providing an effective distance metric, which naturally avoids overfitting. We apply our general framework to text/image data and present empirical results on retrieval and classification to demonstrate the effectiveness and scalability.",,"Distance measure; Distance Metric Learning; Distance metrics; Effective distance; Harmonium model; Latent space models; Multi-modal data; Multiple modalities; Modal analysis; Optimization; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84896062057
"Nica I., Pill I., Quaritsch T., Wotawa F.","The route to success - A performance comparison of diagnosis algorithms",2013,"IJCAI International Joint Conference on Artificial Intelligence",19,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062626&partnerID=40&md5=e68a6fc3cb63c3c1a28afca1d7d2a1ec","Diagnosis, i.e., the identification of root causes for failing or unexpected system behavior, is an important task in practice. Within the last three decades, many different AI-based solutions for solving the diagnosis problem have been presented and have been gaining in attraction. This leaves us with the question of which algorithm to prefer in a certain situation. In this paper we contribute to answering this question. In particular, we compare two classes of diagnosis algorithms. One class exploits conflicts in their search, i.e., sets of system components whose correct behavior contradicts given observations. The other class ignores conflicts and derives diagnoses from observations and the underlying model directly. In our study we use different reasoning engines ranging from an optimized Horn-clause theorem prover to general SAT and constraint solvers. Thus we also address the question whether publicly available general reasoning engines can be used for an efficient diagnosis.",,"Constraint solvers; Diagnosis algorithms; Diagnosis problem; Performance comparison; Reasoning engine; System behaviors; System components; Theorem provers; Artificial intelligence; Engines; Theorem proving; Learning algorithms",Conference Paper,Scopus,2-s2.0-84896062626
"Luo Y., Tao D., Xu C., Li D., Xu C.","Vector-valued multi-view semi-supervised learning for multi-label image classification",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",19,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893379490&partnerID=40&md5=e66ca0f2e9b96926761488f6805e6662","Images are usually associated with multiple labels and comprised of multiple views, due to each image containing several objects (e.g. a pedestrian, bicycle and tree) and multiple visual features (e.g. color, texture and shape). Currently available tools tend to use either labels or features for classification, but both are necessary to describe the image properly. There have been recent successes in using vector-valued functions, which construct matrix-valued kernels, to explore the multi-label structure in the output space. This has motivated us to develop multi-view vector-valued manifold regularization (MV3MR) in order to integrate multiple features. MV3MR exploits the complementary properties of different features, and discovers the intrinsic local geometry of the compact support shared by different features, under the theme of manifold regularization. We validate the effectiveness of the proposed MV3 MR methodology for image classification by conducting extensive experiments on two challenge datasets, PASCAL VOC' 07 and MIR Flickr. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Compact support; Complementary property; Manifold regularizations; Multiple features; Multiple labels; Semi-supervised learning; Vector-valued function; Visual feature; Artificial intelligence; Supervised learning; Image classification",Conference Paper,Scopus,2-s2.0-84893379490
"Van Belle J., Valckenaers P., Vanden Berghe G., Cattrysse D.","A tabu search approach to the truck scheduling problem with multiple docks and time windows",2013,"Computers and Industrial Engineering",19,10.1016/j.cie.2013.09.024,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887108142&doi=10.1016%2fj.cie.2013.09.024&partnerID=40&md5=12d8d420b3c5d0c1fa5335a97983d248","While organizing the cross-docking operations, cross-dock managers are confronted with many decision problems. One of these problems is the truck scheduling problem. This paper presents a truck scheduling problem that is concerned with both inbound and outbound trucks at multiple dock doors. The objective is to minimize the total travel time and the total tardiness. The truck scheduling problem under study is described in detail and a mathematical model of the problem is provided which can be solved to optimality with a mixed integer programming solver, at the expense of a high computation time. Next, a tabu search approach is presented. Experimental results on new benchmark instances indicate that the proposed tabu search is able to find good quality results in a short time period, thus offering potential for integration in cross-docking decision support systems. © 2013 Elsevier Ltd. All rights reserved.","Cross-docking; Logistics; Tabu search; Truck scheduling","Computation time; Crossdocking; Decision problems; Mixed integer programming; Time windows; Time-periods; Total tardiness; Truck scheduling; Artificial intelligence; Benchmarking; Decision support systems; Docks; Hydraulic structures; Logistics; Mathematical models; Scheduling; Tabu search; Trucks",Article,Scopus,2-s2.0-84887108142
"Huang S., Li J., Ye J., Fleisher A., Chen K., Wu T., Reiman E.","A sparse structure learning algorithm for Gaussian Bayesian network identification from high-dimensional data",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",19,10.1109/TPAMI.2012.129,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885013182&doi=10.1109%2fTPAMI.2012.129&partnerID=40&md5=8f1bca277713abcd00d61bb080047bd6","Structure learning of Bayesian Networks (BNs) is an important topic in machine learning. Driven by modern applications in genetics and brain sciences, accurate and efficient learning of large-scale BN structures from high-dimensional data becomes a challenging problem. To tackle this challenge, we propose a Sparse Bayesian Network (SBN) structure learning algorithm that employs a novel formulation involving one L1-norm penalty term to impose sparsity and another penalty term to ensure that the learned BN is a Directed Acyclic Graph (DAG) - a required property of BNs. Through both theoretical analysis and extensive experiments on 11 moderate and large benchmark networks with various sample sizes, we show that SBN leads to improved learning accuracy, scalability, and efficiency as compared with 10 existing popular BN learning algorithms. We apply SBN to a real-world application of brain connectivity modeling for Alzheimer's disease (AD) and reveal findings that could lead to advancements in AD research. © 2013 IEEE.","Bayesian network; Data mining; Machine learning","Alzheimer's disease; Bayesian Networks (bns); Brain connectivity; Directed acyclic graph (DAG); Gaussian bayesian networks; High dimensional data; Modern applications; Structure learning algorithm; Data mining; Learning algorithms; Learning systems; Bayesian networks; algorithm; Alzheimer disease; article; artificial intelligence; Bayes theorem; brain; gene expression profiling; genetics; human; metabolism; normal distribution; Algorithms; Alzheimer Disease; Artificial Intelligence; Bayes Theorem; Brain; Gene Expression Profiling; Humans; Normal Distribution",Article,Scopus,2-s2.0-84885013182
"Flores-Montoya A.E., Albert E., Genaim S.","May-happen-in-parallel based deadlock analysis for concurrent objects",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",19,10.1007/978-3-642-38592-6_19,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885016677&doi=10.1007%2f978-3-642-38592-6_19&partnerID=40&md5=b7f26b2d533ffb378c87b19dae477160","We present a novel deadlock analysis for concurrent objects based on the results inferred by a points-to analysis and a may-happen-in-parallel (MHP) analysis. Similarly to other analysis, we build a dependency graph such that the absence of cycles in the graph ensures deadlock freeness. An MHP analysis provides an over-approximation of the pairs of program points that may be running in parallel. The crux of the method is that the analysis integrates the MHP information within the dependency graph in order to discard unfeasible cycles that otherwise would lead to false positives. We argue that our analysis is more precise and/or efficient than previous proposals for deadlock analysis of concurrent objects. As regards accuracy, we are able to handle cases that other analyses have pointed out as challenges. As regards efficiency, the complexity of our deadlock analysis is polynomial. © 2013 IFIP International Federation for Information Processing.",,"Concurrent objects; Deadlock analysis; Deadlock freeness; Dependency graphs; False positive; Points-to analysis; Program points; Running-in; Artificial intelligence; Computer science",Conference Paper,Scopus,2-s2.0-84885016677
"Torgo L., Ribeiro R.P., Pfahringer B., Branco P.","SMOTE for regression",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",19,10.1007/978-3-642-40669-0_33,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884720472&doi=10.1007%2f978-3-642-40669-0_33&partnerID=40&md5=76532e80b024189c86acfc87880613ac","Several real world prediction problems involve forecasting rare values of a target variable. When this variable is nominal we have a problem of class imbalance that was already studied thoroughly within machine learning. For regression tasks, where the target variable is continuous, few works exist addressing this type of problem. Still, important application areas involve forecasting rare extreme values of a continuous target variable. This paper describes a contribution to this type of tasks. Namely, we propose to address such tasks by sampling approaches. These approaches change the distribution of the given training data set to decrease the problem of imbalance between the rare target cases and the most frequent ones. We present a modification of the well-known Smote algorithm that allows its use on these regression tasks. In an extensive set of experiments we provide empirical evidence for the superiority of our proposals for these particular regression tasks. The proposed SmoteR method can be used with any existing regression algorithm turning it into a general tool for addressing problems of forecasting rare extreme values of a continuous target variable. © 2013 Springer-Verlag.",,"Application area; Class imbalance; Extreme value; General tools; Prediction problem; Regression algorithms; SMOTE algorithm; Training data sets; Artificial intelligence; Forecasting; Regression analysis",Conference Paper,Scopus,2-s2.0-84884720472
"Porbadnigk A.K., Treder M.S., Blankertz B., Antons J.-N., Schleicher R., Möller S., Curio G., Müller K.-R.","Single-trial analysis of the neural correlates of speech quality perception",2013,"Journal of Neural Engineering",19,10.1088/1741-2560/10/5/056003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885435765&doi=10.1088%2f1741-2560%2f10%2f5%2f056003&partnerID=40&md5=ee4513134769c3ab5484ce273a9243e1","Objective. Assessing speech quality perception is a challenge typically addressed in behavioral and opinion-seeking experiments. Only recently, neuroimaging methods were introduced, which were used to study the neural processing of quality at group level. However, our electroencephalography (EEG) studies show that the neural correlates of quality perception are highly individual. Therefore, it became necessary to establish dedicated machine learning methods for decoding subject-specific effects. Approach. The effectiveness of our methods is shown by the data of an EEG study that investigates how the quality of spoken vowels is processed neurally. Participants were asked to indicate whether they had perceived a degradation of quality (signal-correlated noise) in vowels, presented in an oddball paradigm. Main results. We find that the P3 amplitude is attenuated with increasing noise. Single-trial analysis allows one to show that this is partly due to an increasing jitter of the P3 component. A novel classification approach helps to detect trials with presumably non-conscious processing at the threshold of perception. We show that this approach uncovers a non-trivial confounder between neural hits and neural misses. Significance. The combined use of EEG signals and machine learning methods results in a significant 'neural' gain in sensitivity (in processing quality loss) when compared to standard behavioral evaluation; averaged over 11 subjects, this amounts to a relative improvement in sensitivity of 35%. © 2013 IOP Publishing Ltd.",,"Classification approach; Machine learning methods; Neural-processing; Oddball paradigms; Processing quality; Quality perceptions; Single-trial analysis; Subject-specific effects; Electrophysiology; Learning systems; Linguistics; Neuroimaging; Quality control; adult; article; auditory discrimination; controlled study; discriminant analysis; electroencephalography; female; human; human experiment; machine learning; male; nerve potential; neuroimaging; neurophysiology; normal human; perception; priority journal; signal noise ratio; speech analysis; speech quality perception; vowel; Acoustic Stimulation; Algorithms; Alpha Rhythm; Area Under Curve; Artificial Intelligence; Auditory Threshold; Cognition; Data Interpretation, Statistical; Discriminant Analysis; Electroencephalography; Electrooculography; Evoked Potentials; Female; Hearing; Humans; Male; Models, Neurological; Psychomotor Performance; Quality Indicators, Health Care; Speech Discrimination Tests; Speech Perception; Technology",Article,Scopus,2-s2.0-84885435765
"Coletti G., Scozzafava R., Vantaggi B.","Inferential processes leading to possibility and necessity",2013,"Information Sciences",19,10.1016/j.ins.2012.10.034,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880308394&doi=10.1016%2fj.ins.2012.10.034&partnerID=40&md5=7944a222817c254d0476ec80032a4eb7","This paper deals with the upper and lower bounds of a class of uncertainty measures endowed with particular characteristics (decomposability, monotonicity, partial additivity and so on). We consider an initial partial assessment consistent with either probability or possibility or necessity, then we study the upper and lower envelopes of all possible extensions. By resorting to a notion of weak logical independence we get as lower or upper envelope a possibility or a necessity, respectively, starting either from a probability or from a possibility or from a necessity. © 2012 Elsevier Inc. All rights reserved.","Coherent probability; Possibility; Upper and lower envelope; Weak logical independence","Decomposability; Lower envelopes; Monotonicity; Partial assessments; Possibility; Uncertainty measures; Upper and lower bounds; Weak logical independence; Artificial intelligence; Software engineering; Probability",Article,Scopus,2-s2.0-84880308394
"Gilio A., Sanfilippo G.","Quasi conjunction, quasi disjunction, t-norms and t-conorms: Probabilistic aspects",2013,"Information Sciences",19,10.1016/j.ins.2013.03.019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880316759&doi=10.1016%2fj.ins.2013.03.019&partnerID=40&md5=dab0e7167d80281f9342f09bcd12505b","We make a probabilistic analysis related to some inference rules which play an important role in nonmonotonic reasoning. In a coherence-based setting, we study the extensions of a probability assessment defined on n conditional events to their quasi conjunction, and by exploiting duality, to their quasi disjunction. The lower and upper bounds coincide with some well known t-norms and t-conorms: minimum, product, Lukasiewicz, and Hamacher t-norms and their dual t-conorms. On this basis we obtain Quasi And and Quasi Or rules. These are rules for which any finite family of conditional events p-entails the associated quasi conjunction and quasi disjunction. We examine some cases of logical dependencies, and we study the relations among coherence, inclusion for conditional events, and p-entailment. We also consider the Or rule, where quasi conjunction and quasi disjunction of premises coincide with the conclusion. We analyze further aspects of quasi conjunction and quasi disjunction, by computing probabilistic bounds on premises from bounds on conclusions. Finally, we consider biconditional events, and we introduce the notion of an n-conditional event. Then we give a probabilistic interpretation for a generalized Loop rule. In an appendix we provide explicit expressions for the Hamacher t-norm and t-conorm in the unitary hypercube. © 2012 Elsevier Inc. All rights reserved.","Coherence; Generalized Loop rule; Goodman-Nguyen inclusion relation; Lower/upper probability bounds; Quasi conjunction/disjunction; t-Norms/conorms","Generalized Loop rule; Inclusion relation; Probability bound; Quasi conjunction/disjunction; T - Norm; Artificial intelligence; Coherent light; Software engineering; Mathematical operators",Article,Scopus,2-s2.0-84880316759
"Sadjadi H., Hashtrudi-Zaad K., Fichtinger G.","Fusion of electromagnetic trackers to improve needle deflection estimation: Simulation study",2013,"IEEE Transactions on Biomedical Engineering",19,10.1109/TBME.2013.2262658,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884536926&doi=10.1109%2fTBME.2013.2262658&partnerID=40&md5=07aae3a7ca32f0a841372b5d830f3fb3","We present a needle deflection estimation method to anticipate needle bending during insertion into deformable tissue. Using limited additional sensory information, our approach reduces the estimation error caused by uncertainties inherent in the conventional needle deflection estimation methods. We use Kalman filters to combine a kinematic needle deflection model with the position measurements of the base and the tip of the needle taken by electromagnetic (EM) trackers. One EM tracker is installed on the needle base and estimates the needle tip position indirectly using the kinematic needle deflection model. Another EM tracker is installed on the needle tip and estimates the needle tip position through direct, but noisy measurements. Kalman filters are then employed to fuse these two estimates in real time and provide a reliable estimate of the needle tip position, with reduced variance in the estimation error. We implemented this method to compensate for needle deflection during simulated needle insertions and performed sensitivity analysis for various conditions. At an insertion depth of 150 mm, we observed needle tip estimation error reductions in the range of 28% (from 1.8 to 1.3 mm) to 74% (from 4.8 to 1.2 mm), which demonstrates the effectiveness of our method, offering a clinically practical solution. © 1964-2012 IEEE.","Electromagnetic (EM) tracking; Kalman filter (KF); Needle deflection estimation; Sensor fusion; Surgical navigation","Electromagnetic tracker; Estimation error reduction; Needle deflection; Noisy measurements; Practical solutions; Sensor fusion; Sensory information; Surgical navigation; Electromagnetism; Kalman filters; Kinematics; Needles; Estimation; article; clinical practice; electromagnetic field; kinematics; measurement error; needle; reduction; sensitivity analysis; simulation; Ablation Techniques; Artificial Intelligence; Biopsy, Needle; Data Interpretation, Statistical; Electromagnetic Fields; Humans; Image Interpretation, Computer-Assisted; Needles; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique; Surgery, Computer-Assisted",Article,Scopus,2-s2.0-84884536926
"Poullis C.","A framework for automatic modeling from point cloud data",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",19,10.1109/TPAMI.2013.64,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884573272&doi=10.1109%2fTPAMI.2013.64&partnerID=40&md5=6768d5fbdd4c0fff18306c0cae893dc9","We propose a complete framework for the automatic modeling from point cloud data. Initially, the point cloud data are preprocessed into manageable datasets, which are then separated into clusters using a novel two-step, unsupervised clustering algorithm. The boundaries extracted for each cluster are then simplified and refined using a fast energy minimization process. Finally, three-dimensional models are generated based on the roof outlines. The proposed framework has been extensively tested, and the results are reported. © 1979-2012 IEEE.","3D modeling; clustering; point cloud; segmentation; shape refinement; Three-dimensional reconstruction","3-d modeling; clustering; Point cloud; Shape refinement; Three-dimensional reconstruction; Clustering algorithms; Image segmentation; Three dimensional; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; computer simulation; methodology; theoretical model; automated pattern recognition; computer assisted diagnosis; procedures; Algorithms; Artificial Intelligence; Computer Simulation; Image Interpretation, Computer-Assisted; Models, Theoretical; Pattern Recognition, Automated; Algorithms; Artificial Intelligence; Computer Simulation; Image Interpretation, Computer-Assisted; Models, Theoretical; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84884573272
"Alvarez M.A., Luengo D., Lawrence N.D.","Linear latent force models using gaussian processes",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",19,10.1109/TPAMI.2013.86,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884557157&doi=10.1109%2fTPAMI.2013.86&partnerID=40&md5=40c34486cfc5055522b4dc16639e311d","Purely data-driven approaches for machine learning present difficulties when data are scarce relative to the complexity of the model or when the model is forced to extrapolate. On the other hand, purely mechanistic approaches need to identify and specify all the interactions in the problem at hand (which may not be feasible) and still leave the issue of how to parameterize the system. In this paper, we present a hybrid approach using Gaussian processes and differential equations to combine data-driven modeling with a physical model of the system. We show how different, physically inspired, kernel functions can be developed through sensible, simple, mechanistic assumptions about the underlying system. The versatility of our approach is illustrated with three case studies from motion capture, computational biology, and geostatistics. © 1979-2012 IEEE.","differential equations; dynamical systems; Gaussian processes; motion capture data; multitask learning; spatiotemporal covariances","Computational biology; Data-driven approach; Data-driven model; Gaussian Processes; Motion capture data; Multitask learning; Spatiotemporal covariance; Underlying systems; Bioinformatics; Differential equations; Dynamical systems; Gaussian noise (electronic); Learning algorithms; Gaussian distribution; algorithm; article; artificial intelligence; automated pattern recognition; computer simulation; methodology; normal distribution; sample size; statistical model; automated pattern recognition; procedures; Algorithms; Artificial Intelligence; Computer Simulation; Linear Models; Normal Distribution; Pattern Recognition, Automated; Sample Size; Algorithms; Artificial Intelligence; Computer Simulation; Linear Models; Normal Distribution; Pattern Recognition, Automated; Sample Size",Article,Scopus,2-s2.0-84884557157
"Tan H.L., Li Z., Tan Y.H., Rahardja S., Yeo C.","A perceptually relevant mse-based image quality metric",2013,"IEEE Transactions on Image Processing",19,10.1109/TIP.2013.2273671,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884565356&doi=10.1109%2fTIP.2013.2273671&partnerID=40&md5=a8ba6cadb7288450b0ff619c135ec633","Image quality metrics (IQMs), such as the mean squared error (MSE) and the structural similarity index (SSIM), are quantitative measures to approximate perceived visual quality. In this paper, through analyzing the relationship between the MSE and the SSIM under an additive noise distortion model, we propose a perceptually relevant MSE-based IQM, MSE-SSIM, which is expressed in terms of the variance of the source image and the MSE between the source and distorted images. Evaluations on three publicly available databases (LIVE, CSIQ, and TID2008) show that the proposed metric, despite requiring less computation, compares favourably in performance to several existing IQMs. In addition, due to its simplicity, MSE-SSIM is amenable for the use in a wide range of image and video tasks that involve solving an optimization problem. As an example, MSE-SSIM is used as the objective function in designing a Wiener filter that aims at optimizing the perceptual visual quality of the output. Experimental results show that the images filtered with a MSE-SSIM-optimal Wiener filter have better visual quality than those filtered with a MSE-optimal Wiener filter. © 1992-2012 IEEE.","Image quality metric; mean squared error (MSE); structural similarity index (SSIM); Wiener filter","Image quality metrics; Mean squared error; Objective functions; Optimization problems; Perceptual visual quality; Quantitative measures; Structural similarity indices (SSIM); WIENER filters; Image quality; Mean square error; Optimization; algorithm; article; artifact; artificial intelligence; automated pattern recognition; biomimetics; computer assisted diagnosis; human; image enhancement; methodology; reproducibility; sensitivity and specificity; signal noise ratio; vision; automated pattern recognition; biomimetics; computer assisted diagnosis; procedures; Algorithms; Artifacts; Artificial Intelligence; Biomimetics; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Signal-To-Noise Ratio; Visual Perception; Algorithms; Artifacts; Artificial Intelligence; Biomimetics; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Signal-To-Noise Ratio; Visual Perception",Article,Scopus,2-s2.0-84884565356
"Zhang Y., Sinclair II M., Chien A.A.","Improving performance portability in OpenCL programs",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",19,10.1007/978-3-642-38750-0_11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884497801&doi=10.1007%2f978-3-642-38750-0_11&partnerID=40&md5=faa32360d1cdd6bf1862a108a2c79ace","We study the performance portability of OpenCL across diverse architectures including NVIDIA GPU, Intel Ivy Bridge CPU, and AMD Fusion APU. We present detailed performance analysis at assembly level on three exemplar OpenCL benchmarks: SGEMM, SpMV, and FFT. We also identify a number of tuning knobs that are critical to performance portability, including threads-data mapping, data layout, tiling size, data caching, and operation-specific factors. We further demonstrate that proper tuning could improve the OpenCL portable performance from the current 15% to a potential 67% of the state-of-the-art performance on the Ivy Bridge CPU. Finally, we evaluate the current OpenCL programming model, and propose a list of extensions that improve performance portability. © 2013 Springer-Verlag.",,"Assembly levels; Data layouts; Improve performance; Improving performance; Performance analysis; Performance portability; Programming models; State-of-the-art performance; Artificial intelligence; Computer science; Benchmarking",Conference Paper,Scopus,2-s2.0-84884497801
"Bartczuk Ł., Przybył A., Dziwiński P.","Hybrid state variables - Fuzzy logic modelling of nonlinear objects",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",19,10.1007/978-3-642-38658-9_21,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884394490&doi=10.1007%2f978-3-642-38658-9_21&partnerID=40&md5=22810f3b85b5bfea898f8590dac5dbee","In this paper a new hybrid method for modelling of nonlinear dynamic systems is proposed. It uses fuzzy logic system together with state variables technique to obtain the local linear approximation performed continuously for successive operating points. This approach provides good accuracy and allows the use of very convenient and well-known method from linear control theory to analyse the obtained model. © 2013 Springer-Verlag.",,"Fuzzy logic system; Hybrid method; Hybrid state; Linear control theory; Local linear approximation; Nonlinear objects; Operating points; State variables; Fuzzy logic; Linear control systems; Nonlinear dynamical systems; Soft computing; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84884394490
"Hazay C., López-Alt A., Wee H., Wichs D.","Leakage-resilient cryptography from minimal assumptions",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",19,10.1007/978-3-642-38348-9_10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883335633&doi=10.1007%2f978-3-642-38348-9_10&partnerID=40&md5=e7dfb864ebc2eaef9daec8005fc7db1a","We present new constructions of leakage-resilient cryptosystems, which remain provably secure even if the attacker learns some arbitrary partial information about their internal secret key. For any polynomial ℓ, we can instantiate these schemes so as to tolerate up to ℓ bits of leakage. While there has been much prior work constructing such leakage-resilient cryptosystems under concrete number-theoretic and algebraic assumptions, we present the first schemes under general and minimal assumptions. In particular, we construct: - Leakage-resilient public-key encryption from any standard public-key encryption. - Leakage-resilient weak pseudorandom functions, symmetric-key encryption, and message-authentication codes from any one-way function. These are the first constructions of leakage-resilient symmetric-key primitives that do not rely on public-key assumptions. We also get the first constructions of leakage-resilient public-key encryption from ""search assumptions"", such as the hardness of factoring or CDH. Although our schemes can tolerate arbitrarily large amounts of leakage, the tolerated rate of leakage (defined as the ratio of leakage-amount to key-size) is rather poor in comparison to prior results under specific assumptions. As a building block of independent interest, we study a notion of weak hash-proof systems in the public-key and symmetric-key settings. While these inherit some of the interesting security properties of standard hash-proof systems, we can instantiate them under general assumptions. © 2013 International Association for Cryptologic Research.",,"First constructions; Leakage-resilient cryptographies; Message-authentication codes; One-way functions; Partial information; Pseudo-random functions; Public-key encryption; Security properties; Artificial intelligence; Computer science; Public key cryptography",Conference Paper,Scopus,2-s2.0-84883335633
"Pike A., Danner E., Boughton D., Melton F., Nemani R., Rajagopalan B., Lindley S.","Forecasting river temperatures in real time using a stochastic dynamics approach",2013,"Water Resources Research",19,10.1002/wrcr.20389,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883306944&doi=10.1002%2fwrcr.20389&partnerID=40&md5=b7ad5e5803b5b98ef3711794570333a6","We address the growing need for accurate water temperature predictions in regulated rivers to inform decision support systems and protect aquatic habitats. Although many suitable river temperature models exist, few simultaneously model water temperature dynamics while considering uncertainty of predictions and assimilating observations. Here, we employ a stochastic dynamics approach to water temperature modeling that estimates both the water temperature state and its uncertainty by propagating error through a physically based dynamical system. This method involves converting the governing hydrodynamic and heat transport equations into a state space form and assimilating observations via the Kalman Filter. This model, called the River Assessment for Forecasting Temperature (RAFT), closes the heat budget by tracking heat movement using a robust semi-Lagrangian numerical scheme. RAFT considers key thermodynamic processes, including advection, longitudinal dispersion, atmospheric heat fluxes, lateral inflows, streambed heat exchange, and unsteady nonuniform flow. Inputs include gridded meteorological forecasts from a numerical weather prediction model, bathymetric cross-sectional geometry, and temperature and flow measurements at the upstream boundary and tributaries. We applied RAFT to an ∼100 km portion of the Sacramento River in California, downstream of Keswick Dam (a regulatory dam below Shasta Dam), at a spatial resolution of 2 km and a temporal resolution of 15 min. Model prediction error over a 6 month calibration period was on the order of 0.5°C. When temperature and flow gage data were assimilated, the mean prediction error was significantly less (0.25°C). The model accurately predicts the magnitude and timing of diel temperature fluctuations and can provide 72 h water temperature forecasts when linked with meteorological forecasts and real-time flow/temperature monitoring networks. RAFT is potentially scalable to model and forecast fine-grained one-dimensional temperature dynamics covering a broad extent in a variety of regulated rivers provided that adequate input data are available. Key Points The paper details a physically-based river temperature model The model is accurate for a case study on the Sacramento River The model may be applied to other managed rivers ©2013. American Geophysical Union. All Rights Reserved.","Heat Budget; River Management; Sacramento River; uNmerical Prediction; Water Temperature","Heat budget; Heat transport equation; Longitudinal dispersions; Numerical weather prediction models; River management; Sacramento River; Temperature fluctuation; Water temperatures; Artificial intelligence; Atmospheric thermodynamics; Budget control; Decision support systems; Dynamical systems; State space methods; Stochastic systems; Temperature; Uncertainty analysis; Weather forecasting; Rivers; accuracy assessment; calibration; dam; decision support system; error analysis; flow measurement; heat budget; heat flux; heat transfer; hydrodynamics; Kalman filter; meteorology; numerical model; real time; river flow; river management; river water; stochasticity; water temperature; weather forecasting; California; Sacramento River; United States",Article,Scopus,2-s2.0-84883306944
"Penate-Sanchez A., Andrade-Cetto J., Moreno-Noguer F.","Exhaustive linearization for robust camera pose and focal length estimation",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",19,10.1109/TPAMI.2013.36,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883198236&doi=10.1109%2fTPAMI.2013.36&partnerID=40&md5=04ee7e05441e162726d73f7882b11ffe","We propose a novel approach for the estimation of the pose and focal length of a camera from a set of 3D-to-2D point correspondences. Our method compares favorably to competing approaches in that it is both more accurate than existing closed form solutions, as well as faster and also more accurate than iterative ones. Our approach is inspired on the EPnP algorithm, a recent O(n) solution for the calibrated case. Yet we show that considering the focal length as an additional unknown renders the linearization and relinearization techniques of the original approach no longer valid, especially with large amounts of noise. We present new methodologies to circumvent this limitation termed exhaustive linearization and exhaustive relinearization which perform a systematic exploration of the solution space in closed form. The method is evaluated on both real and synthetic data, and our results show that besides producing precise focal length estimation, the retrieved camera pose is almost as accurate as the one computed using the EPnP, which assumes a calibrated camera. © 1979-2012 IEEE.","Camera calibration; perspective-n-point problem","Calibrated cameras; Camera calibration; Closed form solutions; Perspective-n-point problems; Point correspondence; Relinearization; Solution space; Systematic exploration; Cameras; Estimation; Linearization; Iterative methods; algorithm; artificial intelligence; automated pattern recognition; computer assisted diagnosis; computer simulation; image enhancement; procedures; reproducibility; sensitivity and specificity; statistical model; three dimensional imaging; article; automated pattern recognition; computer assisted diagnosis; methodology; three dimensional imaging; Algorithms; Artificial Intelligence; Computer Simulation; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Linear Models; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Algorithms; Artificial Intelligence; Computer Simulation; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Linear Models; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84883198236
"Rahimi Azghadi M., Al-Sarawi S., Abbott D., Iannella N.","A neuromorphic VLSI design for spike timing and rate based synaptic plasticity",2013,"Neural Networks",19,10.1016/j.neunet.2013.03.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880835986&doi=10.1016%2fj.neunet.2013.03.003&partnerID=40&md5=a3bb4714cb06a5a34800df4d02ecf514","Triplet-based Spike Timing Dependent Plasticity (TSTDP) is a powerful synaptic plasticity rule that acts beyond conventional pair-based STDP (PSTDP). Here, the TSTDP is capable of reproducing the outcomes from a variety of biological experiments, while the PSTDP rule fails to reproduce them. Additionally, it has been shown that the behaviour inherent to the spike rate-based Bienenstock-Cooper-Munro (BCM) synaptic plasticity rule can also emerge from the TSTDP rule. This paper proposes an analogue implementation of the TSTDP rule. The proposed VLSI circuit has been designed using the AMS 0.35μm CMOS process and has been simulated using design kits for Synopsys and Cadence tools. Simulation results demonstrate how well the proposed circuit can alter synaptic weights according to the timing difference amongst a set of different patterns of spikes. Furthermore, the circuit is shown to give rise to a BCM-like learning rule, which is a rate-based rule. To mimic an implementation environment, a 1000 run Monte Carlo (MC) analysis was conducted on the proposed circuit. The presented MC simulation analysis and the simulation result from fine-tuned circuits show that it is possible to mitigate the effect of process variations in the proof of concept circuit; however, a practical variation aware design technique is required to promise a high circuit performance in a large scale neural network. We believe that the proposed design can play a significant role in future VLSI implementations of both spike timing and rate based neuromorphic learning systems. © 2013 Elsevier Ltd.","BCM; Neuromorphic VLSI; Rate based plasticity; Spike timing dependent plasticity; Synaptic plasticity","BCM; Biological experiments; Neuromorphic VLSI; Spike timing dependent plasticities; Synaptic plasticity; Synaptic plasticity rules; Variation-aware design; VLSI implementation; Circuit simulation; CMOS integrated circuits; Computer simulation; Design; Electric network analysis; VLSI circuits; Timing circuits; article; controlled study; learning algorithm; mathematical computing; mathematical model; Monte Carlo method; nerve cell network; nerve cell plasticity; nerve conduction; priority journal; triplet based spike timing dependent plasticity; BCM; Neuromorphic VLSI; Rate based plasticity; Spike timing dependent plasticity; Synaptic plasticity; Action Potentials; Animals; Artificial Intelligence; Humans; Neuronal Plasticity; Neurons; Numerical Analysis, Computer-Assisted; Semiconductors; Synapses; Time Factors",Article,Scopus,2-s2.0-84880835986
"Borgwardt S., Koutsouleris N., Aston J., Studerus E., Smieskova R., Riecher-Rössler A., Meisenzahl E.M.","Distinguishing prodromal from first-episode psychosis using neuroanatomical single-subject pattern recognition",2013,"Schizophrenia Bulletin",19,10.1093/schbul/sbs095,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878825019&doi=10.1093%2fschbul%2fsbs095&partnerID=40&md5=2af8ef6dbbc4cb6c5ebad6d36ec06077","Background: The at-risk mental state for psychosis (ARMS) and the first episode of psychosis have been associated with structural brain abnormalities that could aid in the individualized early recognition of psychosis. However, it is unknown whether the development of these brain alterations predates the clinical deterioration of at-risk individuals, or alternatively, whether it parallels the transition to psychosis at the single-subject level. Methods: We evaluated the performance of an magnetic resonance imaging (MRI)-based classification system in classifying disease stages from at-risk individuals with subsequent transition to psychosis (ARMS-T) and patients with first-episode psychosis (FE). Pairwise and multigroup biomarkers were constructed using the structural MRI data of 22 healthy controls (HC), 16 ARMS-T and 23 FE subjects. The performance of these biomarkers was measured in unseen test cases using repeated nested cross-validation. Results: The classification accuracies in the HC vs FE, HC vs ARMS-T, and ARMS-T vs FE analyses were 86.7%, 80.7%, and 80.0%, respectively. The neuroanatomical decision functions underlying these discriminative results particularly involved the frontotemporal, cingulate, cerebellar, and subcortical brain structures. Conclusions: Our findings suggest that structural brain alterations accumulate at the onset of psychosis and occur even before transition to psychosis allowing for the single-subject differentiation of the prodromal and first-episode stages of the disease. Pattern regression techniques facilitate an accurate prediction of these structural brain dynamics at the early stage of psychosis, potentially allowing for the early recognition of individuals at risk of developing psychosis. © The Author 2012. Published by Oxford University Press on behalf of the Maryland Psychiatric Research Center. All rights reserved.","at-risk mental state; early prediction of psychosis; machine learning; multivariate analysis; support vector machine; voxel-based morphometry","adolescent; article; cerebellum; child; cingulate gyrus; clinical article; controlled study; diagnostic accuracy; diagnostic test accuracy study; discriminant analysis; female; frontal lobe; high risk patient; human; male; neuroanatomy; nuclear magnetic resonance imaging; priority journal; prodromal symptom; psychosis; school child; subcortex; support vector machine; temporal lobe; at-risk mental state; early prediction of psychosis; machine learning; multivariate analysis; support vector machine; voxel-based morphometry; Adult; Artificial Intelligence; Biological Markers; Cerebrum; Female; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Male; Middle Aged; Predictive Value of Tests; Prodromal Symptoms; Psychotic Disorders; Retrospective Studies; Risk; Time Factors; Young Adult",Article,Scopus,2-s2.0-84878825019
"Rueda J.L., Erlich I.","Hybrid Mean-Variance Mapping Optimization for solving the IEEE-CEC 2013 competition problems",2013,"2013 IEEE Congress on Evolutionary Computation, CEC 2013",19,10.1109/CEC.2013.6557761,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881606417&doi=10.1109%2fCEC.2013.6557761&partnerID=40&md5=ac2af5939457e3ad1600cdfe0fee3a05","Mean-Variance Mapping Optimization (MVMO) is a recent addition to the emerging field of heuristic optimization algorithms, which has been quite successful in solving a variety of power system optimization problems. This paper introduces a hybrid variant of MVMO (MVMO-SH) for solving the IEEE-CEC 2013 competition test suite. MVMO-SH is based on a swarm scheme of MVMO with embedded local search and multi-parent crossover strategies to increase search diversity and solution quality. Numerical results attest to the promising prospect of MVMO-SH to become a general purpose optimization algorithm. © 2013 IEEE.","Heuristic optimization; mean-variance mapping optimization; single objective optimization; swarm intelligence","Heuristic optimization; Heuristic optimization algorithms; Mapping optimization; Multi-parent crossover; Optimization algorithms; Power system optimization; Single objective optimization; Swarm Intelligence; Artificial intelligence; Evolutionary algorithms; Mapping; Optimization",Conference Paper,Scopus,2-s2.0-84881606417
"Jabbour S., Raddaoui B.","Measuring inconsistency through minimal proofs",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",19,10.1007/978-3-642-39091-3-25,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880758352&doi=10.1007%2f978-3-642-39091-3-25&partnerID=40&md5=da15d1ee2ab5feab4a63d1266812a7d8","Measuring the degree of inconsistency of a knowledge base provides important context information for making easier inconsistency handling. In this paper, we propose a new fine-grained measure to quantify the degree of inconsistency of propositional formulae. Our inconsistency measure uses in an original way the minimal proofs to characterize the responsibility of each formula in the global inconsistency. We give an extension of such measure to quantify the inconsistency of the whole base. Furthermore, we show that our measure satisfies the important properties characterizing an intuitive inconsistency measure. Finally, we address the problem of restoring consistency using an inconsistency measure. © 2013 Springer-Verlag Berlin Heidelberg.",,"Context information; Inconsistency handling; Inconsistency measures; Knowledge base; Measuring inconsistency; Minimal proofs; Artificial intelligence; Computer science; Knowledge based systems",Conference Paper,Scopus,2-s2.0-84880758352
"Limongelli C., Lombardi M., Marani A., Sciarrone F.","A teaching-style based social network for didactic building and sharing",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",19,10.1007/978-3-642-39112-5-110,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880013316&doi=10.1007%2f978-3-642-39112-5-110&partnerID=40&md5=d0eda67a3e91ccb342cf00fdab96531c","Nowadays, teachers tend to build their own didactic local repository composed by learning objects retrieved from web repositories or, in most cases, by self-made didactic material. In this way they do not share their teaching experience, loosing a precious shortcut to a fast professional update and to an improvement of their teaching activity. In this paper we address the problem of helping teachers to retrieve didactic material from a repository through a didactic social network where teachers with similar Teaching Styles, can help each other in retrieving educational material. To this aim a teaching-styles based social network is built following the Grasha TS paradigm. We present a first evaluation of the network embedded in a web application. © 2013 Springer-Verlag Berlin Heidelberg.",,"Didactic materials; Educational materials; Learning objects; Teaching activities; Teaching experience; Teaching styles; WEB application; Web repositories; Artificial intelligence; Education; Social networking (online)",Conference Paper,Scopus,2-s2.0-84880013316
"Holzinger A., Stocker C., Ofner B., Prohaska G., Brabenetz A., Hofmann-Wellenhof R.","Combining HCI, natural language processing, and knowledge discovery - Potential of IBM content analytics as an assistive technology in the biomedical field",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",19,10.1007/978-3-642-39146-0_2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879878402&doi=10.1007%2f978-3-642-39146-0_2&partnerID=40&md5=3f6ac9c54ba1109da40a01f3a062826b","Medical professionals are confronted with a flood of big data most of it containing unstructured information. Such unstructured information is the subset of information, where the information itself describes parts of what constitutes as significant within it, or in other words - structure and information are not completely separable. The best example for such unstructured information is text. For many years, text mining has been an essential area of medical informatics. Although text can easily be created by medical professionals, the support of automatic analyses for knowledge discovery is extremely difficult. We follow the definition that knowledge consists of a set of hypotheses, and knowledge discovery is the process of finding or generating new hypotheses by medical professionals with the aim of getting insight into the data. In this paper we present some lessons learned of ICA for dermatological knowledge discovery, for the first time. We follow the HCI-KDD approach, i.e. with the human expert in the loop matching the best of two worlds: human intelligence with computational intelligence. © 2013 Springer-Verlag.","Content Analytics; data mining; human-computer interaction; Knowledge discovery; medical informatics; Unstructured Information Management","Assistive technology; Automatic analysis; Content Analytics; Human intelligence; Medical informatics; Medical professionals; NAtural language processing; Unstructured information managements; Artificial intelligence; Data mining; Human computer interaction; Information management; Information science; Natural language processing systems",Conference Paper,Scopus,2-s2.0-84879878402
"Frasca M., Bertoni A., Re M., Valentini G.","A neural network algorithm for semi-supervised node label learning from unbalanced data",2013,"Neural Networks",19,10.1016/j.neunet.2013.01.021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875251066&doi=10.1016%2fj.neunet.2013.01.021&partnerID=40&md5=6fe1b87d635c212d998e5f9720f1ad81","Given a weighted graph and a partial node labeling, the graph classification problem consists in predicting the labels of all the nodes. In several application domains, from gene to social network analysis, the labeling is unbalanced: for instance positive labels may be much less than negatives. In this paper we present COSNet (COst Sensitive neural Network), a neural algorithm for predicting node labels in graphs with unbalanced labels. COSNet is based on a 2-parameter family of Hopfield networks, and consists of two main steps: (1) the network parameters are learned through a cost-sensitive optimization procedure; (2) a suitable Hopfield network restricted to the unlabeled nodes is considered and simulated. The reached equilibrium point induces the classification of the unlabeled nodes. The restriction of the dynamics leads to a significant reduction in time complexity and allows the algorithm to nicely scale with large networks. An experimental analysis on real-world unbalanced data, in the context of the genome-wide prediction of gene functions, shows the effectiveness of the proposed approach. © 2013 Elsevier Ltd.","Hopfield neural networks; Learning from unbalanced data; Node label prediction; Semi-supervised learning in graphs","Cost-sensitive neural networks; Experimental analysis; Label predictions; Neural network algorithm; Optimization procedures; Semi-supervised learning; Social Network Analysis; Unbalanced data; Algorithms; Forecasting; Genes; Hopfield neural networks; Neural networks; Social networking (online); Supervised learning; Graph theory; algorithm; analytical parameters; article; artificial neural network; dynamics; gene function; genome analysis; learning; prediction; priority journal; process optimization; simulation; statistical analysis; Algorithms; Artificial Intelligence; Learning; Neural Networks (Computer); Statistics as Topic",Article,Scopus,2-s2.0-84875251066
"Geffner H., Bonet B.","A concise introduction to models and methods for automated planning",2013,"Synthesis Lectures on Artificial Intelligence and Machine Learning",19,10.2200/S00513ED1V01Y201306AIM022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880083926&doi=10.2200%2fS00513ED1V01Y201306AIM022&partnerID=40&md5=9a5d55b4673990c231de08b2f6e45d04","Planning is the model-based approach to autonomous behavior where the agent behavior is derived automatically from a model of the actions, sensors, and goals. The main challenges in planning are computational as all models, whether featuring uncertainty and feedback or not, are intractable in the worst case when represented in compact form. In this book, we look at a variety of models used in AI planning, and at the methods that have been developed for solving them. The goal is to provide a modern and coherent view of planning that is precise, concise, and mostly self-contained, without being shallow. For this, we make no attempt at covering the whole variety of planning approaches, ideas, and applications, and focus on the essentials. The target audience of the book are students and researchers interested in autonomous behavior and planning from an AI, engineering, or cognitive science perspective. Copyright © 2013 by Morgan & Claypool.","action selection; autonomous behavior; belief tracking; domain-independent problem solving; MDP and POMDP planning; model-based control; plan generation and recognition; planning; planning with incomplete information and sensing","Action selection; Autonomous behaviors; Model-based control; Plan generation; Planning with incomplete information; Chemistry; Planning; Software engineering; Speech synthesis; Artificial intelligence",Article,Scopus,2-s2.0-84880083926
"Mandal I., Sairam N.","Accurate telemonitoring of Parkinson's disease diagnosis using robust inference system",2013,"International Journal of Medical Informatics",19,10.1016/j.ijmedinf.2012.10.006,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876973345&doi=10.1016%2fj.ijmedinf.2012.10.006&partnerID=40&md5=3452c89f04ef3ea517b2d3c1d526c07e","This work presents more precise computational methods for improving the diagnosis of Parkinson's disease based on the detection of dysphonia. New methods are presented for enhanced evaluation and recognize Parkinson's disease affected patients at early stage. Analysis is performed with significant level of error tolerance rate and established our results with corrected T-test. Here new ensembles and other machine learning methods consisting of multinomial logistic regression classifier with Haar wavelets transformation as projection filter that outperform logistic regression is used. Finally a novel and reliable inference system is presented for early recognition of people affected by this disease and presents a new measure of the severity of the disease. Feature selection method is based on Support Vector Machines and ranker search method. Performance analysis of each model is compared to the existing methods and examines the main advancements and concludes with propitious results. Reliable methods are proposed for treating Parkinson's disease that includes sparse multinomial logistic regression, Bayesian network, Support Vector Machines, Artificial Neural Networks, Boosting methods and their ensembles. The study aim at improving the quality of Parkinson's disease treatment by tracking them and reinforce the viability of cost effective, regular and precise telemonitoring application. © 2012 Elsevier Ireland Ltd.","Artificial Neural Networks; Boosting; Haar wavelets transformation; Inference system; Multinomial logistic regression classifier; Parkinson's disease corrected T-tests; Ranker search; Statistical inference; Support Vector Machines","Boosting; Haar wavelets; Inference systems; Multinomial logistic regression; Ranker search; Statistical inference; T-tests; Bayesian networks; Logistics; Neural networks; Neurodegenerative diseases; Regression analysis; Support vector machines; Diagnosis; article; artificial neural network; cost effectiveness analysis; decision tree; diagnostic accuracy; disease severity; dysphonia; human; information processing; learning; learning algorithm; machine learning; maximum likelihood method; medical examination; medical expert; Parkinson disease; patient information; performance; prediction; priority journal; probability; robust inference system; support vector machine; telemonitoring; Algorithms; Artificial Intelligence; Case-Control Studies; Humans; Neural Networks (Computer); Parkinson Disease; Support Vector Machines; Telemedicine",Article,Scopus,2-s2.0-84876973345
"Betta G., Capriglione D., Corvino M., Liguori C., Paolillo A.","Face based recognition algorithms: A first step toward a metrological characterization",2013,"IEEE Transactions on Instrumentation and Measurement",19,10.1109/TIM.2013.2252856,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875978165&doi=10.1109%2fTIM.2013.2252856&partnerID=40&md5=b2e4c72a7b1ff41251440eab81b8fd6c","Face-based recognition systems have been increasingly used in many different applications in today's society, starting from surveillance and access control to the authentication for banking activities. Therefore, in the last few years an increasing interest in the performance characterization and improvement of such systems can be found in the scientific community. Most of the methodologies for testing the performance of such systems are based on the evaluation of recognition reliability indexes that are generally related to the probability of a false positive and/or of a false negative. © 1963-2012 IEEE.","Curve fitting; decision support systems; face recognition; image classification; measurement uncertainty","False negatives; Measurement uncertainty; Metrological characterization; Performance characterization; Recognition algorithm; Recognition systems; Reliability Index; Scientific community; Access control; Artificial intelligence; Curve fitting; Decision support systems; Image classification; Uncertainty analysis; Face recognition",Article,Scopus,2-s2.0-84875978165
"Zhang Y., Guo D., Li Z.","Common nature of learning between back-propagation and hopfield-type neural networks for generalized matrix inversion with simplified models",2013,"IEEE Transactions on Neural Networks and Learning Systems",19,10.1109/TNNLS.2013.2238555,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875885639&doi=10.1109%2fTNNLS.2013.2238555&partnerID=40&md5=30cad9c77844e80e742629ba45822d28","In this paper, two simple-structure neural networks based on the error back-propagation (BP) algorithm (i.e., BP-type neural networks, BPNNs) are proposed, developed, and investigated for online generalized matrix inversion. Specifically, the BPNN-L and BPNN-R models are proposed and investigated for the left and right generalized matrix inversion, respectively. In addition, for the same problem-solving task, two discrete-time Hopfield-type neural networks (HNNs) are developed and investigated in this paper. Similar to the classification of the presented BPNN-L and BPNN-R models, the presented HNN-L and HNN-R models correspond to the left and right generalized matrix inversion, respectively. Comparing the BPNN weight-updating formula with the HNN state-transition equation for the specific (i.e., left or right) generalized matrix inversion, we show that such two derived learning-expressions turn out to be the same (in mathematics), although the BP and Hopfield-type neural networks are evidently different from each other a great deal, in terms of network architecture, physical meaning, and training patterns. Numerical results with different illustrative examples further demonstrate the efficacy of the presented BPNNs and HNNs for online generalized matrix inversion and, more importantly, their common natures of learning. © 2012 IEEE.","Back-propagation-type neural networks (BPNN); common nature of learning; discrete-time; generalized matrix inversion; hopfield-type neural networks","Back-propagation-type neural networks (BPNN); common nature of learning; discrete-time; Generalized matrix inversions; Hopfield type neural networks; Equations of state; Network architecture; Neural networks; Backpropagation; artificial intelligence; artificial neural network; biological model; trends; Artificial Intelligence; Models, Neurological; Neural Networks (Computer)",Article,Scopus,2-s2.0-84875885639
"Tothova M., Hosovsky A.","Dynamic simulation model of pneumatic actuator with artificial muscle",2013,"SAMI 2013 - IEEE 11th International Symposium on Applied Machine Intelligence and Informatics, Proceedings",19,10.1109/SAMI.2013.6480994,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875612882&doi=10.1109%2fSAMI.2013.6480994&partnerID=40&md5=55c25de2e8b70e4ca9b3a9097b28c293","Antagonistic actuator with pneumatic artificial muscles (PAMs) is a kinematic structure that consists of two pneumatic muscles acting in opposition to each other, connected through the chain gear. The resulting position of the actuator can be determined by the angle of arm of the load attached to a shaft. Stiffness position of shaft in the given direction is determined by size of pressure of the relevant pneumatic muscle, which is loaded in tension and rotation (position) of shaft is proportional to pressure difference in individual muscle. The main drawback of this actuator is that its dynamic behavior is highly nonlinear. Due to this knowledge of the muscle properties it is necessary to use the model of the muscle. This muscle model of the system is based on modified Hill's basic model which consists of a variable damper and a variable spring connected in parallel. All the work on dynamic model was created in Matlab/Simulink environment. © 2013 IEEE.",,"Artificial muscle; Dynamic behaviors; Kinematic structures; MATLAB/Simulink environment; Muscle properties; Pneumatic artificial muscle; Pneumatic muscle; Pressure differences; Actuators; Artificial intelligence; Computer simulation; Information science; Pneumatic control; Pneumatic equipment; Muscle",Conference Paper,Scopus,2-s2.0-84875612882
"Lorencik D., Sincak P.","Cloud robotics: Current trends and possible use as a service",2013,"SAMI 2013 - IEEE 11th International Symposium on Applied Machine Intelligence and Informatics, Proceedings",19,10.1109/SAMI.2013.6480950,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875592105&doi=10.1109%2fSAMI.2013.6480950&partnerID=40&md5=bf4fcae155cf8374b1ec3f4276ed23f7","Since the cloud computing became widely available, lots of previously algorithms and systems previously thought of as very time consuming became instantly viable. For robotics and AI especially this means that if the power behind the cloud could be harnessed, it would be possible to build smaller, more battery effective robots because there would be no need to have a powerful computer on board, but the brain of the robot can be in the cloud. The idea of remote brain is not a new one, though. Nevertheless, it is possible to create one now. Centralised cloud for robot means that the memory can be nearly infinite, and instantly available to other robots, so the process of learning and exchanging the knowledge can be simplified. Also, this approach will allow for easy change and upgrade of the methods used regardless of robot hardware. © 2013 IEEE.",,"Centralised; Cloud robotics; Process of learning; Robot hardware; Artificial intelligence; Information science; Knowledge management; Robotics; Robots",Conference Paper,Scopus,2-s2.0-84875592105
"De Souza Gomes A., Costa M.A., Defaria T.G.A., Caminhas W.M.","Detection and classification of faults in power transmission lines using functional analysis and computational intelligence",2013,"IEEE Transactions on Power Delivery",19,10.1109/TPWRD.2013.2251752,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880314710&doi=10.1109%2fTPWRD.2013.2251752&partnerID=40&md5=da6545982f01ef053b192659f38e4c5e","The transmission line is the most vulnerable element of any electrical power system due to its large physical dimension. As a consequence, many fault diagnosis algorithms have been proposed in the literature. In general, most proposals use signal-processing analysis and computational intelligence. In this paper, a new model to functionally represent the phases of a transmission line is proposed. The detection and classification strategy are developed from the analysis of the model's parameters and were evaluated using a set of simulated faults and a real database. The results show that the proposed model detects faults very quickly, using a vastly simplified mathematical process, and is able to classify faults accurately. © 1986-2012 IEEE.","Detection and classification of faults; power transmission lines","Diagnosis algorithms; Electrical power system; Physical dimensions; Real database; Artificial intelligence; Computer simulation; Electric power transmission; Power transmission; Signal processing; Transmission line theory; Electric lines",Article,Scopus,2-s2.0-84880314710
"Toninho B., Caires L., Pfenning F.","Higher-order processes, functions, and sessions: A monadic integration",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",19,10.1007/978-3-642-37036-6_20,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874414870&doi=10.1007%2f978-3-642-37036-6_20&partnerID=40&md5=8f8b348cfac889762c8d244c4a9dcddf","In prior research we have developed a Curry-Howard interpretation of linear sequent calculus as session-typed processes. In this paper we uniformly integrate this computational interpretation in a functional language via a linear contextual monad that isolates session-based concurrency. Monadic values are open process expressions and are first class objects in the language, thus providing a logical foundation for higher-order session typed processes. We illustrate how the combined use of the monad and recursive types allows us to cleanly write a rich variety of concurrent programs, including higher-order programs that communicate processes. We show the standard metatheoretic result of type preservation, as well as a global progress theorem, which to the best of our knowledge, is new in the higher-order session typed setting. © 2013 Springer-Verlag.",,"Class objects; Computational interpretations; Concurrent program; Curry-Howard; Functional languages; Higher-order; Higher-order process; Higher-order programs; Logical foundations; Open process; Recursive types; Sequent calculus; Artificial intelligence; Differentiation (calculus)",Conference Paper,Scopus,2-s2.0-84874414870
"Ittoo A., Bouma G.","Minimally-supervised extraction of domain-specific part-whole relations using Wikipedia as knowledge-base",2013,"Data and Knowledge Engineering",19,10.1016/j.datak.2012.06.004,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875252029&doi=10.1016%2fj.datak.2012.06.004&partnerID=40&md5=18c18f4ae865fd725c4c6ac8c1f14f70","We present a minimally-supervised approach for learning part-whole relations from texts. Unlike previous techniques, we focused on sparse, domain-specific texts. The novelty in our approach lies in the use of Wikipedia as a knowledge-base, from which we first acquire a set of reliable patterns that express part-whole relations. This is achieved by a minimally-supervised algorithm. We then use the patterns acquired to extract part-whole relation triples from a collection of sparse, domain-specific texts. Our strategy, of learning in one domain and applying the knowledge in another domain is based upon the notion of domain-adaption. It allows us to overcome the challenges of learning the relations directly from the sparse, domain-specific corpus. Our experimental evaluations reveal that, despite its general-purpose nature, Wikipedia can be exploited as a source of knowledge for improving the performance of domain-specific part-whole relation extraction. As our other contributions, we propose a mechanism that mitigates the negative impact of semantic-drift on minimally-supervised algorithms. Also, we represent the patterns in the extracted relations using sophisticated syntactic structures that avoid the limitations of traditional surface string representations. In addition, we show that domain-specific part-whole relations cannot be conclusively classified in existing taxonomies. © 2012 Elsevier B.V. All rights reserved.","Knowledge management applications; Ontology learning; Question-answering systems; Relation extraction; Text mining","Knowledge management applications; Ontology learning; Question answering systems; Relation extraction; Text mining; Artificial intelligence; Data mining; Extraction; Knowledge management; Semantics; Websites; Natural language processing systems",Conference Paper,Scopus,2-s2.0-84875252029
"Wood D.J., Carlsson L., Eklund M., Norinder U., Stålring J.","QSAR with experimental and predictive distributions: An information theoretic approach for assessing model quality",2013,"Journal of Computer-Aided Molecular Design",19,10.1007/s10822-013-9639-5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880697592&doi=10.1007%2fs10822-013-9639-5&partnerID=40&md5=397ba806f811bab5af4be1637db0d3b9","We propose that quantitative structure-activity relationship (QSAR) predictions should be explicitly represented as predictive (probability) distributions. If both predictions and experimental measurements are treated as probability distributions, the quality of a set of predictive distributions output by a model can be assessed with Kullback-Leibler (KL) divergence: a widely used information theoretic measure of the distance between two probability distributions. We have assessed a range of different machine learning algorithms and error estimation methods for producing predictive distributions with an analysis against three of AstraZeneca's global DMPK datasets. Using the KL-divergence framework, we have identified a few combinations of algorithms that produce accurate and valid compound-specific predictive distributions. These methods use reliability indices to assign predictive distributions to the predictions output by QSAR models so that reliable predictions have tight distributions and vice versa. Finally we show how valid predictive distributions can be used to estimate the probability that a test compound has properties that hit single- or multi- objective target profiles. © 2013 The Author(s).","Applicability domain; Kullback-Leibler divergence; Prediction confidence; Prediction errors; Predictive distributions; QSAR; Quantitative structure-activity relationships","accuracy; article; learning algorithm; mathematical computing; mathematical model; measurement error; prediction; priority journal; probability; quality control; quantitative structure activity relation; statistical distribution; Algorithms; Artificial Intelligence; Humans; Models, Biological; Probability; Quantitative Structure-Activity Relationship",Article,Scopus,2-s2.0-84880697592
"Bagci U., Yao J., Miller-Jaster K., Chen X., Mollura D.J.","Predicting Future Morphological Changes of Lesions from Radiotracer Uptake in 18F-FDG-PET Images",2013,"PLoS ONE",19,10.1371/journal.pone.0057105,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874205032&doi=10.1371%2fjournal.pone.0057105&partnerID=40&md5=97b603a617a91d494527eb7d44519536","We introduce a novel computational framework to enable automated identification of texture and shape features of lesions on 18F-FDG-PET images through a graph-based image segmentation method. The proposed framework predicts future morphological changes of lesions with high accuracy. The presented methodology has several benefits over conventional qualitative and semi-quantitative methods, due to its fully quantitative nature and high accuracy in each step of (i) detection, (ii) segmentation, and (iii) feature extraction. To evaluate our proposed computational framework, thirty patients received 2 18F-FDG-PET scans (60 scans total), at two different time points. Metastatic papillary renal cell carcinoma, cerebellar hemongioblastoma, non-small cell lung cancer, neurofibroma, lymphomatoid granulomatosis, lung neoplasm, neuroendocrine tumor, soft tissue thoracic mass, nonnecrotizing granulomatous inflammation, renal cell carcinoma with papillary and cystic features, diffuse large B-cell lymphoma, metastatic alveolar soft part sarcoma, and small cell lung cancer were included in this analysis. The radiotracer accumulation in patients' scans was automatically detected and segmented by the proposed segmentation algorithm. Delineated regions were used to extract shape and textural features, with the proposed adaptive feature extraction framework, as well as standardized uptake values (SUV) of uptake regions, to conduct a broad quantitative analysis. Evaluation of segmentation results indicates that our proposed segmentation algorithm has a mean dice similarity coefficient of 85.75±1.75%. We found that 28 of 68 extracted imaging features were correlated well with SUVmax (p&lt;0.05), and some of the textural features (such as entropy and maximum probability) were superior in predicting morphological changes of radiotracer uptake regions longitudinally, compared to single intensity feature such as SUVmax. We also found that integrating textural features with SUV measurements significantly improves the prediction accuracy of morphological changes (Spearman correlation coefficient = 0.8715, p&lt;2e-16).",,"fluorodeoxyglucose f 18; adult; aged; alveolar soft part sarcoma; article; clinical article; controlled study; drug uptake; female; granulomatous inflammation; hemangioblastoma; human; image analysis; kidney carcinoma; large cell lymphoma; lung non small cell cancer; lung small cell cancer; lymphomatoid granulomatosis; male; mathematical computing; morphology; neuroendocrine tumor; neurofibroma; positron emission tomography; prediction; Adult; Aged; Algorithms; Artificial Intelligence; Cluster Analysis; Female; Fluorodeoxyglucose F18; Humans; Image Processing, Computer-Assisted; Male; Markov Chains; Middle Aged; Neoplasms; Positron-Emission Tomography and Computed Tomography; Radiopharmaceuticals; Software; Tissue Distribution",Article,Scopus,2-s2.0-84874205032
"Aksu B., Paradkar A., De Matas M., Özer Ö., Güneri T., York P.","A quality by design approach using artificial intelligence techniques to control the critical quality attributes of ramipril tablets manufactured by wet granulation",2013,"Pharmaceutical Development and Technology",19,10.3109/10837450.2012.705294,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870894160&doi=10.3109%2f10837450.2012.705294&partnerID=40&md5=8208e1db0b18dab94d4e9448981c7fdb","Quality by design (QbD) is an essential part of the modern approach to pharmaceutical quality. This study was conducted in the framework of a QbD project involving ramipril tablets. Preliminary work included identification of the critical quality attributes (CQAs) and critical process parameters (CPPs) based on the quality target product profiles (QTPPs) using the historical data and risk assessment method failure mode and effect analysis (FMEA). Compendial and in-house specifications were selected as QTPPs for ramipril tablets. CPPs that affected the product and process were used to establish an experimental design. The results thus obtained can be used to facilitate definition of the design space using tools such as design of experiments (DoE), the response surface method (RSM) and artificial neural networks (ANNs). The project was aimed at discovering hidden knowledge associated with the manufacture of ramipril tablets using a range of artificial intelligence-based software, with the intention of establishing a multi-dimensional design space that ensures consistent product quality. At the end of the study, a design space was developed based on the study data and specifications, and a new formulation was optimized. On the basis of this formulation, a new laboratory batch formulation was prepared and tested. It was confirmed that the explored formulation was within the design space. © 2013 Informa Healthcare USA, Inc.","Artificial neural networks (ANNs); Critical quality attributes (CQAs); Optimization; Quality by design (QbD); Ramipril; Wet granulation","ramipril; article; artificial intelligence; artificial neural network; drug design; drug granulation; drug manufacture; drug quality; priority journal; response surface method; tablet formulation; Angiotensin-Converting Enzyme Inhibitors; Drug Compounding; Neural Networks (Computer); Quality Control; Ramipril; Risk Assessment; Software; Tablets",Article,Scopus,2-s2.0-84870894160
"Schaub N.P., Alimchandani M., Quezado M., Kalina P., Eberhardt J.S., Hughes M.S., Beresnev T., Hassan R., Bartlett D.L., Libutti S.K., Pingpank J.F., Royal R.E., Kammula U.S., Pandalai P., Phan G.Q., Stojadinovic A., Rudloff U., Alexander H.R., Avital I.","A novel nomogram for peritoneal mesothelioma predicts survival",2013,"Annals of Surgical Oncology",19,10.1245/s10434-012-2651-5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878859983&doi=10.1245%2fs10434-012-2651-5&partnerID=40&md5=b8be5f2611b6cb6d65ba5f3100f7e7aa","Background: Malignant peritoneal mesothelioma (MPM) is a rare disease treated with cytoreductive surgery (CRS) and hyperthermic intraperitoneal chemotherapy (HIPEC). Estimation of personalized survival times can potentially guide treatment and surveillance. Methods: We analyzed 104 patients who underwent CRS and cisplatin-based HIPEC for MPM. By means of 25 demographic, laboratory, operative, and histopathological variables, we developed a novel nomogram using machine-learned Bayesian belief networks with stepwise training, testing, and cross-validation. Results: The mean peritoneal carcinomatosis index (PCI) was 15, and 66 % of patients had a completeness of cytoreduction (CC) score of 0 or 1. Eighty-seven percent of patients had epithelioid histology. The median follow-up time was 49 (1-195) months. The 3- and 5-year overall survivals (OS) were 58 and 46 %, respectively. The histological subtype, pre-CRS PCI, and preoperative serum CA-125 had the greatest impact on OS and were included in the nomogram. The mean areas under the receiver operating characteristic curve for the 10-fold cross-validation of the 3- and 5-year models were 0.77 and 0.74, respectively. The graphical calculator or nomogram uses color coding to assist the clinician in quickly estimating individualized patient-specific survival before surgery. Conclusions: Machine-learned Bayesian belief network analysis generated a novel nomogram predicting 3- and 5-year OS in patients treated with CRS and HIPEC for MPM. Pre-CRS estimation of survival times may potentially individualize patient care by influencing the use of systemic therapy and frequency of diagnostic imaging, and might prevent CRS in patients unlikely to achieve favorable outcomes despite surgical intervention. © 2012 Society of Surgical Oncology.",,"CA 125 antigen; cisplatin; fluorouracil; paclitaxel; adolescent; adult; article; Bayesian learning; cancer combination chemotherapy; cancer patient; cancer prognosis; cancer survival; carcinomatous peritonitis; cytoreductive surgery; female; follow up; histopathology; human; human tissue; hyperthermic intraperitoneal chemotherapy; hyperthermic therapy; major clinical study; male; nomogram; overall survival; peritoneum mesothelioma; predictive value; progression free survival; protein blood level; receiver operating characteristic; retrospective study; scoring system; single drug dose; statistical model; survival time; Adolescent; Adult; Aged; Antineoplastic Combined Chemotherapy Protocols; Artificial Intelligence; Bayes Theorem; Chemotherapy, Cancer, Regional Perfusion; Cisplatin; Combined Modality Therapy; Female; Fluorouracil; Follow-Up Studies; Humans; Hyperthermia, Induced; Male; Mesothelioma; Middle Aged; Neoplasm Staging; Nomograms; Paclitaxel; Peritoneal Neoplasms; Prognosis; Survival Rate; Young Adult",Article,Scopus,2-s2.0-84878859983
"Kang F., Li J., Ma Z.","An artificial bee colony algorithm for locating the critical slip surface in slope stability analysis",2013,"Engineering Optimization",19,10.1080/0305215X.2012.665451,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872871115&doi=10.1080%2f0305215X.2012.665451&partnerID=40&md5=4eaf9e03147b1e18f738d7af7b0728a3","Determination of the critical slip surface with the minimum factor of safety of a slope is a difficult constrained global optimization problem. In this article, an artificial bee colony algorithm with a multi-slice adjustment method is proposed for locating the critical slip surfaces of soil slopes, and the Spencer method is employed to calculate the factor of safety. Six benchmark examples are presented to illustrate the reliability and efficiency of the proposed technique, and it is also compared with some well-known or recent algorithms for the problem. The results show that the new algorithm is promising in terms of accuracy and efficiency. © 2013 Copyright Taylor and Francis Group, LLC.","artificial bee colony algorithm; factor of safety; non-circular failure surface; slope stability; swarm intelligence","Adjustment method; Artificial bee colony algorithms; Constrained global optimization; Critical slip surface; Factor of safety; Failure surface; Multi slices; Slope stability analysis; Soil slopes; Swarm Intelligence; Algorithms; Artificial intelligence; Slope stability; Safety factor",Article,Scopus,2-s2.0-84872871115
"Bhalla D., Bansal R.K., Gupta H.O.","Integrating AI based DGA fault diagnosis using Dempster-Shafer Theory",2013,"International Journal of Electrical Power and Energy Systems",19,10.1016/j.ijepes.2012.11.018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872123454&doi=10.1016%2fj.ijepes.2012.11.018&partnerID=40&md5=5337ed41c871f4cd7b91a3adf7047d81","Conventional dissolved gas analysis (DGA) methods and artificial intelligence (AI) techniques based on DGA data have been used for long to diagnose incipient faults in transformers. The Dempster-Shafer Evidential Theory (DST) has been applied to various AI oriented applications where there is uncertainty and conflict. This paper uses DST to integrate the results of incipient fault diagnosis of back propagation neural networks (BP-NN) and fuzzy logic, so as to overcome any conflicts in the type of fault diagnosed. The proposed approach is applied to independent data of different transformers and case studies of historic data of transformer units. This method has been successfully used to identify the type of fault developing within a transformer even if there is conflict in the results of AI techniques applied to DGA data. © 2012 Elsevier Ltd. All rights reserved.","Artificial intelligence; Dempster-Shafer Evidential Theory; Dissolved gas analysis; Transformer fault diagnosis","AI techniques; Back propagation neural networks; Dempster-Shafer evidential theory; Dempster-Shafer theory; Dissolved gas analysis; Historic data; Incipient fault diagnosis; Incipient faults; Transformer fault diagnosis; Artificial intelligence; Neural networks; Fuzzy logic",Article,Scopus,2-s2.0-84872123454
"Mateos C., Pacini E., Garino C.G.","An ACO-inspired algorithm for minimizing weighted flowtime in cloud-based parameter sweep experiments",2013,"Advances in Engineering Software",19,10.1016/j.advengsoft.2012.11.011,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870830246&doi=10.1016%2fj.advengsoft.2012.11.011&partnerID=40&md5=04ba3fe8c5d29817b43bcdaa21e7d7b2","Parameter Sweep Experiments (PSEs) allow scientists and engineers to conduct experiments by running the same program code against different input data. This usually results in many jobs with high computational requirements. Thus, distributed environments, particularly Clouds, can be employed to fulfill these demands. However, job scheduling is challenging as it is an NP-complete problem. Recently, Cloud schedulers based on bio-inspired techniques-which work well in approximating problems with little input information-have been proposed. Unfortunately, existing proposals ignore job priorities, which is a very important aspect in PSEs since it allows accelerating PSE results processing and visualization in scientific Clouds. We present a new Cloud scheduler based on Ant Colony Optimization, the most popular bio-inspired technique, which also exploits well-known notions from operating systems theory. Simulated experiments performed with real PSE job data and other Cloud scheduling policies indicate that our proposal allows for a more agile job handling while reducing PSE completion time. © 2012 Elsevier Ltd. All rights reserved.","Ant Colony Optimization; Cloud Computing; Job scheduling; Parameter sweep experiments; Swarm Intelligence; Weighted flowtime","Artificial intelligence; Cloud computing; Computational complexity; Experiments; Scheduling; Ant Colony Optimization (ACO); Bio-inspired; Bio-inspired techniques; Completion time; Computational requirements; Distributed environments; Input datas; Job scheduling; Program code; Results processing; Scheduling policies; Scientists and engineers; Simulated experiments; Swarm Intelligence; Weighted flowtime; Algorithms",Article,Scopus,2-s2.0-84870830246
"Arnaout J.-P.","Ant colony optimization algorithm for the Euclidean location-allocation problem with unknown number of facilities",2013,"Journal of Intelligent Manufacturing",19,10.1007/s10845-011-0536-2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027925803&doi=10.1007%2fs10845-011-0536-2&partnerID=40&md5=4945ceb1625ef05ed90f2e179e7b15f6","This paper addresses the Euclidean location-allocation problem with an unknown number of facilities, and an objective of minimizing the fixed and transportation costs. This is a NP-hard problem and in this paper, a three-stage ant colony optimization (ACO) algorithm is introduced and its performance is evaluated by comparing its solutions to the solutions of genetic algorithms (GA). The results show that ACO outperformed GA and reached better solutions in a faster computational time. Furthermore, ACO was tested on the relaxed version of the problem where the number of facilities is known, and compared to existing methods in the literature. The results again confirmed the superiority of the proposed algorithm. © 2011 Springer Science+Business Media, LLC.","Ant colony optimization; Design of experiments; Euclidean location-allocation Problem","Computational complexity; Design of experiments; Genetic algorithms; Ant Colony Optimization (ACO); Ant Colony Optimization algorithms; Computational time; Euclidean; Location allocation problem; Transportation cost; Artificial intelligence",Article,Scopus,2-s2.0-85027925803
"Sarno R., Sari P.L.I., Ginardi H., Sunaryono D., Mukhlash I.","Decision mining for multi choice workflow patterns",2013,"Proceeding - 2013 International Conference on Computer, Control, Informatics and Its Applications: ""Recent Challenges in Computer, Control and Informatics"", IC3INA 2013",19,10.1109/IC3INA.2013.6819197,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902438683&doi=10.1109%2fIC3INA.2013.6819197&partnerID=40&md5=0e1de56e511cb5088f9a3db8ca8bcc53","Decision mining is combination of process mining and machine learning technique to retrieve information about how an attribute in a business process affects a case's route choice. It identifies decision point by looking for XOR-splits in petri-net workflow model and analyzing rules for each choice based on available attributes using decision tree. Problem emerges when decision mining technique is used on a workflow that does not use either XOR or AND splits, for example OR-split gateway logic. OR-split does not have explicit representation in petri nets and it makes decision mining algorithm cannot find its decision point. Workflow pattern that uses OR-split as its splitting logic is multi choice. Multi choice does not have its own explicit representation in form of petri net and it is problematic to apply decision mining to this workflow pattern. To make multi choice can be analyzed by decision miner some modification needs to be applied to the petri net representation of this pattern. This paper proposes modification of OR-split gateway representation in petri net. The new representation of OR-split uses combination the existing XOR-split and AND-split to make the model easier to be analyzed using decision miner. The proposed modification do not affect the conformance of event log and process model, but will allow each choice branch to be checked by decision miner. © 2013 IEEE.","decision mining; decision tree; multi choice; process mining; workflow pattern","Algorithms; Artificial intelligence; Decision trees; Information science; Learning systems; Miners; Petri nets; Explicit representation; Machine learning techniques; Mining techniques; Multi choices; Petri net representations; Process mining; Workflow modeling; Workflow patterns; Data mining",Conference Paper,Scopus,2-s2.0-84902438683
"Shu X., Tao P., Li X., Yu Y.","Helium diffusion in tungsten: A molecular dynamics study",2013,"Nuclear Instruments and Methods in Physics Research, Section B: Beam Interactions with Materials and Atoms",19,10.1016/j.nimb.2012.10.028,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884813216&doi=10.1016%2fj.nimb.2012.10.028&partnerID=40&md5=8306c278b8be0e83f29d16b76d46dcde","The diffusion process of He inWis simulated by a molecular dynamics (MD) method with self-developed W-H-He analytic bond-order potentials. The mean squared displacement (MSD) method is employed to determine diffusivity at different temperatures, and then the diffusivity-temperature (D-T) relationship is fitted by the Arrhenius equation to obtain the prefactor and the diffusion barrier. We show the diffusivity of He and the corresponding prefactor and diffusion barrier are different at different temperature ranges. The diffusivities are 9.50 × 10-9 exp(-0.021 eV/kT) m2/s in the temperature range of 50-300 K, 3.61 × 10-8 exp(-0.057 eV/kT) m2/s in 300-1500 K, and 8.562 × 10-8 exp(-0.157 eV/kT) m2/s in 1500-3000 K, respectively, which correspond to different diffusion mechanisms. At a lower temperature, He diffuses directly from one tetrahedral interstitial site (TIS) to another TIS through the diagonal interstitial site with a lower diffusion barrier, while it can diffuse from one TIS to other TIS through the octahedral interstitial site with higher barrier at higher temperature. © 2012 Elsevier B.V. All rights reserved.","Diffusion; Helium; Molecular dynamics; Tungsten","Artificial intelligence; Diffusion; Helium; Molecular dynamics; Tungsten; Arrhenius equation; Bond-order potential; Diffusion mechanisms; Interstitial sites; Lower temperatures; Mean squared displacement; Molecular dynamics methods; Octahedral interstitial site; Diffusion barriers",Article,Scopus,2-s2.0-84884813216
"Koay K.L., Lakatos G., Syrdal D.S., Gácsi M., Bereczky B., Dautenhahn K., Miklósi A., Walters M.L.","Hey! There is someone at your door. A hearing robot using visual communication signals of hearing dogs to communicate intent",2013,"IEEE Symposium on Artificial Life (ALIFE)",19,10.1109/ALIFE.2013.6602436,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929366005&doi=10.1109%2fALIFE.2013.6602436&partnerID=40&md5=223083b3f1f0d33060ac45dd05705110","This paper presents a study of the readability of doginspired visual communication signals in a human-robot interaction scenario. This study was motivated by specially trained hearing dogs which provide assistance to their deaf owners by using visual communication signals to lead them to the sound source. For our human-robot interaction scenario, a robot was used in place of a hearing dog to lead participants to two different sound sources. The robot was preprogrammed with dog-inspired behaviors, controlled by a wizard who directly implemented the dog behavioral strategy on the robot during the trial. By using dog-inspired visual communication signals as a means of communication, the robot was able to lead participants to the sound sources (the microwave door, the front door). Findings indicate that untrained participants could correctly interpret the robot's intentions. Head movements and gaze directions were important for communicating the robot's intention using visual communication signals. © 2013 IEEE.","Human-robot interaction; Robot behaviour; Robotic home companion; Social robot","Acoustic generators; Artificial intelligence; Audition; Man machine systems; Robots; Visual communication; Communication signals; Gaze direction; Head movements; Social robots; Sound source; Human robot interaction",Conference Paper,Scopus,2-s2.0-84929366005
"Lambiase F., Di Ilio A.","Optimization of the clinching tools by means of integrated fe modeling and artificial intelligence techniques",2013,"Procedia CIRP",19,10.1016/j.procir.2013.09.029,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886772208&doi=10.1016%2fj.procir.2013.09.029&partnerID=40&md5=7f56ebc048efa9704ce2dd19b8602df6","In the present work, an optimization of the clinching tools involving extensible dies is performed to increase the clinched joints strength. The clinched joint strength is influenced by the lock parameters, which in turn depend on the clinching tool geometry. A finite element model is developed to predict the effect of the clinching tool geometry on lock parameters and recursively optimize the tool geometry. In order to reduce the number of FE simulation runs, an artificial Neural Network (ANN) model is utilized to predict the behavior of clinched joints produced with a given clinching tools configuration. The ANN is trained and validated by using the results of the finite element model produced under different clinching tools configurations. Finally, an optimization tool based on a Genetic Algorithm tool was developed to demonstrate the effectiveness of the proposed approach.","Forming, clinching, metal sheet; Joining; Mechanichal fastening","Deep neural networks; Genetic algorithms; Geometry; Joining; Locks (fasteners); Manufacture; Neural networks; Sheet metal; Artificial intelligence techniques; Artificial neural network models; FE model; FE-simulation; Joint strength; Mechanichal fastening; Optimization tools; Tool geometry; Finite element method",Conference Paper,Scopus,2-s2.0-84886772208
"Rada-Vilela J., Chica M., Cordón O., Damas S.","A comparative study of multi-objective ant colony optimization algorithms for the time and space assembly line balancing problem",2013,"Applied Soft Computing Journal",19,10.1016/j.asoc.2013.06.014,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885589203&doi=10.1016%2fj.asoc.2013.06.014&partnerID=40&md5=01778cd0ed10e7607dfb70116ca83cca","Assembly lines for mass manufacturing incrementally build production items by performing tasks on them while flowing between workstations. The configuration of an assembly line consists of assigning tasks to different workstations in order to optimize its operation subject to certain constraints such as the precedence relationships between the tasks. The operation of an assembly line can be optimized by minimizing two conflicting objectives, namely the number of workstations and the physical area these require. This configuration problem is an instance of the TSALBP, which is commonly found in the automotive industry. It is a hard combinatorial optimization problem to which finding the optimum solution might be infeasible or even impossible, but finding a good solution is still of great value to managers configuring the line. We adapt eight different Multi-Objective Ant Colony Optimization (MOACO) algorithms and compare their performance on ten well-known problem instances to solve such a complex problem. Experiments under different modalities show that the commonly used heuristic functions deteriorate the performance of the algorithms in time-limited scenarios due to the added computational cost. Moreover, even neglecting such a cost, the algorithms achieve a better performance without such heuristic functions. The algorithms are ranked according to three multi-objective indicators and the differences between the top-4 are further reviewed using statistical significance tests. Additionally, these four best performing MOACO algorithms are favourably compared with the Infeasibility Driven Evolutionary Algorithm (IDEA) designed specifically for industrial optimization problems. © 2013 Elsevier B.V.","Ant Colony Optimization; Automotive industry; Multi-Objective Optimization; Problem; Time and Space Assembly Line Balancing","Ant colony optimization; Artificial intelligence; Assembly; Assembly machines; Automotive industry; Combinatorial optimization; Evolutionary algorithms; Heuristic algorithms; Multiobjective optimization; Problem solving; Production; Ant Colony Optimization algorithms; Assembly line balancing; Combinatorial optimization problems; Industrial optimization; Precedence relationships; Problem; Statistical significance test; Time and space assembly line balancing problems; Optimization",Article,Scopus,2-s2.0-84885589203
"Muñoz E., Capón-García E., Laínez J.M., Espuña A., Puigjaner L.","Considering environmental assessment in an ontological framework for enterprise sustainability",2013,"Journal of Cleaner Production",19,10.1016/j.jclepro.2012.11.032,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879910068&doi=10.1016%2fj.jclepro.2012.11.032&partnerID=40&md5=c78865943677b9cc7ba48abf32248ca0","Enterprises comprise several functions, such as production, marketing, sales, human resources, logistics, safety and environment, which interact with each other. As a result, decision-making becomes highly challenging in the alignment of decisions to support the success of business goals. Specifically, environmental management is closely related to several levels in the enterprise structure, since they share a large amount of data and information. Hence, effective integration of environmental issues by means of tools improving information sharing and communication, may play a crucial role for the enhanced enterprise operation from an environmental perspective. In this sense, knowledge management technologies have proved to be highly promising for supporting this integration task. In this work, an ontological framework is developed as the technology for information and knowledge models sharing for the environmental assessment of the enterprise. The ontological model is applied to a case study considering a supply chain network design-planning and a process scheduling problem. The ontology provides an enterprise decision-making supporting tool by combining different information systems, which adapts and recognizes the different elements associated with the enterprise functions, and facilitates assessing the environmental performance of enterprises. © 2012 Elsevier Ltd. All rights reserved.","Decision support systems; Enterprise model; Environmental considerations; Ontology framework","Artificial intelligence; Decision making; Decision support systems; Environmental impact; Environmental management; Environmental technology; Knowledge management; Ontology; Supply chains; Sustainable development; Enterprise decision-making; Enterprise modeling; Environmental assessment; Environmental considerations; Environmental performance; Knowledge management technology; Process-scheduling problem; Supply chain network design; Information management",Article,Scopus,2-s2.0-84879910068
"Pereira F.C., Rodrigues F., Ben-Akiva M.","Text analysis in incident duration prediction",2013,"Transportation Research Part C: Emerging Technologies",19,10.1016/j.trc.2013.10.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887588622&doi=10.1016%2fj.trc.2013.10.002&partnerID=40&md5=dd6aea927f48cdfe03ffbc35b89a0e85","Due to the heterogeneous case-by-case nature of traffic incidents, plenty of relevant information is recorded in free flow text fields instead of constrained value fields. As a result, such text components enclose considerable richness that is invaluable for incident analysis, modeling and prediction. However, the difficulty to formally interpret such data has led to minimal consideration in previous work.In this paper, we focus on the task of incident duration prediction, more specifically on predicting clearance time, the period between incident reporting and road clearance. An accurate prediction will help traffic operators implement appropriate mitigation measures and better inform drivers about expected road blockage time.The key contribution is the introduction of topic modeling, a text analysis technique, as a tool for extracting information from incident reports in real time. We analyze a dataset of 2. years of accident cases and develop a machine learning based duration prediction model that integrates textual with non-textual features. To demonstrate the value of the approach, we compare predictions with and without text analysis using several different prediction models. Models using textual features consistently outperform the others in nearly all circumstances, presenting errors up to 28% lower than models without such information. © 2013 Elsevier Ltd.","Incident duration prediction; Regression models; Text analysis; Topic modeling","Data mining; Learning systems; Regression analysis; Roads and streets; Artificial intelligence; Duration predictions; Extracting information; Incident duration; Mitigation measures; Modeling and predictions; Regression model; Text analysis; Topic Modeling; Accurate prediction; Forecasting; learning; numerical model; real time; regression analysis; transportation planning; accuracy assessment; algorithm; data set; prediction; traffic management",Article,Scopus,2-s2.0-84887588622
"Bouchard G., Guo S., Yin D.","Convex collective matrix factorization",2013,"Journal of Machine Learning Research",19,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926472391&partnerID=40&md5=04bfd3e67e2a693bc7ac1d86fc7df404","In many applications, multiple interlinked sources of data are available and they cannot be represented by a single adjacency matrix, to which large scale factorization method could be applied. Collective matrix factor- ization is a simple yet powerful approach to jointly factorize multiple matrices, each of which represents a relation between two entity types. Existing algorithms to estimate parameters of collective matrix factorization models are based on non-convex formulations of the problem; in this paper, a convex formulation of this approach is proposed. This enables the derivation of large scale algorithms to estimate the parameters, including an iterative eigenvalue thresholding algorithm. Numerical experiments illustrate the benefits of this new approach. Copyright 2013 by the authors.",,"Artificial intelligence; Eigenvalues and eigenfunctions; Factorization; Iterative methods; Parameter estimation; Adjacency matrices; Collective matrix factorizations; Factorization methods; Large-scale algorithms; Multiple matrices; New approaches; Numerical experiments; Thresholding algorithms; Matrix algebra",Conference Paper,Scopus,2-s2.0-84926472391
"Agrawal S., Goyal N.","Further optimal regret bounds for thompson sampling",2013,"Journal of Machine Learning Research",19,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898938874&partnerID=40&md5=3d8ea210023c4fb38d10f18d80b342a3","Thompson Sampling is one of the oldest heuristics for multi-armed bandit problems. It is a randomized algorithm based on Bayesian ideas, and has recently generated significant interest after several studies demonstrated it to have comparable or better empirical performance compared to the state of the art methods. In this paper, we provide a novel regret analysis for Thompson Sampling that proves the first near-optimal problem-independent bound of O(√NT ln T) on the expected regret of this algorithm. Our novel martingale-based analysis techniques are conceptually simple, and easily extend to distributions other than the Beta distribution. For the version of Thompson Sampling that uses Gaussian priors, we prove a problem-independent bound of O(√NT lnN) on the expected regret, and demonstrate the optimality of this bound by providing a matching lower bound. This lower bound of Ω(√NT lnN) is the first lower bound on the performance of a natural version of Thompson Sampling that is away from the general lower bound of O(√NT) for the multi-armed bandit problem. Our near-optimal problem-independent bounds for Thompson Sampling solve a COLT 2012 open problem of Chapelle and Li. Additionally, our techniques simultaneously provide the optimal problem-dependent bound of (1+ε)Σiln T/d(μi,μ1) +O(Nε2 ) on the expected regret. The optimal problem-dependent regret bound for this problem was first proven recently by Kaufmann et al. [2012b]. Copyright 2013 by the authors.",,"Algorithms; Artificial intelligence; Probability; Statistics; Analysis techniques; Beta distributions; Empirical performance; Multi-armed bandit problem; Optimal problems; Randomized Algorithms; State-of-the-art methods; Thompson samplings; Optimization",Conference Paper,Scopus,2-s2.0-84898938874
"Cheng B., Wang Q., Yang S., Hu X.","An improved ant colony optimization for scheduling identical parallel batching machines with arbitrary job sizes",2013,"Applied Soft Computing Journal",19,10.1016/j.asoc.2012.10.021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881646789&doi=10.1016%2fj.asoc.2012.10.021&partnerID=40&md5=d950993ff656e6d0080ef40ba3d0b896","In this paper we consider the problem of scheduling parallel batching machines with jobs of arbitrary sizes. The machines have identical capacity of size and processing velocity. The jobs are processed in batches given that the total size of jobs in a batch cannot exceed the machine capacity. Once a batch starts processing, no interruption is allowed until all the jobs are completed. First we present a mixed integer programming model of the problem. We show the computational complexity of the problem and optimality properties. Then we propose a novel ant colony optimization method where the Metropolis Criterion is used to select the paths of ants to overcome the immature convergence. Finally, we generate different scales of instances to test the performance. The computational results show the effectiveness of the algorithm, especially for large-scale instances. Copyright © 2012 Published by Elsevier B.V. All rights reserved.","Ant colony optimization; Batching machines, Arbitrary job sizes; Scheduling","Ant colony optimization; Artificial intelligence; Integer programming; Scheduling; Ant colony optimization methods; Batching machine; Computational results; Identical parallel batching machines; Improved ant colony optimization; Job size; Metropolis criterion; Mixed integer programming model; Scheduling algorithms",Article,Scopus,2-s2.0-84881646789
"Eavani H., Satterthwaite T.D., Gur R.E., Gur R.C., Davatzikos C.","Unsupervised learning of functional network dynamics in resting state fMRI.",2013,"Information processing in medical imaging : proceedings of the ... conference",19,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901266629&partnerID=40&md5=10ce1b43957d8aba1d0a59de9e748f5b","Research in recent years has provided some evidence of temporal non-stationarity of functional connectivity in resting state fMRI. In this paper, we present a novel methodology that can decode connectivity dynamics into a temporal sequence of hidden network ""states"" for each subject, using a Hidden Markov Modeling (HMM) framework. Each state is characterized by a unique covariance matrix or whole-brain network. Our model generates these covariance matrices from a common but unknown set of sparse basis networks, which capture the range of functional activity co-variations of regions of interest (ROIs). Distinct hidden states arise due to a variation in the strengths of these basis networks. Thus, our generative model combines a HMM framework with sparse basis learning of positive definite matrices. Results on simulated fMRI data show that our method can effectively recover underlying basis networks as well as hidden states. We apply this method on a normative dataset of resting state fMRI scans. Results indicate that the functional activity of a subject at any point during the scan is composed of combinations of overlapping task-positive/negative pairs of networks as revealed by our basis. Distinct hidden temporal states are produced due to a different set of basis networks dominating the covariance pattern in each state.",,"algorithm; article; artificial intelligence; automated pattern recognition; brain; brain mapping; computer assisted diagnosis; human; image enhancement; methodology; nerve cell network; nuclear magnetic resonance imaging; physiology; reproducibility; rest; sensitivity and specificity; statistical analysis; Algorithms; Artificial Intelligence; Brain; Brain Mapping; Data Interpretation, Statistical; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Nerve Net; Pattern Recognition, Automated; Reproducibility of Results; Rest; Sensitivity and Specificity",Article,Scopus,2-s2.0-84901266629
"Wressnegger C., Schwenk G., Arp D., Rieck K.","A close look on n-grams in intrusion detection: Anomaly detection vs. classification",2013,"Proceedings of the ACM Conference on Computer and Communications Security",18,10.1145/2517312.2517316,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889045953&doi=10.1145%2f2517312.2517316&partnerID=40&md5=ca3548f9b090f4a2bba2fff12deace88","Detection methods based on n-gram models have been widely studied for the identification of attacks and malicious software. These methods usually build on one of two learning schemes: anomaly detection, where a model of normality is constructed from n-grams, or classification, where a discrimination between benign and malicious n-grams is learned. Although successful in many security domains, previous work falls short of explaining why a particular scheme is used and more importantly what renders one favorable over the other for a given type of data. In this paper we provide a close look on n-gram models for intrusion detection. We specifically study anomaly detection and classification using n-grams and develop criteria for data being used in one or the other scheme. Furthermore, we apply these criteria in the scope of web intrusion detection and empirically validate their effectiveness with different learning-based detection methods for client-side and service-side attacks. © 2013 ACM.","intrusion detection; machine learning; n-gram models","Anomaly detection; Client sides; Detection methods; Learning schemes; Malicious software; N-gram models; N-grams; Security domains; Artificial intelligence; Learning systems; Intrusion detection",Conference Paper,Scopus,2-s2.0-84889045953
"Kulick J., Toussaint M., Lang T., Lopes M.","Active learning for teaching a robot grounded relational symbols",2013,"IJCAI International Joint Conference on Artificial Intelligence",18,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896061502&partnerID=40&md5=ad6cb8b4fbe822284add6608febb476e","We investigate an interactive teaching scenario, where a human teaches a robot symbols which abstract the geometric properties of objects. There are multiple motivations for this scenario: First, state-of-the-art methods for relational reinforcement learning demonstrate that we can learn and employ strongly generalizing abstract models with great success for goal-directed object manipulation. However, these methods rely on given grounded action and state symbols and raise the classical question: Where do the symbols come from? Second, existing research on learning from humanrobot interaction has focused mostly on the motion level (e.g., imitation learning). However, if the goal of teaching is to enable the robot to autonomously solve sequential manipulation tasks in a goal-directed manner, the human should have the possibility to teach the relevant abstractions to describe the task and let the robot eventually leverage powerful relational RL methods. In this paper we formalize human-robot teaching of grounded symbols as an active learning problem, where the robot actively generates pick-and-place geometric situations that maximize its information gain about the symbol to be learned. We demonstrate that the learned symbols can be used by a robot in a relational RL framework to learn probabilistic relational rules and use them to solve object manipulation tasks in a goal-directed manner.",,"Geometric properties; Imitation learning; Information gain; Manipulation task; Object manipulation; Probabilistic relational rules; Relational reinforcement learning; State-of-the-art methods; Artificial intelligence; Robots",Conference Paper,Scopus,2-s2.0-84896061502
"Korovin K.","Inst-Gen - A modular approach to instantiation-based automated reasoning",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",18,10.1007/978-3-642-37651-1_10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886782464&doi=10.1007%2f978-3-642-37651-1_10&partnerID=40&md5=185131185382294430375a9c32891313","Inst-Gen is an instantiation-based reasoning method for first-order logic introduced in [18]. One of the distinctive features of Inst-Gen is a modular combination of first-order reasoning with efficient ground reasoning. Thus, Inst-Gen provides a framework for utilising efficient off-the-shelf propositional SAT and SMT solvers as part of general first-order reasoning. In this paper we present a unified view on the developments of the Inst-Gen method: (i) completeness proofs; (ii) abstract and concrete criteria for redundancy elimination, including dismatching constraints and global subsumption; (iii) implementation details and evaluation. © Springer-Verlag Berlin Heidelberg 2013.",,"Automated reasoning; First order logic; First-order; Modular approach; Reasoning methods; Redundancy elimination; Smt solvers; Computer science; Computers; Artificial intelligence",Article,Scopus,2-s2.0-84886782464
"Previti A., Marques-Silva J.","Partial MUS enumeration",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",18,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893412888&partnerID=40&md5=686cbfff62de602f955708883403412d","Minimal explanations of infeasibility find a wide range of uses. In the Boolean domain, these are referred to as Minimal Unsatisfiable Subsets (MUSes). In some settings, one needs to enumerate MUSes of a Boolean formula. Most often the goal is to enumerate all MUSes. In cases where this is computationally infeasible, an alternative is to enumerate some MUSes. This paper develops a novel approach for partial enumeration of MUSes, that complements existing alternatives. If the enumeration of all MUSes is viable, then existing alternatives represent the best option. However, for formulas where the enumeration of all MUSes is unrealistic, our approach provides a solution for enumerating some MUSes within a given time bound. The experimental results focus on formulas for which existing solutions are unable to enumerate MUSes, and shows that the new approach can in most cases enumerate a non-negligible number of MUSes within a given time bound. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Boolean domain; Boolean formulae; New approaches; Partial enumerations; Time bound; Artificial intelligence; Boolean algebra",Conference Paper,Scopus,2-s2.0-84893412888
"Alechina N., Dastani M., Logan B.","Reasoning about normative update",2013,"IJCAI International Joint Conference on Artificial Intelligence",18,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063303&partnerID=40&md5=572e8dcad67173cac4f4b48f78517b01","We consider the problem of updating a multi-agent system with a set of conditional norms. A norm comes into effect when its condition becomes true, and imposes either an obligation or a prohibition on an agent which remains in force until a state satisfying a deadline condition is reached. If the norm is violated, a sanction is imposed on the agent. We define a notion of a normative update of a multi-agent system by a set of conditional norms, and study the problem of checking whether the agent(s) can bring about a state satisfying a property without incurring a specified number of sanctions.",,"Artificial intelligence; Multi agent systems",Conference Paper,Scopus,2-s2.0-84896063303
"Marmier F., Gourc D., Laarz F.","A risk oriented model to assess strategic decisions in new product development projects",2013,"Decision Support Systems",18,10.1016/j.dss.2013.05.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889098339&doi=10.1016%2fj.dss.2013.05.002&partnerID=40&md5=c2e77853e187ddcad268afd162c52e19","The project management team has to respect contractual commitments, in terms of deadlines and budgets, that are often two antagonistic objectives. At the same time, the market becomes more and more demanding as far as costs and delays are concerned while expecting a high quality level. Then, the project management team has to continuously consider novelty and a risk management strategy in order to determine the best balance between benefits and risks. Based on the principles of a synchronized process between risk management and project management, and on the concepts of risk scenario, we propose a decision-making tool to help the project manager choose the best way to improve project success rate while controlling the level of risks. As a finding, the project manager would be able to evaluate and compare different novelties or development strategies taking into account their repercussions on potential risks and risk treatment strategies. Finally, a case study in the aerospace industry and specifically on satellite integration and tests is developed to validate this approach. © 2013 Elsevier B.V.","Decision support system; Project planning; Project variant; Risk management; Scenarios; Treatment strategy","Development strategies; New product development projects; Project planning; Project variant; Risk management strategies; Satellite integration; Scenarios; Treatment strategy; Aerospace industry; Artificial intelligence; Decision support systems; Human resource management; Management science; Managers; Quality management; Risk assessment; Risk management; Risk perception; Project management",Article,Scopus,2-s2.0-84889098339
"Kurokawa D., Lai J.K., Procaccia A.D.","How to cut a cake before the party ends",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",18,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893422702&partnerID=40&md5=0d99a7bef27fe78ef3a0af8b93e41603","For decades researchers have struggled with the problem of envy-free cake cutting: how to divide a divisible good between multiple agents so that each agent likes his own allocation best. Although an envy-free cake cutting protocol was ultimately devised, it is unbounded, in the sense that the number of operations can be arbitrarily large, depending on the preferences of the agents. We ask whether bounded protocols exist when the agents' preferences are restricted. Our main result is an envy-free cake cutting protocol for agents with piecewise linear valuations, which requires a number of operations that is polynomial in natural parameters of the given instance. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Cake cuttings; Multiple agents; Piecewise linear; Piecewise linear techniques; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84893422702
"Fichte J.K., Szeider S.","Backdoors to normality for disjunctive logic programs",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",18,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893421427&partnerID=40&md5=4500f0d30e1c8ee08b48aed3aef8a8fc","Over the last two decades, propositional satisfiability (SAT) has become one of the most successful and widely applied techniques for the solution of NP-complete problems. The aim of this paper is to investigate the oretically how SAT can be utilized for the efficient solution of problems that are harder than NP or co-NP. In particular, we consider the fundamental reasoning problems in propositional disjunctive answer set programming (ASP), BRAVE REASONING and SKEPTICAL REASONING, which ask whether a given atom is contained in at least one or in all answer sets, respectively. Both problems are located at the second level of the Polynomial Hierarchy and thus assumed to be harder than NP or co-NP. One cannot transform the se two reasoning problems into SAT in polynomial time, unless the Polynomial Hierarchy collapses. We show that certain structural aspects of disjunctive logic programs can be utilized to break through this complexity barrier, using new techniques from Parameterized Complexity. In particular, we exhibit transformations from BRAVE and SKEPTICAL REASONING to SAT that run in time O(2κn2) where κ is a structural parameter of the instance and n the input size. In other words, the reduction is fixed-parameter tractable for parameter κ. As the parameter κ. we take the size of a smallest backdoor with respect to the class of normal (i.e., disjunction-free) programs. Such a backdoor is a set of atoms that when deleted makes the program normal. In consequence, the combinatorial explosion, which is expected when transforming a problem from the second level of the Polynomial Hierarchy to the first level, can now be confined to the parameter κ. while the running time of the reduction is polynomial in the input size n, where the order of the polynomial is independent of κ. We show that such a transformation is not possible if we consider backdoors with respect to tightness instead of normality. We think that our approach is applicable to many other hard combinatorial problems that lie beyond NP or co-NP, and thus significantly enlarge the applicability of SAT. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Answer set programming; Combinatorial explosion; Combinatorial problem; Disjunctive logic programs; Parameterized complexity; Polynomial hierarchies; Propositional satisfiability; Structural parameter; Artificial intelligence; Computational complexity; Polynomial approximation; Logic programming",Conference Paper,Scopus,2-s2.0-84893421427
"Chahuara P., Portet F., Vacher M.","Making context aware decision from uncertain information in a smart home: A markov logic network approach",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",18,10.1007/978-3-319-03647-2-6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893422384&doi=10.1007%2f978-3-319-03647-2-6&partnerID=40&md5=09e17cdf67e6232702ec5ad0355d1cc8","This research addresses the issue of building home automation systems reactive to voice for improved comfort and autonomy at home. The focus of this paper is on the context-aware decision process which uses a dedicated Markov Logic Network approach to benefit from the formal logical representation of domain knowledge as well as the ability to handle uncertain facts inferred from real sensor data. The approach has been experiemented in a real smart home with naive and users with special needs. © 2013 Springer International Publishing Switzerland.","Decision making; Knowledge-Based Systems; Reasoning under uncertainty; Sensing and Reasoning Technology","Decision process; Domain knowledge; Home automation systems; Logical representations; Markov logic networks; Real sensor data; Reasoning under uncertainty; Uncertain informations; Artificial intelligence; Automation; Decision making; Knowledge based systems; Markov processes; Intelligent buildings",Conference Paper,Scopus,2-s2.0-84893422384
"Kozak M., Hartley J.","Publication fees for open access journals: Different disciplines - Different methods",2013,"Journal of the American Society for Information Science and Technology",18,10.1002/asi.22972,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887476029&doi=10.1002%2fasi.22972&partnerID=40&md5=1a0f7e39505846d94d08638190ee8044","Many authors appear to think that most open access (OA) journals charge authors for their publications. This brief communication examines the basis for such beliefs and finds it wanting. Indeed, in this study of over 9,000 OA journals included in the Directory of Open Access Journals, only 28% charged authors for publishing in their journals. This figure, however, was highest in various disciplines in medicine (47%) and the sciences (43%) and lowest in the humanities (4%) and the arts (0%). © 2013 ASIS&T.",,"Open Access; Open access journals; Artificial intelligence; Software engineering",Article,Scopus,2-s2.0-84887476029
"Pazis J., Parr R.","PAC optimal exploration in continuous space Markov decision processes",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",18,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893414333&partnerID=40&md5=de7e792f0c6ca5974eab370d9b559da8","Current exploration algorithms can be classified in two broad categories: Heuristic, and PAC optimal. While numerous researchers have used heuristic approaches such as ε-greedy exploration successfully, such approaches lack formal, finite sample guarantees and may need a significant amount of finetuning to produce good results. PAC optimal exploration algorithms, on the other hand, offer strong theoretical guarantees but are inapplicable in domains of realistic size. The goal of this paper is to bridge the gap between theory and practice, by introducing C-PACE, an algorithm which offers strong theoretical guarantees and can be applied to interesting, continuous space problems. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Continuous spaces; Exploration algorithms; Finite samples; Greedy exploration; Heuristic approach; Markov Decision Processes; Theoretical guarantees; Theory and practice; Artificial intelligence; Heuristic algorithms; Heuristic methods; Markov processes; Sampling; Optimization",Conference Paper,Scopus,2-s2.0-84893414333
"Ye D.H., Zikic D., Glocker B., Criminisi A., Konukoglu E.","Modality propagation: coherent synthesis of subject-specific scans with data-driven regularization.",2013,"Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention",18,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894623984&partnerID=40&md5=f6ca4672154ce0fa2343ff08d1c99f82","We propose a general database-driven framework for coherent synthesis of subject-specific scans of desired modality, which adopts and generalizes the patch-based label propagation (LP) strategy. While modality synthesis has received increased attention lately, current methods are mainly tailored to specific applications. On the other hand, the LP framework has been extremely successful for certain segmentation tasks, however, so far it has not been used for estimation of entities other than categorical segmentation labels. We approach the synthesis task as a modality propagation, and demonstrate that with certain modifications the LP framework can be generalized to continuous settings providing coherent synthesis of different modalities, beyond segmentation labels. To achieve high-quality estimates we introduce a new data-driven regularization scheme, in which we integrate intermediate estimates within an iterative search-and-synthesis strategy. To efficiently leverage population data and ensure coherent synthesis, we employ a spatio-population search space restriction. In experiments, we demonstrate the quality of synthesis of different MRI signals (T2 and DTI-FA) from a T1 input, and show a novel application of modality synthesis for abnormality detection in multi-channel MRI of brain tumor patients.",,"algorithm; article; artificial intelligence; automated pattern recognition; brain tumor; computer assisted diagnosis; human; image enhancement; methodology; nuclear magnetic resonance imaging; pathology; reproducibility; sensitivity and specificity; Algorithms; Artificial Intelligence; Brain Neoplasms; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84894623984
"Ganivada A., Ray S.S., Pal S.K.","Fuzzy rough sets, and a granular neural network for unsupervised feature selection",2013,"Neural Networks",18,10.1016/j.neunet.2013.07.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883207118&doi=10.1016%2fj.neunet.2013.07.008&partnerID=40&md5=1f168f28e9cfda21e646a905518e9d04","A granular neural network for identifying salient features of data, based on the concepts of fuzzy set and a newly defined fuzzy rough set, is proposed. The formation of the network mainly involves an input vector, initial connection weights and a target value. Each feature of the data is normalized between 0 and 1 and used to develop granulation structures by a user defined α-value. The input vector and the target value of the network are defined using granulation structures, based on the concept of fuzzy sets. The same granulation structures are also presented to a decision system. The decision system helps in extracting the domain knowledge about data in the form of dependency factors, using the notion of new fuzzy rough set. These dependency factors are assigned as the initial connection weights of the proposed network. It is then trained using minimization of a novel feature evaluation index in an unsupervised manner. The effectiveness of the proposed network, in evaluating selected features, is demonstrated on several real-life datasets. The results of FRGNN are found to be statistically more significant than related methods in 28 instances of 40 instances, i.e., 70% of instances, using the paired t-test. © 2013 Elsevier Ltd.","Feature evaluation; Granular computing; Rough fuzzy computing; Rule based layered network","Connection weights; Feature evaluation; Fuzzy computing; Fuzzy-rough sets; Granulation structure; Real life datasets; Rule based layered networks; Unsupervised feature selection; Fuzzy sets; Granular computing; Granulation; Network layers; Neural networks; Rough set theory; article; artificial neural network; atmosphere; Bayesian learning; breast cancer; classification algorithm; controlled study; feature selection; fuzzy rough granular neural network; fuzzy rough set; fuzzy system; gene expression; heart arrhythmia; iris; machine learning; measurement accuracy; microarray analysis; priority journal; rough set; statistical concepts; telecommunication; waveform; Feature evaluation; Granular computing; Rough fuzzy computing; Rule based layered network; Algorithms; Arrhythmias, Cardiac; Artificial Intelligence; Atmosphere; Bayes Theorem; Cell Cycle; Databases, Factual; Decision Theory; Electronic Mail; Entropy; Fuzzy Logic; Humans; Microarray Analysis; Neoplasms; Neural Networks (Computer); Plants; Semiconductors; Support Vector Machines; Terminology as Topic; Wavelet Analysis",Article,Scopus,2-s2.0-84883207118
"Nie F., Wang H., Huang H., Ding C.","Adaptive loss minimization for semi-supervised elastic embedding",2013,"IJCAI International Joint Conference on Artificial Intelligence",18,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896060974&partnerID=40&md5=5da5600969b9690934cd3f77728531ac","The semi-supervised learning usually only predict labels for unlabeled data appearing in training data, and cannot effectively predict labels for testing data never appearing in training set. To handle this outof- sample problem, many inductive methods make a constraint such that the predicted label matrix should be exactly equal to a linear model. In practice, this constraint is too rigid to capture the manifold structure of data. Motivated by this deficiency, we relax the rigid linear embedding constraint and propose to use an elastic embedding constraint on the predicted label matrix such that the manifold structure can be better explored. To solve our new objective and also a more general optimization problem, we study a novel adaptive loss with efficient optimization algorithm. Our new adaptive loss minimization method takes the advantages of both L1 norm and L2 norm, and is robust to the data outlier under Laplacian distribution and can efficiently learn the normal data under Gaussian distribution. Experiments have been performed on image classification tasks and our approach outperforms other state-of-the-art methods.",,"General optimizations; Inductive method; Laplacian distribution; Loss minimization; Manifold structures; Optimization algorithms; Semi-supervised learning; State-of-the-art methods; Algorithms; Artificial intelligence; Image classification; Optimization; Queueing networks; Supervised learning; Matrix algebra",Conference Paper,Scopus,2-s2.0-84896060974
"Gagné M., Narayan S., Safavi-Naini R.","Short pairing-efficient threshold-attribute-based signature",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",18,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893108037&partnerID=40&md5=bbd3f039b4633fa98ed090689f3381be","In this paper, we construct a new threshold-attribute-based signature (t-ABS) scheme that is significantly more efficient than all previous t-ABS schemes. The verification algorithm requires the computation of only 3 pairing operations, independently of the attribute set of the signature, and the size of the signature is also independent of the number of attributes. The security of all our schemes is reduced to the computational Diffie-Hellman problem. We also show how to achieve shorter public parameters based on the intractability of computational Diffie-Hellman assumption in the random oracle model. © Springer-Verlag Berlin Heidelberg 2013.",,"Attribute sets; Computational diffie-hellman problems; Diffie-Hellman assumption; Random Oracle model; Verification algorithms; Artificial intelligence; Computer science; Computers; Cryptography",Conference Paper,Scopus,2-s2.0-84893108037
"Wang C., She Z., Cao L.","Coupled attribute analysis on numerical data",2013,"IJCAI International Joint Conference on Artificial Intelligence",18,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063878&partnerID=40&md5=6277d59a48ae51e3b2b368c228784652","The usual representation of quantitative data is to formalize it as an information table, which assumes the independence of attributes. In real-world data, attributes are more or less interacted and coupled via explicit or implicit relationships. Limited research has been conducted on analyzing such attribute interactions, which only describe a local picture of attribute couplings in an implicit way. This paper proposes a framework of the coupled attribute analysis to capture the global dependency of continuous attributes. Such global couplings integrate the intra-coupled interaction within an attribute (i.e. The correlations between attributes and their own powers) and inter-coupled interaction among different attributes (i.e. The correlations between attributes and the powers of others) to form a coupled representation for numerical objects by the Taylor-like expansion. This work makes one step forward towards explicitly addressing the global interactions of continuous attributes, verified by the applications in data structure analysis, data clustering, and data classification. Substantial experiments on 13 UCI data sets demonstrate that the coupled representation can effectively capture the global couplings of attributes and outperforms the traditional way, supported by statistical analysis.",,"Attribute analysis; Attribute interactions; Continuous attribute; Data classification; Global interaction; Implicit relationships; Quantitative data; Structure analysis; Artificial intelligence; Clustering algorithms; Couplings",Conference Paper,Scopus,2-s2.0-84896063878
"Huang L.-K., Yang Q., Zheng W.-S.","Online hashing",2013,"IJCAI International Joint Conference on Artificial Intelligence",18,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063756&partnerID=40&md5=55dde7c0f89aa5d9c20251e9b620f4ab","Hash function learning has been recently received more and more attentions in fast search for large scale data. However, existing popular learning based hashing methods are batch-based learning models and thus incur large scale computational problem for learning an optimal model on a large scale of labelled data and cannot handle data which comes sequentially. In this paper, we address the problem by developing an online hashing learning algorithm to get hashing model accommodate to each new pair of data. At the same time the new updated hash model is penalized by the last learned model in order to retain important information learned in previous rounds. We also derive a tight bound for the cumulative loss of our proposed online learning algorithm. The experimental results demonstrate superiority of the proposed online hashing model on searching both metric distance neighbors and semantical similar neighbors in the experiments.",,"Computational problem; Function learning; Hashing method; Large scale data; Learning models; Metric distances; Online learning algorithms; Similar neighbors; Hash functions; Learning algorithms; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84896063756
"De Weerdt M.M., Gerding E.H., Stein S., Robu V., Jennings N.R.","Intention-aware routing to minimise delays at electric vehicle charging stations",2013,"IJCAI International Joint Conference on Artificial Intelligence",18,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896059411&partnerID=40&md5=ca83b05fb9e6d69e469d16853e162a30","En-route charging stations allow electric vehicles to greatly extend their range. However, as a full charge takes a considerable amount of time, there may be significant waiting times at peak hours. To address this problem, we propose a novel navigation system, which communicates its intentions (i.e., routing policies) to other drivers. Using these intentions, our system accurately predicts congestion at charging stations and suggests the most efficient route to its user. We achieve this by extending existing time-dependent stochastic routing algorithms to include the battery's state of charge and charging stations. Furthermore, we describe a novel technique for combining historical information with agent intentions to predict the queues at charging stations. Through simulations we show that our system leads to a significant increase in utility compared to existing approaches that do not explicitly model waiting times or use intentions, in some cases reducing waiting times by over 80% and achieving near-optimal overall journey times.",,"Charging station; Electric vehicle charging; Historical information; Journey time; Novel techniques; Routing policies; State of charge; Use intentions; Artificial intelligence; Computer simulation; Navigation systems; Electric vehicles",Conference Paper,Scopus,2-s2.0-84896059411
"Jiang H., Yan Z., Liu X.","Melt index prediction using optimized least squares support vector machines based on hybrid particle swarm optimization algorithm",2013,"Neurocomputing",18,10.1016/j.neucom.2013.03.006,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881550877&doi=10.1016%2fj.neucom.2013.03.006&partnerID=40&md5=9d375bf5f615dfdca00ba5ff936f990d","Melt index (MI) is considered as one of the most important variables of the quality, which determines the product specifications. Thus, a reliable estimation of MI is crucial in the quality control of the practical processes in the propylene polymerization (PP). An optimal soft sensor, named the least squares support vector machines with Ant Colony-Immune Clone Particle Swarm Optimization (AC-ICPSO-LSSVM), is therefore proposed. It combines the advantages of the high accuracy of LSSVM and the fast convergence of PSO. Furthermore, the immune clone (IC) method is introduced into the PSO algorithm to make the particles of ICPSO diverse and enhance global search capability for avoiding the premature convergence and local optimization of the conventional PSO algorithm. Besides, to widen data range, improve search precision and convergence efficiency, and avoid premature convergence, Ant Colony Optimization (ACO) is introduced to find the initial particles for PSO algorithm. The resultant hybrid AC-ICPSO algorithm is then applied to optimize the parameters of LSSVM, so the optimal prediction model of melt index, AC-ICPSO-LSSVM, is obtained. As the comparative basis, the models of ICPSO-LSSVM, PSO-LSSVM, and LSSVM are also developed respectively. Based on the data from a real PP production plant, a detailed comparison of the models is carried out. These models are also compared with RBF method reported in the open literature. The research results reveal the prediction accuracy and validity of the proposed approach. © 2013 Elsevier B.V.","Ant colony optimization; Immune clone particle swarm optimization; Industrial polypropylene manufacture; Least squares support vector machines; Melt index prediction","Ant Colony Optimization (ACO); Global search capability; Hybrid particle swarm optimization algorithm; Immune clone particle swarm optimizations; Least squares support vector machines; Melt index predictions; Pre-mature convergences; Propylene polymerization; Ant colony optimization; Artificial intelligence; Cloning; Forecasting; Particle swarm optimization (PSO); Polypropylenes; Algorithms; accuracy; algorithm; ant colony immune clone particle swarm optimization; article; information processing; intermethod comparison; least squares support vector machine; melt index prediction; prediction; priority journal; process optimization; regression analysis; statistical model; support vector machine; validity",Article,Scopus,2-s2.0-84881550877
"Borkin M.A., Yeh C.S., Boyd M., MacKo P., Gajos K.Z., Seltzer M., Pfister H.","Evaluation of filesystem provenance visualization tools",2013,"IEEE Transactions on Visualization and Computer Graphics",18,10.1109/TVCG.2013.155,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886463300&doi=10.1109%2fTVCG.2013.155&partnerID=40&md5=3c98c101df18ea1e6e7bd183f1d82e71","Having effective visualizations of filesystem provenance data is valuable for understanding its complex hierarchical structure. The most common visual representation of provenance data is the node-link diagram. While effective for understanding local activity, the node-link diagram fails to offer a high-level summary of activity and inter-relationships within the data. We present a new tool, InProv, which displays filesystem provenance with an interactive radial-based tree layout. The tool also utilizes a new time-based hierarchical node grouping method for filesystem provenance data we developed to match the user's mental model and make data exploration more intuitive. We compared InProv to a conventional node-link based tool, Orbiter, in a quantitative evaluation with real users of filesystem provenance data including provenance data experts, IT professionals, and computational scientists. We also compared in the evaluation our new node grouping method to a conventional method. The results demonstrate that InProv results in higher accuracy in identifying system activity than Orbiter with large complex data sets. The results also show that our new time-based hierarchical node grouping method improves performance in both tools, and participants found both tools significantly easier to use with the new time-based node grouping method. Subjective measures show that participants found InProv to require less mental activity, less physical activity, less work, and is less stressful to use. Our study also reveals one of the first cases of gender differences in visualization; both genders had comparable performance with InProv, but women had a significantly lower average accuracy (56%) compared to men (70%) with Orbiter. © 2013 IEEE.","gender differences; graph/network data; hierarchy data; Provenance data; quantitative evaluation","Gender differences; graph/network data; hierarchy data; Provenance data; Quantitative evaluation; Information technology; Visualization; Tools; adult; algorithm; artificial intelligence; computer graphics; computer interface; computer program; evaluation study; factual database; female; human; image enhancement; information retrieval; male; multimodal imaging; pattern recognition; physiology; procedures; task performance; article; information retrieval; methodology; multimodal imaging; pattern recognition; physiology; Adult; Algorithms; Artificial Intelligence; Computer Graphics; Databases, Factual; Female; Humans; Image Enhancement; Information Storage and Retrieval; Male; Multimodal Imaging; Pattern Recognition, Visual; Software; Task Performance and Analysis; User-Computer Interface; Adult; Algorithms; Artificial Intelligence; Computer Graphics; Databases, Factual; Female; Humans; Image Enhancement; Information Storage and Retrieval; Male; Multimodal Imaging; Pattern Recognition, Visual; Software; Task Performance and Analysis; User-Computer Interface",Article,Scopus,2-s2.0-84886463300
"De Santis E., Rizzi A., Sadeghiany A., Mascioli F.M.F.","Genetic optimization of a fuzzy control system for energy flow management in micro-grids",2013,"Proceedings of the 2013 Joint IFSA World Congress and NAFIPS Annual Meeting, IFSA/NAFIPS 2013",18,10.1109/IFSA-NAFIPS.2013.6608437,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886462936&doi=10.1109%2fIFSA-NAFIPS.2013.6608437&partnerID=40&md5=ad808071063005d7dc69e66b2d008661","In this paper we present an interesting application of Computational Intelligence techniques for the power demand side and flow management optimization in a microgrid. In particular, we used a Fuzzy Logic Controller (FLC) for Time-of use Cost Management program in the microgrid. FLC can either sell and buy energy from outside the microgrid making use of an aggregate of energy storage capacity realized with lithium ion batteries. According to the hybrid Fuzzy-GA paradigm, the Fuzzy Logic Controller that operates decision making on energy flows is optimized by a Genetic Algorithm. The experimental results show that the proposed control system can manage effectively the energy trade with the main grid on the basis of real time prices. © 2013 IEEE.",,"Computational intelligence techniques; Energy flow management; Energy storage capacity; Flow management; Fuzzy logic controllers; Genetic optimization; Lithium-ion battery; Real-time price; Artificial intelligence; Fuzzy logic; Fuzzy systems; Optimization; Electric power distribution",Conference Paper,Scopus,2-s2.0-84886462936
"Riera J.V., Torrens J.","Residual implications on the set of discrete fuzzy numbers",2013,"Information Sciences",18,10.1016/j.ins.2013.06.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880921098&doi=10.1016%2fj.ins.2013.06.008&partnerID=40&md5=d7afae243158e4217a0a4bcf807380d2","In this paper residual implications defined on the set of discrete fuzzy numbers whose support is a set of consecutive natural numbers are studied. A specific construction of these implications is given and some examples are presented showing in particular that such a construction generalizes the case of interval-valued residual implications. The most usual properties for these operations are investigated leading to a residuated lattice structure on the set of discrete fuzzy numbers, that in general is not an MTL-algebra. © 2013 Elsevier Inc. All rights reserved.","Discrete fuzzy number; Fuzzy implication; R-implication; Residuated lattice","Discrete fuzzy numbers; Fuzzy implications; Interval-valued; MTL-algebras; Natural number; R-implication; Residual implications; Residuated lattices; Artificial intelligence; Software engineering; Fuzzy rules",Article,Scopus,2-s2.0-84880921098
"Davendra D., Bialic-Davendra M., Senkerik R.","Scheduling the Lot-Streaming Flowshop scheduling problem with setup time with the chaos-induced Enhanced Differential Evolution",2013,"Proceedings of the 2013 IEEE Symposium on Differential Evolution, SDE 2013 - 2013 IEEE Symposium Series on Computational Intelligence, SSCI 2013",18,10.1109/SDE.2013.6601451,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885234874&doi=10.1109%2fSDE.2013.6601451&partnerID=40&md5=b32dfd65010d2da144c022530d81bfdf","The dissipative Lozi chaotic map is embedded in the Enhanced Differential Evolution (EDE) algorithm, as a pseudorandom generator. This novel chaotic based algorithm is applied to the constraint based Lot-Streaming Flowshop scheduling problem. Two new and unique data sets generated using the Lozi and Dissipative maps are used to compare the chaos embedded EDE (EDEC) and the generic EDE utilising the venerable Mersenne Twister. In total, 100 data sets were tested by the two algorithms, for the idling and the non-idling case, with the EDEC algorithm consistently outperforming the generic version. © 2013 IEEE.",,"Chaotic map; Constraint-based; Differential Evolution; Flow shop scheduling problem; Lot-streaming; Mersenne twisters; Pseudorandom generators; Set-up time; Artificial intelligence; Chaotic systems; Scheduling; Evolutionary algorithms",Conference Paper,Scopus,2-s2.0-84885234874
"Kozlov A., Kudashev O., Matveev Y., Pekhovsky T., Simonchik K., Shulipa A.","SVID speaker recognition system for NIST SRE 2012",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",18,10.1007/978-3-319-01931-4_37,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885073205&doi=10.1007%2f978-3-319-01931-4_37&partnerID=40&md5=ddf318be7be6acbff670fa3c77d9f7f7","A description of the SVID speaker recognition system is presented. This system was developed for submission to the NIST SRE 2012. © 2013 Springer International Publishing.","GMM; JFA; NIST SRE; PLDA; speaker recognition","GMM; JFA; NIST SRE; PLDA; Speaker recognition; Artificial intelligence; Computer science; Speech recognition",Conference Paper,Scopus,2-s2.0-84885073205
"Coppo M., Dezani-Ciancaglini M., Padovani L., Yoshida N.","Inference of global progress properties for dynamically interleaved multiparty sessions",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",18,10.1007/978-3-642-38493-6_4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885015667&doi=10.1007%2f978-3-642-38493-6_4&partnerID=40&md5=2504d755d44348f2868bf3304cb671af","Conventional session type systems guarantee progress within single sessions, but do not usually take into account the dependencies arising from the interleaving of simultaneously active sessions and from session delegations. As a consequence, a well-typed system may fail to have progress, even assuming that helper processes can join the system after its execution has started. In this paper we develop a static analysis technique, specified as a set of syntax-directed inference rules, that is capable of verifying whether a system of processes engaged in simultaneously active multiparty sessions has the progress property. © 2013 IFIP International Federation for Information Processing.",,"Analysis techniques; Inference rules; Multiparty sessions; Session types; Artificial intelligence; Computer science",Conference Paper,Scopus,2-s2.0-84885015667
"Pietruczuk L., Duda P., Jaworski M.","Adaptation of decision trees for handling concept drift",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",18,10.1007/978-3-642-38658-9_41,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884396535&doi=10.1007%2f978-3-642-38658-9_41&partnerID=40&md5=fe70af0a866ceb4a31d28b851a42031d","The problem of data stream mining is widely studied in the literature. Especially difficult to solve is the problem of mining data with occurring concept drift. The most commonly used algorithms are those based on decision trees. In this article we investigate the performance of a few algorithms of constructing decision trees for data stream classification, not explicitly designed to deal with changing distribution of data. We show how to adapt these methods to deal with concept drift and we compare the obtained results. © 2013 Springer-Verlag.",,"Changing distributions; Concept drifts; Data stream classifications; Data stream mining; Algorithms; Artificial intelligence; Data communication systems; Data mining; Forestry; Soft computing; Decision trees; Algorithms; Artificial Intelligence; Communication; Computation; Data Processing; Decision Making; Forestry",Conference Paper,Scopus,2-s2.0-84884396535
"Szczypta J., Przybył A., Cpałka K.","Some aspects of evolutionary designing optimal controllers",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",18,10.1007/978-3-642-38610-7_9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884373993&doi=10.1007%2f978-3-642-38610-7_9&partnerID=40&md5=900ea3b40b3efeea75ffa897a86b6b49","In this paper a new automatic method of control system design was presented. Our method is based on the evolutionary algorithm, which is used for selection of the controller structure as well as for parameters tuning. This is realized by means of testing different controller structures and elimination of spare elements, taking into account theirs impact on control quality factors. Presented method was tested with two control objects of different complexity. © 2013 Springer-Verlag.",,"Automatic method; Control objects; Control quality; Controller structures; Optimal controller; Parameters tuning; Artificial intelligence; Soft computing",Conference Paper,Scopus,2-s2.0-84884373993
"Banharnsakun A., Sirinaovakul B., Achalakul T.","The best-so-far ABC with multiple patrilines for clustering problems",2013,"Neurocomputing",18,10.1016/j.neucom.2012.02.047,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878496046&doi=10.1016%2fj.neucom.2012.02.047&partnerID=40&md5=8e0b12e408ec971948160d665b9e10ed","Clustering is an important process in many application domains such as machine learning, data mining, pattern recognition, image analysis, information retrieval, and bioinformatics. The main objective of clustering is to search for hidden patterns that may exist in datasets. Since the clustering problem is considered to be NP-hard, previous research has applied bio-inspired heuristic methods to solve such problems. In this paper we propose an effective method for clustering using an algorithm inspired by the decision making processes of bee swarms. The algorithm is called the Best-so-far Artificial Bee Colony with multiple patrilines. In the Best-so-far method, the solution direction is biased toward the Best-so-far solution rather than a neighboring solution proposed in the original Artificial Bee Colony algorithm. We introduce another bee-inspired concept called multiple patrilines to further improve the diversity of solutions and allow the calculations to be distributed among multiple computing units. We empirically assess the performance of our proposed method on several standard datasets taken from the UCI Machine Learning Repository. The results show that the proposed method produces solutions that are as good as or better than the current state-of-the-art clustering techniques reported in the literature. Furthermore, to demonstrate the computing performance and scalability of the algorithm, we assess the algorithm on a large disk drive manufacturing dataset. The results indicate that our distributed Best-so-far approach is scalable and produces good solutions while significantly improving the processing time. © 2012 Elsevier B.V.","Best-so-far ABC; Clustering; Distributed environments; Multiple patrilines; Optimization; Parallel computing; Swarm intelligence","Best-so-far ABC; Clustering; Distributed environments; Multiple patrilines; Swarm Intelligence; Bioinformatics; Evolutionary algorithms; Heuristic methods; Learning systems; Optimization; Parallel architectures; Parallel processing systems; Pattern recognition; Problem solving; Clustering algorithms; algorithm; article; Artificial Bee Colony; artificial intelligence; bioinformatics; calculation; cluster analysis; data analysis; data mining; decision making; image analysis; information retrieval; machine learning; priority journal; process optimization; swarm intelligence",Article,Scopus,2-s2.0-84878496046
"Shen S., Deng K., Iosup A., Epema D.","Scheduling jobs in the cloud using on-demand and reserved instances",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",18,10.1007/978-3-642-40047-6_27,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883153688&doi=10.1007%2f978-3-642-40047-6_27&partnerID=40&md5=84f733971b78754775a9871b3b0fc641","Deploying applications in leased cloud infrastructure is increasingly considered by a variety of business and service integrators. However, the challenge of selecting the leasing strategy - larger or faster instances? on-demand or reserved instances? etc.- and to configure the leasing strategy with appropriate scheduling policies is still daunting for the (potential) cloud user. In this work, we investigate leasing strategies and their policies from a broker's perspective. We propose, CoH, a family of Cloud-based, online, Hybrid scheduling policies that minimizes rental cost by making use of both on-demand and reserved instances. We formulate the resource provisioning and job allocation policies as Integer Programming problems. As the policies need to be executed online, we limit the time to explore the optimal solution of the integer program, and compare the obtained solution with various heuristics-based policies; then automatically pick the best one. We show, via simulation and using multiple real-world traces, that the hybrid leasing policy can obtain significantly lower cost than typical heuristics-based policies. © 2013 Springer-Verlag.",,"Cloud infrastructures; Hybrid scheduling; Integer program; Integer programming problems; Job allocation; Optimal solutions; Scheduling jobs; Scheduling policies; Artificial intelligence; Computer science; Integer programming",Conference Paper,Scopus,2-s2.0-84883153688
"Evangelidis G.D., Bauckhage C.","Efficient subframe video alignment using short descriptors",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",18,10.1109/TPAMI.2013.56,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883196707&doi=10.1109%2fTPAMI.2013.56&partnerID=40&md5=98e14cd36d73528b83528f91209709a8","This paper addresses the problem of video alignment. We present efficient approaches that allow for spatiotemporal alignment of two sequences. Unlike most related works, we consider independently moving cameras that capture a 3D scene at different times. The novelty of the proposed method lies in the adaptation and extension of an efficient information retrieval framework that casts the sequences as an image database and a set of query frames, respectively. The efficient retrieval builds on the recently proposed quad descriptor. In this context, we define the 3D Vote Space (VS) by aggregating votes through a multiquerying (multiscale) scheme and we present two solutions based on VS entries; a causal solution that permits online synchronization and a global solution through multiscale dynamic programming. In addition, we extend the recently introduced ECC image-alignment algorithm to the temporal dimension that allows for spatial registration and synchronization refinement with subframe accuracy. We investigate full search and quantization methods for short descriptors and we compare the proposed schemes with the state of the art. Experiments with real videos by moving or static cameras demonstrate the efficiency of the proposed method and verify its effectiveness with respect to spatiotemporal alignment accuracy. © 1979-2012 IEEE.","image/video retrieval; short image descriptors; spatiotemporal alignment; Video synchronization","Alignment accuracy; Causal solutions; Image descriptors; image/video retrieval; Spatial registrations; State of the art; Temporal dimensions; Video synchronizations; Synchronization; Three dimensional; Video cameras; Alignment; algorithm; artificial intelligence; automated pattern recognition; computer assisted diagnosis; image enhancement; image subtraction; photography; procedures; reproducibility; sensitivity and specificity; spatiotemporal analysis; three dimensional imaging; videorecording; article; automated pattern recognition; computer assisted diagnosis; methodology; photography; three dimensional imaging; videorecording; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Pattern Recognition, Automated; Photography; Reproducibility of Results; Sensitivity and Specificity; Spatio-Temporal Analysis; Subtraction Technique; Video Recording; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Pattern Recognition, Automated; Photography; Reproducibility of Results; Sensitivity and Specificity; Spatio-Temporal Analysis; Subtraction Technique; Video Recording",Article,Scopus,2-s2.0-84883196707
"Syed Abdul Mutalib S.N., Juahir H., Azid A., Mohd Sharif S., Latif M.T., Aris A.Z., Zain S.M., Dominick D.","Spatial and temporal air quality pattern recognition using environmetric techniques: A case study in Malaysia",2013,"Environmental Sciences: Processes and Impacts",18,10.1039/c3em00161j,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882963012&doi=10.1039%2fc3em00161j&partnerID=40&md5=3309aef007767b01d8387ab23d445b4a","The objective of this study is to identify spatial and temporal patterns in the air quality at three selected Malaysian air monitoring stations based on an eleven-year database (January 2000-December 2010). Four statistical methods, Discriminant Analysis (DA), Hierarchical Agglomerative Cluster Analysis (HACA), Principal Component Analysis (PCA) and Artificial Neural Networks (ANNs), were selected to analyze the datasets of five air quality parameters, namely: SO 2, NO2, O3, CO and particulate matter with a diameter size of below 10 μm (PM10). The three selected air monitoring stations share the characteristic of being located in highly urbanized areas and are surrounded by a number of industries. The DA results show that spatial characterizations allow successful discrimination between the three stations, while HACA shows the temporal pattern from the monthly and yearly factor analysis which correlates with severe haze episodes that have happened in this country at certain periods of time. The PCA results show that the major source of air pollution is mostly due to the combustion of fossil fuel in motor vehicles and industrial activities. The spatial pattern recognition (S-ANN) results show a better prediction performance in discriminating between the regions, with an excellent percentage of correct classification compared to DA. This study presents the necessity and usefulness of environmetric techniques for the interpretation of large datasets aiming to obtain better information about air quality patterns based on spatial and temporal characterizations at the selected air monitoring stations. © 2013 The Royal Society of Chemistry.",,"carbon monoxide; fossil fuel; nitrogen dioxide; sulfur dioxide; air monitoring; air quality; air sampling; article; artificial neural network; cluster analysis; combustion; discriminant analysis; exhaust gas; factorial analysis; Malaysia; motor vehicle; particle size; particulate matter; principal component analysis; priority journal; Air Pollutants; Air Pollution; Artificial Intelligence; Cluster Analysis; Discriminant Analysis; Environmental Monitoring; Malaysia; Particle Size; Particulate Matter; Principal Component Analysis",Article,Scopus,2-s2.0-84882963012
"Hadj Taieb M.A., Ben Aouicha M., Ben Hamadou A.","Computing semantic relatedness using Wikipedia features",2013,"Knowledge-Based Systems",18,10.1016/j.knosys.2013.06.015,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881311114&doi=10.1016%2fj.knosys.2013.06.015&partnerID=40&md5=830995a11cbcb886109c0fba0c02a1bb","Measuring semantic relatedness is a critical task in many domains such as psychology, biology, linguistics, cognitive science and artificial intelligence. In this paper, we propose a novel system for computing semantic relatedness between words. Recent approaches have exploited Wikipedia as a huge semantic resource that showed good performances. Therefore, we utilized the Wikipedia features (articles, categories, Wikipedia category graph and redirection) in a system combining this Wikipedia semantic information in its different components. The approach is preceded by a pre-processing step to provide for each category pertaining to the Wikipedia category graph a semantic description vector including the weights of stems extracted from articles assigned to the target category. Next, for each candidate word, we collect its categories set using an algorithm for categories extraction from the Wikipedia category graph. Then, we compute the semantic relatedness degree using existing vector similarity metrics (Dice, Overlap and Cosine) and a new proposed metric that performed well as cosine formula. The basic system is followed by a set of modules in order to exploit Wikipedia features to quantify better as possible the semantic relatedness between words. We evaluate our measure based on two tasks: comparison with human judgments using five datasets and a specific application ""solving choice problem"". Our result system shows a good performance and outperforms sometimes ESA (Explicit Semantic Analysis) and TSA (Temporal Semantic Analysis) approaches. © 2013 Elsevier B.V. All rights reserved.","Semantic analysis; Semantic relatedness; Wikipedia; Wikipedia category graph; Word relatedness","Explicit semantic analysis; Pre-processing step; Semantic analysis; Semantic descriptions; Semantic information; Semantic relatedness; Wikipedia; Word relatedness; Artificial intelligence; Data processing; Semantics",Article,Scopus,2-s2.0-84881311114
"Ding Y., Song M., Han J., Yu Q., Yan E., Lin L., Chambers T.","Entitymetrics: Measuring the Impact of Entities",2013,"PLoS ONE",18,10.1371/journal.pone.0071416,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883229820&doi=10.1371%2fjournal.pone.0071416&partnerID=40&md5=5b2d95641b697b25a9cfbaa884dfe6b4","This paper proposes entitymetrics to measure the impact of knowledge units. Entitymetrics highlight the importance of entities embedded in scientific literature for further knowledge discovery. In this paper, we use Metformin, a drug for diabetes, as an example to form an entity-entity citation network based on literature related to Metformin. We then calculate the network features and compare the centrality ranks of biological entities with results from Comparative Toxicogenomics Database (CTD). The comparison demonstrates the usefulness of entitymetrics to detect most of the outstanding interactions manually curated in CTD. © 2013 Ding et al.",,"metformin; article; citation analysis; comparative study; controlled study; entitymetrics; genetic database; information processing; knowledge base; knowledge discovery; knowledge management; Algorithms; Artificial Intelligence; Bibliometrics; Biometry; Databases, Factual; Humans; Knowledge; Metformin; Neural Networks (Computer); Toxicogenetics",Article,Scopus,2-s2.0-84883229820
"Sohn Dr. S., Wagholikar K.B., Li D., Jonnalagadda S.R., Tao C., Elayavilli R.K., Liu H.","Comprehensive temporal information detection from clinical text: Medical events, time, and TLINK identification",2013,"Journal of the American Medical Informatics Association",18,10.1136/amiajnl-2013-001622,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881183249&doi=10.1136%2famiajnl-2013-001622&partnerID=40&md5=bcfc8eff87fb446ef3f79b645f88f9e6","Background: Temporal information detection systems have been developed by the Mayo Clinic for the 2012 i2b2 Natural Language Processing Challenge. Objective: To construct automated systems for EVENT/ TIMEX3 extraction and temporal link (TLINK) identification from clinical text. Materials and methods: The i2b2 organizers provided 190 annotated discharge summaries as the training set and 120 discharge summaries as the test set. Our Event system used a conditional random field classifier with a variety of features including lexical information, natural language elements, and medical ontology. The TIMEX3 system employed a rule-based method using regular expression pattern match and systematic reasoning to determine normalized values. The TLINK system employed both rule-based reasoning and machine learning. All three systems were built in an Apache Unstructured Information Management Architecture framework. Results: Our TIMEX3 system performed the best (F-measure of 0.900, value accuracy 0.731) among the challenge teams. The Event system produced an F-measure of 0.870, and the TLINK system an F-measure of 0.537. Conclusions: Our TIMEX3 system demonstrated good capability of regular expression rules to extract and normalize time information. Event and TLINK machine learning systems required well-defined feature sets to perform well. We could also leverage expert knowledge as part of the machine learning features to further improve TLINK identification performance.",,"APACHE; article; automation; data extraction; hospital discharge; human; information processing; language; machine learning; measurement accuracy; medical history; medical information; time; Artificial Intelligence; Electronic Health Records; Humans; Information Storage and Retrieval; Natural Language Processing; Patient Discharge Summaries; Time",Article,Scopus,2-s2.0-84881183249
"Wang Z., Ye J.","Querying discriminative and representative samples for batch mode active learning",2013,"Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",18,10.1145/2487575.2487643,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016811324&doi=10.1145%2f2487575.2487643&partnerID=40&md5=0823b313bd5c911a010265ebf00bd9c9","Empirical risk minimization (ERM) provides a useful guideline for many machine learning and data mining algorithms. Under the ERM principle, one minimizes an upper bound of the true risk, which is approximated by the summation of empirical risk and the complexity of the candidate classifier class. To guarantee a satisfactory learning performance, ERM requires that the training data are i.i.d. sampled from the unknown source distribution. However, this may not be the case in active learning, where one selects the most informative samples to label and these data may not follow the source distribution. In this paper, we generalize the empirical risk minimization principle to the active learning setting. We derive a novel form of upper bound for the true risk in the active learning setting; by minimizing this upper bound we develop a practical batch mode active learning method. The proposed formulation involves a non-convex integer programming optimization problem. We solve it efficiently by an alternating optimization method. Our method is shown to query the most informative samples while preserving the source distribution as much as possible, thus identifying the most uncertain and representative queries. Experiments on benchmark data sets and real-world applications demonstrate the superior performance of our proposed method in comparison with the state-of-The-Art methods. Copyright © 2013 ACM.","Active learning; Empirical risk minimization; Maximum mean discrepancy; Representative and discriminative","Artificial intelligence; Benchmarking; Data mining; Integer programming; Learning systems; Optimization; Active Learning; Alternating optimizations; Batch mode active learning; Convex integer programming; Empirical risk minimization; Maximum mean discrepancy; Representative and discriminative; State-of-the-art methods; Education",Conference Paper,Scopus,2-s2.0-85016811324
"Su Y., Wang Z.","Pseudo-uninorms and coimplications on a complete lattice",2013,"Fuzzy Sets and Systems",18,10.1016/j.fss.2012.09.017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877720566&doi=10.1016%2fj.fss.2012.09.017&partnerID=40&md5=ca74a835364e407855f8cdd211c2bca5","Pseudo-uninorms are a generalization of uninorms by removing the commutativity from the axioms of the uninorms. In this paper, we further study pseudo-uninorms and coimplications on a complete lattice. Firstly, we discuss the residual coimplications of pseudo-uninorms and give equivalent conditions for left (right) infinitely ∧-distributive pseudo-uninorms. Then, we study some properties of (U,N)-coimplications generated from a pseudo-uninorm and a strong negation. Finally, we investigate the pseudo-uninorms induced by coimplications, present equivalent conditions for right infinitely ∨-distributive coimplications, and provide some conditions such that the operators induced by coimplications are uninorms. © 2012 Elsevier B.V.","Coimplication; Fuzzy connective; Pseudo-uninorm; Uninorm","Coimplication; Commutativity; Complete lattices; Equivalent condition; Fuzzy connectives; Pseudo-uninorm; Strong negations; Uninorms; Artificial intelligence; Fuzzy sets; Mathematical operators",Article,Scopus,2-s2.0-84877720566
"Alon N., Demaine E.D., Hajiaghayi M.T., Leighton T.","Basic network creation games",2013,"SIAM Journal on Discrete Mathematics",18,10.1137/090771478,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880411616&doi=10.1137%2f090771478&partnerID=40&md5=6e67d74624299a783cfb10531b54d43d","We study a natural network creation game, in which each node locally tries to minimize its local diameter or its local average distance to other nodes by swapping one incident edge at a time. The central question is what structure the resulting equilibrium graphs have, in particular, how well they globally minimize diameter. For the local-average-distance version, we prove an upper bound of 2O( √ lg n), a lower bound of 3, and a tight bound of exactly 2 for trees, and give evidence of a general polylogarithmic upper bound. For the local-diameter version, we prove a lower bound of Ω( √ n) and a tight upper bound of 3 for trees. The same bounds apply, up to constant factors, to the price of anarchy. Our network creation games are closely related to the previously studied unilateral network creation game. The main difference is that our model has no parameter α for the link creation cost, so our results effectively apply for all values of αwithout additional effort; furthermore, equilibrium can be checked in polynomial time in our model, unlike in previous models. Our perspective enables simpler proofs that get at the heart of network creation games. © 2013 Society for Industrial and Applied Mathematics.","Equilibrium; Low diameter; Network creation; Network design; Price of anarchy","Constant factors; Low diameters; Natural networks; Network creation; Network design; Polylogarithmic; Polynomial-time; Price of anarchy; Costs; Forestry; Phase equilibria; Polynomial approximation; Artificial intelligence; Costs; Design; Equilibrium; Forestry; Networks",Conference Paper,Scopus,2-s2.0-84880411616
"Lau Q.P., Lee M.L., Hsu W., Wong T.Y.","Simultaneously identifying all true vessels from segmented retinal images",2013,"IEEE Transactions on Biomedical Engineering",18,10.1109/TBME.2013.2243447,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879899337&doi=10.1109%2fTBME.2013.2243447&partnerID=40&md5=1a3cd264b314e631b2bb502a06813ae3","Measurements of retinal blood vessel morphology have been shown to be related to the risk of cardiovascular diseases. The wrong identification of vessels may result in a large variation of these measurements, leading to a wrong clinical diagnosis. In this paper, we address the problem of automatically identifying true vessels as a postprocessing step to vascular structure segmentation. We model the segmented vascular structure as a vessel segment graph and formulate the problem of identifying vessels as one of finding the optimal forest in the graph given a set of constraints. We design a method to solve this optimization problem and evaluate it on a large real-world dataset of 2446 retinal images. Experiment results are analyzed with respect to actual measurements of vessel morphology. The results show that the proposed approach is able to achieve 98.9% pixel precision and 98.7% recall of the true vessels for clean segmented retinal images, and remains robust even when the segmented image is noisy. © 1964-2012 IEEE.","Ophthalmology; optimal vessel forest; retinal image analysis; simultaneous vessel identification; vascular structure","Actual measurements; Cardio-vascular disease; optimal vessel forest; Optimization problems; Retinal blood vessels; Retinal image analysis; Vascular structures; Vessel identification; Data processing; Diagnosis; Forestry; Morphology; Ophthalmology; Optimization; Problem solving; accuracy; algorithm; article; human; image analysis; morphology; optic disk; retina blood vessel; retina image; Algorithms; Angiography; Artificial Intelligence; Fluorescein Angiography; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Retinal Vessels; Sensitivity and Specificity; Anatomy; Data Processing; Diagnosis; Forestry; Image Analysis; Optimization; Problem Solving",Article,Scopus,2-s2.0-84879899337
"Korytkowski P., Rymaszewski S., Wiåniewski T.","Ant colony optimization for job shop scheduling using multi-attribute dispatching rules",2013,"International Journal of Advanced Manufacturing Technology",18,10.1007/s00170-013-4769-4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888370107&doi=10.1007%2fs00170-013-4769-4&partnerID=40&md5=00066a24221491e71584b7fbd82a6b3b","This paper proposes a heuristic method based on ant colony optimization to determine the suboptimal allocation of dynamic multi-attribute dispatching rules to maximize job shop system performance (four measures were analyzed: mean flow time, max flow time, mean tardiness, and max tardiness). In order to assure high adequacy of the job shop system representation, modeling is carried out using discrete-event simulation. The proposed methodology constitutes a framework of integration of simulation and heuristic optimization. Simulation is used for evaluation of the local fitness function for ants. A case study is used in this paper to illustrate how performance of a job shop production system could be affected by dynamic multi-attribute dispatching rule assignment. © 2013 The Author(s).","Ant colony optimization; Discrete-event simulation; Dynamic job shop; Multi-attribute dispatching rules","Dispatching rules; Heuristic optimization; Job shop; Job shop production; Job-Shop scheduling; Multi-attributes; Sub-optimal allocations; System representation; Algorithms; Artificial intelligence; Discrete event simulation; Heuristic methods; Ant colony optimization",Article,Scopus,2-s2.0-84888370107
"Chen Y., Zheng W.-S., Xu X.-H., Lai J.H.","Discriminant subspace learning constrained by locally statistical uncorrelation for face recognition",2013,"Neural Networks",18,10.1016/j.neunet.2013.01.009,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873930159&doi=10.1016%2fj.neunet.2013.01.009&partnerID=40&md5=3dbd8c1012a5652b2570781c4f91035d","High-dimensionality of data and the small sample size problem are two significant limitations for applying subspace methods which are favored by face recognition. In this paper, a new linear dimension reduction method called locally uncorrelated discriminant projections (LUDP) is proposed, which addresses the two problems from a new aspect. More specifically, we propose a locally uncorrelated criterion, which aims to decorrelate learned discriminant factors over data locally rather than globally. It has been shown that the statistical uncorrelation criterion is an important property for reducing dimension and learning robust discriminant projection as well. However, data are always locally distributed, so it is more important to explore locally statistical uncorrelated discriminant information over data. We impose this new constraint into a graph-based maximum margin analysis, so that LUDP also characterizes the local scatter as well as nonlocal scatter, seeking to find a projection that maximizes the difference, rather than the ratio between the nonlocal scatter and the local scatter. Experiments on ORL, Yale, Extended Yale face database B and FERET face database demonstrate the effectiveness of our proposed method. © 2013 Elsevier Ltd.","Face recognition; Feature extraction; Locally uncorrelated discriminant projections; Maximum margin criterion; Subspace methods","Discriminant informations; FERET face database; Graph-based; High dimensionality; Linear dimension reduction; Locally uncorrelated discriminant projections; Maximum margin; Maximum margin criterions; Nonlocal; Small sample size problems; Sub-space methods; Subspace learning; Yale face database; Feature extraction; Face recognition; article; automated pattern recognition; classification; classifier; data base; discriminant analysis; face; face recognition; information processing; learning algorithm; linear discriminant analysis; locally uncorrelated discriminant projection algorithm; nearest neighbor; priority journal; sample size; support vector machine; Animals; Artificial Intelligence; Biometric Identification; Data Mining; Discriminant Analysis; Face; Humans; Image Interpretation, Computer-Assisted; Models, Theoretical; Subtraction Technique",Article,Scopus,2-s2.0-84873930159
"Yuste A.J., Triviño A., Casilari E.","Type-2 fuzzy decision support system to optimise MANET integration into infrastructure-based wireless systems",2013,"Expert Systems with Applications",18,10.1016/j.eswa.2012.10.063,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873132801&doi=10.1016%2fj.eswa.2012.10.063&partnerID=40&md5=1f1f8e21cc6a8550a3e85bb52c2a3a29","Mobile ad hoc networks are able to extend the coverage area of Internet access points by establishing multihop communication paths. Due to diverse factors such as the mobility of the nodes, the propagation conditions or the traffic status, the communication paths present a lifetime. In fact, the quality of the Internet connection mainly depends on the durability of the employed communication routes. In order to improve the network performance, the nodes should select the best route in terms of its remaining lifetime. Since the factors impacting the route lifetime are unpredictable, the route remaining lifetime cannot be analytically derived. Under these circumstances, a fuzzy-logic system outstands as a potential solution to estimate the stability of the routes. This paper analyses the potentiality of this kind of solution. In particular, the paper presents a fuzzy logic system which should be installed in the mobile nodes to distributedly identify the stable routes. In particular, the system is supported by an interval-based type-2 fuzzy logic. Being a type-2 fuzzy logic system, it is able to cope with inexact estimations. This ability is necessary to avoid the use of additional messages which will occupy the scarce wireless medium. On the other hand, an interval-based fuzzy system provides the simplicity demanded by the energy-constrained mobile devices. As a novelty, the two outputs of the interval-based fuzzy system are employed. The use of each output depends on the traffic state of the mobile node. By means of extensive simulations, we demonstrate the goodness of the proposed system. © 2012 Elsevier B.V. All rights reserved.","Ad hoc networks; Fuzzy decision system; Internet connection","Communication path; Communication route; Coverage area; Energy-constrained; Extensive simulations; Fuzzy decision support system; Fuzzy decision system; Fuzzy logic system; Internet access; Internet connection; Mobile nodes; Multi hop communication; Potential solutions; Route lifetime; Traffic state; Type-2 fuzzy logic; Type-2 fuzzy logic system; Wireless medium; Wireless systems; Ad hoc networks; Artificial intelligence; Communication; Decision support systems; Fuzzy logic; Fuzzy systems; Internet; Internet protocols; Mobile devices; Network performance; Mobile ad hoc networks",Article,Scopus,2-s2.0-84873132801
"Toro C.H.F., Gómez Meire S., Gálvez J.F., Fdez-Riverola F.","A hybrid artificial intelligence model for river flow forecasting",2013,"Applied Soft Computing Journal",18,10.1016/j.asoc.2013.04.014,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878132500&doi=10.1016%2fj.asoc.2013.04.014&partnerID=40&md5=0c5403ae25722e7278adc25f83837011","A hybrid hydrologic estimation model is presented with the aim of performing accurate river flow forecasts without the need of using prior knowledge from the experts in the field. The problem of predicting stream flows is a non-trivial task because the various physical mechanisms governing the river flow dynamics act on a wide range of temporal and spatial scales and almost all the mechanisms involved in the river flow process present some degree of nonlinearity. The proposed system incorporates both statistical and artificial intelligence techniques used at different stages of the reasoning cycle in order to calculate the mean daily water volume forecast of the Salvajina reservoir inflow located at the Department of Cauca, Colombia. The accuracy of the proposed model is compared against other well-known artificial intelligence techniques and several statistical tools previously applied in time series forecasting. The results obtained from the experiments carried out using real data from years 1950 to 2006 demonstrate the superiority of the hybrid system. © 2013 Elsevier B.V. All rights reserved.","Black-box approaches; Case-based reasoning; Hybrid forecasting system; Hydrologic models; River flow forecasting","Artificial intelligence techniques; Black box approach; Degree of non-linearity; Hybrid artificial intelligences; Hybrid forecasting; Hydrologic models; River flow forecasting; Temporal and spatial scale; Artificial intelligence; Hybrid systems; Reservoirs (water); Statistical mechanics; Stream flow; Forecasting",Article,Scopus,2-s2.0-84878132500
"Grandi U., Endriss U.","Lifting integrity constraints in binary aggregation",2013,"Artificial Intelligence",18,10.1016/j.artint.2013.05.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878823366&doi=10.1016%2fj.artint.2013.05.001&partnerID=40&md5=303ecb9d11775486a361baa4730eaed7","We consider problems in which several individuals each need to make a yes/no choice regarding a number of issues and these choices then need to be aggregated into a collective choice. Depending on the application at hand, different combinations of yes/no may be considered rational. We describe rationality assumptions as integrity constraints using a simple propositional language and we explore the question of whether or not a given aggregation procedure will lift a given integrity constraint from the individual to the collective level, i.e., whether the collective choice will be rational whenever all individual choices are. © 2013 Elsevier B.V.","Collective decision making; Combinatorial vote; Computational social choice; Judgment aggregation; Multi-issue domains","Collective decision making; Combinatorial vote; Computational social choices; Individual choice; Integrity constraints; Multi-issue; Propositional language; Artificial intelligence; Decision theory",Article,Scopus,2-s2.0-84878823366
"Cismondi F., Celi L.A., Fialho A.S., Vieira S.M., Reti S.R., Sousa J.M.C., Finkelstein S.N.","Reducing unnecessary lab testing in the ICU with artificial intelligence",2013,"International Journal of Medical Informatics",18,10.1016/j.ijmedinf.2012.11.017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876986122&doi=10.1016%2fj.ijmedinf.2012.11.017&partnerID=40&md5=b16c4cfc21415ed393cbbd7d5253b06a","Objectives: To reduce unnecessary lab testing by predicting when a proposed future lab test is likely to contribute information gain and thereby influence clinical management in patients with gastrointestinal bleeding. Recent studies have demonstrated that frequent laboratory testing does not necessarily relate to better outcomes. Design: Data preprocessing, feature selection, and classification were performed and an artificial intelligence tool, fuzzy modeling, was used to identify lab tests that do not contribute an information gain. There were 11 input variables in total. Ten of these were derived from bedside monitor trends heart rate, oxygen saturation, respiratory rate, temperature, blood pressure, and urine collections, as well as infusion products and transfusions. The final input variable was a previous value from one of the eight lab tests being predicted: calcium, PTT, hematocrit, fibrinogen, lactate, platelets, INR and hemoglobin. The outcome for each test was a binary framework defining whether a test result contributed information gain or not. Patients: Predictive modeling was applied to recognize unnecessary lab tests in a real world ICU database extract comprising 746 patients with gastrointestinal bleeding. Main results: Classification accuracy of necessary and unnecessary lab tests of greater than 80% was achieved for all eight lab tests. Sensitivity and specificity were satisfactory for all the outcomes. An average reduction of 50% of the lab tests was obtained. This is an improvement from previously reported similar studies with average performance 37% by [1-3]. Conclusions: Reducing frequent lab testing and the potential clinical and financial implications are an important issue in intensive care. In this work we present an artificial intelligence method to predict the benefit of proposed future laboratory tests. Using ICU data from 746 patients with gastrointestinal bleeding, and eleven measurements, we demonstrate high accuracy in predicting the likely information to be gained from proposed future lab testing for eight common GI related lab tests. Future work will explore applications of this approach to a range of underlying medical conditions and laboratory tests. © 2012 Elsevier Ireland Ltd.","Blood transfusions; False positive reactions; Harm reduction; Non-linear models; Phlebotomy; Predictive value of tests","Blood transfusion; False positive; Harm reduction; Non-linear model; Phlebotomy; Predictive values; Artificial intelligence; Blood pressure; Forecasting; Intensive care units; Laboratories; Testing; calcium; fibrinogen; hemoglobin; lactic acid; article; artificial intelligence; blood pressure; breathing rate; disease classification; fuzzy logic; gastrointestinal hemorrhage; heart rate; hematocrit; human; intensive care unit; international normalized ratio; laboratory test; outcome assessment; oxygen saturation; priority journal; prothrombin time; sensitivity and specificity; temperature; thrombocyte; transfusion; urine; Artificial Intelligence; Blood Pressure Monitoring, Ambulatory; Blood Transfusion; Female; Gastrointestinal Hemorrhage; Heart Rate; Humans; Intensive Care Units; Laboratories; Male; Models, Statistical; Oxygen; Predictive Value of Tests; Respiration; Sensitivity and Specificity; Temperature",Article,Scopus,2-s2.0-84876986122
"Liang J., Mi J., Wei W., Wang F.","An accelerator for attribute reduction based on perspective of objects and attributes",2013,"Knowledge-Based Systems",18,10.1016/j.knosys.2013.01.027,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875757532&doi=10.1016%2fj.knosys.2013.01.027&partnerID=40&md5=c2d54b50da2423df92ef5d5944d2c41d","Feature selection is an active area of research in pattern recognition, machine learning and artificial intelligence, which greatly improves the performance of forecasting or classification. In rough set theory, attribute reduction, as a special form of feature selection, aims to retain the discernability of the original attribute set. To solve this problem, many heuristic attribute reduction algorithms have been proposed in the literature. However, these methods are computationally time-consuming for large scale datasets. Recently, an accelerator was introduced by computing reducts on gradually reducing the size of the universe. Although the accelerator can considerably shorten the computational time, it remains a challenging issue. To further enhance the efficiency of these algorithms, we develop a new accelerator for attribute reduction, which simultaneously reduces the size of the universe and the number of attributes at each iteration of the process of reduction. Based on the new accelerator, several representative heuristic attribute reduction algorithms are accelerated. Experiments show that these accelerated algorithms can significantly reduce computational time while maintaining their results the same as before. © 2013 Elsevier B.V. All rights reserved.","Accelerating algorithm; Attribute reduction; Feature selection; Large scale data; Rough set","Accelerating algorithm; Active area; Attribute reduction; Attribute reduction algorithm; Attribute sets; Computational time; Large scale data; Large-scale datasets; Acceleration; Artificial intelligence; Feature extraction; Heuristic algorithms; Iterative methods; Learning algorithms; Pattern recognition; Rough set theory",Article,Scopus,2-s2.0-84875757532
"Quan T., Zheng T., Yang Z., Ding W., Li S., Li J., Zhou H., Luo Q., Gong H., Zeng S.","NeuroGPS: Automated localization of neurons for brain circuits using L1 minimization model",2013,"Scientific Reports",18,10.1038/srep01414,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876577065&doi=10.1038%2fsrep01414&partnerID=40&md5=b8aafd90be67e0d7a1751714e369807f","Drawing the map of neuronal circuits at microscopic resolution is important to explain how brain works. Recent progresses in fluorescence labeling and imaging techniques have enabled measuring the whole brain of a rodent like a mouse at submicron-resolution. Considering the huge volume of such datasets, automatic tracing and reconstruct the neuronal connections from the image stacks is essential to form the large scale circuits. However, the first step among which, automated location the soma across different brain areas remains a challenge. Here, we addressed this problem by introducing L1 minimization model. We developed a fully automated system, NeuronGlobalPositionSystem (NeuroGPS) that is robust to the broad diversity of shape, size and density of the neurons in a mouse brain. This method allows locating the neurons across different brain areas without human intervention. We believe this method would facilitate the analysis of the neuronal circuits for brain function and disease studies.",,"algorithm; article; artificial intelligence; audiovisual equipment; automated pattern recognition; biological model; brain; computer assisted diagnosis; computer simulation; connectome; cytology; fluorescence microscopy; methodology; nerve cell; synapse; ultrastructure; Algorithms; Artificial Intelligence; Brain; Computer Simulation; Connectome; Image Interpretation, Computer-Assisted; Microscopy, Fluorescence; Models, Anatomic; Models, Neurological; Neurons; Pattern Recognition, Automated; Synapses",Article,Scopus,2-s2.0-84876577065
"Dimitrov I., Flower D.R., Doytchinova I.","AllerTOP - a server for in silico prediction of allergens",2013,"BMC Bioinformatics",18,10.1186/1471-2105-14-S6-S4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884187982&doi=10.1186%2f1471-2105-14-S6-S4&partnerID=40&md5=31d23a2498538ea2e7b3346f07eba4af","Background: Allergy is a form of hypersensitivity to normally innocuous substances, such as dust, pollen, foods or drugs. Allergens are small antigens that commonly provoke an IgE antibody response. There are two types of bioinformatics-based allergen prediction. The first approach follows FAO/WHO Codex alimentarius guidelines and searches for sequence similarity. The second approach is based on identifying conserved allergenicity-related linear motifs. Both approaches assume that allergenicity is a linearly coded property. In the present study, we applied ACC pre-processing to sets of known allergens, developing alignment-independent models for allergen recognition based on the main chemical properties of amino acid sequences.Results: A set of 684 food, 1,156 inhalant and 555 toxin allergens was collected from several databases. A set of non-allergens from the same species were selected to mirror the allergen set. The amino acids in the protein sequences were described by three z-descriptors (z1, z2and z3) and by auto- and cross-covariance (ACC) transformation were converted into uniform vectors. Each protein was presented as a vector of 45 variables. Five machine learning methods for classification were applied in the study to derive models for allergen prediction. The methods were: discriminant analysis by partial least squares (DA-PLS), logistic regression (LR), decision tree (DT), naïve Bayes (NB) and k nearest neighbours (kNN). The best performing model was derived by kNN at k = 3. It was optimized, cross-validated and implemented in a server named AllerTOP, freely accessible at http://www.pharmfac.net/allertop. AllerTOP also predicts the most probable route of exposure. In comparison to other servers for allergen prediction, AllerTOP outperforms them with 94% sensitivity.Conclusions: AllerTOP is the first alignment-free server for in silico prediction of allergens based on the main physicochemical properties of proteins. Significantly, as well allergenicity AllerTOP is able to predict the route of allergen exposure: food, inhalant or toxin. © 2013 Dimitrov et al.; licensee BioMed Central Ltd.",,"Antibody response; K nearest neighbours (k-NN); Logistic regressions; Machine learning methods; Partial least square (PLS); Physicochemical property; Protein sequences; Sequence similarity; Amino acids; Bioinformatics; Decision trees; Discriminant analysis; Forecasting; Learning systems; Least squares approximations; Proteins; Allergens; allergen; protein; toxin; algorithm; amino acid sequence; article; artificial intelligence; Bayes theorem; biology; chemistry; computer simulation; food allergy; human; hypersensitivity; immunology; methodology; protein database; Algorithms; Allergens; Amino Acid Sequence; Artificial Intelligence; Bayes Theorem; Computational Biology; Computer Simulation; Databases, Protein; Food Hypersensitivity; Humans; Hypersensitivity; Proteins; Toxins, Biological",Article,Scopus,2-s2.0-84884187982
"Feldmann M., Kötzing T.","Optimizing expected path lengths with ant colony optimization using fitness proportional update",2013,"FOGA 2013 - Proceedings of the 12th ACM Workshop on Foundations of Genetic Algorithms",18,10.1145/2460239.2460246,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875997638&doi=10.1145%2f2460239.2460246&partnerID=40&md5=d40543e5e1d42dd6e399e760eaa533c9","We study the behavior of a Max-Min Ant System (MMAS) on the stochastic single-destination shortest path (SDSP) problem. Two previous papers already analyzed this setting for two slightly different MMAS algorithms, where the pheromone update fitness-independently rewards edges of the best-so-far solution. The first paper showed that, when the best-so-far solution is not reevaluated and the stochastic nature of the edge weights is due to noise, the MMAS will find a tree of edges successfully and efficiently identify a shortest path tree with minimal noise-free weights. The second paper used reevaluation of the best-so-far solution and showed that the MMAS finds paths which beat any other path in direct comparisons, if existent. For both results, for some random variables, this corresponds to a tree with minimal expected weights. In this work we analyze a variant of MMAS that works with fitness-proportional update on stochastic-weight graphs with arbitrary random edge weights from [0; 1]. For δ such that any suboptimal path is worse by at least δ than an optimal path, then, with suitable parameters, the graph will be optimized after O ( n3 ln (n=δ)/ δ3) iterations (in expectation). In order to prove the above result, the multiplicative and the variable drift theorem are adapted to continuous search spaces. Copyright © 2013 ACM.","Ant colony optimization; Singledestination shortest path; Stochastic problem; Theory","Max min ant system(MMAS); Random edge weights; Search spaces; Shortest path; Shortest path tree; Stochastic nature; Stochastic problems; Theory; Ant colony optimization; Artificial intelligence; Forestry; Genetic algorithms; Health; Stochastic systems; Graph theory; Algorithms; Optimization; Problem Solving; Random Processes",Conference Paper,Scopus,2-s2.0-84875997638
"Riche N., Mancas M., Culibrk D., Crnojevic V., Gosselin B., Dutoit T.","Dynamic saliency models and human attention: A comparative study on videos",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",18,10.1007/978-3-642-37431-9_45,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875890929&doi=10.1007%2f978-3-642-37431-9_45&partnerID=40&md5=190a790e0a5c0f6b5e379d927462c761","Significant progress has been made in terms of computational models of bottom-up visual attention (saliency). However, efficient ways of comparing these models for still images remain an open research question. The problem is even more challenging when dealing with videos and dynamic saliency. The paper proposes a framework for dynamic-saliency model evaluation, based on a new database of diverse videos for which eye-tracking data has been collected. In addition, we present evaluation results obtained for 4 state-of-the-art dynamic-saliency models, two of which have not been verified on eye-tracking data before. © 2013 Springer-Verlag.",,"Bottom-up visual attentions; Comparative studies; Computational model; Evaluation results; Human attention; Model evaluation; Research questions; Still images; Artificial intelligence; Computer vision",Conference Paper,Scopus,2-s2.0-84875890929
"Rose S.","Mortality risk score prediction in an elderly population using machine learning",2013,"American Journal of Epidemiology",18,10.1093/aje/kws241,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874773224&doi=10.1093%2faje%2fkws241&partnerID=40&md5=872d594c9f96625be2a5bcabe1891fe1","Standard practice for prediction often relies on parametric regression methods. Interesting new methods from the machine learning literature have been introduced in epidemiologic studies, such as random forest and neural networks. However, a priori, an investigator will not know which algorithm to select and may wish to try several. Here I apply the super learner, an ensembling machine learning approach that combines multiple algorithms into a single algorithm and returns a prediction function with the best cross-validated mean squared error. Super learning is a generalization of stacking methods. I used super learning in the Study of Physical Performance and Age-Related Changes in Sonomans (SPPARCS) to predict death among 2,066 residents of Sonoma, California, aged 54 years or more during the period 1993-1999. The super learner for predicting death (risk score) improved upon all single algorithms in the collection of algorithms, although its performance was similar to that of several algorithms. Super learner outperformed the worst algorithm (neural networks) by 44% with respect to estimated cross-validated mean squared error and had an R2 value of 0.201. The improvement of super learner over random forest with respect to R2 was approximately 2-fold. Alternatives for risk score prediction include the super learner, which can provide improved performance. © 2013 © The Author 2013. Published by Oxford University Press on behalf of the Johns Hopkins Bloomberg School of Public Health. All rights reserved. For permissions, please e-mail: journals.permissions@oup.com.","aging; estimation techniques; machine learning; mortality; regression analysis","aging population; algorithm; epidemiology; health risk; machinery; mortality; prediction; risk factor; adult; aged; algorithm; article; artificial neural network; female; geriatrics; human; human experiment; machine learning; male; mortality; performance; prediction; risk assessment; scoring system; study of physical performance and age related changes in sonomans method; United States; Aged; Aged, 80 and over; Algorithms; Artificial Intelligence; California; Epidemiologic Methods; Humans; Middle Aged; Mortality; Regression Analysis; Risk Assessment; California; United States",Article,Scopus,2-s2.0-84874773224
"Chávez-Ramírez A.U., Vallejo-Becerra V., Cruz J.C., Ornelas R., Orozco G., Muñoz-Guerrero R., Arriaga L.G.","A hybrid power plant (Solar-Wind-Hydrogen) model based in artificial intelligence for a remote-housing application in Mexico",2013,"International Journal of Hydrogen Energy",18,10.1016/j.ijhydene.2012.11.140,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873130494&doi=10.1016%2fj.ijhydene.2012.11.140&partnerID=40&md5=ef1b4cc7671af16588d8469ae745ee5d","World fossil fuel reserve is expected to be exhausted in coming few decades. Therefore, the decentralization of energy production requires the design and integration of different energy sources and conversion technologies to meet the power demand for single remote housing applications in a sustainable way under various weather conditions. This work focuses on the integration of photovoltaic (PV) system, micro-wind turbine (WT), Polymeric Exchange Membrane Fuel Cell (PEM-FC) stack and PEM water electrolyzer (PEM-WE), for a sustained power generation system (2.5 kW). The main contribution of this work is the hybridization of alternate energy sources with the hydrogen conversion systems using mid-term and short-term storage models based in artificial intelligence techniques built from experimental data (measurements obtained from the site of interest), this models allow to obtain better accuracy in performance prediction (PVMSE = 8.4%, PEM-FCMSE = 2.4%, PEM-WEMSE = 1.96%, GSRMSE = 7.9%, WTMSE = 14%) with a practical design and dynamic under intelligent control strategies to build an autonomous system. Copyright © 2012, Hydrogen Energy Publications, LLC. Published by Elsevier Ltd. All rights reserved.","Artificial intelligence; Electrolyzer; Fuel cell; Hybrid power generation plant; Photovoltaic; Wind turbine","Artificial intelligence techniques; Autonomous systems; Conversion systems; Conversion technology; Electrolyzers; Energy productions; Energy source; Exchange membranes; Hybrid power generation; Hybrid power plants; Me-xico; Micro-wind turbines; Model-based OPC; Performance prediction; Photovoltaic; Photovoltaic systems; Power demands; Power generation systems; Site of interests; Storage model; Weather conditions; Artificial intelligence; Digital storage; Electrolytic cells; Fossil fuels; Fuel cells; Housing; Hydrogen; Hydrogen storage; Proven reserves; Water supply; Wind turbines; Solar power generation",Article,Scopus,2-s2.0-84873130494
"Sokolova M.V., Serrano-Cuerda J., Castillo J.C., Fernández-Caballero A.","A fuzzy model for human fall detection in infrared video",2013,"Journal of Intelligent and Fuzzy Systems",18,10.3233/IFS-2012-0548,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873578374&doi=10.3233%2fIFS-2012-0548&partnerID=40&md5=43d766c794567b48012ce8d3b82b814c","Fall detection, especially for elderly people, is a challenging problem which demands new products and technologies. In this paper a fuzzy model for fall detection and inactivity monitoring in infrared video is presented. The classification features proposed include geometric and kinematic parameters associated with more or less sudden changes in the tracked human-related regions of interest. A complete segmentation and tracking algorithm for infrared video as well as a fuzzy fall detection and confirmation algorithm are introduced. The proposed system is capable of identifying true and false falls, enhanced with inactivity monitoring aimed at confirming the need for medical assistance and/or care. The fall indicators used as well as their fuzzy model is explained in detail. The fuzzy model has been tested for a wide number of static and dynamic falls, demonstrating exciting initial results. © 2013-IOS Press and the authors. All rights reserved.","Fall detection; fuzzy system; infrared video; video segmentation and tracking","Classification features; Elderly people; Fall detection; Fuzzy models; Human fall detection; Infrared video; Kinematic parameters; New product; Regions of interest; Segmentation and tracking; Static and dynamic; Sudden change; Video segmentation; Artificial intelligence; Fuzzy systems; Image processing; Algorithms",Article,Scopus,2-s2.0-84873578374
"Varnavas A., Carrell T., Penney G.","Increasing the automation of a 2D-3D registration system",2013,"IEEE Transactions on Medical Imaging",18,10.1109/TMI.2012.2227337,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873291158&doi=10.1109%2fTMI.2012.2227337&partnerID=40&md5=8e42cbac85063484c888b2ebcfed2920","Routine clinical use of 2D-3D registration algorithms for Image Guided Surgery remains limited. A key aspect for routine clinical use of this technology is its degree of automation, i.e., the amount of necessary knowledgeable interaction between the clinicians and the registration system. Current image-based registration approaches usually require knowledgeable manual interaction during two stages: for initial pose estimation and for verification of produced results. We propose four novel techniques, particularly suited to vertebra-based registration systems, which can significantly automate both of the above stages. Two of these techniques are based upon the intraoperative 'insertion' of a virtual fiducial marker into the preoperative data. The remaining two techniques use the final registration similarity value between multiple CT vertebrae and a single fluoroscopy vertebra. The proposed methods were evaluated with data from 31 operations (31 CT scans, 419 fluoroscopy images). Results show these methods can remove the need for manual vertebra identification during initial pose estimation, and were also very effective for result verification, producing a combined true positive rate of 100% and false positive rate equal to zero. This large decrease in required knowledgeable interaction is an important contribution aiming to enable more widespread use of 2D-3D registration technology. © 2012 IEEE.","2D-3D registration; Image guided surgery; registration verification","2-D-3-D registration; Clinical use; Degree of automation; False positive rates; Fiducial marker; Fluoroscopy images; Image guided surgery; Image-based; Intra-operative; Manual interaction; Novel techniques; Pose estimation; Registration systems; True positive rates; Algorithms; Computerized tomography; Fluorescent screens; Musculoskeletal system; Image registration; algorithm; aorta aneurysm; article; automation; clinical practice; computer assisted tomography; endovascular aneurysm repair; fiducial marker; fluoroscopy; human; image quality; registration; retrospective study; validation process; vertebra; algorithm; artificial intelligence; automated pattern recognition; computer assisted diagnosis; computer assisted surgery; image enhancement; image subtraction; methodology; reproducibility; sensitivity and specificity; three dimensional imaging; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique; Surgery, Computer-Assisted",Article,Scopus,2-s2.0-84873291158
"Kumari S., Agrawal M., Tiwari S.","Impact of elevated CO2 and elevated O3 on Beta vulgaris L.: Pigments, metabolites, antioxidants, growth and yield",2013,"Environmental Pollution",18,10.1016/j.envpol.2012.11.021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872077932&doi=10.1016%2fj.envpol.2012.11.021&partnerID=40&md5=02b7b519fba30040dd034764234ee68f","The present study was conducted to assess morphological, biochemical and yield responses of palak (Beta vulgaris L. cv Allgreen) to ambient and elevated levels of CO2 and O3, alone and in combination. As compared to the plants grown in charcoal filtered air (ACO2), growth and yield of the plants increased under elevated CO2 (ECO 2) and decreased under combination of ECO2 with elevated O3 (ECO2 + EO3), ambient O3 (ACO2 + AO3) and elevated O3 (EO3). Lipid peroxidation, ascorbic acid, catalase and glutathione reductase activities enhanced under all treatments and were highest in EO3. Foliar starch and organic carbon contents increased under ECO2 and ECO2 + EO3 and reduced under EO3 and ACO2 + AO 3. Foliar N content declined in all treatments compared to ACO 2 resulting in alteration of C/N ratio. This study concludes that ambient level of CO2 is not enough to counteract O3 impact, but elevated CO2 has potential to counteract the negative effects of future O3 level. © 2012 Elsevier Ltd. All rights reserved.","Antioxidants; Elevated CO2; Elevated O3; Growth; Lipid peroxidation; Pigment; Yield","Antioxidants; Elevated CO; Elevated O<sub>3</sub>; Lipid peroxidation; Yield; Artificial intelligence; Charcoal; Forestry; Growth (materials); Lipids; Organic acids; Pigments; Carbon dioxide; ascorbic acid; carbon dioxide; catalase; glutathione reductase; malonaldehyde; organic carbon; ozone; antioxidant; ascorbic acid; carbon dioxide; ozone; antioxidant; carbon dioxide; carbon emission; crop yield; environmental stress; growth response; lipid; metabolite; pigment; air pollution; antioxidant activity; article; beet; biomass; chlorophyll content; enzyme activity; lipid peroxidation; nonhuman; plant growth; plant leaf; plant root; plant yield; root length; air pollutant; drug effect; growth, development and aging; metabolism; photosynthesis; physiology; Beta vulgaris; Air Pollutants; Antioxidants; Ascorbic Acid; Beta vulgaris; Carbon Dioxide; Lipid Peroxidation; Ozone; Photosynthesis; Plant Leaves",Article,Scopus,2-s2.0-84872077932
"Lin Y., Tang Y.Y., Fang B., Shang Z., Huang Y., Wang S.","A visual-attention model using earth mover's distance-based saliency measurement and nonlinear feature combination",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",18,10.1109/TPAMI.2012.119,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871765802&doi=10.1109%2fTPAMI.2012.119&partnerID=40&md5=742473ba2ac073b3a9a51088da29de4c","This paper introduces a new computational visual-attention model for static and dynamic saliency maps. First, we use the Earth Mover's Distance (EMD) to measure the center-surround difference in the receptive field, instead of using the Difference-of-Gaussian filter that is widely used in many previous visual-attention models. Second, we propose to take two steps of biologically inspired nonlinear operations for combining different features: combining subsets of basic features into a set of super features using the Lm-norm and then combining the super features using the Winner-Take-All mechanism. Third, we extend the proposed model to construct dynamic saliency maps from videos by using EMD for computing the center-surround difference in the spatiotemporal receptive field. We evaluate the performance of the proposed model on both static image data and video data. Comparison results show that the proposed model outperforms several existing models under a unified evaluation setting. © 2012 IEEE.","dynamic saliency maps; earth mover's distance (EMD); saliency maps; spatiotemporal receptive field (STRF); Visual attention","Biologically inspired; Comparison result; Distance-based; Earth Mover's distance; Nonlinear feature combination; Nonlinear operation; Receptive fields; Saliency map; Spatiotemporal receptive field; Static and dynamic; Static images; Video data; Visual Attention; Winner take alls; Behavioral research; Image segmentation; algorithm; article; artificial intelligence; attention; automated pattern recognition; biological model; biomimetics; computer assisted diagnosis; computer simulation; eye fixation; human; methodology; nonlinear system; pattern recognition; physiology; reproducibility; sensitivity and specificity; Algorithms; Artificial Intelligence; Attention; Biomimetics; Computer Simulation; Fixation, Ocular; Humans; Image Interpretation, Computer-Assisted; Models, Biological; Nonlinear Dynamics; Pattern Recognition, Automated; Pattern Recognition, Visual; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84871765802
"Sparks R., Madabhushi A.","Statistical shape model for manifold regularization: Gleason grading of prostate histology",2013,"Computer Vision and Image Understanding",18,10.1016/j.cviu.2012.11.011,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885381855&doi=10.1016%2fj.cviu.2012.11.011&partnerID=40&md5=8d765ef8fd9f6ab7363a682b6f01e72c","Gleason patterns of prostate cancer histopathology, characterized primarily by morphological and architectural attributes of histological structures (glands and nuclei), have been found to be highly correlated with disease aggressiveness and patient outcome. Gleason patterns 4 and 5 are highly correlated with more aggressive disease and poorer patient outcome, while Gleason patterns 1-3 tend to reflect more favorable patient outcome. Because Gleason grading is done manually by a pathologist visually examining glass (or digital) slides subtle morphologic and architectural differences of histological attributes, in addition to other factors, may result in grading errors and hence cause high inter-observer variability. Recently some researchers have proposed computerized decision support systems to automatically grade Gleason patterns by using features pertaining to nuclear architecture, gland morphology, as well as tissue texture. Automated characterization of gland morphology has been shown to distinguish between intermediate Gleason patterns 3 and 4 with high accuracy. Manifold learning (ML) schemes attempt to generate a low dimensional manifold representation of a higher dimensional feature space while simultaneously preserving nonlinear relationships between object instances. Classification can then be performed in the low dimensional space with high accuracy. However ML is sensitive to the samples contained in the dataset; changes in the dataset may alter the manifold structure. In this paper we present a manifold regularization technique to constrain the low dimensional manifold to a specific range of possible manifold shapes. The range being determined via a statistical shape model of manifolds (SSMM). In this work we demonstrate applications of the SSMM in (1) identifying samples on the manifold which contain noise, defined as those samples which deviate from the SSMM, and (2) accurate out-of-sample extrapolation (OSE) of newly acquired samples onto a manifold constrained by the SSMM. We demonstrate these applications of the SSMM in the context of distinguish between Gleason patterns 3 and 4 using glandular morphologic features in a prostate histopathology dataset of 58 patient studies. Identifying and eliminating noisy samples from the manifold via the SSMM results in a statistically significant improvement in area under the receiver operator characteristic curve (AUC), 0.832 ± 0.048 with removal of noisy samples compared to a AUC of 0.779 ± 0.075 without removal of samples. The use of the SSMM for OSE of newly acquired glands also shows statistically significant improvement in AUC, 0.834 ± 0.051 with the SSMM compared to 0.779 ± 0.054 without the SSMM. Similar results were observed for the synthetic Swiss Roll and Helix datasets. © 2013 Elsevier Inc. All rights reserved.","Gleason grading; Manifold learning; Prostate histology; Regularization; Statistical shape models","Artificial intelligence; Decision support systems; Diseases; Histology; Urology; Gleason grading; Higher dimensional features; Low-dimensional manifolds; Manifold learning; Out-of-sample extrapolations; Receiver operator characteristic curves; Regularization; Statistical shape model; Grading",Article,Scopus,2-s2.0-84885381855
"Mohanty I., Kalita J., Das S., Pahwa A., Buehler E.","Ant algorithms for the optimal restoration of distribution feeders during cold load pickup",2013,"2003 IEEE Swarm Intelligence Symposium, SIS 2003 - Proceedings",18,10.1109/SIS.2003.1202258,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942097611&doi=10.1109%2fSIS.2003.1202258&partnerID=40&md5=374a54dca663d17915bb285b8384d0ab","The ant colony algorithm is a new technique for combinatorial optimization borrowed from swarm intelligence. This paper outlines an ant colony algorithm to compute the optimal order of restoring sections in a power distribution system. Restoration of distribution feeders after long interruptions creates cold load pickup conditions due to loss of diversity among the loads. The distribution system load may have to be restored step-by-step using sectionalizing switches under such conditions to prevent overheating of substation transformer. The restoration time is dependent on the order in which sections are restored. Results obtained using this method for two test cases are presented including a comparison with the simulated annealing algorithm. © 2003 IEEE.","Ant colony optimization; Computer science; Distributed computing; Load modeling; Power engineering and energy; Power engineering computing; Power system restoration; Springs; Switches; Water heating","Ant colony optimization; Artificial intelligence; Combinatorial optimization; Computer aided engineering; Computer science; Current limiting reactors; Distributed computer systems; Electric load management; Pickups; Restoration; Simulated annealing; Springs (components); Switches; Transformer substations; Load modeling; Power engineering and energies; Power engineering computing; Power system restoration; Water heating; Algorithms",Conference Paper,Scopus,2-s2.0-84942097611
"Huang X., Soergel D.","Relevance: An improved framework for explicating the notion",2013,"Journal of the American Society for Information Science and Technology",18,10.1002/asi.22811,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871211882&doi=10.1002%2fasi.22811&partnerID=40&md5=20fa29d2d41ac1e3478b5d85f74e566c","Synthesizing and building on many ideas from the literature, this article presents an improved conceptual framework that clarifies the notion of relevance with its many elements, variables, criteria, and situational factors. Relevance is defined as a Relationship (R) between an Information Object (I) and an Information Need (N) (which consists of Topic, User, Problem/Task, and Situation/Context) with focus on R. This defines Relevance-as-is (conceptual relevance, strong relevance). To determine relevance, an Agent A (a person or system) operates on a representation I′ of the information object and a representation N′ of the information need, resulting in relevance-as-determined (operational measure of relevance, weak relevance, an approximation). Retrieval tests compare relevance-as-determined by different agents. This article discusses and compares two major approaches to conceptualizing relevance: the entity-focused approach (focus on elaborating the entities involved in relevance) and the relationship-focused approach (focus on explicating the relational nature of relevance). The article argues that because relevance is fundamentally a relational construct the relationship-focused approach deserves a higher priority and more attention than it has received. The article further elaborates on the elements of the framework with a focus on clarifying several critical issues on the discourse on relevance. © 2012 ASIS&T.","aboutness; relevance","aboutness; Conceptual frameworks; Critical issues; Information need; Information object; relevance; Situational factors; Artificial intelligence; Software engineering; Information science",Article,Scopus,2-s2.0-84871211882
"Chang W.-D.","Nonlinear CSTR control system design using an artificial bee colony algorithm",2013,"Simulation Modelling Practice and Theory",18,10.1016/j.simpat.2012.11.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871205918&doi=10.1016%2fj.simpat.2012.11.002&partnerID=40&md5=35d8e7737a2c2db030b1be70aaa291db","This paper proposes a novel controller design method based on using artificial bee colony (ABC) algorithms for an unstable nonlinear continuously stirred tank reactor (CSTR) chemical system. Such CSTR process is highly nonlinear and its dynamic is significantly dominated by system parameters. It is a good challenge to access the controller design performance when the controller is applied in the CSTR control system. The commonly used proportional-integral-derivative (PID) controller is taken into account in this study, and tuning three PID control gains is carried out by the artificial bee colony algorithm. With the use of the optimal ABC algorithm, PID controller gains can be derived suitably by means of minimizing the cost function given in advance. Finally, several control operations are provided to confirm the feasibility and effectiveness of the proposed method. We also discuss the influence of algorithm initial conditions on the control performance with many different tests. © 2012 Elsevier B.V. All rights reserved.","Artificial bee colony (ABC) algorithm; Continuously stirred tank reactor (CSTR); PID controller; Swarm intelligence","Abc algorithms; Artificial bee colonies; Artificial bee colony algorithms; Chemical systems; Continuously stirred tank reactor; Control gains; Control operations; Control performance; Controller design method; Controller designs; Initial conditions; PID controllers; Proportional integral derivative controllers; Swarm Intelligence; Artificial intelligence; Electric control equipment; Evolutionary algorithms; Tanks (containers); Three term control systems; Controllers",Article,Scopus,2-s2.0-84871205918
"Vasseur V., Kamp L.M., Negro S.O.","A comparative analysis of Photovoltaic Technological Innovation Systems including international dimensions: The cases of Japan and the Netherlands",2013,"Journal of Cleaner Production",18,10.1016/j.jclepro.2013.01.017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879904940&doi=10.1016%2fj.jclepro.2013.01.017&partnerID=40&md5=2280053b4250e21800bd7b79301d4fdf","This paper investigates the development and diffusion of photovoltaic (PV) technology in Japan and The Netherlands. Both cases are analysed with the Technological Innovation Systems (TIS) framework, which focuses on a particular technology and includes all those factors that influence the development and diffusion of a technology. This framework proposes seven System Functions: key processes that need to be fulfilled for a TIS to function well. We include international factors in the framework such as import share, export activities, foreign direct investment and international knowledge exchange. Japan is found to have a well functioning PV TIS with a strong industry and a large market for PV, whereas the Dutch have so far established considerably less. Based on the differences in the cases we formulate a number of lessons for the Dutch PV TIS. 1) System functions such as guidance of the search (F4) and advocacy coalitions (F7) need to be strengthened in order to reinforce each other in a positive way; 2) formulate, maintain and translate consistent long-term goals into long-term policies and stimulation measures in order to allow a gradual build-up of networks and skills/expertise; 3) build up and maintain legitimacy within the institutional setting; and 4) strengthen the TIS in order for the SMEs to grow in terms of capital and capabilities. © 2013 Elsevier Ltd. All rights reserved.","International dimensions; Japan; Photovoltaics; Technological Innovation Systems; The Netherlands","Knowledge management; Transfer functions; International dimensions; Japan; Netherlands; Photovoltaics; Technological innovation systems; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84879904940
"Goodman N.D.","The principles and practice of probabilistic programming",2013,"ACM SIGPLAN Notices",18,10.1145/2480359.2429117,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877879156&doi=10.1145%2f2480359.2429117&partnerID=40&md5=a65c22331e46c8632d4e094de0a104c5","Probabilities describe degrees of belief, and probabilistic inference describes rational reasoning under uncertainty. It is no wonder, then, that probabilistic models have exploded onto the scene of modern artificial intelligence, cognitive science, and applied statistics. These are all sciences of inference under uncertainty. But as probabilistic models have become more sophisticated, the tools to formally describe them and to perform probabilistic inference have wrestled with new complexity. For an example of this, one needs look no further than the fundamental operation for inference, probabilistic conditioning, which forms a posterior distribution over executions from the prior distribution specified by the program. A wide variety of probabilistic models are useful for diverse tasks, including unsupervised learning, vision, planning, and statistical model selection.","Probabilistic models; Probabilistic programs","Fundamental operations; Posterior distributions; Principles and practices; Probabilistic inference; Probabilistic models; Probabilistic programming; Probabilistic programs; Reasoning under uncertainty; Artificial intelligence; Probability distributions",Conference Paper,Scopus,2-s2.0-84877879156
"Hu B., Chen Y., Keogh E.","Time series classification under more realistic assumptions",2013,"SIAM International Conference on Data Mining 2013, SMD 2013",18,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960490697&partnerID=40&md5=89f9b5ef35e276d826cafb380101d21e","Most literature on time series classification assumes that the beginning and ending points of the pattern of interest can be correctly identified, both during the training phase and later deployment. In this work, we argue that this assumption is unjustified, and this has in many cases led to unwarranted optimism about the performance of the proposed algorithms. As we shall show, the task of correctly extracting individual gait cycles, heartbeats, gestures, behaviors, etc., is generally much more difficult than the task of actually classifying those patterns. We propose to mitigate this problem by introducing an alignment-free time series classification framework. The framework requires only very weakly annotated data, such as ""in this ten minutes of data, we see mostly normal heartbeats."" and by generalizing the classic machine learning idea of data editing to streaming/continuous data, allows us to build robust, fast and accurate classifiers. We demonstrate on several diverse real-world problems that beyond removing unwarranted assumptions and requiring essentially no human intervention, our framework is both significantly faster and significantly more accurate than current state-of-the-art approaches.",,"Artificial intelligence; Data mining; Learning systems; Alignment-free; Data editing; Human intervention; Real-world problem; State-of-the-art approach; Time series classifications; Training phase; Weakly annotated data; Time series",Conference Paper,Scopus,2-s2.0-84960490697
"Lizier J.T., Flecker B., Williams P.L.","Towards a synergy-based approach to measuring information modification",2013,"IEEE Symposium on Artificial Life (ALIFE)",18,10.1109/ALIFE.2013.6602430,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84931384462&doi=10.1109%2fALIFE.2013.6602430&partnerID=40&md5=5ad3dac07e6035542114b717053b0f1c","Distributed computation in artificial life and complex systems is often described in terms of component operations on information: information storage, transfer and modification. Information modification remains poorly described however, with the popularly-understood examples of glider and particle collisions in cellular automata being only quantitatively identified to date using a heuristic (separable information) rather than a proper information-theoretic measure.We outline how a recentlyintroduced axiomatic framework for measuring information redundancy and synergy, called partial information decomposition, can be applied to a perspective of distributed computation in order to quantify component operations on information. Using this framework, we propose a new measure of information modification that captures the intuitive understanding of information modification events as those involving interactions between two or more information sources. We also consider how the local dynamics of information modification in space and time could be measured, and suggest a new axiom that redundancy measures would need to meet in order to make such local measurements. Finally, we evaluate the potential for existing redundancy measures to meet this localizability axiom. © 2013 IEEE.",,"Artificial intelligence; Distributed computer systems; Information theory; Mobile security; Redundancy; Axiomatic framework; Distributed computations; Information redundancies; Information sources; Intuitive understanding; Measure of information; Partial information; Particle collision; Information use",Conference Paper,Scopus,2-s2.0-84931384462
"Zou B., Li L., Xu Z., Luo T., Tang Y.Y.","Generalization performance of fisher linear discriminant based on markov sampling",2013,"IEEE Transactions on Neural Networks and Learning Systems",18,10.1109/TNNLS.2012.2230406,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894071719&doi=10.1109%2fTNNLS.2012.2230406&partnerID=40&md5=b7094bae7213df478902dd1b506e0e3e","Fisher linear discriminant (FLD) is a well-known method for dimensionality reduction and classification that projects high-dimensional data onto a low-dimensional space where the data achieves maximum class separability. The previous works describing the generalization ability of FLD have usually been based on the assumption of independent and identically distributed (i.i.d.) samples. In this paper, we go far beyond this classical framework by studying the generalization ability of FLD based on Markov sampling. We first establish the bounds on the generalization performance of FLD based on uniformly ergodic Markov chain (u.e.M.c.) samples, and prove that FLD based on u.e.M.c. samples is consistent. By following the enlightening idea from Markov chain Monto Carlo methods, we also introduce a Markov sampling algorithm for FLD to generate u.e.M.c. samples from a given data of finite size. Through simulation studies and numerical studies on benchmark repository using FLD, we find that FLD based on u.e.M.c. samples generated by Markov sampling can provide smaller misclassification rates compared to i.i.d. samples. © 2012 IEEE.","Fisher linear discriminant (FLD); Generalization performance; Markov sampling; Uniformly ergodic Markov chain","Artificial intelligence; Computer networks; Dimensionality reduction; Ergodic markov chains; Fisher linear discriminants; Generalization ability; Generalization performance; High dimensional data; Low-dimensional spaces; Misclassification rates; Markov processes",Article,Scopus,2-s2.0-84894071719
"Tsukada T., Tamura T., Kitagawa S., Fukuyama Y.","Optimal operational planning for cogeneration system using particle swarm optimization",2013,"2003 IEEE Swarm Intelligence Symposium, SIS 2003 - Proceedings",18,10.1109/SIS.2003.1202259,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942090201&doi=10.1109%2fSIS.2003.1202259&partnerID=40&md5=1ceae75ac230a84f707f086c31bc2290","This paper proposes optimal operational planning for a cogeneration system (CGS) using particle swarm optimization (PSO). CGS is usually connected to various facilities such as refrigerators, reservoirs, and cooling towers. In order to generate optimal operational planning for CGS, startup and shutdown status, and input values of the facilities for each control interval should be determined. The facilities may have nonlinear input-output characteristics. Therefore, the problem can be formulated as a mixed-integer nonlinear optimization problem (MINLP). PSO can be easily expanded to treat MINLP. The simple expansion of PSO for the optimal generation system operational planning problem is proposed and the proposed method is applied to typical CGS planning problems with promising results. © 2003 IEEE.","Carbon capture and storage; Cogeneration; Content addressable storage; Cooling; Fuels; Optimal control; Particle swarm optimization; Poles and towers; Refrigerators; Reservoirs","Artificial intelligence; Associative storage; Carbon; Carbon capture; Cogeneration plants; Cooling; Fuel storage; Fueling; Integer programming; Nonlinear programming; Optimization; Petroleum reservoirs; Plant shutdowns; Refrigerators; Carbon capture and storage; Cogeneration; Cogeneration systems; Mixed-integer nonlinear optimization problems; Operational planning; Optimal controls; Optimal operational planning; Poles and towers; Particle swarm optimization (PSO)",Conference Paper,Scopus,2-s2.0-84942090201
"Fedor P., Perdukova D.","Energy optimization of a dynamic system controller",2013,"Advances in Intelligent Systems and Computing",18,10.1007/978-3-642-33018-6_37,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868135873&doi=10.1007%2f978-3-642-33018-6_37&partnerID=40&md5=11056930ec2a77cfe022dd4963f56728","The paper deals with the application of fuzzy logic in the method of designing the parameters of a continuous dynamic system controller that is energetically optimal and at the same time meets the desired dynamic control parameters. The suitable controller parameters are established on the basis of a fuzzy model of the system, generated through its identification from the measured inputs and outputs. The proposed method has been verified by simulations on an example of parameter design for a PI controller of a DC drive with non-linear load. In comparison with a standardly designed PI controller with constant parameters for the whole operational space of the DC drive it is possible to save approximately 24.39 % of electric power at each dynamic motion of the drive. © 2013 Springer-Verlag Berlin Heidelberg.",,"Artificial intelligence; Fuzzy logic; Soft computing; Constant parameters; Continuous dynamic systems; Controller parameter; DC drives; Dynamic controls; Dynamic motions; Electric power; Energy optimization; Fuzzy models; Nonlinear load; Operational space; Parameter designs; PI Controller; System controllers; Optimization",Conference Paper,Scopus,2-s2.0-84868135873
"Kirchner E.A., Kim S.K., Straube S., Seeland A., Wöhrle H., Krell M.M., Tabie M., Fahle M.","On the applicability of brain reading for predictive human-machine interfaces in robotics",2013,"PLoS ONE",17,10.1371/journal.pone.0081732,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891528944&doi=10.1371%2fjournal.pone.0081732&partnerID=40&md5=56c2471b6bb04d518cc1c7d3332a5583","The ability of today's robots to autonomously support humans in their daily activities is still limited. To improve this, predictive human-machine interfaces (HMIs) can be applied to better support future interaction between human and machine. To infer upcoming context-based behavior relevant brain states of the human have to be detected. This is achieved by brain reading (BR), a passive approach for single trial EEG analysis that makes use of supervised machine learning (ML) methods. In this work we propose that BR is able to detect concrete states of the interacting human. To support this, we show that BR detects patterns in the electroencephalogram (EEG) that can be related to event-related activity in the EEG like the P300, which are indicators of concrete states or brain processes like target recognition processes. Further, we improve the robustness and applicability of BR in application-oriented scenarios by identifying and combining most relevant training data for single trial classification and by applying classifier transfer. We show that training and testing, i.e., application of the classifier, can be carried out on different classes, if the samples of both classes miss a relevant pattern. Classifier transfer is important for the usage of BR in application scenarios, where only small amounts of training examples are available. Finally, we demonstrate a dual BR application in an experimental setup that requires similar behavior as performed during the teleoperation of a robotic arm. Here, target recognition processes and movement preparation processes are detected simultaneously. In summary, our findings contribute to the development of robust and stable predictive HMIs that enable the simultaneous support of different interaction behaviors. © 2013 Kirchner et al.",,"adult; article; behavior; brain function; brain reading; classifier; electroencephalogram; event related potential; human; human experiment; human machine interface; information processing; machine learning; male; motor performance; normal human; prediction; recognition; robotics; Artificial Intelligence; Brain; Brain Mapping; Electroencephalography; Humans; Robotics; User-Computer Interface",Article,Scopus,2-s2.0-84891528944
"Park S., Kim Y.-D., Choi S.","Hierarchical Bayesian matrix factorization with side information",2013,"IJCAI International Joint Conference on Artificial Intelligence",17,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896061714&partnerID=40&md5=33d2ec13e57389b5287cf8d75f3bf4c2","Bayesian treatment of matrix factorization has been successfully applied to the problem of collaborative prediction, where unknown ratings are determined by the predictive distribution, inferring posterior distributions over user and item factor matrices that are used to approximate the user-item matrix as their product. In practice, however, Bayesian matrix factorization suffers from cold-start problems, where inferences are required for users or items about which a sufficient number of ratings are not gathered. In this paper we present a method for Bayesian matrix factorization with side information, to handle cold-start problems. To this end, we place Gaussian-Wishart priors on mean vectors and precision matrices of Gaussian user and item factor matrices, such that mean of each prior distribution is regressed on corresponding side information. We develop variational inference algorithms to approximately compute posterior distributions over user and item factor matrices. In addition, we provide Bayesian Cramér-Rao Bound for our model, showing that the hierarchical Bayesian matrix factorization with side information improves the reconstruction over the standard Bayesian matrix factorization where the side information is not used. Experiments on MovieLens data demonstrate the useful behavior of our model in the case of cold-start problems.",,"Bayesian matrix factorizations; Cold start problems; Collaborative predictions; Hierarchical bayesian; Matrix factorizations; Posterior distributions; Predictive distributions; Variational inference; Artificial intelligence; Inference engines; Matrix algebra",Conference Paper,Scopus,2-s2.0-84896061714
"Tseng K.C., Hsu C.-L., Chuang Y.-H.","Designing an intelligent health monitoring system and exploring user acceptance for the elderly",2013,"Journal of Medical Systems",17,10.1007/s10916-013-9967-y,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883672866&doi=10.1007%2fs10916-013-9967-y&partnerID=40&md5=c230d78be010ca431241a5c71bab2e54","Recently, many healthcare or health monitoring systems are proposed to improve life quality of the elderly in the aging process. The elderly are generally with poor health and low information literacy. Low information literacy might be an obstacle of using such systems. This research considered the characteristics and the needs of the elderly and developed an intelligent health monitoring system for the elderly with low information literacy living in the nursing home. The system is intelligent since it can monitor the health status of the elderly based on clinical and medical knowledge, provide an easy-to-understand and easy-to-use user interface for the elderly, and automatically send important or emergency feedback to caregivers. Finally, we explored the user acceptance for the elderly using our proposed system based on the unified theory of acceptance and user of technology model. The experimental results indicate the developed system is highly accepted by the elderly in terms of performance expectation, endeavor expectation, social influence, and facilitating condition. © 2013 Springer Science+Business Media New York.","Elderly; Health monitoring; Nursing home; User acceptance; Vital sign","aged; article; behavior; caregiver; communication protocol; computer interface; content validity; convergent validity; elderly care; expectation; face validity; feedback system; female; health care need; health monitoring system; human; hypothesis; information literacy; information technology; male; nursing home; program acceptability; questionnaire; reliability; social aspect; telemonitoring; wireless communication; artificial intelligence; devices; home for the aged; Internet; middle aged; nursing home; patient education; physiologic monitoring; procedures; quality of life; social support; Taiwan; Aged; Artificial Intelligence; Female; Homes for the Aged; Humans; Internet; Male; Middle Aged; Monitoring, Physiologic; Nursing Homes; Patient Education as Topic; Quality of Life; Social Support; Taiwan; Wireless Technology",Article,Scopus,2-s2.0-84883672866
"Shao H., Marwah M., Ramakrishnan N.","A temporal motif mining approach to unsupervised energy disaggregation: Applications to residential and commercial buildings",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",17,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893384215&partnerID=40&md5=8f4306c0a2b140b2cedd6aa2aab0af4a","Non-intrusive appliance load monitoring has emerged as an attractive approach to study energy consumption patterns without instrumenting every device in a building. The ensuing computational problem is to disaggregate total energy usage into usage by specific devices, to gain insight into consumption patterns. We exploit the temporal ordering implicit in on/off events of devices to uncover motifs (episodes) corresponding to the operation of individual devices. Extracted motifs are then subjected to a sequence of constraint checks to ensure that the resulting episodes are interpretable. Our results reveal that motif mining is adept at distinguishing devices with multiple power levels and at disentangling the combinatorial operation of devices. With suitably configured processing steps, we demonstrate the applicability of our method to both residential and commercial buildings. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Commercial building; Computational problem; Consumption patterns; Disaggregation; Individual devices; Non-intrusive appliance load monitoring; Processing steps; Temporal order; Artificial intelligence; Housing; Office buildings; Energy utilization",Conference Paper,Scopus,2-s2.0-84893384215
"Guns T., Dries A., Tack G., Nijssen S., De Raedt L.","MiningZinc: A modeling language for constraint-based mining",2013,"IJCAI International Joint Conference on Artificial Intelligence",17,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063791&partnerID=40&md5=50b05ed829c8283ef738c2c24d1542d7","We introduce Mining Zinc, a general framework for constraint-based pattern mining, one of the most popular tasks in data mining. MiningZinc consists of two key components: a language component and a toolchain component. The language allows for high-level and natural modeling of mining problems, such that MiningZinc models closely resemble definitions found in the data mining literature. It is inspired by the Zinc family of languages and systems and supports user-defined constraints and optimization criteria. The toolchain allows for finding solutions to the models. It ensures the solver independence of the language and supports both standard constraint solvers and specialized data mining systems. Automatic model transformations enable the efficient use of different solvers and systems. The combination of both components allows one to rapidly model constraint-based mining problems and execute these with a wide variety of methods. We demonstrate this experimentally for a number of well-known solvers and data mining tasks.",,"Automatic modeling; Constraint-based mining; Data mining system; Finding solutions; Language component; Languages and systems; Optimization criteria; Standard constraints; Artificial intelligence; Data mining; Optimization; Zinc; High level languages",Conference Paper,Scopus,2-s2.0-84896063791
"Letchford J., Conitzer V.","Solving security games on graphs via marginal probabilities",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",17,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888342622&partnerID=40&md5=084fd3b948bbf4f22f4be63fe621c998","Security games involving the allocation of multiple security resources to defend multiple targets generally have an exponential number of pure strategies for the defender. One method that has been successful in addressing this computational issue is to instead directly compute the marginal probabilities with which the individual resources are assigned (first pursued by Kiekintveld et al. (2009)). However, in sufficiently general settings, there exist games where these marginal solutions are not implementable, that is, they do not correspond to any mixed strategy of the defender. In this paper, we examine security games where the defender tries to monitor the vertices of a graph, and we show how the type of graph, the type of schedules, and the type of defender resources affect the applicability of this approach. In some settings, we show the approach is applicable and give a polynomial-time algorithm for computing an optimal defender strategy; in other settings, we give counterexample games that demonstrate that the approach does not work, and prove NP-hardness results for computing an optimal defender strategy. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Computational issues; Exponential numbers; Marginal probability; Mixed strategy; Multiple securities; Multiple targets; Np-hardness results; Polynomial-time algorithms; Algorithms; Artificial intelligence; Optimization; Computer games",Conference Paper,Scopus,2-s2.0-84888342622
"Fernandes B.J.T., Cavalcanti G.D.C., Ren T.I.","Lateral inhibition pyramidal neural network for image classification",2013,"IEEE Transactions on Cybernetics",17,10.1109/TCYB.2013.2240295,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890071682&doi=10.1109%2fTCYB.2013.2240295&partnerID=40&md5=5faf852aede25c9529c70f8464640b88","The human visual system is one of the most fascinating and complex mechanisms of the central nervous system that enables our capacity to see. It is through the visual system that we are able to accomplish from the most simple task such as object recognition to the most complex visual interpretation, understanding and perception. Inspired by this sophisticated system, two models based on the properties of the human visual system are proposed. These models are designed based on the concepts of receptive and inhibitory fields. The first model is a pyramidal neural network with lateral inhibition, called lateral inhibition pyramidal neural network. The second proposed model is a supervised image segmentation system, called segmentation and classification based on receptive fields. This work shows that the combination of these two models is beneficial, and the results obtained are better than that of other state-of-the-art methods. © 2013 IEEE.","Image processing; Neural network; Pattern recognition; Receptive fields","Central nervous systems; Human Visual System; Image segmentation system; Lateral inhibition; Receptive fields; Sophisticated system; State-of-the-art methods; Visual interpretation; Image classification; Image processing; Object recognition; Pattern recognition; Neural networks; algorithm; article; artificial intelligence; artificial neural network; biomimetics; human; methodology; nerve cell inhibition; nerve cell network; pattern recognition; physiology; pyramidal nerve cell; vision; Algorithms; Artificial Intelligence; Biomimetics; Humans; Nerve Net; Neural Inhibition; Neural Networks (Computer); Pattern Recognition, Visual; Pyramidal Cells; Visual Perception",Article,Scopus,2-s2.0-84890071682
"Li P., Bu J., Chen C., He Z., Cai D.","Relational multimanifold coclustering",2013,"IEEE Transactions on Cybernetics",17,10.1109/TSMCB.2012.2234108,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888350280&doi=10.1109%2fTSMCB.2012.2234108&partnerID=40&md5=9191016a533b210ee91ee04265812265","Coclustering targets on grouping the samples (e.g., documents and users) and the features (e.g., words and ratings) simultaneously. It employs the dual relation and the bilateral information between the samples and features. In many real-world applications, data usually reside on a submanifold of the ambient Euclidean space, but it is nontrivial to estimate the intrinsic manifold of the data space in a principled way. In this paper, we focus on improving the coclustering performance via manifold ensemble learning, which is able to maximally approximate the intrinsic manifolds of both the sample and feature spaces. To achieve this, we develop a novel coclustering algorithm called relational multimanifold coclustering based on symmetric nonnegative matrix trifactorization, which decomposes the relational data matrix into three submatrices. This method considers the intertype relationship revealed by the relational data matrix and also the intratype information reflected by the affinity matrices encoded on the sample and feature data distributions. Specifically, we assume that the intrinsic manifold of the sample or feature space lies in a convex hull of some predefined candidate manifolds. We want to learn a convex combination of them to maximally approach the desired intrinsic manifold. To optimize the objective function, the multiplicative rules are utilized to update the submatrices alternatively. In addition, both the entropic mirror descent algorithm and the coordinate descent algorithm are exploited to learn the manifold coefficient vector. Extensive experiments on documents, images, and gene expression data sets have demonstrated the superiority of the proposed algorithm compared with other well-established methods. © 2013 IEEE.","Coordinate descent algorithm (CDA); entropic mirror descent algorithm (EMDA); manifold ensemble learning; nonnegative matrix trifactorization; relational coclustering","Co-clustering; Coordinate descent; Ensemble learning; entropic mirror descent algorithm (EMDA); Non-negative matrix; Algorithms; Gene expression; Mirrors; Matrix algebra; algorithm; article; artificial intelligence; automated pattern recognition; computer simulation; decision support system; methodology; statistical model; Algorithms; Artificial Intelligence; Computer Simulation; Decision Support Techniques; Models, Statistical; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84888350280
"Ristovski K., Radosavljevic V., Vucetic S., Obradovic Z.","Continuous Conditional Random Fields for efficient regression in large fully connected graphs",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",17,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893390296&partnerID=40&md5=da301bc4edd37f8b32968eab937d79db","When used for structured regression, powerful Conditional Random Fields (CRFs) are typically restricted to modeling effects of interactions among examples in local neighborhoods. Using more expressive representation would result in dense graphs, making these methods impractical for large-scale applications. To address this issue, we propose an effective CRF model with linear scale-up properties regarding approximate learning and inference for structured regression on large, fully connected graphs. The proposed method is validated on real-world large-scale problems of image denoising and remote sensing. In conducted experiments, we demonstrated that dense connectivity provides an improvement in prediction accuracy. Inference time of less than ten seconds on graphs with millions of nodes and trillions of edges makes the proposed model an attractive tool for large-scale, structured regression problems. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Conditional random field; Conditional Random Fields(CRFs); Large-scale applications; Large-scale problem; Local neighborhoods; Modeling effects; Prediction accuracy; Regression problem; Artificial intelligence; Random processes; Regression analysis; Graph theory",Conference Paper,Scopus,2-s2.0-84893390296
"Heydari A., Balakrishnan S.N.","Fixed-final-time optimal control of nonlinear systems with terminal constraints",2013,"Neural Networks",17,10.1016/j.neunet.2013.07.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882796772&doi=10.1016%2fj.neunet.2013.07.002&partnerID=40&md5=0429d68b6971a227b18e890149d09967","A model-based reinforcement learning algorithm is developed in this paper for fixed-final-time optimal control of nonlinear systems with soft and hard terminal constraints. Convergence of the algorithm, for linear in the weights neural networks, is proved through a novel idea by showing that the training algorithm is a contraction mapping. Once trained, the developed neurocontroller is capable of solving this class of optimal control problems for different initial conditions, different final times, and different terminal constraint surfaces providing some mild conditions hold. Three examples are provided and the numerical results demonstrate the versatility and the potential of the developed technique. © 2013 Elsevier Ltd.","Adaptive critics; Fixed-final-time optimal control; Reinforcement learning; Terminal state constraint","Adaptive critic; Contraction mappings; Hard terminal constraints; Model-based reinforcement learning; Optimal control problem; Optimal controls; Terminal constraint; Terminal state; Learning algorithms; Neural networks; Nonlinear systems; Optimal control systems; Reinforcement learning; Control; article; artificial neural network; calculation; independent variable; learning algorithm; natural selection; nonlinear system; priority journal; Adaptive critics; Fixed-final-time optimal control; Reinforcement learning; Terminal state constraint; Algorithms; Artificial Intelligence; Computer Simulation; Linear Models; Neural Networks (Computer); Nonlinear Dynamics",Article,Scopus,2-s2.0-84882796772
"Kalinowski T., Narodytska N., Walsh T., Xia L.","Strategic behavior when allocating indivisible goods sequentially",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",17,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893349273&partnerID=40&md5=76b50faa7a36c43be6fd51316b16ce5e","We study a simple sequential allocation mechanism for allocating indivisible goods between agents in which agents take turns to pick items.We focus on agents behaving strategically. We view the allocation procedure as a finite repeated game with perfect information. We show that with just two agents, we can compute the unique subgame perfect Nash equilibrium in linear time. With more agents, computing the subgame perfect Nash equilibria is more difficult. There can be an exponential number of equilibria and computing even one of them is PSPACE-hard. We identify a special case, when agents value many of the items identically, where we can efficiently compute the subgame perfect Nash equilibria. We also consider the effect of externalities and modifications to the mechanism that make it strategy proof. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Exponential numbers; Indivisible good; IT strategies; Nash equilibria; Perfect informations; Repeated games; Sequential allocations; Strategic Behavior; Artificial intelligence; Telecommunication networks",Conference Paper,Scopus,2-s2.0-84893349273
"Kouvaros P., Lomuscio A.","A cutoff technique for the verification of parameterised interpreted systems with parameterised environments",2013,"IJCAI International Joint Conference on Artificial Intelligence",17,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896061801&partnerID=40&md5=54eda174f377370a5588e76b00665e68","We put forward a cutoff technique for determining the number of agents that is sufficient to consider when checking temporal-epistemic specifications on a system of any size. We identify a special class of interleaved interpreted systems for which we give a parameterised semantics and an abstraction methodology. This enables us to overcome the significant limitations in expressivity present in the state-of-the-art. We present an implementation and discuss experimental results.",,"Cutoff technique; Interpreted systems; Special class; Artificial intelligence; Semantics; Parameterization",Conference Paper,Scopus,2-s2.0-84896061801
"Darari F., Nutt W., Pirrò G., Razniewski S.","Completeness statements about RDF data sources and their use for query answering",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",17,10.1007/978-3-642-41335-3_5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891934653&doi=10.1007%2f978-3-642-41335-3_5&partnerID=40&md5=ecbd3098c5536da423d98fddf9da209d","With thousands of RDF data sources available on the Web covering disparate and possibly overlapping knowledge domains, the problem of providing high-level descriptions (in the form of metadata) of their content becomes crucial. In this paper we introduce a theoretical framework for describing data sources in terms of their completeness. We show how existing data sources can be described with completeness statements expressed in RDF. We then focus on the problem of the completeness of query answering over plain and RDFS data sources augmented with completeness statements. Finally, we present an extension of the completeness framework for federated data sources. © 2013 Springer-Verlag.",,"Data-sources; High level description; Knowledge domains; Query answering; RDF data; Theoretical framework; Artificial intelligence; Computer science; Computers; Semantic Web",Conference Paper,Scopus,2-s2.0-84891934653
"Salek M., Bachrach Y., Key P.","Hotspotting - A probabilistic graphical model for image object localization through crowdsourcing",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",17,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893351765&partnerID=40&md5=92f0ea77eac550984c0cc5cf7bcb9820","Object localization is an image annotation task which consists of finding the location of a target object in an image. It is common to crowdsource annotation tasks and aggregate responses to estimate the true annotation. While for other kinds of annotations consensus is simple and powerful, it cannot be applied to object localization as effectively due to the task's rich answer space and inherent noise in responses. We propose a probabilistic graphical model to localize objects in images based on responses from the crowd. We improve upon natural aggregation methods such as the mean and the median by simultaneously estimating the difficulty level of each question and skill level of every participant. We empirically evaluate our model on crowdsourced data and show that our method outperforms simple aggregators both in estimating the true locations and in ranking participants by their ability. We also propose a simple adaptive sourcing scheme that works well for very sparse datasets. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Aggregation methods; Crowdsourcing; Image annotation; Image objects; Inherent noise; Object localization; Probabilistic graphical models; Simple adaptive; Artificial intelligence; Estimation; Object recognition",Conference Paper,Scopus,2-s2.0-84893351765
"Hu Y., Wang F., Kambhampati S.","Listening to the crowd: Automated analysis of events via aggregated twitter sentiment",2013,"IJCAI International Joint Conference on Artificial Intelligence",17,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062601&partnerID=40&md5=3c4a2b27aa4da23ba7051b4a1fb14720","Individuals often express their opinions on social media platforms like Twitter and Facebook during public events such as the U.S. Presidential debate and the Oscar awards ceremony. Gleaning insights from these posts is of importance to analyzing the impact of the event. In this work, we consider the problem of identifying the segments and topics of an event that garnered praise or criticism, according to aggregated Twitter responses. We propose a flexible factorization framework, SOCSENT, to learn factors about segments, topics, and sentiments. To regulate the learning process, several constraints based on prior knowledge on sentiment lexicon, sentiment orientations (on a few tweets) as well as tweets alignments to the event are enforced. We implement our approach using simple update rules to get the optimal solution. We evaluate the proposed method both quantitatively and qualitatively on two large-scale tweet datasets associated with two events from different domains to show that it improves significantly over baseline models.",,"Automated analysis; Baseline models; Different domains; Learning process; Optimal solutions; Prior knowledge; Sentiment lexicons; Social media platforms; Artificial intelligence; Data processing; Social networking (online); Aggregates",Conference Paper,Scopus,2-s2.0-84896062601
"Bueno-Crespo A., García-Laencina P.J., Sancho-Gómez J.-L.","Neural architecture design based on extreme learning machine",2013,"Neural Networks",17,10.1016/j.neunet.2013.06.010,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880928668&doi=10.1016%2fj.neunet.2013.06.010&partnerID=40&md5=de739868bd37762ba6bb1a4d7652a2fa","Selection of the optimal neural architecture to solve a pattern classification problem entails to choose the relevant input units, the number of hidden neurons and its corresponding interconnection weights. This problem has been widely studied in many research works but their solutions usually involve excessive computational cost in most of the problems and they do not provide a unique solution. This paper proposes a new technique to efficiently design the MultiLayer Perceptron (MLP) architecture for classification using the Extreme Learning Machine (ELM) algorithm. The proposed method provides a high generalization capability and a unique solution for the architecture design. Moreover, the selected final network only retains those input connections that are relevant for the classification task. Experimental results show these advantages. © 2013 Elsevier Ltd.","Architecture design; Extreme learning machine; Multilayer perceptron; Neural networks","Architecture designs; Extreme learning machine; Generalization capability; Interconnection weight; Multi layer perceptron; Multi-layer perceptron architecture (MLP); Number of hidden neurons; Pattern classification problems; Design; Knowledge acquisition; Learning systems; Neural networks; Network architecture; accuracy; article; artificial neural network; back propagation; controlled study; extreme learning machine; intermethod comparison; machine learning; perceptron; priority journal; Architecture design; Extreme learning machine; Multilayer perceptron; Neural networks; Algorithms; Artificial Intelligence; Computer Systems; Data Interpretation, Statistical; Neural Networks (Computer); Neurons; Reproducibility of Results",Article,Scopus,2-s2.0-84880928668
"Kohli P., Salek M., Stoddard G.","A fast bandit algorithm for recommendations to users with heterogeneous tastes",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",17,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893373455&partnerID=40&md5=f35768e12da02e033ae0a3a6e5c9a077","We study recommendation in scenarios where there's no prior information about the quality of content in the system. We present an online algorithm that continually optimizes recommendation relevance based on behavior of past users. Our method trades weaker theoretical guarantees in asymptotic performance than the state-of-the-art for stronger theoretical guarantees in the online setting. We test our algorithm on real-world data collected from previous recommender systems and show that our algorithm learns faster than existing methods and performs equally well in the long-run. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserve.",,"Asymptotic performance; On-line algorithms; On-line setting; Prior information; Quality of contents; Real-world; Theoretical guarantees; Artificial intelligence; Algorithms",Conference Paper,Scopus,2-s2.0-84893373455
"Kuate R.T., He M., Chli M., Wang H.H.","An intelligent broker agent for energy trading: An MDP approach",2013,"IJCAI International Joint Conference on Artificial Intelligence",17,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062350&partnerID=40&md5=49e3725684496b9e28ee1b6985ec6c78","This paper details the development and evaluation of AstonTAC, an energy broker that successfully participated in the 2012 Power Trading Agent Competition (Power TAC). AstonTAC buys electrical energy from the wholesale market and sells it in the retail market. The main focus of the paper is on the broker's bidding strategy in the wholesale market. In particular, it employs Markov Decision Processes (MDP) to purchase energy at low prices in a day-ahead power wholesale market, and keeps energy supply and demand balanced. Moreover, we explain how the agent uses Non-Homogeneous Hidden Markov Model (NHHMM) to forecast energy demand and price. An evaluation and analysis of the 2012 Power TAC finals show that AstonTAC is the only agent that can buy energy at low price in the wholesale market and keep energy imbalance low.",,"Bidding strategy; Electrical energy; Energy imbalances; Energy supplies; Evaluation and analysis; Markov Decision Processes; Non-homogeneous; Wholesale markets; Artificial intelligence; Economics; Hidden Markov models; Commerce",Conference Paper,Scopus,2-s2.0-84896062350
"Wittmeier S., Alessandro C., Bascarevic N., Dalamagkidis K., Devereux D., Diamond A., Jäntsch M., Jovanovic K., Knight R., Marques H.G., Milosavljevic P., Mitra B., Svetozarevic B., Potkonjak V., Pfeifer R., Knoll A., Holland O.","Toward anthropomimetic robotics: development, simulation, and control of a musculoskeletal torso.",2013,"Artificial life",17,10.1162/ARTL_a_00088,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875674507&doi=10.1162%2fARTL_a_00088&partnerID=40&md5=ce2f92750fe423b03e576cf8e1ff88ac","Anthropomimetic robotics differs from conventional approaches by capitalizing on the replication of the inner structures of the human body, such as muscles, tendons, bones, and joints. Here we present our results of more than three years of research in constructing, simulating, and, most importantly, controlling anthropomimetic robots. We manufactured four physical torsos, each more complex than its predecessor, and developed the tools required to simulate their behavior. Furthermore, six different control approaches, inspired by classical control theory, machine learning, and neuroscience, were developed and evaluated via these simulations or in small-scale setups. While the obtained results are encouraging, we are aware that we have barely exploited the potential of the anthropomimetic design so far. But, with the tools developed, we are confident that this novel approach will contribute to our understanding of morphological computation and human motor control in the future.",,"algorithm; anthropometry; article; artificial intelligence; artificial neural network; computer program; computer simulation; equipment design; histology; human; mechanoreceptor; methodology; physics; physiology; robotics; skeleton; tendon; theoretical model; trunk; Algorithms; Anthropometry; Artificial Intelligence; Computer Simulation; Equipment Design; Humans; Mechanoreceptors; Models, Theoretical; Neural Networks (Computer); Physics; Robotics; Skeleton; Software; Tendons; Torso",Article,Scopus,2-s2.0-84875674507
"Zhu X.","Persistent homology: An introduction and a new text representation for natural language processing",2013,"IJCAI International Joint Conference on Artificial Intelligence",17,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062400&partnerID=40&md5=3e565d6f74437b5374e1e26b651eead0","Persistent homology is a mathematical tool from topological data analysis. It performs multi-scale analysis on a set of points and identifies clusters, holes, and voids therein. These latter topological structures complement standard feature representations, making persistent homology an attractive feature extractor for artificial intelligence. Research on persistent homology for AI is in its infancy, and is currently hindered by two issues: the lack of an accessible introduction to AI researchers, and the paucity of applications. In response, the first part of this paper presents a tutorial on persistent homology specifically aimed at a broader audience without sacrificing mathematical rigor. The second part contains one of the first applications of persistent homology to natural language processing. Specifically, our Similarity Filtration with Time Skeleton (SIFTS) algorithm identifies holes that can be interpreted as semantic ""tie-backs"" in a text document, providing a new document structure representation. We illustrate our algorithm on documents ranging from nursery rhymes to novels, and on a corpus with child and adolescent writings.",,"Feature representation; Mathematical tools; Multi scale analysis; NAtural language processing; Persistent homology; Text representation; Topological data analysis; Topological structure; Algorithms; Natural language processing systems; Semantics; Topology; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84896062400
"Nie F., Wang H., Huang H., Ding C.","Early active learning via robust representation and structured sparsity",2013,"IJCAI International Joint Conference on Artificial Intelligence",17,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896061560&partnerID=40&md5=6d9cdcadc7a67197dca397706a1d37c0","Labeling training data is quite time-consuming but essential for supervised learning models. To solve this problem, the active learning has been studied and applied to select the informative and representative data points for labeling. However, during the early stage of experiments, only a small number (or none) of labeled data points exist, thus the most representative samples should be selected first. In this paper, we propose a novel robust active learning method to handle the early stage experimental design problem and select the most representative data points. Selecting the representative samples is an NP-hard problem, thus we employ the structured sparsity-inducing norm to relax the objective to an efficient convex formulation. Meanwhile, the robust sparse representation loss function is utilized to reduce the effect of outliers. A new efficient optimization algorithm is introduced to solve our non-smooth objective with low computational cost and proved global convergence. Empirical results on both single-label and multi-label classification benchmark data sets show the promising results of our method.",,"Active learning methods; Computational costs; Global conver-gence; Multi-label classifications; Optimization algorithms; Representative sample; Sparse representation; Structured sparsities; Algorithms; Artificial intelligence; Classification (of information); Computational complexity; Computational efficiency",Conference Paper,Scopus,2-s2.0-84896061560
"Bredereck R., Cheny J., Woegingerz G.J.","Are there any nicely structured preference profiles nearby?",2013,"IJCAI International Joint Conference on Artificial Intelligence",17,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062266&partnerID=40&md5=8828d0ebd6b3acb0c35124b35b7ecc0a","We investigate the problem of deciding whether a given preference profile is close to a nicely structured preference profile of a certain type, as for instance single-peaked, single-caved, single-crossing, value-restricted, best-restricted, worst-restricted, medium-restricted, or group-separable profiles. We measure this distance by the number of voters or alternatives that have to be deleted so as to reach a nicely structured profile. Our results classify all considered problem variants with respect to their computational complexity, and draw a clear line between computationally tractable (polynomial time solvable) and computationally intractable (NP-hard) questions.",,"NP-hard; Polynomial-time; Structured profiles; Polynomial approximation; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84896062266
"Stuckey P.J., Tack G.","MiniZinc with functions",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",17,10.1007/978-3-642-38171-3_18,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885219903&doi=10.1007%2f978-3-642-38171-3_18&partnerID=40&md5=a206d350642b523cf68b9f71eab262ee","Functional relations are ubiquitous in combinatorial problems - the Global Constraint Catalog lists 120 functional constraints. This paper argues that the ability to express functional constraints with functional syntax leads to more elegant and readable models, and that it enables a better translation of the models to different underlying solving technologies such as CP, MIP, or SAT. Yet, most modelling languages only support built-in functions, such as arithmetic, Boolean, or array access operations. Custom, user-defined functions are either not catered for at all, or they have an ad-hoc implementation without a useful semantics in Boolean contexts and not exploiting potential optimisations. This paper develops a translation from MiniZinc with user-defined functions to FlatZinc. The translation respects the relational semantics of MiniZinc, correctly dealing with partial functions in arbitrary Boolean contexts. At the same time, it takes advantage of the full potential of common subexpression elimination. © Springer-Verlag 2013.",,"Combinatorial problem; Common subexpression elimination; Functional constraints; Functional relation; Global constraints; Modelling language; Relational semantics; User Defined Functions; Artificial intelligence; Computer programming; Constraint theory; Operations research; Semantics; Boolean functions",Conference Paper,Scopus,2-s2.0-84885219903
"Liu Q., Kwoh C.K., Li J.","Binding affinity prediction for protein-ligand complexes based on β contacts and B factor",2013,"Journal of Chemical Information and Modeling",17,10.1021/ci400450h,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888606432&doi=10.1021%2fci400450h&partnerID=40&md5=48cead4db2d395cf609f9aa58b7bc1f5","Accurate determination of protein-ligand binding affinity is a fundamental problem in biochemistry useful for many applications including drug design and protein-ligand docking. A number of scoring functions have been proposed for the prediction of protein-ligand binding affinity. However, accurate prediction is still a challenging problem because poor performance is often seen in the evaluation under the leave-one-cluster-out cross-validation (LCOCV). We introduce a new scoring function named B2BScore to improve the prediction performance. B2BScore integrates two physicochemical properties for protein-ligand binding affinity prediction. One is the property of β contacts. A β contact between two atoms requires no other atoms to interrupt the atomic contact and assumes that the two atoms should have enough direct contact area. The other is the property of B factor to capture the atomic mobility in the dynamic protein-ligand binding process. Tested on the PDBBind2009 data set, B2BScore shows superior prediction performance to existing methods on independent test data as well as under the LCOCV evaluation framework. In particular, B2BScore achieves a significant LCOCV improvement across 26 protein clusters - a big increase of the averaged Pearson's correlation coefficients from 0.418 to 0.518 and a significant decrease of standard deviation of the coefficients from 0.352 to 0.196. We also identified several important and intuitive contact descriptors of protein-ligand binding through the random forest learning in B2BScore. Some of these descriptors are closely related to contacts between carbon atoms without covalent-bond oxygen/nitrogen, preferred contacts of metal ions, interfacial backbone atoms from proteins, or π rings. Some others are negative descriptors relating to those contacts with nitrogen atoms without covalent-bond hydrogens or nonpreferred contacts of metal ions. These descriptors can be directly used to guide protein-ligand docking. © 2013 American Chemical Society.",,"Accurate prediction; Evaluation framework; Pearson's correlation coefficients; Physicochemical property; Prediction performance; Protein-ligand binding affinities; Protein-ligand complexes; Protein-ligand docking; Binding energy; Complexation; Correlation methods; Decision trees; Docking; Forecasting; Function evaluation; Metal ions; Proteins; Atoms; ligand; protein; algorithm; article; artificial intelligence; binding site; chemistry; decision tree; drug design; hydrogen bond; methodology; molecular docking; protein binding; protein database; protein domain; protein secondary structure; thermodynamics; Algorithms; Artificial Intelligence; Binding Sites; Databases, Protein; Decision Trees; Drug Design; Hydrogen Bonding; Ligands; Molecular Docking Simulation; Protein Binding; Protein Interaction Domains and Motifs; Protein Structure, Secondary; Proteins; Research Design; Thermodynamics",Article,Scopus,2-s2.0-84888606432
"Vehlow C., Reinhardt T., Weiskopf D.","Visualizing fuzzy overlapping communities in networks",2013,"IEEE Transactions on Visualization and Computer Graphics",17,10.1109/TVCG.2013.232,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886647978&doi=10.1109%2fTVCG.2013.232&partnerID=40&md5=6ca5c4c21cdcb88023317a6dddf74877","An important feature of networks for many application domains is their community structure. This is because objects within the same community usually have at least one property in common. The investigation of community structure can therefore support the understanding of object attributes from the network topology alone. In real-world systems, objects may belong to several communities at the same time, i.e., communities can overlap. Analyzing fuzzy community memberships is essential to understand to what extent objects contribute to different communities and whether some communities are highly interconnected. We developed a visualization approach that is based on node-link diagrams and supports the investigation of fuzzy communities in weighted undirected graphs at different levels of detail. Starting with the network of communities, the user can continuously drill down to the network of individual nodes and finally analyze the membership distribution of nodes of interest. Our approach uses layout strategies and further visual mappings to graphically encode the fuzzy community memberships. The usefulness of our approach is illustrated by two case studies analyzing networks of different domains: social networking and biological interactions. The case studies showed that our layout and visualization approach helps investigate fuzzy overlapping communities. Fuzzy vertices as well as the different communities to which they belong can be easily identified based on node color and position. © 2013 IEEE.","fuzzy clustering; graph visualization; Overlapping community visualization; uncertainty visualization","Biological interactions; Community structures; Graph visualization; Important features; Node-link diagrams; Overlapping communities; Uncertainty visualization; Weighted undirected graph; Electric network topology; Fuzzy clustering; Social sciences; Visualization; Computer networks; algorithm; article; artificial intelligence; computer interface; computer simulation; fuzzy logic; image enhancement; methodology; statistical model; image enhancement; procedures; Algorithms; Artificial Intelligence; Computer Simulation; Fuzzy Logic; Image Enhancement; Models, Statistical; User-Computer Interface; Algorithms; Artificial Intelligence; Computer Simulation; Fuzzy Logic; Image Enhancement; Models, Statistical; User-Computer Interface",Article,Scopus,2-s2.0-84886647978
"Meghdadi A.H., Irani P.","Interactive exploration of surveillance video through action shot summarization and trajectory visualization",2013,"IEEE Transactions on Visualization and Computer Graphics",17,10.1109/TVCG.2013.168,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886643615&doi=10.1109%2fTVCG.2013.168&partnerID=40&md5=281a0d5fc212094a4901f9dfa4fff72b","We propose a novel video visual analytics system for interactive exploration of surveillance video data. Our approach consists of providing analysts with various views of information related to moving objects in a video. To do this we first extract each object's movement path. We visualize each movement by (a) creating a single action shot image (a still image that coalesces multiple frames), (b) plotting its trajectory in a space-time cube and (c) displaying an overall timeline view of all the movements. The action shots provide a still view of the moving object while the path view presents movement properties such as speed and location. We also provide tools for spatial and temporal filtering based on regions of interest. This allows analysts to filter out large amounts of movement activities while the action shot representation summarizes the content of each movement. We incorporated this multi-part visual representation of moving objects in sViSIT, a tool to facilitate browsing through the video content by interactive querying and retrieval of data. Based on our interaction with security personnel who routinely interact with surveillance video data, we identified some of the most common tasks performed. This resulted in designing a user study to measure time-to-completion of the various tasks. These generally required searching for specific events of interest (targets) in videos. Fourteen different tasks were designed and a total of 120 min of surveillance video were recorded (indoor and outdoor locations recording movements of people and vehicles). The time-to-completion of these tasks were compared against a manual fast forward video browsing guided with movement detection. We demonstrate how our system can facilitate lengthy video exploration and significantly reduce browsing time to find events of interest. Reports from expert users identify positive aspects of our approach which we summarize in our recommendations for future video visual analytics systems. © 2013 IEEE.","surveillance video; video browsing and exploration; video summarization; Video visual analytics; video visualization","Surveillance video; Video browsing; Video summarization; Video visualization; Visual analytics; Monitoring; Tools; Video recording; Visualization; Security systems; algorithm; artificial intelligence; automated pattern recognition; computer assisted diagnosis; computer graphics; computer interface; photography; procedures; reproducibility; sensitivity and specificity; videorecording; article; automated pattern recognition; computer assisted diagnosis; methodology; photography; videorecording; Algorithms; Artificial Intelligence; Computer Graphics; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Photography; Reproducibility of Results; Sensitivity and Specificity; User-Computer Interface; Video Recording; Algorithms; Artificial Intelligence; Computer Graphics; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Photography; Reproducibility of Results; Sensitivity and Specificity; User-Computer Interface; Video Recording",Article,Scopus,2-s2.0-84886643615
"Hall M.M., Toms E.","Building a common framework for IIR evaluation",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",17,10.1007/978-3-642-40802-1_3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886394767&doi=10.1007%2f978-3-642-40802-1_3&partnerID=40&md5=bd23710e514340eb43caa5f168d2bad6","Cranfield-style evaluations standardised Information Retrieval (IR) evaluation practices, enabling the creation of programmes such as TREC, CLEF, and INEX, and long-term comparability of IR systems. However, the methodology does not translate well into the Interactive IR (IIR) domain, where the inclusion of the user into the search process and the repeated interaction between user and system creates more variability than the Cranfield-style evaluations can support. As a result, IIR evaluations of various systems have tended to be non-comparable, not because the systems vary, but because the methodologies used are non-comparable. In this paper we describe a standardised IIR evaluation framework, that ensures that IIR evaluations can share a standardised baseline methodology in much the same way that TREC, CLEF, and INEX imposed a process on IR evaluation. The framework provides a common baseline, derived by integrating existing, validated evaluation measures, that enables inter-study comparison, but is also flexible enough to support most kinds of IIR studies. This is achieved through the use of a ""pluggable"" system, into which any web-based IIR interface can be embedded. The framework has been implemented and the software will be made available to reduce the resource commitment required for IIR studies. © 2013 Springer-Verlag.","evaluation; interactive information retrieval; methodology","evaluation; Evaluation framework; Evaluation measures; Interactive information retrieval; Ir systems; methodology; Resource commitment; Search process; Artificial intelligence; Computer science; Information retrieval",Conference Paper,Scopus,2-s2.0-84886394767
"Ge H., Wang Y., Li C., Chen N., Xie Y., Xu M., He Y., Gu X., Wu R., Gu Q., Zeng L., Xu J.","Molecular dynamics-based virtual screening: Accelerating the drug discovery process by high-performance computing",2013,"Journal of Chemical Information and Modeling",17,10.1021/ci400391s,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887043744&doi=10.1021%2fci400391s&partnerID=40&md5=2bbd51f37fbb1c8aed75fd4fc23509ed","High-performance computing (HPC) has become a state strategic technology in a number of countries. One hypothesis is that HPC can accelerate biopharmaceutical innovation. Our experimental data demonstrate that HPC can significantly accelerate biopharmaceutical innovation by employing molecular dynamics-based virtual screening (MDVS). Without using HPC, MDVS for a 10K compound library with tens of nanoseconds of MD simulations requires years of computer time. In contrast, a state of the art HPC can be 600 times faster than an eight-core PC server is in screening a typical drug target (which contains about 40K atoms). Also, careful design of the GPU/CPU architecture can reduce the HPC costs. However, the communication cost of parallel computing is a bottleneck that acts as the main limit of further virtual screening improvements for drug innovations. © 2013 American Chemical Society.",,"Communication cost; Compound libraries; Drug discovery process; High-performance computing; MD simulation; State of the art; Strategic technologies; Virtual Screening; Computer simulation; Parallel architectures; Molecular dynamics; Human immunodeficiency virus proteinase; Human immunodeficiency virus proteinase inhibitor; ligand; peroxisome proliferator activated receptor alpha; sialidase; algorithm; article; artificial intelligence; binding site; chemical database; chemistry; computer interface; drug antagonism; drug development; economics; equipment; high throughput screening; human; molecular docking; molecular dynamics; molecular library; protein binding; protein database; structure activity relation; thermodynamics; Algorithms; Artificial Intelligence; Binding Sites; Databases, Chemical; Databases, Protein; Drug Discovery; High-Throughput Screening Assays; HIV Protease; HIV Protease Inhibitors; Humans; Ligands; Molecular Docking Simulation; Molecular Dynamics Simulation; Neuraminidase; PPAR alpha; Protein Binding; Small Molecule Libraries; Structure-Activity Relationship; Thermodynamics; User-Computer Interface",Article,Scopus,2-s2.0-84887043744
"Li Y., Ngom A.","Sparse representation approaches for the classification of high-dimensional biological data",2013,"BMC Systems Biology",17,10.1186/1752-0509-7-S4-S6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880712800&doi=10.1186%2f1752-0509-7-S4-S6&partnerID=40&md5=e66fe281aa1d2fca014d1c4a5f7c50e9","Background: High-throughput genomic and proteomic data have important applications in medicine including prevention, diagnosis, treatment, and prognosis of diseases, and molecular biology, for example pathway identification. Many of such applications can be formulated to classification and dimension reduction problems in machine learning. There are computationally challenging issues with regards to accurately classifying such data, and which due to dimensionality, noise and redundancy, to name a few. The principle of sparse representation has been applied to analyzing high-dimensional biological data within the frameworks of clustering, classification, and dimension reduction approaches. However, the existing sparse representation methods are inefficient. The kernel extensions are not well addressed either. Moreover, the sparse representation techniques have not been comprehensively studied yet in bioinformatics. Results: In this paper, a Bayesian treatment is presented on sparse representations. Various sparse coding and dictionary learning models are discussed. We propose fast parallel active-set optimization algorithm for each model. Kernel versions are devised based on their dimension-free property. These models are applied for classifying high-dimensional biological data. Conclusions: In our experiment, we compared our models with other methods on both accuracy and computing time. It is shown that our models can achieve satisfactory accuracy, and their performance are very efficient. © 2013 Li and Ngom; licensee BioMed Central Ltd.",,"algorithm; article; artificial intelligence; Bayes theorem; biology; methodology; Algorithms; Artificial Intelligence; Bayes Theorem; Computational Biology",Article,Scopus,2-s2.0-84880712800
"Dalton A., Ólaighin G.","Comparing supervised learning techniques on the task of physical activity recognition",2013,"IEEE Journal of Biomedical and Health Informatics",17,10.1109/TITB.2012.2223823,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885101472&doi=10.1109%2fTITB.2012.2223823&partnerID=40&md5=85cada78d49bbfe8cf74e123707def33","The objective of this study was to compare the performance of base-level and meta-level classifiers on the task of physical activity recognition. Five wireless kinematic sensors were attached to each subject (n = 25) while they completed a range of basic physical activities in a controlled laboratory setting. Subjects were then asked to carry out similar self-annotated physical activities in a random order and in an unsupervised environment. A combination of time-domain and frequency-domain features was extracted from the sensor data including the first four central moments, zero-crossing rate, average magnitude, sensor cross correlation, sensor autocorrelation, spectral entropy, and dominant frequency components. A reduced feature set was generated using a wrapper subset evaluation technique with a linear forward search and this feature set was employed for classifier comparison. The meta-level classifier AdaBoostM1 with C4.5 Graft as its baselevel classifier achieved an overall accuracy of 95%. Equal sized datasets of subject-independent data and subject-dependent data were used to train this classifier and high recognition rates could be achieved without the need for user specific training. Furthermore, it was found that an accuracy of 88% could be achieved using data from the ankle and wrist sensors only. © 2012 IEEE.","Activity recognition; Base-level and meta-level classifiers; Kinematic sensors","Activity recognition; Controlled laboratories; Cross correlations; Dominant frequency; Frequency domains; Kinematic sensors; Overall accuracies; Zero crossing rate; Pattern recognition; Sensors; Classification (of information); algorithm; ambulatory monitoring; ankle; article; artificial intelligence; automated pattern recognition; biomechanics; classification; comparative study; daily life activity; human; methodology; physiology; signal processing; wrist; ambulatory monitoring; automated pattern recognition; classification; daily life activity; procedures; Activities of Daily Living; Algorithms; Ankle; Artificial Intelligence; Biomechanical Phenomena; Humans; Monitoring, Ambulatory; Pattern Recognition, Automated; Signal Processing, Computer-Assisted; Wrist; Activities of Daily Living; Algorithms; Ankle; Artificial Intelligence; Biomechanical Phenomena; Humans; Monitoring, Ambulatory; Pattern Recognition, Automated; Signal Processing, Computer-Assisted; Wrist",Article,Scopus,2-s2.0-84885101472
"Helbig M., Engelbrecht A.P.","Issues with performance measures for dynamic multi-objective optimisation",2013,"Proceedings of the 2013 IEEE Symposium on Computational Intelligence in Dynamic and Uncertain Environments, CIDUE 2013 - 2013 IEEE Symposium Series on Computational Intelligence, SSCI 2013",17,10.1109/CIDUE.2013.6595767,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881592284&doi=10.1109%2fCIDUE.2013.6595767&partnerID=40&md5=e3ed64e11972483dbced057868576d9c","In recent years a number of algorithms were proposed to solve dynamic multi-objective optimisation problems. However, a major problem in the field of dynamic multi-objective optimisation is a lack of standard performance measures to quantify the quality of solutions found by an algorithm. In addition, the selection of performance measures may lead to misleading results. This paper highlights issues that may cause misleading results when comparing dynamic multi-objective optimisation algorithms with performance measures that are currently used in the field. © 2013 IEEE.","dynamic multi-objective optimisation; performance measures","Performance measure; Quality of solution; Standard performance; Algorithms; Artificial intelligence; Multiobjective optimization",Conference Paper,Scopus,2-s2.0-84881592284
"Buettner R.","Cognitive workload of humans using artificial intelligence systems: Towards objective measurement applying eye-tracking technology",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",17,10.1007/978-3-642-40942-4-4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885032381&doi=10.1007%2f978-3-642-40942-4-4&partnerID=40&md5=c1faf8437e732dcce49ce836f022f7fd","Replying to corresponding research calls I experimentally investigate whether a higher level of artificial intelligence support leads to a lower user cognitive workload. Applying eye-tracking technology I show how the user's cognitive workload can be measure more objectively by capturing eye movements and pupillary responses. Within a laboratory environment which adequately reflects a realistic working situation, the probands use two distinct systems with similar user interfaces but very different levels of artificial intelligence support. Recording and analyzing objective eye-tracking data (i.e. pupillary diameter mean, pupillary diameter deviation, number of gaze fixations and eye saccade speed of both left and right eyes)-all indicating cognitive workload-I found significant systematic cognitive workload differences between both test systems. My results indicated that a higher AI-support leads to lower user cognitive workload. © 2013 Springer-Verlag Berlin Heidelberg.","argumentation-based negotiation; argumentation-generation; artificial intelligence support; cognitive workload; eye movements; eye saccades; eye-tracking; pupillary diameter","Argumentation-based negotiation; argumentation-generation; Cognitive workloads; Eye saccades; Eye-tracking; pupillary diameter; Artificial intelligence; Intelligent networks; Tracking (position); User interfaces; Eye movements",Conference Paper,Scopus,2-s2.0-84885032381
"Hu M., Li H., Chen Y., Wu Q., Rose G.S.","BSB training scheme implementation on memristor-based circuit",2013,"Proceedings of the 2013 IEEE Symposium on Computational Intelligence for Security and Defense Applications, CISDA 2013 - 2013 IEEE Symposium Series on Computational Intelligence, SSCI 2013",17,10.1109/CISDA.2013.6595431,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884994113&doi=10.1109%2fCISDA.2013.6595431&partnerID=40&md5=960c5121033849b1ede2e414383de3ff","In this work, we propose a hardware realization of the Brain-State-in-a-Box (BSB) neural network model training algorithm. This method can be implemented as an analog/digital mixed-signal circuit to train memristor crossbar arrays within BSB circuits. The training effect is demonstrated through experimentation and the quality as an auto-associative memory is also analyzed and compared with software based training methods. The impacts of non-ideal device characteristics and fabrication defects in crossbar arrays are discussed. Our hardware architecture shows great potential for low power, high speed, small hardware size computations, and provides inherent security features. © 2013 IEEE.","brain-state-in-a-box; crossbar array; memristor; neural network; Neuromorphic hardware","Auto-associative Memory; Brain-State-in-a-Box; Crossbar arrays; Device characteristics; Hardware architecture; Memristor; Mixed-signal circuits; Neuromorphic hardwares; Artificial intelligence; Associative processing; Hardware; Memristors; Neural networks; Passive filters",Conference Paper,Scopus,2-s2.0-84884994113
"Moreira-Matias L., Gama J., Ferreira M., Mendes-Moreira J., Damas L.","On predicting the taxi-passenger demand: A real-time approach",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",17,10.1007/978-3-642-40669-0_6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884707266&doi=10.1007%2f978-3-642-40669-0_6&partnerID=40&md5=b9ae70d98863c4803bcc06be04521679","Informed driving is becoming a key feature to increase the sustainability of taxi companies. Some recent works are exploring the data broadcasted by each vehicle to provide live information for decision making. In this paper, we propose a method to employ a learning model based on historical GPS data in a real-time environment. Our goal is to predict the spatiotemporal distribution of the Taxi-Passenger demand in a short time horizon. We did so by using learning concepts originally proposed to a well-known online algorithm: the perceptron [1]. The results were promising: we accomplished a satisfactory performance to output the next prediction using a short amount of resources. © 2013 Springer-Verlag.","auto-regressive integrated moving average (ARIMA); data streams; GPS data; online learning; perceptron; taxi-passenger demand","Auto-regressive integrated moving average; Data stream; GPS data; Online learning; Perceptron; taxi-passenger demand; Artificial intelligence; Forecasting; Global positioning system; Sustainable development; Taxicabs",Conference Paper,Scopus,2-s2.0-84884707266
"Keramitsoglou I., Kiranoudis C.T., Maiheu B., De Ridder K., Daglis I.A., Manunta P., Paganini M.","Heat wave hazard classification and risk assessment using artificial intelligence fuzzy logic",2013,"Environmental Monitoring and Assessment",17,10.1007/s10661-013-3170-y,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884350565&doi=10.1007%2fs10661-013-3170-y&partnerID=40&md5=7496abb191bffd27943d8f3f837dc248","The average summer temperatures as well as the frequency and intensity of hot days and heat waves are expected to increase due to climate change. Motivated by this consequence, we propose a methodology to evaluate the monthly heat wave hazard and risk and its spatial distribution within large cities. A simple urban climate model with assimilated satellite-derived land surface temperature images was used to generate a historic database of urban air temperature fields. Heat wave hazard was then estimated from the analysis of these hourly air temperatures distributed at a 1-km grid over Athens, Greece, by identifying the areas that are more likely to suffer higher temperatures in the case of a heat wave event. Innovation lies in the artificial intelligence fuzzy logic model that was used to classify the heat waves from mild to extreme by taking into consideration their duration, intensity and time of occurrence. The monthly hazard was subsequently estimated as the cumulative effect from the individual heat waves that occurred at each grid cell during a month. Finally, monthly heat wave risk maps were produced integrating geospatial information on the population vulnerability to heat waves calculated from socio-economic variables. © 2013 Springer Science+Business Media Dordrecht.","Fuzzy logic; Heat wave; Satellite images; Urban climate","Cumulative effects; Geo-spatial informations; Heat waves; Land surface temperature; Satellite images; Summer temperature; Urban climate modeling; Urban climates; Artificial intelligence; Atmospheric temperature; Climate change; Fuzzy logic; Maps; Risk assessment; Hazards; air temperature; artificial intelligence; climate modeling; fuzzy mathematics; hazard assessment; risk assessment; satellite imagery; spatial distribution; urban climate; air temperature; article; artificial intelligence; climate change; environmental temperature; fuzzy logic; Greece; hazard assessment; heat stress; high temperature; humidity; methodology; morbidity; mortality; risk assessment; socioeconomics; summer; urban area; Artificial Intelligence; Climate Change; Fuzzy Logic; Hot Temperature; Humans; Models, Theoretical; Risk Assessment; Athens [Attica]; Attica; Greece",Article,Scopus,2-s2.0-84884350565
"Szemis J.M., Dandy G.C., Maier H.R.","A multiobjective ant colony optimization approach for scheduling environmental flow management alternatives with application to the River Murray, Australia",2013,"Water Resources Research",17,10.1002/wrcr.20518,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884995596&doi=10.1002%2fwrcr.20518&partnerID=40&md5=5f82b05218434462db50721d5b66e4af","In regulated river systems, such as the River Murray in Australia, the efficient use of water to preserve and restore biota in the river, wetlands, and floodplains is of concern for water managers. Available management options include the timing of river flow releases and operation of wetland flow control structures. However, the optimal scheduling of these environmental flow management alternatives is a difficult task, since there are generally multiple wetlands and floodplains with a range of species, as well as a large number of management options that need to be considered. Consequently, this problem is a multiobjective optimization problem aimed at maximizing ecological benefit while minimizing water allocations within the infrastructure constraints of the system under consideration. This paper presents a multiobjective optimization framework, which is based on a multiobjective ant colony optimization approach, for developing optimal trade-offs between water allocation and ecological benefit. The framework is applied to a reach of the River Murray in South Australia. Two studies are formulated to assess the impact of (i) upstream system flow constraints and (ii) additional regulators on this trade-off. The results indicate that unless the system flow constraints are relaxed, there is limited additional ecological benefit as allocation increases. Furthermore the use of regulators can increase ecological benefits while using less water. The results illustrate the utility of the framework since the impact of flow control infrastructure on the trade-offs between water allocation and ecological benefit can be investigated, thereby providing valuable insight to managers. Key Points Optimal environmental flow schedules are vital in preserving river health A multi-objective environmental flow management framework has been developed The framework has been applied to a river reach in the River Murray, Australia ©2013. American Geophysical Union. All Rights Reserved.","ant colony optimization; environmental flow; Murray River; scheduling","Control infrastructures; Ecological benefits; Efficient use of water; Environmental flow; Management options; Multi-objective optimization problem; Murray River; Optimal scheduling; Algorithms; Ant colony optimization; Artificial intelligence; Ecology; Economic and social effects; Environmental management; Floods; Flow control; Management; Managers; Multiobjective optimization; Scheduling; Wetlands; Rivers; ecological approach; flow control; optimization; river flow; river water; streamflow; Australia; Murray River",Article,Scopus,2-s2.0-84884995596
"Bao B.-K., Liu G., Hong R., Yan S., Xu C.","General subspace learning with corrupted training data via graph embedding",2013,"IEEE Transactions on Image Processing",17,10.1109/TIP.2013.2273665,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884547638&doi=10.1109%2fTIP.2013.2273665&partnerID=40&md5=b12813900dad5772dea9c33a2efb0f62","We address the following subspace learning problem: supposing we are given a set of labeled, corrupted training data points, how to learn the underlying subspace, which contains three components: an intrinsic subspace that captures certain desired properties of a data set, a penalty subspace that fits the undesired properties of the data, and an error container that models the gross corruptions possibly existing in the data. Given a set of data points, these three components can be learned by solving a nuclear norm regularized optimization problem, which is convex and can be efficiently solved in polynomial time. Using the method as a tool, we propose a new discriminant analysis (i.e., supervised subspace learning) algorithm called Corruptions Tolerant Discriminant Analysis (CTDA), in which the intrinsic subspace is used to capture the features with high within-class similarity, the penalty subspace takes the role of modeling the undesired features with high between-class similarity, and the error container takes charge of fitting the possible corruptions in the data. We show that CTDA can well handle the gross corruptions possibly existing in the training data, whereas previous linear discriminant analysis algorithms arguably fail in such a setting. Extensive experiments conducted on two benchmark human face data sets and one object recognition data set show that CTDA outperforms the related algorithms. © 2013 IEEE.","corrupted training data; discriminant analysis; graph embedding; Subspace learning","Graph embeddings; Linear discriminant analysis; Polynomial-time; Regularized optimization problems; Related algorithms; Subspace learning; Three component; Training data; Containers; Discriminant analysis; Object recognition; Polynomial approximation; Algorithms; algorithm; anatomy and histology; artifact; artificial intelligence; automated pattern recognition; biometry; computer assisted diagnosis; face; human; image enhancement; procedures; reproducibility; sensitivity and specificity; article; automated pattern recognition; biometry; computer assisted diagnosis; face; histology; methodology; Algorithms; Artifacts; Artificial Intelligence; Biometry; Face; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Algorithms; Artifacts; Artificial Intelligence; Biometry; Face; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84884547638
"Viñas M., Bozkus Z., Fraguela B.B.","Exploiting heterogeneous parallelism with the Heterogeneous Programming Library",2013,"Journal of Parallel and Distributed Computing",17,10.1016/j.jpdc.2013.07.013,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885951736&doi=10.1016%2fj.jpdc.2013.07.013&partnerID=40&md5=2af198f2d5f879e4b41461c3757684c6","While recognition of the advantages of heterogeneous computing is steadily growing, the issues of programmability and portability hinder its exploitation. The introduction of the OpenCL standard was a major step forward in that it provides code portability, but its interface is even more complex than that of other approaches. In this paper, we present the Heterogeneous Programming Library (HPL), which permits the development of heterogeneous applications addressing both portability and programmability while not sacrificing high performance. This is achieved by means of an embedded language and data types provided by the library with which generic computations to be run in heterogeneous devices can be expressed. A comparison in terms of programmability and performance with OpenCL shows that both approaches offer very similar performance, while outlining the programmability advantages of HPL. © 2013 Elsevier Inc. All rights reserved.","Code generation; Heterogeneity; Libraries; OpenCL; Parallelism; Portability; Programmability","Code Generation; Heterogeneity; OpenCL; Parallelism; Programmability; Artificial intelligence; Computer programming; Libraries; Computer software portability",Article,Scopus,2-s2.0-84885951736
"Brakerski Z., Rothblum G.N.","Obfuscating conjunctions",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",17,10.1007/978-3-642-40084-1_24,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884472198&doi=10.1007%2f978-3-642-40084-1_24&partnerID=40&md5=8c9df2879e2bb0e202b44eeab2959023","We show how to securely obfuscate the class of conjunction functions (functions like f(x1,...,xn) = x1 ∧ ¬x4 ∧ ¬x6 ∧⋯∧ xn-2). Given any function in the class, we produce an obfuscated program which preserves the input-output functionality of the given function, but reveals nothing else. Our construction is based on multilinear maps, and can be instantiated using the recent candidates proposed by Garg, Gentry and Halevi (EUROCRYPT 2013) and by Coron, Lepoint and Tibouchi (CRYPTO 2013). We show that the construction is secure when the conjunction is drawn from a distribution, under mild assumptions on the distribution. Security follows from multilinear entropic variants of the Diffie-Hellman assumption. We conjecture that our construction is secure for any conjunction, regardless of the distribution from which it is drawn. We offer supporting evidence for this conjecture, proving that our obfuscator is secure for any conjunction against generic adversaries. © 2013 International Association for Cryptologic Research.",,"Conjunction functions; Diffie-Hellman assumption; Input-output; Multilinear maps; Artificial intelligence; Computer science; Cryptography",Conference Paper,Scopus,2-s2.0-84884472198
"Krogstie B.R., Prilla M., Pammer V.","Understanding and supporting reflective learning processes in the workplace: The CSRL model",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",17,10.1007/978-3-642-40814-4_13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883313328&doi=10.1007%2f978-3-642-40814-4_13&partnerID=40&md5=410dccc8cb4ee593ad4d06b43e0952de","Reflective learning is a mechanism to turn experience into learning. As a mechanism for self-directed learning, it has been found to be critical for success at work. This is true for individual employees, teams and whole organizations. However, most work on reflection can be found in educational contexts, and there is only little work regarding the connection of reflection on individual, group and organization levels. In this paper, we propose a model that can describe cases of reflective learning at work (CSRL). The model represents reflective learning processes as intertwined learning cycles. In contrast to other models of reflective learning, the CSRL model can describe both individual and collaborative learning and learning that impacts larger parts of an organization. It provides terminology to describe and discuss motivations for reflective learning, including triggers, objectives for and objects of reflective learning. The paper illustrates how the model helps to analyse and differentiate cases of reflective learning at work and to design tool support for such settings. © 2013 Springer-Verlag.",,"Collaborative learning; Design tool; Educational context; Learning cycle; Reflective learning; Self-directed learning; Artificial intelligence; Computer science",Conference Paper,Scopus,2-s2.0-84883313328
"Pengel B.E., Krzhizhanovskaya V.V., Melnikova N.B., Shirshov G.S., Koelewijn A.R., Pyayt A.L., Mokhov I.I.","Flood early warning system: Sensors and internet",2013,"IAHS-AISH Publication",17,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883437303&partnerID=40&md5=9a6c5c9463733c6fa7982011c66c132c","The UrbanFlood early warning system (EWS) is designed to monitor data from very large sensor networks in flood defences such as embankments, dikes, levees, and dams. The EWS, based on the internet, uses real-time sensor information and Artificial Intelligence (AI) to immediately calculate the probability of dike failure, the ensuing scenarios of dike breaching, predicted flood spreading and escape routes for people from the affected areas. Results are presented on interactive decision support systems that assist flood defence managers and public authorities during flood events. It can also be applied for policy development and for everyday dike condition assessment. The separate Virtual Dike module can be used for advanced research into failure mechanisms and dike stability. By consulting international stakeholders the designers ensured that the EWS is well aligned with user requirements. © 2013 IAHS Press.","Climate change; Early warning system; Flood; ICT; Sensor networks","Advanced researches; Condition assessments; Early Warning System; Early warning systems; ICT; Interactive decision support system; Policy development; Public authorities; Artificial intelligence; Climate change; Decision support systems; Flood control; Internet; Levees; Sensor networks; Floods; artificial intelligence; climate change; dike; early warning system; flood; Internet; policy development; probability; sensor",Conference Paper,Scopus,2-s2.0-84883437303
"Ren Z., Yuan J., Liu W.","Minimum near-convex shape decomposition",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",17,10.1109/TPAMI.2013.67,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883157016&doi=10.1109%2fTPAMI.2013.67&partnerID=40&md5=711a9075888d84e23059aec09b199464","Shape decomposition is a fundamental problem for part-based shape representation. We propose the minimum near-convex decomposition (MNCD) to decompose arbitrary shapes into minimum number of 'near-convex' parts. The near-convex shape decomposition is formulated as a discrete optimization problem by minimizing the number of nonintersecting cuts. Two perception rules are imposed as constraints into our objective function to improve the visual naturalness of the decomposition. With the degree of near-convexity a user-specified parameter, our decomposition is robust to local distortions and shape deformation. The optimization can be efficiently solved via binary integer linear programming. Both theoretical analysis and experiment results show that our approach outperforms the state-of-the-art results without introducing redundant parts and thus leads to robust shape representation. © 1979-2012 IEEE.","discrete optimization; Shape decomposition; shape representation","Discrete optimization; Discrete optimization problems; Local distortion; Minimizing the number of; Objective functions; Shape decomposition; Shape deformation; Shape representation; Integer programming; Optimization; algorithm; artificial intelligence; automated pattern recognition; computer assisted diagnosis; computer simulation; image enhancement; pattern recognition; procedures; statistical model; article; automated pattern recognition; computer assisted diagnosis; image enhancement; methodology; Algorithms; Artificial Intelligence; Computer Simulation; Form Perception; Image Enhancement; Image Interpretation, Computer-Assisted; Models, Statistical; Pattern Recognition, Automated; Algorithms; Artificial Intelligence; Computer Simulation; Form Perception; Image Enhancement; Image Interpretation, Computer-Assisted; Models, Statistical; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84883157016
"Perez D., Samothrakis S., Lucas S.M., Rohlfshagen P.","Rolling horizon evolution versus tree search for navigation in single-player real-time games",2013,"GECCO 2013 - Proceedings of the 2013 Genetic and Evolutionary Computation Conference",17,10.1145/2463372.2463413,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883115479&doi=10.1145%2f2463372.2463413&partnerID=40&md5=2a80201278ff7f6bb8658bb1f364747f","In real-time games, agents have limited time to respond to environmental cues. This requires either a policy defined up-front or, if one has access to a generative model, a very efficient rolling horizon search. In this paper, different search techniques are compared in a simple, yet interesting, realtime game known as the Physical Travelling Salesman Problem (PTSP). We introduce a rolling horizon version of a simple evolutionary algorithm that handles macro-actions and compare it against Monte Carlo Tree Search (MCTS), an approach known to perform well in practice, as well as random search. The experimental setup employs a variety of settings for both the action space of the agent as well as the algorithms used. We show that MCTS is able to handle very fine-grained searches whereas evolution performs better as we move to coarser-grained actions; the choice of algorithm becomes irrelevant if the actions are even more coarse-grained. We conclude that evolutionary algorithms can be a viable and competitive alternative to MCTS. Copyright © 2013 ACM.","Ai; Evolutionary algorithms; Games; Mcts; Real time","Environmental cues; Games; Generative model; Mcts; Monte Carlo tree search (MCTS); Real time; Search technique; Travelling salesman problem; Artificial intelligence; Real time systems; Traveling salesman problem; Evolutionary algorithms",Conference Paper,Scopus,2-s2.0-84883115479
"Kilsdonk E., Peute L.W., Riezebos R.J., Kremer L.C., Jaspers M.W.M.","From an expert-driven paper guideline to a user-centred decision support system: A usability comparison study",2013,"Artificial Intelligence in Medicine",17,10.1016/j.artmed.2013.04.004,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883561340&doi=10.1016%2fj.artmed.2013.04.004&partnerID=40&md5=af77bd59e4462d385f604dbfb3b2718a","Objective: To assess whether a user-centred prototype clinical decision support system (CDSS) providing patient-specific advice better supports healthcare practitioners in terms of (a) types of usability problems detected and (b) effective and efficient retrieval of childhood cancer survivor's follow-up screening procedures compared to an expert-driven paper-based guideline. Methods and materials: A user-centred design (UCD) process was employed to design a prototype CDSS. Usability problems in information retrieval with the paper-based guideline were assessed by think-aloud analysis with 13 participants. Both simple and more complex tasks were applied. The analysis provided input for the UCD process of the prototype. The usability of the prototype CDSS was subsequently evaluated by think-aloud analysis with the same participants. Usability problems of the paper-based guideline and the prototype CDSS were compared by using the classification of usability problems scheme. In addition, efficiency (time to complete task) and effectiveness (completeness of retrieved screening procedures) of information retrieval of participants in the expert-driven paper-based guideline and the user-centred prototype CDSS were compared. Results: Usability problems in both the paper-based guideline and the CDSS prototype were mainly classified as 'incongruent with participants' mental model'. The prototype CDSS reduced this type of problem from 17 to 6 problems. The time to perform simple information retrieval tasks increased by 58. s when using the prototype CDSS, however, it resulted in a 58% improvement in task completeness compared to the paper-based guideline. The time to perform complex scenarios decreased by 3:50. min with the prototype CDSS, with 17% higher completeness compared to the paper-based guideline. Conclusion: Analysis showed that usability problems experienced by healthcare practitioners when using a paper-based guideline could be overcome by implementing the guideline in a user-centred CDSS design. Although different types of usability problems were experienced with the prototype CDSS, they did not inhibit effective and efficient performance of tasks in the system. The usability problem analysis of the paper-based guideline effectively supported comparison of usability problems found in the two information retrieval systems and it supported the UCD of the CDSS. © 2013 Elsevier B.V.","Childhood cancer survivors; Clinical decision support system; Screening guidelines; Usability analysis; User-centred design","Childhood cancers; Clinical decision support systems; Comparison study; Methods and materials; Screening procedures; Usability analysis; Usability problems; User centred design; Artificial intelligence; Diseases; Health care; Information retrieval systems; Usability engineering; Decision support systems; adult; analytical error; article; cancer survivor; childhood cancer; clinical decision making; clinical decision support system; clinical effectiveness; controlled study; decision support system; follow up; health care personnel; human; information retrieval; intermethod comparison; paper based guideline; practice guideline; priority journal; process design; time; user centred design; Childhood cancer survivors; Clinical decision support system; Screening guidelines; Usability analysis; User-centred design; Decision Support Systems, Clinical; Guidelines as Topic; Outcome Assessment (Health Care)",Article,Scopus,2-s2.0-84883561340
"Moncayo-Martínez L.A., Zhang D.Z.","Optimising safety stock placement and lead time in an assembly supply chain using bi-objective MAX-MIN ant system",2013,"International Journal of Production Economics",17,10.1016/j.ijpe.2012.12.024,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880920764&doi=10.1016%2fj.ijpe.2012.12.024&partnerID=40&md5=f61bc44e5000a130b85f6b073f0665c8","The problem of placing safety inventory over a network, which assembles a product, is a challenging issue in supply chain design (SCD) because manufacturers always want to reduce inventory all over the supply chain (SC). Moreover, the process of designing a SC and then placing inventory, to offer high service level at the lowest possible cost, across a complex SC, is not an easy task for decision makers. In this paper we use the SC representation proposed by Graves and Willems (2000), Manufacturing & Service Operations Management 2 (1), 68-83, where a SC is divided into many supplying, manufacturing, and delivering stages. Our problem is to select one resource option to perform each stage, and based on the selected options to place an amount of inventory (in-progress and on-hand) at each stage, in order to offer a satisfactory customer service level with as low as possible total supply chain cost. A resource option here represents a supplier, a manufacturing plant (production line), or a transport mode in a supplying, manufacturing, or delivering stage, respectively. We developed an approach based on ant colony optimisation (ACO) to minimise simultaneously the total supply chain cost and the products' lead time to ensure product deliveries without delays. What are new in our approach are the bi-objective function and the computational efficiency of our ACO-based approach. In addition, ACO has not been applied to solve the inventory placement problem. As a validation of the model, we: (a) describe a successful application at CIFUNSA, one of the largest iron foundry in the world, and (b) compare different CPU time instances and metrics about multi-objective optimisation. © 2013 Elsevier B.V.","Ant colony; Meta-heuristics; Multi-objective optimisation; Safety stock placement; Supply chain design","Ant colonies; Ant colony optimisation; Customer service levels; Manufacturing plant; Meta heuristics; Safety stock placement; Service operations management; Supply chain design; Algorithms; Ant colony optimization; Artificial intelligence; Complex networks; Costs; Manufacture; Multiobjective optimization; Product design; Supply chains",Conference Paper,Scopus,2-s2.0-84880920764
"Aydin N., Murat A.","A swarm intelligence based sample average approximation algorithm for the capacitated reliable facility location problem",2013,"International Journal of Production Economics",17,10.1016/j.ijpe.2012.10.019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880923312&doi=10.1016%2fj.ijpe.2012.10.019&partnerID=40&md5=61a44d272f5d3dd6768efc4bf0e4ecbb","We present a novel hybrid method, swarm intelligence based sample average approximation (SIBSAA), for solving the capacitated reliable facility location problem (CRFLP). The CRFLP extends the well-known capacitated fixed-cost facility problem by accounting for the unreliability of facilities. The standard SAA procedure, while effectively used in many applications, can lead to poor solution quality if the selected sample sizes are not sufficiently large. With larger sample sizes, however, the SAA method is not practical due to the significant computational effort required. The proposed SIBSAA method addresses this limitation by using smaller samples and repetitively applying the SAA method while injecting social learning in the solution process inspired by the swarm intelligence of particle swarm optimization. We report on experimental study results showing that the SIBSAA improves the computational efficiency significantly while attaining same or better solution quality than the SAA method. © 2012 Elsevier B.V.","Facility location; Reliable; Sample average approximation; Stochastic programming; Swarm intelligence","Computational effort; Experimental studies; Facility location problem; Facility locations; Reliable; Sample average approximation; Solution quality; Swarm Intelligence; Approximation algorithms; Cost accounting; Particle swarm optimization (PSO); Stochastic programming; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84880923312
"Wang X., Li G.-Z.","Multilabel learning via random label selection for protein subcellular multilocations prediction",2013,"IEEE/ACM Transactions on Computational Biology and Bioinformatics",17,10.1109/TCBB.2013.21,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883001258&doi=10.1109%2fTCBB.2013.21&partnerID=40&md5=bc391b685f87f461fabbb9316d048d00","Prediction of protein subcellular localization is an important but challenging problem, particularly when proteins may simultaneously exist at, or move between, two or more different subcellular location sites. Most of the existing protein subcellular localization methods are only used to deal with the single-location proteins. In the past few years, only a few methods have been proposed to tackle proteins with multiple locations. However, they only adopt a simple strategy, that is, transforming the multilocation proteins to multiple proteins with single location, which does not take correlations among different subcellular locations into account. In this paper, a novel method named random label selection (RALS) (multilabel learning via RALS), which extends the simple binary relevance (BR) method, is proposed to learn from multilocation proteins in an effective and efficient way. RALS does not explicitly find the correlations among labels, but rather implicitly attempts to learn the label correlations from data by augmenting original feature space with randomly selected labels as its additional input features. Through the fivefold cross-validation test on a benchmark data set, we demonstrate our proposed method with consideration of label correlations obviously outperforms the baseline BR method without consideration of label correlations, indicating correlations among different subcellular locations really exist and contribute to improvement of prediction performance. Experimental results on two benchmark data sets also show that our proposed methods achieve significantly higher performance than some other state-of-the-art methods in predicting subcellular multilocations of proteins. © 2004-2012 IEEE.","multilabel learning; multilocation proteins; Protein subcellular localization; random label selection","Cross-validation tests; Multi-label learning; Multilocation; Prediction performance; Protein subcellular localization; random label selection; State-of-the-art methods; Subcellular location; Benchmarking; Forecasting; Statistical tests; Proteins; protein; protein; animal; article; artificial intelligence; biology; chemistry; classification; eukaryote; human; intracellular space; methodology; protein database; statistical model; biology; classification; intracellular space; procedures; Animals; Artificial Intelligence; Computational Biology; Databases, Protein; Eukaryota; Humans; Intracellular Space; Models, Statistical; Proteins; Animals; Artificial Intelligence; Computational Biology; Databases, Protein; Eukaryota; Humans; Intracellular Space; Models, Statistical; Proteins",Article,Scopus,2-s2.0-84883001258
"Kaliszyk C., Krauss A.","Scalable LCF-style proof translation",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",17,10.1007/978-3-642-39634-2_7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882747064&doi=10.1007%2f978-3-642-39634-2_7&partnerID=40&md5=1f4592516331f015596a99feab71d6af","All existing translations between proof assistants have been notoriously sluggy, resource-demanding, and do not scale to large developments, which has lead to the general perception that the whole approach is probably not practical. We aim to show that the observed inefficiencies are not inherent, but merely a deficiency of the existing implementations. We do so by providing a new implementation of a theory import from HOL Light to Isabelle/HOL, which achieves decent performance and scalability mostly by avoiding the mistakes of the past. After some preprocessing, our tool can import large HOL Light developments faster than HOL Light processes them. Our main target and motivation is the Flyspeck development, which can be imported in a few hours on commodity hardware. We also provide mappings for most basic types present in the developments including lists, integers and real numbers. This papers outlines some design considerations and presents a few of our extensive measurements, which reveal interesting insights in the low-level structure of larger proof developments. © 2013 Springer-Verlag.",,"Commodity hardware; Design considerations; Isabelle/HOl; Low-level structures; Performance and scalabilities; Proof assistant; Proof development; Real number; Artificial intelligence; Computer science; Theorem proving",Conference Paper,Scopus,2-s2.0-84882747064
"Kim J., Shin H.","Breast cancer survivability prediction using labeled, unlabeled, and pseudo-labeled patient data",2013,"Journal of the American Medical Informatics Association",17,10.1136/amiajnl-2012-001570,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882786709&doi=10.1136%2famiajnl-2012-001570&partnerID=40&md5=21f2f42833c7231129c011769ece28c9","Background: Prognostic studies of breast cancer survivability have been aided by machine learning algorithms, which can predict the survival of a particular patient based on historical patient data. However, it is not easy to collect labeled patient records. It takes at least 5 years to label a patient record as 'survived' or 'not survived'. Unguided trials of numerous types of oncology therapies are also very expensive. Confidentiality agreements with doctors and patients are also required to obtain labeled patient records. Proposed method: These difficulties in the collection of labeled patient data have led researchers to consider semi-supervised learning (SSL), a recent machine learning algorithm, because it is also capable of utilizing unlabeled patient data, which is relatively easier to collect. Therefore, it is regarded as an algorithm that could circumvent the known difficulties. However, the fact is yet valid even on SSL that more labeled data lead to better prediction. To compensate for the lack of labeled patient data, we may consider the concept of tagging virtual labels to unlabeled patient data, that is, 'pseudo-labels,' and treating them as if they were labeled. Results: Our proposed algorithm, 'SSL Co-training', implements this concept based on SSL. SSL Co-training was tested using the surveillance, epidemiology, and end results database for breast cancer and it delivered a mean accuracy of 76% and a mean area under the curve of 0.81.",,"accuracy; algorithm; area under the curve; article; breast cancer; cancer epidemiology; cancer survival; data base; female; human; machine learning; patient coding; prediction; Breast Cancer Survivability; Co Training; Machine Learning; Semi Supervised Learning; Algorithms; Artificial Intelligence; Breast Neoplasms; Computational Biology; Computer Simulation; Humans; Individualized Medicine; Pattern Recognition, Automated; Prognosis; Survival Analysis",Article,Scopus,2-s2.0-84882786709
"Laudani A., Riganti Fulginei F., Salvini A., Schmid M., Conforto S.","CFSO3: A new supervised swarm-based optimization algorithm",2013,"Mathematical Problems in Engineering",17,10.1155/2013/560614,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886945530&doi=10.1155%2f2013%2f560614&partnerID=40&md5=5ace42a8495ce1a37a84ccde53a6ba18","We present CFSO3, an optimization heuristic within the class of the swarm intelligence, based on a synergy among three different features of the Continuous Flock-of-Starlings Optimization. One of the main novelties is that this optimizer is no more a classical numerical algorithm since it now can be seen as a continuous dynamic system, which can be treated by using all the mathematical instruments available for managing state equations. In addition, CFSO3 allows passing from stochastic approaches to supervised deterministic ones since the random updating of parameters, a typical feature for numerical swam-based optimization algorithms, is now fully substituted by a supervised strategy: in CFSO3 the tuning of parameters is a priori designed for obtaining both exploration and exploitation. Indeed the exploration, that is, the escaping from a local minimum, as well as the convergence and the refinement to a solution can be designed simply by managing the eigenvalues of the CFSO state equations. Virtually in CFSO3, just the initial values of positions and velocities of the swarm members have to be randomly assigned. Both standard and parallel versions of CFSO3 together with validations on classical benchmarks are presented. © 2013 Antonino Laudani et al.",,"Continuous dynamic systems; Exploration and exploitation; Flock-of-starlings optimizations; Numerical algorithms; Optimization algorithms; Stochastic approach; Swarm Intelligence; Tuning of parameters; Artificial intelligence; Eigenvalues and eigenfunctions; Equations of state; Optimization; Algorithms",Article,Scopus,2-s2.0-84886945530
"Ortiz A., Górriz J.M., Ramírez J., Salas-González D.","Improving MRI segmentation with probabilistic GHSOM and multiobjective optimization",2013,"Neurocomputing",17,10.1016/j.neucom.2012.08.047,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878018986&doi=10.1016%2fj.neucom.2012.08.047&partnerID=40&md5=87e1eee4e523621a4f50d5b3705cdb12","In the last years, the improvements in Magnetic Resonance Imaging systems (MRI) provide new and additional ways to diagnose some brain disorders such as schizophrenia or the Alzheimer disease. One way to figure out these disorders from a MRI is through image segmentation. Image segmentation consist in partitioning an image into different regions. These regions determine different tissues present on the image. This results in a very interesting tool for neuroanatomical analyses. In this paper we present a segmentation method based on the Growing Hierarchical Self-Organizing Map and multiobjective-based feature selection to optimize the performance of the segmentation process. Since the features extracted from the image result crucial for the final performance of the segmentation process, optimized features are computed to maximize the performance of the segmentation process on each plane. The experiments performed on this paper use real brain scans from the Internet Brain Segmentation Repository (IBSR) and the Alzheimer Disease Neuroimaging Initiative (ADNI). Moreover, a comparison with other methods using the IBSR database shows that our method outperforms other algorithms. © 2012 Elsevier B.V.","Image segmentation; MRI; Multiobjective optimization; Self-Organizing Maps","Alzheimer disease; Brain disorders; Brain segmentation; Growing hierarchical self-organizing maps; MRI segmentation; Neuroanatomical analysis; Segmentation methods; Segmentation process; Conformal mapping; Image segmentation; Magnetic resonance imaging; Multiobjective optimization; Neurodegenerative diseases; Tissue; Neuroimaging; algorithm; Alzheimer disease; article; artificial intelligence; brain size; cerebrospinal fluid; classifier; cortical thickness (brain); gray matter; growing hierarchical self organizing map; hippocampus; image processing; image segmentation; multiobjective optimization; neuroimaging; nuclear magnetic resonance imaging; priority journal; white matter",Article,Scopus,2-s2.0-84878018986
"Mileo A., Abdelrahman A., Policarpio S., Hauswirth M.","StreamRule: A nonmonotonic stream reasoning system for the semantic web",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",17,10.1007/978-3-642-39666-3_23,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881141709&doi=10.1007%2f978-3-642-39666-3_23&partnerID=40&md5=807164a31206608bd9afa9288609708a","Stream reasoning is an emerging research field focused on dynamic processing and continuous reasoning over huge volumes of streaming data. Finding the right trade-off between scalability and expressivity is a key challenge in this area. In this paper, we want to provide a baseline for exploring the applicability of complex reasoning to the Web of Data based on a solution that combines results and approaches from database research, stream processing, and nonmonotonic logic programming. © 2013 Springer-Verlag.",,"Database research; Nonmonotonic; Nonmonotonic logic; Research fields; Stream processing; Stream reasonings; Streaming data; Web of datum; Artificial intelligence; Computer science; Logic programming",Conference Paper,Scopus,2-s2.0-84881141709
"Vatsavai R.R.","Gaussian multiple instance learning approach for mapping the slums of the world using very high resolution imagery",2013,"Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",17,10.1145/2487575.2488210,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973890288&doi=10.1145%2f2487575.2488210&partnerID=40&md5=4de5da282de1cafde5b2d950c38f687c","In this paper, we present a computationally efficient algorithm based on multiple instance learning for mapping informal settlements (slums) using very high-resolution remote sensing imagery. From remote sensing perspective, informal settlements share unique spatial characteristics that distinguish them from other urban structures like industrial, commercial, and formal residential settlements. However, regular pattern recognition and machine learning methods, which are predominantly single-instance or per-pixel classifiers, often fail to accurately map the informal settlements as they do not capture the complex spatial patterns. To overcome these limitations we employed a multiple instance based machine learning approach, where groups of contiguous pixels (image patches) are modeled as generated by a Gaussian distribution. We have conducted several experiments on very high-resolution satellite imagery, representing four unique geographic regions across the world. Our method showed consistent improvement in accurately identifying informal settlements. Copyright © 2013 ACM.","Mil; Remote sensing; Spatial data mining","Artificial intelligence; Data mining; Housing; Learning systems; Mapping; Pattern recognition; Pixels; Remote sensing; Satellite imagery; Computationally efficient; Machine learning approaches; Machine learning methods; Multiple instance learning; Spatial characteristics; Spatial data mining; Very high resolution; Very high resolution satellite imagery; Education",Conference Paper,Scopus,2-s2.0-84973890288
"Mendoza M.R., da Fonseca G.C., Loss-Morais G., Alves R., Margis R., Bazzan A.L.C.","RFMirTarget: Predicting Human MicroRNA Target Genes with a Random Forest Classifier",2013,"PLoS ONE",17,10.1371/journal.pone.0070153,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880777833&doi=10.1371%2fjournal.pone.0070153&partnerID=40&md5=21aec81bf6e188ab99670919893f1366","MicroRNAs are key regulators of eukaryotic gene expression whose fundamental role has already been identified in many cell pathways. The correct identification of miRNAs targets is still a major challenge in bioinformatics and has motivated the development of several computational methods to overcome inherent limitations of experimental analysis. Indeed, the best results reported so far in terms of specificity and sensitivity are associated to machine learning-based methods for microRNA-target prediction. Following this trend, in the current paper we discuss and explore a microRNA-target prediction method based on a random forest classifier, namely RFMirTarget. Despite its well-known robustness regarding general classifying tasks, to the best of our knowledge, random forest have not been deeply explored for the specific context of predicting microRNAs targets. Our framework first analyzes alignments between candidate microRNA-target pairs and extracts a set of structural, thermodynamics, alignment, seed and position-based features, upon which classification is performed. Experiments have shown that RFMirTarget outperforms several well-known classifiers with statistical significance, and that its performance is not impaired by the class imbalance problem or features correlation. Moreover, comparing it against other algorithms for microRNA target prediction using independent test data sets from TarBase and starBase, we observe a very promising performance, with higher sensitivity in relation to other methods. Finally, tests performed with RFMirTarget show the benefits of feature selection even for a classifier with embedded feature importance analysis, and the consistency between relevant features identified and important biological properties for effective microRNA-target gene alignment. © 2013 Mendoza et al.",,"microRNA; accuracy; algorithm; article; classification; classifier; computer prediction; intermethod comparison; random forest; sensitivity and specificity; Algorithms; Artificial Intelligence; Base Sequence; Computational Biology; Gene Expression Regulation; Humans; MicroRNAs; Models, Genetic; Sequence Alignment; Software",Article,Scopus,2-s2.0-84880777833
"Al-Jasmi A., Nasr H., Goel H.K., Moricca G., Carvajal G.A., Dhar J., Querales M., Villamizar M.A., Cullick A.S., Rodriguez J.A., Velasquez G., Yong Z., Bermudez F., Kain J.","ESP ""smart flow"" integrates quality and control data for diagnostics and optimization in real time",2013,"Society of Petroleum Engineers - SPE Digital Energy Conference and Exhibition 2013",17,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880499031&partnerID=40&md5=fd6e7d1bbf0e97325ccd5edf521da16e","Intelligent digital oilfield (iDOF) operations include the transfer, monitoring, visualization, analysis, and interpretation of real-time data. Enabling this process requires a significant investment to upgrade surface, subsurface, and well instrumentation and also the installation of a sophisticated infrastructure for data transmission and visualization. Once upgraded, the system then has the capability to transfer massive quantities of data, converting it into real information at the right time. The transformation of raw data into information is achieved through intelligent, automated work processes, which are referred to here as ""smart flows,"" which assist engineers in their daily well surveillance activities, helping make them more productive and improve decision making. A major oil and gas operator in the Middle East has invested in such an infrastructure and is developing a set of smart flows for key activities and work flows for its production operations, with the ultimate goal of improved asset performance. The project includes a smart flow to monitor, diagnose, and optimize wells that include electric submersible pumps (ESP); 50% of the wells in this field use ESPs. The ESP smart flow includes leading-edge technologies, such as: variable speed drive controller, subsurface equipment and sensors, advanced diagnostics based on artificial-intelligence agents, analysis of sensors signals, and automatically identifying ESP optimum operating conditions. Using a steady-state nodal-analysis model combined with an artificial intelligent technique, the ESP smart flow is designed to provide rapid diagnostics and optimization in real time, generating actions, such as decreasing and increasing the pump frequency and choke setting. The ultimate benefit is to detect the signals that foresee unexpected well downtime and predict ESP system pump failures. The paper describes the main functionalities of the ESP smart flow as a powerful optimization tool that is capable of providing an interactive monitoring system that can assist operations personnel in managing ESP-operated wells. Copyright 2013, Society of Petroleum Engineers.",,"Advanced diagnostics; Artificial intelligent techniques; Electric submersible pumps; Oil and gas operators; Optimization tools; Optimum operating conditions; Production operations; Sub-surface equipment; Artificial intelligence; Exhibitions; Investments; Metadata; Oil fields; Optimization; Sensors; Submersible pumps; Variable speed drives; Visualization; Quality control",Conference Paper,Scopus,2-s2.0-84880499031
"Al-Jasmi A., Goel H.K., Nasr H., Carvajal G.A., Johnson D.W., Cullick A.S., Rodriguez J.A., Moricca G., Velasquez G., Villamizar M., Querales M.","A surveillance ""smart flow"" for intelligent digital production operations",2013,"Society of Petroleum Engineers - SPE Digital Energy Conference and Exhibition 2013",17,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880455937&partnerID=40&md5=a2ad98ed58ba36d83918b27ce1ec2c21","Intelligent digital oilfield (iDOF) operations include the transfer, monitoring, visualization, analysis, and interpretation of real-time data. Enabling this process requires a significant investment to upgrade surface, subsurface, and well instrumentation and also the installation of a sophisticated infrastructure for data transmission and visualization. Once upgraded, the system has the capability to transfer massive quantities of data, converting it into real information at the right time. The transformation of raw data into information is achieved through intelligent, automated work processes, referred to here as ""smart flows,"" which assist engineers in their daily well surveillance activities, helping make them more productive and improve decision making. A major oil and gas operator in the Middle East has invested in such an infrastructure and is developing a set of smart flows for key activities and work flows for its production operations, with the ultimate goal of improved asset performance. This paper explains the development of the production surveillance smart flow, which provides engineers with automated artificial intelligence that analyzes data, provides guidance on well operations, and, when necessary, gives warning and alarms as conditions warrant them. The suite of artificial intelligence consists of advanced correlation statistics, neural-network predictive algorithms, and expert systems. Copyright 2013, Society of Petroleum Engineers.",,"Correlation statistics; Digital oilfield; Digital production; Massive quantities; Oil and gas operators; Predictive algorithms; Production operations; Real-time data; Artificial intelligence; Exhibitions; Expert systems; Metadata; Monitoring; Oil fields; Visualization; Investments",Conference Paper,Scopus,2-s2.0-84880455937
"Chihani Z., Miller D., Renaud F.","Foundational proof certificates in first-order logic",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",17,10.1007/978-3-642-38574-2_11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879974077&doi=10.1007%2f978-3-642-38574-2_11&partnerID=40&md5=e75ae71d7b249d238e6cfdfe39fb4ba0","It is the exception that provers share and trust each others proofs. One reason for this is that different provers structure their proof evidence in remarkably different ways, including, for example, proof scripts, resolution refutations, tableaux, Herbrand expansions, natural deductions, etc. In this paper, we propose an approach to foundational proof certificates as a means of flexibly presenting proof evidence so that a relatively simple and universal proof checker can check that a certificate does, indeed, elaborate to a formal proof. While we shall limit ourselves to first-order logic in this paper, we shall not limit ourselves in many other ways. Our framework for defining and checking proof certificates will work with classical and intuitionistic logics and with proof structures as diverse as resolution refutations, matings, and natural deduction. © 2013 Springer-Verlag.",,"First order logic; Formal proofs; Intuitionistic logic; Matings; Natural deduction; Proof checkers; Resolution refutation; Artificial intelligence; Computer science; Formal logic",Conference Paper,Scopus,2-s2.0-84879974077
"Kan M., Xu D., Shan S., Li W., Chen X.","Learning prototype hyperplanes for face verification in the wild",2013,"IEEE Transactions on Image Processing",17,10.1109/TIP.2013.2256918,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879969121&doi=10.1109%2fTIP.2013.2256918&partnerID=40&md5=e8df0fab1390b10f32188b1b31bbc08f","In this paper, we propose a new scheme called Prototype Hyperplane Learning (PHL) for face verification in the wild using only weakly labeled training samples (i.e., we only know whether each pair of samples are from the same class or different classes without knowing the class label of each sample) by leveraging a large number of unlabeled samples in a generic data set. Our scheme represents each sample in the weakly labeled data set as a mid-level feature with each entry as the corresponding decision value from the classification hyperplane (referred to as the prototype hyperplane) of one Support Vector Machine (SVM) model, in which a sparse set of support vectors is selected from the unlabeled generic data set based on the learnt combination coefficients. To learn the optimal prototype hyperplanes for the extraction of mid-level features, we propose a Fisher's Linear Discriminant-like (FLD-like) objective function by maximizing the discriminability on the weakly labeled data set with a constraint enforcing sparsity on the combination coefficients of each SVM model, which is solved by using an alternating optimization method. Then, we use the recent work called Side-Information based Linear Discriminant (SILD) analysis for dimensionality reduction and a cosine similarity measure for final face verification. Comprehensive experiments on two data sets, Labeled Faces in the Wild (LFW) and YouTube Faces, demonstrate the effectiveness of our scheme. © 1992-2012 IEEE.","Face verification in the wild; mid-level feature representation; prototype hyperplane learning","Alternating optimizations; Cosine similarity measures; Dimensionality reduction; Face Verification; Labeled faces in the wilds (LFW); Linear discriminants; Mid-level features; prototype hyperplane learning; Discriminant analysis; Support vector machines; Geometry; algorithm; anatomy and histology; artificial intelligence; automated pattern recognition; biometry; computer assisted diagnosis; face; human; image enhancement; image subtraction; procedures; reproducibility; sensitivity and specificity; three dimensional imaging; automated pattern recognition; biometry; computer assisted diagnosis; face; histology; letter; methodology; three dimensional imaging; Algorithms; Artificial Intelligence; Biometry; Face; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique; Algorithms; Artificial Intelligence; Biometry; Face; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-84879969121
"Juan-Verdejo A., Baars H.","Decision support for partially moving applications to the cloud - The example of business intelligence",2013,"HotTopiCS 2013 - Proceedings of the 2013 International Workshop on Hot Topics in Cloud Services",17,10.1145/2462307.2462316,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879779335&doi=10.1145%2f2462307.2462316&partnerID=40&md5=0d24147f9049b59f1691f6167a6458fa","Cloud computing services have evolved to a sourcing option that promises a wide range of benefits, such as increased scalability and flexibility at reduced costs. However, many enterprise applications are subject to strict requirements - e.g. regarding privacy, security and availability - and are embedded into complex enterprise IT architectures with a multitude of interdependencies. For these reasons, many decision makers have developed a sceptical stance towards cloud computing. A solution might be a hybrid (local/cloud infrastructure) approach where only suited components are migrated to a cloud infrastructure. This, however, has significant architectural consequences that need to be taken into account. This contribution suggests a cloud migration framework that will be implemented as an IT-based decision support system based on modelling the interdependencies between components. The approach is illustrated with the example of Business Intelligence (BI), i.e. integrated approaches to management support. The underlying decision model would particularly consider data transfer volumes, performance, sensitivity of cloud-based data repositories, as well as exposure to public networks. The potential of such an approach is illustrated with a selected set of BI scenarios. Based on this, conclusions are derived and generalised for approaches taking into account deployments on both the local premises and cloud infrastructures. Copyright © 2013 ACM.","Business intelligence; Cloud computing; Decision-making; Enterprise applications; Migration; Multiple criteria decision making; Security policies","Cloud computing services; Cloud infrastructures; Enterprise applications; Integrated approach; Management support; Migration; Multiple criteria decision making; Security policy; Artificial intelligence; Cloud computing; Competitive intelligence; Complex networks; Data transfer; Decision making; Decision support systems; Distributed database systems; Industry; Web services; Management science",Conference Paper,Scopus,2-s2.0-84879779335
"Che Z., Zhang S., Shao Y., Fan L., Xu H., Yu X., Zhi X., Yao X., Zhang R.","Synthesis and quantitative structure-activity relationship (QSAR) study of novel N-arylsulfonyl-3-acylindole arylcarbonyl hydrazone derivatives as nematicidal agents",2013,"Journal of Agricultural and Food Chemistry",17,10.1021/jf400536q,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879376935&doi=10.1021%2fjf400536q&partnerID=40&md5=887c73c31a90d7cc99f5bbde7d250758","In continuation of our program aimed at the discovery and development of natural-product-based pesticidal agents, 54 novel N-arylsulfonyl-3-acylindole arylcarbonyl hydrazone derivatives were prepared, and their structures were well characterized by 1H NMR, 13C NMR, HRMS, ESI-MS, and mp. Their nematicidal activity was evaluated against that of the pine wood nematode, Bursaphelenchus xylophilus in vivo. Among all of the derivatives, especially V-12 and V-39 displayed the best promising nematicidal activity with LC 50 values of 1.0969 and 1.2632 mg/L, respectively. This suggested that introduction of R1 and R2 together as the electron-withdrawing substituents, R3 as the methyl group, and R 4 as the phenyl with the electron-donating substituents could be taken into account for further preparation of these kinds of compounds as nematicidal agents. Six selected descriptors are a WHIM descriptor (E1m), two GETAWAY descriptors (R1m+ and R3m+), a Burden eigenvalues descriptor (BEHm8), and two edge-adjacency index descriptors (EEig05x and EEig13d). Quantitative structure-activity relationship (QSAR) studies demonstrated that the structural factors, such as molecular mass (a negative correlation with the bioactivity) and molecular polarity (a positive correlation with bioactivity), are likely to govern the nematicidal activities of these compounds. For this model, the correlation coefficient (R2 training set), the leave-one-out cross-validation correlation coefficient (Q2 LOO), and the 7-fold cross-validation correlation coefficient (Q 2 7-fold) were 0.791, 0.701, and 0.715, respectively. The external cross-validation correlation coefficient (Q2 ext) and the root-mean-square error for the test set (RMSEtest set) were 0.774 and 3.412, respectively. This study will pave the way for future design, structural modification, and development of indole derivatives as nematicidal agents. © 2013 American Chemical Society.","botanical pesticide; Bursaphelenchus xylophilus; hydrazone; indole; nematicidal activity; QSAR; structural modification","Botanical pesticides; Bursaphelenchus xylophilus; Hydrazones; indole; Nematicidal activity; QSAR; Structural modifications; Eigenvalues and eigenfunctions; Functional groups; Nuclear magnetic resonance spectroscopy; Polycyclic aromatic hydrocarbons; Statistical methods; Computational chemistry; antinematodal agent; hydrazone derivative; pesticide; animal; article; artificial intelligence; biological model; biology; chemical structure; chemistry; China; drug effect; expert system; growth, development and aging; LD 50; methodology; nematode; quantitative structure activity relation; synthesis; validation study; Animals; Antinematodal Agents; Artificial Intelligence; Chemistry, Agricultural; China; Computational Biology; Expert Systems; Hydrazones; Lethal Dose 50; Models, Biological; Molecular Structure; Nematoda; Pesticides; Quantitative Structure-Activity Relationship; Bursaphelenchus; Bursaphelenchus xylophilus",Article,Scopus,2-s2.0-84879376935
"Nunes T.M., De Albuquerque V.H.C., Papa J.P., Silva C.C., Normando P.G., Moura E.P., Tavares J.M.R.S.","Automatic microstructural characterization and classification using artificial intelligence techniques on ultrasound signals",2013,"Expert Systems with Applications",17,10.1016/j.eswa.2012.12.025,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874662110&doi=10.1016%2fj.eswa.2012.12.025&partnerID=40&md5=9c75a55272cc5fff931b6c0d9daabc45","Secondary phases such as Laves and carbides are formed during the final solidification stages of nickel based superalloy coatings deposited during the gas tungsten arc welding cold wire process. However, when aged at high temperatures, other phases can precipitate in the microstructure, like the γ″ and δ phases. This work presents a new application and evaluation of artificial intelligent techniques to classify (the background echo and backscattered) ultrasound signals in order to characterize the microstructure of a Ni-based alloy thermally aged at 650 and 950 °C for 10, 100 and 200 h. The background echo and backscattered ultrasound signals were acquired using transducers with frequencies of 4 and 5 MHz. Thus with the use of features extraction techniques, i.e.; detrended fluctuation analysis and the Hurst method, the accuracy and speed in the classification of the secondary phases from ultrasound signals could be studied. The classifiers under study were the recent optimum-path forest (OPF) and the more traditional support vector machines and Bayesian. The experimental results revealed that the OPF classifier was the fastest and most reliable. In addition, the OPF classifier revealed to be a valid and adequate tool for microstructure characterization through ultrasound signals classification due to its speed, sensitivity, accuracy and reliability. © 2013 Elsevier B.V. All rights reserved.","Bayesian classifiers; Detrended fluctuation analysis and Hurst method; Feature extraction; Nickel-based alloy; Non-destructive inspection; Optimum-path forest; Support vector machines; Thermal aging","Bayesian classifier; Detrended fluctuation analysis; Nickel based alloy; Non destructive inspection; Optimum-path forests; Artificial intelligence; Carbides; Feature extraction; Forestry; Microstructure; Nickel; Nickel coatings; Support vector machines; Thermal aging; Ultrasonic waves; Alloy; Carbides; Coatings; Forestry; Microstructure; Nickel",Article,Scopus,2-s2.0-84874662110
"He L., Yang Z., Zhao Z., Lin H., Li Y.","Extracting Drug-Drug Interaction from the Biomedical Literature Using a Stacked Generalization-Based Approach",2013,"PLoS ONE",17,10.1371/journal.pone.0065814,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878962314&doi=10.1371%2fjournal.pone.0065814&partnerID=40&md5=27e1e5548093a0fd587e0e027029adae","Drug-drug interaction (DDI) detection is particularly important for patient safety. However, the amount of biomedical literature regarding drug interactions is increasing rapidly. Therefore, there is a need to develop an effective approach for the automatic extraction of DDI information from the biomedical literature. In this paper, we present a Stacked Generalization-based approach for automatic DDI extraction. The approach combines the feature-based, graph and tree kernels and, therefore, reduces the risk of missing important features. In addition, it introduces some domain knowledge based features (the keyword, semantic type, and DrugBank features) into the feature-based kernel, which contribute to the performance improvement. More specifically, the approach applies Stacked generalization to automatically learn the weights from the training data and assign them to three individual kernels to achieve a much better performance than each individual kernel. The experimental results show that our approach can achieve a better performance of 69.24% in F-score compared with other systems in the DDI Extraction 2011 challenge task. © 2013 He et al.",,"analytical error; area under the curve; article; comparative study; correlation coefficient; data extraction; drug inhibition; drug interaction; drug metabolism; information processing; kernel method; linear regression analysis; machine learning; mathematical analysis; medical literature; receiver operating characteristic; semantics; stacked generalization; support vector machine; Algorithms; Artificial Intelligence; Drug Interactions; Humans; Reproducibility of Results; Software",Article,Scopus,2-s2.0-84878962314
"Li X., Dick A., Shen C., Zhang Z., Van Den Hengel A., Wang H.","Visual tracking with spatio-temporal dempster-shafer information fusion",2013,"IEEE Transactions on Image Processing",17,10.1109/TIP.2013.2253478,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878498837&doi=10.1109%2fTIP.2013.2253478&partnerID=40&md5=51070e39316eaf5269c009435e847cca","A key problem in visual tracking is how to effectively combine spatio-temporal visual information from throughout a video to accurately estimate the state of an object. We address this problem by incorporating Dempster-Shafer (DS) information fusion into the tracking approach. To implement this fusion task, the entire image sequence is partitioned into spatially and temporally adjacent subsequences. A support vector machine (SVM) classifier is trained for object/nonobject classification on each of these subsequences, the outputs of which act as separate data sources. To combine the discriminative information from these classifiers, we further present a spatio-temporal weighted DS (STWDS) scheme. In addition, temporally adjacent sources are likely to share discriminative information on object/nonobject classification. To use such information, an adaptive SVM learning scheme is designed to transfer discriminative information across sources. Finally, the corresponding DS belief function of the STWDS scheme is embedded into a Bayesian tracking model. Experimental results on challenging videos demonstrate the effectiveness and robustness of the proposed tracking approach. © 1992-2012 IEEE.","Adaptive; appearance model; Dempster-Shafer (DS) information fusion; multisource discriminative learning; support vector machine (SVM) learning; visual tracking","Adaptive; Appearance models; Dempster-shafer; Discriminative learning; Visual Tracking; Information fusion; Tracking (position); Uncertainty analysis; Support vector machines; algorithm; article; artificial intelligence; automated pattern recognition; Bayes theorem; computer assisted diagnosis; image enhancement; image subtraction; methodology; reproducibility; sensitivity and specificity; spatiotemporal analysis; automated pattern recognition; computer assisted diagnosis; procedures; Algorithms; Artificial Intelligence; Bayes Theorem; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Spatio-Temporal Analysis; Subtraction Technique; Algorithms; Artificial Intelligence; Bayes Theorem; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Spatio-Temporal Analysis; Subtraction Technique",Article,Scopus,2-s2.0-84878498837
"Noschinski L., Emmes F., Giesl J.","Analyzing innermost runtime complexity of term rewriting by dependency pairs",2013,"Journal of Automated Reasoning",17,10.1007/s10817-013-9277-6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877627811&doi=10.1007%2fs10817-013-9277-6&partnerID=40&md5=ed8ef746dd829b947c0936e7558a09e9","We present a modular framework to analyze the innermost runtime complexity of term rewrite systems automatically. Our method is based on the dependency pair framework for termination analysis. In contrast to previous work, we developed a direct adaptation of successful termination techniques from the dependency pair framework in order to use them for complexity analysis. By extensive experimental results, we demonstrate the power of our method compared to existing techniques. © 2013 Springer Science+Business Media Dordrecht.","Complexity analysis; Dependency pairs; Term rewriting; Termination analysis","Complexity analysis; Dependency pairs; Modular framework; Run time complexity; Term rewrite systems; Term rewriting; Termination analysis; Automata theory; Software engineering; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84877627811
"Hatzi O., Vrakas D., Bassiliades N., Anagnostopoulos D., Vlahavas I.","The PORSCE II framework: Using AI planning for automated Semantic Web service composition",2013,"Knowledge Engineering Review",17,10.1017/S0269888912000392,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877601091&doi=10.1017%2fS0269888912000392&partnerID=40&md5=46a6599a593c543a8af5bff27ef0a109","This paper presents PORSCE II, an integrated system that performs automatic Semantic Web service composition exploiting artificial intelligence (AI) techniques, specifically planning. Essential steps in achieving Web service composition include the translation of the Web service composition problem into a solver-ready planning domain and problem, followed by the acquisition of solutions, and the translation of the solutions back to Web service terms. The solutions to the problem, that is, the descriptions of the desired composite service, are obtained by means of external domain-independent planning systems, they are visualized and finally evaluated. Throughout the entire process, the system exploits semantic information extracted from the semantic descriptions of the available Web services and the corresponding ontologies, in order to perform composition under semantic awareness and relaxation. Copyright © Cambridge University Press 2013.",,"Composite services; Domain-independent planning; Integrated systems; Planning domains; Semantic descriptions; Semantic information; Semantic web service compositions; Web service composition; Artificial intelligence; Semantics; Web services; Websites; Quality of service",Article,Scopus,2-s2.0-84877601091
"Wang N., Zeng W., Chen L.","SACICA: A sparse approximation coefficient-based ICA model for functional magnetic resonance imaging data analysis",2013,"Journal of Neuroscience Methods",17,10.1016/j.jneumeth.2013.03.014,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877894720&doi=10.1016%2fj.jneumeth.2013.03.014&partnerID=40&md5=dd5be75f101909c6166587b8540fe535","Independent component analysis (ICA) has been widely used in functional magnetic resonance imaging (fMRI) data to evaluate the functional connectivity, which assumes that the sources of functional networks are statistically independent. Recently, many researchers have demonstrated that sparsity is an effective assumption for fMRI signal separation. In this research, we present a sparse approximation coefficient-based ICA (SACICA) model to analyse fMRI data, which is a promising combination model of sparse features and an ICA technique.The SACICA method consists of three procedures. The wavelet packet decomposition procedure, which decomposes the fMRI data into wavelet tree nodes with different degrees of sparsity, is first. Then, the sparse approximation coefficients set formation procedure, in which an effective Lp norm is proposed to measure the sparse degree of the distinct wavelet tree nodes, is second. The ICA decomposition and reconstruction procedure, which utilises the sparse approximation coefficients set of the fMRI data, is last.The hybrid data experimental results demonstrated that the SACICA method exhibited the stronger spatial source reconstruction ability with respect to the unsmoothed fMRI data and better detection sensitivity of the functional signal on the smoothed fMRI data than the FastICA method. Furthermore, task-related experiments also revealed that SACICA was not only effective in discovering the functional networks but also exhibited a better detection sensitivity of the visual-related functional signal. In addition, the SACICA combined with Fast-FENICA proposed by Wang et al. (2012) was demonstrated to conduct the group analysis effectively on the resting-state data set. © 2013 Elsevier B.V.","FMRI; ICA; Lp norm; Sparsity; Wavelet packet decomposition","article; data analysis; functional magnetic resonance imaging; independent component analysis; priority journal; sensitivity analysis; sparse approximation coefficient based independent component analysis; wavelet analysis; Algorithms; Artificial Intelligence; Brain; Brain Mapping; Data Interpretation, Statistical; Humans; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Nerve Net; Pattern Recognition, Automated; Principal Component Analysis; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84877894720
"Martius G., Der R., Ay N.","Information Driven Self-Organization of Complex Robotic Behaviors",2013,"PLoS ONE",17,10.1371/journal.pone.0063400,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878336340&doi=10.1371%2fjournal.pone.0063400&partnerID=40&md5=3915b6fbf5eb8bbca473e39aed9f7724","Information theory is a powerful tool to express principles to drive autonomous systems because it is domain invariant and allows for an intuitive interpretation. This paper studies the use of the predictive information (PI), also called excess entropy or effective measure complexity, of the sensorimotor process as a driving force to generate behavior. We study nonlinear and nonstationary systems and introduce the time-local predicting information (TiPI) which allows us to derive exact results together with explicit update rules for the parameters of the controller in the dynamical systems framework. In this way the information principle, formulated at the level of behavior, is translated to the dynamics of the synapses. We underpin our results with a number of case studies with high-dimensional robotic systems. We show the spontaneous cooperativity in a complex physical system with decentralized control. Moreover, a jointly controlled humanoid robot develops a high behavioral variety depending on its physics and the environment it is dynamically embedded into. The behavior can be decomposed into a succession of low-dimensional modes that increasingly explore the behavior space. This is a promising way to avoid the curse of dimensionality which hinders learning systems to scale well. © 2013 Martius et al.",,"article; behavior; conceptual framework; entropy; information processing; learning algorithm; physics; robotics; sensory feedback; synapse; time local predicting information; Algorithms; Animals; Artificial Intelligence; Behavior, Animal; Computer Simulation; Entropy; Information Theory; Markov Chains; Nonlinear Dynamics; Personal Autonomy; Robotics",Article,Scopus,2-s2.0-84878336340
"Kirchner E.A., Albiez J.C., Seeland A., Jordan M., Kirchner F.","Towards assistive robotics for home rehabilitation",2013,"BIODEVICES 2013 - Proceedings of the International Conference on Biomedical Electronics and Devices",17,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877993804&partnerID=40&md5=0d479dbfb1f00a175cfdbd8047f7cb8e","In this paper, we want to point out the possibilities that arise from the latest advances in robotic exoskeleton design and control. We show that approaches of artificial intelligence research and robotics that integrate psychophysiological data analysis offer the possibility to assist disabled people in their everyday lives. Thus, continuous long term rehabilitation training and daily support can be provided in the future to help them to regain motor functions. We outline a possible scenario for fully embedded home rehabilitation and its components. The presented work further investigates two challenges of the application of such a system in more detail: (i) improvement of the interaction between the patient and the supporting interface and (ii) enhancement of reliability of predictions made about the patients intention. In the experimental part we demonstrate that the exoskeleton control can compensate for gravitational loads, imposed by the device itself. Further, we present results that show that movement onset prediction can be made based on different psychophysiological measures, and can be improved with respect to their reliability.","Cognitive human-robot interaction; Exoskeleton; Psychophysiological data; Rehabilitation robotics; Virtual reality and interfaces","Cognitive human-robot interactions; Exoskeleton; Psychophysiological data; Rehabilitation robotics; Virtual reality and interfaces; Artificial intelligence; Handicapped persons; Human computer interaction; Virtual reality; Robotics",Conference Paper,Scopus,2-s2.0-84877993804
"Smith F., Proietti M.","Rule-based behavioral reasoning on semantic business processes",2013,"ICAART 2013 - Proceedings of the 5th International Conference on Agents and Artificial Intelligence",17,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877940702&partnerID=40&md5=1e026bdc68ee582332defd6aa0e2898f","We propose a rule-based framework for representing and reasoning about business processes from both the procedural and ontological point of views. To this end we define a rule-based procedural semantics for a relevant fragment of BPMN, a very popular graphical notation for specifying business processes. Our semantics defines a state transition system by following an approach similar to the Fluent Calculus, and allows us to specify state change in terms of preconditions and effects of the enactment of activities. Then we show how the procedural process knowledge can be seamlessly integrated with the domain knowledge specified by using the OWL-RL rule-based ontology language. Our framework provides a wide range of reasoning services by using standard logic programming inference engines. In particular, we can perform very sophisticated reasoning tasks by combining both procedural and domain dependent knowledge. A preliminary implementation shows that our approach is effective in practice.","Business processes; Ontologies; Rule-based reasoning; Verification","Behavioral reasoning; Business Process; Graphical notation; Ontology language; Procedural semantics; Process knowledge; Rule based reasoning; State transition systems; Artificial intelligence; Logic programming; Ontology; Verification; Semantics",Conference Paper,Scopus,2-s2.0-84877940702
"Jaya Bharata Reddy M., Venkata Rajesh D., Mohanta D.K.","Robust transmission line fault classification using wavelet multi-resolution analysis",2013,"Computers and Electrical Engineering",17,10.1016/j.compeleceng.2013.02.013,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878832294&doi=10.1016%2fj.compeleceng.2013.02.013&partnerID=40&md5=5ab409279e428428513dba3165a40185","With the advent of high speed communication technology, global positioning system (GPS) and artificial intelligence (AI) techniques, there has been a paradigm shift in the parlance of power grid operation and control. The power system is in a phase of transition towards smart grid with the aid of these techniques to combat against contingencies, leading to reduction of failures and blackouts. The transmission lines are considered to be the back bone of the grid and traverse over difficult terrains. With the advancements in digital relaying and wide-area protection along with GPS technology, philosophy of protection has also undergone a paradigm change to take care of such challenges. This paper explores the possibility of transmission line protection for multi-generator system using wavelet Multi-Resolution Analysis (MRA) technique along with GPS. The inputs for the wavelet transform are the synchronized currents measured by remote telemetry units (RTUs) in conjunction with GPS technology at different buses. The classification algorithm uses wavelet MRA technique to extract features of the transient current signals based on harmonics generated due to abrupt change of currents in a three-phase transmission line caused by different faults. The major contribution of this paper is that the classification algorithm is immune to the effects of fault inception angle, fault impedance, fault distance and power angle. The results validate the efficacy of the proposed algorithm for real time smart grid operation. © 2013 Elsevier Ltd. All rights reserved.",,"Classification algorithm; Fault inception angles; High-speed communications; Power grid operations; Transient current signals; Transmission line protection; Wavelet multi-resolution analysis; Wide area protection; Algorithms; Artificial intelligence; Electric equipment protection; Electric fault currents; Smart power grids; Transmission line theory; Electric lines",Article,Scopus,2-s2.0-84878832294
"Van Hees V.T., Golubic R., Ekelund U., Brage S.","Impact of study design on development and evaluation of an activity-type classifier",2013,"Journal of Applied Physiology",17,10.1152/japplphysiol.00984.2012,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878568785&doi=10.1152%2fjapplphysiol.00984.2012&partnerID=40&md5=274f2f8db377a708e87d691adf8f3576","Methods to classify activity types are often evaluated with an experimental protocol involving prescribed physical activities under confined (laboratory) conditions, which may not reflect real-life conditions. The present study aims to evaluate how study design may impact on classifier performance in real life. Twenty-eight healthy participants (21-53 yr) were asked to wear nine triaxial accelerometers while performing 58 activity types selected to simulate activities in real life. For each sensor location, logistic classifiers were trained in subsets of up to 8 activities to distinguish between walking and nonwalking activities and were then evaluated in all 58 activities. Different weighting factors were used to convert the resulting confusion matrices into an estimation of the confusion matrix as would apply in the real-life setting by creating four different real-life scenarios, as well as one traditional laboratory scenario. The sensitivity of a classifier estimated with a traditional laboratory protocol is within the range of estimates derived from real-life scenarios for any body location. The specificity, however, was systematically overestimated by the traditional laboratory scenario. Walking time was systematically overestimated, except for lower back sensor data (range: 7-757%). In conclusion, classifier performance under confined conditions may not accurately reflect classifier performance in real life. Future studies that aim to evaluate activity classification methods are warranted to pay special attention to the representativeness of experimental conditions for real-life conditions. Copyright © 2013 the American Physiological Society.","Accelerometry; Classification; Monitor; Physical activity","actimetry; adult; article; artificial intelligence; classification; daily life activity; equipment; equipment design; female; human; male; medical research; methodology; middle aged; motor activity; principal component analysis; questionnaire; reproducibility; signal processing; statistical model; time; transducer; walking; Actigraphy; Activities of Daily Living; Adult; Artificial Intelligence; Biomedical Research; Equipment Design; Female; Humans; Logistic Models; Male; Middle Aged; Motor Activity; Principal Component Analysis; Questionnaires; Reproducibility of Results; Research Design; Signal Processing, Computer-Assisted; Time Factors; Transducers; Walking; Young Adult",Article,Scopus,2-s2.0-84878568785
"Sun C., Haibin D.","Artificial bee colony optimized controller for unmanned rotorcraft pendulum",2013,"Aircraft Engineering and Aerospace Technology",17,10.1108/00022661311302715,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875725762&doi=10.1108%2f00022661311302715&partnerID=40&md5=81e029524e230682dba7618f6ae2e649","Purpose - The purpose of this paper is to propose a new algorithm for pendulum-like oscillation control of an unmanned rotorcraft (UR) in a reconnaissance mission and improve the stabilizing performance of the UR's hover and stare. Design/methodology/approach - The algorithm is based on linear-quadratic regulator (LQR), of which the determinable parameters are optimized by the artificial bee colony (ABC) algorithm, a newly developed algorithm inspired by swarm intelligence and motivated by the intelligent behaviour of honey bees. Findings - The proposed algorithm is tested in a UR simulation environment and achieves stabilization of the pendulum oscillation in less than 4s. Research limitations/implications - The presented algorithm and design strategy can be extended for other types of complex control missions where relative parameters must be optimized to get a better control performance. Practical implications - The ABC optimized control system developed can be easily applied to practice and can safely stabilize the UR during hover and stare, which will considerably improve the stability of the UR and lead to better reconnaissance performance. Originality/value - This research presents a new algorithm to control the pendulum-like oscillation of URs, whose performance of hover and stare is a key issue when carrying out new challenging reconnaissance missions in urban warfare. Simulation results show that the presented algorithm performs better than traditional methods and the design process is simpler and easier. © Emerald Group Publishing Limited.","Aircraft; Artificial bee colony; Controllers; Linear quadratic regulator; Oscillations; Pendulum oscillation; Programming and algorithm theory; Unmanned rotorcraft","Artificial bee colonies; Linear quadratic regulator; Oscillations; Programming and algorithm theory; Unmanned rotorcrafts; Aircraft; Algorithms; Artificial intelligence; Controllers; Flight control systems; Helicopter rotors; Optimization; Pendulums; Rotors; Parameter estimation",Article,Scopus,2-s2.0-84875725762
"Zhang Y., Wu L., Wang S.","Solving two-dimensional HP model by firefly algorithm and simplified energy function",2013,"Mathematical Problems in Engineering",17,10.1155/2013/398141,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875530447&doi=10.1155%2f2013%2f398141&partnerID=40&md5=fc02e89f6d8da4d78d102f1b1c1a771b","In order to solve the HP model of the protein folding problem, we investigated traditional energy function and pointed out that its discrete property cannot give direction of the next step to the searching point, causing a challenge to optimization algorithms. Therefore, we introduced the simplified energy function into a turn traditional discrete energy function to continuous one. The simplified energy function totals the distance between all pairs of hydrophobic amino acids. To optimize the simplified energy function, we introduced the latest swarm intelligence algorithm, the firefly algorithm (FA). FA is a hot nature-inspired technique and has been used for solving nonlinear multimodal optimization problems in dynamic environment. We also proposed the code scheme strategy to apply FA to the simplified HP model with the clash test strategy. The experiment took 14 sequences of different chain lengths from 18 to 100 as the dataset and compared the FA with standard genetic algorithm and immune genetic algorithm. Each algorithm ran 20 times. The averaged energy convergence results show that FA achieves the lowest values. It concludes that it is effective to solve 2D HP model by the firefly algorithm and the simplified energy function. © 2013 Yudong Zhang et al.",,"Dynamic environments; Hydrophobic amino acids; Immune genetic algorithms; Multimodal optimization problems; Optimization algorithms; Protein folding problem; Standard genetic algorithm; Swarm intelligence algorithms; Amino acids; Artificial intelligence; Bioluminescence; Genetic algorithms; Optimization; Protein folding; Mathematical models",Article,Scopus,2-s2.0-84875530447
"Leuty V., Boger J., Young L., Hoey J., Mihailidis A.","Engaging older adults with dementia in creative occupations using artificially intelligent assistive technology",2013,"Assistive Technology",17,10.1080/10400435.2012.715113,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877970142&doi=10.1080%2f10400435.2012.715113&partnerID=40&md5=722eefeedc95db74669b777c2ceeab86","Engagement in creative occupations has been shown to promote well-being for older adults with dementia. Providing access to such occupations is often difficult, as successful participation requires face-time with a person who is knowledgeable in facilitating engagement as well as access to any required resources, such as an arts studio. In response, a computer-based device, the Engaging Platform for Art Development (ePAD), was created to with the aim of enabling more independent access to art creation. ePAD is a an artificially intelligent touch-screen device that estimates a client's level of engagement and provides prompts to encourage engagement if the client becomes disengaged. ePAD is customizable such that an art therapist can choose themes and tools that they feel reflect their client's needs and preferences. This article presents a mixed-methods study that evaluated ePAD's usability by six older adult (with mild-to-moderate dementia) and art therapist dyads. Usability measures suggest that all participants found ePAD engaging but did not find prompts effective. Future development of ePAD includes improving the prompts, implementing the recommendations made by participants in this research, and long-term testing in more naturalistic art therapy contexts. © 2013 Copyright 2013 RESNA.","assistive technology; human-computer interface; intelligent systems; memory impairment; older adults; quality of life","Assistive technology; Computer-based devices; Human computer interfaces; Intelligent assistive technology; Long-term testing; Older adults; Quality of life; Usability measures; Employment; Intelligent systems; Touch screens; Neurodegenerative diseases; aged; art therapy; article; artificial intelligence; clinical trial; computer interface; creativity; dementia; equipment; health personnel attitude; human; patient satisfaction; psychological aspect; self help; very elderly; Aged; Aged, 80 and over; Art Therapy; Artificial Intelligence; Attitude of Health Personnel; Creativity; Dementia; Humans; Patient Satisfaction; Self-Help Devices; User-Computer Interface",Article,Scopus,2-s2.0-84877970142
"Hong Z., Lee C.","A decision support system for procurement risk management in the presence of spot market",2013,"Decision Support Systems",17,10.1016/j.dss.2012.12.031,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877768865&doi=10.1016%2fj.dss.2012.12.031&partnerID=40&md5=67b9255ca1fe5bb3cf74407296a80375","In the presence of spot market, this paper presents a decision support system to model risks for procurement processes and to design a robust purchasing plan, including supplier selection and order allocation. Taking advantages of contract supplier and spot market, the buyer can better meet business requirements in this dynamic business environment. However, there are limitations of existing methods for modeling multiple correlated risks to support decision makers for allocating orders among multiple suppliers in the presence of spot market. Therefore, Monte Carlo simulation algorithm termed as Expected Profit-Supply at Risk (A-EPSaR) is proposed to quantify each supplier's risk so as to let decision maker realize the trade-off between profit and risk. The goal programming model helps to allocate orders among the supplier pool and the contract-spot allocation model can assign orders between the spot market and the supplier pool, respectively. The significance of this paper is to propose a novel decision support framework which helps the buyer to make optimal and robust procurement decision including supplier selection and order allocation among multiple supplier sources in the existence of correlated demand, yield and spot price uncertainties. A case study is used to illustrate the performance of the proposed framework and the proposed methods show the promising result. © 2012 Elsevier B.V. All rights reserved.","Goal programming; Monte Carlo simulation; Order allocation; Procurement risk management; Spot market; Supplier selection","Decision support framework; Dynamic business environment; Goal programming; Goal programming model; Monte- carlo simulations; Order allocation; Spot market; Supplier selection; Artificial intelligence; Decision making; Lakes; Monte Carlo methods; Profitability; Risk management; Sales; Decision support systems",Article,Scopus,2-s2.0-84877768865
"Won K.-J., Zhang X., Wang T., Ding B., Raha D., Snyder M., Ren B., Wang W.","Comparative annotation of functional regions in the human genome using epigenomic data",2013,"Nucleic Acids Research",17,10.1093/nar/gkt143,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877254043&doi=10.1093%2fnar%2fgkt143&partnerID=40&md5=83fe21b079a1c6e873d0b7baec36d748","Epigenetic regulation is dynamic and cell-type dependent. The recently available epigenomic data in multiple cell types provide an unprecedented opportunity for a comparative study of epigenetic landscape. We developed a machine-learning method called ChroModule to annotate the epigenetic states in eight ENCyclopedia Of DNA Elements cell types. The trained model successfully captured the characteristic histone-modification patterns associated with regulatory elements, such as promoters and enhancers, and showed superior performance on identifying enhancers compared with the state-of-art methods. In addition, given the fixed number of epigenetic states in the model, ChroModule allows straightforward illustration of epigenetic variability in multiple cell types. Using this feature, we found that invariable and variable epigenetic states across cell types correspond to housekeeping functions and stimulus response, respectively. Especially, we observed that enhancers, but not the other regulatory elements, dictate cell specificity, as similar cell types share common enhancers, and cell-type-specific enhancers are often bound by transcription factors playing critical roles in that cell type. More interestingly, we found some genomic regions are dormant in cell type but primed to become active in other cell types. These observations highlight the usefulness of ChroModule in comparative analysis and interpretation of multiple epigenomes. © 2013 The Author(s).",,"DNA; histone H3; transcription factor; article; cell specificity; cell type; ChroModule; DNA determination; enhancer region; epigenetics; functional genomics; genetic variability; histone modification; human; human cell; machine learning; priority journal; promoter region; Artificial Intelligence; Binding Sites; Enhancer Elements, Genetic; Epigenesis, Genetic; Genome, Human; Humans; Molecular Sequence Annotation; Promoter Regions, Genetic; Transcription Factors",Article,Scopus,2-s2.0-84877254043
"Abbasi M.-A., Liu H.","Measuring user credibility in social media",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",17,10.1007/978-3-642-37210-0_48,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874813795&doi=10.1007%2f978-3-642-37210-0_48&partnerID=40&md5=dc6633522817ee9460abc0a623ca552b","People increasingly use social media to get first-hand news and information. During disasters such as Hurricane Sandy and the tsunami in Japan people used social media to report injuries as well as send out their requests. During social movements such as Occupy Wall Street (OWS) and the Arab Spring, people extensively used social media to organize their events and spread the news. As more people rely on social media for political, social, and business events, it is more susceptible to become a place for evildoers to use it to spread misinformation and rumors. Therefore, users have the challenge to discern which piece of information is credible or not. They also need to find ways to assess the credibility of information. This problem becomes more important when the source of the information is not known to the consumer. In this paper we propose a method to measure user credibility in social media. We study the situations in which we cannot assess the credibility of the content or the credibility of the user (source of the information) based on the user's profile. We propose the CredRank algorithm to measure user credibility in social media. The algorithm analyzes social media users' online behavior to measure their credibility. © 2013 Springer-Verlag.","Behavior Analysis; Information Credibility; Misinformation","Behavior analysis; Information credibilities; Misinformation; Online behavior; Social media; Social movements; User's profiles; Wall streets; Artificial intelligence; Algorithms",Conference Paper,Scopus,2-s2.0-84874813795
"Kothari S., Phan J.H., Young A.N., Wang M.D.","Histological image classification using biologically interpretable shape-based features",2013,"BMC Medical Imaging",17,10.1186/1471-2342-13-9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874812053&doi=10.1186%2f1471-2342-13-9&partnerID=40&md5=be7a43ef8907d679d3205a02513ee0cb","Background: Automatic cancer diagnostic systems based on histological image classification are important for improving therapeutic decisions. Previous studies propose textural and morphological features for such systems. These features capture patterns in histological images that are useful for both cancer grading and subtyping. However, because many of these features lack a clear biological interpretation, pathologists may be reluctant to adopt these features for clinical diagnosis.Methods: We examine the utility of biologically interpretable shape-based features for classification of histological renal tumor images. Using Fourier shape descriptors, we extract shape-based features that capture the distribution of stain-enhanced cellular and tissue structures in each image and evaluate these features using a multi-class prediction model. We compare the predictive performance of the shape-based diagnostic model to that of traditional models, i.e., using textural, morphological and topological features.Results: The shape-based model, with an average accuracy of 77%, outperforms or complements traditional models. We identify the most informative shapes for each renal tumor subtype from the top-selected features. Results suggest that these shapes are not only accurate diagnostic features, but also correlate with known biological characteristics of renal tumors.Conclusions: Shape-based analysis of histological renal tumor images accurately classifies disease subtypes and reveals biologically insightful discriminatory features. This method for shape-based analysis can be extended to other histological datasets to aid pathologists in diagnostic and therapeutic decisions. © 2013 Kothari et al.; licensee BioMed Central Ltd.",,"algorithm; article; artificial intelligence; automated pattern recognition; biopsy; computer assisted diagnosis; human; image enhancement; methodology; neoplasm; pathology; reproducibility; sensitivity and specificity; Algorithms; Artificial Intelligence; Biopsy; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Neoplasms; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84874812053
"Abdelaziz A.Y., Osama R.A., Elkhodary S.M.","Distribution systems reconfiguration using ant colony optimization and harmony search algorithms",2013,"Electric Power Components and Systems",17,10.1080/15325008.2012.755232,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874591988&doi=10.1080%2f15325008.2012.755232&partnerID=40&md5=73931326adc3139a69dc38f1163e72c3","One objective of the feeder reconfiguration problem in distribution systems is to minimize the distribution network total power loss for a specific load. For this problem, mathematical modeling is a non-linear mixed integer problem that is generally hard to solve. This article proposes two heuristic algorithms inspired from natural phenomena to solve the network reconfiguration problem: (1) ""real ant-behavior-inspired"" ant colony optimization implemented in the hyper cube framework and (2) the ""musician behavior-inspired"" harmony search algorithm. The optimization problem is formulated taking into account the operational constraints of distribution systems. A 32-bus system and a 118-bus distribution were selected for optimizing the configuration to minimize the losses. The results of reconfiguration using the proposed algorithms show that both of them yield the optimum configuration with minimum power loss for each case study; however, the harmony search required shorter simulation time but more practice of the iterative process than the hyper cube-ant colony optimization. Implementing the ant colony optimization in the hyper cube framework resulted in a more robust and easier handling of pheromone trails than the standard ant colony optimization. Copyright © Taylor & Francis Group, LLC.","ant colony optimization; distribution networks; harmony search; power loss; reconfiguration","Colony optimization; Distribution systems; Feeder reconfigurations; Harmony search; Harmony search algorithms; Hyper-cubes; Iterative process; Mixed integer problems; Natural phenomena; Network re-configuration; Operational constraints; Optimization problems; Optimum configurations; Pheromone trails; Power-losses; reconfiguration; Simulation time; Specific loads; Total power; Ant colony optimization; Electric power distribution; Geometry; Heuristic algorithms; Iterative methods; Learning algorithms; Artificial intelligence",Article,Scopus,2-s2.0-84874591988
"Gu J., Nayar S.K., Grinspun E., Belhumeur P.N., Ramamoorthi R.","Compressive structured light for recovering inhomogeneous participating media",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",17,10.1109/TPAMI.2012.130,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874512204&doi=10.1109%2fTPAMI.2012.130&partnerID=40&md5=80193bdd4792dd0ba2d5ea257cef485b","We propose a new method named compressive structured light for recovering inhomogeneous participating media. Whereas conventional structured light methods emit coded light patterns onto the surface of an opaque object to establish correspondence for triangulation, compressive structured light projects patterns into a volume of participating medium to produce images which are integral measurements of the volume density along the line of sight. For a typical participating medium encountered in the real world, the integral nature of the acquired images enables the use of compressive sensing techniques that can recover the entire volume density from only a few measurements. This makes the acquisition process more efficient and enables reconstruction of dynamic volumetric phenomena. Moreover, our method requires the projection of multiplexed coded illumination, which has the added advantage of increasing the signal-to-noise ratio of the acquisition. Finally, we propose an iterative algorithm to correct for the attenuation of the participating medium during the reconstruction process. We show the effectiveness of our method with simulations as well as experiments on the volumetric recovery of multiple translucent layers, 3D point clouds etched in glass, and the dynamic process of milk drops dissolving in water. © 1979-2012 IEEE.","Applications and Expert Knowledge-Intensive Systems; Artificial Intelligence; Atmospheric measurements; Cameras; Computer vision; Computing Methodologies; Image Processing and Computer Vision; Image reconstruction; Image Representation; Media; Modeling and recovery of physical attributes; Particle measurements; Photometry; Scene Analysis; Spatial resolution; Vision and Scene Understanding; Volume measurement; Volumetric","Applications and Expert Knowledge-Intensive Systems; Atmospheric measurement; Computing methodologies; Image processing and computer vision; Image representations; Media; Modeling and recovery of physical attributes; Particle measurement; Scene analysis; Spatial resolution; Vision and scene understanding; Volumetric; Algorithms; Artificial intelligence; Cameras; Computer vision; Image reconstruction; Models; Photometry; Sensors; Three dimensional computer graphics; Volume measurement; Recovery",Article,Scopus,2-s2.0-84874512204
"Fischer B., Inverso O., Parlato G.","CSeq: A sequentialization tool for C: (Competition contribution)",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",17,10.1007/978-3-642-36742-7_46,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874416774&doi=10.1007%2f978-3-642-36742-7_46&partnerID=40&md5=91cfee0c81de64adff939aa18affcd97","Sequentialization translates concurrent programs into equivalent non-deterministic sequential programs so that the different concurrent schedules no longer need to be handled explicitly. It can thus be used as a concurrency pre-processor for many sequential program verification techniques. CSeq implements sequentialization for C and uses ESBMC as sequential verification backend [5]. © 2013 Springer-Verlag.",,"Concurrent program; Sequential programs; Artificial intelligence; Algorithms",Conference Paper,Scopus,2-s2.0-84874416774
"Sáez C., Bresó A., Vicente J., Robles M., García-Gómez J.M.","An HL7-CDA wrapper for facilitating semantic interoperability to rule-based Clinical Decision Support Systems",2013,"Computer Methods and Programs in Biomedicine",17,10.1016/j.cmpb.2012.10.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874233955&doi=10.1016%2fj.cmpb.2012.10.003&partnerID=40&md5=232a36b9c85e1dc490973ff5b74e8e30","The success of Clinical Decision Support Systems (CDSS) greatly depends on its capability of being integrated in Health Information Systems (HIS). Several proposals have been published up to date to permit CDSS gathering patient data from HIS. Some base the CDSS data input on the HL7 reference model, however, they are tailored to specific CDSS or clinical guidelines technologies, or do not focus on standardizing the CDSS resultant knowledge. We propose a solution for facilitating semantic interoperability to rule-based CDSS focusing on standardized input and output documents conforming an HL7-CDA wrapper. We define the HL7-CDA restrictions in a HL7-CDA implementation guide. Patient data and rule inference results are mapped respectively to and from the CDSS by means of a binding method based on an XML binding file. As an independent clinical document, the results of a CDSS can present clinical and legal validity. The proposed solution is being applied in a CDSS for providing patient-specific recommendations for the care management of outpatients with diabetes mellitus. © 2012 Elsevier Ireland Ltd.","CDA; Clinical Decision Support Systems; Electronic Health Records; HL7; Reuse; Rule-based; Semantic interoperability","CDA; Clinical decision support systems; Electronic health record; HL7; Reuse; Rule based; Semantic interoperability; Artificial intelligence; Hospital data processing; Interoperability; Semantics; Decision support systems; article; body mass; clinical data repository; clinical decision making; decision support system; diabetes mellitus; electronic medical record; human; knowledge base; machine learning; markup language; medical informatics; medical information system; outpatient; patient care; patient coding; physical activity; practice guideline; Systematized Nomenclature of Medicine; telemedicine; Algorithms; Computer Communication Networks; Decision Support Systems, Clinical; Diabetes Mellitus; Health Level Seven; Humans; Medical Record Linkage; Medical Records Systems, Computerized; Probability; Programming Languages; Reproducibility of Results; Semantics; Software; Systems Integration",Article,Scopus,2-s2.0-84874233955
"Sieglaff J.M., Hartung D.C., Feltz W.F., Cronce L.M., Lakshmanan V.","A satellite-based convective cloud object tracking and multipurpose data fusion tool with application to developing convection",2013,"Journal of Atmospheric and Oceanic Technology",17,10.1175/JTECH-D-12-00114.1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875650513&doi=10.1175%2fJTECH-D-12-00114.1&partnerID=40&md5=82f27f5067dbc91ed803e46dda47514a","Studying deep convective clouds requires the use of available observation platforms with high temporal and spatial resolution, as well as other non-remote sensing meteorological data (i.e., numerical weather prediction model output, conventional observations, etc.). Such data are often at different temporal and spatial resolutions, and consequently, there exists the need to fuse these different meteorological datasets into a single framework. This paper introduces a methodology to identify and track convective cloud objects from convective cloud infancy [as few as three Geostationary Operational Environmental Satellite (GOES) infrared (IR) pixels] into the mature phase (hundreds of GOES IR pixels) using only geostationary imager IR window observations for the purpose of monitoring the initial growth of convective clouds. The object tracking system described within builds upon the Warning Decision Support System-Integrated Information (WDSS-II) object tracking capabilities. The system uses an IR-window-based field as input to WDSS-II for cloud object identification and tracking and a Cooperative Institute for Meteorological Satellite Studies at theUniversity of Wisconsin (UW-CIMSS)-developed postprocessing algorithmto combineWDSS-II cloud object output. The final output of the system is used to fuse multiple meteorological datasets into a single cloud object framework. The object tracking system performance analysis shows improved object tracking performance with both increased temporal resolution of the geostationary data and increased cloud object size. The system output is demonstrated as an effective means for fusing a variety of meteorological data including raw satellite observations, satellite algorithm output, radar observations, and derived output, numerical weather prediction model output, and lightning detection data for studying the initial growth of deep convective clouds and temporal trends of such data. © 2013 American Meteorological Society.","Convective clouds; Convective storms; Forecast verification/skill; Remote sensing; Satellite observations; Thunderstorms","Convective clouds; Convective storms; Forecast verification/skill; Geostationary operational environmental satellites; Meteorological datasets; Numerical weather prediction models; Object identification and tracking; Satellite observations; Artificial intelligence; Atmospheric thermodynamics; Clouds; Data fusion; Decision support systems; Geostationary satellites; Mathematical models; Meteorology; Pixels; Remote sensing; Thunderstorms; Tracking (position); algorithm; atmospheric convection; convective cloud; convective system; detection method; GOES; observational method; performance assessment; satellite data; spatial resolution; thunderstorm; tracking; weather forecasting",Article,Scopus,2-s2.0-84875650513
"Delellis P., Porfiri M., Bollt E.M.","Topological analysis of group fragmentation in multiagent systems",2013,"Physical Review E - Statistical, Nonlinear, and Soft Matter Physics",17,10.1103/PhysRevE.87.022818,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874524429&doi=10.1103%2fPhysRevE.87.022818&partnerID=40&md5=7481e3dcf59197917255b8771e004ee6","In social animals, the presence of conflicts of interest or multiple leaders can promote the emergence of two or more subgroups. Such subgroups are easily recognizable by human observers, yet a quantitative and objective measure of group fragmentation is currently lacking. In this paper, we explore the feasibility of detecting group fragmentation by embedding the raw data from the individuals' motions on a low-dimensional manifold and analyzing the topological features of this manifold. To perform the embedding, we employ the isomap algorithm, which is a data-driven machine learning tool extensively used in computer vision. We implement this procedure on a data set generated by a modified à la Vicsek model, where agents are partitioned into two or more subsets and an independent leader is assigned to each subset. The dimensionality of the embedding manifold is shown to be a measure of the number of emerging subgroups in the selected observation window and a cluster analysis is proposed to aid the interpretation of these findings. To explore the feasibility of using this approach to characterize group fragmentation in real time and thus reduce the computational cost in data processing and storage, we propose an interpolation method based on an inverse mapping from the embedding space to the original space. The effectiveness of the interpolation technique is illustrated on a test-bed example with potential impact on the regulation of collective behavior of animal groups using robotic stimuli. © 2013 American Physical Society.",,"Animal groups; Collective behavior; Computational costs; Conflicts of interest; Data set; Human observers; Interpolation method; Interpolation techniques; Inverse mapping; IsoMap algorithms; Low-dimensional manifolds; Modified a; Multiple leaders; Objective measure; Observation window; Potential impacts; Real time; Social animals; Topological analysis; Topological features; Vicsek models; Animals; Cluster analysis; Data processing; Digital storage; Interpolation; Multi agent systems; Topology; algorithm; article; artificial intelligence; automated pattern recognition; feasibility study; human; methodology; social behavior; Algorithms; Artificial Intelligence; Feasibility Studies; Humans; Pattern Recognition, Automated; Social Behavior",Article,Scopus,2-s2.0-84874524429
"Rafiei H., Ghodsi R.","A bi-objective mathematical model toward dynamic cell formation considering labor utilization",2013,"Applied Mathematical Modelling",17,10.1016/j.apm.2012.05.015,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870255062&doi=10.1016%2fj.apm.2012.05.015&partnerID=40&md5=28e491178f557cb3d0c261b7f9f06980","This paper addresses dynamic cell formation problem (DCFP) via a new bi-objective mathematical formulation. Although literature body of DCFP includes a vast number of research instances, human-related issues are mostly neglected as an important aspect of DCFP in the corresponding literature. In this paper, the first objective function seeks to minimize relevant costs of the problem including machine procurement and relocation costs, machine variable cost, inter-cell movement and intra-cell movement costs, overtime cost and labor shifting cost between cells, while labor utilization of the modeled DCFP is maximized through the second objective function. Due to NP-hardness of DCFP, an ant colony optimization (ACO) meta-heuristic is developed for the first time in the literature to tackle the problem. Also, authors enhance diversification of the developed algorithm is enhanced by hybridization of the proposed ACO algorithm with a genetic one. Finally, some numerical samples are generated randomly to validate the proposed model by which strengths of the developed algorithm is shown. © 2012 Elsevier Inc.","Ant colony optimization; Bi-objective optimization; Dynamic cell formation; Genetic algorithm; Human-related issues; Labor utilization","Ant Colony Optimization (ACO); Bi-objective optimization; Dynamic cell formation; Human-related issues; Labor utilization; Artificial intelligence; Cellular manufacturing; Cost accounting; Genetic algorithms; Mathematical models; Costs",Article,Scopus,2-s2.0-84870255062
"Sousa I.J., Ferreira M.-J.U., Molnár J., Fernandes M.X.","QSAR studies of macrocyclic diterpenes with P-glycoprotein inhibitory activity",2013,"European Journal of Pharmaceutical Sciences",17,10.1016/j.ejps.2012.11.012,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872528917&doi=10.1016%2fj.ejps.2012.11.012&partnerID=40&md5=59317f56b405e026e09529727b416aa3","Multidrug resistance (MDR) represents a major limitation for cancer chemotherapy. There are several mechanisms of MDR but the most important is associated with P-glycoprotein (P-gp) overexpression. The development of modulators of P-gp that are able to re-establish drug sensitivity of resistant cells has been considered a promising approach for overcoming MDR. Macrocyclic lathyrane and jatrophane- type diterpenes from Euphorbia species were found to be strong MDR reversing agents. In this study we applied quantitative structure-activity relationship (QSAR) methodology in order to identify the most relevant molecular features of macrocyclic diterpenes with P-gp inhibitory activity and to determine which structural modifications can be performed to improve their activity. Using experimental biological data at two concentrations (4 and 40 lg/ml), we developed a QSAR model for a set of 51 bioactive diterpenic compounds which includes lathyrane and jatrophane-type diterpenes and another model just for jatrophanes. The cross-validation correlation values for all diterpenes QSAR models developed for biological activities at compound concentrations of 4 and 40 lg/ml were 0.758 and 0.729, respectively. Regarding the prediction ability, we get R2 pred values of 0.765 and 0.534 for biological activities at compound concentrations of 4 and 40 lg/ml, respectively. Applying the cross-validation test to jatrophanes QSAR models, we obtained 0.680 and 0.787 for biological activities at compound concentrations of 4 and 40 lg/ml concentrations, respectively. For the same concentrations, the obtained R2 pred values for jatrophanes models were 0.541 and 0.534, respectively. The obtained models were statistically valid and showed high prediction ability. © 2012 Elsevier B.V. All rights reserved.","Macrocyclic diterpenes; Multidrug resistance; Multiple linear regression; P-glycoprotein; Quantitative structure-activity relationship","diterpene; jatrophane; lathyrane; multidrug resistance protein; unclassified drug; verapamil; ABCB1 protein, human; antineoplastic agent; diterpene; fluorescent dye; jatrophane; lathyrane; macrocyclic compound; multidrug resistance protein; recombinant protein; rhodamine 123; tumor protein; animal cell; article; biological activity; chemical structure; concentration response; controlled study; lipophilicity; lymphoma cell line; mouse; nonhuman; prediction; priority journal; quantitative analysis; quantitative structure activity relation; animal; antagonists and inhibitors; artificial intelligence; chemical structure; chemistry; conformation; drug effects; drug resistance; genetics; human; metabolism; multidrug resistance; osmolarity; quantitative structure activity relation; transport at the cellular level; tumor cell line; validation study; Animals; Antineoplastic Agents, Phytogenic; Artificial Intelligence; Biological Transport; Cell Line, Tumor; Diterpenes; Drug Resistance, Multiple; Drug Resistance, Neoplasm; Fluorescent Dyes; Humans; Macrocyclic Compounds; Mice; Models, Molecular; Molecular Conformation; Neoplasm Proteins; Osmolar Concentration; P-Glycoproteins; Quantitative Structure-Activity Relationship; Recombinant Proteins; Rhodamine 123",Article,Scopus,2-s2.0-84872528917
"Tablan V., Roberts I., Cunningham H., Bontcheva K.","GATECloud.net: A platform for large-scale, open-source text processing on the cloud",2013,"Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences",17,10.1098/rsta.2012.0071,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874405263&doi=10.1098%2frsta.2012.0071&partnerID=40&md5=12c467ada0c7dc2cc6f3fa0e76ded278","Cloud computing is increasingly being regarded as a key enabler of the 'democratization of science', because on-demand, highly scalable cloud computing facilities enable researchers anywhere to carry out data-intensive experiments. In the context of natural language processing (NLP), algorithms tend to be complex, which makes their parallelization and deployment on cloud platforms a non-trivial task. This study presents a new, unique, cloud-based platform for large-scale NLP research-GATECloud. net. It enables researchers to carry out data-intensive NLP experiments by harnessing the vast, on-demand compute power of the Amazon cloud. Important infrastructural issues are dealt with by the platform, completely transparently for the researcher: load balancing, efficient data upload and storage, deployment on the virtual machines, security and fault tolerance. We also include a cost-benefit analysis and usage evaluation. © 2012 The Author(s) Published by the Royal Society. All rights reserved.","Big data; Cloud computing; Text mining","Cloud computing; Data mining; Digital storage; Experiments; Fault tolerance; Text processing; Big datum; Cloud platforms; Cloud-based; Computing facilities; Data intensive; NAtural language processing; Non-trivial tasks; On demands; Open-source; Parallelizations; Text mining; Virtual machines; Natural language processing systems; algorithm; article; artificial intelligence; computer interface; computer program; Internet; natural language processing; Algorithms; Artificial Intelligence; Internet; Natural Language Processing; Software; User-Computer Interface",Article,Scopus,2-s2.0-84874405263
"Hunt J.D., Bañares-Alcántara R., Hanbury D.","A new integrated tool for complex decision making: Application to the UK energy sector",2013,"Decision Support Systems",17,10.1016/j.dss.2012.12.010,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875416653&doi=10.1016%2fj.dss.2012.12.010&partnerID=40&md5=87233f3b2fdddce80265f2fce5dd1482","This paper presents a new integrated tool and decision support framework to approach complex problems resulting from the interaction of many multi-criteria issues. The framework is embedded in an integrated tool called OUTDO 1 (Oxford University Tool for Decision Organisation). OUTDO integrates Multi-Criteria Decision Analysis (MCDA), decision rationale representation and management, and probabilistic forecasting to explore how changes in external parameters affect complicated and uncertain decision-making processes. OUTDO's features are showcased with a case study which recommends future energy sources for the Yorkshire and the Humber region in the UK, taking into consideration water consumption and the possibility of desalination of sea water. © 2012 Elsevier B.V.","Decision Support System; Energy and water; IBIS; MCDA","Decision making process; Decision support framework; IBIS; MCDA; Multi-criteria decision analysis; Oxford University; Probabilistic forecasting; Water consumption; Artificial intelligence; Decision support systems; Desalination; Uncertainty analysis; Water filtration; Water supply; Decision making",Article,Scopus,2-s2.0-84875416653
"Nair B.G., Newman S.-F., Peterson G.N., Schwid H.A.","Smart anesthesia managerTM (SAM)-a real-time decision support system for anesthesia care during surgery",2013,"IEEE Transactions on Biomedical Engineering",17,10.1109/TBME.2012.2205384,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871728360&doi=10.1109%2fTBME.2012.2205384&partnerID=40&md5=4e3f19aba252868e317f7141b11bf6d2","Anesthesia information management systems (AIMS) are being increasingly used in the operating room to document anesthesia care. We developed a system, Smart Anesthesia ManagerTM (SAM) that works in conjunction with an AIMS to provide clinical and billing decision support. SAM interrogates AIMS database in near real time, detects issues related to clinical care, billing and compliance, and material waste. Issues and the steps for their resolution are brought to the attention of the anesthesia provider in real time through 'pop-up' messages overlaid on top of AIMS screens or text pages. SAM improved compliance to antibiotic initial dose and redose to 99.3 ± 0.7% and 83.9 ± 3.4% from 88.5 ± 1.4% and 62.5 ± 1.6%, respectively. Beta-blocker protocol compliance increased to 94.6 ± 3.5% from 60.5 ± 8.6%. Inadvertent gaps (&gt;15 min) in blood pressure monitoring were reduced to 34 ± 30 min/1000 cases from 192 ± 58 min/1000 cases. Additional billing charge capture of invasive lines procedures worth 144,732 per year and 1,200 compliant records were achieved with SAM. SAM was also able to reduce wastage of inhalation anesthetic agents worth 120,168 per year. © 1964-2012 IEEE.","Anesthesia; anesthesia information management systems; clinical decision support; real-time decision support","Anesthesia; Billing charges; Blood-pressure monitoring; Clinical care; Clinical decision support; Decision supports; Information management systems; Inhalation anesthetics; Material wastes; Near-real time; Real time; Real-time decision support systems; Artificial intelligence; Blood pressure; Decision support systems; Information management; Managers; Anesthesiology; antibiotic agent; beta adrenergic receptor blocking agent; desflurane; inhalation anesthetic agent; isoflurane; sevoflurane; anesthesia; anesthesia information management system; anesthesist; article; billing and claims; blood pressure monitoring; clinical decision making; controlled study; data base; decision support system; human; information system; patient care; patient compliance; postanesthesia care; surgical technique; Anesthesia; Computer Communication Networks; Decision Support Systems, Clinical; Humans; Monitoring, Intraoperative; User-Computer Interface",Article,Scopus,2-s2.0-84871728360
"Wang X., Wang Z., Xu X.","An improved artificial bee colony approach to QoS-aware service selection",2013,"Proceedings - IEEE 20th International Conference on Web Services, ICWS 2013",17,10.1109/ICWS.2013.60,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891782956&doi=10.1109%2fICWS.2013.60&partnerID=40&md5=4242a3165da7a82cb9e19abed9955f1e","As available services accumulate on the Internet, QoS-aware service selection (SSP) becomes an increasingly difficult task. Since Artificial Bee Colony algorithm (ABC) has been successful in solving many problems as a simpler implementation of swarm intelligence, its application to SSP is promising. However, ABC was initially designed for numerical optimization, and its effectiveness highly depends on what we call optimality continuity property of the solution space, i.e., similar variable values (or neighboring solutions) result in similar objective values (or evaluation results). We will show that SSP does not possess such property. We further propose an approximation approach based on greedy search strategies for ABC, to overcome this problem. In this approach, neighboring solutions are generated for a composition greedily based on the neighboring services of its component services. Two algorithms with different neighborhood measures are presented based on this approach. The resulting neighborhood structure of the proposed algorithms is analogical to that of continuous functions, so that the advantages of ABC can be fully leveraged in solving SSP. Also, they are pure online algorithms which are as simple as canonical ABC. The rationale of the proposed approach is discussed and the complexity of the algorithms is analyzed. Experiments conducted against canonical ABC indicate that the proposed algorithms can achieve better optimality within limited time. © 2013 IEEE.","approximation algorithms; artificial bee colony algorithm; neighborhood search; QoS-aware service selection","Approximation algorithms; Artificial intelligence; Optimization; Web services; Websites; Approximation approach; Artificial bee colonies; Artificial bee colony algorithms; Artificial bee colony algorithms (ABC); Neighborhood search; Neighborhood structure; Numerical optimizations; Service selection; Evolutionary algorithms",Conference Paper,Scopus,2-s2.0-84891782956
"Kozina S., Gjoreski H., Gams M., Luštrek M.","Efficient Activity Recognition and Fall Detection Using Accelerometers",2013,"Communications in Computer and Information Science",17,10.1007/978-3-642-41043-7_2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901370635&doi=10.1007%2f978-3-642-41043-7_2&partnerID=40&md5=066a0553aebb2c623b6b54d12d47fe9d","Ambient assisted living (AAL) systems need to understand the user's situation, which makes activity recognition an important component. Falls are one of the most critical problems of the elderly, so AAL systems often incorporate fall detection. We present an activity recognition (AR) and fall detection (FD) system aiming to provide robust real-time performance. It uses two wearable accelerometers, since this is probably the most mature technology for such purpose. For the AR, we developed an architecture that combines rules to recognize postures, which ensures that the behavior of the system is predictable and robust, and classifiers trained with machine learning algorithms, which provide maximum accuracy in the cases that cannot be handled by the rules. For the FD, rules are used that take into account high accelerations associated with falls and the recognized horizontal orientation (e.g., falling is often followed by lying). The system was tested on a dataset containing a wide range of activities, two different types of falls and two events easily mistaken for falls. The F-measure of the AR was 99 %, even though it was never tested on the same persons it was trained on. The F-measure of the FD was 78 % due to the difficulty of the events to be recognized and the need for real-time performance, which made it impossible to rely on the recognition of long lying after a fall. © Springer-Verlag Berlin Heidelberg 2013.","Accelerometers; Activity recognition; Ambient assisted living; Fall detection; Machine learning; Rules","Accelerometers; Artificial intelligence; Competition; Finite difference method; Learning algorithms; Learning systems; Activity recognition; Ambient assisted living; Ambient assisted living (AAL); Fall detection; High acceleration; Maximum accuracies; Real time performance; Rules; Pattern recognition",Conference Paper,Scopus,2-s2.0-84901370635
"Paxton C., Niculescu-Mizil A., Saria S.","Developing predictive models using electronic medical records: challenges and pitfalls.",2013,"AMIA ... Annual Symposium proceedings / AMIA Symposium. AMIA Symposium",17,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901259802&partnerID=40&md5=11ecf0db45eb468dd98039bc46e2a98f","While Electronic Medical Records (EMR) contain detailed records of the patient-clinician encounter - vital signs, laboratory tests, symptoms, caregivers' notes, interventions prescribed and outcomes - developing predictive models from this data is not straightforward. These data contain systematic biases that violate assumptions made by off-the-shelf machine learning algorithms, commonly used in the literature to train predictive models. In this paper, we discuss key issues and subtle pitfalls specific to building predictive models from EMR. We highlight the importance of carefully considering both the special characteristics of EMR as well as the intended clinical use of the predictive model and show that failure to do so could lead to developing models that are less useful in practice. Finally, we describe approaches for training and evaluating models on EMR using early prediction of septic shock as our example application.",,"area under the curve; article; artificial intelligence; biological model; electronic medical record; human; patient care; prognosis; septic shock; Area Under Curve; Artificial Intelligence; Electronic Health Records; Humans; Models, Biological; Patient Care; Prognosis; Shock, Septic",Article,Scopus,2-s2.0-84901259802
"Huang J., Piech C., Nguyen A., Guibas L.","Syntactic and functional variability of a million code submissions in a machine learning MOOC",2013,"CEUR Workshop Proceedings",17,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924963220&partnerID=40&md5=f208d86c9c26b66e71f772d7e9145dd0","In the first offering of Stanford's Machine Learning Massive Open-Access Online Course (MOOC) there were over a million programming submissions to 42 assignments - a dense sampling of the range of possible solutions. In this paper we map out the syntax and functional similarity of the submissions in order to explore the variation in solutions. While there was a massive number of submissions, there is a much smaller set of unique approaches. This redundancy in student solutions can be leveraged to ""force multiply"" teacher feedback.",,"Artificial intelligence; Syntactics; Teaching; Functional similarity; Online course; Open Access; Stanford; Teacher feedback; Learning systems",Conference Paper,Scopus,2-s2.0-84924963220
"Parsopoulos K.E., Vrahatis M.N.","Computing periodic orbits of nondifferentiable/discontinuous mappings through particle swarm optimization",2013,"2003 IEEE Swarm Intelligence Symposium, SIS 2003 - Proceedings",17,10.1109/SIS.2003.1202244,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942092247&doi=10.1109%2fSIS.2003.1202244&partnerID=40&md5=8e97b84a7e1fdc257029bc7fa5da7c02","Periodic orbits of nonlinear mappings play a central role in the study of dynamical systems. Traditional root finding algorithms, such as the Newton-family algorithms, have been widely applied for the detection of periodic orbits. However, in the case of discontinuous/nondifferentiable mappings and mappings with poorly behaved partial derivatives, this approach is not valid. In such cases, stochastic optimization algorithms have proved to be a valuable tool. In this paper, a new approach for computing periodic orbits through particle swarm optimization is introduced. The results indicate that the algorithm is robust and efficient. Moreover, the method can be combined with established techniques, such as deflection, to detect several periodic orbits of a mapping. Finally, the minor effort which is required to implement the proposed approach renders it an efficient alternative for computing periodic orbits of nonlinear mappings. © 2003 IEEE.","Approximation algorithms; Artificial intelligence; Linear approximation; Mathematics; Orbits; Organisms; Particle swarm optimization; Polynomials; Robustness; Stochastic processes","Algorithms; Artificial intelligence; Dynamical systems; Mapping; Mathematical techniques; Optimization; Orbits; Particle swarm optimization (PSO); Polynomials; Random processes; Robustness (control systems); Linear approximations; Non-differentiable; Nonlinear mappings; Organisms; Partial derivatives; Periodic orbits; Root finding algorithms; Stochastic optimization algorithm; Approximation algorithms",Conference Paper,Scopus,2-s2.0-84942092247
"Laurent F., Valderrama M., Besserve M., Guillard M., Lachaux J.-P., Martinerie J., Florence G.","Multimodal information improves the rapid detection of mental fatigue",2013,"Biomedical Signal Processing and Control",17,10.1016/j.bspc.2013.01.007,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027916939&doi=10.1016%2fj.bspc.2013.01.007&partnerID=40&md5=3c01d62396455d5e0d44948e06f07e1f","One of the major challenge in the detection of mental states is improving the accuracy of brain activity-based detectors with additional information from extracranial signals. We assessed the suitability, for real-time mental fatigue detection, of EEG, EOG and ECG measurements, taken separately or together. Thirteen subjects performed six blocks of switching tasks. For each participant, the block with the lowest error rate from the first two blocks and the block with the highest error rate from the last three blocks were discriminated with a machine learning algorithm (support vector machine). The classification scores obtained with ECG or EOG were greater than would be expected by chance (>50%) for time windows of at least 8 s. EEG was the best single mode of detection, with classification scores ranging from 80 ± 3% with a 4 s time window to 94 ± 2% with a 30 s time window. The addition of ECG and EOG features to EEG features significantly increased classification scores for short time windows (e.g., to 86 ± 3% with a 4 s time window, p < 0.001). For short time windows (up to 12 s), ECG significantly increased the discriminatory power of EEG, whereas EOG did not. These results demonstrate that mental state detection on the basis of extracerebral measurements is feasible and that a combination of EEG and ECG is particularly appropriate for the rapid detection of mental fatigue. ©2013 Elsevier Ltd. All rights reserved.","Classification; ECG; EEG; EOG; Mental fatigue; Task switching","Artificial intelligence; Brain; Classification (of information); Diseases; Electrocardiography; Electroencephalography; Learning algorithms; Learning systems; Discriminatory power; Ecg measurements; EOG; Mental fatigue; Multi-modal information; Rapid detection; Short time windows; Task switching; Vehicle routing; Article; auditory feedback; classification algorithm; dysthymia; electrocardiogram; electroencephalogram; electrooculogram; executive function; eyelid reflex; heart rate; heart rate variability; human; machine learning; male; mental health; priority journal; response time; right handedness; support vector machine; task performance",Article,Scopus,2-s2.0-85027916939
"Zhang P., You X., Xu D.","A novel method for vessel skeleton extraction",2013,"Proceedings - International Conference on Machine Learning and Cybernetics",17,10.1109/ICMLC.2013.6890455,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907258724&doi=10.1109%2fICMLC.2013.6890455&partnerID=40&md5=24e2a9b3d2a0ab1082be69eb915cf983","A novel vessel skeleton extraction method is presented in this work. Our work consists of three steps in a coarse-to-fine style: Firstly, by modeling the distance transform and its gradient vector field, the average outward flux of the gradient vector field is computed to coarsely label all image points. Then we introduce a topology-based shape thinning algorithm for extracting vessel skeleton tree. At last, a skeleton tree refinement algorithm is applied to get a precise extraction of vessel skeleton. The proposed method is parameter-free and computationally efficient. The validity and efficiency of the method is tested on two public database of human eye retina vessel images. © 2013 IEEE.","Average Outward Flux; Segmentation; Vessel Skeleton","Artificial intelligence; Extraction; Forestry; Image segmentation; Learning systems; Trees (mathematics); Coarse-to-fine; Computationally efficient; Distance transforms; Gradient vector field; Public database; Refinement algorithms; Thinning algorithm; Vessel skeleton; Musculoskeletal system",Conference Paper,Scopus,2-s2.0-84907258724
"Kisilevich S., Keim D., Rokach L.","A GIS-based decision support system for hotel room rate estimation and temporal price prediction: The hotel brokers' context",2013,"Decision Support Systems",17,10.1016/j.dss.2012.10.038,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871722559&doi=10.1016%2fj.dss.2012.10.038&partnerID=40&md5=c82636fc901803c53e774e857754edf5","The vastly increasing number of online hotel room bookings is not only intensifying the competition in the travel industry as a whole, but also prompts travel intermediates (i.e. e-companies that aggregate information about different travel products from different travel suppliers) into a fierce competition for the best prices of travel products, i.e. hotel rooms. An important factor that affects revenues is the ability to conclude profitable deals with different travel suppliers. However, the profitability of a contract not only depends on the communication skills of a contract manager. It significantly depends on the objective information obtained about a specific travel supplier and his/her products. While the contract manager usually has a broad knowledge of the travel business in general, collecting and processing specific information about travel suppliers is usually a time and cost expensive task. Our goal is to develop a tool that assists the travel intermediate to acquire the missing strategic information about individual hotels in order to leverage profitable deals. We present a GIS-based decision support system that can both, estimate objective hotel room rates using essential hotel and locational characteristics and predict temporal room rate prices. Information about objective hotel room rates allows for an objective comparison and provides the basis for a realistic computation of the contract's profitability. The temporal prediction of room rates can be used for monitoring past hotel room rates and for adjusting the price of the future contract. This paper makes three major contributions. First, we present a GIS-based decision support system, the first of its kind, for hotel brokers. Second, the DSS can be applied to virtually any part of the world, which makes it a very attractive business tool in real-life situations. Third, it integrates a widely used data mining framework that provides access to dozens of ready to run algorithms to be used by a domain expert and it offers the possibility of adding new algorithms once they are developed. The system has been designed and evaluated in close cooperation with a company that develops travel technology solutions, in particular inventory management and pricing solutions for many well-known websites and travel agencies around the world. This company has also provided us with real, large datasets to evaluate the system. We demonstrate the functionality of the DSS using the hotel data in the area of Barcelona, Spain. The results indicate the potential usefulness of the proposed system. © 2012 Elsevier B.V.","Data mining; Geographic Information Systems; Hedonic methods; Hotels; Keywords; Price prediction; Regression analysis","Barcelona , Spain; Business tools; Communication skills; Data mining frameworks; Domain experts; Future contract; Hedonic methods; Inventory management; Keywords; Large datasets; Objective information; Price prediction; Rate estimation; Specific information; Technology solutions; Temporal prediction; Travel agency; Travel industry; Algorithms; Artificial intelligence; Costs; Data mining; Decision support systems; Economics; Forecasting; Geographic information systems; Industry; Inventory control; Management; Managers; Profitability; Regression analysis; Hotels",Article,Scopus,2-s2.0-84871722559
"Leydesdorff L., Zhou P., Bornmann L.","How can journal impact factors be normalized across fields of science? An assessment in terms of percentile ranks and fractional counts",2013,"Journal of the American Society for Information Science and Technology",17,10.1002/asi.22765,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871180922&doi=10.1002%2fasi.22765&partnerID=40&md5=3d8123144f64b082218be59ab8632028","Using the CD-ROM version of the Science Citation Index 2010 (N = 3,705 journals), we study the (combined) effects of (a) fractional counting on the impact factor (IF) and (b) transformation of the skewed citation distributions into a distribution of 100 percentiles and six percentile rank classes (top-1%, top-5%, etc.). Do these approaches lead to field-normalized impact measures for journals? In addition to the 2-year IF (IF2), we consider the 5-year IF (IF5), the respective numerators of these IFs, and the number of Total Cites, counted both as integers and fractionally. These various indicators are tested against the hypothesis that the classification of journals into 11 broad fields by PatentBoard/NSF (National Science Foundation) provides statistically significant between-field effects. Using fractional counting the between-field variance is reduced by 91.7% in the case of IF5, and by 79.2% in the case of IF2. However, the differences in citation counts are not significantly affected by fractional counting. These results accord with previous studies, but the longer citation window of a fractionally counted IF5 can lead to significant improvement in the normalization across fields. © 2012 ASIS&T.","citation indexes; information science; journals","Citation distribution; citation indexes; Impact factor; journals; National Science Foundations; Science citation index; Artificial intelligence; Information science; Software engineering",Article,Scopus,2-s2.0-84871180922
"Ltifi H., Kolski C., Ayed M.B., Alimi A.M.","A human-centred design approach for developing dynamic decision support system based on knowledge discovery in databases",2013,"Journal of Decision Systems",16,10.1080/12460125.2012.759485,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883355885&doi=10.1080%2f12460125.2012.759485&partnerID=40&md5=3d4b0c103b64d0085363f28df1012a6f","This paper presents a human-centred design approach for developing Decision Support Systems (DSS) based on a Knowledge Discovery in Databases (KDD) process. The KDD process generates a set of software modules. Our approach is based on a critical study of design methods. It uses the Unified Process (UP), which proposes a general framework; however, the UP does not include enough Human-Computer Interaction (HCI) elements. We suggest enriching the UP activities from the HCI perspective, adding HCI elements. The proposed approach is applied to a KDD-based Dynamic Medical DSS. © 2013 Taylor & Francis.","Dynamic decision support systems; Human-centred Design; Human-computer interaction; Knowledge Discovery in databases; Software process","Decision support system (dss); Dynamic decision; Human computer interaction (HCI); Human-centred designs; Knowledge discovery in database; Software modules; Software process; Unified process; Artificial intelligence; Database systems; Design; Human computer interaction; Decision support systems",Article,Scopus,2-s2.0-84883355885
"Tang Y., He H., Wen J.","Comparative study between HDP and PSS on DFIG damping control",2013,"IEEE Symposium on Computational Intelligence Applications in Smart Grid, CIASG",16,10.1109/CIASG.2013.6611499,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890054544&doi=10.1109%2fCIASG.2013.6611499&partnerID=40&md5=6c529ce2f8424fb22897d5600d3b046c","In this paper, heuristic dynamic programming (HDP) based supplementary control is compared with the power system stabilizer (PSS) based supplementary control for doubly-fed induction generators (DFIG), namely, the active power damping control. The traditional design of PSS control on DFIG damping is based on eigenvalue analysis. For such analysis, disturbances are considered sufficiently small to permit the nonlinear model representing the power system to be linearized and expressed in state space form. When the disturbance or the system configuration changes, this kind of design is easy to become entrapped in local minimal and the robust ability of the controller is not guaranteed. On the other side, the HDP based supplementary damping controller analyzed in this paper is 'model-free' with on-line learning capability: once a system state is observed, an action will be subsequently produced based on the performance index function. The obtained results by such a HDP supplementary controller on a benchmark power system are compared with the traditional PSS controller, demonstrating the improved control performance and robustness. © 2013 IEEE.",,"Comparative studies; Doubly-fed induction generator; Eigenvalue analysis; Heuristic dynamic programming; Performance indices; Power system stabilizer (PSS); Supplementary damping controllers; System configurations; Artificial intelligence; Benchmarking; Damping; Eigenvalues and eigenfunctions; Smart power grids; State space methods; Controllers",Conference Paper,Scopus,2-s2.0-84890054544
"Abramson M., Aha D.W.","User authentication from web browsing behavior",2013,"FLAIRS 2013 - Proceedings of the 26th International Florida Artificial Intelligence Research Society Conference",16,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889837918&partnerID=40&md5=783f4eb310620235f302c7e6bce03807","As anticipated in True Names by Vernor Vinge, identity has been recognized as our most valued possession in cyberspace. Attribution is a key concept in enabling trusted identities and deterring malicious activities. As more people use the Web to communicate, work, and otherwise have fun, is it possible to uniquely identify someone based on their Web browsing behavior or to differentiate between two persons based solely on their Web browsing histories? Based on a user study, this paper provides some insights into these questions. We describe characteristic features of Web browsing behavior and present our algorithm and analysis of an ensemble learning approach leveraging from those features for user authentication. Copyright © 2013, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Browsing behavior; Browsing history; Cyberspaces; Ensemble learning approach; Malicious activities; User authentication; User study; Artificial intelligence; Authentication",Conference Paper,Scopus,2-s2.0-84889837918
"Biggio B., Pillai I., Rota Bulò S., Ariu D., Pelillo M., Roli F.","Is data clustering in adversarial settings secure?",2013,"Proceedings of the ACM Conference on Computer and Communications Security",16,10.1145/2517312.2517321,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888997884&doi=10.1145%2f2517312.2517321&partnerID=40&md5=5db9cebc81772533d3dd0e11f159bcb9","Clustering algorithms have been increasingly adopted in security applications to spot dangerous or illicit activities. However, they have not been originally devised to deal with deliberate attack attempts that may aim to subvert the clustering process itself. Whether clustering can be safely adopted in such settings remains thus questionable. In this work we propose a general framework that allows one to identify potential attacks against clustering algorithms, and to evaluate their impact, by making specific assumptions on the adversary's goal, knowledge of the attacked system, and capabilities of manipulating the input data. We show that an attacker may significantly poison the whole clustering process by adding a relatively small percentage of attack samples to the input data, and that some attack samples may be obfuscated to be hidden within some existing clusters. We present a case study on single-linkage hierarchical clustering, and report experiments on clustering of malware samples and handwritten digits. © 2013 ACM.","adversarial learning; clustering; computer security; malware detection; security evaluation; unsupervised learning","Adversarial learning; clustering; Clustering process; Handwritten digit; Hier-archical clustering; Malware detection; Security application; Security evaluation; Artificial intelligence; Cluster analysis; Computer crime; Input output programs; Security of data; Unsupervised learning; Clustering algorithms",Conference Paper,Scopus,2-s2.0-84888997884
"Han Y., Yang Y., Zhou X.","Co-regularized ensemble for feature selection",2013,"IJCAI International Joint Conference on Artificial Intelligence",16,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062105&partnerID=40&md5=5850310d6340fd35980d3c5b67df22da","Supervised feature selection determines feature relevance by evaluating feature's correlation with the classes. Joint minimization of a classifier's loss function and an ℓ2;1-norm regularization has been shown to be effective for feature selection. However, the appropriate feature subset learned from different classifiers' loss function may be different. Less effort has been made on improving the performance of feature selection by the ensemble of different classifiers' criteria and take advantages of them. Furthermore, for the cases when only a few labeled data per class are available, overfitting would be a potential problem and the performance of each classifier is restrained. In this paper, we add a joint ℓ2;1-norm on multiple feature selection matrices to ensemble different classifiers' loss function into a joint optimization framework. This added co-regularization term has twofold role in enhancing the effect of regularization for each criterion and uncovering common irrelevant features. The problem of over-fitting can be alleviated and thus the performance of feature selection is improved. Extensive experiment on different data types demonstrates the effectiveness of our algorithm.",,"Data type; Feature relevance; Feature subset; Joint optimization; Labeled data; Loss functions; Overfitting; Potential problems; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84896062105
"Myneni S., Cobb N.K., Cohen T.","Finding meaning in social media: Content-based social network analysis of QuitNet to identify new opportunities for health promotion",2013,"Studies in Health Technology and Informatics",16,10.3233/978-1-61499-289-9-807,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894348733&doi=10.3233%2f978-1-61499-289-9-807&partnerID=40&md5=66ccc6d5e5abca354052d77fdb043c79","Unhealthy behaviors increase individual health risks and are a socioeconomic burden. Harnessing social influence is perceived as fundamental for interventions to influence health-related behaviors. However, the mechanisms through which social influence occurs are poorly understood. Online social networks provide the opportunity to understand these mechanisms as they digitally archive communication between members. In this paper, we present a methodology for content-based social network analysis, combining qualitative coding, automated text analysis, and formal network analysis such that network structure is determined by the content of messages exchanged between members. We apply this approach to characterize the communication between members of QuitNet, an online social network for smoking cessation. Results indicate that the method identifies meaningful theme-based social sub-networks. Modeling social network data using this method can provide us with theme-specific insights such as the identities of opinion leaders and sub-community clusters. Implications for design of targeted social interventions are discussed. © 2013 IMIA and IOS Press.","behavior change; content analysis; Online social networks; smoking cessation","artificial intelligence; data mining; health behavior; health promotion; meaningful use criteria; medical information; natural language processing; procedures; smoking cessation; social media; social support; statistics and numerical data; Artificial Intelligence; Data Mining; Health Behavior; Health Communication; Health Promotion; Meaningful Use; Natural Language Processing; Smoking Cessation; Social Media; Social Support",Conference Paper,Scopus,2-s2.0-84894348733
"Liu J., Wang C., Danilevsky M., Han J.","Large-scale spectral clustering on graphs",2013,"IJCAI International Joint Conference on Artificial Intelligence",16,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896060960&partnerID=40&md5=a260616474ee7abc21c5b7cf08f20a24","Graph clustering has received growing attention in recent years as an important analytical technique, both due to the prevalence of graph data, and the usefulness of graph structures for exploiting intrinsic data characteristics. However, as graph data grows in scale, it becomes increasingly more challenging to identify clusters. In this paper we propose an efficient clustering algorithm for largescale graph data using spectral methods. The key idea is to repeatedly generate a small number of ""supernodes"" connected to the regular nodes, in order to compress the original graph into a sparse bipartite graph. By clustering the bipartite graph using spectral methods, we are able to greatly improve efficiency without losing considerable clustering power. Extensive experiments show the effectiveness and efficiency of our approach.",,"Bipartite graphs; Effectiveness and efficiencies; Graph clustering; Graph structures; Intrinsic data; Spectral clustering; Spectral methods; Supernodes; Artificial intelligence; Clustering algorithms; Spectroscopy; Graph theory",Conference Paper,Scopus,2-s2.0-84896060960
"Labati R.D., Genovese A., Piuri V., Scotti F.","Contactless fingerprint recognition: A neural approach for perspective and rotation effects reduction",2013,"IEEE Workshop on Computational Intelligence in Biometrics and Identity Management, CIBIM",16,10.1109/CIBIM.2013.6607909,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891536190&doi=10.1109%2fCIBIM.2013.6607909&partnerID=40&md5=083eb3bc492faf3f5772dd8cee91cbb5","Contactless fingerprint recognition systems are being researched in order to reduce intrinsic limitations of traditional biometric acquisition technologies, encompassing the release of latent fingerprints on the sensor platen, non-linear spatial distortions in the captured samples, and relevant image differences with respect to the moisture level and pressure of the fingertip on the sensor surface. © 2013 IEEE.",,"Fingerprint Recognition; Fingerprint recognition systems; Image difference; Latent fingerprint; Moisture level; Rotation effect; Sensor surfaces; Spatial distortion; Biometrics; Sensors; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84891536190
"Li Y., Tsonev D., Haas H.","Non-DC-biased OFDM with optical spatial modulation",2013,"IEEE International Symposium on Personal, Indoor and Mobile Radio Communications, PIMRC",16,10.1109/PIMRC.2013.6666185,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893277190&doi=10.1109%2fPIMRC.2013.6666185&partnerID=40&md5=9ea3b4951344c3d382e95dfff63ea3cc","In this paper, a novel optical Orthogonal Frequency Division Multiplexing (OFDM) modulation approach is presented. This method uses the Optical Spatial Modulation (OSM) technique to obtain positive and real-valued signals which are required by an Optical Wireless Communication (OWC) system. In comparison to existing OFDM methods applied to the OSM system, the new scheme, Non-DC-biased OFDM (NDC-OFDM), has significant advantages. Compared to DC-biased Optical OFDM (DCO-OFDM), NDC-OFDM avoids DC-biasing and, thus, improves the power efficiency. Moreover, the spectral and power efficiency of the new approach are better than the well-known unipolar optical modulation scheme, Asymmetrically Clipped Optical OFDM (ACO-OFDM). The bit-error ratio (BER) performances of these three methods are compared. Compared to ACO-OFDM and DCO-OFDM, NDC-OFDM has an energy saving gain of at least 5 dB for the same spectral efficiency. The improvement comes at the expense of additional hardware at the transmitter and receiver. However, visible light communication (VLC) systems typically are equipped with multiple low-cost Light Emitting Diodes (LEDs) to fulfill minimum indoor lighting conditions. © 2013 IEEE.","MIMO; Optical OFDM; Optical spatial modulation; Optical wireless communication","Asymmetrically clipped Optical OFDM; Optical OFDM; Optical orthogonal frequency division multiplexing (OFDM); Optical wireless communication systems; Optical wireless communications; Spatial modulations; Transmitter and receiver; Visible light communications (VLC); Artificial intelligence; Computer software maintenance; Efficiency; Light emitting diodes; MIMO systems; Modulation; Orthogonal frequency division multiplexing; Wireless telecommunication systems; Optical fiber communication",Conference Paper,Scopus,2-s2.0-84893277190
"Oyama S., Baba Y., Sakurai Y., Kashima H.","Accurate integration of crowdsourced labels using workers' self-reported confidence scores",2013,"IJCAI International Joint Conference on Artificial Intelligence",16,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896061022&partnerID=40&md5=7f38b4ff8f4b9ae371bb6632e9422666","We have developed a method for using confidence scores to integrate labels provided by crowdsourcing workers. Although confidence scores can be useful information for estimating the quality of the provided labels, a way to effectively incorporate them into the integration process has not been established. Moreover, some workers are overconfident about the quality of their labels while others are underconfident, and some workers are quite accurate in judging the quality of their labels. This differing reliability of the confidence scores among workers means that the probability distributions for the reported confidence scores differ among workers. To address this problem, we extended the Dawid-Skene model and created two probabilistic models in which the values of unobserved true labels are inferred from the observed provided labels and reported confidence scores by using the expectation-maximization algorithm. Results of experiments using actual crowdsourced data for image labeling and binary question answering tasks showed that incorporating workers' confidence scores can improve the accuracy of integrated crowdsourced labels.",,"Confidence score; Crowdsourcing; Expectation-maximization algorithms; Image labeling; Integration process; Probabilistic models; Question Answering Task; Workers'; Algorithms; Probability distributions; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84896061022
"Qin L., Zhu X.","Promoting diversity in recommendation by entropy regularizer",2013,"IJCAI International Joint Conference on Artificial Intelligence",16,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062634&partnerID=40&md5=e99ce4f8b1c312023cd17f92b6349ee1","We study the problem of diverse promoting recommendation task: selecting a subset of diverse items that can better predict a given user's preference. Recommendation techniques primarily based on user or item similarity can suffer from the risk that users cannot get expected information from the over-specified recommendation lists. In this paper, we propose an entropy regularizer to capture the notion of diversity. The entropy regularizer has good properties in that it satisfies monotonicity and submodularity, such that when we combine it with a modular rating set function, we get submodular objective function, which can be maximized approximately by efficient greedy algorithm, with provable constant factor guarantee of optimality. We apply our approach on the top-Κ prediction problem and evaluate its performance on Movie- Lens data set, which is a standard database containing movie rating data collected from a popular online movie recommender system. We compare our model with the state-of-the-art recommendation algorithms. Our experiments show that entropy regularizer effectively captures diversity and hence improves the performance of recommendation task.",,"Constant factors; Expected informations; Greedy algorithms; Objective functions; Prediction problem; Recommendation algorithms; Recommendation techniques; User's preferences; Algorithms; Artificial intelligence; Motion pictures; Entropy",Conference Paper,Scopus,2-s2.0-84896062634
"Bienvenu M., Ortiz M., Šimkus M.","Conjunctive regular path queries in lightweight description logics",2013,"IJCAI International Joint Conference on Artificial Intelligence",16,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063315&partnerID=40&md5=51bac69714277de7333a4ba012612397","Conjunctive regular path queries are an expressive extension of the well-known class of conjunctive queries and have been extensively studied in the database community. Somewhat surprisingly, there has been little work aimed at using such queries in the context of description logic (DL) knowledge bases, and all existing results target expressive DLs, even though lightweight DLs are considered better-suited for data-intensive applications. This paper aims to bridge this gap by providing algorithms and tight complexity bounds for answering two-way conjunctive regular path queries over DL knowledge bases formulated in lightweight DLs of the DL-Lite and EL families.",,"Complexity bounds; Conjunctive queries; Data-intensive application; Database community; Description logic; Dl-lite; Knowledge basis; Regular path queries; Artificial intelligence; Formal languages; Query languages; Data description",Conference Paper,Scopus,2-s2.0-84896063315
"Tian H., Zhao Y., Ni R., Qin L., Li X.","LDFT-based watermarking resilient to local desynchronization attacks",2013,"IEEE Transactions on Cybernetics",16,10.1109/TCYB.2013.2245415,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890032899&doi=10.1109%2fTCYB.2013.2245415&partnerID=40&md5=e2b0f085473f3bba176371bda582db8d","Up to now, a watermarking scheme that is robust against desynchronization attacks (DAs) is still a grand challenge. Most image watermarking resynchronization schemes in literature can survive individual global DAs (e.g., rotation, scaling, translation, and other affine transforms), but few are resilient to challenging cropping and local DAs. The main reason is that robust features for watermark synchronization are only globally invariable rather than locally invariable. In this paper, we present a blind image watermarking resynchronization scheme against local transform attacks. First, we propose a new feature transform named local daisy feature transform (LDFT), which is not only globally but also locally invariable. Then, the binary space partitioning (BSP) tree is used to partition the geometrically invariant LDFT space. In the BSP tree, the location of each pixel is fixed under global transform, local transform, and cropping. Lastly, the watermarking sequence is embedded bit by bit into each leaf node of the BSP tree by using the logarithmic quantization index modulation watermarking embedding method. Simulation results show that the proposed watermarking scheme can survive numerous kinds of distortions, including common image-processing attacks, local and global DAs, and noninvertible cropping. © 2013 IEEE.","Local daisy feature transform (LDFT); Robust; Watermarking","Binary space partitioning trees; De-synchronization attacks; Feature transform; Quantization index modulation; Robust; Watermark synchronization; Watermarking embedding; Watermarking schemes; Binary trees; Computer crime; Digital watermarking; Forestry; Watermarking; Image watermarking; Forestry; Image Analysis; Water Marks; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; computer graphics; computer security; information processing; methodology; packaging; theoretical model; Algorithms; Artificial Intelligence; Computer Graphics; Computer Security; Data Compression; Image Interpretation, Computer-Assisted; Models, Theoretical; Pattern Recognition, Automated; Product Labeling",Article,Scopus,2-s2.0-84890032899
"Ni Z., Fang X., He H., Zhao D., Xu X.","Real-time tracking on adaptive critic design with uniformly ultimately bounded condition",2013,"IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning, ADPRL",16,10.1109/ADPRL.2013.6614987,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891525568&doi=10.1109%2fADPRL.2013.6614987&partnerID=40&md5=1e2c888de2d30655799d84dc6365cf34","In this paper, we proposed a new nonlinear tracking controller based on heuristic dynamic programming (HDP) with the tracking filter. Specifically, we integrate a goal network into the regular HDP design and provide the critic network with detailed internal reward signal to help the value function approximation. The architecture is explicitly explained with the tracking filter, goal network, critic network and action network, respectively. We provide the stability analysis of our proposed controller with Lyapunov approach. It is shown that the filtered tracking errors and the weights estimation errors in neural networks are all uniformly ultimately bounded (UUB) under certain conditions. Finally, we compare our proposed approach with regular HDP approach in virtual reality (VR)/Simulink environment to justify the improved control performance. © 2013 IEEE.",,"Adaptive critic designs; Heuristic dynamic programming; Nonlinear tracking controller; Real time tracking; Stability analysis; Uniformly ultimately bounded; Value function approximation; Weights estimation; Artificial intelligence; Errors; Reinforcement learning; Virtual reality; Dynamic programming",Conference Paper,Scopus,2-s2.0-84891525568
"Gilroy S.W., Porteous J., Charles F., Cavazza M., Soreq E., Raz G., Ikar L., Or-Borichov A., Ben-Arie U., Klovatch I., Hendler T.","A brain-computer interface to a plan-based narrative",2013,"IJCAI International Joint Conference on Artificial Intelligence",16,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062663&partnerID=40&md5=1b55b42ca1182c50eb10e35fe2bbc020","Interactive Narrative is a form of digital entertainment heavily based on AI techniques to support narrative generation and user interaction, significant progress arriving with the adoption of planning techniques. However, there is a lack of unified models that integrate generation, user responses and interaction. This paper addresses this by revisiting existing Interactive Narrative paradigms, granting explicit status to users' disposition towards story characters as part of narrative generation as well as adding support for new forms of interaction. We demonstrate this with a novel Brain-Computer Interface (BCI) design, incorporating empathy for a main character derived from brain signals within filmic conceptions of narrative which drives generation using planning techniques. Results from an experimental study with a fullyimplemented system demonstrate the effectiveness of a EEG neurofeedback-based approach, showing that subjects can successfully modulate empathic support of a character in a medical drama. MRI analysis also shows activations in associated regions of the brain during expression of support.",,"AI techniques; Brain signals; Digital entertainment; Interactive narrative; MRI-analysis; Planning techniques; Unified model; User interaction; Artificial intelligence; Interfaces (materials); Brain computer interface",Conference Paper,Scopus,2-s2.0-84896062663
"Muggleton S., Lin D.","Meta-Interpretive Learning of higher-order dyadic datalog: Predicate invention revisited",2013,"IJCAI International Joint Conference on Artificial Intelligence",16,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896061233&partnerID=40&md5=fe133ab1dad8cae1321c20da685af037","In recent years Predicate Invention has been underexplored within Inductive Logic Programming due to difficulties in formulating efficient search mechanisms. However, a recent paper demonstrated that both predicate invention and the learning of recursion can be efficiently implemented for regular and context-free grammars, by way of abduction with respect to a meta-interpreter. New predicate symbols are introduced as constants representing existentially quantified higher-order variables. In this paper we generalise the approach of Meta-Interpretive Learning (MIL) to that of learning higher-order dyadic datalog programs. We show that with an infinite signature the higher-order dyadic datalog class H2 2 has universal Turing expressivity though H2 2 is decidable given a finite signature. Additionally we show that Knuth-Bendix ordering of the hypothesis space together with logarithmic clause bounding allows our Dyadic MIL implementation MetagolD to PAC-learn minimal cardinailty H2 p definitions. This result is consistent with our experiments which indicate that MetagolD efficiently learns compact H2 2 definitions involving predicate invention for robotic strategies and higher-order concepts in the NELL language learning domain.",,"Datalog programs; Finite signatures; Higher-order; Hypothesis space; Knuth-Bendix order; Language learning; Meta-interpreters; Search mechanism; Artificial intelligence; Patents and inventions",Conference Paper,Scopus,2-s2.0-84896061233
"Jøsang A., Guo G., Pini M.S., Santini F., Xu Y.","Combining recommender and reputation systems to produce better online advice",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,10.1007/978-3-642-41550-0_12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893240083&doi=10.1007%2f978-3-642-41550-0_12&partnerID=40&md5=8dd962f7e7ec62efaf697bac2200c484","Although recommender systems and reputation systems have quite different theoretical and technical bases, both types of systems have the purpose of providing advice for decision making in e-commerce and online service environments. The similarity in purpose makes it natural to integrate both types of systems in order to produce better online advice, but their difference in theory and implementation makes the integration challenging. In this paper, we propose to use mappings to subjective opinions from values produced by recommender systems as well as from scores produced by reputation systems, and to combine the resulting opinions within the framework of subjective logic. © 2013 Springer-Verlag.",,"On-line service; Reputation systems; Subjective Logic; Artificial intelligence; Electronic commerce; Recommender systems; Online systems",Conference Paper,Scopus,2-s2.0-84893240083
"Pfeifer R., Marques H.G., Iida F.","Soft robotics: The next generation of intelligent machines",2013,"IJCAI International Joint Conference on Artificial Intelligence",16,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896061173&partnerID=40&md5=ecc3394dde5014bdf70538aa6e15e128","There has been an increasing interest in applying biological principles to the design and control of robots. Unlike industrial robots that are programmed to execute a rather limited number of tasks, the new generation of bio-inspired robots is expected to display a wide range of behaviours in unpredictable environments, as well as to interact safely and smoothly with human co-workers. In this article, we put forward some of the properties that will characterize these new robots: soft materials, flexible and stretchable sensors, modular and efficient actuators, self-organization and distributed control. We introduce a number of design principles; in particular, we try to comprehend the novel design space that now includes soft materials and requires a completely different way of thinking about control. We also introduce a recent case study of developing a complex humanoid robot, discuss the lessons learned and speculate about future challenges and perspectives.",,"Bio-inspired robots; Biological principles; Design and control; Distributed control; Future challenges; Intelligent machine; Self organizations; Unpredictable environments; Anthropomorphic robots; Artificial intelligence; Biology; Machine design",Conference Paper,Scopus,2-s2.0-84896061173
"Agrawal P., Garg V.K., Narayanam R.","Link label prediction in signed social networks",2013,"IJCAI International Joint Conference on Artificial Intelligence",16,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896061815&partnerID=40&md5=2f5f0c2bb794a7fccf23c49e721f1f70","Online social networks continue to witness a tremendous growth both in terms of the number of registered users and their mutual interactions. In this paper, we focus on online signed social networks where positive interactions among the users signify friendship or approval, whereas negative interactions indicate antagonism or disapproval. We introduce a novel problem which we call the link label prediction problem: Given the information about signs of certain links in a social network, we want to learn the nature of relationships that exist among the users by predicting the sign, positive or negative, of the remaining links. We propose a matrix factorization based technique MF-LiSP that exhibits strong generalization guarantees. We also investigate the applicability of in this setting. Our experiments on Wiki-Vote, Epinions and Slashdot data sets strongly corroborate the efficacy of these approaches.",,"Link labels; Matrix factorizations; Mutual interaction; Negative interaction; On-line social networks; Positive interaction; Artificial intelligence; Forecasting; Social networking (online)",Conference Paper,Scopus,2-s2.0-84896061815
"Pommerening F., Röger G., Helmert M.","Getting the most out of pattern databases for classical planning",2013,"IJCAI International Joint Conference on Artificial Intelligence",16,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896064121&partnerID=40&md5=44e151931d0e77e47fd03db815f21713","The iPDB procedure by Haslum et al. is the state-of-the-art method for computing additive abstraction heuristics for domain-independent planning. It performs a hill-climbing search in the space of pattern collections, combining information from multiple patterns in the so-called canonical heuristic. We show how stronger heuristic estimates can be obtained through linear programming. An experimental evaluation demonstrates the strength of the new technique on the IPC benchmark suite.",,"Benchmark suites; Classical planning; Domain-independent planning; Experimental evaluation; Multiple patterns; Pattern collections; Pattern database; State-of-the-art methods; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84896064121
"Yu J., LaValle S.M.","Structure and intractability of optimal multi-robot path planning on graphs",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",16,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893394436&partnerID=40&md5=62988674004cdbe4fe6ef42587964a87","In this paper, we study the structure and computational complexity of optimal multi-robot path planning problems on graphs. Our results encompass three formulations of the discrete multi-robot path planning problem, including a variant that allows synchronous rotations of robots along fully occupied, disjoint cycles on the graph. Allowing rotation of robots provides a more natural model for multi-robot path planning because robots can communicate. Our optimality objectives are to minimize the total arrival time, the makespan (last arrival time), and the total distance. On the structure side, we show that, in general, these objectives demonstrate a pairwise Pareto optimal structure and cannot be simultaneously optimized. On the computational complexity side, we extend previous work and show that, regardless of the underlying multi-robot path planning problem, these objectives are all intractable to compute. In particular, our NP-hardness proof for the time optimal versions, based on a minimal and direct reduction from the 3-satisfiability problem, shows that these problems remain NP-hard even when there are only two groups of robots (i.e. robots within each group are interchangeable). Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Direct Reduction; Disjoint cycle; Multi-robot path planning; Natural models; Pareto optimal structures; Synchronous rotation; Time-optimal; Total distances; Artificial intelligence; Computational complexity; Scheduling algorithms; Structural optimization; Robots",Conference Paper,Scopus,2-s2.0-84893394436
"Schulte S., Hoenisch P., Venugopal S., Dustdar S.","Introducing the vienna platform for elastic processes",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,10.1007/978-3-642-37804-1_19,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892947577&doi=10.1007%2f978-3-642-37804-1_19&partnerID=40&md5=79a8fd951259c6988de60c04ba710ba0","Resource-intensive tasks are playing an increasing role in business processes. The emergence of Cloud computing has enabled the deployment of such tasks onto resources sourced on-demand from Cloud providers. This has enabled so-called elastic processes that are able to dynamically adjust their resource usage to meet varying workloads. Traditional Business Process Management Systems (BPMSs) do not consider the needs of elastic processes such as monitoring facilities, tracking the current and future system landscape, reasoning about optimally utilizing resources given Quality of Service constraints, and executing necessary actions (e.g., start/stop servers, move services). This paper introduces ViePEP, a research BPMS capable of handling the aforementioned requirements of elastic processes. © Springer-Verlag 2013.",,"Business Process; Business process management systems; Cloud providers; Elastic process; On demands; Quality of Service constraints; Resource usage; Computer science; Computers; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84892947577
"Sakai Y., Emura K., Hanaoka G., Kawai Y., Matsuda T., Omote K.","Group signatures with message-dependent opening",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884472675&partnerID=40&md5=e3d3c9c360289b9591800743f8acdf84","This paper introduces a new capability of the group signature, called message-dependent opening. It is intended to weaken the higher trust put on an opener, that is, no anonymity against an opener is provided by ordinary group signature. In a group signature system with message-dependent opening (GS-MDO), in addition to the opener, we set up the admitter which is not able to open any user's identity but admits the opener to open signatures by specifying messages whose signatures should be opened. For any signature whose corresponding message is not specified by the admitter, the opener cannot extract the signer's identity from it. In this paper, we present formal definitions and constructions of GS-MDO. Furthermore, we also show that GS-MDO implies identity-based encryption, and thus for designing a GS-MDO scheme, identity-based encryption is crucial. Actually, we propose a generic construction of GS-MDO from identity-based encryption and adaptive NIZK proofs, and its specific instantiation from the Groth-Sahai proof system by constructing a new (κ-resilient) identity-based encryption scheme which is compatible to the Groth-Sahai proof. © Springer-Verlag Berlin Heidelberg 2013.",,"Formal definition; Generic construction; Group signatures; Identity Based Encryption; Proof system; Signer's identity; Artificial intelligence; Computer science; Computers; Cryptography",Conference Paper,Scopus,2-s2.0-84884472675
"Michalak T.P., Rahwan T., Jennings N.R., Szczepański P.L., Skibski O., Narayanam R., Wooldridge M.J.","Computational analysis of connectivity games with applications to the investigation of terrorist networks",2013,"IJCAI International Joint Conference on Artificial Intelligence",16,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062539&partnerID=40&md5=948694fb66312b79a7bba17462c94247","We study a recently developed centrality metric to identify key players in terrorist organisations due to Lindelauf et al. [2013]. This metric, which involves computation of the Shapley value for connectivity games on graphs proposed by Amer and Gimenez [2004], was shown to produce substantially better results than previously used standard centralities. In this paper, we present the first computational analysis of this class of coalitional games, and propose two algorithms for computing Lindelauf et al.'s centrality metric. Our first algorithm is exact, and runs in time linear by number of connected subgraphs in the network. As shown in the numerical simulations, our algorithm identifies key players in the WTC 9/11 terrorist network, constructed of 36 members and 125 links, in less than 40 minutes. In contrast, a general-purpose Shapley value algorithm would require weeks to solve this problem. Our second algorithm is approximate and can be used to study much larger networks.",,"Coalitional game; Computational analysis; Connected subgraphs; Connectivity games; Larger networks; Shapley value; Terrorist networks; Artificial intelligence; Computational methods; Computer games; Game theory; Terrorism; Algorithms",Conference Paper,Scopus,2-s2.0-84896062539
"Soufiani H.A., Parkes D.C., Xia L.","Preference elicitation for General Random Utility Models",2013,"Uncertainty in Artificial Intelligence - Proceedings of the 29th Conference, UAI 2013",16,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888194225&partnerID=40&md5=1341f99029ab53cb2949800f42232745","This paper discusses General Random Utility Models (GRUMs). These are a class of parametric models that generate partial ranks over alternatives given attributes of agents and alternatives. We propose two preference elicitation scheme for GRUMs developed from principles in Bayesian experimental design, one for social choice and the other for personalized choice. We couple this with a general Monte-Carlo-Expectation-Maximization (MC-EM) based algorithm for MAP inference under GRUMs. We also prove uni-modality of the likelihood functions for a class of GRUMs. We examine the performance of various criteria by experimental studies, which show that the proposed elicitation scheme increases the precision of estimation.",,"Bayesian experimental designs; Likelihood functions; MAP inferences; Parametric models; Preference elicitation; Random utility model; Social choice; Algorithms; Inference engines; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84888194225
"Tung C.-W.","Prediction of pupylation sites using the composition of k-spaced amino acid pairs",2013,"Journal of Theoretical Biology",16,10.1016/j.jtbi.2013.07.009,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881234023&doi=10.1016%2fj.jtbi.2013.07.009&partnerID=40&md5=e10f697ae2fbff7686a5795ec9879cb4","Pupylation is an important post-translational modification in prokaryotes. A prokaryotic ubiquitin-like protein (Pup) is attached to proteins as a signal for selective degradation by proteasome. Several proteomics methods have been developed for the identification of pupylated proteins and pupylation sites. However, pupylation sites of many experimentally identified pupylated proteins are still unknown. The development of sequence-based prediction methods can help to accelerate the identification of pupylation sites and gain insights into the substrate specificity and regulatory functions of pupylation. A novel tool iPUP is developed for the computational identification of pupylation sites. A composition of k-spaced amino acid pairs is utilized to represent a peptide sequence. Top ranked k-spaced amino acid pairs are subsequently selected by using a sequential backward feature elimination algorithm. The 10-fold cross-validation performance of iPUP trained by using the composition of 150 top ranked k-spaced amino acid pairs and support vector machines is 0.83 for the area under receiver operating characteristic curve. The importance analysis of k-spaced amino acid pairs shows that terminal space-containing pairs are useful for discriminating pupylation sites from non-pupylation sites. A sequence analysis confirms that lysines close to C-terminus tend to be pupylated. In contrast, lysines close to N-terminus are less likely to be pupylated. The iPUP tool can predict pupylation sites with probability scores for prioritizing promising pupylation sites. Both the online server and the standalone software of iPUP are freely available for academic use at http://cwtung.kmu.edu.tw/ipup. © 2013 Elsevier Ltd.","Feature selection; K-spaced amino acid pairs; Pupylation; Software; Support vector machine","prokaryotic ubiquitin like protein; ubiquitin; unclassified drug; amino acid; protein; ubiquitin; algorithm; amino acid; artificial intelligence; chemical composition; degradation; prokaryote; protein; software; algorithm; amino acid composition; amino acid sequence; article; carboxy terminal sequence; computer program; enzyme specificity; nonhuman; online system; prediction; priority journal; process development; prokaryote; protein processing; pupylation; scoring system; support vector machine; validation process; web browser; chemistry; metabolism; molecular genetics; protein database; reproducibility; Algorithms; Amino Acid Sequence; Amino Acids; Databases, Protein; Molecular Sequence Data; Protein Processing, Post-Translational; Proteins; Reproducibility of Results; Software; Ubiquitins",Article,Scopus,2-s2.0-84881234023
"Baltazar P., Caires L., Vasconcelos V.T., Vieira H.T.","A type system for flexible role assignment in multiparty communicating systems",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,10.1007/978-3-642-41157-1_6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886414647&doi=10.1007%2f978-3-642-41157-1_6&partnerID=40&md5=5c1fe3c0bd8d8534ac5815d2eb4ba691","Communication protocols in distributed systems often specify the roles of the parties involved in the communications, namely for enforcing security policies or task assignment purposes. Ensuring that implementations follow role-based protocol specifications is challenging, especially in scenarios found, e.g., in business processes and web applications, where multiple peers are involved, single peers impersonate several roles, or single roles are carried out by several peers. We present a type-based analysis for statically verifying role-based multi-party interactions, based on a simple π-calculus model and prior work on conversation types. Our main result ensures that well-typed systems follow the role-based protocols prescribed by the types, including systems where roles are flexibly assigned to processes. © 2013 Springer-Verlag.",,"Business Process; Distributed systems; Multi-party interactions; Protocol specifications; Security policy; Task assignment; Type-based analysis; WEB application; Artificial intelligence; Computer science",Conference Paper,Scopus,2-s2.0-84886414647
"Vanden Broucke S.K.L.M., De Weerdt J., Vanthienen J., Baesens B.","A comprehensive benchmarking framework (CoBeFra) for conformance analysis between procedural process models and event logs in ProM",2013,"Proceedings of the 2013 IEEE Symposium on Computational Intelligence and Data Mining, CIDM 2013 - 2013 IEEE Symposium Series on Computational Intelligence, SSCI 2013",16,10.1109/CIDM.2013.6597244,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885665956&doi=10.1109%2fCIDM.2013.6597244&partnerID=40&md5=0cf59163715e2a5efa52759fb3064f4b","Process mining encompasses the research area which is concerned with knowledge discovery from information system event logs. Within the process mining research area, two prominent tasks can be discerned. First of all, process discovery deals with the automatic construction of a process model out of an event log. Secondly, conformance checking focuses on the assessment of the quality of a discovered or designed process model in respect to the actual behavior as captured in event logs. Hereto, multiple techniques and metrics have been developed and described in the literature. However, the process mining domain still lacks a comprehensive framework for assessing the goodness of a process model from a quantitative perspective. In this study, we describe the architecture of an extensible framework within ProM, allowing for the consistent, comparative and repeatable calculation of conformance metrics. For the development and assessment of both process discovery as well as conformance techniques, such a framework is considered greatly valuable. © 2013 IEEE.",,"Automatic construction; Conformance checking; Event logs; Extensible framework; Process Discovery; Process mining; Process model; Process Modeling; Artificial intelligence; Data mining",Conference Paper,Scopus,2-s2.0-84885665956
"Song L., Minku L.L., Yao X.","The impact of parameter tuning on software effort estimation using learning machines",2013,"ACM International Conference Proceeding Series",16,10.1145/2499393.2499396,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924914270&doi=10.1145%2f2499393.2499396&partnerID=40&md5=2f3c7dae1d620883db867df316e02f1e","Background: The use of machine learning approaches for software effort estimation (SEE) has been studied for more than a decade. Most studies performed comparisons of different learning machines on a number of data sets. However, most learning machines have more than one parameter that needs to be tuned, and it is unknown to what extent parameter settings may affect their performance in SEE. Many works seem to make an implicit assumption that parameter settings would not change the outcomes significantly. Aims: To investigate to what extent parameter settings affect the performance of learning machines in SEE, and what learning machines are more sensitive to their parameters. Method: Considering an online learning scenario where learning machines are updated with new projects as they become available, systematic experiments were performed using five learning machines under several different parameter settings on three data sets. Results: While some learning machines such as bagging using regression trees were not so sensitive to parameter settings, others such as multilayer perceptrons were affected dramatically. Combining learning machines into bagging ensembles helped making them more robust against different parameter settings. The average performance of k-NN across different projects was not so much affected by different parameter settings, but the parameter settings that obtained the best average performance across time steps were not so consistently the best throughout time steps as in the other approaches. Conclusions: Learning machines that are more/less sensitive to different parameter settings were identified. The different sensitivity obtained by different learning machines shows that sensitivity to parameters should be considered as one of the criteria for evaluation of SEE approaches. A good learning machine for SEE is not only one which is able to achieve superior performance, but also one that is either less dependent on parameter settings or to which good parameter choices are easy to make.","Ensembles; Machine learning; Online learning; Sensitivity to parameters; Software effort estimation","Artificial intelligence; E-learning; Learning systems; Nearest neighbor search; Parameter estimation; Software engineering; Criteria for evaluations; Ensembles; Learning machines; Machine learning approaches; Online learning; Sensitivity to parameters; Software effort estimation; Systematic experiment; Education",Conference Paper,Scopus,2-s2.0-84924914270
"Araujo J.F., Sujit P.B., Sousa J.B.","Multiple UAV area decomposition and coverage",2013,"Proceedings of the 2013 IEEE Symposium on Computational Intelligence for Security and Defense Applications, CISDA 2013 - 2013 IEEE Symposium Series on Computational Intelligence, SSCI 2013",16,10.1109/CISDA.2013.6595424,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884995611&doi=10.1109%2fCISDA.2013.6595424&partnerID=40&md5=f54a397b6f41774f601708579f04ddbb","Multiple UAVs can be used to cover a region effectively. Area coverage involves two stages-area decomposition into cells and path planning inside the cells. The area is decomposed using sweeping technique. For path planning inside the cells, a novel method is developed where optimal number of lanes are generated to minimize the number of UAV turns to accomplish the mission in minimum time. Also to allow operator interaction and precedence in coverage, a novel lawnmower/Zamboni planner is designed. Simulation results are presented for multiple UAVs are presented and flight test results for one UAV performing a persistent mission is presented. © 2013 IEEE.",,"Area coverages; Flight test; Minimum time; Multiple uav; Multiple UAVs; Optimal number; Artificial intelligence; Motion planning",Conference Paper,Scopus,2-s2.0-84884995611
"Kalampokis E., Tambouris E., Tarabanis K.","Linked open government data analytics",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,10.1007/978-3-642-40358-3-9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885013362&doi=10.1007%2f978-3-642-40358-3-9&partnerID=40&md5=da03fc23facd48386906819f8eb6afc6","Although the recently launched Open Government Data (OGD) movement promised to provide a number of benefits, recent studies have shown that its full potential has not yet realized. The difficulty in exploiting open data seems surprising if we consider the huge importance data have in modern societies. In this paper we claim that the real value of OGD will unveil from performing data analytics on top of combined statistical datasets that were previously closed in disparate sources and can now be linked to provide unexpected and unexplored insights. To support this claim, we describe the linked OGD analytics concept along with its technical requirements and demonstrate its end-user value employing a use case related to UK general elections. The use case revealed that there is a significant relationship between the probability one of the two main political parties (i.e. Labour Party and Conservative Party) to win in a UK constituency and the unemployment rate in the same constituency. © 2013 IFIP International Federation for Information Processing.","data analytics; linked data; Open government data; statistics","Conservative party; Data analytics; General Elections; Linked datum; Open government data; Political parties; Technical requirement; Unemployment rates; Artificial intelligence; Computer science; Statistics; Government data processing",Conference Paper,Scopus,2-s2.0-84885013362
"Chua A.Y.K., Banerjee S.","So fast so good: An analysis of answer quality and answer speed in community Question-answering sites",2013,"Journal of the American Society for Information Science and Technology",16,10.1002/asi.22902,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883866321&doi=10.1002%2fasi.22902&partnerID=40&md5=e6dfd799744164d5d7ab68340ee03faf","The authors investigate the interplay between answer quality and answer speed across question types in community question-answering sites (CQAs). The research questions addressed are the following: (a) How do answer quality and answer speed vary across question types? (b) How do the relationships between answer quality and answer speed vary across question types? (c) How do the best quality answers and the fastest answers differ in terms of answer quality and answer speed across question types? (d) How do trends in answer quality vary over time across question types? From the posting of 3,000 questions in six CQAs, 5,356 answers were harvested and analyzed. There was a significant difference in answer quality and answer speed across question types, and there were generally no significant relationships between answer quality and answer speed. The best quality answers had better overall answer quality than the fastest answers but generally took longer to arrive. In addition, although the trend in answer quality had been mostly random across all question types, the quality of answers appeared to improve gradually when given time. By highlighting the subtle nuances in answer quality and answer speed across question types, this study is an attempt to explore a territory of CQA research that has hitherto been relatively uncharted. © 2013 ASIS&T.","information resources management","Information resources management; Question Answering; Question type; Research questions; Artificial intelligence; Software engineering; Speed",Article,Scopus,2-s2.0-84883866321
"Iglesias J.E., Sabuncu M.R., Van Leemput K.","Improved inference in Bayesian segmentation using Monte Carlo sampling: Application to hippocampal subfield volumetry",2013,"Medical Image Analysis",16,10.1016/j.media.2013.04.005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879887285&doi=10.1016%2fj.media.2013.04.005&partnerID=40&md5=618c1ed3f98ce9897df6fe89c0f6bbc1","Many segmentation algorithms in medical image analysis use Bayesian modeling to augment local image appearance with prior anatomical knowledge. Such methods often contain a large number of free parameters that are first estimated and then kept fixed during the actual segmentation process. However, a faithful Bayesian analysis would marginalize over such parameters, accounting for their uncertainty by considering all possible values they may take. Here we propose to incorporate this uncertainty into Bayesian segmentation methods in order to improve the inference process. In particular, we approximate the required marginalization over model parameters using computationally efficient Markov chain Monte Carlo techniques. We illustrate the proposed approach using a recently developed Bayesian method for the segmentation of hippocampal subfields in brain MRI scans, showing a significant improvement in an Alzheimer's disease classification task. As an additional benefit, the technique also allows one to compute informative ""error bars"" on the volume estimates of individual structures. © 2013 Elsevier B.V.","Bayesian modeling; Hippocampal subfields; Monte Carlo sampling; Segmentation","Bayesian modeling; Bayesian segmentation; Computationally efficient; Individual structures; Markov chain Monte Carlo techniques; Monte Carlo sampling; Segmentation algorithms; Subfields; Bayesian networks; Image segmentation; Magnetic resonance imaging; Monte Carlo methods; Uncertainty analysis; aged; Alzheimer disease; article; Bayes theorem; brain size; controlled study; disease classification; female; hippocampus; human; major clinical study; male; Monte Carlo method; neuroimaging; nuclear magnetic resonance imaging; priority journal; probability; volumetry; Bayesian modeling; Hippocampal subfields; Monte Carlo sampling; Segmentation; Algorithms; Alzheimer Disease; Artificial Intelligence; Bayes Theorem; Data Interpretation, Statistical; Hippocampus; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Monte Carlo Method; Organ Size; Pattern Recognition, Automated; Reproducibility of Results; Sample Size; Sensitivity and Specificity",Article,Scopus,2-s2.0-84879887285
"Kwitt R., Vasconcelos N., Razzaque S., Aylward S.","Localizing target structures in ultrasound video - A phantom study",2013,"Medical Image Analysis",16,10.1016/j.media.2013.05.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879880158&doi=10.1016%2fj.media.2013.05.003&partnerID=40&md5=2e601deb658b7e77369522a9944349b5","The problem of localizing specific anatomic structures using ultrasound (US) video is considered. This involves automatically determining when an US probe is acquiring images of a previously defined object of interest, during the course of an US examination.Localization using US is motivated by the increased availability of portable, low-cost US probes, which inspire applications where inexperienced personnel and even first-time users acquire US data that is then sent to experts for further assessment. This process is of particular interest for routine examinations in underserved populations as well as for patient triage after natural disasters and large-scale accidents, where experts may be in short supply.The proposed localization approach is motivated by research in the area of dynamic texture analysis and leverages several recent advances in the field of activity recognition. For evaluation, we introduce an annotated and publicly available database of US video, acquired on three phantoms. Several experiments reveal the challenges of applying video analysis approaches to US images and demonstrate that good localization performance is possible with the proposed solution. © 2013 Elsevier B.V.","Dynamic textures; Ultrasound imaging; Video analysis","Activity recognition; Anatomic structures; Dynamic textures; Localization performance; Natural disasters; Ultrasound imaging; Ultrasound videos; Video analysis; Probes; Textures; Ultrasonic imaging; Ultrasonics; accident; anatomic landmark; article; emergency health service; human; image analysis; mechanical probe; natural disaster; priority journal; ultrasound; videorecording; Dynamic textures; Ultrasound imaging; Video analysis; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Phantoms, Imaging; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique; Ultrasonography; Video Recording",Article,Scopus,2-s2.0-84879880158
"McGillion M.L., Herbert J.S., Pine J.M., Keren-Portnoy T., Vihman M.M., Matthews D.E.","Supporting early vocabulary development: What sort of responsiveness matters",2013,"IEEE Transactions on Autonomous Mental Development",16,10.1109/TAMD.2013.2275949,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884509106&doi=10.1109%2fTAMD.2013.2275949&partnerID=40&md5=e0ddb708a45e49158bb678b64555fce8","Maternal responsiveness has been positively related with a range of socioemotional and cognitive outcomes including language. A substantial body of research has explored different aspects of verbal responsiveness. However, perhaps because of the many ways in which it can be operationalized, there is currently a lack of consensus around what type of responsiveness is most helpful for later language development. The present study sought to address this problem by considering both the semantic and temporal dimensions of responsiveness on a single cohort while controlling for level of parental education and the overall amount of communication on the part of both the caregiver and the infant. We found that only utterances that were both semantically appropriate and temporally linked to an infant vocalization were related to infant expressive vocabulary at 18 mo. © 2013 IEEE.","Dyadic interaction; maternal responsiveness; vocabulary development","Dyadic interaction; Language development; maternal responsiveness; Temporal dimensions; vocabulary development; Artificial intelligence; Software engineering; Semantics",Article,Scopus,2-s2.0-84884509106
"Andrikopoulos V., Strauch S., Leymann F.","Decision support for application migration to the cloud challenges and vision",2013,"CLOSER 2013 - Proceedings of the 3rd International Conference on Cloud Computing and Services Science",16,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884488576&partnerID=40&md5=4bb1647955da079d00e0f5270ee0af5e","The success of Cloud computing has encouraged many application developers to consider migrating their applications to the Cloud. Given the early market dominance of the IaaS service model, many existing works focus on selecting the best service provider for a set of criteria related to the virtualization and hosting of the application. In this work, we aim to progress the State of the Art by formulating a vision of a decision support system that incorporates multiple dimensions and different analysis tasks in feedback relationships with each other. The research challenges that need to be addressed towards this direction are identified and related to the different aspects of migration of applications to the various Cloud service models.","Application migration; Cloud-enabling applications; Decision support","Application developers; Application migrations; Cloud service models; Decision supports; Multiple dimensions; Research challenges; Service provider; State of the art; Artificial intelligence; Cloud computing; Decision support systems",Conference Paper,Scopus,2-s2.0-84884488576
"Nalepa G.J., Kluza K., Kaczor K.","Proposal of an inference engine architecture for business rules and processes",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,10.1007/978-3-642-38610-7_42,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884409952&doi=10.1007%2f978-3-642-38610-7_42&partnerID=40&md5=fdec30f4766849872f6fda4bd6ebf19a","In this paper, we discuss a new architecture for integrating and executing business process models with rules. It is based on a workflow engine that runs a BPMN-based business process model. On the lower level, rules are used to express the specific parts of the business logic. Rules working in the same context are grouped into a single task in the process model. Such a rule task is modeled by a formally defined decision table, which is designed in a visual way and its quality can be formally verified. In the runtime environment, tables are executed by a dedicated rule inference engine controlled by a workflow engine. © 2013 Springer-Verlag.",,"Business logic; Business process model; Business rules; Process Modeling; Runtime environments; Workflow engines; Artificial intelligence; Decision tables; Engines; Soft computing",Conference Paper,Scopus,2-s2.0-84884409952
"Di Ruscio D., Iovino L., Pierantonio A.","A Methodological Approach for the Coupled Evolution of Metamodels and ATL Transformations",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,10.1007/978-3-642-38883-5_9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883711752&doi=10.1007%2f978-3-642-38883-5_9&partnerID=40&md5=45e9bbb5809a6dcfa79242008c4872eb","Model-Driven Engineering is a software discipline that relies on (meta) models as first class entities and that aims to develop, maintain and evolve software by exploiting model transformations. Analogously to software, metamodels are subject to evolutionary pressures which might compromise a wide range of artefacts including transformations. In contrast with the problem of metamodel/model co-evolution, the problem of adapting model transformations according to the changes operated on the corresponding metamodels is to a great extent unexplored. This is largely due to its intricacy but also to the difficulty in having a mature process which on one hand is able to evaluate the cost and benefits of adaptations, and on the other hand ensures that consistent methods are used to maintain quality and design integrity during the adaptation. This paper proposes a methodological approach to the coupled evolution of ATL transformations aiming at evaluating its sustainability prior to any adaptation step based on the assessment of change impact significance. © 2013 Springer-Verlag.",,"Change impacts; Co-evolution; Cost and benefits; Coupled evolution; Methodological approach; Model transformation; Model-driven Engineering; Software disciplines; Artificial intelligence; Computer science; Mathematical models",Conference Paper,Scopus,2-s2.0-84883711752
"Boella G., Janssen M., Hulstijn J., Humphreys L., Van Der Torre L.","Managing legal interpretation in regulatory compliance",2013,"Proceedings of the International Conference on Artificial Intelligence and Law",16,10.1145/2514601.2514605,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883530202&doi=10.1145%2f2514601.2514605&partnerID=40&md5=3e39c0eef2798bfce5542a54a00ab2dd","Maintaining regulatory compliance is an increasing area of concern for business. Legal Knowledge Management systems that combine repositories of legislation with legal ontologies can support the work of in-house compliance managers. But there are challenges to overcome, of interpreting legal knowledge and mapping that knowledge onto business processes, and developing systems that can adequately handle the complexity with clarity and ease. In this paper we extend the Legal Knowledge Management system Eunomos to deal with alternative interpretations of norms connecting it with Business Process Management systems. Moreover, we propose a workflow involving the different roles in a company, which takes legal interpretation into account in mapping norms and processes, using Eunomos as a support. Copyright 2013 ACM.",,"Business Process; Business process management systems; Legal knowledge; Legal ontology; Artificial intelligence; Knowledge acquisition; Knowledge based systems; Regulatory compliance; Management",Conference Paper,Scopus,2-s2.0-84883530202
"Didona D., Felber P., Harmanci D., Romano P., Schenker J.","Identifying the optimal level of parallelism in transactional memory applications",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,10.1007/978-3-642-40148-0_17,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883136743&doi=10.1007%2f978-3-642-40148-0_17&partnerID=40&md5=0701fea9abeae380a5babfed9cfe3a57","In this paper we investigate the issue of automatically identifying the ""natural"" degree of parallelism of an application using software transactional memory (STM), i.e., the workload-specific multiprogramming level that maximizes application's performance. We discuss the importance of adapting the concurrency level to the workload in two different scenarios, a shared-memory and a distributed STM infrastructure. We propose and evaluate two alternative self-tuning methodologies, explicitly tailored for the considered scenarios. In shared-memory STM, we show that lightweight, black-box approaches relying solely on on-line exploration can be extremely effective. For distributed STMs, we introduce a novel hybrid approach that combines model-driven performance forecasting techniques and on-line exploration in order to take the best of the two techniques, namely enhancing robustness despite model's inaccuracies, and maximizing convergence speed towards optimum solutions. © 2013 Springer-Verlag.",,"Black box approach; Convergence speed; Degree of parallelism; Multiprogramming levels; Optimum solution; Performance forecasting; Software transactional memory; Transactional memory; Artificial intelligence; Computer science; Multiprogramming",Conference Paper,Scopus,2-s2.0-84883136743
"García J.M., Junghans M., Ruiz D., Agarwal S., Ruiz-Cortés A.","Integrating semantic Web services ranking mechanisms using a common preference model",2013,"Knowledge-Based Systems",16,10.1016/j.knosys.2013.04.007,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879879576&doi=10.1016%2fj.knosys.2013.04.007&partnerID=40&md5=714af2243db9dfada4f111e805ee6298","Service ranking has been long-acknowledged to play a fundamental role in helping users to select the best offerings among services retrieved from a search request. There exist many ranking mechanisms, each one providing ad hoc preference models that offer different levels of expressiveness. Consequently, applying a single mechanism to a particular scenario constrains the user to define preferences based on that mechanism's facilities. Furthermore, a more flexible solution that uses several independent mechanisms will face interoperability issues because of the differences between preference models provided by each ranking mechanism. In order to overcome these issues, we propose a Preference-based Universal Ranking Integration (PURI) framework that enables the combination of several ranking mechanisms using a common, holistic preference model. Using PURI, different ranking mechanisms are seamlessly and transparently integrated, offering a single façade to define preferences using highly expressive facilities that are not only decoupled from the concrete mechanisms that perform the ranking process, but also allow to exploit synergies from the combination of integrated mechanisms. We also thoroughly present a particular application scenario in the SOA4All EU project and evaluate the benefits and applicability of PURI in further domains. © 2013 Elsevier B.V. All rights reserved.","Preference models; Semantic Web services; Service ranking; Service retrieval; Systems integration","Application scenario; Preference models; Ranking mechanisms; Ranking process; Service ranking; Service retrieval; Single mechanisms; Systems integration; Artificial intelligence; Semantic Web; Software engineering",Article,Scopus,2-s2.0-84879879576
"Chang X., Wang B., Liu J., Wang W., Muppala J.K.","Green cloud virtual network provisioning based ant colony optimization",2013,"GECCO 2013 - Proceedings of the 2013 Genetic and Evolutionary Computation Conference Companion",16,10.1145/2464576.2482735,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882364445&doi=10.1145%2f2464576.2482735&partnerID=40&md5=e9668afa8d17859f51ba99121c1d6d02","Network virtualization is being regarded as a promising technology to create an ecosystem for cloud computing applications. One critical issue in network virtualization technology is power-efficient virtual network embedding (PEVNE), which deals with the physical resource allocation to virtual nodes and links of a virtual network while minimizing the energy consumption in the cloud data center. When the node and link constraints (including CPU, memory, network bandwidth, and network delay) are both taken into account, the VN embedding problem is NP-hard, even in the offline case. This paper aims to investigate the ability of the Ant-Colony-Optimization (ACO) technique in handling PE-VNE problem. We propose an ACObased heuristic PE-VNE algorithm, called E-ACO. E-ACO minimizes the energy consumption by considering the embedding power consumption in the node mapping phase and by making an implicit coordination between the node and link mapping phases. Extensive simulations are conducted to evaluate the performance of the proposed algorithm and investigate different energy-aware link embedding algorithms on the ability of E-ACO.","Ant colony optimization; Cloud data center; Energy consumption; Mixed integer programming; Optimization; Virtual network embedding","Cloud data centers; Computing applications; Embedding algorithms; Embedding problems; Extensive simulations; Mixed integer programming; Network virtualization; Virtual network embedding; Algorithms; Artificial intelligence; Energy utilization; Optimization; Virtual reality; Ant colony optimization",Conference Paper,Scopus,2-s2.0-84882364445
"Lin C., Karlson E.W., Canhao H., Miller T.A., Dligach D., Chen P.J., Perez R.N.G., Shen Y., Weinblatt M.E., Shadick N.A., Plenge R.M., Savova G.K.","Automatic Prediction of Rheumatoid Arthritis Disease Activity from the Electronic Medical Records",2013,"PLoS ONE",16,10.1371/journal.pone.0069932,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881569536&doi=10.1371%2fjournal.pone.0069932&partnerID=40&md5=b3e5f274684da5fe027b18345f81a2e7","Objective:We aimed to mine the data in the Electronic Medical Record to automatically discover patients' Rheumatoid Arthritis disease activity at discrete rheumatology clinic visits. We cast the problem as a document classification task where the feature space includes concepts from the clinical narrative and lab values as stored in the Electronic Medical Record.Materials and Methods:The Training Set consisted of 2792 clinical notes and associated lab values. Test Set 1 included 1749 clinical notes and associated lab values. Test Set 2 included 344 clinical notes for which there were no associated lab values. The Apache clinical Text Analysis and Knowledge Extraction System was used to analyze the text and transform it into informative features to be combined with relevant lab values.Results:Experiments over a range of machine learning algorithms and features were conducted. The best performing combination was linear kernel Support Vector Machines with Unified Medical Language System Concept Unique Identifier features with feature selection and lab values. The Area Under the Receiver Operating Characteristic Curve (AUC) is 0.831 (σ = 0.0317), statistically significant as compared to two baselines (AUC = 0.758, σ = 0.0291). Algorithms demonstrated superior performance on cases clinically defined as extreme categories of disease activity (Remission and High) compared to those defined as intermediate categories (Moderate and Low) and included laboratory data on inflammatory markers.Conclusion:Automatic Rheumatoid Arthritis disease activity discovery from Electronic Medical Record data is a learnable task approximating human performance. As a result, this approach might have several research applications, such as the identification of patients for genome-wide pharmacogenetic studies that require large sample sizes with precise definitions of disease activity and response to therapies. © 2013 Lin et al.",,"article; controlled study; disease activity; electronic medical record; genetic association; human; kernel method; learning algorithm; machine learning; major clinical study; patient identification; pharmacogenetics; prediction; receiver operating characteristic; rheumatoid arthritis; support vector machine; task performance; Unified Medical Language System; Antirheumatic Agents; Arthritis, Rheumatoid; Artificial Intelligence; Data Mining; Disease Progression; Electronic Health Records; Humans; ROC Curve; Support Vector Machines",Article,Scopus,2-s2.0-84881569536
"Bouma H., Burghouts G., Penning L.D., Hanckmann P., Ten Hove J.-M., Korzec S., Kruithof M., Landsmeer S., Van Leeuwen C., Van Den Broek S., Halma A., Den Hollander R., Schutte K.","Recognition and localization of relevant human behavior in videos",2013,"Proceedings of SPIE - The International Society for Optical Engineering",16,10.1117/12.2015877,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881058556&doi=10.1117%2f12.2015877&partnerID=40&md5=09c18a5f8068d00c3043cb2ace20c926","Ground surveillance is normally performed by human assets, since it requires visual intelligence. However, especially for military operations, this can be dangerous and is very resource intensive. Therefore, unmanned autonomous visualintelligence systems are desired. In this paper, we present an improved system that can recognize actions of a human and interactions between multiple humans. Central to the new system is our agent-based architecture. The system is trained on thousands of videos and evaluated on realistic persistent surveillance data in the DARPA Mind's Eye program, with hours of videos of challenging scenes. The results show that our system is able to track the people, detect and localize events, and discriminate between different behaviors, and it performs 3.4 times better than our previous system. © 2013 SPIE.","Action recognition; Artificial intelligence; Computer vision; Image retrieval; Visual intelligence","Action recognition; Agent-based architecture; Ground surveillance; Human assets; Human behaviors; Persistent surveillance; Visual intelligence; Artificial intelligence; Computer vision; Image retrieval; Military operations; National security; Sensors; Security systems",Conference Paper,Scopus,2-s2.0-84881058556
"Leme L.A.P.P., Lopes G.R., Nunes B.P., Casanova M.A., Dietze S.","Identifying candidate datasets for data interlinking",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,10.1007/978-3-642-39200-9_29,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880871939&doi=10.1007%2f978-3-642-39200-9_29&partnerID=40&md5=b4de69b0e986ef85efd65dcf5144c968","One of the design principles that can stimulate the growth and increase the usefulness of the Web of data is URIs linkage. However, the related URIs are typically in different datasets managed by different publishers. Hence, the designer of a new dataset must be aware of the existing datasets and inspect their content to define sameAs links. This paper proposes a technique based on probabilistic classifiers that, given a datasets S to be published and a set T of known published datasets, ranks each T i â̂̂ T according to the probability that links between S and T i can be found by inspecting the most relevant datasets. Results from our technique show that the search space can be reduced up to 85%, thereby greatly decreasing the computational effort. © 2013 Springer-Verlag Berlin Heidelberg.","Bayesian classifier; data interlinking; datasets recommendation; Linked Data","Bayesian classifier; Computational effort; Data interlinking; datasets recommendation; Design Principles; Linked datum; Probabilistic classifiers; Search spaces; Artificial intelligence; Computer science",Conference Paper,Scopus,2-s2.0-84880871939
"Mendizabal-Ruiz E.G., Rivera M., Kakadiaris I.A.","Segmentation of the luminal border in intravascular ultrasound B-mode images using a probabilistic approach",2013,"Medical Image Analysis",16,10.1016/j.media.2013.02.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878663347&doi=10.1016%2fj.media.2013.02.003&partnerID=40&md5=1d0a3fb2e5d4210d06fd7fcff96002b3","Intravascular ultrasound (IVUS) is a catheter-based medical imaging technique that produces cross-sectional images of blood vessels and is particularly useful for studying atherosclerosis. In this paper, we present a computational method for the delineation of the luminal border in IVUS B-mode images. The method is based in the minimization of a probabilistic cost function (that deforms a parametric curve) which defines a probability field that is regularized with respect to the given likelihoods of the pixels belonging to blood and non-blood. These likelihoods are obtained by a Support Vector Machine classifier trained using samples of the lumen and non-lumen regions provided by the user in the first frame of the sequence to be segmented. In addition, an optimization strategy is introduced in which the direction of the steepest descent and Broyden-Fletcher-Goldfarb-Shanno optimization methods are linearly combined to improve convergence. Our proposed method (MRK) is capable of segmenting IVUS B-mode images from different systems and transducer frequencies without the need of any parameter tuning, and it is robust with respect to changes of the B-mode reconstruction parameters which are subjectively adjusted by the interventionist. We validated the proposed method on six 20. MHz and six 40. MHz IVUS stationary sequences corresponding to regions with different degrees of stenosis, and evaluated its performance by comparing the segmentation results with manual segmentation by two observers. Furthermore, we compared our method with the segmentation results on the same sequences as provided by the authors of three other segmentation methods available in the literature. The performance of all methods was quantified using Dice and Jaccard similarity indexes, Hausdorff distance, linear regression and Bland-Altman analysis. The results indicate the advantages of our method for the segmentation of the lumen contour. © 2013 Elsevier B.V.","Contour parameterization; Coronary arteries; IVUS; Probabilistic segmentation; Ultrasound","Broyden-Fletcher-Goldfarb-Shanno; Coronary arteries; Intravascular ultrasound; IVUS; Optimization strategy; Probabilistic approaches; Reconstruction parameters; Support vector machine classifiers; Blood vessels; Medical imaging; Ultrasonics; Image segmentation; article; B scan; human; image analysis; image reconstruction; intravascular ultrasound; nonhuman; priority journal; probability; radiological parameters; support vector machine; transducer; Algorithms; Artificial Intelligence; Coronary Stenosis; Data Interpretation, Statistical; Echocardiography; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Ultrasonography, Interventional",Article,Scopus,2-s2.0-84878663347
"Demaine E.D., Patitz M.J., Rogers T.A., Schweller R.T., Summers S.M., Woods D.","The two-handed tile assembly model is not intrinsically universal",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,10.1007/978-3-642-39206-1_34,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880274791&doi=10.1007%2f978-3-642-39206-1_34&partnerID=40&md5=cb43fc89ce7470bcca341182b08380ec","In this paper, we study the intrinsic universality of the well-studied Two-Handed Tile Assembly Model (2HAM), in which two ""supertile"" assemblies, each consisting of one or more unit-square tiles, can fuse together (self-assemble) whenever their total attachment strength is at least the global temperature τ. Our main result is that for all τ′ < τ, each temperature-τ′ 2HAM tile system cannot simulate at least one temperature-τ 2HAM tile system. This impossibility result proves that the 2HAM is not intrinsically universal, in stark contrast to the simpler abstract Tile Assembly Model which was shown to be intrinsically universal (The tile assembly model is intrinsically universal, FOCS 2012). On the positive side, we prove that, for every fixed temperature τ ≥ 2, temperature-τ 2HAM tile systems are intrinsically universal: for each τ there is a single universal 2HAM tile set U that, when appropriately initialized, is capable of simulating the behavior of any temperature τ 2HAM tile system. As a corollary of these results we find an infinite set of infinite hierarchies of 2HAM systems with strictly increasing power within each hierarchy. Finally, we show how to construct, for each τ, a temperature-τ 2HAM system that simultaneously simulates all temperature-τ 2HAM systems. © 2013 Springer-Verlag.",,"Fixed temperature; Global temperatures; Impossibility results; Intrinsic universalities; Positive sides; Self-assemble; Tile assembly models; Artificial intelligence; Computer science; Automata theory",Conference Paper,Scopus,2-s2.0-84880274791
"San Pedro M.O.Z., Baker R.S.J.D., Gowda S.M., Heffernan N.T.","Towards an understanding of affect and knowledge from student interaction with an intelligent tutoring system",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,10.1007/978-3-642-39112-5-5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880004192&doi=10.1007%2f978-3-642-39112-5-5&partnerID=40&md5=cd9fdb751f8035d77eb7818fb72b5324","Csikszentmihalyi's Flow theory states that a balance between challenge and skill leads to high engagement, overwhelming challenge leads to anxiety or frustration, and insufficient challenge leads to boredom. In this paper, we test this theory within the context of student interaction with an intelligent tutoring system. Automated detectors of student affect and knowledge were developed, validated, and applied to a large data set. The results did not match Flow theory: boredom was more common for poorly-known material, and frustration was common both for very difficult material and very easy material. These results suggest that design for optimal engagement within online learning may require further study of the factors leading students to become bored on difficult material, and frustrated on very well-known material. © 2013 Springer-Verlag Berlin Heidelberg.","Affect Modeling; Boredom; Engaged Concentration; Frustration; Intelligent Tutoring System; Prior Knowledge","Affect modeling; Boredom; Frustration; Intelligent tutoring system; Prior knowledge; Artificial intelligence; Computer aided instruction; Education computing; Materials; Students",Conference Paper,Scopus,2-s2.0-84880004192
"Westerfield G., Mitrovic A., Billinghurst M.","Intelligent augmented reality training for assembly tasks",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,10.1007/978-3-642-39112-5-55,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879998051&doi=10.1007%2f978-3-642-39112-5-55&partnerID=40&md5=0d9166d641f8c0c8b2d43c71d68a3d1e","We investigate the combination of Augmented Reality (AR) with Intelligent Tutoring Systems (ITS) to assist with training for manual assembly tasks. Our approach combines AR graphics with adaptive guidance from the ITS to provide a more effective learning experience. We have developed a modular software framework for intelligent AR training systems, and a prototype based on this framework that teaches novice users how to assemble a computer motherboard. An evaluation found that our intelligent AR system improved test scores by 25% and that task performance was 30% faster compared to the same AR training system without intelligent support. We conclude that using intelligent AR tutor can significantly improve learning compared to traditional AR training. © 2013 Springer-Verlag Berlin Heidelberg.","Assembly skills; Augmented reality; Intelligent tutoring","Adaptive guidance; Computer motherboard; Effective learning; Intelligent support; Intelligent tutoring; Intelligent tutoring system; Modular softwares; Task performance; Artificial intelligence; Augmented reality; Computer aided instruction; Computer programming; Teaching; Personnel training",Conference Paper,Scopus,2-s2.0-84879998051
"Tarek B., Said D., Benbouzid M.E.H.","Maximum Power Point Tracking Control for Photovoltaic System Using Adaptive Neuro-Fuzzy 'ANFIS'",2013,"2013 8th International Conference and Exhibition on Ecological Vehicles and Renewable Energies, EVER 2013",16,10.1109/EVER.2013.6521559,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879964028&doi=10.1109%2fEVER.2013.6521559&partnerID=40&md5=f7d71a40724f11e1a1daf5ef2cad3f29","Due to scarcity of fossil fuel and increasing demand of power supply, we are forced to utilize the renewable energy resources. Considering easy availability and vast potential, world has turned to solar photovoltaic energy to meet out its ever increasing energy demand. The mathematical modeling and simulation of the photovoltaic system is implemented in the MATLAB/Simulink environment and the same thing is tested and validated using Artificial Intelligent (AI) like ANFIS. This paper presents Maximum Power Point Tracking Control for Photovoltaic System Using Adaptive Neuro- Fuzzy 'ANFIS'. The PV array has an optimum operating point to generate maximum power at some particular point called maximum power point (MPP). To track this maximum power point and to draw maximum power from PV arrays, MPPT controller is required in a stand-alone PV system. Due to the nonlinearity in the output characteristics of PV array, it is very much essential to track the MPPT of the PV array for varying maximum power point due to the insolation variation. In order to track the MPPT conventional controller like Adaptive Neuro-Fuzzy 'ANFIS' and fuzzy logic controller is proposed and simulated. The output of the controller, pulse generated from PWM can switch MOSFET to change the duty cycle of boost DC-DC converter. The result reveals that the maximum power point is tracked satisfactorily for varying insolation condition. © 2013 IEEE.","ANFIS; boost DC-DC; fuzzy logic; MPPT; Photovoltaic; Proportional Integral Controller; Pulse Width Modulation","ANFIS; boost DC-DC; MPPT; Photovoltaic; Proportional integral controllers; Artificial intelligence; Automobile exhibitions; Computer simulation; DC-DC converters; Ecology; Fuzzy logic; Incident solar radiation; Lead acid batteries; Navigation; Pulse width modulation; Renewable energy resources; Photovoltaic cells",Conference Paper,Scopus,2-s2.0-84879964028
"Pal S.K., Banerjee R., Dutta S., Sarma S.S.","An insight into the Z-number approach to CWW",2013,"Fundamenta Informaticae",16,10.3233/FI-2013-831,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879488278&doi=10.3233%2fFI-2013-831&partnerID=40&md5=5eeef2f307d5346ef2e214a1955ec7a4","The Z-number is a new fuzzy-theoretic concept, proposed by Zadeh in 2011. It extends the basic philosophy of Computing With Words (CWW) to include the perception of uncertainty of the information conveyed by a natural language statement. The Z-number thus, serves as a model of linguistic summarization of natural language statements, a technique to merge human-affective perspectives with CWW, and consequently can be envisaged to play a radical role in the domain of CWW-based system design and Natural Language Processing (NLP). This article presents a comprehensive investigation of the Z-number approach to CWW. We present here: a) an outline of our understanding of the generic architecture, algorithm and challenges underlying CWW in general; b) a detailed study of the Z-number methodology - where we propose an algorithm for CWW using Z-numbers, define a Z-number based operator for the evaluation of the level of requirement satisfaction, and describe simulation experiments of CWW utilizing Z-numbers; and c) analyse the strengths and the challenges of the Z-numbers, and suggest possible solution strategies. We believe that this article would inspire research on the need for inclusion of human-behavioural aspects into CWW, as well as the integration of CWW and NLP.","affective computing; Cognition; dialogue-based systems; fuzzy sets; linguistics; machine learning; natural computing; Natural Language Processing (NLP); perceptions; soft computing; text-summarization","Affective Computing; Cognition; Natural Computing; NAtural language processing; text-summarization; Artificial intelligence; Behavioral research; Fuzzy sets; Learning algorithms; Learning systems; Linguistics; Sensory perception; Soft computing; Natural language processing systems",Article,Scopus,2-s2.0-84879488278
"Johnson A.E.W., Kramer A.A., Clifford G.D.","A new severity of illness scale using a subset of acute physiology and chronic health evaluation data elements shows comparable predictive accuracy",2013,"Critical Care Medicine",16,10.1097/CCM.0b013e31828a24fe,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880558601&doi=10.1097%2fCCM.0b013e31828a24fe&partnerID=40&md5=9d9479b032a224d569164af29e9be390","OBJECTIVES:: Severity of illness scores have gained considerable interest for their use in predicting outcomes such as mortality and length of stay. The most sophisticated scoring systems require the collection of numerous physiologic measurements, making their use in real-time difficult. A severity of illness score based on a few parameters that can be captured electronically would be of great benefit. Using a machine-learning technique known as particle swarm optimization, we attempted to reduce the number of physiologic parameters collected in the Acute Physiology, Age, and Chronic Health Evaluation IV system without losing predictive accuracy. DESIGN:: Retrospective cohort study of ICU admissions from 2007 to 2011. SETTING:: Eighty-six ICUs at 49 U.S. hospitals where an Acute Physiology, Age, and Chronic Health Evaluation IV system had been installed. PATIENTS:: 81,087 admissions, of which 72,474 did not have any missing values. INTERVENTIONS:: None. MEASUREMENTS AND MAIN RESULTS:: Machine-learning algorithms were used to come up with the minimal set of variables that were capable of yielding an accurate severity of illness score: the Oxford Acute Severity of Illness Score. Predictive models of ICU mortality using Oxford Acute Severity of Illness Score were developed on admissions during 2007-2009 and validated on admissions during 2010-2011. The most parsimonious Oxford Acute Severity of Illness Score consisted of seven physiologic measurements, elective surgery, age, and prior length of stay. Predictive models of ICU mortality using Oxford Acute Severity of Illness Score achieved an area under the receiver operating characteristic curve of 0.88 and calibrated well. CONCLUSIONS:: A reduced severity of illness score had discrimination and calibration equivalent to more complex existing models. This was accomplished in large part using machine-learning algorithms, which can effectively account for the nonlinear associations between physiologic parameters and outcome. Copyright © 2013 by the Society of Critical Care Medicine and Lippincott.","APACHE; data mining; mortality; predictive models; severity of illness","accuracy; age; APACHE; article; critically ill patient; elective surgery; hospital admission; human; intensive care unit; learning algorithm; length of stay; machine learning; major clinical study; mortality; oxford acute severity of illness score; priority journal; receiver operating characteristic; scoring system; APACHE; Artificial Intelligence; Chronic Disease; Continental Population Groups; Female; Health Status Indicators; Humans; Intensive Care Units; Length of Stay; Male; Middle Aged; Retrospective Studies; ROC Curve; Sensitivity and Specificity",Article,Scopus,2-s2.0-84880558601
"Ganesan K., Acharya R.U., Chua C.K., Min L.C., Mathew B., Thomas A.K.","Decision support system for breast cancer detection using mammograms",2013,"Proceedings of the Institution of Mechanical Engineers, Part H: Journal of Engineering in Medicine",16,10.1177/0954411913480669,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884559302&doi=10.1177%2f0954411913480669&partnerID=40&md5=4e7c0108f9b0bce9cfe1db9b6bb0a7b0","Mammograms are by far one of the most preferred methods of screening for breast cancer. Early detection of breast cancer can improve survival rates to a greater extent. Although the analysis and diagnosis of breast cancer are done by experienced radiologists, there is always the possibility of human error. Interobserver and intraobserver errors occur frequently in the analysis of medical images, given the high variability between every patient. Also, the sensitivity of mammographic screening varies with image quality and expertise of the radiologist. So, there is no golden standard for the screening process. To offset this variability and to standardize the diagnostic procedures, efforts are being made to develop automated techniques for diagnosis and grading of breast cancer images. This article presents a classification pipeline to improve the accuracy of differentiation between normal, benign, and malignant mammograms. Several features based on higher-order spectra, local binary pattern, Laws' texture energy, and discrete wavelet transform were extracted from mammograms. Feature selection techniques based on sequential forward, backward, plus-l-takeaway-r, individual, and branch-and-bound selections using the Mahalanobis distance criterion were used to rank the features and find classification accuracies for combination of several features based on the ranking. Six classifiers were used, namely, decision tree classifier, fisher classifier, linear discriminant classifier, nearest mean classifier, Parzen classifier, and support vector machine classifier. We evaluated our proposed methodology with 300 mammograms obtained from the Digital Database for Screening Mammography and 300 mammograms from the Singapore Anti-Tuberculosis Association CommHealth database. Sensitivity, specificity, and accuracy values were used to compare the performances of the classifiers. Our results show that the decision tree classifier demonstrated an excellent performance compared to other classifiers with classification accuracy, sensitivity, and specificity of 91% for the Digital Database for Screening Mammography database and 96.8% for the Singapore Anti-Tuberculosis Association CommHealth database. © IMechE 2013.","Cancer; Classification; Feature selection; Mammogram; Texture","Cancer; Decision tree classifiers; Digital database for screening mammographies; Early detection of breast cancer; Linear discriminant classifier; Mammogram; Nearest mean classifiers; Support vector machine classifiers; Artificial intelligence; Data mining; Database systems; Decision support systems; Decision trees; Discrete wavelet transforms; Diseases; Errors; Feature extraction; Grading; Mammography; Medical imaging; Textures; X ray screens; Classification (of information); adult; aged; Breast Neoplasms; computer assisted diagnosis; decision support system; factual database; female; human; mammography; middle aged; pathology; procedures; radiography; support vector machine; article; breast tumor; classification; computer assisted diagnosis; feature selection; Mammogram; mammography; methodology; neoplasm; texture; Adult; Aged; Breast Neoplasms; Databases, Factual; Decision Support Systems, Clinical; Diagnosis, Computer-Assisted; Female; Humans; Image Interpretation, Computer-Assisted; Mammography; Middle Aged; Support Vector Machines; cancer; classification; feature selection; Mammogram; texture; Adult; Aged; Breast Neoplasms; Databases, Factual; Decision Support Systems, Clinical; Diagnosis, Computer-Assisted; Female; Humans; Image Interpretation, Computer-Assisted; Mammography; Middle Aged; Support Vector Machines",Article,Scopus,2-s2.0-84884559302
"McGregor C.","Big data in neonatal intensive care",2013,"Computer",16,10.1109/MC.2013.157,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879118481&doi=10.1109%2fMC.2013.157&partnerID=40&md5=f9171b6bd1b87780f0191ded4d669e8e","The effective use of big data within neonatal intensive care units has great potential to support a new wave of clinical discovery, leading to earlier detection and prevention of a wide range of deadly medical conditions. The Web extra at http://youtu.be/OIQBCboQs0g is a video in which author Carolyn McGregor expands on her article 'Big Data in Neonatal Intensive Care' and discusses how the effective use of big data within neonatal intensive care units has great potential to support a new wave of clinical discovery, leading to earlier detection and prevention of a wide range of deadly medical conditions. © 1970-2012 IEEE.","big data; clinical decision support systems; neonatal intensive care; online health analytics; physiological data","Big datum; Clinical decision support systems; Neonatal intensive care; online health analytics; Physiological data; Artificial intelligence; Decision support systems; Neonatal monitoring",Article,Scopus,2-s2.0-84879118481
"Moros J., Serrano J., Gallego F.J., Macías J., Laserna J.J.","Recognition of explosives fingerprints on objects for courier services using machine learning methods and laser-induced breakdown spectroscopy",2013,"Talanta",16,10.1016/j.talanta.2013.02.026,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886421291&doi=10.1016%2fj.talanta.2013.02.026&partnerID=40&md5=5114c7ff23fe1567d34e883c8d0481c3","During recent years laser-induced breakdown spectroscopy (LIBS) has been considered one of the techniques with larger ability for trace detection of explosives. However, despite of the high sensitivity exhibited for this application, LIBS suffers from a limited selectivity due to difficulties in assigning the molecular origin of the spectral emissions observed. This circumstance makes the recognition of fingerprints a latent challenging problem. In the present manuscript the sorting of six explosives (chloratite, ammonal, DNT, TNT, RDX and PETN) against a broad list of potential harmless interferents (butter, fuel oil, hand cream, olive oil, y), all of them in the form of fingerprints deposited on the surfaces of objects for courier services, has been carried out. When LIBS information is processed through a multi-stage architecture algorithm built from a suitable combination of 3 learning classifiers, an unknown fingerprint may be labeled into a particular class. Neural network classifiers trained by the Levenberg-Marquardt rule were decided within 3D scatter plots projected onto the subspace of the most useful features extracted from the LIBS spectra. Experimental results demonstrate that the presented algorithm sorts fingerprints according to their hazardous character, although its spectral information is virtually identical in appearance, with rates of false negatives and false positives not beyond of 10%. These reported achievements mean a step forward in the technology readiness level of LIBS for this complex application related to defense, homeland security and force protection. © 2013 Elsevier. B.V. All rights reserved.","Decision tree; Fingerprints; Harmless; Home-made explosives; LIBS; Machine learning","Artificial intelligence; Atomic emission spectroscopy; Classification (of information); Decision trees; Explosives detection; Laser induced breakdown spectroscopy; Learning systems; Olive oil; Palmprint recognition; Pattern recognition; Trace analysis; Fingerprints; Harmless; Laserinduced breakdown spectroscopy (LIBS); LIBS; Machine learning methods; Neural network classifier; Spectral information; Technology readiness levels; Explosives; explosive; article; artificial intelligence; decision tree; methodology; spectroscopy; Artificial Intelligence; Decision Trees; Explosive Agents; Spectrum Analysis",Article,Scopus,2-s2.0-84886421291
"Xu S., Feng D., Yan Z., Zhang L., Li N., Jing L., Wang J.","Ant-based swarm algorithm for charging coordination of electric vehicles",2013,"International Journal of Distributed Sensor Networks",16,10.1155/2013/268942,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878663221&doi=10.1155%2f2013%2f268942&partnerID=40&md5=b800e3e27b4f2fa74f3784d362c96147","Uncontrolled charging of large-scale electric vehicles (EVs) can affect the safe and economic operation of power systems, especially at the distribution level. The centralized EVs charging optimization methods require complete information of physical appliances and using habits, which will cause problems of high dimensionality and communication block. Given this, an ant-based swarm algorithm (ASA) is proposed to realize the EVs charging coordination at the transformer level, which can overcome the drawbacks of centralized control method. First, the EV charging load model is developed, and the charging management structure based on swarm intelligence is presented. Second, basic data of the EV using habit is sampled by the Monte Carlo method, and the ASA is applied to realize the load valley filling. The load fluctuation and the transformer capacity are also considered in the algorithm. Finally, the charging coordination of 500 EVs under a 12.47 KV transformer is simulated to demonstrate the validity of the proposed method. © 2013 Shaolun Xu et al.",,"Centralized control; Charging managements; Complete information; Electric Vehicles (EVs); High dimensionality; Optimization method; Transformer capacity; Uncontrolled charging; Artificial intelligence; Electric power systems; Electric vehicles; Monte Carlo methods; Algorithms",Article,Scopus,2-s2.0-84878663221
"Ben Romdhane L., Chaabani Y., Zardi H.","A robust ant colony optimization-based algorithm for community mining in large scale oriented social graphs",2013,"Expert Systems with Applications",16,10.1016/j.eswa.2013.04.021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878291778&doi=10.1016%2fj.eswa.2013.04.021&partnerID=40&md5=8a528e35a23af9a670208e6e8085bbc9","Community detection plays a key role in such important fields as biology, sociology and computer science. For example, detecting the communities in protein-protein interactions networks helps in understanding their functionalities. Most existing approaches were devoted to community mining in undirected social networks (either weighted or not). In fact, despite their ubiquity, few proposals were interested in community detection in oriented social networks. For example, in a friendship network, the influence between individuals could be asymmetric; in a networked environment, the flow of information could be unidirectional. In this paper, we propose an algorithm, called ACODIG, for community detection in oriented social networks. ACODIG uses an objective function based on measures of density and purity and incorporates the information about edge orientations in the social graph. ACODIG uses ant colony for its optimization. Simulation results on real-world as well as power law artificial benchmark networks reveal a good robustness of ACODIG and an efficiency in computing the real structure of the network. © 2013 Elsevier Ltd. All rights reserved.","Ant colony optimization; Community detection; NP-complete; Social networks","Community detection; Friendship networks; Networked environments; NP Complete; Objective functions; Optimization-based algorithm; Protein-protein interactions; Social Networks; Ant colony optimization; Artificial intelligence; Population dynamics; Proteins; Social networking (online); Algorithms",Article,Scopus,2-s2.0-84878291778
"Hu X.-B., Wang M., Di Paolo E.","Calculating complete and exact pareto front for multiobjective optimization: A new deterministic approach for discrete problems",2013,"IEEE Transactions on Cybernetics",16,10.1109/TSMCB.2012.2223756,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890426388&doi=10.1109%2fTSMCB.2012.2223756&partnerID=40&md5=33f1a7833535e854a2e1a8f5f71e50a9","Searching the Pareto front for multiobjective optimization problems usually involves the use of a population-based search algorithm or of a deterministic method with a set of different single aggregate objective functions. The results are, in fact, only approximations of the real Pareto front. In this paper, we propose a new deterministic approach capable of fully determining the real Pareto front for those discrete problems for which it is possible to construct optimization algorithms to find the k best solutions to each of the single-objective problems. To this end, two theoretical conditions are given to guarantee the finding of the actual Pareto front rather than its approximation. Then, a general methodology for designing a deterministic search procedure is proposed. A case study is conducted, where by following the general methodology, a ripple-spreading algorithm is designed to calculate the complete exact Pareto front for multiobjective route optimization. When compared with traditional Pareto front search methods, the obvious advantage of the proposed approach is its unique capability of finding the complete Pareto front. This is illustrated by the simulation results in terms of both solution quality and computational efficiency. © 2012 IEEE.","Multiobjective optimization; Pareto front; Ripple-spreading algorithm; Route optimization","Deterministic approach; Deterministic methods; General methodologies; Multi-objective optimization problem; Objective functions; Optimization algorithms; Pareto front; Route optimization; Algorithms; Multiobjective optimization; Pareto principle; algorithm; artificial intelligence; automated pattern recognition; decision support system; game; procedures; signal processing; article; automated pattern recognition; methodology; Algorithms; Artificial Intelligence; Decision Support Techniques; Game Theory; Pattern Recognition, Automated; Signal Processing, Computer-Assisted; Algorithms; Artificial Intelligence; Decision Support Techniques; Game Theory; Pattern Recognition, Automated; Signal Processing, Computer-Assisted",Article,Scopus,2-s2.0-84890426388
"Sallem A., Benhala B., Kotti M., Fakhfakh M., Ahaitouf A., Loulou M.","Application of swarm intelligence techniques to the design of analog circuits: Evaluation and comparison",2013,"Analog Integrated Circuits and Signal Processing",16,10.1007/s10470-013-0054-6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878111187&doi=10.1007%2fs10470-013-0054-6&partnerID=40&md5=a44f3ead249cfb1a0c77ed97c60e5a7e","Swarm intelligence (SI) techniques are more and more used by analog designers in order to optimally size their circuits/systems' performances. A particular interest is accorded to the multi-objective algorithms due to the fact that in most cases analog, mixed signal and radio-frequency sizing problems encompass at least two non-commensurable conflicting objectives. In most of the published papers, Pareto fronts are provided, and in the best cases they are compared to the tradeoff fronts obtained using other classical metaheuristics, but in a very subjective way. In this paper we present a comparison that deals with the multi-objective optimal design of analog circuits via the SI technique and other famous metaheuristics. Performance metrics are used to compare the obtained results. The paper argues and shows that SI techniques and particularly the particle swarm optimization technique is a priori the most adequate metaheuristic to use in the field of analog circuit sizing. © 2013 Springer Science+Business Media New York.","Analog circuit; C-metric; Hyper-volume indicator; Metaheuristics; Multiobjective optimization; Pareto front; Swarm intelligence","C-metric; Conflicting objectives; Meta heuristics; Multi objective algorithm; Pareto front; Particle swarm optimization technique; Swarm Intelligence; Swarm intelligence techniques; Artificial intelligence; Heuristic algorithms; Multiobjective optimization; Analog circuits",Article,Scopus,2-s2.0-84878111187
"Bremer J., Sonnenschein M.","Constraint-handling for optimization with support vector surrogate models: A novel decoder approach",2013,"ICAART 2013 - Proceedings of the 5th International Conference on Agents and Artificial Intelligence",16,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877965611&partnerID=40&md5=c537e2077f764bf7b6d1f47c5ea5e155","A new application for support vector machines is their use for meta-modeling feasible regions in constrained optimization problems. We here describe a solution for the still unsolved problem of a standardized integration of such models into (evolutionary) optimization algorithms with the help of a new decoder based approach. This goal is achieved by constructing a mapping function that maps the whole unconstrained domain of a given problem to the region of feasible solutions with the help of the the support vector model. The applicability to real world problems is demonstrated using the load balancing problem from the smart grid domain.","Constraint modeling; Evolutionary optimization; Smart grid; Soft computing; SVDD","Constrained optimi-zation problems; Constraint handling; Constraint modeling; Evolutionary optimizations; Load balancing problem; Optimization algorithms; Smart grid; SVDD; Algorithms; Artificial intelligence; Smart power grids; Soft computing; Constrained optimization",Conference Paper,Scopus,2-s2.0-84877965611
"Li Y., Zhang X.","Web-based construction waste estimation system for building construction projects",2013,"Automation in Construction",16,10.1016/j.autcon.2013.05.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884503509&doi=10.1016%2fj.autcon.2013.05.002&partnerID=40&md5=e7686a3c9436c17a281caa8ce8fab16a","This paper proposes a web-based construction waste estimation system (WCWES) for building construction projects incorporating the concepts of work breakdown structure, material quantity takeoff, material classification, material conversion ratios, material wastage levels, and the mass balance principle. The WCWES integrates online data input modules and online analytical modules for the quantification of different kinds of waste generated in the construction process at the project level. It facilitates accessibility, interfacing, connectivity and information sharing of users in carrying out a wide range of construction waste estimation tasks for sustainable construction waste management. A hypothetical building construction project is used to demonstrate the application and usefulness of the WCWES. © 2013 Elsevier B.V.","Construction waste; Decision support system; Estimation; Online analytical processing; Sustainable development","Building construction projects; Construction process; Construction wastes; Material classification; Material conversions; On-line analytical processing; Sustainable construction; Work breakdown structure; Artificial intelligence; Decision support systems; Estimation; Sustainable development; Websites; Construction",Article,Scopus,2-s2.0-84884503509
"Bodduluri S., Newell J.D., Hoffman E.A., Reinhardt J.M.","Registration-based lung mechanical analysis of chronic obstructive pulmonary disease (COPD) using a supervised machine learning framework",2013,"Academic Radiology",16,10.1016/j.acra.2013.01.019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875896927&doi=10.1016%2fj.acra.2013.01.019&partnerID=40&md5=e19090c8a30f8666e886ec0c2e43e36b","Rationale and Objectives: This study evaluated the performance of computed tomography (CT)-derived biomechanical based features of lung function and the presence and severity of chronic obstructive pulmonary disease (COPD). It performed well when compared to CT-derived density and textural features of lung function and the presence and severity of COPD. Materials and Methods: A total of 162 subjects (Global Initiative for Chronic Obstructive Lung Disease [GOLD] stages 0-4 and nonsmokers) subjects with CT scan performed at total lung capacity or expiration to functional residual capacity were evaluated. CT-derived biomechanical, density, and textural feature sets were compared to forced expiratory volume in 1 second (FEV1)%, FEV1/forced vital capacity, and total St. George's respiratory questionnaire scores. The ability of these feature sets to assess the presence and severity of COPD was also evaluated. Optimal features are selected by linear forward feature selection and the classification is done using k nearest neighbor learning algorithm. Results: The proposed biomechanical features showed good correlations with the pulmonary function tests and health status metrics. In COPD versus non-COPD classification, biomechanical feature set achieved an area under the curve (AUC) of 0.85 performing well in comparison to density (AUC = 0.83) and texture (AUC = 0.89) feature sets. Classifying the subjects into the severity of GOLD stage using biomechanical features (AUC = 0.81) performed better than the density- and texture-based feature sets, AUC = 0.76 and 0.73, respectively. The biomechanical features performed better alone than in combination with the other two feature sets. Conclusion: This study shows the effectiveness of CT-derived biomechanical measures in the assessment of airflow obstruction and quality of life in subjects with COPD. CT-derived biomechanical features performed well in assessing the presence and severity of COPD. © 2013 AUR.","CAD; COPD; Lung; Mechanics; Registration","aged; article; biomechanics; chronic obstructive lung disease; controlled study; female; forced expiratory volume; forced vital capacity; functional residual capacity; human; k nearest neighbor; learning algorithm; lung function test; lung mechanics; major clinical study; male; priority journal; quality of life; questionnaire; total lung capacity; Aged; Algorithms; Artificial Intelligence; Female; Humans; Lung; Male; Pulmonary Disease, Chronic Obstructive; Radiographic Image Interpretation, Computer-Assisted; Reproducibility of Results; Respiratory Function Tests; Respiratory Mechanics; Sensitivity and Specificity; Subtraction Technique; Tomography, X-Ray Computed",Article,Scopus,2-s2.0-84875896927
"Kattan A., Abdullah R.","A dynamic self-adaptive harmony search algorithm for continuous optimization problems",2013,"Applied Mathematics and Computation",16,10.1016/j.amc.2013.02.074,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876157643&doi=10.1016%2fj.amc.2013.02.074&partnerID=40&md5=f4a33eb52c08245e6e3a379b2424d5d5","In solving global optimization problems for continuous functions, researchers often rely on metaheuristic algorithms to overcome the computational drawbacks of the existing numerical methods. A metaheuristic is an evolutionary algorithm that does not require the functions in the problem to satisfy specific conditions or mathematical properties. A recently proposed metaheuristic is the harmony search algorithm, which was inspired by the music improvisation process and has been applied successfully in the solution of various global optimization problems. However, the overall performance of this algorithm and its convergence properties are quite sensitive to the initial parameter settings. Several improvements of the harmony search algorithm have been proposed to incorporate self-adaptive features. In these modified versions of the algorithm, the parameters are automatically tuned during the optimization process to achieve superior results. This paper proposes a new dynamic and self-adaptive harmony search algorithm in which two of the optimization parameters, the pitch adjustment rate and the bandwidth, are auto-tuned. These two parameters have substantial influence on the quality of the final solution. The proposed algorithm utilizes two new quality measures to dynamically drive the optimization process: the current best-to-worst ratio of the harmony memory fitness function and the improvisation acceptance rate. The key difference between the proposed algorithm and most competing methods is that the values of the pitch adjustment rate and bandwidth are determined independently of the current improvisation count and therefore vary dynamically rather than monotonically. The results demonstrate the superiority of the proposed algorithm over various other recent methods based on several common benchmarking functions. © 2013 Elsevier Inc. All rights reserved.","Computational intelligence; Evolutionary algorithms; Metaheuristic","Benchmarking functions; Continuous optimization problems; Global optimization problems; Harmony search algorithms; Mathematical properties; Meta heuristic algorithm; Metaheuristic; Optimization parameter; Artificial intelligence; Bandwidth; Evolutionary algorithms; Functions; Global optimization; Learning algorithms; Optimization; Parameter estimation; Problem solving; Computer music",Article,Scopus,2-s2.0-84876157643
"Hardy J., Campbell M.","Contingency planning over probabilistic obstacle predictions for autonomous road vehicles",2013,"IEEE Transactions on Robotics",16,10.1109/TRO.2013.2254033,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882451183&doi=10.1109%2fTRO.2013.2254033&partnerID=40&md5=ea76ab7b45b3a213d9cca2557785d18c","This paper presents a novel optimization-based path planner that is capable of planning multiple contingency paths to directly account for uncertainties in the future trajectories of dynamic obstacles. This planner addresses the particular problem of probabilistic collision avoidance for autonomous road vehicles that are required to safely interact, in close proximity, with other vehicles with unknown intentions. The presented path planner utilizes an efficient spline-based trajectory representation and fast but accurate collision probability bounds to simultaneously optimize multiple continuous contingency paths in real time. These collision probability bounds are efficient enough for real-time evaluation, yet accurate enough to allow for practical close-proximity driving behaviors such as passing an obstacle vehicle in an adjacent lane. An obstacle trajectory clustering algorithm is also presented to enable the path planner to scale to multiple-obstacle scenarios. Simulation results show that the contingency planner allows for a more aggressive driving style than planning a single path without compromising the overall safety of the robot. © 2004-2012 IEEE.","Artificial intelligence reasoning methods; collision avoidance; contingency planning; field robots; nonholonomic motion planning","Aggressive driving; Collision probability; Contingency planning; Field robot; Multiple contingencies; Nonholonomic motion planning; Reasoning methods; Trajectory clustering; Artificial intelligence; Clustering algorithms; Collision avoidance; Motion planning; Optimization; Roads and streets; Trajectories; Transport properties; Vehicles; Robot programming",Article,Scopus,2-s2.0-84882451183
"Yang J., Zeng X., Zhong S., Wu S.","Effective neural network ensemble approach for improving generalization performance",2013,"IEEE Transactions on Neural Networks and Learning Systems",16,10.1109/TNNLS.2013.2246578,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876101027&doi=10.1109%2fTNNLS.2013.2246578&partnerID=40&md5=7734cf763d6a17db478d269c86b66567","This paper, with an aim at improving neural networks' generalization performance, proposes an effective neural network ensemble approach with two novel ideas. One is to apply neural networks' output sensitivity as a measure to evaluate neural networks' output diversity at the inputs near training samples so as to be able to select diverse individuals from a pool of well-trained neural networks; the other is to employ a learning mechanism to assign complementary weights for the combination of the selected individuals. Experimental results show that the proposed approach could construct a neural network ensemble with better generalization performance than that of each individual in the ensemble combining with all the other individuals, and than that of the ensembles with simply averaged weights. © 2012 IEEE.","Diversity ensemble learning; fusion; neural network ensemble; sensitivity","Ensemble learning; Generalization performance; Learning mechanism; Neural network ensembles; sensitivity; Training sample; Artificial intelligence; Computer networks; Fusion reactions; Neural networks; algorithm; artificial neural network; computer simulation; human; learning; signal processing; Algorithms; Computer Simulation; Humans; Learning; Neural Networks (Computer); Signal Processing, Computer-Assisted",Article,Scopus,2-s2.0-84876101027
"Ahlswede R., Csiszár I.","On oblivious transfer capacity",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,10.1007/978-3-642-36899-8-6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875984557&doi=10.1007%2f978-3-642-36899-8-6&partnerID=40&md5=c2552dfac1c1131a4abd38580bceb92e","Upper and lower bounds to the oblivious transfer (OT) capacity of discrete memoryless channels and multiple sources are obtained, for 1 of 2 strings OT with honest but curious participants. The upper bounds hold also for one-string OT. The results provide the exact value of OT capacity for a specified class of models, and the necessary and sufficient condition of its positivity, in general. © Springer-Verlag Berlin Heidelberg 2013.","entropy difference bound; generalized erasure channel; honest but curious; oblivios transfer; one of two strings; secret key; wiretap channel","Entropy differences; Erasure channels; honest but curious; oblivios transfer; one of two strings; Secret key; Wire-tap channels; Artificial intelligence; Communication channels (information theory)",Conference Paper,Scopus,2-s2.0-84875984557
"Di Nuovo A.G., Marocco D., Di Nuovo S., Cangelosi A.","Autonomous learning in humanoid robotics through mental imagery",2013,"Neural Networks",16,10.1016/j.neunet.2012.09.019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875885766&doi=10.1016%2fj.neunet.2012.09.019&partnerID=40&md5=1b8cd55e82c4adfc15366d68f6592bea","In this paper we focus on modeling autonomous learning to improve performance of a humanoid robot through a modular artificial neural networks architecture. A model of a neural controller is presented, which allows a humanoid robot iCub to autonomously improve its sensorimotor skills. This is achieved by endowing the neural controller with a secondary neural system that, by exploiting the sensorimotor skills already acquired by the robot, is able to generate additional imaginary examples that can be used by the controller itself to improve the performance through a simulated mental training. Results and analysis presented in the paper provide evidence of the viability of the approach proposed and help to clarify the rational behind the chosen model and its implementation. © 2012 Elsevier Ltd.","Autonomous learning; Ballistic movement; Mental imagery; Mental training; Recurrent neural networks","Autonomous learning; Ballistic movements; Humanoid robotics; Improve performance; Mental imagery; Mental trainings; Modular artificial neural networks; Neural controller; Anthropomorphic robots; Recurrent neural networks; Personnel training; article; artificial intelligence; artificial neural network; autonomous learning; control system; human; imagery; learning; mental capacity; mental imagery; motor control; nonbiological model; positive feedback; priority journal; psychomotor performance; robotics; sensorimotor function; Artificial Intelligence; Feedback; Humans; Imagination; Models, Theoretical; Motor Skills; Movement; Neural Networks (Computer); Perception; Robotics",Article,Scopus,2-s2.0-84875885766
"García-Valdez M., Trujillo L., Fernández De Vega F., Merelo Guervós J.J., Olague G.","EvoSpace: A distributed evolutionary platform based on the tuple space model",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,10.1007/978-3-642-37192-9-50,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875683692&doi=10.1007%2f978-3-642-37192-9-50&partnerID=40&md5=38ab90fe6a2cc1229ec7a63644becff6","This paper presents EvoSpace, a Cloud service for the development of distributed evolutionary algorithms. EvoSpace is based on the tuple space model, an associatively addressed memory space shared by several processes. Remote clients, called EvoWorkers, connect to EvoSpace and periodically take a subset of individuals from the global population, perform evolutionary operations on them, and return a set of new individuals. Several EvoWorkers carry out the evolutionary search in parallel and asynchronously, interacting with each other through the central repository. EvoSpace is designed to be domain independent and flexible, in the sense that in can be used with different types of evolutionary algorithms and applications. In this paper, a genetic algorithm is tested on the EvoSpace platform using a well-known benchmark problem, achieving promising results compared to a standard evolutionary system. © Springer-Verlag Berlin Heidelberg 2013.","Cloud Computing; Distributed Evolutionary Algorithms; Tuple Space","Bench-mark problems; Distributed evolutionary algorithms; Domain independents; Evolutionary operations; Evolutionary search; Evolutionary system; Global population; Tuple space; Artificial intelligence; Cloud computing; Evolutionary algorithms",Conference Paper,Scopus,2-s2.0-84875683692
"Li Y., Wang G., Chen H., Shi L., Qin L.","An Ant Colony Optimization Based Dimension Reduction Method for High-Dimensional Datasets",2013,"Journal of Bionic Engineering",16,10.1016/S1672-6529(13)60219-X,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876724768&doi=10.1016%2fS1672-6529%2813%2960219-X&partnerID=40&md5=d4300fcadd0f86a4292cfe4b41219b14","In this paper, a bionic optimization algorithm based dimension reduction method named Ant Colony Optimization -Selection (ACO-S) is proposed for high-dimensional datasets. Because microarray datasets comprise tens of thousands of features (genes), they are usually used to test the dimension reduction techniques. ACO-S consists of two stages in which two well-known ACO algorithms, namely ant system and ant colony system, are utilized to seek for genes, respectively. In the first stage, a modified ant system is used to filter the nonsignificant genes from high-dimensional space, and a number of promising genes are reserved in the next step. In the second stage, an improved ant colony system is applied to gene selection. In order to enhance the search ability of ACOs, we propose a method for calculating priori available heuristic information and design a fuzzy logic controller to dynamically adjust the number of ants in ant colony system. Furthermore, we devise another fuzzy logic controller to tune the parameter (q0) in ant colony system. We evaluate the performance of ACO-S on five microarray datasets, which have dimensions varying from 7129 to 12000. We also compare the performance of ACO-S with the results obtained from four existing well-known bionic optimization algorithms. The comparison results show that ACO-S has a notable ability to generate a gene subset with the smallest size and salient features while yielding high classification accuracy. The comparative results generated by ACO-S adopting different classifiers are also given. The proposed method is shown to be a promising and effective tool for mining high-dimension data and mobile robot navigation. © 2013 Jilin University.","Ant colony optimization; Feature selection; Gene selection; High-dimensional data","Classification accuracy; Dimension reduction method; Dimension reduction techniques; Fuzzy logic controllers; Gene selection; High dimensional data; High dimensional spaces; Improved ant colony systems; Ant colony optimization; Artificial intelligence; Constrained optimization; Data reduction; Feature extraction; Filtration; Fuzzy logic; Genes; Heuristic methods; Search engines; Algorithms",Article,Scopus,2-s2.0-84876724768
"Giguère S., Marchand M., Laviolette F., Drouin A., Corbeil J.","Learning a peptide-protein binding affinity predictor with kernel ridge regression",2013,"BMC Bioinformatics",16,10.1186/1471-2105-14-82,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877639844&doi=10.1186%2f1471-2105-14-82&partnerID=40&md5=1040760db377b785adad853ba46ab7d2","Background: The cellular function of a vast majority of proteins is performed through physical interactions with other biomolecules, which, most of the time, are other proteins. Peptides represent templates of choice for mimicking a secondary structure in order to modulate protein-protein interaction. They are thus an interesting class of therapeutics since they also display strong activity, high selectivity, low toxicity and few drug-drug interactions. Furthermore, predicting peptides that would bind to a specific MHC alleles would be of tremendous benefit to improve vaccine based therapy and possibly generate antibodies with greater affinity. Modern computational methods have the potential to accelerate and lower the cost of drug and vaccine discovery by selecting potential compounds for testing in silico prior to biological validation.Results: We propose a specialized string kernel for small bio-molecules, peptides and pseudo-sequences of binding interfaces. The kernel incorporates physico-chemical properties of amino acids and elegantly generalizes eight kernels, comprised of the Oligo, the Weighted Degree, the Blended Spectrum, and the Radial Basis Function. We provide a low complexity dynamic programming algorithm for the exact computation of the kernel and a linear time algorithm for it's approximation. Combined with kernel ridge regression and SupCK, a novel binding pocket kernel, the proposed kernel yields biologically relevant and good prediction accuracy on the PepX database. For the first time, a machine learning predictor is capable of predicting the binding affinity of any peptide to any protein with reasonable accuracy. The method was also applied to both single-target and pan-specific Major Histocompatibility Complex class II benchmark datasets and three Quantitative Structure Affinity Model benchmark datasets.Conclusion: On all benchmarks, our method significantly (p-value ≤ 0.057) outperforms the current state-of-the-art methods at predicting peptide-protein binding affinities. The proposed approach is flexible and can be applied to predict any quantitative biological activity. Moreover, generating reliable peptide-protein binding affinities will also improve system biology modelling of interaction pathways. Lastly, the method should be of value to a large segment of the research community with the potential to accelerate the discovery of peptide-based drugs and facilitate vaccine development. The proposed kernel is freely available at http://graal.ift.ulaval.ca/downloads/gs-kernel/. © 2013 Giguère et al.; licensee BioMed Central Ltd.",,"Dynamic programming algorithm; Kernel ridge regressions; Linear-time algorithms; Major histocompatibility complex class; Physico-chemical properties of amino acids; Protein-protein interactions; Quantitative structures; State-of-the-art methods; Amino acids; Approximation algorithms; Binding energy; Bioactivity; Chemical properties; Computational methods; Drug interactions; Forecasting; Molecular biology; Radial basis function networks; Regression analysis; Vaccines; Peptides; HLA antigen class 2; peptide; algorithm; allele; article; artificial intelligence; binding site; chemistry; computer simulation; genetics; immunology; metabolism; methodology; protein analysis; protein domain; Algorithms; Alleles; Artificial Intelligence; Binding Sites; Computer Simulation; Histocompatibility Antigens Class II; Peptides; Protein Interaction Domains and Motifs; Protein Interaction Mapping",Article,Scopus,2-s2.0-84877639844
"Jordt A., Koch R.","Direct model-based tracking of 3D object deformations in depth and color video",2013,"International Journal of Computer Vision",16,10.1007/s11263-012-0572-1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874649143&doi=10.1007%2fs11263-012-0572-1&partnerID=40&md5=be9bd4c2477ce7b85fc84d4240e44acf","The tracking of deformable objects using video data is a demanding research topic due to the inherent ambiguity problems, which can only be solved using additional assumptions about the deformation. Image feature points, commonly used to approach the deformation problem, only provide sparse information about the scene at hand. In this paper a tracking approach for deformable objects in color and depth video is introduced that does not rely on feature points or optical flow data but employs all the input image information available to find a suitable deformation for the data at hand. A versatile NURBS based deformation space is defined for arbitrary complex triangle meshes, decoupling the object surface complexity from the complexity of the deformation. An efficient optimization scheme is introduced that is able to calculate results in real-time (25 Hz). Extensive synthetic and real data tests of the algorithm and its features show the reliability of this approach. © 2012 Springer Science+Business Media, LLC.","Deformation; Range video; Tracking","3D object; Color video; Complex triangles; Deformable object; Deformation problems; Deformation space; Depth videos; Image feature points; Input image; Model-based tracking; Object surface; Optimization scheme; Range video; Research topics; Synthetic and real data; Tracking approaches; Video data; Artificial intelligence; Software engineering; Surface discharges; Deformation",Article,Scopus,2-s2.0-84874649143
"Kelling S., Lagoze C., Wong W.-K., Yu J., Damoulas T., Gerbracht J., Fink D., Gomes C.","E Bird: A human/computer learning network to improve biodiversity conservation and research",2013,"AI Magazine",16,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876183480&partnerID=40&md5=0d3f3bf85f8b5336b541513096b92a8a","EBird is a citizen-science project that takes advantage of the human observational capacity to identify birds to species, and uses these observations to accurately represent patterns of bird occurrences across broad spatial and temporal extents. eBird employs artificial intelligence techniques such as machine learning to improve data quality by taking advantage of the synergies between human computation and mechanical computation. We call this a human/computer learning network, whose core is an active learning feedback loop between humans and machines that dramatically improves the quality of both and thereby continually improves the effectiveness of the network as a whole. In this article we explore how human/computer learning networks can leverage the contributions of human observers and process their contributed data with artificial intelligence algorithms leading to a computational power that far exceeds the sum of the individual parts. Copyright © 2013, Association for the Advancement of Artificial Intelligence.",,"Artificial intelligence algorithms; Artificial intelligence techniques; Biodiversity conservation; Computational power; Human computation; Human observers; Learning network; Mechanical computations; Artificial intelligence; Conservation; Learning systems",Conference Paper,Scopus,2-s2.0-84876183480
"Fink M., Haverkort H., Nöllenburg M., Roberts M., Schuhmann J., Wolff A.","Drawing metro maps using Bézier curves",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,10.1007/978-3-642-36763-2_41,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874167176&doi=10.1007%2f978-3-642-36763-2_41&partnerID=40&md5=b8b21f8f624cddbf91cb6eff04f415f8","The automatic layout of metro maps has been investigated quite intensely over the last few years. Previous work has focused on the octilinear drawing style where edges are drawn horizontally, vertically, or diagonally at 45°. Inspired by manually created curvy metro maps, we advocate the use of the curvilinear drawing style; we draw edges as Bézier curves. Since we forbid metro lines to bend (even in stations), the user of such a map can trace the metro lines easily. In order to create such drawings, we use the force-directed framework. Our method is the first that directly represents and operates on edges as curves. © 2013 Springer-Verlag.",,"Automatic layout; Drawing styles; Force-Directed; Metro lines; Metro maps; Artificial intelligence; Drawing (graphics)",Conference Paper,Scopus,2-s2.0-84874167176
"Choi S.G., Katz J., Wee H., Zhou H.-S.","Efficient, adaptively secure, and composable oblivious transfer with a single, global CRS",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,10.1007/978-3-642-36362-7_6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873980614&doi=10.1007%2f978-3-642-36362-7_6&partnerID=40&md5=3943667352127ff5dcd8af6efb3cf9a9","We present a general framework for efficient, universally composable oblivious transfer (OT) protocols in which a single, global, common reference string (CRS) can be used for multiple invocations of oblivious transfer by arbitrary pairs of parties. In addition: - Our framework is round-efficient. E.g., under the DLIN or SXDH assumptions we achieve round-optimal protocols with static security, or 3-round protocols with adaptive security (assuming erasure). - Our resulting protocols are more efficient than any known previously, and in particular yield protocols for string OT using O(1) exponentiations and communicating O(1) group elements. Our result improves on that of Peikert et al. (Crypto 2008), which uses a CRS whose length depends on the number of parties in the network and achieves only static security. Compared to Garay et al. (Crypto 2009), we achieve adaptive security with better round complexity and efficiency. © 2013 International Association for Cryptologic Research.",,"Adaptive security; Common reference string; Composable; Exponentiations; Oblivious transfer; Round complexity; Static security; Universally composable; Artificial intelligence; Public key cryptography",Conference Paper,Scopus,2-s2.0-84873980614
"Wilbik A., Keller J.M.","A fuzzy measure similarity between sets of linguistic summaries",2013,"IEEE Transactions on Fuzzy Systems",16,10.1109/TFUZZ.2012.2214225,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873358272&doi=10.1109%2fTFUZZ.2012.2214225&partnerID=40&md5=233b21fcd826ab52c2c622cf527ff97c","In this paper, we consider the problem of evaluating the similarity of two sets of linguistic summaries of sensor data. Huge amounts of available data cause a dramatic need for summarization. In continuous monitoring, it is useful to compare one time interval of data with another, for example, to detect anomalies or to predict the onset of a change from a normal state. Assuming that summaries capture the essence of the data, it is sufficient to compare only those summaries, i.e., they are descriptive features for recognition. In previous work, we developed a similarity measure between two individual summaries and proved that the associated dissimilarity is a metric. Additionally, we proposed some basic methods to combine these similarities into an aggregate value. Here, we develop a novel parameter free method, which is based on fuzzy measures and integrals, to fuse individual similarities that will produce a closeness measurement between sets of summaries. We provide a case study from the eldercare domain where the goal is to compare different nighttime patterns for change detection. The reasons for studying linguistic summaries for eldercare are twofold: First, linguistic summaries are the natural communication tool for health care providers in a decision support system, and second, due to the extremely large volume of raw data, these summaries create compact features for an automated reasoning for detection and prediction of health changes as part of the decision support system. © 2012 IEEE.","Anomaly detection; fuzzy measure; linguistic summaries; similarity; Sugeno integral","Anomaly detection; Fuzzy measures; Linguistic summaries; similarity; Sugeno integrals; Artificial intelligence; Decision support systems; Fuzzy systems; Linguistics",Article,Scopus,2-s2.0-84873358272
"Capote J.A., Alvear D., Abreu O., Cuesta A., Alonso V.","A real-time stochastic evacuation model for road tunnels",2013,"Safety Science",16,10.1016/j.ssci.2012.02.006,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868096075&doi=10.1016%2fj.ssci.2012.02.006&partnerID=40&md5=bbfba3ca347d71127b41bd6be839fc21","Traditionally most of egress models have been mainly used for performance-base assessment and forensic analyses. But their use has been extended to other applications such as real-time mode. In this paper we present EvacTunnel® a real-time model integrated in a Decision Support System (DSS) for emergency management in road tunnel. The proposed model is based on the idea that, in some scenarios such as road tunnels, egress calculations can be performed by addressing a small set of random parameters that have a great impact on outcomes. These parameters are identified as pre-movement times (recognition and response time), travel distances and unimpeded walking speeds. Based on Monte Carlo methods, the model has the capability to perform multiple simulations by changing random variables. As a first stage of verification process, the proposed model is compared with other validated evacuation models. The results are essentially coincident in all models. Based on this analysis it can be argued that the model provides consistent and reasonable results. But the main difference is that the proposed model can provide results faster than real-time (less than 5. s) while the run time of the other models is really higher. © 2012 Elsevier Ltd.","Emergency management; Evacuation Egress Modelling; Real-time; Road tunnels; Stochastic approach","Emergency management; Evacuation models; Forensic analysis; Pre movements; Random parameters; Real-time; Real-time mode; Real-time models; Road tunnel; Runtimes; Stochastic approach; Travel distance; Verification process; Walking speed; Artificial intelligence; Civil defense; Computer simulation; Decision support systems; Disasters; Monte Carlo methods; Risk management; Roads and streets; Stochastic systems; Stochastic models; article; calculation; computer program; computer simulation; decision support system; emergency; information processing; intermethod comparison; Monte Carlo method; priority journal; response time; statistical model; statistical parameters; traffic safety; travel; walking speed",Article,Scopus,2-s2.0-84868096075
"Güneralp I., Filippi A.M., Hales B.U.","River-flow boundary delineation from digital aerial photography and ancillary images using Support Vector Machines",2013,"GIScience and Remote Sensing",16,10.1080/15481603.2013.778560,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877685404&doi=10.1080%2f15481603.2013.778560&partnerID=40&md5=09ac47591713218596274ef90d9d2753","Delineation of river-flow boundaries constitutes an important step in various river-related studies, including river hydraulic modeling, flow-width estimations, and river and floodplain habitat mapping and assessment. Increasing the level of automation of delineation of flow boundaries from synoptic remote-sensing images provides great potential, by reducing the labor cost, especially for studies focusing on long river reaches and those examining flow changes over time. This article investigates the boundary delineation of river channel flow from aerial photographs using Support Vector Machine (SVM) and image-derived ancillary data layers. It also includes a quantitative evaluation of delineation accuracy. The findings show that SVM performs satisfactory delineations of the boundaries, and the ancillary data layers generated using edge detectors and spatial domain texture statistics particularly increase delineation accuracy. Moreover, a multiscale evaluation scheme allows for examining the performance of SVM for the whole river reach, as well as that for the subriver sections with varied geomorphic and environmental conditions. © 2013 Taylor and Francis Group, LLC.","aerial photography; boundary delineation; flow; river; Support Vector Machines","aerial photography; artificial intelligence; channel flow; environmental conditions; floodplain; habitat; mapping; remote sensing; river flow; synoptic meteorology",Article,Scopus,2-s2.0-84877685404
"Barzilay O., Wolf A.","Adaptive rehabilitation games",2013,"Journal of Electromyography and Kinesiology",16,10.1016/j.jelekin.2012.09.004,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873137538&doi=10.1016%2fj.jelekin.2012.09.004&partnerID=40&md5=b424de93125b86a65b44e67c4bf72ea6","In conventional neuromuscular rehabilitation, patients are required to perform biomechanical exercises to recover their neuromotor abilities. These physiotherapeutic tasks are defined by the physiotherapist, according to his estimate of the patient's pathologic neuromotor function. The definition of the task is mainly qualitative and it is often merely demonstrated to the patient as a gesture to reproduce. Success of the treatment relies then on the accuracy and repetition of the motor training.We propose a novel approach to neuromotor training by combining the advantages of a virtual reality platform with biofeedback information on the training subject from biometric equipment and with the computational power of artificial neural networks. In a calibration stage, the subject performs motor training on a known task to train the network. Once trained, the tuned network generates a new patient-specific task, based on the definition of the subject's expected performance dictated by the therapist. The system was tested for upper limb rehabilitation on healthy subjects. We measured a 33% improvement in the triceps performance (p=.0.027). The novelty of the proposed approach lies in its use of learning systems to the estimation of biological models. © 2012 Elsevier Ltd.","Adaptive systems; Artificial neural network; Electromyography; Neuromuscular rehabilitation","adult; article; artificial intelligence; artificial neural network; biceps brachii muscle; biomechanics; cognitive rehabilitation; electromyogram; exercise; feedback system; human; human experiment; motion analysis system; motor control; motor performance; muscle training; neuromuscular function; normal human; physiotherapy; positive feedback; priority journal; triceps brachii muscle; virtual reality; Humans; Movement Disorders; Neurofeedback; Software; Therapy, Computer-Assisted; Treatment Outcome; User-Computer Interface; Video Games",Article,Scopus,2-s2.0-84873137538
"Alvear D., Abreu O., Cuesta A., Alonso V.","Decision support system for emergency management: Road tunnels",2013,"Tunnelling and Underground Space Technology",16,10.1016/j.tust.2012.10.005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872381448&doi=10.1016%2fj.tust.2012.10.005&partnerID=40&md5=94ace04017624a5e0451c1f917367570","Recent disasters have shown that road tunnels are especially complex environments for decision making. A fast and effective response to emergencies in road tunnels is a key factor for life safety. This paper presents a decision support system (DSS) for emergency management in road tunnels. Based on a specific methodology, the system provides the operator with decision recommendations to deal with the emergency in real time. Furthermore, the system uses predictive tools to estimate the severity of the accident or incident, as well as rescue and evacuation times. This information is very useful during the first stages of an emergency when information is scarce, incomplete and inaccurate, yet the tunnel operator is required to make the right decisions under a high level of stress. The DSS reduces the decision circle and allows the operator to make critical decisions based on dynamic alternatives. The system has been tested in various hypothetical emergency cases based on the Tunnel of Lantueno in the A-67 Highway, Spain. The application cases show that the DSS provides reasonable and consistent results. © 2012 Elsevier Ltd.","Decision support system; Emergency management; Real-time; Road tunnels","Complex environments; Emergency management; Key factors; Life safety; ON dynamics; Predictive tools; Real-time; Road tunnel; System use; Artificial intelligence; Civil defense; Decision support systems; Disasters; Risk management; Roads and streets; Highway planning; decision making; decision support system; disaster management; safety; tunnel; Spain",Article,Scopus,2-s2.0-84872381448
"McKnight U.S., Finkel M.","A system dynamics model for the screening-level long-term assessment of human health risks at contaminated sites",2013,"Environmental Modelling and Software",16,10.1016/j.envsoft.2012.07.007,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871798391&doi=10.1016%2fj.envsoft.2012.07.007&partnerID=40&md5=0c5b74af5aa1165a1cb14900c47efd45","For the design of sustainable and cost-effective management strategies for contaminated sites, decision makers need appropriate tools, i.e. environmental decision support systems to assist them in the planning, assessment, selection and optimisation of possible alternatives. We propose a novel system dynamics model, CARO-PLUS (Cost-efficiency Analysis of Remediation Options), which provides estimates of current and future risks originating from soil and groundwater contamination. Utilising a source-pathway-receptor concept, the model particularly addresses the presence of multi-compound non-aqueous phase liquids in porous media, which have been identified as major sources of groundwater contamination at many of these sites. Simplified approaches for the description of contaminant release and transport, as well as of exposure pathways for human health risk assessment, allow for a fast and effective screening model, which is particularly qualified to support early decisions within a tiered management approach at contaminated sites. CARO-PLUS is applied to assess the long-term risks originating from a kerosene contamination at a former military airfield in Germany. Monte Carlo simulations are performed to account for the large uncertainty in model parameters at early decision levels. The results of the application show that the implementation of monitored natural attenuation might be a feasible management strategy for the site, and provide guidance for additional, more detailed investigations. © 2012 Elsevier Ltd.","Contaminated sites; EDSS; Human health risk assessment; Monitored natural attenuation; Risk-based land management; System dynamics; Uncertainty; Vensim","Contaminated sites; EDSS; Human health risk assessment; Land managements; Monitored natural attenuation; System Dynamics; Uncertainty; Vensim; Artificial intelligence; Contamination; Cost benefit analysis; Decision making; Decision support systems; Groundwater; Monte Carlo methods; Porous materials; Risk assessment; System theory; Geologic models; assessment method; bioremediation; cost-benefit analysis; decision making; groundwater pollution; health risk; Monte Carlo analysis; pollutant source; pollution effect; risk factor; soil pollution; Germany",Article,Scopus,2-s2.0-84871798391
"Shekarian E., Gholizadeh A.A.","Application of adaptive network based fuzzy inference system method in economic welfare",2013,"Knowledge-Based Systems",16,10.1016/j.knosys.2012.10.013,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871924441&doi=10.1016%2fj.knosys.2012.10.013&partnerID=40&md5=c0b19eef125dfc1722eef6ab30aabb5c","Welfare analysis is a very important issue in the context of socio-economic. There is a huge literature about economic welfare and exist many conventional techniques and traditional methods that are used in almost all economic welfare studies. Regardless of the method used in welfare studies, the key element to promote the welfare of households in a community is to identify factors affecting welfare. For the first time, Adaptive Network Based Fuzzy Inference System (ANFIS) which is a powerful artificial intelligence tool is used as a new approach in economic welfare. To do this, real micro-data including some characteristics of households and housing units were used. We identified the most important factors affecting household welfare applying the ANFIS method. The ANFIS was then parameterized using these factors in order to predict the welfare measure. The empirical results showed that the ANFIS method outperforms the multiple regression method. © 2012 Elsevier B.V. All rights reserved.","ANFIS; Artificial intelligence; Economic welfare; Input selection; Prediction","Adaptive network based fuzzy inference system; ANFIS; ANFIS method; Artificial intelligence tools; Community IS; Conventional techniques; Economic welfare; Housing units; Input selection; Key elements; Multiple regressions; Parameterized; Socio-economics; Welfare analysis; Artificial intelligence; Forecasting; Fuzzy systems",Article,Scopus,2-s2.0-84871924441
"Zhang B., Gao J., Gao L., Sun X.","Improvements in the ant colony optimization algorithm for endmember extraction from hyperspectral images",2013,"IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing",16,10.1109/JSTARS.2012.2236821,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877923161&doi=10.1109%2fJSTARS.2012.2236821&partnerID=40&md5=b73abb6347930f6aa15f9187731eed21","Endmember extraction is a vital step in spectral unmixing of hyperspectral images. The Ant Colony Optimization (ACO) algorithm has been recently developed for endmember extraction from hyperspectral data. However, this algorithm may result in a local optimal solution for some hyperspectral images without prescient information, and also has limitation in computational performance. Therefore, in this paper, we proposed several new methods to improve the ACO algorithm for endmember extraction (ACOEE). Firstly, the heuristic information was optimized to improve the algorithm accuracy. In the improved ACOEE, only the pheromones were adopted as the heuristic information when there was no prescient information about hyperspectral data. Then, to enhance algorithm performance, an elitist strategy was proposed to lessen the iteration numbers without reducing the accuracy, and the parallel implementation of ACOEE on graphics processing units (GPUs) also was utilized to shorten the computational time per iteration. The experiment for real hyperspectral data demonstrated that both the endmember extraction accuracy and the computational performance of ACOEE benefited from these methods. © 2008-2012 IEEE.","Ant colony optimization; elitist strategy; endmember extraction; GPUs","Ant Colony Optimization algorithms; Computational performance; Elitist strategies; Endmember extraction; GPUs; Graphics processing units; Local optimal solution; Parallel implementations; Ant colony optimization; Artificial intelligence; Computer graphics; Extraction; Iterative methods; Program processors; Spectroscopy; Algorithms; accuracy assessment; algorithm; data processing; heuristics; image analysis; optimization; spectral analysis",Article,Scopus,2-s2.0-84877923161
"Yu C.Y., Lam K.C.","A Decision Support System for the determination of concession period length in transportation project under BOT contract",2013,"Automation in Construction",16,10.1016/j.autcon.2012.11.012,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872066527&doi=10.1016%2fj.autcon.2012.11.012&partnerID=40&md5=09b9427849545dba38563c34a2fd08a3","The determination of the concession period length directly affects both the involved government and private investors' financial returns and risks in a Build-Operate-Transfer project. In existing methods, the concession period is usually determined by the concessionaire depending on their expected investment return, or predicted without comprehensive analysis of the influential factors. In view of this, a Decision Support System for concession period length determination (CPLD) was developed and, as demonstrated herein provides a possible way of solving the concession period problem, especially under the impact of influential factors. The influential factors were firstly investigated. Then the overall model was developed using the Monte Carlo method. A dataset of a simulated highway project was employed to verify the model. The results show that the developed model can generate a set of alternatives, among which a reasonable one could be selected after balancing the interests of both sides. © 2012 Elsevier B.V.","Concession period length determination; Decision Support System; Monte Carlo","Build-operate-transfer projects; Comprehensive analysis; Data sets; Developed model; Financial returns; Highway projects; Influential factors; Investment returns; MONTE CARLO; Overall-model; Period length; Private investors; Transportation projects; Artificial intelligence; Computer simulation; Decision support systems; Highway engineering; Monte Carlo methods; Privatization; Transportation routes; Factor analysis",Article,Scopus,2-s2.0-84872066527
"Lake B.M., Salakhutdinov R., Tenenbaum J.B.","One-shot learning by inverting a compositional causal process",2013,"Advances in Neural Information Processing Systems",16,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898998554&partnerID=40&md5=c8cdaa2579f55887a186c9bb1a66533f","People can learn a new visual class from just one example, yet machine learning algorithms typically require hundreds or thousands of examples to tackle the same problems. Here we present a Hierarchical Bayesian model based on compositionality and causality that can learn a wide range of natural (although simple) visual concepts, generalizing in human-like ways from just one image. We evaluated performance on a challenging one-shot classification task, where our model achieved a human-level error rate while substantially outperforming two deep learning models. We also tested the model on another conceptual task, generating new examples, by using a ""visual Turing test"" to show that our model produces human-like performance.",,"Artificial intelligence; Bayesian networks; Classification tasks; Compositionality; Deep learning; Error rate; Hierarchical Bayesian modeling; One-shot learning; Turing tests; Visual concept; Learning algorithms",Conference Paper,Scopus,2-s2.0-84898998554
"Van Schie G., Wallis M.G., Leifland K., Danielsson M., Karssemeijer N.","Mass detection in reconstructed digital breast tomosynthesis volumes with a computer-aided detection system trained on 2D mammograms",2013,"Medical Physics",16,10.1118/1.4791643,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876161237&doi=10.1118%2f1.4791643&partnerID=40&md5=4f45e87f2e78e2d5ab172e54c15ae939","Purpose: To develop a computer-aided detection (CAD) system for masses in digital breast tomosynthesis (DBT) which can make use of an existing CAD system for detection of breast masses in full-field digital mammography (FFDM). This approach has the advantage that large digital screening databases that are becoming available can be used for training. DBT is currently not used for screening which makes it hard to obtain sufficient data for training. Methods: The proposed CAD system is applied to reconstructed DBT volumes and consists of two stages. In the first stage, an existing 2D CAD system is applied to slabs composed of multiple DBT slices, after processing the slabs to a representation similar to that of the FFDM training data. In the second stage, the authors group detections obtained in the slabs that detect the same object and determine the 3D location of the grouped findings using one of three different approaches, including one that uses a set of features extracted from the DBT slabs. Experiments were conducted to determine performance of the CAD system, the optimal slab thickness for this approach and the best method to establish the 3D location. Experiments were performed using a database of 192 patients (752 DBT volumes). In 49 patients, one or more malignancies were present which were described as a mass, architectural distortion, or asymmetry. Free response receiver operating characteristic analysis and bootstrapping were used for statistical evaluation. Results: Best performance was obtained when slab thickness was in the range of 1-2 cm. Using the feature based 3D localization procedure developed in the study, accurate 3D localization could be obtained in most cases. Case sensitivities of 80 and 90 were achieved at 0.35 and 0.99 false positives per volume, respectively. Conclusions: This study indicates that there may be a large benefit in using 2D mammograms for the development of CAD for DBT and that there is no need to exclusively limit development to DBT data. © 2013 American Association of Physicists in Medicine.","computer-aided detection; digital breast tomosynthesis; mammography; mass","adult; article; bootstrapping; breast tumor; cancer screening; data base; digital mammography; female; human; major clinical study; priority journal; receiver operating characteristic; thickness; training; aged; algorithm; artificial intelligence; automated pattern recognition; computer assisted tomography; image quality; mammography; methodology; middle aged; radiography; reproducibility; sensitivity and specificity; tumor volume; very elderly; Adult; Aged; Aged, 80 and over; Algorithms; Artificial Intelligence; Breast Neoplasms; Female; Humans; Mammography; Middle Aged; Pattern Recognition, Automated; Radiographic Image Enhancement; Reproducibility of Results; Sensitivity and Specificity; Tomography, X-Ray Computed; Tumor Burden",Article,Scopus,2-s2.0-84876161237
"Spina D., Gonzalo J., Amigó E.","Discovering filter keywords for company name disambiguation in twitter",2013,"Expert Systems with Applications",16,10.1016/j.eswa.2013.03.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885044015&doi=10.1016%2fj.eswa.2013.03.001&partnerID=40&md5=8ddf47071581b65e729946af435e8a8d","A major problem in monitoring the online reputation of companies, brands, and other entities is that entity names are often ambiguous (apple may refer to the company, the fruit, the sin ger, etc.). The prob- lem is particularly hard in microblogging services such as Twitter, where texts are very short and there is little context to disambiguate.In this paper we address the filtering task of determining, out of a set of tweets that contain a company name, which ones do refer to the company.Our approach relies on the identification of filter keywords : those whose presence in a tweet reliably confirm(positive keywords) or discard (negative keywords) that the tweet refers to the company. We describe an algorithm to extract filter keywords that does not use any previously annotated data about the target company. The algorithm allows to classify 58% of the tweets with 75% accuracy; and those can be used to feed a machine learning algorithm to obtain a complete classification of all tweets with an overall accuracy of 73%. In comparison, a 10-fold validation of the same machine learning algo- rithm provides an accuracy of 85%, i.e., our unsupervised algorithm has a 14% loss with respect to its supervised counterpart. Our study also shows that (i) filter keywords for Twitter does not directly derive from the public in for- mation about the company in the Web: a manual selection of keywords from relevant web sources only covers 15% of the tweets with 86% accuracy;(ii) filter keywords can indeed be a productive way of clas- sifying tweets: the five best possible keywords cover, in average,28% of the tweets for acompany inour test collection. © 2013 Elsevier Ltd. All rights reserved.","Filtering; Name disambiguation; Online reputation management; Twitter","Artificial intelligence; Bandpass filters; Data mining; Filtration; Learning systems; Social networking (online); Complete classification; Micro-blogging services; Name disambiguation; Online reputation managements; Overall accuracies; Test Collection; Twitter; Unsupervised algorithms; Learning algorithms",Article,Scopus,2-s2.0-84885044015
"Urvoy T., Clerot F., Féraud R., Naamane S.","Generic exploration and K-armed voting bandits",2013,"30th International Conference on Machine Learning, ICML 2013",16,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897542612&partnerID=40&md5=35b4729368e5d5f14c96652cb15fadb4","We study a stochastic online learning scheme with partial feedback where the utility of decisions is only observable through an estimation of the environment parameters. We propose a generic pure-exploration algorithm, able to cope with various utility functions from multi-armed bandits settings to dueling bandits. The primary application of this setting is to offer a natural generalization of dueling bandits for situations where the environment parameters reflect the idiosyncratic preferences of a mixed crowd. Copyright 2013 by the author(s).",,"Artificial intelligence; Software engineering; Multi armed bandit; Natural generalization; Online learning scheme; Partial feedback; Utility functions; Learning systems",Conference Paper,Scopus,2-s2.0-84897542612
"Rickenberg T.A., Gebhardt A., Breitner M.H.","A decision support system for the optimization of car sharing stations",2013,"ECIS 2013 - Proceedings of the 21st European Conference on Information Systems",16,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905841493&partnerID=40&md5=9b2c840df9906dcfdfa5b432d70a25da","Approximately half of the world's population is living in cities and it continues to grow. Along with urbanization, scarce natural resources, rising energy costs, shortage of space, increasing traffic congestion, and environmental pollution require populations to rethink personal vehicle ownership. Car sharing is an alternative that allows individuals to satisfy their mobility needs and addresses modern transportation issues. The location and accessibility of car sharing stations are critical success factors. We provide decision support for planning car sharing stations, both existing and new ones. Therefore, we constructed and evaluated research artifacts according to the design science research principles. We suggest an optimization model to determine the prime location and size of car sharing stations. Based on this model, a decision support system (DSS) called OptCarShare 1.0 is used for exact optimization. This system integrates several applications to import, edit, and export data, solve the problem numerically and visualize optimization results. Using a major German city with 500,000 people to illustrate solutions, we evaluate and show the applicability of the DSS OptCarShare 1.0. According to Green IS, our DSS can provide a contribution to environmental sustainability.","Car sharing; Decision support system (DSS); Green IS.; Optimization model","Artificial intelligence; Design; Information systems; Mathematical models; Optimization; Traffic congestion; Car sharing; Critical success factor; Decision support system (dss); Design-science researches; Environmental pollutions; Environmental sustainability; Green is; Optimization modeling; Decision support systems",Conference Paper,Scopus,2-s2.0-84905841493
"Aguilera P.A., Fernández A., Ropero R.F., Molina L.","Groundwater quality assessment using data clustering based on hybrid Bayesian networks",2013,"Stochastic Environmental Research and Risk Assessment",16,10.1007/s00477-012-0676-8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872496467&doi=10.1007%2fs00477-012-0676-8&partnerID=40&md5=7445b35158515a29a4720db449275b18","Bayesian networks (BNs) have become a standard in the field of Artificial Intelligence as a means of dealing with uncertainty and risk modelling. In recent years, there has been particular interest in the simultaneous use of continuous and discrete domains, obviating the need for discretization, using so-called hybrid BNs. In these hybrid environments, Mixtures of Truncated Exponentials (MTEs) provide a suitable solution for working without any restriction. The objective of this study is the assessment of groundwater quality through the design and application of a probabilistic clustering, based on hybrid Bayesian networks with MTEs. Firstly, the results obtained allows the differentiation of three groups of sampling points, indicating three different classes of groundwater quality. Secondly, the probability that a sampling point belongs to each cluster allows the uncertainty in the clusters to be assessed, as well as the risks associated in terms of water quality management. The methodology developed could be applied to other fields in environmental sciences. © 2012 Springer-Verlag Berlin Heidelberg.","Groundwater quality; Hybrid Bayesian networks; Mixtures of truncated exponentials; Probabilistic data clustering","Bayesian Networks (bns); Data clustering; Design and application; Discrete domains; Discretizations; Environmental science; Ground-water qualities; Groundwater quality assessment; Hybrid Bayesian networks; Mixtures of truncated exponentials; Probabilistic clustering; Probabilistic data; Sampling points; Simultaneous use; Suitable solutions; Artificial intelligence; Bayesian networks; Clustering algorithms; Groundwater; Hybrid sensors; Risks; Uncertainty analysis; Water conservation; Water management; Water supply; Water quality; assessment method; Bayesian analysis; cluster analysis; groundwater; probability; uncertainty analysis; water quality",Article,Scopus,2-s2.0-84872496467
"Xia L., Robock A.","Impacts of a nuclear war in South Asia on rice production in Mainland China",2013,"Climatic Change",16,10.1007/s10584-012-0475-8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871442907&doi=10.1007%2fs10584-012-0475-8&partnerID=40&md5=2d957918ccb5d1e376eea2c24bbbcb2b","A regional nuclear war between India and Pakistan with a 5 Tg black carbon injection into the upper troposphere would produce significant climate changes for a decade, including cooling, reduction of solar radiation, and reduction of precipitation, which are all important factors controlling agricultural productivity. We used the Decision Support System for Agrotechnology Transfer agricultural simulation model to simulate regional nuclear war impacts on rice yield in 24 provinces in China. We first evaluated the model by forcing it with daily weather data and management practices for the period 1980-2008 for 24 provinces in China, and compared the results to observations of rice yields in China. Then we perturbed observed weather data using climate anomalies for a 10-year period from a nuclear war simulation. We perturbed each year of the 30-year climate record with anomalies from each year of the 10-year nuclear war simulations for different regions in China. We found that rice production would decline by an average of 21 % for the first 4 years after soot injection, and would slowly recover in the following years. For the next 6 years, the reduction in rice production was about 10 %. Different regions responded differently to climate changes from nuclear war. Rice production in northern China was damaged severely, while regions along the south and east coasts showed a positive response to regional nuclear war. Although we might try to adapt to a perturbed climate by enhancing rice planting activity in southern and eastern China or increasing fertilizer usage, both methods have severe limitations. The best solution to avoid nuclear war impacts on agriculture is to avoid nuclear war, and this can only be guaranteed with a nuclear-weapon-free world. © 2012 Springer Science+Business Media B.V.",,"Agricultural productivity; Agrotechnology transfer; Black carbon; Climate anomalies; Climate record; East coast; Eastern China; Management practices; Northern China; Pakistan; Rice production; Rice yield; Simulation model; South Asia; Upper troposphere; Weather data; Agriculture; Artificial intelligence; Climate change; Computer simulation; Decision support systems; Meteorology; Sun; Nuclear weapons; carbon; climate change; crop production; crop yield; data set; ecological modeling; fertilizer application; nuclear weapon; rice; soot; troposphere; South Asia",Article,Scopus,2-s2.0-84871442907
"He L., Parikh N.A.","Automated detection of white matter signal abnormality using T2 relaxometry: Application to brain segmentation on term MRI in very preterm infants",2013,"NeuroImage",16,10.1016/j.neuroimage.2012.08.081,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867475394&doi=10.1016%2fj.neuroimage.2012.08.081&partnerID=40&md5=89e51e1722533e3fe83538dec91dc0b7","Hyperintense white matter signal abnormalities, also called diffuse excessive high signal intensity (DEHSI), are observed in up to 80% of very preterm infants on T2-weighted MRI scans at term-equivalent age. DEHSI may represent a developmental stage or diffuse microstructural white matter abnormalities. Automated quantitative assessment of DEHSI severity may help resolve this debate and improve neonatal brain tissue segmentation. For T2-weighted sequence without fluid attenuation, the signal intensity distribution of DEHSI greatly overlaps with that of cerebrospinal fluid (CSF) making its detection difficult. Furthermore, signal intensities of T2-weighted images are susceptible to magnetic field inhomogeneity. Increased signal intensities caused by field inhomogeneity may be confused with DEHSI. To overcome these challenges, we propose an algorithm to detect DEHSI using T2 relaxometry, whose reflection of the rapid changes in free water content provides improved distinction between CSF and DEHSI over that of conventional T2-weighted imaging. Moreover, the parametric transverse relaxation time T2 is invulnerable to magnetic field inhomogeneity. We conducted computer simulations to select an optimal detection parameter and to validate the proposed method. We also demonstrated that brain tissue segmentation is further enhanced by incorporating DEHSI detection for both simulated preterm infant brain images and in vivo in very preterm infants imaged at term-equivalent age. © 2012 Elsevier Inc.","Brain segmentation; Diffuse excessive high signal intensity (DEHSI); Extremely low birth weight infants; Magnetic resonance imaging; T2 relaxometry; White matter signal abnormalities (WMSA)","article; automation; brain tissue; cerebrospinal fluid; clinical article; computer simulation; developmental stage; diagnostic imaging; diagnostic test; diffuse excessive high signal intensity; diffusion weighted imaging; disease severity; human; human tissue; in vivo study; infant; magnetic field; nuclear magnetic resonance imaging; prematurity; priority journal; quantitative analysis; simulation; susceptibility weighted imaging; T2 relaxometry; water content; white matter lesion; Algorithms; Artificial Intelligence; Brain; Diffusion Tensor Imaging; Female; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Infant, Extremely Premature; Male; Nerve Fibers, Myelinated; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84867475394
"Neshat M., Sepidnam G., Sargolzaei M.","Swallow swarm optimization algorithm: A new method to optimization",2013,"Neural Computing and Applications",16,10.1007/s00521-012-0939-9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880747667&doi=10.1007%2fs00521-012-0939-9&partnerID=40&md5=62c2af6ff239b5abc5a0a70709ce50db","This paper presents an exposition of a new method of swarm intelligence-based algorithm for optimization. Modeling swallow swarm movement and their other behavior, this optimization method represents a new optimization method. There are three kinds of particles in this method: explorer particles, aimless particles, and leader particles. Each particle has a personal feature but all of them have a central colony of flying. Each particle exhibits an intelligent behavior and, perpetually, explores its surroundings with an adaptive radius. The situations of neighbor particles, local leader, and public leader are considered, and a move is made then. Swallow swarm optimization algorithm has proved high efficiency, such as fast move in flat areas (areas that there is no hope to find food and, derivation is equal to zero), not getting stuck in local extremum points, high convergence speed, and intelligent participation in the different groups of particles. SSO algorithm has been tested by 19 benchmark functions. It achieved good results in multimodal, rotated and shifted functions. Results of this method have been compared to standard PSO, FSO algorithm, and ten different kinds of PSO. © 2012 Springer-Verlag London Limited.","Benchmark function; Computational intelligence; Fish swarm optimization; Particle swarm optimization; Swallow swarm optimization (SSO)","Benchmark functions; Convergence speed; Intelligent behavior; Local extremum; Optimization method; Personal features; Swarm optimization; Swarm optimization algorithms; Artificial intelligence; Particle swarm optimization (PSO); Algorithms",Article,Scopus,2-s2.0-84880747667
"Wright P.","Cyber-physical product manufacturing",2013,"Manufacturing Letters",16,10.1016/j.mfglet.2013.10.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899046350&doi=10.1016%2fj.mfglet.2013.10.001&partnerID=40&md5=e537197ba1d500ffd79f9d99727422a9","The U.S. and E.U. countries still can lead international manufacturing by exploiting Cyber Physical System (CPS) technologies such as wireless system integration, wireless controls, machine learning, and sensor-based manufacturing. This deeper level of sensing, cross-platform-communication and control enhances a product's design, its production systems, its in-service performance, and its sustainability over its life cycle. © 2013 Society of Manufacturing Engineers (SME).","High-value products; Lifecycle; Manufacturing; Sensor nets; Service","Artificial intelligence; Embedded systems; Learning systems; Manufacture; Product design; Communication and control; Cyber-physical systems (CPS); High-value products; Production system; Sensor net; Service; Service performance; Wireless control; Life cycle",Article,Scopus,2-s2.0-84899046350
"Lee J.-Y., Matsushita Y., Shi B., Kweon I.S., Ikeuchi K.","Radiometric calibration by rank minimization",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",16,10.1109/TPAMI.2012.66,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870159146&doi=10.1109%2fTPAMI.2012.66&partnerID=40&md5=18f3ff1c855a96e86e52590bd85ad945","We present a robust radiometric calibration framework that capitalizes on the transform invariant low-rank structure in the various types of observations, such as sensor irradiances recorded from a static scene with different exposure times, or linear structure of irradiance color mixtures around edges. We show that various radiometric calibration problems can be treated in a principled framework that uses a rank minimization approach. This framework provides a principled way of solving radiometric calibration problems in various settings. The proposed approach is evaluated using both simulation and real-world datasets and shows superior performance to previous approaches. © 1979-2012 IEEE.","camera response function; low-rank structure; Radiometric calibration; rank minimization","Camera response functions; Color mixture; Linear structures; Radiometric calibrations; Real-world datasets; Calibration; Mathematical transformations; Radiometry; algorithm; article; artificial intelligence; automated pattern recognition; calibration; light; methodology; photometry; Algorithms; Artificial Intelligence; Calibration; Light; Pattern Recognition, Automated; Photometry",Article,Scopus,2-s2.0-84870159146
"Jafarpour B., Abidi S.S.R.","Merging disease-specific clinical guidelines to handle comorbidities in a clinical decision support setting",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,10.1007/978-3-642-38326-7_5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887282485&doi=10.1007%2f978-3-642-38326-7_5&partnerID=40&md5=8699655c5bd3a6dcc97dc5fb14c152c2","From a clinical decision support perspective the treatment of co-morbid diseases is a challenge since it demands the coordination between the disease-specific therapeutic plans of the co-morbid diseases. Although clinical guidelines provide clinical recommendations, they focus on a single disease and for comorbid disease management there is a requirement to have multiple concurrently active clinical guidelines. Merging computerized clinical practice guidelines (CPG) related to comorbidities and using them in clinical decision support systems is a potential solution to manage comorbidities in a clinical decision support system. We have developed a CPG merging framework to merge computerized CPG. The central aspect of our framework is a merge representation ontology that captures the merging criteria to achieve the merging of multiple CPG whilst satisfying medical, workflow, institutional and temporal constraints. We have used our framework successfully to create therapy plans for patients treated for Atrial Fibrillation and Chronic Heart Failure comorbidity. © 2013 Springer-Verlag.","Comorbidity; Execution Engine; OWL; Practice Guideline; SWRL","Artificial intelligence; Decision support systems; Epidemiology; Medical applications; Merging; Patient monitoring; Co morbidities; Execution engine; OWL; Practice Guideline; SWRL; Diseases",Conference Paper,Scopus,2-s2.0-84887282485
"Amer M.R., Todorovic S., Fern A., Zhu S.-C.","Monte carlo tree search for scheduling activity recognition",2013,"Proceedings of the IEEE International Conference on Computer Vision",16,10.1109/ICCV.2013.171,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898797429&doi=10.1109%2fICCV.2013.171&partnerID=40&md5=1949ab6e7875da7e8481df085924096e","This paper addresses recognition of human activities with stochastic structure, characterized by variable space-time arrangements of primitive actions, and conducted by a variable number of actors. Our approach classifies the activity of interest as well as identifies the relevant foreground in the video. Each activity representation is considered as a mixture distribution of BoWs captured by a Sum-Product Network (SPN). In our approach, SPN represents a linear mixture of many bags-of-words (BoWs) where each BoW represents an important foreground part of the activity. This mixture distribution is efficiently computed by organizing the BoWs in a hierarchy, where children BoWs are nested within parent BoWs. SPN allows us to model this mixture since it consists of terminal nodes representing BoWs, product nodes, and sum nodes organized in a number of layers. The products are aimed at encoding particular configurations of primitive actions, and the sums serve to capture their alternative configurations. SPN inference amounts to parsing the SPN graph, which yields the most probable explanation (MPE) of the video foreground. SPN inference has linear complexity in the number of nodes, under fairly general conditions, enabling fast and scalable recognition. The connectivity of SPN and the parameters of BoW distributions are learned under weak supervision using a variational EM algorithm. For our evaluation, we have compiled and annotated a new Volleyball dataset. Our classification accuracy and localization results are superior to those of the state of the art on current benchmarks as well as our Volleyball datasets. © 2013 IEEE.","Activity Recogition; And-Or Graphs; Event Analysis; Stochastic Grammars; Video Parsing","Algorithms; Artificial intelligence; Classification (of information); Complex networks; SportS; Stochastic systems; Alternative configurations; And-Or Graphs; Event analysis; Monte-Carlo tree searches; Most probable explanation; Stochastic grammars; Variational em algorithms; Video parsing; Mixtures",Conference Paper,Scopus,2-s2.0-84898797429
"Utkin V.I., Poznyak A.S.","Adaptive Sliding Mode Control",2013,"Lecture Notes in Control and Information Sciences",16,10.1007/978-3-642-36986-5_2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904699262&doi=10.1007%2f978-3-642-36986-5_2&partnerID=40&md5=a701efa7651605db10b02f48dcccc1a1","The main obstacles for application of Sliding Mode Control are two interconnected phenomena: chattering and high activity of control action. It is well known that the amplitude of chattering is proportional to the magnitude of a discontinuous control. These two problems can be handled simultaneously if the magnitude is reduced to a minimal admissible level defined by the conditions for the sliding mode to exist. Here an adaptation methodology is discussed for obtaining the minimum possible value of control based on two approaches developed in recent publications: The σ - adaptation, providing adequate adjustments of the magnitude of a discontinuous control within the ""reaching phase"", that is, when the state trajectories are out of a sliding surface; Dynamic adaptation or the adaptation within a sliding mode (on a sliding surface), based on the, so-called, equivalent control obtained by the direct measurements of the output signals of a first-order low-pass filter containing in the input the discontinuous control with the specially adapted magnitude value. The application of these methodologies is illustrated by the corresponding adaptive versions of the Super Twist controller. It is shown that the adaptation based on the equivalent control application enables reducing of the control action magnitude to the minimum possible value keeping the property of finite-time convergence. © Springer-Verlag Berlin Heidelberg 2013.",,"Artificial intelligence; Control; Software engineering; Adaptive sliding mode control; Adaptive versions; Direct measurement; Discontinuous control; Dynamic adaptations; Equivalent control; Finite-time convergence; State trajectory; Sliding mode control",Conference Paper,Scopus,2-s2.0-84904699262
"Huang T.-W., Zaretzki J., Bergeron C., Bennett K.P., Breneman C.M.","DR-Predictor: Incorporating flexible docking with specialized electronic reactivity and machine learning techniques to predict CYP-mediated sites of metabolism",2013,"Journal of Chemical Information and Modeling",15,10.1021/ci4004688,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896500405&doi=10.1021%2fci4004688&partnerID=40&md5=79dd34729c8383968d26598d8d8bc523","Computational methods that can identify CYP-mediated sites of metabolism (SOMs) of drug-like compounds have become required tools for early stage lead optimization. In recent years, methods that combine CYP binding site features with CYP/ligand binding information have been sought in order to increase the prediction accuracy of such hybrid models over those that use only one representation. Two challenges that any hybrid ligand/structure-based method must overcome are (1) identification of the best binding pose for a specific ligand with a given CYP and (2) appropriately incorporating the results of docking with ligand reactivity. To address these challenges we have created Docking-Regioselectivity-Predictor (DR-Predictor) - a method that incorporates flexible docking-derived information with specialized electronic reactivity and multiple-instance-learning methods to predict CYP-mediated SOMs. In this study, the hybrid ligand-structure-based DR-Predictor method was tested on substrate sets for CYP 1A2 and CYP 2A6. For these data, the DR-Predictor model was found to identify the experimentally observed SOM within the top two predicted rank-positions for 86% of the 261 1A2 substrates and 83% of the 100 2A6 substrates. Given the accuracy and extendibility of the DR-Predictor method, we anticipate that it will further facilitate the prediction of CYP metabolism liabilities and aid in in-silico ADMET assessment of novel structures. © 2013 American Chemical Society.",,"Learning systems; Ligands; Metabolism; Physiology; Substrates; Flexible docking; Hybrid ligand; Hybrid model; In-silico; Lead optimization; Machine learning techniques; Novel structures; Prediction accuracy; Forecasting; coumarin 7 hydroxylase; coumarin 7-hydroxylase; CYP1A2 protein, human; cytochrome P450 1A2; ligand; unspecific monooxygenase; article; artificial intelligence; biotransformation; chemical phenomena; chemistry; enzyme active site; enzyme specificity; human; hydrogen bond; metabolism; molecular docking; molecular library; protein binding; structure activity relation; thermodynamics; Artificial Intelligence; Aryl Hydrocarbon Hydroxylases; Biotransformation; Catalytic Domain; Cytochrome P-450 CYP1A2; Humans; Hydrogen Bonding; Hydrophobic and Hydrophilic Interactions; Ligands; Molecular Docking Simulation; Protein Binding; Small Molecule Libraries; Structure-Activity Relationship; Substrate Specificity; Thermodynamics",Article,Scopus,2-s2.0-84896500405
"Szlek J., Pacławski A., Llau R., Jachowicz R., Mendyk A.","Heuristic modeling of macromolecule release from PLGA microspheres",2013,"International Journal of Nanomedicine",15,10.2147/IJN.S53364,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888879464&doi=10.2147%2fIJN.S53364&partnerID=40&md5=e7dfe693c66e46886f1a2ea8166e9214","Dissolution of protein macromolecules from poly(lactic-co-glycolic acid) (PLGA) particles is a complex process and still not fully understood. As such, there are difficulties in obtaining a predictive model that could be of fundamental significance in design, development, and optimization for medical applications and toxicity evaluation of PLGA-based multiparticulate dosage form. In the present study, two models with comparable goodness of fit were proposed for the prediction of the macromolecule dissolution profile from PLGA micro- and nanoparticles. In both cases, heuristic techniques, such as artificial neural networks (ANNs), feature selection, and genetic programming were employed. Feature selection provided by fscaret package and sensitivity analysis performed by ANNs reduced the original input vector from a total of 300 input variables to 21, 17, 16, and eleven; to achieve a better insight into generalization error, two cut-off points for every method was proposed. The best ANNs model results were obtained by monotone multi-layer perceptron neural network (MON-MLP) networks with a root-mean-square error (RMSE) of 15.4, and the input vector consisted of eleven inputs. The complicated classical equation derived from a database consisting of 17 inputs was able to yield a better generalization error (RMSE) of 14.3. The equation was characterized by four parameters, thus feasible (applicable) to standard nonlinear regression techniques. Heuristic modeling led to the ANN model describing macromolecules release profiles from PLGA microspheres with good predictive efficiency. Moreover genetic programming technique resulted in classical equation with comparable predictability to the ANN model. © 2013 Szlek et al.","Artificial neural networks; Feature selection; Genetic programming; Molecular descriptors; Poly(lactic-co-glycolic acid) (PLGA) microparticles","alpha 1 antitrypsin; amyloid beta protein; asparaginase; bovine insulin; bovine serum albumin; chymotrypsin; human serum albumin; insulin; lysozyme; microsphere; ovalbumin; polyglactin; recombinant epidermal growth factor; recombinant erythropoietin; recombinant growth hormone; article; artificial neural network; computer analysis; computer prediction; drug release; drug solubility; feature selection; genetic programming; mathematical model; nonlinear regression analysis; artificial neural networks; feature selection; genetic programming; molecular descriptors; poly(lactic-co-glycolic acid) (PLGA) microparticles; Artificial Intelligence; Drug Carriers; Lactic Acid; Microspheres; Models, Molecular; Models, Statistical; Nanoparticles; Polyglycolic Acid; Proteins",Article,Scopus,2-s2.0-84888879464
"Sheldon D., Farnsworth A., Irvine J., Van Doren B., Webb K., Dietterich T.G., Kelling S.","Approximate Bayesian inference for reconstructing velocities of migrating birds from weather radar",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",15,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893422640&partnerID=40&md5=bf74f0497a63b01512236dcb9431b683","Archived data from the WSR-88D network of weather radars in the US hold detailed information about the continent-scale migratory movements of birds over the last 20 years. However, significant technical challenges must be overcome to understand this information and harness its potential for science and conservation. We present an approximate Bayesian inference algorithm to reconstruct the velocity fields of birds migrating in the vicinity of a radar station. This is part of a larger project to quantify bird migration at large scales using weather radar data. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Approximate Bayesian inference; Archived data; Bird migration; Migrating birds; Migratory movements; Technical challenges; Velocity field; Weather radar data; Artificial intelligence; Bayesian networks; Birds; Inference engines; Velocity; Meteorological radar",Conference Paper,Scopus,2-s2.0-84893422640
"Saçar M.D., Hamzeiy H., Allmer J.","Can MiRBase provide positive data for machine learning for the detection of MiRNA hairpins?",2013,"Journal of integrative bioinformatics",15,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891760538&partnerID=40&md5=7b82fd938c1f855193e1b43384c1d259","Experimental detection and validation of miRNAs is a tedious, time-consuming, and expensive process. Computational methods for miRNA gene detection are being developed so that the number of candidates that need experimental validation can be reduced to a manageable amount. Computational methods involve homology-based and ab inito algorithms. Both approaches are dependent on positive and negative training examples. Positive examples are usually derived from miRBase, the main resource for experimentally validated miRNAs. We encountered some problems with miRBase which we would like to report here. Some problems, among others, we encountered are that folds presented in miRBase are not always the fold with the minimum free energy; some entries do not seem to conform to expectations of miRNAs, and some external accession numbers are not valid. In addition, we compared the prediction accuracy for the same negative dataset when the positive data came from miRBase or miRTarBase and found that the latter led to more precise prediction models. We suggest that miRBase should introduce some automated facilities for ensuring data quality to overcome these problems.",,"microRNA; microRNA; algorithm; article; artificial intelligence; chemistry; conformation; genetics; human; molecular genetics; nucleic acid database; nucleotide sequence; sequence alignment; chemistry; Algorithms; Artificial Intelligence; Base Sequence; Databases, Nucleic Acid; Humans; MicroRNAs; Molecular Sequence Data; Nucleic Acid Conformation; Sequence Alignment; Algorithms; Artificial Intelligence; Base Sequence; Databases, Nucleic Acid; Humans; MicroRNAs; Molecular Sequence Data; Nucleic Acid Conformation; Sequence Alignment",Article,Scopus,2-s2.0-84891760538
"Lee M., Hwang J., Yoe H.","Agricultural production system based on IoT",2013,"Proceedings - 16th IEEE International Conference on Computational Science and Engineering, CSE 2013",15,10.1109/CSE.2013.126,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900346725&doi=10.1109%2fCSE.2013.126&partnerID=40&md5=713dae74fea743a1d1880a8b55021b91","There has been much research and various attempts to apply new IoT technology to agricultural areas. However, IoT for the agriculture should be considered differently against the same areas such as industrial, logistics. This paper presents the IoT-based agricultural production system for stabilizing supply and demand of agricultural products while developing the environment sensors and prediction system for the growth and production amount of crops by gathering its environmental information. Currently, the demand by consumption of agricultural products could be predicted quantitatively, however, the variation of harvest and production by the change of farm's cultivated area, weather change, disease and insect damage etc. could not be predicted, so that the supply and demand of agricultural products has not been controlled properly. To overcome it, this paper designed the IoT-based monitoring system to analyze crop environment, and the method to improve the efficiency of decision making by analyzing harvest statistics. Therefore, this paper developed the decision support system to forecast agricultural production using IoT sensors. This system was also a unified system that supports the processes sowing seeds through selling agricultural products to consumers. 3 Corresponding author The IoT-based agricultural production system through correlation analysis between the crop statistical information and agricultural environment information has enhanced the ability of farmers, researchers, and government officials to analyze current conditions and predict future harvest. Additionally, agricultural products quality can be improved because farmers observe whole cycle from seeding to selling using this IoT-based decision support system. © 2013 IEEE.","Agriculture; Decision support; IoT; Monitoring; Statistics","Agricultural environments; Agricultural production system; Agricultural productions; Decision supports; Environmental information; Government officials; IoT; Statistical information; Agriculture; Artificial intelligence; Crops; Decision support systems; Economics; Forecasting; Harvesting; Monitoring; Production engineering; Sensors; Statistics; Seed",Conference Paper,Scopus,2-s2.0-84900346725
"Taha A.M., Mustapha A., Chen S.-D.","Naive Bayes-guided bat algorithm for feature selection",2013,"The Scientific World Journal",15,10.1155/2013/325973,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893863229&doi=10.1155%2f2013%2f325973&partnerID=40&md5=12405b4254ba7b7eb95d3ddf53cca428","When the amount of data and information is said to double in every 20 months or so, feature selection has become highly important and beneficial. Further improvements in feature selection will positively affect a wide array of applications in fields such as pattern recognition, machine learning, or signal processing. Bio-inspired method called Bat Algorithm hybridized with a Naive Bayes classifier has been presented in this work. The performance of the proposed feature selection algorithm was investigated using twelve benchmark datasets from different domains and was compared to three other well-known feature selection algorithms. Discussion focused on four perspectives: number of features, classification accuracy, stability, and feature generalization. The results showed that BANB significantly outperformed other algorithms in selecting lower number of features, hence removing irrelevant, redundant, or noisy features while maintaining the classification accuracy. BANB is also proven to be more stable than other methods and is capable of producing more general feature subsets. © 2013 Ahmed Majid Taha et al.",,"algorithm; animal; article; artificial intelligence; automated pattern recognition; bat; Bayes theorem; biomimetics; echolocation; methodology; physiology; Algorithms; Animals; Artificial Intelligence; Bayes Theorem; Biomimetics; Chiroptera; Echolocation; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84893863229
"Kronegger M., Pfandler A., Pichler R.","Parameterized complexity of optimal planning: A detailed map",2013,"IJCAI International Joint Conference on Artificial Intelligence",15,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896061314&partnerID=40&md5=d5aaa10490aa26c34182704a4a959336","The goal of this paper is a systematic parameterized complexity analysis of different variants of propositional STRIPS planning. We identify several natural problem parameters and study all possible combinations of 9 parameters in 6 different settings. These settings arise, for instance, from the distinction if negative effects of actions are allowed or not. We provide a complete picture by establishing for each case either paraNP-hardness (i.e., the parameter combination does not help) or W[t]-completeness with t ∈ f1; 2g (i.e., fixed-parameter intractability), or FPT (i.e., fixed-parameter tractability).",,"Fixed-parameter tractability; Optimal planning; Parameter combination; Parameterized complexity; Problem parameters; STRIPS planning; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84896061314
"Xu M., Li Y.-F., Zhou Z.-H.","Multi-label learning with PRO LOSS",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",15,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893377206&partnerID=40&md5=5f4abc3e8381f122563ad05208484f42","Multi-label learning methods assign multiple labels to one object. In practice, in addition to differentiating relevant labels from irrelevant ones, it is often desired to rank the relevant labels for an object, whereas the rankings of irrelevant labels are not important. Such a requirement, however, cannot be met because most existing methods were designed to optimize existing criteria, yet there is no criterion which encodes the aforementioned requirement. In this paper, we present a new criterion, PRO LOSS, concerning the prediction on all labels as well as the rankings of only relevant labels. We then propose ProSVM which optimizes PRO LOSS efficiently using alternating direction method of multipliers. We further improve its efficiency with an upper approximation that reduces the number of constraints from O(T2) to O(T), where T is the number of labels. Experiments show that our proposals are not only superior on PRO LOSS, but also highly competitive on existing evaluation criteria. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Alternating direction method of multipliers; Evaluation criteria; Its efficiencies; Multi-label learning; Multiple labels; Upper approximation; Artificial intelligence; Learning systems",Conference Paper,Scopus,2-s2.0-84893377206
"Chen Y., Zhao J., Hu X., Zhang X., Li Z., Chua T.-S.","From interest to function: Location estimation in social media",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",15,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893418660&partnerID=40&md5=035512cb4e27cd5f6d5e278dd9529465","Recent years have witnessed the tremendous development of social media, which attracts a vast number of Internet users. The high-dimension content generated by these users provides an unique opportunity to understand their behavior deeply. As one of the most fundamental topics, location estimation attracts more and more research efforts. Different from the previous literature, we find that user's location is strongly related to user interest. Based on this, we first build a detection model to mine user interest from short text. We then establish the mapping between location function and user interest before presenting an efficient framework to predict the user's location with convincing fidelity. Thorough evaluations and comparisons on an authentic data set show that our proposed model significantly outperforms the state-of-the-arts approaches. Moreover, the high efficiency of our model also guarantees its applicability in real-world scenarios. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Detection models; High dimensions; Internet users; Location estimation; Real-world scenario; Research efforts; Social media; User interests; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84893418660
"Takeuchi K., Ishiguro K., Kimura A., Sawada H.","Non-negative Multiple Matrix Factorization",2013,"IJCAI International Joint Conference on Artificial Intelligence",15,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063115&partnerID=40&md5=a597098d97d7290c194f48c12a954a9e","Non-negative Matrix Factorization (NMF) is a traditional unsupervised machine learning technique for decomposing a matrix into a set of bases and coefficients under the non-negative constraint. NMF with sparse constraints is also known for extracting reasonable components from noisy data. However, NMF tends to give undesired results in the case of highly sparse data, because the information included in the data is insufficient to decompose. Our key idea is that we can ease this problem if complementary data are available that we could integrate into the estimation of the bases and coefficients. In this paper, we propose a novel matrix factorization method called Non-negative Multiple Matrix Factorization (NMMF), which utilizes complementary data as auxiliary matrices that share the row or column indices of the target matrix. The data sparseness is improved by decomposing the target and auxiliary matrices simultaneously, since auxiliary matrices provide information about the bases and coefficients. We formulate NMMF as a generalization of NMF, and then present a parameter estimation procedure derived from the multiplicative update rule. We examined NMMF in both synthetic and real data experiments. The effect of the auxiliary matrices appeared in the improved NMMF performance. We also confirmed that the bases that NMMF obtained from the real data were intuitive and reasonable thanks to the non-negative constraint.",,"Complementary data; Matrix factorizations; Multiple matrices; Multiplicative updates; Nonnegative matrix factorization; Synthetic and real data; Target matrices; Unsupervised machine learning; Artificial intelligence; Factorization; Learning systems; Matrix algebra",Conference Paper,Scopus,2-s2.0-84896063115
"Amaneddine N., Condotta J.-F., Sioutis M.","Efficient approach to solve the minimal labeling problem of temporal and spatial Qualitative Constraints",2013,"IJCAI International Joint Conference on Artificial Intelligence",15,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896064048&partnerID=40&md5=0b7b28099f257bd6d7945722a16b3956","The Interval Algebra (IA) and a subset of the Region Connection Calculus (RCC), namely RCC-8, are the dominant Artificial Intelligence approaches for representing and reasoning about qualitative temporal and topological relations respectively. Such qualitative information can be formulated as a Qualitative Constraint Network (QCN). In this paper, we focus on the minimal labeling problem (MLP) and we propose an algorithm to efficiently derive all the feasible base relations of a QCN. Our algorithm considers chordal QCNs and a new form of partial consistency which we define as ◆ G-consistency. Further, the proposed algorithm uses tractable subclasses of relations having a specific patchwork property for which ◇-consistency implies the consistency of the input QCN. Experimentations with QCNs of IA and RCC-8 show the importance and efficiency of this new approach.",,"Interval algebra; Minimal labeling; New approaches; Qualitative constraints; Qualitative information; Region connection calculus; Temporal and spatial; Topological relations; Artificial intelligence; Algorithms",Conference Paper,Scopus,2-s2.0-84896064048
"Yang R., Jiang A.X., Tambe M., Ordóñez F.","Scaling-up security games with boundedly rational adversaries: A cutting-plane approach",2013,"IJCAI International Joint Conference on Artificial Intelligence",15,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896064002&partnerID=40&md5=a4bab74c6f341d9066b8759bca57deae","To improve the current real-world deployments of Stackelberg security games (SSGs), it is critical now to efficiently incorporate models of adversary bounded rationality in large-scale SSGs. Unfortunately, previously proposed branch-and-price approaches fail to scale-up given the non-convexity of such models, as we show with a realization called COCOMO. Therefore, we next present a novel cutting-plane algorithm called BLADE to scale-up SSGs with complex adversary models,with three key novelties: (i) an efficient scalable separation oracle to generate deep cuts; (ii) a heuristic that uses gradient to further improve the cuts; (iii) techniques for quality-efficiency tradeoff.",,"Adversary models; Bounded rationality; Branch and price; Cutting plane algorithms; Cutting-plane approach; Nonconvexity; Security games; Stackelberg; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84896064002
"Dang J., Zhang Z., Wu L.","A novel receiver for ACO-OFDM in visible light communication",2013,"IEEE Communications Letters",15,10.1109/LCOMM.2013.111113.132223,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892587137&doi=10.1109%2fLCOMM.2013.111113.132223&partnerID=40&md5=6bf3dd1f800312f04585dfe01a5eba63","This letter proposes a novel receiver for asymmetrically clipped optical orthogonal frequency division multiplexing (ACO-OFDM) system in visible light communication to enhance its performance. In ACO-OFDM, the even subcarriers are distorted by the clipping noise and are simply discarded in the conventional receiver. In this letter we show that the signals on the even subcarriers can be viewed as data-dependent precoded OFDM. A novel receiver based on minimum mean square error criteria is then proposed to exploit the even subcarriers to collect additional diversity and coding gains, leading to improved bit error rate performance. Simulations show that the proposed receiver provides significant signal-to-noise ratio gain (up to 10 dB) over the conventional receiver, especially in non-line-of-sight channels. Moreover, the proposed receiver is also superior to some existing advanced ones. © 1997-2012 IEEE.","ACO-OFDM; iterative receiving; MIMO; visible light communication","ACO-OFDM; Bit error rate (BER) performance; Clipping noise; iterative receiving; Minimum mean square errors; Non-line-of-sight; Optical orthogonal frequency division multiplexing; Visible light communications; Electrical engineering; Mathematical models; MIMO systems; Artificial intelligence",Article,Scopus,2-s2.0-84892587137
"Wang G., Cui Y.","On line tool wear monitoring based on auto associative neural network",2013,"Journal of Intelligent Manufacturing",15,10.1007/s10845-012-0636-7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888642906&doi=10.1007%2fs10845-012-0636-7&partnerID=40&md5=6550ed7bad53a364391293a4f47e233c","This paper presents a new tool wear monitoring method based on auto associative neural network. The main advantage of the model lies that it can be built only by the data under normal cutting condition. Therefore, the training samples of the tool wear status are no longer needed during the training process that makes it easier to be applied in real industrial environment than other neural network models. An averaged distance indicator is proposed to denote not only the occurrence of the tool wear but also its severity. Moreover, the Levenberg-Marquardt (LM) training algorithm is introduced to improve the convergence accuracy of the auto associative neural network. Based on the proposed method, a framework for online tool condition monitoring is illustrated and the cutting force data under different tool wear status are collected to simulate the online modeling and monitoring process for the rough and finish milling respectively. The results show that the proposed indicator can reflect the evolution process of tool wear correctly and the LM algorithm is more accurate in comparison with the gradient descent methods. Therefore, it casts new light on practical application of neural network in the field of on line tool condition monitoring. © 2012 Springer Science+Business Media, LLC.","Artificial intelligence; Auto associative neural network; Monitoring; Tool wear","Autoassociative neural networks; Gradient Descent method; Industrial environments; Levenberg-Marquardt; Neural network model; Tool wear; Tool wear monitoring; Training algorithms; Artificial intelligence; Damage detection; Monitoring; Neural networks; Wear of materials",Article,Scopus,2-s2.0-84888642906
"Zhang H., El-Gaaly T., Elgammal A., Jiang Z.","Joint object and pose recognition using homeomorphic manifold analysis",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",15,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893350904&partnerID=40&md5=8cab2a2cc24413a3c63b6cbf43396714","Object recognition is a key precursory challenge in the fields of object manipulation and robotic/AI visual reasoning in general. Recognizing object categories, particular instances of objects and viewpoints/poses of objects are three critical subproblems robots must solve in order to accurately grasp/manipulate objects and reason about their environments. Multi-view images of the same object lie on intrinsic low-dimensional manifolds in descriptor spaces (e.g. visual/ depth descriptor spaces). These object manifolds share the same topology despite being geometrically different. Each object manifold can be represented as a deformed version of a unified manifold. The object manifolds can thus be parametrized by its homeomorphic mapping/reconstruction from the unified manifold. In this work, we construct a manifold descriptor from this mapping between homeomorphic manifolds and use it to jointly solve the three challenging recognition sub-problems. We extensively experiment on a challenging multi-modal (i.e. RGBD) dataset and other object pose datasets and achieve state-of- the -art results. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Depth descriptor; Intrinsic low-dimensional manifolds; Manifold analysis; Multi-view image; Object categories; Object manipulation; Pose recognition; Visual reasoning; Artificial intelligence; Object recognition",Conference Paper,Scopus,2-s2.0-84893350904
"Mahmoudi B., Pohlmeyer E.A., Prins N.W., Geng S., Sanchez J.C.","Towards autonomous neuroprosthetic control using Hebbian reinforcement learning",2013,"Journal of Neural Engineering",15,10.1088/1741-2560/10/6/066005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889056477&doi=10.1088%2f1741-2560%2f10%2f6%2f066005&partnerID=40&md5=a371fd4fbfb5d817d39cce30ed6b06bd","Objective. Our goal was to design an adaptive neuroprosthetic controller that could learn the mapping from neural states to prosthetic actions and automatically adjust adaptation using only a binary evaluative feedback as a measure of desirability/undesirability of performance. Approach. Hebbian reinforcement learning (HRL) in a connectionist network was used for the design of the adaptive controller. The method combines the efficiency of supervised learning with the generality of reinforcement learning. The convergence properties of this approach were studied using both closed-loop control simulations and open-loop simulations that used primate neural data from robot-assisted reaching tasks. Main results. The HRL controller was able to perform classification and regression tasks using its episodic and sequential learning modes, respectively. In our experiments, the HRL controller quickly achieved convergence to an effective control policy, followed by robust performance. The controller also automatically stopped adapting the parameters after converging to a satisfactory control policy. Additionally, when the input neural vector was reorganized, the controller resumed adaptation to maintain performance. Significance. By estimating an evaluative feedback directly from the user, the HRL control algorithm may provide an efficient method for autonomous adaptation of neuroprosthetic systems. This method may enable the user to teach the controller the desired behavior using only a simple feedback signal. © 2013 IOP Publishing Ltd.",,"Adaptive controllers; Closed-loop control; Connectionist networks; Convergence properties; Open-loop simulations; Robust performance; Satisfactory control; Sequential learning; Algorithms; Reinforcement learning; Controllers; algorithm; article; binocular convergence; feedback system; hebbian reinforcement learning; learning; neuroprosthesis; nonhuman; priority journal; simulation; Algorithms; Animals; Artificial Intelligence; Callithrix; Learning; Neural Prostheses; Random Allocation; Reinforcement (Psychology)",Article,Scopus,2-s2.0-84889056477
"Pan S., Zhu X.","Graph classification with imbalanced class distributions and noise",2013,"IJCAI International Joint Conference on Artificial Intelligence",15,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063101&partnerID=40&md5=f42ca2619a1d45d92dd367672d5b3a2e","Recent years have witnessed an increasing number of applications involving data with structural dependency and graph representations. For these applications, it is very common that their class distribution is imbalanced with minority samples being only a small portion of the population. Such imbalanced class distributions impose significant challenges to the learning algorithms. This problem is further complicated with the presence of noise or outliers in the graph data. In this paper, we propose an imbalanced graph boosting algorithm, igBoost, that progressively selects informative subgraph patterns from imbalanced graph data for learning. To handle class imbalance, we take class distributions into consideration to assign different weight values to graphs. The distance of each graph to its class center is also considered to adjust the weight to reduce the impact of noisy graph data. The weight values are integrated into the iterative subgraph feature selection and margin learning process to achieve maximum benefits. Experiments on realworld graph data with different degrees of class imbalance and noise demonstrate the algorithm performance.",,"Algorithm performance; Boosting algorithm; Class distributions; Graph classification; Graph representation; Imbalanced class; Learning process; Real-world graphs; Algorithms; Artificial intelligence; Iterative methods",Conference Paper,Scopus,2-s2.0-84896063101
"Merritt M., Varga S., Krueger J.","Ontogenesis of the socially extended mind",2013,"Cognitive Systems Research",15,10.1016/j.cogsys.2013.03.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883458696&doi=10.1016%2fj.cogsys.2013.03.001&partnerID=40&md5=b2f08cb983958507b9e8d3ed29b96953","I consider the developmental origins of the socially extended mind. First, I argue that, from birth, the physical interventions caregivers use to regulate infant attention and emotion (gestures, facial expressions, direction of gaze, body orientation, patterns of touch and vocalization, etc.) are part of the infant's socially extended mind; they are external mechanisms that enable the infant to do things she could not otherwise do, cognitively speaking. Second, I argue that these physical interventions encode the norms, values, and patterned practices distinctive of their specific sociocultural milieu. Accordingly, not only do they enhance and extend the infant's cognitive competence. They also entrain the infant to think and act in culturally appropriate ways. These physical interventions are thus arguably the earliest examples of social practices that scaffold the infant's cognitive development and shape the development of their cultural education. © 2013 Elsevier B.V.","Attention; Emotion; Extended cognition; Self-regulation; Social cognition; Vygotsky","Attention; Emotion; Extended cognition; Self-regulation; Social cognition; Vygotsky; Artificial intelligence; Cognitive systems; Scaffolds; affect; article; attention; cognition; cognitive development; cultural factor; emotion; emotionality; extended cognition; facial expression; gaze; gesture; human; learning; orientation; priority journal; psychosocial development; social cognition; vocalization",Article,Scopus,2-s2.0-84883458696
"De Cat B., Bogaerts B., Devriendt J., Denecker M.","Model expansion in the presence of function symbols using constraint programming",2013,"Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI",15,10.1109/ICTAI.2013.159,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897677415&doi=10.1109%2fICTAI.2013.159&partnerID=40&md5=971d0af167c2677303c52446bc423368","The traditional approach to Model Expansion (MX) is to reduce the theory to a propositional language and apply a search algorithm to the resulting theory. Function symbols are typically replaced by predicate symbols representing the graph of the function, an operation that blows up the reduced theory. In this paper, we present an improved approach to handle function symbols in a ground-and-solve methodology, building on ideas from Constraint Programming. We do so in the context of FO(.)IDP, the knowledge representation language that extends First-Order Logic (FO) with, among others, inductive definitions, arithmetic and aggregates. An MX algorithm is developed, consisting of (i) a grounding algorithm for FO(.)^IDP, parametrised by the function symbols allowed to occur in the reduced theory, and (ii) a search algorithm for unrestricted, ground FO(.)^IDP. The ideas are implemented in the IDP knowledge-base system and experimental evaluation shows that both more compact groundings and improved search performance are obtained. © 2013 IEEE.","Constraint Programming; Grounding; Knowledge Representation; Model Expansion","Constraint programming; Experimental evaluation; Inductive definitions; Knowledge representation language; Model expansion; Propositional language; Search performance; Traditional approaches; Artificial intelligence; Constraint theory; Electric grounding; Knowledge representation; Learning algorithms; Tools; Computer programming",Conference Paper,Scopus,2-s2.0-84897677415
"Abel F., Gao Q., Houben G.-J., Tao K.","Twitter-based user modeling for news recommendations",2013,"IJCAI International Joint Conference on Artificial Intelligence",15,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896061030&partnerID=40&md5=a1d6cfa76ffd8b9cc85ecba049d2268f","In this paper, we study user modeling on Twitter. We investigate different strategies for mining user interest profiles from microblogging activities ranging from strategies that analyze the semantic meaning of Twitter messages to strategies that adapt to temporal patterns that can be observed in the microblogging behavior. We evaluate the quality of the user modeling methods in the context of a personalized news recommendation system. Our results reveals that an understanding of the semantic meaning of microposts is key for generating high-quality user profiles.",,"High quality; Microblogging; News recommendation; Personalized news; Temporal pattern; User interest profile; User Modeling; User profile; Artificial intelligence; Semantics; Social networking (online); Mathematical models",Conference Paper,Scopus,2-s2.0-84896061030
"Amadini R., Gabbrielli M., Mauro J.","An empirical evaluation of portfolios approaches for solving CSPs",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",15,10.1007/978-3-642-38171-3_21,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879978727&doi=10.1007%2f978-3-642-38171-3_21&partnerID=40&md5=96a750faabe2228affa2546e8c4494c7","Recent research in areas such as SAT solving and Integer Linear Programming has shown that the performances of a single arbitrarily efficient solver can be significantly outperformed by a portfolio of possibly slower on-average solvers. We report an empirical evaluation and comparison of portfolio approaches applied to Constraint Satisfaction Problems (CSPs). We compared models developed on top of off-theshelf machine learning algorithms with respect to approaches used in the SAT field and adapted for CSPs, considering different portfolio sizes and using as evaluation metrics the number of solved problems and the time taken to solve them. Results indicate that the best SAT approaches have top performances also in the CSP field and are slightly more competitive than simple models built on top of classification algorithms. © Springer-Verlag 2013.",,"Classification algorithm; Efficient solvers; Empirical evaluations; Evaluation metrics; Integer Linear Programming; Recent researches; SAT-solving; Artificial intelligence; Computer programming; Constraint theory; Integer programming; Learning algorithms; Operations research; Constraint satisfaction problems",Conference Paper,Scopus,2-s2.0-84879978727
"Chapman W.W., Hillert D., Velupillai S., Kvist M., Skeppstedt M., Chapman B.E., Conway M., Tharp M., Mowery D.L., Deleger L.","Extending the NegEx lexicon for multiple languages",2013,"Studies in Health Technology and Informatics",15,10.3233/978-1-61499-289-9-677,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894290313&doi=10.3233%2f978-1-61499-289-9-677&partnerID=40&md5=218edd501e29b4007f6eaea1f8b2ebb5","We translated an existing English negation lexicon (NegEx) to Swedish, French, and German and compared the lexicon on corpora from each language. We observed Zipf's law for all languages, i.e., a few phrases occur a large number of times, and a large number of phrases occur fewer times. Negation triggers 'no' and 'not' were common for all languages; however, other triggers varied considerably. The lexicon is available in OWL and RDF format and can be extended to other languages. We discuss the challenges in translating negation triggers to other languages and issues in representing multilingual lexical knowledge. © 2013 IMIA and IOS Press.","knowledge representation; Natural language processing","artificial intelligence; controlled vocabulary; electronic medical record; France; Germany; natural language processing; nomenclature; semantics; Sweden; translating (language); United States; Artificial Intelligence; France; Germany; Medical Records Systems, Computerized; Natural Language Processing; Semantics; Sweden; Terminology as Topic; Translating; United States; Vocabulary, Controlled",Conference Paper,Scopus,2-s2.0-84894290313
"Solomakhin D., Montali M., Tessaris S., De Masellis R.","Verification of artifact-centric systems: Decidability and modeling issues",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",15,10.1007/978-3-642-45005-1_18,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892427421&doi=10.1007%2f978-3-642-45005-1_18&partnerID=40&md5=1e0ad230950400cf605f93a33c359b12","Artifact-centric business processes have recently emerged as an approach in which processes are centred around the evolution of business entities, called artifacts, giving equal importance to control-flow and data. The recent Guard-State-Milestone (GSM) framework provides means for specifying business artifacts lifecycles in a declarative manner, using constructs that match how executive-level stakeholders think about their business. However, it turns out that formal verification of GSM is undecidable even for very simple propositional temporal properties. We attack this challenging problem by translating GSM into a well-studied formal framework.We exploit this translation to isolate an interesting class of ""state-bounded"" GSM models for which verification of sophisticated temporal properties is decidable. We then introduce some guidelines to turn an arbitrary GSM model into a state-bounded, verifiable model. © 2013 Springer-Verlag.","artifact-centric systems; formal verification; guard-stage-milestone","Artifact-centric; Business Artifacts; Business entities; Business Process; Control-flow; Formal verifications; guard-stage-milestone; Temporal property; Artificial intelligence; Computer science; Computers; Computability and decidability",Conference Paper,Scopus,2-s2.0-84892427421
"Brylinski M.","Nonlinear scoring functions for similarity-based ligand docking and binding affinity prediction",2013,"Journal of Chemical Information and Modeling",15,10.1021/ci400510e,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888602591&doi=10.1021%2fci400510e&partnerID=40&md5=c592c81d1b23b38dd8d58a7e833b46a1","A common strategy for virtual screening considers a systematic docking of a large library of organic compounds into the target sites in protein receptors with promising leads selected based on favorable intermolecular interactions. Despite a continuous progress in the modeling of protein-ligand interactions for pharmaceutical design, important challenges still remain, thus the development of novel techniques is required. In this communication, we describe eSimDock, a new approach to ligand docking and binding affinity prediction. eSimDock employs nonlinear machine learning-based scoring functions to improve the accuracy of ligand ranking and similarity-based binding pose prediction, and to increase the tolerance to structural imperfections in the target structures. In large-scale benchmarking using the Astex/CCDC data set, we show that 53.9% (67.9%) of the predicted ligand poses have RMSD of &lt;2 Å (&lt;3 Å). Moreover, using binding sites predicted by recently developed eFindSite, eSimDock models ligand binding poses with an RMSD of 4 Å for 50.0-39.7% of the complexes at the protein homology level limited to 80-40%. Simulations against non-native receptor structures, whose mean backbone rearrangements vary from 0.5 to 5.0 Å Cα-RMSD, show that the ratio of docking accuracy and the estimated upper bound is at a constant level of ∼0.65. Pearson correlation coefficient between experimental and predicted by eSimDock Ki values for a large data set of the crystal structures of protein-ligand complexes from BindingDB is 0.58, which decreases only to 0.46 when target structures distorted to 3.0 Å Cα-RMSD are used. Finally, two case studies demonstrate that eSimDock can be customized to specific applications as well. These encouraging results show that the performance of eSimDock is largely unaffected by the deformations of ligand binding regions, thus it represents a practical strategy for across-proteome virtual screening using protein models. eSimDock is freely available to the academic community as a Web server at http://www.brylinski.org/esimdock. © 2013 American Chemical Society.",,"Academic community; Binding affinities; Intermolecular interactions; Pearson correlation coefficients; Protein-ligand complexes; Protein-ligand interactions; Structural imperfections; Virtual Screening; Binding energy; Complexation; Correlation methods; Digital libraries; Forecasting; Proteins; Ligands; blood clotting factor 10a; CDK2 protein, human; cyclin dependent kinase 2; ligand; protein kinase inhibitor; proteinase inhibitor; algorithm; article; artificial intelligence; Bayes theorem; binding site; chemistry; computer program; drug design; human; methodology; molecular docking; protein binding; protein database; protein domain; protein secondary structure; quality control; receiver operating characteristic; thermodynamics; X ray crystallography; Algorithms; Artificial Intelligence; Bayes Theorem; Benchmarking; Binding Sites; Crystallography, X-Ray; Cyclin-Dependent Kinase 2; Databases, Protein; Drug Design; Factor Xa; Humans; Ligands; Molecular Docking Simulation; Protease Inhibitors; Protein Binding; Protein Interaction Domains and Motifs; Protein Kinase Inhibitors; Protein Structure, Secondary; Research Design; ROC Curve; Software; Thermodynamics",Article,Scopus,2-s2.0-84888602591
"Alrajeh N.A., Lloret J.","Intrusion detection systems based on artificial intelligence techniques in wireless sensor networks",2013,"International Journal of Distributed Sensor Networks",15,10.1155/2013/351047,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887452209&doi=10.1155%2f2013%2f351047&partnerID=40&md5=c1b328b9b77101da75c5d95b42f018e8","Intrusion detection system (IDS) is regarded as the second line of defense against network anomalies and threats. IDS plays an important role in network security. There are many techniques which are used to design IDSs for specific scenario and applications. Artificial intelligence techniques are widely used for threats detection. This paper presents a critical study on genetic algorithm, artificial immune, and artificial neural network (ANN) based IDSs techniques used in wireless sensor network (WSN). © 2013 Nabil Ali Alrajeh and J. Lloret.",,"Artificial immune; Artificial intelligence techniques; Intrusion Detection Systems; Network anomalies; Computer crime; Intrusion detection; Neural networks; Wireless sensor networks",Article,Scopus,2-s2.0-84887452209
"Gleicher M.","Explainers: Expert explorations with crafted projections",2013,"IEEE Transactions on Visualization and Computer Graphics",15,10.1109/TVCG.2013.157,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886710481&doi=10.1109%2fTVCG.2013.157&partnerID=40&md5=38245dd9f4a88de0bfec1d385786dd43","This paper introduces an approach to exploration and discovery in high-dimensional data that incorporates a user's knowledge and questions to craft sets of projection functions meaningful to them. Unlike most prior work that defines projections based on their statistical properties, our approach creates projection functions that align with user-specified annotations. Therefore, the resulting derived dimensions represent concepts defined by the user's examples. These especially crafted projection functions, or explainers, can help find and explain relationships between the data variables and user-designated concepts. They can organize the data according to these concepts. Sets of explainers can provide multiple perspectives on the data. Our approach considers tradeoffs in choosing these projection functions, including their simplicity, expressive power, alignment with prior knowledge, and diversity. We provide techniques for creating collections of explainers. The methods, based on machine learning optimization frameworks, allow exploring the tradeoffs. We demonstrate our approach on model problems and applications in text analysis. © 1995-2012 IEEE.","exploration; High-dimensional spaces; support vector machines","Expressive power; High dimensional data; High dimensional spaces; Learning optimizations; Model problems; Prior knowledge; Projection function; Statistical properties; Computer graphics; Natural resources exploration; Software engineering; Support vector machines; Commerce; algorithm; article; artificial intelligence; automated pattern recognition; computer graphics; computer interface; expert witness; methodology; reproducibility; sensitivity and specificity; automated pattern recognition; expert witness; procedures; Algorithms; Artificial Intelligence; Computer Graphics; Expert Testimony; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; User-Computer Interface; Algorithms; Artificial Intelligence; Computer Graphics; Expert Testimony; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; User-Computer Interface",Article,Scopus,2-s2.0-84886710481
"Hu X., Bradel L., Maiti D., House L., North C., Leman S.","Semantics of directly manipulating spatializations",2013,"IEEE Transactions on Visualization and Computer Graphics",15,10.1109/TVCG.2013.188,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886646432&doi=10.1109%2fTVCG.2013.188&partnerID=40&md5=5baa92979ef53395f3967b3bbd640a25","When high-dimensional data is visualized in a 2D plane by using parametric projection algorithms, users may wish to manipulate the layout of the data points to better reflect their domain knowledge or to explore alternative structures. However, few users are well-versed in the algorithms behind the visualizations, making parameter tweaking more of a guessing game than a series of decisive interactions. Translating user interactions into algorithmic input is a key component of Visual to Parametric Interaction (V2PI) [13]. Instead of adjusting parameters, users directly move data points on the screen, which then updates the underlying statistical model. However, we have found that some data points that are not moved by the user are just as important in the interactions as the data points that are moved. Users frequently move some data points with respect to some other 'unmoved' data points that they consider as spatially contextual. However, in current V2PI interactions, these points are not explicitly identified when directly manipulating the moved points. We design a richer set of interactions that makes this context more explicit, and a new algorithm and sophisticated weighting scheme that incorporates the importance of these unmoved data points into V2PI. © 1995-2012 IEEE.","statistical models; visual analytics; Visual to parametric interaction","Adjusting parameters; Alternative structure; High dimensional data; Parametric interactions; Projection algorithms; Statistical modeling; Visual analytics; Weighting scheme; Algorithms; Semantics; Visualization; Parameter estimation; algorithm; artificial intelligence; automated pattern recognition; computer assisted diagnosis; computer graphics; computer interface; image enhancement; multimodal imaging; procedures; reproducibility; semantics; sensitivity and specificity; article; automated pattern recognition; computer assisted diagnosis; methodology; multimodal imaging; Algorithms; Artificial Intelligence; Computer Graphics; Image Enhancement; Image Interpretation, Computer-Assisted; Multimodal Imaging; Pattern Recognition, Automated; Reproducibility of Results; Semantics; Sensitivity and Specificity; User-Computer Interface; Algorithms; Artificial Intelligence; Computer Graphics; Image Enhancement; Image Interpretation, Computer-Assisted; Multimodal Imaging; Pattern Recognition, Automated; Reproducibility of Results; Semantics; Sensitivity and Specificity; User-Computer Interface",Article,Scopus,2-s2.0-84886646432
"Zhu X., Li J., Wu D., Wang H., Liang C.","Balancing accuracy, complexity and interpretability in consumer credit decision making: A C-TOPSIS classification approach",2013,"Knowledge-Based Systems",15,10.1016/j.knosys.2013.08.004,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883881070&doi=10.1016%2fj.knosys.2013.08.004&partnerID=40&md5=ddc01982617290a67b3d220f424cdfad","Accuracy, complexity and interpretability are very important in credit classification. However, most approaches cannot perform well in all the three aspects simultaneously. The objective of this study is to put forward a classification approach named C-TOPSIS that can balance the three aspects well. C-TOPSIS is based on the rationale of TOPSIS (Technique for Order Preference by Similarity to Ideal Solution). TOPSIS is famous for reliable evaluation results and quick computing process and it is easy to understand and use. However, it is a ranking approach and three challenges have to be faced for modifying TOPSIS into a classification approach. C-TOPSIS works out three strategies to overcome the challenges and retains the advantages of TOPSIS. So C-TOPSIS is deduced to have reliable classification results, high computational efficiency and ease of use and understanding. Our findings in the experiment verify the advantages of C-TOPSIS. In comparison with 7 popular approaches on 2 widely used UCI credit datasets, C-TOPSIS ranks 2nd in accuracy, 1st in complexity and is in 1st rank in interpretability. Only C-TOPSIS ranks among the top 3 in all the three aspects, which verifies that C-TOPSIS can balance accuracy, complexity and interpretability well. © 2013 Elsevier B.V. All rights reserved.","Bank risk evaluation; Credit classification; Credit risk; Credit scoring; Support vector machine; TOPSIS","Credit classifications; Credit risks; Credit scoring; Risk evaluation; TOPSIS; Artificial intelligence; Software engineering; Support vector machines; Risk assessment",Article,Scopus,2-s2.0-84883881070
"Mian A., Hu Y., Hartley R., Owens R.","Image set based face recognition using self-regularized non-negative coding and adaptive distance metric learning",2013,"IEEE Transactions on Image Processing",15,10.1109/TIP.2013.2282996,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886936934&doi=10.1109%2fTIP.2013.2282996&partnerID=40&md5=a30fb970b8f7f4c19cf5539da9dc9f6d","Simple nearest neighbor classification fails to exploit the additional information in image sets. We propose self-regularized nonnegative coding to define between set distance for robust face recognition. Set distance is measured between the nearest set points (samples) that can be approximated from their orthogonal basis vectors as well as from the set samples under the respective constraints of self-regularization and nonnegativity. Self-regularization constrains the orthogonal basis vectors to be similar to the approximated nearest point. The nonnegativity constraint ensures that each nearest point is approximated from a positive linear combination of the set samples. Both constraints are formulated as a single convex optimization problem and the accelerated proximal gradient method with linear-time Euclidean projection is adapted to efficiently find the optimal nearest points between two image sets. Using the nearest points between a query set and all the gallery sets as well as the active samples used to approximate them, we learn a more discriminative Mahalanobis distance for robust face recognition. The proposed algorithm works independently of the chosen features and has been tested on gray pixel values and local binary patterns. Experiments on three standard data sets show that the proposed method consistently outperforms existing state-of-the-art methods. © 2013 IEEE.","distance metric learning; face recognition; Image set classification; nonnegative coding","Accelerated proximal gradient methods; Convex optimization problems; Distance Metric Learning; Image sets; Nearest neighbor classification; Non negatives; Non-negativity constraints; State-of-the-art methods; Convex optimization; Gradient methods; Face recognition; algorithm; article; artificial intelligence; biometry; face; factual database; histology; human; image processing; methodology; principal component analysis; Algorithms; Artificial Intelligence; Biometric Identification; Databases, Factual; Face; Humans; Image Processing, Computer-Assisted; Principal Component Analysis",Article,Scopus,2-s2.0-84886936934
"Helbig M., Engelbrecht A.P.","Benchmarks for dynamic multi-objective optimisation",2013,"Proceedings of the 2013 IEEE Symposium on Computational Intelligence in Dynamic and Uncertain Environments, CIDUE 2013 - 2013 IEEE Symposium Series on Computational Intelligence, SSCI 2013",15,10.1109/CIDUE.2013.6595776,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885067140&doi=10.1109%2fCIDUE.2013.6595776&partnerID=40&md5=cdecac233a5d748944f7d793c519af76","When algorithms solve dynamic multi-objective optimisation problems (DMOOPs), benchmark functions should be used to determine whether the algorithm can overcome specific difficulties that can occur in real-world problems. However, for dynamic multi-objective optimisation (DMOO) there are no standard benchmark functions that are used. This article proposes characteristics of an ideal set of DMOO benchmark functions, as well as suggested DMOOPs for each characteristic. The limitations of current DMOOPs and studies of dynamic multi-objective optimisation algorithms (DMOAs) are highlighted. In addition, new DMOO benchmark functions with complicated Pareto-optimal sets (POSs) and approaches to develop DMOOPs with either an isolated or deceptive Pareto-optimal front (POF) are introduced to address identified limitations of current DMOOPs. © 2013 IEEE.","Dynamic multi-objective benchmark functions; ideal benchmark function suite","Benchmark functions; Multi objective; Pareto-optimal front; Pareto-optimal sets; Real-world problem; Algorithms; Artificial intelligence; Multiobjective optimization",Conference Paper,Scopus,2-s2.0-84885067140
"Zheng Y., Lin S., Kang S.B., Xiao R., Gee J.C., Kambhamettu C.","Single-image vignetting correction from gradient distribution symmetries",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",15,10.1109/TPAMI.2012.210,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884994064&doi=10.1109%2fTPAMI.2012.210&partnerID=40&md5=8b064ccb12c026654a7a5587d8a33936","We present novel techniques for single-image vignetting correction based on symmetries of two forms of image gradients: semicircular tangential gradients (SCTG) and radial gradients (RG). For a given image pixel, an SCTG is an image gradient along the tangential direction of a circle centered at the presumed optical center and passing through the pixel. An RG is an image gradient along the radial direction with respect to the optical center. We observe that the symmetry properties of SCTG and RG distributions are closely related to the vignetting in the image. Based on these symmetry properties, we develop an automatic optical center estimation algorithm by minimizing the asymmetry of SCTG distributions, and also present two methods for vignetting estimation based on minimizing the asymmetry of RG distributions. In comparison to prior approaches to single-image vignetting correction, our methods do not rely on image segmentation and they produce more accurate results. Experiments show our techniques to work well for a wide range of images while achieving a speed-up of 3-5 times compared to a state-of-the-art method. © 2013 IEEE.","Bias correction; Camera calibration; Low-level vision; Nonuniformity correction; Vignetting correction","Bias correction; Camera calibration; Gradient distributions; Low-level vision; Nonuniformity correction; State-of-the-art methods; Symmetry properties; Tangential directions; Artificial intelligence; Computer vision; Pixels; algorithm; article; human; image enhancement; image processing; methodology; Algorithms; Humans; Image Enhancement; Image Processing, Computer-Assisted",Article,Scopus,2-s2.0-84884994064
"Bisquert P., Cayrol C., De Saint-Cyr F.D., Lagasquie-Schiex M.-C.","Enforcement in argumentation is a kind of update",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",15,10.1007/978-3-642-40381-1-3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884991274&doi=10.1007%2f978-3-642-40381-1-3&partnerID=40&md5=55bf851b062a8dbf7a7194bd8c713197","In the literature, enforcement consists in changing an argumentation system in order to force it to accept a given set of arguments. In this paper, we extend this notion by allowing incomplete information about the initial argumentation system. Generalized enforcement is an operation that maps a propositional formula describing a system and a propositional formula that describes a goal, to a new formula describing the possible resulting systems. This is done under some constraints about the allowed changes. We give a set of postulates restraining the class of enforcement operators and provide a representation theorem linking them to a family of proximity relations on argumentation systems. © 2013 Springer-Verlag Berlin Heidelberg.","belief change; dynamics in argumentation","Argumentation systems; Belief change; Incomplete information; Propositional formulas; Representation theorem; Artificial intelligence; Computer science",Conference Paper,Scopus,2-s2.0-84884991274
"Raudys S.","Portfolio of automated trading systems: Complexity and learning set size issues",2013,"IEEE Transactions on Neural Networks and Learning Systems",15,10.1109/TNNLS.2012.2230405,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884942671&doi=10.1109%2fTNNLS.2012.2230405&partnerID=40&md5=cfe04b08622c52c2d9447293403fe417","In this paper, we consider using profit/loss histories of multiple automated trading systems (ATSs) as N input variables in portfolio management. By means of multivariate statistical analysis and simulation studies, we analyze the influences of sample size (L) and input dimensionality on the accuracy of determining the portfolio weights. We find that degradation in portfolio performance due to inexact estimation of N means and N(N - 1)/2 correlations is proportional to N/L; however, estimation of N variances does not worsen the result. To reduce unhelpful sample size/dimensionality effects, we perform a clustering of N time series and split them into a small number of blocks. Each block is composed of mutually correlated ATSs. It generates an expert trading agent based on a nontrainable 1/ N portfolio rule. To increase the diversity of the expert agents, we use training sets of different lengths for clustering. In the output of the portfolio management system, the regularized mean-variance framework-based fusion agent is developed in each walk-forward step of an out-of-sample portfolio validation experiment. Experiments with the real financial data (2003-2012) confirm the effectiveness of the suggested approach. © 2013 IEEE.","Complexity; Efficient-market hypothesis; Investments; Markowitz; Multiagent systems; Optimization; Portfolios; Regularization; Sample size","Complexity; Efficient-market hypothesis; Markowitz; Portfolios; Regularization; Sample sizes; Experiments; Financial data processing; Investments; Multi agent systems; Optimization; Commerce; artificial intelligence; artificial neural network; information processing; procedures; statistical model; Artificial Intelligence; Automatic Data Processing; Models, Economic; Neural Networks (Computer)",Article,Scopus,2-s2.0-84884942671
"Krishnanand K.R., Hasani S.M.F., Panigrahi B.K., Panda S.K.","Optimal power flow solution using self-evolving brain-storming inclusive teaching-learning-based algorithm",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",15,10.1007/978-3-642-38703-6_40,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884887335&doi=10.1007%2f978-3-642-38703-6_40&partnerID=40&md5=8c29cc13b00a2a22bda1d252c4c6d39e","In this paper, a new hybrid self-evolving algorithm is presented with its application to a highly nonlinear problem in electrical engineering. The optimal power flow problem described here focuses on the minimization of the fuel costs of the thermal units while maintaining the voltage stability at each of the load buses. There are various restrictions on acceptable voltage levels, capacitance levels of shunt compensation devices and transformer taps making it highly complex and nonlinear. The hybrid algorithm discussed here is a combination of the learning principles from Brain Storming Optimization algorithm and Teaching-Learning-Based Optimization algorithm, along with a self-evolving principle applied to the control parameter. The strategies used in the proposed algorithm makes it self-adaptive in performing the search over the multi-dimensional problem domain. The results on an IEEE 30 Bus system indicate that the proposed algorithm is an excellent candidate in dealing with the optimal power flow problems. © 2013 Springer-Verlag Berlin Heidelberg.","Brain-Storming Optimization; Non-dominated sorting; Optimal power flow; Teaching-learning-based optimization","Control parameters; Multidimensional problems; Non-dominated Sorting; Nonlinear problems; Optimal power flow problem; Optimal power flows; Optimization algorithms; Teaching-learning-based optimizations; Acoustic generators; Artificial intelligence; Electric load flow; Electrical engineering; Optimization; Storms; Voltage control; Learning algorithms",Conference Paper,Scopus,2-s2.0-84884887335
"Bellavia F., Fanfani M., Pazzaglia F., Colombo C.","Robust selective stereo SLAM without loop closure and bundle adjustment",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",15,10.1007/978-3-642-41181-6_47,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884719042&doi=10.1007%2f978-3-642-41181-6_47&partnerID=40&md5=293447e99cb52c98d35af4a7fc14b0ef","This paper presents a novel stereo SLAM framework, where a robust loop chain matching scheme for tracking keypoints is combined with an effective frame selection strategy. The proposed approach, referred to as selective SLAM (SSLAM), relies on the observation that the error in the pose estimation propagates from the uncertainty of the three-dimensional points. This is higher for distant points, corresponding to matches with low temporal flow disparity in the images. Comparative results based on the reference KITTI evaluation framework show that SSLAM is effective and can be implemented efficiently, as it does not require any loop closure or bundle adjustment. © 2013 Springer-Verlag.","feature matching; frame selection; RANSAC; SLAM; Structure from Motion","Feature matching; Frame selection; RANSAC; SLAM; Structure from motion; Artificial intelligence; Computer science; Image analysis",Conference Paper,Scopus,2-s2.0-84884719042
"Aristondo O., García-Lapresta J.L., Lasso De La Vega C., Marques Pereira R.A.","Classical inequality indices, welfare and illfare functions, and the dual decomposition",2013,"Fuzzy Sets and Systems",15,10.1016/j.fss.2013.02.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880918385&doi=10.1016%2fj.fss.2013.02.001&partnerID=40&md5=df65eefaf84b71a26e269797b949a758","In the traditional framework, social welfare functions depend on the mean income and on the income inequality. An alternative illfare framework has been developed to take into account the disutility of unfavorable variables. The illfare level is assumed to increase with the inequality of the distribution. In some social and economic fields, such as those related to employment, health, education, or deprivation, the characteristics of the individuals in the population are represented by bounded variables, which encode either achievements or shortfalls. Accordingly, both the social welfare and the social illfare levels may be assessed depending on the framework we focus on. In this paper we propose a unified dual framework in which welfare and illfare levels can both be investigated and analyzed in a natural way. The dual framework leads to the consistent measurement of achievements and shortfalls, thereby overcoming one important difficulty of the traditional approach, in which the focus on achievements or shortfalls often leads to different inequality rankings. A number of welfare functions associated with inequality indices are OWA operators. Specifically this paper considers the welfare functions associated with the classical inequality measures due to Gini, Bonferroni, and De Vergottini. These three indices incorporate different value judgments in the measurement of inequality, leading to different behavior under income transfers between individuals in the population. In the bounded variables representation, we examine the dual decomposition and the orness degree of the three classical welfare/illfare functions in the standard framework of aggregation functions on the [0,1]n domain. The dual decomposition of each welfare/illfare function into a self-dual central index and an anti-self-dual inequality index leads to the consistent measurement of achievements and shortfalls. © 2013 Elsevier B.V. All rights reserved.","Aggregation functions; Classical Gini, Bonferroni, and De Vergottini inequality indices; Dual decomposition; Illfare functions; Income inequality and social welfare; Orness; WA and OWA operators; Welfare functions","Aggregation functions; Classical Gini, Bonferroni, and De Vergottini inequality indices; Dual decomposition; Orness; OWA operators; Social welfare; Welfare functions; Artificial intelligence; Fuzzy sets",Article,Scopus,2-s2.0-84880918385
"Kim S., Kim J.","Occupancy mapping and surface reconstruction using local Gaussian processes with kinect sensors",2013,"IEEE Transactions on Cybernetics",15,10.1109/TCYB.2013.2272592,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890367573&doi=10.1109%2fTCYB.2013.2272592&partnerID=40&md5=533fa212fc0d7af103ad2ab423708380","Although RGB-D sensors have been successfully applied to visual SLAM and surface reconstruction, most of the applications aim at visualization. In this paper, we propose a noble method of building continuous occupancy maps and reconstructing surfaces in a single framework for both navigation and visualization. Particularly, we apply a Bayesian nonparametric approach, Gaussian process classification, to occupancy mapping. However, it suffers from high-computational complexity of O(n3) + O(n2m), where n and m are the numbers of training and test data, respectively, limiting its use for large-scale mapping with huge training data, which is common with high-resolution RGB-D sensors. Therefore, we partition both training and test data with a coarse-to-fine clustering method and apply Gaussian processes to each local clusters. In addition, we consider Gaussian processes as implicit functions, and thus extract iso-surfaces from the scalar fields, continuous occupancy maps, using marching cubes. By doing that, we are able to build two types of map representations within a single framework of Gaussian processes. Experimental results with 2-D simulated data show that the accuracy of our approximated method is comparable to previous work, while the computational time is dramatically reduced. We also demonstrate our method with 3-D real data to show its feasibility in large-scale environments. © 2013 IEEE.","Continuous occupancy maps; Gaussian processes; RGB-D mapping; Surface reconstruction","Clustering methods; Computational time; Gaussian process classifications; Gaussian Processes; Implicit function; Map representations; Nonparametric approaches; Occupancy maps; Cluster analysis; Gaussian noise (electronic); Mapping; Sensors; Surface reconstruction; Visualization; Gaussian distribution; actimetry; algorithm; article; artificial intelligence; automated pattern recognition; computer; computer simulation; equipment; image enhancement; methodology; normal distribution; recreation; statistical analysis; three dimensional imaging; transducer; whole body imaging; automated pattern recognition; devices; procedures; three dimensional imaging; whole body imaging; Actigraphy; Algorithms; Artificial Intelligence; Computer Peripherals; Computer Simulation; Data Interpretation, Statistical; Image Enhancement; Imaging, Three-Dimensional; Normal Distribution; Pattern Recognition, Automated; Transducers; Video Games; Whole Body Imaging; Actigraphy; Algorithms; Artificial Intelligence; Computer Peripherals; Computer Simulation; Data Interpretation, Statistical; Image Enhancement; Imaging, Three-Dimensional; Normal Distribution; Pattern Recognition, Automated; Transducers; Video Games; Whole Body Imaging",Article,Scopus,2-s2.0-84890367573
"Calle-Alonso F., Pérez C.J., Arias-Nicolás J.P., Martín J.","Computer-aided diagnosis system: A Bayesian hybrid classification method",2013,"Computer Methods and Programs in Biomedicine",15,10.1016/j.cmpb.2013.05.029,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883793131&doi=10.1016%2fj.cmpb.2013.05.029&partnerID=40&md5=4767aa1f75db533ad1d34f2e990f0ba3","A novel method to classify multi-class biomedical objects is presented. The method is based on a hybrid approach which combines pairwise comparison, Bayesian regression and the k-nearest neighbor technique. It can be applied in a fully automatic way or in a relevance feedback framework. In the latter case, the information obtained from both an expert and the automatic classification is iteratively used to improve the results until a certain accuracy level is achieved, then, the learning process is finished and new classifications can be automatically performed. The method has been applied in two biomedical contexts by following the same cross-validation schemes as in the original studies. The first one refers to cancer diagnosis, leading to an accuracy of 77.35% versus 66.37%, originally obtained. The second one considers the diagnosis of pathologies of the vertebral column. The original method achieves accuracies ranging from 76.5% to 96.7%, and from 82.3% to 97.1% in two different cross-validation schemes. Even with no supervision, the proposed method reaches 96.71% and 97.32% in these two cases. By using a supervised framework the achieved accuracy is 97.74%. Furthermore, all abnormal cases were correctly classified. © 2013 Elsevier Ireland Ltd.","Bayesian methodology; Classification; Computer-aided diagnosis; Relevance feedback","Automatic classification; Bayesian methodology; Bayesian regression; Computer-aided diagnosis system; Hybrid classification; K-nearest neighbors; Pair-wise comparison; Relevance feedback; Classification (of information); Computer aided diagnosis; Iterative methods; article; Bayes theorem; breast cancer; cancer diagnosis; classification; computer assisted diagnosis; diagnostic imaging; diagnostic test accuracy study; human; k nearest neighbor; medical informatics; sensitivity and specificity; spine disease; Bayesian methodology; Classification; Computer-aided diagnosis; Relevance feedback; Algorithms; Artificial Intelligence; Bayes Theorem; Breast Neoplasms; Classification; Diagnosis, Computer-Assisted; Female; Humans; Pattern Recognition, Automated; Spinal Diseases",Article,Scopus,2-s2.0-84883793131
"Wang H., Wang J.","2DPCA with L1-norm for simultaneously robust and sparse modelling",2013,"Neural Networks",15,10.1016/j.neunet.2013.06.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879599851&doi=10.1016%2fj.neunet.2013.06.002&partnerID=40&md5=78438009754d5b0bfde6a269e3ebac20","Robust dimensionality reduction is an important issue in processing multivariate data. Two-dimensional principal component analysis based on L1-norm (2DPCA-L1) is a recently developed technique for robust dimensionality reduction in the image domain. The basis vectors of 2DPCA-L1, however, are still dense. It is beneficial to perform a sparse modelling for the image analysis. In this paper, we propose a new dimensionality reduction method, referred to as 2DPCA-L1 with sparsity (2DPCAL1-S), which effectively combines the robustness of 2DPCA-L1 and the sparsity-inducing lasso regularization. It is a sparse variant of 2DPCA-L1 for unsupervised learning. We elaborately design an iterative algorithm to compute the basis vectors of 2DPCAL1-S. The experiments on image data sets confirm the effectiveness of the proposed approach. © 2013 Elsevier Ltd.","2DPCA-L1; Dimensionality reduction; Lasso regularization; Robust modelling","2DPCA-L1; Dimensionality reduction; Dimensionality reduction method; Image datasets; Iterative algorithm; Lasso regularization; Multivariate data; Two-dimensional principal component analysis; Algorithms; Principal component analysis; Data handling; 2DPCA L1 method; algorithm; article; image analysis; learning; mathematical analysis; mathematical computing; multivariate analysis; principal component analysis; priority journal; procedures; statistical model; statistical parameters; 2DPCA-L1; Dimensionality reduction; Lasso regularization; Robust modelling; Algorithms; Artificial Intelligence; Face; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Principal Component Analysis",Article,Scopus,2-s2.0-84879599851
"Hetmaniok E., Słota D., Zielonka A.","Experimental verification of immune recruitment mechanism and clonal selection algorithm applied for solving the inverse problems of pure metal solidification",2013,"International Communications in Heat and Mass Transfer",15,10.1016/j.icheatmasstransfer.2013.07.009,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883248125&doi=10.1016%2fj.icheatmasstransfer.2013.07.009&partnerID=40&md5=0f9f4d11b1aae52f0504369250e3e57a","In the paper an application of two immune algorithms - Immune Recruitment Mechanism and Clonal Selection Algorithm - in procedures for solving the inverse problems of pure metals solidification is presented. Discussed problems consist in reconstruction of boundary conditions (forms of the heat transfer coefficient and the heat flux) on the basis of temperature measurements. For verifying the effectiveness of investigated algorithms the experimental data obtained in the solidification of aluminum have been used. An example of applying considered procedures for determining the cooling conditions of the continuously cast ingot is also presented. © 2013 Elsevier Ltd.","Artificial intelligence; Continuous casting; Inverse Stefan problem; Solidification","Cast ingots; Clonal selection algorithms; Cooling conditions; Experimental verification; Immune algorithms; Pure metals; Recruitment mechanism; Stefan problem; Algorithms; Artificial intelligence; Continuous casting; Differential equations; Heat flux; Solidification; Temperature measurement; Inverse problems",Article,Scopus,2-s2.0-84883248125
"Yao C., Spurlock D.M., Armentano L.E., Page C.D., VandeHaar M.J., Bickhart D.M., Weigel K.A.","Random Forests approach for identifying additive and epistatic single nucleotide polymorphisms associated with residual feed intake in dairy cattle",2013,"Journal of Dairy Science",15,10.3168/jds.2012-6237,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884349446&doi=10.3168%2fjds.2012-6237&partnerID=40&md5=091295247d15852fc168ebb7511777f7","Feed efficiency is an economically important trait in the beef and dairy cattle industries. Residual feed intake (RFI) is a measure of partial efficiency that is independent of production level per unit of body weight. The objective of this study was to identify significant associations between single nucleotide polymorphism (SNP) markers and RFI in dairy cattle using the Random Forests (RF) algorithm. Genomic data included 42,275 SNP genotypes for 395 Holstein cows, whereas phenotypic measurements were daily RFI from 50 to 150. d postpartum. Residual feed intake was defined as the difference between an animal's feed intake and the average intake of its cohort, after adjustment for year and season of calving, year and season of measurement, age at calving nested within parity, days in milk, milk yield, body weight, and body weight change. Random Forests is a widely used machine-learning algorithm that has been applied to classification and regression problems. By analyzing the tree structures produced within RF, the 25 most frequent pairwise SNP interactions were reported as possible epistatic interactions. The importance scores that are generated by RF take into account both main effects of variables and interactions between variables, and the most negative value of all importance scores can be used as the cutoff level for declaring SNP effects as significant. Ranking by importance scores, 188 SNP surpassed the threshold, among which 38 SNP were mapped to RFI quantitative trait loci (QTL) regions reported in a previous study in beef cattle, and 2 SNP were also detected by a genome-wide association study in beef cattle. The ratio of number of SNP located in RFI QTL to the total number of SNP in the top 188 SNP chosen by RF was significantly higher than in all 42,275 whole-genome markers. Pathway analysis indicated that many of the top 188 SNP are in genomic regions that contain annotated genes with biological functions that may influence RFI. Frequently occurring ancestor-descendant SNP pairs can be explored as possible epistatic effects for further study. The importance scores generated by RF can be used effectively to identify large additive or epistatic SNP and informative QTL. The consistency in results of our study and previous studies in beef cattle indicates that the genetic architecture of RFI in dairy cattle might be similar to that of beef cattle. © 2013 American Dairy Science Association.","Dairy cattle; Random Forest; Residual feed intake; Single nucleotide polymorphism","Animalia; Bos; genetic marker; algorithm; animal; animal food; artificial intelligence; body weight; Bovinae; eating; epistasis; female; genetic association; genetic marker; genetics; genotype; meat; phenotype; quantitative trait locus; randomization; single nucleotide polymorphism; Algorithms; Animal Feed; Animals; Artificial Intelligence; Body Weight; Cattle; Eating; Epistasis, Genetic; Female; Genetic Markers; Genome-Wide Association Study; Genotype; Meat; Phenotype; Polymorphism, Single Nucleotide; Quantitative Trait Loci; Random Allocation",Article,Scopus,2-s2.0-84884349446
"Jose J., Potluri S., Tomko K., Panda D.K.","Designing scalable Graph500 benchmark with hybrid MPI+OpenSHMEM programming models",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",15,10.1007/978-3-642-38750-0_9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884485993&doi=10.1007%2f978-3-642-38750-0_9&partnerID=40&md5=6b04f02fda265011ef60d9aca465d6ee","MPI has been the de-facto programming model for scientific parallel applications. However, it is hard to extract the maximum performance for irregular data-driven applications using MPI. The Partitioned Global Address Space (PGAS) programming models present an alternative approach to improve programmability. The lower overhead in one-sided communication and the global view of data in PGAS models have the potential to increase the performance at scale. In this study, we take up 'Concurrent Search' kernel of Graph500 - a highly data driven irregular benchmark - and redesign it using both MPI and OpenSHMEM constructs. We also implement load balancing in Graph500. Our performance evaluations using MVAPICH2-X (Unified MPI+PGAS Communication Runtime over InfiniBand) indicate a 59% reduction in execution time for the hybrid design, compared to the best performing MPI based design at 8,192 cores. © 2013 Springer-Verlag.","Graph500; Hybrid; MPI; OpenSHMEM","Data-driven applications; Graph500; Hybrid; MPI; One-sided communications; OpenSHMEM; Parallel application; Partitioned Global Address Space; Artificial intelligence; Computer science; Communication",Conference Paper,Scopus,2-s2.0-84884485993
"Rudlosky S.D., Fuelberg H.E.","Documenting storm severity in the mid-atlantic region using lightning and radar information",2013,"Monthly Weather Review",15,10.1175/MWR-D-12-00287.1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883826222&doi=10.1175%2fMWR-D-12-00287.1&partnerID=40&md5=7f62e841a2265542b3212cc16c0d1a94","Storm severity in the mid-Atlantic region of the United States is examined using lightning, radar, and model-derived information. Automated Warning Decision Support System (WDSS) procedures are developed to create grids of lightning and radar parameters, cluster individual storm features, and data mine the lightning and radar attributes of 1252 severe and nonsevere storms. The study first examines the influence of serial correlation and uses autocorrelation functionsto document the persistence of lightning and radar parameters. Decorrelation times are found to vary by parameter, storm severity, and mathematical operator, but the great majority are between three and six lags, suggesting that consecutive 2-min storm samples (following a storm) are effectively independent after only 6-12 min. The study next describes the distribution of lightning jumps in severe and nonsevere storms, differences between various types of severe storms (e.g., severe wind only), and relationships between lightning and radar parameters. The 2s lightning jump algorithm (witha 10 flashes min21 activation threshold) yields 0.92 jumps h21 for nonsevere storms and 1.44 jumps h21 in severe storms. Applying a 10-mm maximum expected size of hail (MESH) threshold to the 2s lightning jump algorithm reduces the frequency of lightning jumps in nonsevere storms to 0.61 jumps h21. Although radarderived parameters are comparable between storms that produce severe wind plus hail and those that produce tornadoes, tornadic storms exhibit much greater intracloud (IC) and cloud-to-ground (CG) flash rates. Correlations furtherillustrate that lightning data provide complementarystorm-scale information to radarderived measures of storm intensity © 2013 American Meteorological Society.",,"Activation thresholds; Automated warnings; De correlations; Lightning datum; Radar information; Serial correlation; Severe storms; Storm intensity; Algorithms; Artificial intelligence; Decision support systems; Lightning; Mathematical operators; Precipitation (meteorology); Radar; Storms; algorithm; cloud to ground lightning; decision support system; radar; severe weather; storm; warning system; United States",Article,Scopus,2-s2.0-84883826222
"Smiti A., Eloudi Z.","Soft DBSCAN: Improving DBSCAN clustering method using fuzzy set theory",2013,"2013 6th International Conference on Human System Interactions, HSI 2013",15,10.1109/HSI.2013.6577851,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883715470&doi=10.1109%2fHSI.2013.6577851&partnerID=40&md5=10a0fc4deef5dad403c3c6f9d210510b","Clustering is one of the most valuable methods of computational intelligence field, in which sets of related objects are cataloged into clusters. Almost all of the well-known clustering algorithms require input number of clusters which is hard to determine but have a significant influence on the clustering result. Furthermore, the majority is not robust enough towards noisy data. In contrast, density based methods, such as DBSCAN, have obvious advantages over explicit samples. They discover the number of clusters, as well as, they detect noises. Additionally, the shape of such clusters can also be irregular. However, they have difficulties in handling the challenges posed by the collection of natural data which is often vague. This paper presents an efficient clustering technique, named 'Soft DBSCAN' that combines DBSCAN and fuzzy set theory. Our new method is galvanized by Fuzzy C Means in the way of using the fuzzy membership functions. The results of our method show that it is efficient not only in handling noises, contrary to Fuzzy C Means, but also, able to assign one data point into more than one cluster. Simulative experiments are carried out on a variety of datasets, throughout different evaluation's criteria, which highlight the soft DBSCAN's effectiveness and cluster validity to check the good quality of clustering results. © 2013 IEEE.",,"Cluster validity; Clustering results; Clustering techniques; DBSCAN clustering; Density-based method; Fuzzy membership function; Number of clusters; Quality of clustering; Artificial intelligence; Fuzzy set theory; Clustering algorithms",Conference Paper,Scopus,2-s2.0-84883715470
"Juhl L., Guldstrand Larsen K., Raskin J.-F.","Optimal bounds for multiweighted and parametrised energy games",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",15,10.1007/978-3-642-39698-4_15,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883232467&doi=10.1007%2f978-3-642-39698-4_15&partnerID=40&md5=1885edb0590d6b80213f32e05df5c98b","Multiweighted energy games are two-player multiweighted games that concern the existence of infinite runs subject to a vector of lower and upper bounds on the accumulated weights along the run. We assume an unknown upper bound and calculate the set of vectors of upper bounds that allow an infinite run to exist. For both a strict and a weak upper bound we show how to construct this set by employing results from previous works, including an algorithm given by Valk and Jantzen for finding the set of minimal elements of an upward closed set. Additionally, we consider energy games where the weight of some transitions is unknown, and show how to find the set of suitable weights using the same algorithm. © 2013 Springer-Verlag.",,"Closed set; Lower and upper bounds; Minimal elements; Optimal bounds; Upper Bound; Artificial intelligence; Computer science; Algorithms",Conference Paper,Scopus,2-s2.0-84883232467
"Wang G., Forsyth D., Hoiem D.","Improved object categorization and detection using comparative object similarity",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",15,10.1109/TPAMI.2013.58,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883165952&doi=10.1109%2fTPAMI.2013.58&partnerID=40&md5=1d30f9f6d4d61337fa02fae0899d70ad","Due to the intrinsic long-tailed distribution of objects in the real world, we are unlikely to be able to train an object recognizer/detector with many visual examples for each category. We have to share visual knowledge between object categories to enable learning with few or no training examples. In this paper, we show that local object similarity information - statements that pairs of categories are similar or dissimilar - is a very useful cue to tie different categories to each other for effective knowledge transfer. The key insight: Given a set of object categories which are similar and a set of categories which are dissimilar, a good object model should respond more strongly to examples from similar categories than to examples from dissimilar categories. To exploit this category-dependent similarity regularization, we develop a regularized kernel machine algorithm to train kernel classifiers for categories with few or no training examples. We also adapt the state-of-the-art object detector to encode object similarity constraints. Our experiments on hundreds of categories from the Labelme dataset show that our regularized kernel classifiers can make significant improvement on object categorization. We also evaluate the improved object detector on the PASCAL VOC 2007 benchmark dataset. © 1979-2012 IEEE.","Comparative object similarity; deformable part model; kernel machines; object categorization; object detection; PASCAL VOC; sharing; SVM","Comparative object similarity; Deformable part models; Kernel machine; Object categorization; Object Detection; sharing; SVM; Data processing; Knowledge management; Object recognition; Classifiers; algorithm; artificial intelligence; automated pattern recognition; computer assisted diagnosis; image enhancement; image subtraction; procedures; reproducibility; sensitivity and specificity; signal processing; three dimensional imaging; article; automated pattern recognition; computer assisted diagnosis; image enhancement; methodology; three dimensional imaging; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Signal Processing, Computer-Assisted; Subtraction Technique; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Signal Processing, Computer-Assisted; Subtraction Technique",Article,Scopus,2-s2.0-84883165952
"Otero F.E.B., Freitas A.A.","Improving the interpretability of classification rules discovered by an ant colony algorithm",2013,"GECCO 2013 - Proceedings of the 2013 Genetic and Evolutionary Computation Conference",15,10.1145/2463372.2463382,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883103815&doi=10.1145%2f2463372.2463382&partnerID=40&md5=ddb9186ec746b5d875cff96d3fc06878","The vast majority of Ant Colony Optimization (ACO) algorithms for inducing classification rules use an ACO-based procedure to create a rule in an one-at-a-time fashion. An improved search strategy has been proposed in the cAnt-MinerPB algorithm, where an ACO-based procedure is used to create a complete list of rules (ordered rules)-i.e., the ACO search is guided by the quality of a list of rules, instead of an individual rule. In this paper we propose an extension of the cAnt-MinerPB algorithm to discover a set of rules (unordered rules). The main motivation for discovering a set of rules is to improve the interpretation of individual rules and evaluate the impact on the predictive accuracy of the algorithm. We also propose a new measure to evaluate the interpretability of the discovered rules to mitigate the fact that the commonly-used model size measure ignores how the rules are used to make a class prediction. Comparisons with state-of-the-art rule induction algorithms and the cAnt-MinerPB producing ordered rules are also presented. Copyright © 2013 ACM.","Ant colony optimization; Classification; Data mining; Sequential covering; Unordered rules","Ant colony algorithms; Ant Colony Optimization algorithms; Classification rules; Predictive accuracy; Rule induction algorithms; Search strategies; Sequential covering; Unordered rules; Ant colony optimization; Artificial intelligence; Classification (of information); Data mining; Miners; Algorithms",Conference Paper,Scopus,2-s2.0-84883103815
"Chivilikhin D., Ulyantsev V.","MuACOsm - A new mutation-based ant colony optimization algorithm for learning finite-state machines",2013,"GECCO 2013 - Proceedings of the 2013 Genetic and Evolutionary Computation Conference",15,10.1145/2463372.2463440,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883115800&doi=10.1145%2f2463372.2463440&partnerID=40&md5=91d3222c60d871ca266bdbba8d698822","In this paper we present MuACOsm - a new method of learning Finite-State Machines (FSM) based on Ant Colony Optimization (ACO) and a graph representation of the search space. The input data is a set of events, a set of actions and the number of states in the target FSM. The goal is to maximize the given fitness function, which is defined on the set of all FSMs with given parameters. The new algorithm is compared with evolutionary algorithms and a genetic programming related approach on the well-known Artificial Ant problem. Copyright © 2013 ACM.","Ant colony optimization; Finite-state machine; Induction; Learning","Ant Colony Optimization (ACO); Ant Colony Optimization algorithms; Finite-state; Fitness functions; Graph representation; Induction; Learning; Method of learning; Ant colony optimization; Artificial intelligence; Evolutionary algorithms; Genetic programming; Asynchronous machinery",Conference Paper,Scopus,2-s2.0-84883115800
"Beliakov G., James S.","Stability of weighted penalty-based aggregation functions",2013,"Fuzzy Sets and Systems",15,10.1016/j.fss.2013.01.007,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887097606&doi=10.1016%2fj.fss.2013.01.007&partnerID=40&md5=2c76d3d662a1adc025d533767afefd8c","In many practical applications, the need arises to aggregate data of varying dimension. Following from the self-identity property, some recent studies have looked at the stability of aggregation operators in terms of their behavior as the dimensionality is increased from n-1 to n. We use the penalty-based representation of aggregation functions in order to investigate the conditions for weighting vectors associated with some important weighted families, extending on the results already established for quasi-arithmetic means. In particular, we obtain results for quasi-medians and functions that involve a reordering of the inputs such as the OWA and order statistics. © 2013 Elsevier B.V.","Aggregation functions; Means; Medians; OWA; Quasi-arithmetic means; Stability; Weighting triangles","Aggregation functions; Means; Medians; OWA; Quasiarithmetic means; Weighting triangles; Artificial intelligence; Convergence of numerical methods; Fuzzy sets",Article,Scopus,2-s2.0-84887097606
"Haug P.J., Ferraro J.P., Holmen J., Wu X., Mynam K., Ebert M., Dean N., Jones J.","An ontology-driven, diagnostic modeling system",2013,"Journal of the American Medical Informatics Association",15,10.1136/amiajnl-2012-001376,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881324266&doi=10.1136%2famiajnl-2012-001376&partnerID=40&md5=a9ee54e6baaaf16d1af110915457f2f9","Objectives: To present a system that uses knowledge stored in a medical ontology to automate the development of diagnostic decision support systems. To illustrate its function through an example focused on the development of a tool for diagnosing pneumonia. Materials and methods: We developed a system that automates the creation of diagnostic decision-support applications. It relies on a medical ontology to direct the acquisition of clinic data from a clinical data warehouse and uses an automated analytic system to apply a sequence of machine learning algorithms that create applications for diagnostic screening. We refer to this system as the ontology-driven diagnostic modeling system (ODMS). We tested this system using samples of patient data collected in Salt Lake City emergency rooms and stored in Intermountain Healthcare's enterprise data warehouse. Results: The system was used in the preliminary development steps of a tool to identify patients with pneumonia in the emergency department. This tool was compared with a manually created diagnostic tool derived from a curated dataset. The manually created tool is currently in clinical use. The automatically created tool had an area under the receiver operating characteristic curve of 0.920 (95% CI 0.916 to 0.924), compared with 0.944 (95% CI 0.942 to 0.947) for the manually created tool. Discussion: Initial testing of the ODMS demonstrates promising accuracy for the highly automated results and illustrates the route to model improvement. Conclusions: The use of medical knowledge, embedded in ontologies, to direct the initial development of diagnostic computing systems appears feasible.",,"abnormal respiratory sound; algorithm; article; clinical feature; data base; decision support system; diagnosis; emergency ward; human; information system; machine learning; medical terminology; nursing assessment; ontology driven diagnostic modeling system; patient coding; pneumonia; receiver operating characteristic; screening test; speech discrimination; thorax radiography; vital sign; artificial intelligence; data mining; decision support system; Diagnostic System; emergency health service; International Classification of Diseases; linguistics; ontology; pneumonia; Data Mining; Diagnostic System; Ontology; Pneumonia; Algorithms; Artificial Intelligence; Decision Support Systems, Clinical; Emergency Service, Hospital; Humans; International Classification of Diseases; Pneumonia; ROC Curve; Vocabulary, Controlled",Article,Scopus,2-s2.0-84881324266
"Xu Y., Wang Y., Liu T., Tsujii J., Chang E.I.-C.","An end-to-end system to identify temporal relation in discharge summaries: 2012 i2b2 challenge",2013,"Journal of the American Medical Informatics Association",15,10.1136/amiajnl-2012-001607,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882800112&doi=10.1136%2famiajnl-2012-001607&partnerID=40&md5=0971e5ee6f9ee906cabe3e937a71e021","Objective: To create an end-to-end system to identify temporal relation in discharge summaries for the 2012 i2b2 challenge. The challenge includes event extraction, timex extraction, and temporal relation identification. Design: An end-to-end temporal relation system was developed. It includes three subsystems: an event extraction system (conditional random fields (CRF) name entity extraction and their corresponding attribute classifiers), a temporal extraction system (CRF name entity extraction, their corresponding attribute classifiers, and context-free grammar based normalization system), and a temporal relation system (10 multi-support vector machine (SVM) classifiers and a Markov logic networks inference system) using labeled sequential pattern mining, syntactic structures based on parse trees, and results from a coordination classifier. Micro-averaged precision (P), recall (R), averaged P&R (P&R), and F measure (F) were used to evaluate results. Results: For event extraction, the system achieved 0.9415 (P), 0.8930 (R), 0.9166 (P&R), and 0.9166 (F). The accuracies of their type, polarity, and modality were 0.8574, 0.8585, and 0.8560, respectively. For timex extraction, the system achieved 0.8818, 0.9489, 0.9141, and 0.9141, respectively. The accuracies of their type, value, and modifier were 0.8929, 0.7170, and 0.8907, respectively. For temporal relation, the system achieved 0.6589, 0.7129, 0.6767, and 0.6849, respectively. For end-to-end temporal relation, it achieved 0.5904, 0.5944, 0.5921, and 0.5924, respectively. With the F measure used for evaluation, we were ranked first out of 14 competing teams (event extraction), first out of 14 teams (timex extraction), third out of 12 teams (temporal relation), and second out of seven teams (end-to-end temporal relation). Conclusions: The system achieved encouraging results, demonstrating the feasibility of the tasks defined by the i2b2 organizers. The experiment result demonstrates that both global and local information is useful in the 2012 challenge.",,"accuracy; article; hospital discharge; hospital information system; information processing; probability; support vector machine; Context-Free Grammar; Coordination; Dependency Tree; Labeled Sequential Pattern; Syntax; Artificial Intelligence; Electronic Health Records; Humans; Information Storage and Retrieval; Natural Language Processing; Patient Discharge Summaries; Time; Translational Medical Research",Article,Scopus,2-s2.0-84882800112
"Chang W.-J., Chen P.-H., Yang C.-T.","Robust fuzzy congestion control of TCP/AQM router via perturbed Takagi-Sugeno fuzzy models",2013,"International Journal of Fuzzy Systems",15,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881513938&partnerID=40&md5=7a9846a74e00f1c2d1255070fcc99803","Because the number of transmission control protocol sessions usually varies from time to time, a perturbed Takagi-Sugeno fuzzy model is used in this paper to represent the dynamic transmission control protocol network systems. According to the proposed perturbed Takagi-Sugeno fuzzy model, a robust fuzzy controller design approach is investigated to achieve the congestion avoidance for the transmission control protocol network systems. In order to accomplish the above mission, some sufficient conditions are derived based on the Lyapunov stability theory. By solving these sufficient stability conditions, an active queue management router can be obtained via the proposed robust fuzzy congestion control technique. © 2013 TFSA.","Congestion control and robust fuzzy control; Perturbed Takagi-Sugeno fuzzy model; TCP network system","Active queue management routers; Lyapunov stability theory; Number of transmissions; Robust fuzzy control; Robust fuzzy controller; Sufficient conditions; Takagi Sugeno fuzzy models; TCP network systems; Artificial intelligence; Software engineering; Transmission control protocol",Article,Scopus,2-s2.0-84881513938
"Kochenberger G.A., Hao J.-K., Lü Z., Wang H., Glover F.","Solving large scale max cut problems via tabu search",2013,"Journal of Heuristics",15,10.1007/s10732-011-9189-8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880918833&doi=10.1007%2fs10732-011-9189-8&partnerID=40&md5=94f4c7987bba7fae25b24ceac54583a8","In recent years many algorithms have been proposed in the literature for solving the Max-Cut problem. In this paper we report on the application of a new Tabu Search algorithm to large scale Max-cut test problems. Our method provides best known solutions for many well-known test problems of size up to 10,000 variables, although it is designed for the general unconstrained quadratic binary program (UBQP), and is not specialized in any way for the Max-Cut problem. © 2011 Springer Science+Business Media, LLC.","Combinatorial optimization; Max Cut problem; Metaheuristics","Binary programs; Max-cut; MAX-CUT problem; Meta heuristics; Tabu search algorithms; Test problem; Artificial intelligence; Combinatorial optimization; Heuristic algorithms; Software engineering; Software testing",Article,Scopus,2-s2.0-84880918833
"Bauer R., Columbus T., Rutter I., Wagner D.","Search-space size in contraction hierarchies",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",15,10.1007/978-3-642-39206-1_9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880307247&doi=10.1007%2f978-3-642-39206-1_9&partnerID=40&md5=148051ca7865cb93a2adf23c19e5058e","Contraction hierarchies are a speed-up technique to improve the performance of shortest-path computations, which works very well in practice. Despite convincing practical results, there is still a lack of theoretical explanation for this behavior. In this paper, we develop a theoretical framework for studying search space sizes in contraction hierarchies. We prove the first bounds on the size of search spaces that depend solely on structural parameters of the input graph, that is, they are independent of the edge lengths. To achieve this, we establish a connection with the well-studied elimination game. Our bounds apply to graphs with treewidth k, and to any minor-closed class of graphs that admits small separators. For trees, we show that the maximum search space size can be minimized efficiently, and the average size can be approximated efficiently within a factor of 2. We show that, under a worst-case assumption on the edge lengths, our bounds are comparable to the recent results of Abraham et al. [1], whose analysis depends also on the edge lengths. As a side result, we link their notion of highway dimension (a parameter that is conjectured to be small, but is unknown for all practical instances) with the notion of pathwidth. This is the first relation of highway dimension with a well-known graph parameter. © 2013 Springer-Verlag.",,"Graph parameters; Search space size; Search spaces; Shortest-path; Speed-up techniques; Structural parameter; Theoretical explanation; Theoretical framework; Artificial intelligence; Computer science; Automata theory",Conference Paper,Scopus,2-s2.0-84880307247
"Lane H.C., Cahill C., Foutz S., Auerbach D., Noren D., Lussenhop C., Swartout W.","The effects of a pedagogical agent for informal science education on learner behaviors and self-efficacy",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",15,10.1007/978-3-642-39112-5-32,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879999616&doi=10.1007%2f978-3-642-39112-5-32&partnerID=40&md5=50cd2783e8c47528c8d2daba61f6745f","We describe Coach Mike, an animated pedagogical agent for informal computer science education, and report findings from two experiments that provide initial evidence for the efficacy of the system. In the first study, we found that Coach Mike's presence led to 20% longer holding times, increased acceptance of programming challenges, and reduced misuse of the exhibit, but had limited cumulative impact on attitudes, awareness, and knowledge beyond what the host exhibit already achieved. In the second study, we compared two different versions of Coach Mike and found that the use of enthusiasm and self-regulatory feedback led to greater self-efficacy for programming. © 2013 Springer-Verlag Berlin Heidelberg.","Computer science education; Enthusiasm; Informal science education; Intelligent tutoring systems; Pedagogical agents; Self-efficacy","Computer Science Education; Enthusiasm; Informal science education; Intelligent tutoring system; Pedagogical agents; Self efficacy; Computer aided instruction; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84879999616
"Heule M.J.H., Hunt Jr. W.A., Wetzler N.","Verifying refutations with extended resolution",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",15,10.1007/978-3-642-38574-2_24,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879904141&doi=10.1007%2f978-3-642-38574-2_24&partnerID=40&md5=5dd82ff6eff672a235b53816c2f7b132","Modern SAT solvers use preprocessing and inprocessing techniques that are not solely based on resolution; existing unsatisfiability proof formats do not support SAT solvers using such techniques. We present a new proof format for checking unsatisfiability proofs produced by SAT solvers that use techniques such as extended resolution and blocked clause addition. Our new format was designed with three goals: proofs should be easy to generate, proofs should be compact, and validating proofs must be simple. We show how existing preprocessors and solvers can be modified to generate proofs in our new format. Additionally, we implemented a mechanically-verified proof checker in ACL2 and a proof checker in C for the proposed format. © 2013 Springer-Verlag.",,"New formats; Proof checkers; SAT solvers; Artificial intelligence; Computer science; Boolean functions",Conference Paper,Scopus,2-s2.0-84879904141
"Papadopoulos C.T., Okelly M.E.J., Tsadiras A.K.","A DSS for the buffer allocation of production lines based on a comparative evaluation of a set of search algorithms",2013,"International Journal of Production Research",15,10.1080/00207543.2012.752585,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880569649&doi=10.1080%2f00207543.2012.752585&partnerID=40&md5=ee8abff192c64ececbe65370801e25b1","In the design of production lines, the classical approach to the buffer allocation problem (BAP) is to use a search algorithm in association with an evaluative algorithm to obtain the mathematical optimum of the specified objective function. In practice, a choice often has to be made regarding which search algorithm to use for the efficient solution of the BAP. This paper gives the results of a carefully selected set of experiments on short (K = number of stations = 3, 4, 11 stations), medium length (K = 12, 13, 30 stations) and long lines (K = 40, 50, 100 stations) and, within each line, small N (N = total number of buffer slots = K/2 if K is even; = (K - 1)/2 if K is odd), medium N (N = K + 1) and large N (N = 2K) to evaluate the effectiveness of the following five search algorithms: simulated annealing, genetic, tabu search, myopic and complete enumeration (where possible). The production lines are balanced and the single exponential machine at each station is perfectly reliable. All the experiments were run on a readily available desktop PC with the following specifications: Windows XP Professional Version 2002 Service Pack 3, Pentium® Dual-Core CPU E5300@2.60 GHz, 2.00 GB RAM. The measures of performance used are CPU time required and closeness to the maximum throughput achieved. The five search algorithms are ranked in respect to these two measures and certain findings regarding their performance over the experimental set are noted. The distributions of buffer slots to storage areas for the algorithm(s) leading to maximum throughput are examined and certain patterns are found, leading to indications for design rules. Based on the results of the above experiments, two additional sets of experiments were carried out, one using the simulated annealing algorithm for production lines of K = 3 to 20 and N = 1 to 20 (accounting for a total number of 360 different production lines) and another using the myopic algorithm for production lines of K = 3 to 80 and N = 1 to 120 (accounting for a total number of 9360 different production lines). These results may be used as references for comparison purposes in the international literature. Using the results from all sets of experiments, a decision support system (DSS) is designed and implemented which, as is illustrated, may assist production line designers in making decisions regarding the most appropriate of the five search algorithms tested to use for the BAP-A (the dual problem) and the BAP-B (the primal problem) for a wide class of production lines (consisting of K = 3 to 80 and N = 1 to 120). © 2013 Copyright Taylor and Francis Group, LLC.","buffer allocation problem; comparative efficiency; decision support system; production lines; search algorithms","Buffer allocation; Comparative efficiencies; Decision support system (dss); Measures of performance; Production line; Search Algorithms; Simulated annealing algorithms; Windows XP Professionals; Artificial intelligence; Decision support systems; Functions; Learning algorithms; Reservation systems; Simulated annealing; Throughput; Experiments",Article,Scopus,2-s2.0-84880569649
"Han H.-G., Wang L.-D., Qiao J.-F.","Efficient self-organizing multilayer neural network for nonlinear system modeling",2013,"Neural Networks",15,10.1016/j.neunet.2013.01.015,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875257054&doi=10.1016%2fj.neunet.2013.01.015&partnerID=40&md5=aa827d4f23c540cf71b56e4c5ce5e365","It has been shown extensively that the dynamic behaviors of a neural system are strongly influenced by the network architecture and learning process. To establish an artificial neural network (ANN) with self-organizing architecture and suitable learning algorithm for nonlinear system modeling, an automatic axon-neural network (AANN) is investigated in the following respects. First, the network architecture is constructed automatically to change both the number of hidden neurons and topologies of the neural network during the training process. The approach introduced in adaptive connecting-and-pruning algorithm (ACP) is a type of mixed mode operation, which is equivalent to pruning or adding the connecting of the neurons, as well as inserting some required neurons directly. Secondly, the weights are adjusted, using a feedforward computation (FC) to obtain the information for the gradient during learning computation. Unlike most of the previous studies, AANN is able to self-organize the architecture and weights, and to improve the network performances. Also, the proposed AANN has been tested on a number of benchmark problems, ranging from nonlinear function approximating to nonlinear systems modeling. The experimental results show that AANN can have better performances than that of some existing neural networks. © 2013.","Adaptive connecting and pruning algorithm; Automatic axon-neural network; Feedforward computation; Information theory; Modeling","Bench-mark problems; Better performance; Feed-Forward; Mixed-mode operations; Nonlinear functions; Nonlinear system modeling; Number of hidden neurons; Pruning algorithms; Information theory; Learning algorithms; Models; Multilayer neural networks; Network architecture; Network performance; Neurons; Nonlinear systems; adaptive connecting and pruning algorithm; article; artificial neural network; automatic axon neural network; feedforward computation; intermethod comparison; learning algorithm; nonlinear system; priority journal; quality control; Algorithms; Artificial Intelligence; Axons; Neural Networks (Computer); Neurons; Nonlinear Dynamics",Article,Scopus,2-s2.0-84875257054
"Latha Shankar B., Basavarajappa S., Kadadevaramath R.S., Chen J.C.H.","A bi-objective optimization of supply chain design and distribution operations using non-dominated sorting algorithm: A case study",2013,"Expert Systems with Applications",15,10.1016/j.eswa.2013.03.047,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878276990&doi=10.1016%2fj.eswa.2013.03.047&partnerID=40&md5=b2569782ea18aac3c5fe2cc2dce4d7f6","This paper considers simultaneous optimization of strategic design and distribution decisions for three-echelon supply chain architecture consisting of following three players; suppliers, production plants, and distribution centers (DCs). The key design decisions considered are: the number and location of plants in the system, the flow of raw materials from suppliers to plants, the quantity of products to be shipped from plants to distribution centers, so as to minimize the combined facility location, production, inventory, and shipment costs and maximize fill rate. To achieve this, three-echelon network model is mathematically represented and solved using swarm intelligence based Multi-objective Hybrid Particle Swarm Optimization algorithm (MOHPSO). This heuristic incorporates non-dominated sorting (NDS) procedure to achieve bi-objective optimization of two conflicting objectives. The applicability of proposed optimization algorithm was then tested by applying it to standard test problems found in literature. On achieving comparable results, the approach was applied to actual data of a pump manufacturing industry. The results show that the proposed solution approach performs efficiently. © 2013 Elsevier Ltd. All rights reserved.","Bi-objective; Non-dominating sorting; Particle swarm; Supply chain; Swarm intelligence; Three-echelon","Bi objectives; Bi-objective optimization; Hybrid particle swarm optimization algorithm; Non-dominated sorting algorithms; Particle swarm; Supply chain architecture; Swarm Intelligence; Three-echelon; Algorithms; Artificial intelligence; Location; Particle swarm optimization (PSO); Screening; Ships; Warehouses; Supply chains",Article,Scopus,2-s2.0-84878276990
"Tolu S., Vanegas M., Garrido J.A., Luque N.R., Ros E.","Adaptive and predictive control of a simulated robot arm",2013,"International Journal of Neural Systems",15,10.1142/S012906571350010X,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877295696&doi=10.1142%2fS012906571350010X&partnerID=40&md5=1c21f0f2b3b3c09822102c397056719b","In this work, a basic cerebellar neural layer and a machine learning engine are embedded in a recurrent loop which avoids dealing with the motor error or distal error problem. The presented approach learns the motor control based on available sensor error estimates (position, velocity, and acceleration) without explicitly knowing the motor errors. The paper focuses on how to decompose the input into different components in order to facilitate the learning process using an automatic incremental learning model (locally weighted projection regression (LWPR) algorithm). LWPR incrementally learns the forward model of the robot arm and provides the cerebellar module with optimal pre-processed signals. We present a recurrent adaptive control architecture in which an adaptive feedback (AF) controller guarantees a precise, compliant, and stable control during the manipulation of objects. Therefore, this approach efficiently integrates a bio-inspired module (cerebellar circuitry) with a machine learning component (LWPR). The cerebellar-LWPR synergy makes the robot adaptable to changing conditions. We evaluate how this scheme scales for robot-arms of a high number of degrees of freedom (DOFs) using a simulated model of a robot arm of the new generation of light weight robots (LWRs). © 2013 World Scientific Publishing Company.","adaptive learning; Light weight robot; locally weighted projection regression; recurrent control architecture","Adaptive control architecture; Adaptive learning; Control architecture; Incremental learning; Light weight robots; Locally weighted projection regressions; Number of degrees of freedom; Predictive control; Learning algorithms; Learning systems; Robotic arms; Computer simulation; adaptation; algorithm; arm; artificial intelligence; biological model; cerebellum; computer simulation; cytology; feedback system; human; movement (physiology); nerve cell; physiology; predictive value; robotics; article; movement (physiology); nerve cell; physiology; Adaptation, Physiological; Algorithms; Arm; Artificial Intelligence; Cerebellum; Computer Simulation; Feedback; Humans; Models, Neurological; Movement; Neurons; Predictive Value of Tests; Robotics; Adaptation, Physiological; Algorithms; Arm; Artificial Intelligence; Cerebellum; Computer Simulation; Feedback; Humans; Models, Neurological; Movement; Neurons; Predictive Value of Tests; Robotics",Article,Scopus,2-s2.0-84877295696
"Gilio A., Sanfilippo G.","Probabilistic entailment in the setting of coherence: The role of quasi conjunction and inclusion relation",2013,"International Journal of Approximate Reasoning",15,10.1016/j.ijar.2012.11.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875225966&doi=10.1016%2fj.ijar.2012.11.001&partnerID=40&md5=ace2e83ff1378291eaf44b3c6502d64a","In this paper, by adopting a coherence-based probabilistic approach to default reasoning, we focus the study on the logical operation of quasi conjunction and the Goodman-Nguyen inclusion relation for conditional events. We recall that quasi conjunction is a basic notion for defining consistency of conditional knowledge bases. By deepening some results given in a previous paper we show that, given any finite family of conditional events F and any nonempty subset S of F, the family F p-entails the quasi conjunction C(S); then, given any conditional event E|H, we analyze the equivalence between p-entailment of E|H from F and p-entailment of E|H from C(S), where S is some nonempty subset of F We also illustrate some alternative theorems related with p-consistency and p-entailment. Finally, we deepen the study of the connections between the notions of p-entailment and inclusion relation by introducing for a pair (F,E|H) the (possibly empty) class K of the subsets S of F such that CS implies E|H. We show that the class K satisfies many properties; in particular K is additive and has a greatest element which can be determined by applying a suitable algorithm. © 2012 Elsevier Inc. All.","Coherence; Goodman-Nguyen's inclusion relation; p-Entailment; Probabilistic default reasoning; QAND rule; Quasi conjunction","Inclusion relation; p-Entailment; Probabilistic default reasonings; QAND rule; Quasi conjunction; Artificial intelligence; Coherent light; Software engineering",Article,Scopus,2-s2.0-84875225966
"Wang S., Jiang X., Wu Y., Cui L., Cheng S., Ohno-Machado L.","EXpectation Propagation LOgistic REgRession (EXPLORER): Distributed privacy-preserving online model learning",2013,"Journal of Biomedical Informatics",15,10.1016/j.jbi.2013.03.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878185145&doi=10.1016%2fj.jbi.2013.03.008&partnerID=40&md5=aaa653ba03727125585ed9aaf52d001e","We developed an EXpectation Propagation LOgistic REgRession (EXPLORER) model for distributed privacy-preserving online learning. The proposed framework provides a high level guarantee for protecting sensitive information, since the information exchanged between the server and the client is the encrypted posterior distribution of coefficients. Through experimental results, EXPLORER shows the same performance (e.g., discrimination, calibration, feature selection, etc.) as the traditional frequentist logistic regression model, but provides more flexibility in model updating. That is, EXPLORER can be updated one point at a time rather than having to retrain the entire data set when new observations are recorded. The proposed EXPLORER supports asynchronized communication, which relieves the participants from coordinating with one another, and prevents service breakdown from the absence of participants or interrupted communications. © 2013 Elsevier Inc.","Clinical information systems; Decision support systems; Distributed privacy-preserving modeling; Expectation propagation; Logistic regression","Clinical information system; Expectation Propagation; Logistic regression models; Logistic regressions; Online learning; Posterior distributions; Privacy preserving; Sensitive informations; Artificial intelligence; Decision support systems; E-learning; Logistics; Medical information systems; Regression analysis; article; decision support system; expectation propagation logistic regression; information dissemination; interpersonal communication; logistic regression analysis; medical information system; online system; priority journal; Calibration; Learning; Logistic Models; Online Systems; Privacy; Regression Analysis",Article,Scopus,2-s2.0-84878185145
"Cafaro M., Mirto M., Aloisio G.","Preference-Based Matchmaking of Grid Resources with CP-Nets",2013,"Journal of Grid Computing",15,10.1007/s10723-012-9235-2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878018778&doi=10.1007%2fs10723-012-9235-2&partnerID=40&md5=86e0ee37e0eadef995153ba15ff06162","We deal with the problem of preference-based matchmaking of computational resources belonging to a Grid. We introduce CP-Nets, a recent development in the field of Artificial Intelligence, as a means to deal with user's preferences in the context of Grid scheduling. We discuss CP-Nets from a theoretical perspective and then analyze, qualitatively and quantitatively, their impact on the matchmaking process, with the help of a Grid simulator we developed for this purpose. Many different experiments have been setup and carried out, and we report here our main findings and the lessons learnt. © 2012 Springer Science+Business Media Dordrecht.","CP-Nets; Grids; Matchmaking","Computational resources; CP-nets; Grid resource; Grid scheduling; Grids; Matchmaking; User's preferences; Grid computing; Hardware; Artificial intelligence",Article,Scopus,2-s2.0-84878018778
"Sjöberg C., Ahnesjö A.","Multi-atlas based segmentation using probabilistic label fusion with adaptive weighting of image similarity measures",2013,"Computer Methods and Programs in Biomedicine",15,10.1016/j.cmpb.2012.12.006,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877055478&doi=10.1016%2fj.cmpb.2012.12.006&partnerID=40&md5=e2385a06f643167072bb755f2e8bc10c","Label fusion multi-atlas approaches for image segmentation can give better segmentation results than single atlas methods. We present a multi-atlas label fusion strategy based on probabilistic weighting of distance maps. Relationships between image similarities and segmentation similarities are estimated in a learning phase and used to derive fusion weights that are proportional to the probability for each atlas to improve the segmentation result. The method was tested using a leave-one-out strategy on a database of 21 pre-segmented prostate patients for different image registrations combined with different image similarity scorings. The probabilistic weighting yields results that are equal or better compared to both fusion with equal weights and results using the STAPLE algorithm. Results from the experiments demonstrate that label fusion by weighted distance maps is feasible, and that probabilistic weighted fusion improves segmentation quality more the stronger the individual atlas segmentation quality depends on the corresponding registered image similarity. The regions used for evaluation of the image similarity measures were found to be more important than the choice of similarity measure. © 2013 Elsevier Ireland Ltd.","Atlas based segmentation; Deformable registration; Label fusion; Multi-atlas segmentation; Radiotherapy prostate; Segmentation","Adaptive weighting; Atlas-based segmentation; Deformable registration; Label fusions; Multi atlas-based segmentations; Segmentation quality; Segmentation results; Similarity measure; Image analysis; Image fusion; Urology; Image segmentation; article; atlas; data base; human; image analysis; image processing; linear regression analysis; probability; system analysis; weight; Adenocarcinoma; Algorithms; Artificial Intelligence; Humans; Male; Models, Statistical; Prostatic Neoplasms; Radiographic Image Interpretation, Computer-Assisted; Radiotherapy Planning, Computer-Assisted; Tomography, X-Ray Computed",Article,Scopus,2-s2.0-84877055478
"Rodríguez-López R.","On the existence of solutions to periodic boundary value problems for fuzzy linear differential equations",2013,"Fuzzy Sets and Systems",15,10.1016/j.fss.2012.11.007,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875215198&doi=10.1016%2fj.fss.2012.11.007&partnerID=40&md5=0cb1300baf830414690adbfeb8866fec","In this work, we provide sufficient conditions which guarantee the existence of solutions to periodic boundary value problems for first-order linear fuzzy differential equations by using generalized differentiability and switching points. In comparison with some previous works, we consider equations whose coefficient may change its sign a finite number of times in the interval of interest. We also study the existence of solutions which are crisp (or real) at the switching points where the diameter of the level sets changes from nonincreasing to nondecreasing character. © 2012 Elsevier B.V.","First-order fuzzy differential equations; Fuzzy real numbers; Generalized differentiability; Periodic boundary value problems","Existence of Solutions; Fuzzy differential equations; Fuzzy real numbers; Generalized differentiability; Linear differential equation; Linear fuzzy differential equations; Periodic boundary value problems; Sufficient conditions; Artificial intelligence; Fuzzy sets; Boundary value problems",Article,Scopus,2-s2.0-84875215198
"Taherdangkoo M., Bagheri M.H.","A powerful hybrid clustering method based on modified stem cells and Fuzzy C-means algorithms",2013,"Engineering Applications of Artificial Intelligence",15,10.1016/j.engappai.2013.03.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876899318&doi=10.1016%2fj.engappai.2013.03.002&partnerID=40&md5=d03397d88e7203e22734601a889a82c2","One of the simple techniques for Data Clustering is based on Fuzzy C-means (FCM) clustering which describes the belongingness of each data to a cluster by a fuzzy membership function instead of a crisp value. However, the results of fuzzy clustering depend highly on the initial state selection and there is also a high risk for getting the best results when the datasets are large. In this paper, we present a hybrid algorithm based on FCM and modified stem cells algorithms, we called it SC-FCM algorithm, for optimum clustering of a dataset into K clusters. The experimental results obtained by using the new algorithm on different well-known datasets compared with those obtained by K-means algorithm, FCM, Genetic Algorithm (GA), Particle Swarm Optimization (PSO), Ant Colony Optimization (ACO), Artificial Bee Colony (ABC) Algorithm demonstrate the better performance of the new algorithm. © 2013 Elsevier Ltd.","Data clustering; Fuzzy C-means algorithm; SC-FCM algorithm; Stem cells algorithm (SCA)","Ant Colony Optimization (ACO); Artificial bee colony algorithms (ABC); Better performance; Data clustering; Fuzzy C means clustering; Fuzzy C-means algorithms; Fuzzy membership function; Stem Cells Algorithm (SCA); Artificial intelligence; Cluster analysis; Copying; Fuzzy clustering; Genetic algorithms; Particle swarm optimization (PSO); Stem cells; Clustering algorithms",Article,Scopus,2-s2.0-84876899318
"Yu Z., Chen H., You J., Han G., Li L.","Hybrid Fuzzy Cluster Ensemble Framework for Tumor Clustering from Biomolecular Data",2013,"IEEE/ACM Transactions on Computational Biology and Bioinformatics",15,10.1109/TCBB.2013.59,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896404801&doi=10.1109%2fTCBB.2013.59&partnerID=40&md5=e99bb3bcbe18df12a0069a53b10bdd1c","Cancer class discovery using biomolecular data is one of the most important tasks for cancer diagnosis and treatment. Tumor clustering from gene expression data provides a new way to perform cancer class discovery. Most of the existing research works adopt single-clustering algorithms to perform tumor clustering is from biomolecular data that lack robustness, stability, and accuracy. To further improve the performance of tumor clustering from biomolecular data, we introduce the fuzzy theory into the cluster ensemble framework for tumor clustering from biomolecular data, and propose four kinds of hybrid fuzzy cluster ensemble frameworks (HFCEF), named as HFCEF-I, HFCEF-II, HFCEF-III, and HFCEF-IV, respectively, to identify samples that belong to different types of cancers. The difference between HFCEF-I and HFCEF-II is that they adopt different ensemble generator approaches to generate a set of fuzzy matrices in the ensemble. Specifically, HFCEF-I applies the affinity propagation algorithm (AP) to perform clustering on the sample dimension and generates a set of fuzzy matrices in the ensemble based on the fuzzy membership function and base samples selected by AP. HFCEF-II adopts AP to perform clustering on the attribute dimension, generates a set of subspaces, and obtains a set of fuzzy matrices in the ensemble by performing fuzzy c-means on subspaces. Compared with HFCEF-I and HFCEF-II, HFCEF-III and HFCEF-IV consider the characteristics of HFCEF-I and HFCEF-II. HFCEF-III combines HFCEF-I and HFCEF-II in a serial way, while HFCEF-IV integrates HFCEF-I and HFCEF-II in a concurrent way. HFCEFs adopt suitable consensus functions, such as the fuzzy c-means algorithm or the normalized cut algorithm (Ncut), to summarize generated fuzzy matrices, and obtain the final results. The experiments on real data sets from UCI machine learning repository and cancer gene expression profiles illustrate that 1) the proposed hybrid fuzzy cluster ensemble frameworks work well on real data sets, especially biomolecular data, and 2) the proposed approaches are able to provide more robust, stable, and accurate results when compared with the state-of-the-art single clustering algorithms and traditional cluster ensemble approaches. © 2013 IEEE.","cancer discovery; Cluster ensemble; gene expression profiles; tumor clustering","Algorithms; Artificial intelligence; Copying; Diagnosis; Diseases; Fuzzy clustering; Fuzzy systems; Gene expression; Genes; Learning systems; Matrix algebra; Membership functions; Tumors; cancer discovery; Cancer gene expression; Cluster ensembles; Fuzzy C-means algorithms; Fuzzy membership function; Gene expression profiles; Single clustering algorithms; UCI machine learning repository; Clustering algorithms; algorithm; classification; cluster analysis; factual database; fuzzy logic; gene expression profiling; genetics; human; metabolism; neoplasm; procedures; Algorithms; Cluster Analysis; Databases, Factual; Fuzzy Logic; Gene Expression Profiling; Humans; Neoplasms",Article,Scopus,2-s2.0-84896404801
"Yet B., Bastani K., Raharjo H., Lifvergren S., Marsh W., Bergman B.","Decision support system for Warfarin therapy management using Bayesian networks",2013,"Decision Support Systems",15,10.1016/j.dss.2012.10.007,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878259809&doi=10.1016%2fj.dss.2012.10.007&partnerID=40&md5=fddf4228c536aedbb10287638758627e","Warfarin therapy is known as a complex process because of the variation in the patients' response. Failure to deal with such variation may lead to death as a result of thrombosis or bleeding. The possible sources of variation such as concomitant illnesses and drug interactions have to be investigated by the clinician in order to deal with the variation. This paper describes a decision support system (DSS) using Bayesian networks for assisting clinicians to make better decisions in Warfarin therapy management. The DSS is developed in collaboration with a Swedish hospital group that manages Warfarin therapy for more than 3000 patients. The proposed model can assist the clinician in making dose-adjustment and follow-up interval decisions, investigating variation causes, and evaluating bleeding and thrombosis risks related to therapy. The model is built upon previous findings from medical literature, the knowledge of domain experts, and large dataset of patients. © 2012 Elsevier B.V.","Anticoagulant therapy; Bayesian networks; Decision support systems; Warfarin therapy","Anticoagulant therapy; Complex Processes; Decision support system (dss); Follow-up intervals; Medical literatures; Sources of variation; Therapy management; Warfarin therapy; Artificial intelligence; Bayesian networks; Decision support systems; Diseases; Drug interactions; Patient treatment; Drug products",Article,Scopus,2-s2.0-84878259809
"Chatfield K., Zisserman A.","VISOR: Towards on-the-fly large-scale object category retrieval",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",15,10.1007/978-3-642-37444-9_34,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875911982&doi=10.1007%2f978-3-642-37444-9_34&partnerID=40&md5=ce9aa1a094cd5a7c27428eac841c4db4","This paper addresses the problem of object category retrieval in large unannotated image datasets. Our aim is to enable both fast learning of an object category model, and fast retrieval over the dataset. With these elements we show that new visual concepts can be learnt on-the-fly, given a text description, and so images of that category can then be retrieved from the dataset in realtime. To this end we compare state of the art encoding methods and introduce a novel cascade retrieval architecture, with a focus on achieving the best trade-off between three important performance measures for a realtime system of this kind, namely: (i) class accuracy, (ii) memory footprint, and (iii) speed. We show that an on-the-fly system is possible and compare its performance (using noisy training images) to that of using carefully curated images. For this evaluation we use the VOC 2007 dataset together with 100k images from ImageNet to act as distractors. © 2013 Springer-Verlag.",,"Encoding methods; Fast retrievals; Large-scale objects; Memory footprint; Object categories; Performance measure; State of the art; Visual concept; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84875911982
"Lee J., Lee J.","Hidden Information Revealed by Optimal Community Structure from a Protein-Complex Bipartite Network Improves Protein Function Prediction",2013,"PLoS ONE",15,10.1371/journal.pone.0060372,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875959983&doi=10.1371%2fjournal.pone.0060372&partnerID=40&md5=502d84eaccbab2c000ebd3392320090f","The task of extracting the maximal amount of information from a biological network has drawn much attention from researchers, for example, predicting the function of a protein from a protein-protein interaction (PPI) network. It is well known that biological networks consist of modules/communities, a set of nodes that are more densely inter-connected among themselves than with the rest of the network. However, practical applications of utilizing the community information have been rather limited. For protein function prediction on a network, it has been shown that none of the existing community-based protein function prediction methods outperform a simple neighbor-based method. Recently, we have shown that proper utilization of a highly optimal modularity community structure for protein function prediction can outperform neighbor-assisted methods. In this study, we propose two function prediction approaches on bipartite networks that consider the community structure information as well as the neighbor information from the network: 1) a simple screening method and 2) a random forest based method. We demonstrate that our community-assisted methods outperform neighbor-assisted methods and the random forest method yields the best performance. In addition, we show that using the optimal community structure information is essential for more accurate function prediction for the protein-complex bipartite network of Saccharomyces cerevisiae. Community detection can be carried out either using a modified modularity for dealing with the original bipartite network or first projecting the network into a single-mode network (i.e., PPI network) and then applying community detection to the reduced network. We find that the projection leads to the loss of information in a significant way. Since our prediction methods rely only on the network topology, they can be applied to various fields where an efficient network-based analysis is required. © 2013 Lee, Lee.",,"accuracy; article; calculation; cell division; community structure; controlled study; nonhuman; prediction; protein function; protein processing; protein protein interaction; protein transport; random forest; reproducibility; Saccharomyces cerevisiae; simulation; Algorithms; Artificial Intelligence; Computational Biology; Protein Conformation; Protein Interaction Maps; Saccharomyces cerevisiae; Saccharomyces cerevisiae Proteins; Saccharomyces cerevisiae",Article,Scopus,2-s2.0-84875959983
"Guo Q., Diaz F., Yom-Tov E.","Updating users about time critical events",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",15,10.1007/978-3-642-36973-5_41,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875468744&doi=10.1007%2f978-3-642-36973-5_41&partnerID=40&md5=12edb62530e0b525da5519f6b745d097","During unexpected events such as natural disasters, individuals rely on the information generated by news outlets to form their understanding of these events. This information, while often voluminous, is frequently degraded by the inclusion of unimportant, duplicate, or wrong information. It is important to be able to present users with only the novel, important information about these events as they develop. We present the problem of updating users about time critical news events, and focus on the task of deciding which information to select for updating users as an event develops. We propose a solution to this problem which incorporates techniques from information retrieval and multi-document summarization and evaluate this approach on a set of historic events using a large stream of news documents. We also introduce an evaluation method which is significantly less expensive than traditional approaches to temporal summarization. © 2013 Springer-Verlag.",,"Multi-document summarization; Natural disasters; Time-critical; Traditional approaches; Unexpected events; Artificial intelligence; Information retrieval",Conference Paper,Scopus,2-s2.0-84875468744
"Huang H.-Z., Liu Y., Li Y., Xue L., Wang Z.","New evaluation methods for conceptual design selection using computational intelligence techniques",2013,"Journal of Mechanical Science and Technology",15,10.1007/s12206-013-0123-x,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875399011&doi=10.1007%2fs12206-013-0123-x&partnerID=40&md5=ae0a5e97597d639f17f2aaded15d11df","The conceptual design selection, which aims at choosing the best or most desirable design scheme among several candidates for the subsequent detailed design stage, oftentimes requires a set of tools to conduct design evaluation. Using computational intelligence techniques, such as fuzzy logic, neural network, genetic algorithm, and physical programming, several design evaluation methods are put forth in this paper to realize the conceptual design selection under different scenarios. Depending on whether an evaluation criterion can be quantified or not, the linear physical programming (LPP) model and the RAOGA-based fuzzy neural network (FNN) model can be utilized to evaluate design alternatives in conceptual design stage. Furthermore, on the basis of Vanegas and Labib's work, a multi-level conceptual design evaluation model based on the new fuzzy weighted average (NFWA) and the fuzzy compromise decision-making method is developed to solve the design evaluation problem consisting of many hierarchical criteria. The effectiveness of the proposed methods is demonstrated via several illustrative examples. © 2013 The Korean Society of Mechanical Engineers and Springer-Verlag Berlin Heidelberg.","Conceptual design selection; Design evaluation; Fuzzy compromise decision-making; Fuzzy logic; Genetic algorithm; Linear physical programming; Neural network; NFWA","Computational intelligence techniques; Conceptual design stages; Decision-making method; Design evaluation; Fuzzy compromise decision-making; Fuzzy neural network model; Linear physical programming; NFWA; Artificial intelligence; Decision making; FORTH (programming language); Fuzzy logic; Fuzzy neural networks; Genetic algorithms; Neural networks; Conceptual design",Article,Scopus,2-s2.0-84875399011
"Yu-Hsin Chen G.","A new data structure of solution representation in hybrid ant colony optimization for large dynamic facility layout problems",2013,"International Journal of Production Economics",15,10.1016/j.ijpe.2012.12.012,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874534643&doi=10.1016%2fj.ijpe.2012.12.012&partnerID=40&md5=5f505a0e89f3044368adc1b55e832fb5","A dynamic facility layout problem (DFLP) is concerned with finding a set of facility layouts across multiple time periods that minimizes the total cost of material flows and rearrangement costs. Unlike other heuristic approaches that focus mainly on the searching aspect, this research takes another approach by streamlining the data structure of solution representation to improve the solution swapping and storing activities within a meta-heuristic framework. The experimental results from testing the data encoding and decoding schemes on a DFLP data set have been quite promising in terms of solution quality and computational time. © 2012 Elsevier B.V.","Ant colony optimization; Dynamic facility layout problem; Encoding/decoding schemes; Solution representation","Computational time; Data encoding; Data set; Dynamic facility layout problem; Dynamic facility layout problem (DFLP); Encoding/decoding; Facility layout; Heuristic approach; Hybrid ant colony optimization; Material Flow; Metaheuristic; Multiple-time periods; Solution quality; Solution representation; Algorithms; Ant colony optimization; Artificial intelligence; Heuristic methods; Plant layout; Statistical tests; Data structures",Conference Paper,Scopus,2-s2.0-84874534643
"Naredo E., Trujillo L., Martínez Y.","Searching for novel classifiers",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",15,10.1007/978-3-642-37207-0_13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875113669&doi=10.1007%2f978-3-642-37207-0_13&partnerID=40&md5=de0a620d9f1b18e59af7c7389c3f5627","Natural evolution is an open-ended search process without an a priori fitness function that needs to be optimized. On the other hand, evolutionary algorithms (EAs) rely on a clear and quantitative objective. The Novelty Search algorithm (NS) substitutes fitness-based selection with a novelty criteria; i.e., individuals are chosen based on their uniqueness. To do so, individuals are described by the behaviors they exhibit, instead of their phenotype or genetic content. NS has mostly been used in evolutionary robotics, where the concept of behavioral space can be clearly defined. Instead, this work applies NS to a more general problem domain, classification. To this end, two behavioral descriptors are proposed, each describing a classifier's performance from two different perspectives. Experimental results show that NS-based search can be used to derive effective classifiers. In particular, NS is best suited to solve difficult problems, where exploration needs to be encouraged and maintained. © 2013 Springer-Verlag.","Classification; Genetic Programming; Novelty Search","Classifier's performance; Evolutionary algorithms (EAs); Evolutionary robotics; Fitness functions; Natural evolution; Novelty Search; Quantitative objectives; Search Algorithms; Artificial intelligence; Classification (of information); Genetic programming",Conference Paper,Scopus,2-s2.0-84875113669
"Galitsky B.","Machine learning of syntactic parse trees for search and classification of text",2013,"Engineering Applications of Artificial Intelligence",15,10.1016/j.engappai.2012.09.017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873997220&doi=10.1016%2fj.engappai.2012.09.017&partnerID=40&md5=8706359c871096ae5862901aed30fa51","We build an open-source toolkit which implements deterministic learning to support search and text classification tasks. We extend the mechanism of logical generalization towards syntactic parse trees and attempt to detect weak semantic signals from them. Generalization of syntactic parse tree as a syntactic similarity measure is defined as the set of maximum common sub-trees and performed at a level of paragraphs, sentences, phrases and individual words. We analyze semantic features of such similarity measure and compare it with semantics of traditional anti-unification of terms. Nearest-neighbor machine learning is then applied to relate a sentence to a semantic class. Using syntactic parse tree-based similarity measure instead of bag-of-words and keyword frequency approach, we expect to detect a weak semantic signal otherwise unobservable. The proposed approach is evaluated in a four distinct domains where a lack of semantic information makes classification of sentences rather difficult. We describe a toolkit which is a part of Apache Software Foun-dation project OpenNLP, designed to aid search engineers in tasks requiring text relevance assessment. © 2012 Elsevier Ltd. All rights reserved.","Machine learning; Parse trees; Text classification; Text search","Anti-unification; Bag of words; Deterministic learning; Frequency approach; Nearest-neighbors; Open-source; Parse trees; Relevance assessments; Semantic class; Semantic features; Semantic information; Similarity measure; Syntactic parse tree; Syntactic similarities; Text classification; Text search; Tree-based; Unobservable; Classification (of information); Learning systems; Semantics; Signal detection; Syntactics; Text processing; Forestry; Artificial Intelligence; Classification; Data Processing; Forestry; Information Retrieval",Article,Scopus,2-s2.0-84873997220
"Khalili-Damghani K., Sadi-Nezhad S.","A decision support system for fuzzy multi-objective multi-period sustainable project selection",2013,"Computers and Industrial Engineering",15,10.1016/j.cie.2013.01.016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876098809&doi=10.1016%2fj.cie.2013.01.016&partnerID=40&md5=158077c208e4dd8c244915d222414fe3","In this paper, a Decision Support System (DSS) is developed to solve sustainable Multi-Objective Project Selection problem with Multi-Period Planning Horizon (MOPS-MPPH). First, a TOPSIS based fuzzy goal programming (FGP) is proposed which considered uncertain DM preferences on priority of achievement level of fuzzy goals. The FGP essentially considers economic factors like cost, profit, and budget. The output of FGP and other affecting factors (i.e. social and environmental effects, risk of investment, strategic alliance, and organizational readiness) are treated as inputs of a fuzzy rule based system to estimate fitness value of an investment. Properties of the proposed DSS are discussed. It also is compared with an existing procedure on historical data of a financial and credit institute. © 2013 Elsevier Ltd. All rights reserved.","Decision support system; Fuzzy goal programming; Fuzzy inference systems; Fuzzy relations; Sustainable multi-objective project selection","Decision support system (dss); Fuzzy goal programming; Fuzzy inference systems; Fuzzy relations; Organizational readiness; Project selection; Project selection problem; Social and environmental; Computer programming; Decision support systems; Profitability; Risk perception; Artificial intelligence",Article,Scopus,2-s2.0-84876098809
"Jamal S., Periwal V., Scaria V.","Predictive modeling of anti-malarial molecules inhibiting apicoplast formation",2013,"BMC Bioinformatics",15,10.1186/1471-2105-14-55,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873729520&doi=10.1186%2f1471-2105-14-55&partnerID=40&md5=71fb2339072db91cc648a41f1235601b","Background: Malaria is a major healthcare problem worldwide resulting in an estimated 0.65 million deaths every year. It is caused by the members of the parasite genus Plasmodium. The current therapeutic options for malaria are limited to a few classes of molecules, and are fast shrinking due to the emergence of widespread resistance to drugs in the pathogen. The recent availability of high-throughput phenotypic screen datasets for antimalarial activity offers a possibility to create computational models for bioactivity based on chemical descriptors of molecules with potential to accelerate drug discovery for malaria.Results: In the present study, we have used high-throughput screen datasets for the discovery of apicoplast inhibitors of the malarial pathogen as assayed from the delayed death response. We employed machine learning approach and developed computational predictive models to predict the biological activity of new antimalarial compounds. The molecules were further evaluated for common substructures using a Maximum Common Substructure (MCS) based approach.Conclusions: We created computational models using state-of-the-art machine learning algorithms. The models were evaluated based on multiple statistical criteria. We found Random Forest based approach provides for better accuracy as assessed from ROC curve analysis. We further evaluated the active molecules using a substructure based approach to identify common substructures enriched in the active set. We argue that the computational models generated could be effectively used to screen large molecular datasets to prioritize them for phenotypic screens, drastically reducing cost while improving the hit rate. © 2013 Jamal et al; licensee BioMed Central Ltd.",,"Antimalarial activity; Antimalarial compounds; Chemical descriptors; Healthcare problems; High-throughput screens; Machine learning approaches; Predictive modeling; Statistical criterion; Bioactivity; Computational methods; Decision trees; Diseases; Health care; Learning algorithms; Learning systems; Molecules; antimalarial agent; algorithm; article; artificial intelligence; chemistry; computer simulation; drug development; drug effect; high throughput screening; Plasmodium falciparum; Algorithms; Antimalarials; Artificial Intelligence; Computer Simulation; Drug Discovery; High-Throughput Screening Assays; Plasmodium falciparum",Article,Scopus,2-s2.0-84873729520
"Savojardo C., Fariselli P., Casadio R.","BETAWARE: A machine-learning tool to detect and predict transmembrane beta-barrel proteins in prokaryotes",2013,"Bioinformatics",15,10.1093/bioinformatics/bts728,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874329516&doi=10.1093%2fbioinformatics%2fbts728&partnerID=40&md5=180211794f2bbbc48652e2f014eb2d32","The annotation of membrane proteins in proteomes is an important problem of Computational Biology, especially after the development of high-throughput techniques that allow fast and efficient genome sequencing. Among membrane proteins, transmembrane β-barrels (TMBBs) are poorly represented in the database of protein structures (PDB) and difficult to identify with experimental approaches. They are, however, extremely important, playing key roles in several cell functions and bacterial pathogenicity. TMBBs are included in the lipid bilayer with a β-barrel structure and are presently found in the outer membranes of Gram-negative bacteria, mitochondria and chloroplasts. Recently, we developed two top-performing methods based on machine-learning approaches to tackle both the detection of TMBBs in sets of proteins and the prediction of their topology. Here, we present our BETAWARE program that includes both approaches and can run as a standalone program on a linux-based computer to easily address in-home massive protein annotation or filtering.Availability and implementation: http://www.biocomp.unibo.it/ ∼savojard/betawarecl. © 2013 The Author.",,"bacterial protein; membrane protein; outer membrane protein; article; artificial intelligence; chemistry; computer program; protein secondary structure; Artificial Intelligence; Bacterial Outer Membrane Proteins; Bacterial Proteins; Membrane Proteins; Protein Structure, Secondary; Software",Article,Scopus,2-s2.0-84874329516
"Jannach D., Zanker M.","Modeling and solving distributed configuration problems: A CSP-based approach",2013,"IEEE Transactions on Knowledge and Data Engineering",15,10.1109/TKDE.2011.236,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873322758&doi=10.1109%2fTKDE.2011.236&partnerID=40&md5=a7d5ed85523b9506c8cf4969e7a18dfc","Product configuration can be defined as the task of tailoring a product according to the specific needs of a customer. Due to the inherent complexity of this task, which for example includes the consideration of complex constraints or the automatic completion of partial configurations, various Artificial Intelligence techniques have been explored in the last decades to tackle such configuration problems. Most of the existing approaches adopt a single-site, centralized approach. In modern supply chain settings, however, the components of a customizable product may themselves be configurable, thus requiring a multisite, distributed approach. In this paper, we analyze the challenges of modeling and solving such distributed configuration problems and propose an approach based on Distributed Constraint Satisfaction. In particular, we advocate the use of Generative Constraint Satisfaction for knowledge modeling and show in an experimental evaluation that the use of generic constraints is particularly advantageous also in the distributed problem solving phase. © 1989-2012 IEEE.","distributed constraint satisfaction; Product configuration","Artificial intelligence techniques; Automatic completion; Centralized approaches; Complex constraints; Constraint Satisfaction; Customizable; Distributed approaches; Distributed configuration; Distributed constraint satisfaction; Distributed problem solving; Experimental evaluation; Inherent complexity; Knowledge modeling; Multi-site; Partial configuration; Product configuration; Supply chains; Problem solving",Article,Scopus,2-s2.0-84873322758
"Yang L., Hanneke S., Carbonell J.","A theory of transfer learning with applications to active learning",2013,"Machine Learning",15,10.1007/s10994-012-5310-y,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880569678&doi=10.1007%2fs10994-012-5310-y&partnerID=40&md5=550e028fb4584e50005e518dae172603","We explore a transfer learning setting, in which a finite sequence of target concepts are sampled independently with an unknown distribution from a known family. We study the total number of labeled examples required to learn all targets to an arbitrary specified expected accuracy, focusing on the asymptotics in the number of tasks and the desired accuracy. Our primary interest is formally understanding the fundamental benefits of transfer learning, compared to learning each target independently from the others. Our approach to the transfer problem is general, in the sense that it can be used with a variety of learning protocols. As a particularly interesting application, we study in detail the benefits of transfer for self-verifying active learning; in this setting, we find that the number of labeled examples required for learning with transfer is often significantly smaller than that required for learning each target independently. © 2012 The Author(s).","Active learning; Bayesian learning; Multi-task learning; Sample complexity; Statistical learning theory; Transfer learning","Active Learning; Bayesian learning; Multitask learning; Sample complexity; Statistical learning theory; Transfer learning; Artificial intelligence; Software engineering",Article,Scopus,2-s2.0-84880569678
"Kim M.-S., Lee S., Cypher D., Golmie N.","Performance analysis of fast handover for proxy Mobile IPv6",2013,"Information Sciences",15,10.1016/j.ins.2012.07.016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867577699&doi=10.1016%2fj.ins.2012.07.016&partnerID=40&md5=24ff4ca8645cf9436f2fec4461cb692b","In Proxy Mobile IPv6 (PMIPv6), any involvement by the Mobile Node (MN) is not required, so that any tunneling overhead can be removed from over-the-air. However, during the PMIPv6 handover process, there still exists a period when the MN is unable to send or receive packets because of PMIPv6 protocol operations, suffering from handover latency and data loss. Thus, to reduce the handover latency and data loss in PMIPv6, Fast Handover for PMIPv6 (PFMIPv6) is being standardized in the IETF. Nevertheless, PFMIPv6 has a few weaknesses: (1) handover initiation can be false, resulting in the PFMIPv6 handover process done so far becoming unnecessary. (2) Extra signaling is introduced in setting up an IP-in-IP tunnel between the serving and the new Mobile Access Gateways (MAGs). Therefore, in this paper, we present our study on the protocol overhead and performance aspects of PFMIPv6 in comparison with PMIPv6. We quantify the signaling overhead and the enhanced handover latency and data loss by conducting a thorough analysis of the performance aspects. The analysis is very meaningful to obtain important insights on how PFMIPv6 improves the handover performance over PMIPv6, especially in a highway vehicular traffic scenario where Base Stations (BSs)/Access Points (APs) can be placed in one dimensional space and MN's movements are quasi one-dimensional, so that the degree of certainty for an anticipated handover is increased. Further, our analytical study is verified by simulation results. © 2012 Elsevier Inc. All rights reserved.","Fast handover; Handover latency; Packet loss; Proxy Mobile IPv6; Signaling cost; Vehicular network","Fast handovers; Handover latency; Proxy Mobile IPv6; Signaling costs; Vehicular networks; Artificial intelligence; Packet loss; Software engineering; Internet protocols",Article,Scopus,2-s2.0-84867577699
"Mensink T., Verbeek J., Csurka G.","Tree-structured CRF models for interactive image labeling",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",15,10.1109/TPAMI.2012.100,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871806574&doi=10.1109%2fTPAMI.2012.100&partnerID=40&md5=5747016bb4ea0a2a384e4fdcabb95ff5","We propose structured prediction models for image labeling that explicitly take into account dependencies among image labels. In our tree-structured models, image labels are nodes, and edges encode dependency relations. To allow for more complex dependencies, we combine labels in a single node and use mixtures of trees. Our models are more expressive than independent predictors, and lead to more accurate label predictions. The gain becomes more significant in an interactive scenario where a user provides the value of some of the image labels at test time. Such an interactive scenario offers an interesting tradeoff between label accuracy and manual labeling effort. The structured models are used to decide which labels should be set by the user, and transfer the user input to more accurate predictions on other image labels. We also apply our models to attribute-based image classification, where attribute predictions of a test image are mapped to class probabilities by means of a given attribute-class mapping. Experimental results on three publicly available benchmark datasets show that in all scenarios our structured models lead to more accurate predictions, and leverage user input much more effectively than state-of-the-art independent models. © 2012 IEEE.","content analysis and indexing; object recognition; Pattern recognition application computer vision; pattern recognition interactive systems; statistical pattern recognition","Accurate prediction; Benchmark datasets; Class probabilities; Content analysis; Dependency relation; Image labeling; Independent model; Independent predictors; Interactive system; Statistical pattern recognition; Structured model; Structured prediction; Test images; Test time; User input; Computer vision; Forecasting; Object recognition; Forestry; Forecasts; Forestry; Image Analysis; Pattern Recognition; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; computer interface; documentation; hospital information system; image enhancement; methodology; reproducibility; sensitivity and specificity; Algorithms; Artificial Intelligence; Documentation; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Radiology Information Systems; Reproducibility of Results; Sensitivity and Specificity; User-Computer Interface",Article,Scopus,2-s2.0-84871806574
"Le Mortellec A., Clarhaut J., Sallez Y., Berger T., Trentesaux D.","Embedded holonic fault diagnosis of complex transportation systems",2013,"Engineering Applications of Artificial Intelligence",15,10.1016/j.engappai.2012.09.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870066993&doi=10.1016%2fj.engappai.2012.09.008&partnerID=40&md5=a7cd3791337660e0c222eade592ca21a","The use of electronic equipment and embedded computing technologies in modern complex transportation systems continues to grow in a highly competitive market, in which product maintainability and availability is vital. These technological advances also make fault diagnosis and maintenance interventions much more challenging, since these operations require a deep understanding of the entire system. This paper proposes a holonic cooperative fault diagnosis approach, along with a generic architecture, to increase the embedded diagnosis capabilities of complex transportation systems. This concept is applied to the fault diagnosis of door systems of a railway transportation system. © 2012 Elsevier Ltd. All rights reserved.","Cooperative fault diagnosis; Corrective maintenance; Embedded diagnosis; Holonic architecture; Model-based diagnosis; Railway transportation system","Competitive markets; Corrective maintenance; Door system; Embedded computing; Entire system; Generic architecture; Holonic architecture; Holonics; Model based diagnosis; Railway transportation; Technological advances; Transportation system; Applications; Artificial intelligence; Railroads",Article,Scopus,2-s2.0-84870066993
"Baskaran R., Victer Paul P., Dhavachelvan P.","Ant colony optimization for data cache technique in MANET",2013,"Advances in Intelligent Systems and Computing",15,10.1007/978-81-322-0740-5_104,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871284907&doi=10.1007%2f978-81-322-0740-5_104&partnerID=40&md5=17a1b8636a7df9fd7b952e974518758c","Mobile ad hoc networks (MANETs) are collection of distributed nodes which communicate using multi-hop wireless links. In order to manage the frequent topology change and ineffective communication, Distributed Spanning Tree (DST) interconnection technique can be used. DST technique guarantee network connectivity, efficient routing and maintain network performance in MANET. In MANET, nodes work for tasks of similar goal (common interest). So, most of the nodes try to access the similar data at different period. By using Data cache system (DCS), we can improve the efficiency of MANET to some extent [1]. In this paper, Ant Colony Optimization (ACO) technique is used to enhance the efficiency of data cache system in MANET at higher level. ACO technique improves the data transfer speed by finding optimal path between nodes of MANET in dynamic fashion. Analysis from simulation of our proposed work shows that data cache system efficiency can be improved using DST technique. © 2013 Springer.",,"Algorithms; Artificial intelligence; Cache memory; Data transfer; Distributed parameter networks; Efficiency; Network performance; Ant Colony Optimization (ACO); Data caches; Data transfer speed; Distributed nodes; Distributed spanning trees; Efficient routing; Interconnection technique; Multihop; Network connectivity; Optimal paths; Wireless link; Mobile ad hoc networks",Conference Paper,Scopus,2-s2.0-84871284907
"Janjua N.K., Hussain F.K., Hussain O.K.","Semantic information and knowledge integration through argumentative reasoning to support intelligent decision making",2013,"Information Systems Frontiers",15,10.1007/s10796-012-9365-x,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883050957&doi=10.1007%2fs10796-012-9365-x&partnerID=40&md5=66b300d09e97bf6909b88164142671e4","The availability of integrated, high quality information is a pre-requisite for a decision support system (DSS) to aid in the decision-making process. The introduction of semantic web ensures the seamless integration of information derived from diverse sources and transforms the DSS to an adoptable and flexible SemanticWeb-DSS (Web-DSS).However, due to the monotonic nature of the layered development of semantic web, it lacks the capability to represent, reason and integrate incomplete and conflicting information. This, in turn, renders an enterprise incapable of knowledge integration; that is, integration of information about a subject that could potentially be incomplete, inconsistent and distributed among different Web-DSS within or across enterprises. In this article, we address the issues of incomplete and inconsistent semantic information and knowledge integration by using argumentation and argumentation schemes. We discuss the Argumentation-enabled Information Integration Web-DSS (Web@IDSS) along with its syntax and semantics for semantic information integration, and devise a methodology for sharing the results of Web@IDSS in Argument Interchange Format (AIF) format. We also discuss Argumentation-enabled Knowledge Integration Web-DSS (Web@KIDSS) for semantic knowledge integration. We provide formal syntax and semantics for the Web@KIDSS, propose a conceptual framework, and describe it in detail. We present the algorithms for knowledge integration and the prototype application for validation of results. © Springer Science+Business Media New York 2013.","Argumentation; Argumentation schemes; Information integration; Semantic web; Web based DSS","Artificial intelligence; Automobile drivers; Decision making; Decision support systems; Industry; Information management; Information retrieval; Semantics; Syntactics; Argumentation; Argumentation schemes; Decision making process; Decision support system (dss); High quality information; Information integration; Intelligent decision making; Web based; Semantic Web",Article,Scopus,2-s2.0-84883050957
"Ruiz-Rodriguez F.J., Gomez-Gonzalez M., Jurado F.","Optimization of radial systems with biomass fueled gas engine from a metaheuristic and probabilistic point of view",2013,"Energy Conversion and Management",15,10.1016/j.enconman.2012.09.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867435645&doi=10.1016%2fj.enconman.2012.09.002&partnerID=40&md5=09cd8a552472a32d66c814f317601019","This paper shows that the technical constraints must be considered in radial distribution networks, where the voltage regulation is one of the primary problems to be dealt in distributed generation systems based on biomass fueled engine. Loads and distributed generation production are modeled as random variables. Results prove that the proposed method can be applied for the keeping of voltages within desired limits at all load buses of a distribution system with biomass fueled gas engines. To evaluate the performance of this distribution system, this paper has developed a probabilistic model that takes into account the random nature of lower heat value of biomass and load. The Cornish-Fisher expansion is used for approximating quantiles of a random variable. This work introduces a hybrid method that utilizes a new optimization method based on swarm intelligence and probabilistic radial load flow. It is demonstrated the reduction in computation time achieved by the more efficient probabilistic load flow in comparison to Monte Carlo simulation. Acceptable solutions are reached in a smaller number of iterations. Therefore, convergence is more rapidly attained and computational cost is significantly lower than that required for Monte Carlo methods. © 2012 Elsevier Ltd. All rights reserved.","Binary Particle Swarm Optimization; Biomass; Monte Carlo method; Optimal power flow; Probabilistic load flow","Binary particle swarm optimization; Computation time; Computational costs; Cornish-Fisher expansion; Distributed generation system; Distribution systems; Heat value; Hybrid method; Metaheuristic; Monte Carlo Simulation; Number of iterations; Optimal power flows; Optimization method; Probabilistic load flow; Probabilistic models; Radial distribution networks; Radial loads; Radial systems; Swarm Intelligence; Technical constraints; Artificial intelligence; Biomass; Distributed power generation; Electric load flow; Electric power system interconnection; Gas engines; Random variables; Voltage regulators; Monte Carlo methods",Article,Scopus,2-s2.0-84867435645
"Cerreta M., Poli G.","A complex values map of marginal urban landscapes: An experiment in Naples (Italy)",2013,"International Journal of Agricultural and Environmental Information Systems",15,10.4018/ijaeis.2013070103,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898909139&doi=10.4018%2fijaeis.2013070103&partnerID=40&md5=22f0a5e17f07128b3d5fb86d7054a07b","The paper seeks to highlight how the character and values of marginal landscapes can be considered examples of the ""third landscape"" and how they can enable the mapping of tangible and intangible change (and its relative effects) through the selection of appropriate spatial indicators. A Dynamic Spatial Decision Support System was constructed to help identify the complex values characterizing a selected area in Naples' marginal urban landscape. The System consists of a set of selected indicators that represent the area's distinguishing characteristics with specific focus on its critical and potential factors. The formulation of spatial indicators helps identify the ecological, socio-economic and urban characteristics of the case-study area, along with their historic evolution over a specific time period. Time History Analysis (THA) enables the identification of permanent elements and transformations along with the relationships among the various urban landscape components over time. The synergistic interaction between the appropriately structured Geographic Information System (GIS) and the multi-criteria Analytic Hierarchy Process (AHP) method help to generate a complex values map of the analyzed landscape to support the definition and location of strategic actions. Copyright © 2013, IGI Global.","Analytic Hierarchy Process (AHP); Complex Values; Dynamic Spatial Decision Support System (DSDSS); Multi-Criteria Analysis (MCA); Time History Analysis (THA); Urban Landscape","Analytic hierarchy process; Artificial intelligence; Decision support systems; Hierarchical systems; Analytic hierarchy process (ahp); Complex values; Dynamic spatial; Multi-criteria analysis; Time history analysis; Urban landscape; Geographic information systems; analytical hierarchy process; decision support system; GIS; landscape; mapping method; multicriteria analysis; urban area; Campania [Italy]; Italy; Naples; Napoli [Campania]",Article,Scopus,2-s2.0-84898909139
"Rychtyckyj N., Ostrowski D., Schleis G., Reynolds R.G.","Using cultural algorithms in industry",2013,"2003 IEEE Swarm Intelligence Symposium, SIS 2003 - Proceedings",15,10.1109/SIS.2003.1202266,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942155266&doi=10.1109%2fSIS.2003.1202266&partnerID=40&md5=94ac0121804ec939abf531899c2841f4","Evolutionary computation has been successfully applied in a variety of problem domains and applications. In this paper we discuss the use of a specific form of evolutionary computation known as cultural algorithms that has been applied successfully in various real-world applications to solve problems of a very dynamic and complex nature. Cultural algorithms introduce a learning component into an evolutionary framework that influences the search strategy and is in turn modified by the best-performing members of the population during the entire process. Cultural algorithms have been used in various applications, including fraud analysis for automotive accident claims, the re-engineering of a dynamic automobile manufacturing knowledge base, the modeling of pricing strategies for automobiles in a multi-agent environment and for data mining. © 2003 IEEE.","Automobile manufacture; Computer industry; Cultural differences; Evolutionary computation; Information technology; Laboratories; Manufacturing industries; Pricing; Vehicle dynamics; Vehicles","Accidents; Algorithms; Amphibious vehicles; Artificial intelligence; Automobile manufacture; Automobiles; Calculations; Costs; Data mining; Information technology; Knowledge based systems; Laboratories; Manufacture; Multi agent systems; Vehicles; Automobile manufacturing; Computer industry; Cultural Algorithm; Cultural difference; Evolutionary framework; Manufacturing industries; Multi-agent environment; Vehicle dynamics; Evolutionary algorithms",Conference Paper,Scopus,2-s2.0-84942155266
"Ruz G.A., Goles E.","Learning gene regulatory networks using the bees algorithm",2013,"Neural Computing and Applications",15,10.1007/s00521-011-0750-z,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871945481&doi=10.1007%2fs00521-011-0750-z&partnerID=40&md5=335f827e4ee0bc93d3078e9adc04395f","Learning gene regulatory networks under the threshold Boolean network model is presented. To accomplish this, the swarm intelligence technique called the bees algorithm is formulated to learn networks with predefined attractors. The resulting technique is compared with simulated annealing through simulations. The ability of the networks to preserve the attractors when the updating schemes is changed from parallel to sequential is analyzed as well. Results show that Boolean networks are not very robust when the updating scheme is changed. Robust networks were found only for limit cycle length equal to two and specific network topologies. Throughout the simulations, the bees algorithm outperformed simulated annealing, showing the effectiveness of this swarm intelligence technique for this particular application. © 2011 Springer-Verlag London Limited.","Attractors; Boolean networks; Simulated annealing; Swarm intelligence; The bees algorithm","Attractors; Bees algorithms; Boolean network models; Boolean Networks; Gene regulatory networks; Limit cycle; Network topology; Robust network; Swarm Intelligence; Swarm intelligence techniques; Artificial intelligence; Boolean algebra; Dynamical systems; Electric network topology; Genes; Simulated annealing; Algorithms",Article,Scopus,2-s2.0-84871945481
"Zhang J., Wang Y., Vassilev J.","SocConnect: A personalized social network aggregator and recommender",2013,"Information Processing and Management",15,10.1016/j.ipm.2012.07.006,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878112328&doi=10.1016%2fj.ipm.2012.07.006&partnerID=40&md5=364669d8cca36dbafd637634f16712c0","Users of Social Networking Sites (SNSs) like Facebook, LinkedIn or Twitter, are facing two problems: (1) it is difficult for them to keep track of their social friendships and friends' social activities scattered across different SNSs; and (2) they are often overwhelmed by the huge amount of social data (friends' updates and other activities). To address these two problems, we propose a user-centric system called ''SocConnect'' (Social Connect) for aggregating social data from different SNSs and allowing users to create personalized social and semantic contexts for their social data. Users can blend and group friends on different SNSs, and rate the friends and their activities as favourite, neutral or disliked. Soc- Connect then provides personalized recommendation of friends' activities that may be interesting to each user, using machine learning techniques. A prototype is also implemented to demonstrate these functionalities of SocConnect. Evaluation on real users confirms that users generally like the proposed functionalities of our system, and machine learning can be effectively applied to provide personalized recommendation of friends' activities and help users deal with cognitive overload. © 2012 Elsevier Ltd. All rights reserved.","Personalized recommendation of activities; Social network aggregator; Social networking site","Artificial intelligence; Learning systems; Semantics; Websites; Cognitive overload; Machine learning techniques; Personalized recommendation; Social activities; Social network aggregators; Social networking sites; Social networking sites (SNSs); User-centric systems; Social networking (online)",Article,Scopus,2-s2.0-84878112328
"Hemmatian H., Fereidoon A., Sadollah A., Bahreininejad A.","Optimization of laminate stacking sequence for minimizing weight and cost using elitist ant system optimization",2013,"Advances in Engineering Software",15,10.1016/j.advengsoft.2012.11.005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871157665&doi=10.1016%2fj.advengsoft.2012.11.005&partnerID=40&md5=f5b3a5e142e10f2c24f24b8bd4411adb","This paper presents the application of ant colony optimization (ACO) for the multi-objective optimization of hybrid laminates for obtaining minimum weight and cost. The investigated laminate is made of glass-epoxy and graphite-epoxy plies to combine the lightness and economical attributes of the first with the high-stiffness property of the second using a modified variation of ACO so called the elitist ant system (EAS) in order to make the tradeoff between the cost and weight as the objective functions. First natural frequency was considered as a constraint. The obtained results using the EAS method including the Pareto set, optimum stacking sequences, and the number of plies made of either glass or graphite fibers were compared with those using the genetic algorithm (GA) and any colony system (ACS) reported in literature. The comparisons confirm the advantage of hybridization and showed that the EAS algorithm outperformed the GA and ACS in terms of function's value and constraint accuracy. © 2013 Elsevier Ltd. All rights reserved.","Ant colony optimization; Elitist ant system; Genetic algorithm; Laminated composite materials; Multi-objective optimization; Stacking sequence","Artificial intelligence; Costs; Glass; Graphite fibers; Laminated composites; Multiobjective optimization; Ant Colony Optimization (ACO); Ant systems; Hybrid laminates; Minimum weight; Multi objective optimizations (MOO); Objective functions; Pareto set; Stacking sequence; Genetic algorithms",Article,Scopus,2-s2.0-84871157665
"Allahviranloo M., Recker W.","Daily activity pattern recognition by using support vector machines with multiple classes",2013,"Transportation Research Part B: Methodological",15,10.1016/j.trb.2013.09.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886782743&doi=10.1016%2fj.trb.2013.09.008&partnerID=40&md5=befea52819fad95d6dbba8ae38c9c8da","The focus of this paper is to learn the daily activity engagement patterns of travelers using Support Vector Machines (SVMs), a modeling approach that is widely used in Artificial intelligence and Machine Learning. It is postulated that an individual's choice of activities depends not only on socio-demographic characteristics but also on previous activities of individual on the same day. In the paper, Markov Chain models are used to study the sequential choice of activities. The dependencies among activity type, activity sequence and socio-demographic data are captured by employing hidden Markov models. In order to learn model parameters, we use sequential multinomial logit models (MNL) and multiclass Support Vector Machines (K-SVM) with two different dependency structures. In the first dependency structure, it is assumed that type of activity at time '. t' depends on the last previous activity and socio-demographic data, whereas in the second structure we assume that activity selection at time '. t' depends on all of the individual's previous activity types on the same day and socio-demographic characteristics. The models are applied to data drawn from a set of California households and a comparison of the accuracy of estimation of activity types and their sequence in the agenda, indicates the superiority of K-SVM models over MNL. Additionally, we show that accuracy in estimating activity patterns increases using different sets of explanatory variables or tuning parameters of the kernel function in K-SVM. © 2013 Elsevier Ltd.","Activity pattern recognition; Activity sequence; Hidden Markov Models (HMMs); Support Vector Machines (SVMs)","Artificial intelligence; Hidden Markov models; Learning systems; Markov processes; Pattern recognition; Population statistics; Vectors; Activity patterns; Activity sequence; Daily activity patterns; Hidden markov models (HMMs); Multi-class support vector machines; Multinomial logit model; Socio-demographic characteristics; Support vector machine (SVMs); Support vector machines; accuracy assessment; artificial intelligence; diurnal activity; estimation method; learning; Markov chain; model test; numerical model; pattern recognition; travel behavior; California; United States",Article,Scopus,2-s2.0-84886782743
"Yadav P., Mishra N., Sharma S.","A secure video steganography with encryption based on LSB technique",2013,"2013 IEEE International Conference on Computational Intelligence and Computing Research, IEEE ICCIC 2013",15,10.1109/ICCIC.2013.6724212,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894255343&doi=10.1109%2fICCIC.2013.6724212&partnerID=40&md5=ba42dd7fb760fd7f362dc02f6d051d07","Need of hiding information from intruders has been around since ancient times. Nowadays Digital media is getting advanced like text, image, audio, video etc. To maintain the secrecy of information, different methods of hiding have been evolved. One of them is Steganography, which means hiding information under some other information without noticeable change in cover information. Recently Video Steganography has become a boon for providing large amount of data to be transferred secretly. Video is simply a sequence of images, hence much space is available in between for hiding information. In proposed scheme video steganography is used to hide a secret video stream in cover video stream. Each frame of secret video will be broken into individual components then converted into 8-bit binary values, and encrypted using XOR with secret key and encrypted frames will be hidden in the least significant bit of each frames using sequential encoding of Cover video. To enhance more security each bit of secret frames will be stored in cover frames following a pattern BGRRGBGR. © 2013 IEEE.","Encryption; Image steganography; Sequential Coding; Video Steganography","Artificial intelligence; Cryptography; Digital storage; Image coding; Video streaming; Hiding informations; Image steganography; Individual components; Least significant bits; Lsb techniques; Sequence of images; Sequential coding; Video steganography; Steganography",Conference Paper,Scopus,2-s2.0-84894255343
"Collan M., Fedrizzi M., Luukka P.","A multi-expert system for ranking patents: An approach based on fuzzy pay-off distributions and a TOPSIS-AHP framework",2013,"Expert Systems with Applications",15,10.1016/j.eswa.2013.02.012,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885030135&doi=10.1016%2fj.eswa.2013.02.012&partnerID=40&md5=53564c44f6a1dd7ae1a053d915657a0e","The aim of this paper is to introduce a decision support system that ranks patents based on multiple expert evaluations. The presented approach starts with the creation of three value scenarios for each considered patent by each expert. These are used for the construction of individual fuzzy pay-off distribution functions for the patent value; a consensual fuzzy pay-off distribution is then determined starting from the individual distributions. Possibilistic moments are calculated from the consensus pay-off distribution for each patent and used in ranking them with TOPSIS. It is further showed how the analytic hierarchy process (AHP) can be used to include additional decision variables into the patent selection, thus allowing for a two-tier decision making process. The system is illustrated with a numerical example and the usability of the system and the combination of methods it includes for patent portfolio selection in the real world context is discussed. © 2013 Elsevier Ltd. All rights reserved.","AHP; Consensus; Patents; Pay-off method; Possibilistic moments; TOPSIS","Analytic hierarchy process; Artificial intelligence; Decision making; Decision support systems; Distribution functions; Expert systems; Hierarchical systems; Numerical methods; Consensus; Patents; Pay-off method; Possibilistic moments; TOPSIS; Patents and inventions",Article,Scopus,2-s2.0-84885030135
"Elewa H.H., Shohaib R.E., Qaddah A.A., Nousir A.M.","Determining groundwater protection zones for the Quaternary aquifer of northeastern Nile Delta using GIS-based vulnerability mapping",2013,"Environmental Earth Sciences",15,10.1007/s12665-012-1740-x,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872005632&doi=10.1007%2fs12665-012-1740-x&partnerID=40&md5=80c0b7176ff7326f9b1e621d07964c95","The area of study lies at the northeastern part of Nile Delta. Global shoreline regression and sea-level rise have their own-bearing on the groundwater salinization due to seawater intrusion. A new adopted approach for vulnerability mapping using the hydrochemical investigations, geographic information system and a weighted multi-criteria decision support system (WMCDSS) was developed to determine the trend of groundwater contamination by seawater intrusion. Six thematic layers were digitally integrated and assigned different weights and rates. These have been created to comprise the most decisive criteria used for the delineation of groundwater degradation due to seawater intrusion. These criteria are represented by the total dissolved solids, well discharge, sodium adsorption ratio, hydrochemical parameter (Cl/HCO3), hydraulic conductivity and water types. The WMCDSS modeling was tried, where a groundwater vulnerability map with four classes ranging from very low to high vulnerability was gained. The map pinpointed the promising localities for groundwater protection, which are almost represented by the very low or low vulnerability areas (53. 69 % of the total study area). The regions having high and moderate groundwater vulnerability occupy 46. 31 % of total study area, which designate to a deteriorated territory of groundwater quality, and needs special treatment and cropping pattern before use. However, the moderate groundwater vulnerability class occupies an area of about 28.77 % of the total mapped area, which highlighted the need for certain management practices to prevent the saltwater intrusion from expanding further to the south. There was a good correlation of the constructed vulnerability map with the recently gathered water quality data and hydrochemical facies evolution. The plotting of water quality data on Piper trilinear diagram revealed the evolution of freshwater into the mixing and the saline zones as an impact of seawater intrusion, which validates the model results. © 2012 Springer-Verlag.","Egypt; Geographic information system; Groundwater protection zones; River Nile Delta; Saltwater intrusion; Vulnerability mapping","Cropping patterns; Egypt; Good correlations; Ground water protection; Ground-water qualities; Groundwater contamination; Groundwater vulnerability; Hydrochemical facies; Hydrochemical parameters; Hydrochemicals; Low-to-high; Management practices; Model results; Multi-criteria decision support systems; Nile delta; Piper Trilinear diagram; Sea level rise; Sodium adsorption ratio; Special treatments; Study areas; Thematic layers; Total dissolved solids; Vulnerability maps; Water quality data; Water types; Aquifers; Artificial intelligence; Decision support systems; Geographic information systems; Hydrochemistry; Mapping; Salt water intrusion; Sea level; Water quality; Groundwater resources; adsorption; aquifer; data set; facies; GIS; management practice; mapping method; Quaternary; saline intrusion; salinization; sea level change; vulnerability; water quality; Egypt; Nile Delta",Article,Scopus,2-s2.0-84872005632
"Paul S., Boutsidis C., Magdon-Ismail M., Drineas P.","Random projections for support vector machines",2013,"Journal of Machine Learning Research",15,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954208045&partnerID=40&md5=47b0488392db8823dde8eb3e8c97e119","Let X be a data matrix of rank ρ, representing n points in d-dimensional space. The linear support vector machine constructs a hyperplane separator that maximizes the 1- norm soft margin. We develop a new oblivious dimension reduction technique which is precomputed and can be applied to any input matrix X. We prove that, with high probability, the margin and minimum enclosing ball in the feature space are preserved to within ε-relative error, ensuring comparable generalization as in the original space. We present extensive experiments with real and synthetic data to support our theory. Copyright 2013 by the authors.",,"Artificial intelligence; Matrix algebra; D-dimensional spaces; Dimension reduction techniques; High probability; Linear Support Vector Machines; Minimum enclosing ball; Random projections; Relative errors; Synthetic data; Support vector machines",Conference Paper,Scopus,2-s2.0-84954208045
"Nikam V.B., Meshram B.B.","Modeling rainfall prediction using data mining method: A bayesian approach",2013,"Proceedings of International Conference on Computational Intelligence, Modelling and Simulation",15,10.1109/CIMSim.2013.29,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892738290&doi=10.1109%2fCIMSim.2013.29&partnerID=40&md5=c77e878d22e58cc1c61aa1efa413824e","Weather forecasting has been one of the most scientifically and technologically challenging problem around the world. Weather data is one of the meteorological data that is rich with important information, which can be used for weather prediction We extract knowledge from weather historical data collected from Indian Meteorological Department (IMD) Pune. From the collected weather data comprising of 36 attributes, only 7 attributes are most relevant to rainfall prediction. We made data preprocessing and data transformation on raw weather data set, so that it shall be possible to work on Bayesian, the data mining, prediction model used for rainfall prediction. The model is trained using the training data set and has been tested for accuracy on available test data. The meteorological centers uses high performance computing and supercomputing power to run weather prediction model. To address the issue of compute intensive rainfall prediction model, we proposed and implemented data intensive model using data mining technique. Our model works with good accuracy and takes moderate compute resources to predict the rainfall. We have used Bayesian approach to prove our model for rainfall prediction, and found to be working well with good accuracy. © 2013 IEEE.","Bayesian; Data Mining; High Performance Computing; Rainfall prediction","Artificial intelligence; Bayesian networks; Mathematical models; Meteorology; Rain; Weather forecasting; Bayesian; Bayesian approaches; Data mining methods; Data transformation; High performance computing; Meteorological data; Rainfall prediction; Weather prediction model; Data mining",Conference Paper,Scopus,2-s2.0-84892738290
"Hosseini R., Brusilovsky P.","JavaParser: A fine-grain concept indexing tool for java problems",2013,"CEUR Workshop Proceedings",15,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925013137&partnerID=40&md5=68f0051058145d9e431059b7c4248fc0","Multi-concept nature of problems in the domain of programming languages requires fine-grained indexing which is critical for sequencing purposes. In this paper, we propose an approach for extracting this set of concepts in a reliable automated way using JavaParser tool. To demonstrate the importance of fine-grained sequencing, we provide an example showing how this information can be used for problem sequencing during exam preparation.","Indexing; Java programming; Parser; Sequencing","Artificial intelligence; Computer programming; Indexing (of information); Problem oriented languages; Concept indexing; Fine grained; Fine grains; Java programming; Parser; Problem sequencing; Sequencing; Java programming language",Conference Paper,Scopus,2-s2.0-84925013137
"Dondelinger F., Filippone M., Rogers S., Husmeier D.","ODE parameter inference using adaptive gradient matching with Gaussian processes",2013,"Journal of Machine Learning Research",15,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928994738&partnerID=40&md5=b5059d035622ed23f17ac2fdaf6f9c47","Parameter inference in mechanistic models based on systems of coupled differential equa- tions is a topical yet computationally chal- lenging problem, due to the need to fol- low each parameter adaptation with a nu- merical integration of the differential equa- tions. Techniques based on gradient match- ing, which aim to minimize the discrepancy between the slope of a data interpolant and the derivatives predicted from the differen- tial equations, offer a computationally ap- pealing shortcut to the inference problem. The present paper discusses a method based on nonparametric Bayesian statistics with Gaussian processes due to Calderhead et al. (2008), and shows how inference in this model can be substantially improved by consistently sampling from the joint distribution of the ODE parameters and GP hyperparameters. We demonstrate the efficiency of our adaptive gradient matching technique on three bench- mark systems, and perform a detailed com- parison with the method in Calderhead et al. (2008) and the explicit ODE integration ap- proach, both in terms of parameter inference accuracy and in terms of computational effi- ciency. Copyright 2013 by the authors.",,"Artificial intelligence; Gaussian distribution; Gaussian noise (electronic); Gaussian Processes; Gradient matching; Inference problem; Joint distributions; Mechanistic models; Non-parametric Bayesian; Parameter adaptation; Parameter inference; Ordinary differential equations",Conference Paper,Scopus,2-s2.0-84928994738
"Smirnov A., Kashevnik A., Ponomarev A., Shilov N., Schekotov M., Teslya N.","Recommendation system for tourist attraction information service",2013,"Conference of Open Innovation Association, FRUCT",15,10.1109/FRUCT.2013.6737957,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898799122&doi=10.1109%2fFRUCT.2013.6737957&partnerID=40&md5=fc4aa2a3ba82c923954ea7ec6adf33e4","The paper proposes a description of information decision support system in the tourism domain and a set of methods and algorithms for generating recommendations for a user that allow significant increase of the system usability. The system generates for the user recommendations which attractions at the moment are better to attend based on the user preferences and the current situation in the location area. The system also allows showing the user information about interesting attraction in more detail, which is based on analyzing information evaluations made by other users. © 2013 FRUCT Oy.","context management; mobile devices; ontology; ratings; recommendations; tourism","Artificial intelligence; Decision support systems; Innovation; Mobile devices; Mobile telecommunication systems; Ontology; Rating; Context management; Current situation; Information evaluation; recommendations; tourism; Tourist attractions; User information; User recommendations; Information services",Conference Paper,Scopus,2-s2.0-84898799122
"Alterman M., Schechner Y.Y., Perona P., Shamir J.","Detecting motion through dynamic refraction",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",15,10.1109/TPAMI.2012.192,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870188466&doi=10.1109%2fTPAMI.2012.192&partnerID=40&md5=1fb91174bca9a5767703c6ada66b2f75","Refraction causes random dynamic distortions in atmospheric turbulence and in views across a water interface. The latter scenario is experienced by submerged animals seeking to detect prey or avoid predators, which may be airborne or on land. Man encounters this when surveying a scene by a submarine or divers while wishing to avoid the use of an attention-drawing periscope. The problem of inverting random refracted dynamic distortions is difficult, particularly when some of the objects in the field of view (FOV) are moving. On the other hand, in many cases, just those moving objects are of interest, as they reveal animal, human, or machine activity. Furthermore, detecting and tracking these objects does not necessitate handling the difficult task of complete recovery of the scene. We show that moving objects can be detected very simply, with low false-positive rates, even when the distortions are very strong and dominate the object motion. Moreover, the moving object can be detected even if it has zero mean motion. While the object and distortion motions are random and unknown, they are mutually independent. This is expressed by a simple motion feature which enables discrimination of moving object points versus the background. © 1979-2012 IEEE.","classification; distortion; Motion detection; random media; refraction","Dynamic distortions; Field of views; Mean motion; Motion detection; Motion features; Moving objects; Mutually independents; Object motion; Random dynamics; Random media; Water interface; Animals; Atmospheric turbulence; Classification (of information); Distortion (waves); Refraction; algorithm; article; artifact; artificial intelligence; automated pattern recognition; computer assisted diagnosis; methodology; motion; refractometry; Algorithms; Artifacts; Artificial Intelligence; Image Interpretation, Computer-Assisted; Motion; Pattern Recognition, Automated; Refractometry",Article,Scopus,2-s2.0-84870188466
"Hsin K.-Y., Ghosh S., Kitano H.","Combining machine learning systems and multiple docking simulation packages to improve docking prediction reliability for network pharmacology",2013,"PLoS ONE",14,10.1371/journal.pone.0083922,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896705093&doi=10.1371%2fjournal.pone.0083922&partnerID=40&md5=b66638db91c066ad9b28efd075d410a3","Increased availability of bioinformatics resources is creating opportunities for the application of network pharmacology to predict drug effects and toxicity resulting from multi-target interactions. Here we present a high-precision computational prediction approach that combines two elaborately built machine learning systems and multiple molecular docking tools to assess binding potentials of a test compound against proteins involved in a complex molecular network. One of the two machine learning systems is a re-scoring function to evaluate binding modes generated by docking tools. The second is a binding mode selection function to identify the most predictive binding mode. Results from a series of benchmark validations and a case study show that this approach surpasses the prediction reliability of other techniques and that it also identifies either primary or off-targets of kinase inhibitors. Integrating this approach with molecular network maps makes it possible to address drug safety issues by comprehensively investigating network-dependent effects of a drug or drug candidate. © 2013 Hsin et al.",,"phosphotransferase inhibitor; protein; accuracy; article; bioinformatics; case study; controlled study; drug protein binding; drug research; drug safety; drug targeting; machine learning; mathematical computing; molecular docking; pharmacology; prediction; reliability; scoring system; simulation; validation study; Artificial Intelligence; Computational Biology; Drug Discovery; Gene Regulatory Networks; Humans; Molecular Docking Simulation; Protein Binding; Protein Interaction Maps; Protein Kinase Inhibitors; Protein Kinases",Article,Scopus,2-s2.0-84896705093
"Crossley S.A., Roscoe R., McNamara D.S.","Using automatic scoring models to detect changes in student writing in an intelligent tutoring system",2013,"FLAIRS 2013 - Proceedings of the 26th International Florida Artificial Intelligence Research Society Conference",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889850660&partnerID=40&md5=f2bee37cafbe0bcae12a6c3d3972dcb3","This study compares automated scoring increases and linguistic changes for student writers in two groups: a group that used an intelligent tutoring system embedded with an automated writing evaluation component (Writing Pal) and a group that used only the automated writing evaluation component. The primary goal is to examine automated scoring differences in both groups from pretest to posttest essays to investigate score gains and linguistic development. The study finds that both groups show significant increases in automated writing scores and significant development in lexical, syntactic, cohesion, and rhetorical features. However, the Writing-Pal group shows greater raw frequency gains (i.e., negative v. positive gains). Copyright © 2013, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Automatic scoring; Frequency gains; Intelligent tutoring system; Student writing; Artificial intelligence; Computer aided instruction; Education computing; Linguistics; Automation",Conference Paper,Scopus,2-s2.0-84889850660
"Chen S., Amid D., Shir O.M., Limonad L., Boaz D., Anaby-Tavor A., Schreck T.","Self-organizing maps for multi-objective Pareto Frontiers",2013,"IEEE Pacific Visualization Symposium",14,10.1109/PacificVis.2013.6596140,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889069849&doi=10.1109%2fPacificVis.2013.6596140&partnerID=40&md5=491582939b5eb936498f5820466c85d4","Decision makers often need to take into account multiple conflicting objectives when selecting a solution for their problem. This can result in a potentially large number of candidate solutions to be considered. Visualizing a Pareto Frontier, the optimal set of solutions to a multi-objective problem, is considered a difficult task when the problem at hand spans more than three objective functions. We introduce a novel visual-interactive approach to facilitate coping with multi-objective problems. We propose a characterization of the Pareto Frontier data and the tasks decision makers face as they reach their decisions. Following a comprehensive analysis of the design alternatives, we show how a semantically-enhanced Self-Organizing Map, can be utilized to meet the identified tasks. We argue that our newly proposed design provides both consistent orientation of the 2D mapping as well as an appropriate visual representation of individual solutions. We then demonstrate its applicability with two real-world multi-objective case studies. We conclude with a preliminary empirical evaluation and a qualitative usefulness assessment. © 2013 IEEE.","[Computing Methodologies]: Machine Learning - Machine Learning Approaches Neural Networks; [Human-Centered Computing]: Visualization - Visualization Design and Evaluation Methods; [Information Systems]: Information Systems Applications - Decision Support Systems","Comprehensive analysis; Conflicting objectives; Consistent orientations; Information systems application; Machine learning approaches; Multi-objective problem; Visual representations; Visualization designs; Artificial intelligence; Conformal mapping; Decision making; Decision support systems; Information systems; Learning systems; Multiobjective optimization; Visualization; Ground vehicle parts and equipment",Conference Paper,Scopus,2-s2.0-84889069849
"Ganzfried S., Sandholm T.","Action translation in extensive-form games with large action spaces: Axioms, paradoxes, and the pseudo-harmonic mapping",2013,"IJCAI International Joint Conference on Artificial Intelligence",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062132&partnerID=40&md5=115122e70c8f176d556fad0c5acda812","When solving extensive-form games with large action spaces, typically significant abstraction is needed to make the problem manageable from a modeling or computational perspective. When this occurs, a procedure is needed to interpret actions of the opponent that fall outside of our abstraction (by mapping them to actions in our abstraction). This is called an action translation mapping. Prior action translation mappings have been based on heuristics without theoretical justification. We show that the prior mappings are highly exploitable and that most of them violate certain natural desiderata. We present a new mapping that satisfies these desiderata and has significantly lower exploitability than the prior mappings. Furthermore, we observe that the cost of this worst-case performance benefit (low exploitability) is not high in practice; our mapping performs competitively with the prior mappings against no-limit Texas Hold'em agents submitted to the 2012 Annual Computer Poker Competition. We also observe several paradoxes that can arise when performing action abstraction and translation; for example, we show that it is possible to improve performance by including suboptimal actions in our abstraction and excluding optimal actions.",,"Action spaces; Extensive-form games; Improve performance; Optimal actions; Texas Hold'em; Worst-case performance; Abstracting; Artificial intelligence; Mapping",Conference Paper,Scopus,2-s2.0-84896062132
"Roy P., Pachet F.","Enforcing meter in finite-length Markov sequences",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893367524&partnerID=40&md5=c41846724f5ee51fabe7ea59d7e0c80f","Markov processes are increasingly used to generate finite-length sequences that imitate a given style. However, Markov processes are notoriously difficult to control. Recently, Markov constraints have been introduced to give users some control on generated sequences. Markov constraints reformulate finite-length Markov sequence generation in the framework of constraint satisfaction (CSP). However, in practice, this approach is limited to local constraints and its performance is low for global constraints, such as cardinality or arithmetic constraints. This limitation prevents generated sequences to satisfy structural properties which are independent of the style, but inherent to the domain, such as meter. In this article, we introduce meter, a constraint that ensures a sequence is 1) Markovian with regards to a given corpus and 2) follows metrical rules expressed as cumulative cost functions. Additionally, meter can simultaneously enforce cardinality constraints. We propose a domain consistency algorithm whose complexity is pseudo-polynomial. This result is obtained thanks to a theorem on the growth of sumsets by Khovanskii. We illustrate our constraint on meter-constrained music generation problems that were so far not solvable by any other technique. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Arithmetic constraints; Cardinality constraints; Constraint Satisfaction; Cumulative cost; Domain consistency; Finite-length sequence; Global constraints; Local constraints; Artificial intelligence; Constraint theory; Markov processes; Constraint satisfaction problems",Conference Paper,Scopus,2-s2.0-84893367524
"Guo Y., Xue W.","Probabilistic multi-label classification with sparse feature learning",2013,"IJCAI International Joint Conference on Artificial Intelligence",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063365&partnerID=40&md5=8eea385e1471f129ebe7b41806971302","Multi-label classification is a critical problem in many areas of data analysis such as image labeling and text categorization. In this paper we propose a probabilistic multi-label classification model based on novel sparse feature learning. By employing an individual sparsity inducing ℓ1-norm and a group sparsity inducing ℓ2;1-norm, the proposed model has the capacity of capturing both label interdependencies and common predictive model structures. We formulate this sparse norm regularized learning problem as a non-smooth convex optimization problem, and develop a fast proximal gradient algorithm to solve it for an optimal solution. Our empirical study demonstrates the efficacy of the proposed method on a set of multi-label tasks given a limited number of labeled training instances.",,"Critical problems; Empirical studies; Gradient algorithm; Multi-label classifications; Non-smooth convex optimizations; Optimal solutions; Predictive modeling; Text categorization; Convex optimization; Model structures; Text processing; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84896063365
"Mossakowski T., Lange C., Kutz O.","Three semantics for the core of the distributed ontology language",2013,"IJCAI International Joint Conference on Artificial Intelligence",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063600&partnerID=40&md5=c07a8da50b8fa2a31602337b0e8ff433","The Distributed Ontology Language DOL, currently being stand ardized as ISO WD 17347 within the OntoIOp (Ontology Integration and Interoperability) activity of ISO/TC 37, provides a unified framework for (1) ontologies formalized in heterogeneous logics, (2) modular ontologies, (3) links between ontologies, and (4) ontology annotation. A DOL ontology consists of modules formalized in languages such as OWL or Common Logic, serialized in the existing syntaxes of these languages. On top, DOL's meta level allows for expressing heterogeneous ontologies and links between ontologies, including (heterogeneous) imports and alignments, conservative extensions, and theory interpretations. We present the abstract syntax of these meta-level constructs, with three alternative semantics: direct, translational, and collapsed semantics.",,"Alternative Semantics; Conservative extensions; Heterogeneous ontology; Modular ontologies; Ontology annotations; Ontology integration; Ontology language; Theory interpretation; Artificial intelligence; Semantics; Syntactics; Ontology",Conference Paper,Scopus,2-s2.0-84896063600
"Wang Z., Li J., Tang J.","Boosting cross-lingual knowledge linking via concept annotation",2013,"IJCAI International Joint Conference on Artificial Intelligence",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896061804&partnerID=40&md5=2982439da51708a676b3cd39f8914d24","Automatically discovering cross-lingual links (CLs) between wikis can largely enrich the cross-lingual knowledge and facilitate knowledge sharing across different languages. In most existing approaches for cross-lingual knowledge linking, the seed CLs and the inner link structures are two important factors for finding new CLs. When there are insufficient seed CLs and inner links, discovering new CLs becomes a challenging problem. In this paper, we propose an approach that boosts cross-lingual knowledge linking by concept annotation. Given a small number of seed CLs and inner links, our approach first enriches the inner links in wikis by using concept annotation method, and then predicts new CLs with a regression-based learning model. These two steps mutually reinforce each other, and are executed iteratively to find as many CLs as possible. Experimental results on the English and Chinese Wikipedia data show that the concept annotation can effectively improve the quantity and quality of predicted CLs. With 50,000 seed CLs and 30% of the original inner links in Wikipedia, our approach discovered 171,393 more CLs in four runs when using concept annotation.",,"Annotation methods; Cross-lingual; Knowledge-sharing; Learning models; Link structure; Wikipedia; Artificial intelligence; Iterative methods; Websites",Conference Paper,Scopus,2-s2.0-84896061804
"Basu S., Christensen J.","Teaching classification boundaries to humans",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893402531&partnerID=40&md5=f760b1874e1b075addf2632c17e10a1f","Given a classification task, what is the best way to teach the resulting boundary to a human? While machine learning techniques can provide excellent methods for finding the boundary, including the selection of examples in an online setting, the y tell us little about how we would teach a human the same task. We propose to investigate the problem of example selection and presentation in the context of teaching humans, and explore a variety of mechanisms in the interests of finding what may work best. In particular, we begin with the baseline of random presentation and the n examine combinations of several mechanisms: the indication of an example's relative difficulty, the use of the shaping heuristic from the cognitive science literature (moving from easier examples to harder ones), and a novel kernel-based ""coverage model"" of the subject's mastery of the task. From our experiments on 54 human subjects learning and performing a pair of synthetic classification tasks via our teaching system, we found that we can achieve the greatest gains with a combination of shaping and the coverage model. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Classification boundary; Classification tasks; Cognitive science; Coverage models; Example selection; Machine learning techniques; On-line setting; Teaching systems; Artificial intelligence; Learning systems",Conference Paper,Scopus,2-s2.0-84893402531
"Zhang R., Hanzo L.","Multi-layer modulation for intensity-modulated direct-detection optical OFDM",2013,"Journal of Optical Communications and Networking",14,10.1364/JOCN.5.001402,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891526870&doi=10.1364%2fJOCN.5.001402&partnerID=40&md5=1a2462af2c6b3e060fe1a0d3d6002889","Multi-layer modulation (MLM)-aided intensity-modulated direct-detection (IM/DD) DC-biased optical orthogonal frequency division multiplexing (DCO-OFDM) and asymmetrically clipped optical OFDM (ACO-OFDM) are considered. More explicitly, we propose a double turbo receiver (DTR) for jointly detecting the MLM and for compensating the clipping distortion. Additionally, a genetic-algorithm-aided weight optimization is pursued for seeking an increased MLM bits per symbol (BPS) throughput. Our numerical results demonstrate that for ACO-OFDM, at the throughput of ¿ = 3 BPS, a 3 dB gain was attained for Q = 15 DTR iterations without requiring weight optimization. For DCO-OFDM, an even more significant gain of 12 and 8 dB was observed at the throughput of ¿ = 3 BPS and ¿ = 4 BPS, respectively, without any clipping distortion compensation. © 2009-2012 OSA.","Genetic algorithm; Optical modulation; Optical OFDM; Peak-to-average-power-ratio reduction; Turbo receiver","Asymmetrically clipped Optical OFDM; Clipping distortion; Intensity-modulated; Numerical results; Optical OFDM; Optical orthogonal frequency division multiplexing; Turbo receivers; Weight optimization; Algorithms; Ant colony optimization; Artificial intelligence; Genetic algorithms; Light modulation; Modulation; Nonlinear distortion; Optical fiber communication; Throughput; Orthogonal frequency division multiplexing",Article,Scopus,2-s2.0-84891526870
"Liu Y., Jiao L.C., Shang F., Yin F., Liu F.","An efficient matrix bi-factorization alternative optimization method for low-rank matrix recovery and completion",2013,"Neural Networks",14,10.1016/j.neunet.2013.06.013,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880970412&doi=10.1016%2fj.neunet.2013.06.013&partnerID=40&md5=a7ae757d35ae2d789735be4c8d06dbb2","In recent years, matrix rank minimization problems have aroused considerable interests from machine learning, data mining and computer vision communities. All of these problems can be solved via their convex relaxations which minimize the trace norm instead of the rank of the matrix, and have to be solved iteratively and involve singular value decomposition (SVD) at each iteration. Therefore, those algorithms for trace norm minimization problems suffer from high computation cost of multiple SVDs. In this paper, we propose an efficient Matrix Bi-Factorization (MBF) method to approximate the original trace norm minimization problem and mitigate the computation cost of performing SVDs. The proposed MBF method can be used to address a wide range of low-rank matrix recovery and completion problems such as low-rank and sparse matrix decomposition (LRSD), low-rank representation (LRR) and low-rank matrix completion (MC). We also present three small scale matrix trace norm models for LRSD, LRR and MC problems, respectively. Moreover, we develop two concrete linearized proximal alternative optimization algorithms for solving the above three problems. Experimental results on a variety of synthetic and real-world data sets validate the efficiency, robustness and effectiveness of our MBF method comparing with the state-of-the-art trace norm minimization algorithms. © 2013 Elsevier Ltd.","Low rank representation; Low-rank and sparse matrix decomposition; Matrix completion; Rank minimization; Trace norm minimization","Low-rank representations; Matrix completion; Rank minimizations; Sparse matrices; Trace-norms; Algorithms; Factorization; Relaxation processes; Singular value decomposition; Virtual reality; Iterative methods; algorithm; article; automation; data mining; image analysis; intermethod comparison; low rank and sparse matrix decomposition; low rank completion; low rank representation; machine learning; mathematical analysis; mathematical computing; matrix bifactorization; priority journal; process optimization; singular value decomposition; Low rank representation; Low-rank and sparse matrix decomposition; Matrix completion; Rank minimization; Trace norm minimization; Algorithms; Artificial Intelligence; Cluster Analysis; Computer Simulation; Data Interpretation, Statistical; Data Mining; Linear Models; Models, Statistical",Article,Scopus,2-s2.0-84880970412
"Pacharawongsakda E., Theeramunkong T.","Predict subcellular locations of singleplex and multiplex proteins by semi-supervised learning and dimension-reducing general mode of Chou's PseAAC",2013,"IEEE Transactions on Nanobioscience",14,10.1109/TNB.2013.2272014,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892612782&doi=10.1109%2fTNB.2013.2272014&partnerID=40&md5=399e35047f9f7147f22919897c3f0b73","Predicting protein subcellular location is one of major challenges in Bioinformatics area since such knowledge helps us understand protein functions and enables us to select the targeted proteins during drug discovery process. While many computational techniques have been proposed to improve predictive performance for protein subcellular location, they have several shortcomings. In this work, we propose a method to solve three main issues in such techniques; i) manipulation of multiplex proteins which may exist or move between multiple cellular compartments, ii) handling of high dimensionality in input and output spaces and iii) requirement of sufficient labeled data for model training. Towards these issues, this work presents a new computational method for predicting proteins which have either single or multiple locations. The proposed technique, namely iFLAST-CORE, incorporates the dimensionality reduction in the feature and label spaces with co-training paradigm for semi-supervised multi-label classification. For this purpose, the Singular Value Decomposition (SVD) is applied to transform the high-dimensional feature space and label space into the lower-dimensional spaces. After that, due to limitation of labeled data, the co-training regression makes use of unlabeled data by predicting the target values in the lower-dimensional spaces of unlabeled data. In the last step, the component of SVD is used to project labels in the lower-dimensional space back to those in the original space and an adaptive threshold is used to map a numeric value to a binary value for label determination. A set of experiments on viral proteins and gram-negative bacterial proteins evidence that our proposed method improve the classification performance in terms of various evaluation metrics such as Aiming (or Precision), Coverage (or Recall) and macro F-measure, compared to the traditional method that uses only labeled data. © 2002-2011 IEEE.","Co-training; Dimensionality reduction; Gene ontology; Multi-label classification; Semi-supervised learning; Subcellular location","Co-training; Dimensionality reduction; Gene ontology; Multi-label classifications; Semi-supervised learning; Subcellular location; Bioinformatics; Forecasting; Molecular biology; Multiplexing; Singular value decomposition; Supervised learning; Proteins; protein; amino acid sequence; article; artificial intelligence; biology; chemistry; gene ontology; intracellular space; metabolism; methodology; protein database; statistical model; Amino Acid Sequence; Artificial Intelligence; Computational Biology; Databases, Protein; Gene Ontology; Intracellular Space; Models, Statistical; Proteins",Article,Scopus,2-s2.0-84892612782
"Dias R., Fonseca M.J.","Improving music recommendation in session-based collaborative filtering by using temporal context",2013,"Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI",14,10.1109/ICTAI.2013.120,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897699357&doi=10.1109%2fICTAI.2013.120&partnerID=40&md5=2e18be74d2e19e3bf804048e67ca00d3","Music recommendation systems based on Collaborative Filtering methods have been extensively developed over the last years. Typically, they work by analyzing the past user-song relationships, and provide informed guesses based on the overall information collected from other users. Although the music listening behavior is a repetitive and time-dependent process, these methods have not taken this into account and only consider user-song interaction for recommendation. In this work, we explore the usage of temporal context and session diversity in Session-based Collaborative Filtering techniques for music recommendation. We compared two techniques to capture the users' listening patterns over time: one explicitly extracts temporal properties and session diversity, to group and compare the similarity of sessions, the other uses a generative topic modeling algorithm, which is able to implicitly model temporal patterns. We evaluated the developed algorithms by measuring the Hit Ratio, and the Mean Reciprocal Rank. Results reveal that the inclusion of temporal information, either explicitly or implicitly, increases significantly the accuracy of the recommendation, while compared to the traditional session-based CF. © 2013 IEEE.","Collaborative Filtering; Listening Sessions; Music Recommendation; Temporal Context","Collaborative filtering methods; Collaborative filtering techniques; Listening Sessions; Music recommendation; Music Recommendation System; Temporal Context; Time-dependent process; Topic modeling algorithms; Artificial intelligence; Collaborative filtering; Tools; Algorithms",Conference Paper,Scopus,2-s2.0-84897699357
"Benlic U., Hao J.-K.","Breakout local search for the vertex separator problem",2013,"IJCAI International Joint Conference on Artificial Intelligence",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063827&partnerID=40&md5=f4ae0b9ccf4a0c68208e125748a9c573","In this paper, we propose the first heuristic approach for the vertex separator problem (VSP), based on Breakout Local Search (BLS). BLS is a recent meta-heuristic that follows the general framework of the popular Iterated Local Search (ILS) with a particular focus on the perturbation strategy. Based on some relevant information on search history, it tries to introduce the most suitable degree of diversification by determining adaptively the number and type of moves for the next perturbation phase. The proposed heuristic is highly competitive with the exact state-of-art approaches from the literature on the current VSP benchmark. Moreover, we present for the first time computational results for a set of large graphs with up to 3000 vertices, which constitutes a new challenging benchmark for VSP approaches.","Adaptive diversification mechanism; Breakout local search; Meta-heuristic; Vertex separator problem","Computational results; Heuristic approach; Iterated local search; Large graphs; Local search; Metaheuristic; Search history; Vertex separators; Artificial intelligence; Heuristic algorithms; Heuristic methods; Separators",Conference Paper,Scopus,2-s2.0-84896063827
"Yu G., Rangwala H., Domeniconi C., Zhang G., Zhang Z.","Protein Function Prediction by Integrating Multiple Kernels",2013,"IJCAI International Joint Conference on Artificial Intelligence",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063179&partnerID=40&md5=c9d51b4d537daa485b0ab529662112f9","Determining protein function constitutes an exercise in integrating information derived from several heterogeneous high-throughput experiments. To utilize the information spread across multiple sources in a combined fashion, these data sources are transformed into kernels. Several protein function prediction methods follow a two-phased approach: they first optimize the weights on individual kernels to produce a composite kernel, and then train a classifier on the composite kernel. As such, these methods result in an optimal composite kernel, but not necessarily in an optimal classifier. On the other hand, some methods optimize the loss of binary classifiers, and learn weights for the different kernels iteratively. A protein has multiple functions, and each function can be viewed as a label. These methods solve the problem of optimizing weights on the input kernels for each of the labels. This is computationally expensive and ignores inter-label correlations. In this paper, we propose a method called Protein Function Prediction by Integrating Multiple Kernels (ProMK). ProMK iteratively optimizes the phases of learning optimal weights and reducing the empirical loss of a multi-label classifier for each of the labels simultaneously, using a combined objective function. ProMK can assign larger weights to smooth kernels and downgrade the weights on noisy kernels. We evaluate the ability of ProMK to predict the function of proteins using several standard benchmarks. We show that our approach performs better than previously proposed protein function prediction approaches that integrate data from multiple networks, and multi-label multiple kernel learning methods.",,"Binary classifiers; Composite kernels; High throughput experiments; Integrating information; Multiple Kernel Learning; Objective functions; Optimal classifiers; Protein function prediction; Artificial intelligence; Optimization; Proteins; Throughput; Iterative methods",Conference Paper,Scopus,2-s2.0-84896063179
"Chen N., Zhu J., Xia F., Zhang B.","Generalized relational topic models with data augmentation",2013,"IJCAI International Joint Conference on Artificial Intelligence",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062198&partnerID=40&md5=a1b52b41db1e7465e89acbcf22a34f0c","Relational topic models have shown promise on analyzing document network structures and discovering latent topic representations. This paper presents three extensions: 1) unlike the common link likelihood with a diagonal weight matrix that allows the-same-topic interactions only, we generalize it to use a full weight matrix that captures all pairwise topic interactions and is applicable to asymmetric networks; 2) instead of doing standard Bayesian inference, we perform regularized Bayesian inference with a regularization parameter to deal with the imbalanced link structure issue in common real networks; and 3) instead of doing variational approximation with strict mean-field assumptions, we present a collapsed Gibbs sampling algorithm for the generalized relational topic models without making restricting assumptions. Experimental results demonstrate the significance of these extensions on improving the prediction performance, and the time efficiency can be dramatically improved with a simple fast approximation method.",,"Asymmetric networks; Bayesian inference; Data augmentation; Fast approximation; Network structures; Prediction performance; Regularization parameters; Variational approximation; Artificial intelligence; Bayesian networks; Inference engines; Approximation algorithms",Conference Paper,Scopus,2-s2.0-84896062198
"Williams T., Cantrell R., Briggs G., Schermerhorn P., Scheutz M.","Grounding natural language references to unvisited and hypothetical locations",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893405074&partnerID=40&md5=1c756c1a7d360553b0e75c5a29223456","While much research exists on resolving spatial natural language references to known locations, little work deals with handling references to unknown locations. In this paper we introduce and evaluate algorithms integrated into a cognitive architecture which allow an agent to learn about its environment while resolving references to both known and unknown locations. We also describe how multiple components in the architecture jointly facilitate these capabilities. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Cognitive architectures; Multiple components; Natural languages; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84893405074
"Lutz C., Seylan I., Wolter F.","Ontology-based data access with closed predicates is inherently intractable (sometimes)",2013,"IJCAI International Joint Conference on Artificial Intelligence",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062307&partnerID=40&md5=3e4c2bd7fadd48f07daafbf32099a028","When answering queries in the presence of ontologies, adopting the closed world assumption for some predicates easily results in intractability. We analyze this situation on the level of individual ontologies formulated in the description logics DL-Lite and EL and show that in all cases where answering conjunctive queries (CQs) with (open and) closed predicates is tractable, it coincides with answering CQs with all predicates assumed open. In this sense, CQ answering with closed predicates is inherently intractable. Our analysis also yields a dichotomy between AC0 and CONP for CQ answering w.r.t. ontologies formulated in DL-Lite and a dichotomy between PTIME and CONP for EL. Interestingly, the situation is less dramatic in the more expressive description logic ELI, where we find ontologies for which CQ answering is in PTIME, but does not coincide with CQ answering where all predicates are open.",,"Answering queries; Closed world assumption; Conjunctive queries (cqs); Description logic; Dl-lite; Expressive description logic; Ontology-based data access; Artificial intelligence; Formal languages; Data description",Conference Paper,Scopus,2-s2.0-84896062307
"Yamada M., Kimura A., Naya F., Sawada H.","Change-point detection with feature selection in high-dimensional time-series data",2013,"IJCAI International Joint Conference on Artificial Intelligence",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896061276&partnerID=40&md5=9041ac9ba521836d26c7584b45f314ba","Change-point detection is the problem of finding abrupt changes in time-series, and it is attracting a lot of attention in the artificial intelligence and data mining communities. In this paper, we present a supervised learning based change-point detection approach in which we use the separability of past and future data at time t (they are labeled as +1 and -1) as plausibility of change-points. Based on this framework, we propose a detection measure called the additive Hilbert-Schmidt Independence Criterion (aHSIC), which is defined as the weighted sum of the HSIC scores between features and its corresponding binary labels. Here, the HSIC is a kernelbased independence measure. A novel aspect of the aHSIC score is that it can incorporate feature selection during its detection measure estimation. More specifically, we first select features that are responsible for an abrupt change by using a supervised approach, and then compute the aHSIC score by employing the selected features. Thus, compared with traditional detection measures, our approach tends to be robust as regards noise features, and so the aHSIC is suitable for a use with high-dimensional time-series change-point detection problems. We demonstrate that the proposed change-point detection method is promising through extensive experiments on synthetic data sets and a real-world human activity data set.",,"Change point detection; Change-point detection problems; Data mining community; High-dimensional; Hilbert-schmidt independence criterions; Human activities; Independence measure; Synthetic datasets; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84896061276
"Jefferson C., Nightingale P.","Extending simple tabular reduction with short supports",2013,"IJCAI International Joint Conference on Artificial Intelligence",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062689&partnerID=40&md5=d25ac17c1c5434f8097ae725671db52f","Constraint propagation is one of the key techniques in constraint programming, and a large body of work has built up around it. Special-purpose constraint propagation algorithms frequently make implicit use of short supports - by examining a subset of the variables, they can infer support (a justification that a variable-value pair still forms part of a solution to the constraint) for all other variables and values and save substantial work. Recently short supports have been used in general purpose propagators, and (when the constraint is amenable to short supports) speed ups of more than three orders of magnitude have been demonstrated. In this paper we present SHORTSTR2, a development of the Simple Tabular Reduction algorithm STR2+. We show that SHORTSTR2 is complementary to the existing algorithms SHORTGAC and HAGGISGAC that exploit short supports, while being much simpler. When a constraint is amenable to short supports, the short support set can be exponentially smaller than the full-length support set. Therefore SHORTSTR2 can efficiently propagate many constraints that STR2+ cannot even load into memory. We also show that SHORTSTR2 can be combined with a simple algorithm to identify short supports from full-length supports, to provide a superior drop-in replacement for STR2+.",,"Constraint programming; Constraint propagation; Constraint propagation algorithms; Reduction algorithms; Short support; SIMPLE algorithm; Three orders of magnitude; Artificial intelligence; Computer programming; Constraint theory; Algorithms",Conference Paper,Scopus,2-s2.0-84896062689
"Yan R., Jiang H., Lapata M., Lin S.-D., Lv X., Li X.","I, poet: Automatic Chinese poetry composition through a generative summarization framework under constrained optimization",2013,"IJCAI International Joint Conference on Artificial Intelligence",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062805&partnerID=40&md5=39a300ddecd17d96df77ced27fa5ebef","Part of the long lasting cultural heritage of China is the classical ancient Chinese poems which follow strict formats and complicated linguistic rules. Automatic Chinese poetry composition by programs is considered as a challenging problem in computational linguistics and requires high Artificial Intelligence assistance, and has not been well addressed. In this paper, we formulate the poetry composition task as an optimization problem based on a generative summarization framework under several constraints. Given the user specified writing intents, the system retrieves cand idate terms out of a large poem corpus, and then orders these terms to fit into poetry formats, satisfying tonal and rhythm requirements. The optimization process under constraints is conducted via iterative term substitutions till convergence, and outputs the subset with the highest utility as the generated poem. For experiments, we perform generation on large datasets of 61,960 classic poems from Tang and Song Dynasty of China. A comprehensive evaluation, using both human judgments and ROUGE scores, has demonstrated the effectiveness of our proposed approach.",,"Comprehensive evaluation; Cultural heritages; Human judgments; Large datasets; Linguistic rules; Long lasting; Optimization problems; Computational linguistics; Constrained optimization; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84896062805
"Spielman D.A., Wang H., Wright J.","Exact recovery of sparsely-used dictionaries",2013,"IJCAI International Joint Conference on Artificial Intelligence",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896060530&partnerID=40&md5=f858c5c1c0cf00e8a5c192f1a89f5e3b","We consider the problem of learning sparsely used dictionaries with an arbitrary square dictionary and a rand om, sparse coefficient matrix. We prove that O(n log n) samples are sufficient to uniquely determine the coefficient matrix. Based on this proof, we design a polynomial-time algorithm, called Exact Recovery of Sparsely-Used Dictionaries (ERSpUD), and prove that it probably recovers the dictionary and coefficient matrix when the coefficient matrix is sufficiently sparse. Simulation results show that ER-SpUD reveals the true dictionary as well as the coefficients with probability higher than many state-of-the-art algorithms.",,"Coefficient matrix; Exact recoveries; Polynomial-time algorithms; State-of-the-art algorithms; Artificial intelligence; Algorithms",Conference Paper,Scopus,2-s2.0-84896060530
"Wang Y., Tong Y., Zeng M.","Ranking scientific articles by exploiting citations, authors, journals, and time information",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893397780&partnerID=40&md5=c5885dbdfeee96241a3ba5d71160c91c","Ranking scientific articles is an important but challenging task, partly due to the dynamic nature of the evolving publication network. In this paper, we mainly focus on two problems: (1) how to rank articles in the heterogeneous network; and (2) how to use time information in the dynamic network in order to obtain a better ranking result. To tackle the problems, we propose a graph-based ranking method, which utilizes citations, authors, journals/conferences and the publication time information collaboratively. The experiments were carried out on two public datasets. The result shows that our approach is practical and ranks scientific articles more accurately than the state-of-art methods. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Dynamic nature; Dynamic network; Graph-based; Publication networks; Ranking methods; Scientific articles; State-of-art methods; Time information; Heterogeneous networks; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84893397780
"Xu Y., Lin S., Wong D.W., Liu J., Xu D.","Efficient reconstruction-based optic cup localization for glaucoma screening.",2013,"Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894631707&partnerID=40&md5=a12d2e2e11b093de8181f5502f930a45","We present a reconstruction-based learning technique to localize the optic cup in fundus images for glaucoma screening. In contrast to previous approaches which rely on low-level visual cues, our method instead considers the input image as a whole and infers its optic cup parameters from a codebook of manually labeled reference images based on their similarity to the input and their contribution towards reconstructing the input image. We show that this approach can be formulated as a closed-form solution without any search, which leads to highly efficient and 100% repeatable computation. Our tests on the ORIGA and SCES datasets show that the performance of this method compares favorably to those of previous techniques while operating at faster speeds. This suggests much promise for this approach to be used in practice for screening.",,"algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; glaucoma; human; image enhancement; mass screening; methodology; ophthalmoscopy; optic disk; pathology; reproducibility; sensitivity and specificity; Algorithms; Artificial Intelligence; Glaucoma; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Mass Screening; Ophthalmoscopy; Optic Disk; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84894631707
"Lukasiewicz T., Martinez M.V., Simari G.I.","Preference-based query answering in Datalog+/- ontologies",2013,"IJCAI International Joint Conference on Artificial Intelligence",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062188&partnerID=40&md5=8db52b2b4add0c7b0fae289a7c7267c1","The study of preferences has a long tradition in many disciplines, but it has only relatively recently entered the realm of data management through their application in answering queries to relational databases. The current revolution in data availability through the Web and, perhaps most importantly in the last few years, social media sites and applications, puts ontology languages at the forefront of data and information management technologies. In this paper, we propose the first (to our knowledge) integration of ontology languages with preferences as in relational databases by developing PrefDatalog+/-, an extension of the Datalog+/- family of languages with preference management formalisms closely related to those previously studied for relational databases. We focus on two kinds of answers to queries that are relevant to this setting, skyline and κ-rank (a generalization of top-κ queries), and develop algorithms for computing these answers to both DAQs (disjunctions of atomic queries) and CQs (conjunctive queries). We show that DAQ answering in PrefDatalog+/- can be done in polynomial time in the data complexity, as in relational databases, as long as query answering can also be done in polynomial time (in the data complexity) in the underlying classical ontology.",,"Answering queries; Conjunctive queries; Data and information; Data availability; Ontology language; Preference managements; Query answering; Relational Database; Artificial intelligence; Database systems; Information management; Polynomial approximation; Query processing; Ontology",Conference Paper,Scopus,2-s2.0-84896062188
"Cimatti A., Micheli A., Roveri M.","Timelines with temporal uncertainty",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893398800&partnerID=40&md5=8c667691c17a25807a5de665b967da5a","Timelines are a formalism to model planning domains where the temporal aspects are predominant, and have been used in many real-world applications. Despite their practical success, a major limitation is the inability to model temporal uncertainty, i.e. the fact that the plan executor cannot decide the actual duration of some activities. In this paper we make two key contributions. First, we propose a comprehensive, semantically well founded framework that (conservatively) extends with temporal uncertainty the state of the art timeline approach. Second, we focus on the problem of producing time-triggered plans that are robust with respect to temporal uncertainty, under a bounded horizon. In this setting, we present the first complete algorithm, and we show how it can be made practical by leveraging the power of Satisfiability Modulo Theories. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Planning domains; Real-world; Satisfiability modulo Theories; State of the art; Temporal aspects; Temporal uncertainty; Time triggered; Well-founded framework; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84893398800
"Didaci L., Fumera G., Roli F.","Diversity in classifier ensembles: Fertile concept or dead end?",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,10.1007/978-3-642-38067-9_4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892916635&doi=10.1007%2f978-3-642-38067-9_4&partnerID=40&md5=6cf5bb4c872db07be8b514961b63b87f","Diversity is deemed a crucial concept in the field of multiple classifier systems, although no exact definition has been found so far. Existing diversity measures exhibit some issues, both from the theoretical viewpoint, and from the practical viewpoint of ensemble construction. We propose to address some of these issues through the derivation of decompositions of classification error, analogue to the well-known bias-variance-covariance and ambiguity decompositions of regression error. We then discuss whether the resulting decompositions can provide a more clear definition of diversity, and whether they can be exploited more effectively for the practical purpose of ensemble construction. © Springer-Verlag 2013.","Ambiguity decomposition.; Bias-variance-covariance; Decomposition; Diversity","Ambiguity decomposition; Bias-variance-covariance; Classification errors; Classifier ensembles; Diversity; Ensemble construction; Multiple classifier systems; Regression errors; Artificial intelligence; Computer science; Computers; Decomposition",Conference Paper,Scopus,2-s2.0-84892916635
"Nadiri A.A., Fijani E., Tsai F.T.-C., Moghaddam A.A.","Supervised committee machine with artificial intelligence for prediction of fluoride concentration",2013,"Journal of Hydroinformatics",14,10.2166/hydro.2013.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884375203&doi=10.2166%2fhydro.2013.008&partnerID=40&md5=fd787b7e04db0c40b987f043d4ffc9e0","The study introduces a supervised committee machine with artificial intelligence (SCMAI) method to predict fluoride in ground water of Maku, Iran. Ground water is the main source of drinking water for the area. Management of fluoride anomaly needs better prediction of fluoride concentration. However, the complex hydrogeological characteristics cause difficulties to accurately predict fluoride concentration in basaltic formation, non-basaltic formation, and mixing zone. SCMAI predicts fluoride by a nonlinear combination of individual AI models through an artificial intelligent system. Factor analysis is used to identify effective fluoride-correlated hydrochemical parameters as input to AI models. Four AI models, Sugeno fuzzy logic, Mamdani fuzzy logic, artificial neural network (ANN), and neuro-fuzzy are employed to predict fluoride concentration. The results show that all of these models have similar fitting to the fluoride data in the Maku area, and do not predict well for samples in the mixing zone. The SCMAI employs an ANN model to re-predict the fluoride concentration based on the four AI model predictions. The result shows improvement to the CMAI method, a committee machine with the linear combination of AI model predictions. The results also show significant fitting improvement to individual AI models, especially for fluoride prediction in the mixing zone. © IWA Publishing 2013.","Artificial intelligence; Artificial neural network; Committee machine; Fuzzy logic; Ground water quality; Neuro-fuzzy",,Conference Paper,Scopus,2-s2.0-84884375203
"Hyttinen A., Hoyer P.O., Eberhardt F., Järvisalo M.","Discovering cyclic causal models with latent variables: A general SAT-based procedure",2013,"Uncertainty in Artificial Intelligence - Proceedings of the 29th Conference, UAI 2013",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888160038&partnerID=40&md5=f86ee07272f0bb3a78aa4e4fc39f34cd","We present a very general approach to learning the structure of causal models based on d-separation constraints, obtained from any given set of overlapping passive observational or experimental data sets. The procedure allows for both directed cycles (feedback loops) and the presence of latent variables. Our approach is based on a logical representation of causal pathways, which permits the integration of quite general background knowledge, and inference is performed using a Boolean satisfiability (SAT) solver. The procedure is complete in that it exhausts the available information on whether any given edge can be determined to be present or absent, and returns ""unknown"" otherwise. Many existing constraint-based causal discovery algorithms can be seen as special cases, tailored to circumstances in which one or more restricting assumptions apply. Simulations illustrate the effect of these assumptions on discovery and how the present algorithm scales.",,"Back-ground knowledge; Boolean satisfiability; Causal model; Constraint-based; Discovery algorithm; Feed-back loop; Latent variable; Logical representations; Artificial intelligence; Algorithms",Conference Paper,Scopus,2-s2.0-84888160038
"Maier M., Marazopoulou K., Arbour D., Jensen D.","A sound and complete algorithm for learning causal models from relational data",2013,"Uncertainty in Artificial Intelligence - Proceedings of the 29th Conference, UAI 2013",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888165194&partnerID=40&md5=253e2de4c763b4914f5e146487f92823","The PC algorithm learns maximally oriented causal Bayesian networks. However, there is no equivalent complete algorithm for learning the structure of relational models, a more expressive generalization of Bayesian networks. Recent developments in the theory and representation of relational models support lifted reasoning about conditional independence. This enables a powerful constraint for orienting bivariate dependencies and forms the basis of a new algorithm for learning structure. We present the relational causal discovery (RCD) algorithm that learns causal relational models. We prove that RCD is sound and complete, and we present empirical results that demonstrate effectiveness.",,"Bivariate; Causal Bayesian network; Causal model; Conditional independences; Learning structure; Relational data; Relational Model; Sound and complete; Artificial intelligence; Bayesian networks; Learning algorithms",Conference Paper,Scopus,2-s2.0-84888165194
"Kamran M., Suhail S., Farooq M.","A robust, distortion minimizing technique for watermarking relational databases using once-for-all usability constraints",2013,"IEEE Transactions on Knowledge and Data Engineering",14,10.1109/TKDE.2012.227,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887963611&doi=10.1109%2fTKDE.2012.227&partnerID=40&md5=6053249e2739c0f9862ac644ca65a9fb","Ownership protection on relational databases - shared with collaborators (or intended recipients) - demands developing a watermarking scheme that must be able to meet four challenges: 1) it should be robust against different types of attacks that an intruder could launch to corrupt the embedded watermark; 2) it should be able to preserve the knowledge in the databases to make them an effective component of knowledge-aware decision support systems; 3) it should try to strike a balance between the conflicting requirements of database owners, who require soft usability constraints, and database recipients who want tight usability constraints that ensure minimum distortions in the data; and 4) last but not least, it should not require that a database owner defines usability constraints for each type of application and every recipient separately. The major contribution of this paper is a robust and efficient watermarking scheme for relational databases that is able to meet all above-mentioned four challenges. The results of our experiments prove that the proposed scheme achieves 100 percent decoding accuracy even if only one watermarked row is left in the database. © 1989-2012 IEEE.","Data quality; Data usability constraints; Database watermarking; Distortion free; Ownership protection; Right protection; Robust watermarking","Data quality; Data usability; Database watermarking; Distortion-free; Ownership protection; Right protection; Robust watermarking; Artificial intelligence; Decision support systems; Usability engineering; Watermarking; Database systems",Article,Scopus,2-s2.0-84887963611
"Cheng S., Shi Y., Qin Q., Gao S.","Solution clustering analysis in brain storm optimization algorithm",2013,"Proceedings of the 2013 IEEE Symposium on Swarm Intelligence, SIS 2013 - 2013 IEEE Symposium Series on Computational Intelligence, SSCI 2013",14,10.1109/SIS.2013.6615167,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886787470&doi=10.1109%2fSIS.2013.6615167&partnerID=40&md5=53b23f3eba48c8012e9b7fffb91fe728","In swarm intelligence algorithms, premature convergence happens partially due to the solutions getting clustered together, and not diverging again. However, solution clustering is not always harmful for optimization. The solution clustering strategy is utilized in brain storm optimization (BSO) to guide individuals to move toward the better and better areas. The information of clusters indicates the solutions' distribution in the search space, which could be utilized to reveal the landscapes and other proprieties of problems being optimized. In this paper, the solution clustering, and other properties of the brain storm optimization algorithm are analyzed and discussed. Experimental results show that brain storm optimization is a very promising algorithm for solving different kinds of problems. © 2013 IEEE.","brain storm optimization; convergence; exploration/exploitation; population diversity; solution clustering; Swarm intelligence","Algorithm for solving; convergence; Exploration/exploitation; Optimization algorithms; Population diversity; Pre-mature convergences; Swarm Intelligence; Swarm intelligence algorithms; Algorithms; Optimization; Storms; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84886787470
"Walker R., Slingsby A., Dykes J., Xu K., Wood J., Nguyen P.H., Stephens D., Wong B.L.W., Zheng Y.","An extensible framework for provenance in human terrain visual analytics",2013,"IEEE Transactions on Visualization and Computer Graphics",14,10.1109/TVCG.2013.132,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886694540&doi=10.1109%2fTVCG.2013.132&partnerID=40&md5=7f564762d3fd3e2f9c02b0b13b90df2b","We describe and demonstrate an extensible framework that supports data exploration and provenance in the context of Human Terrain Analysis (HTA). Working closely with defence analysts we extract requirements and a list of features that characterise data analysed at the end of the HTA chain. From these, we select an appropriate non-classified data source with analogous features, and model it as a set of facets. We develop ProveML, an XML-based extension of the Open Provenance Model, using these facets and augment it with the structures necessary to record the provenance of data, analytical process and interpretations. Through an iterative process, we develop and refine a prototype system for Human Terrain Visual Analytics (HTVA), and demonstrate means of storing, browsing and recalling analytical provenance and process through analytic bookmarks in ProveML. We show how these bookmarks can be combined to form narratives that link back to the live data. Throughout the process, we demonstrate that through structured workshops, rapid prototyping and structured communication with intelligence analysts we are able to establish requirements, and design schema, techniques and tools that meet the requirements of the intelligence community. We use the needs and reactions of defence analysts in defining and steering the methods to validate the framework. © 2013 IEEE.","bookmarks; framework; Human terrain analysis; narratives; provenance","bookmarks; framework; narratives; provenance; Terrain analysis; Iterative methods; Visualization; Landforms; algorithm; article; artificial intelligence; automated pattern recognition; computer graphics; computer interface; human; methodology; reproducibility; sensitivity and specificity; automated pattern recognition; procedures; Algorithms; Artificial Intelligence; Computer Graphics; Humans; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; User-Computer Interface; Algorithms; Artificial Intelligence; Computer Graphics; Humans; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; User-Computer Interface",Article,Scopus,2-s2.0-84886694540
"Sekanina L., Vasicek Z.","Approximate circuit design by means of evolvable hardware",2013,"Proceedings of the 2013 IEEE International Conference on Evolvable Systems, ICES 2013 - 2013 IEEE Symposium Series on Computational Intelligence, SSCI 2013",14,10.1109/ICES.2013.6613278,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886663174&doi=10.1109%2fICES.2013.6613278&partnerID=40&md5=4ddf45ec40f60eb321f31f1688de2c3e","This paper deals with evolutionary design of approximate circuits. This class of circuits is characterized by relaxing the requirement on functional equivalence between the specification and implementation in order to reduce the area on a chip or minimize energy consumption. We proposed a CGP-based automated design method which enables to find a good trade off between key circuit parameters (functionality, area and power consumption). In particular, the digital approximate circuits consisting of elementary gates are addressed in this paper. Experimental results are provided for combinational single-output circuits and adders where two different metrics are compared for the error assessment. © 2013 IEEE.",,"Automated design methods; Circuit designs; Circuit parameter; Elementary gates; Error assessment; Evolutionary design; Evolvable hardware; Functional equivalence; Artificial intelligence; Digital circuits; Energy utilization; Equivalence classes; Integrated circuit manufacture",Conference Paper,Scopus,2-s2.0-84886663174
"Leydesdorff L., Radicchi F., Bornmann L., Castellano C., De Nooy W.","Field-normalized impact factors (IFs): A comparison of rescaling and fractionally counted IFs",2013,"Journal of the American Society for Information Science and Technology",14,10.1002/asi.22911,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885675834&doi=10.1002%2fasi.22911&partnerID=40&md5=c6c27f28f1a7a854f082211d2f1c7edd","Two methods for comparing impact factors and citation rates across fields of science are tested against each other using citations to the 3,705 journals in the Science Citation Index 2010 (CD-Rom version of SCI) and the 13 field categories used for the Science and Engineering Indicators of the U.S. National Science Board. We compare (a) normalization by counting citations in proportion to the length of the reference list (1/N of references) with (b) rescaling by dividing citation scores by the arithmetic mean of the citation rate of the cluster. Rescaling is analytical and therefore independent of the quality of the attribution to the sets, whereas fractional counting provides an empirical strategy for normalization among sets (by evaluating the between-group variance). By the fairness test of Radicchi and Castellano (), rescaling outperforms fractional counting of citations for reasons that we consider. © 2013 ASIS&T.","evaluation; journals","Arithmetic mean; Citation score; evaluation; journals; National Science Boards; Reference list; Science and engineering; Science citation index; Artificial intelligence; Software engineering",Article,Scopus,2-s2.0-84885675834
"Rodriguez-Lujan I., Bailador G., Sanchez-Avila C., Herrero A., Vidal-De-Miguel G.","Analysis of pattern recognition and dimensionality reduction techniques for odor biometrics",2013,"Knowledge-Based Systems",14,10.1016/j.knosys.2013.08.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883853793&doi=10.1016%2fj.knosys.2013.08.002&partnerID=40&md5=b23febe56643f9cfd652af5704a4f551","In this paper, we analyze the performance of several well-known pattern recognition and dimensionality reduction techniques when applied to mass-spectrometry data for odor biometric identification. Motivated by the successful results of previous works capturing the odor from other parts of the body, this work attempts to evaluate the feasibility of identifying people by the odor emanated from the hands. By formulating this task according to a machine learning scheme, the problem is identified with a small-sample-size supervised classification problem in which the input data is formed by mass spectrograms from the hand odor of 13 subjects captured in different sessions. The high dimensionality of the data makes it necessary to apply feature selection and extraction techniques together with a simple classifier in order to improve the generalization capabilities of the model. Our experimental results achieve recognition rates over 85% which reveals that there exists discriminatory information in the hand odor and points at body odor as a promising biometric identifier. © 2013 Elsevier B.V. All rights reserved.","Biometrics; Feature selection; Mass-spectrometry; Odor; Supervised classification","Biometric identifications; Biometric identifiers; Dimensionality reduction techniques; Feature selection and extractions; Generalization capability; High dimensionality; Spectrograms; Supervised classification; Artificial intelligence; Feature extraction; Odors; Software engineering; Biometrics",Article,Scopus,2-s2.0-84883853793
"Chalco-Cano Y., Román-Flores H.","Some remarks on fuzzy differential equations via differential inclusions",2013,"Fuzzy Sets and Systems",14,10.1016/j.fss.2013.04.017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884648457&doi=10.1016%2fj.fss.2013.04.017&partnerID=40&md5=4d732b97a986453d12ee25103be1c689","In this paper we discuss the formulation and procedure for solving fuzzy differential equations via differential inclusions. We give several examples showing the correct and incorrect procedure for solving fuzzy differential equations. We show the connection between fuzzy differential equations and fuzzy differential inclusions. Finally, we give some remarks on numerical algorithms for solving fuzzy differential equations via differential inclusions. © 2013 Elsevier B.V. All rights reserved.","Differential inclusions; Fuzzy differential equations; Fuzzy differential inclusions","Artificial intelligence; Fuzzy sets; Differential inclusions; Fuzzy differential equations; Fuzzy differential inclusions; Numerical algorithms; Differential equations",Article,Scopus,2-s2.0-84884648457
"Patrascu A., Patriciu V.-V.","Beyond digital forensics. A cloud computing perspective over incident response and reporting",2013,"SACI 2013 - 8th IEEE International Symposium on Applied Computational Intelligence and Informatics, Proceedings",14,10.1109/SACI.2013.6609018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886473304&doi=10.1109%2fSACI.2013.6609018&partnerID=40&md5=4509edfb240e26dc0e70f94b216145e7","Cloud computing represents a different paradigm in distributed computing that involves more and more researchers. In this context, we can see that there is a need for knowing where and when a certain data is processed or stored. Compared with classic digital forensic, the field of cloud forensic poses a lot of difficulties since data is not stored on a single storage unit and furthermore it involves the use of virtualization technologies. In this paper we will present in detail a new and novel way of monitoring user activity in cloud environments using a secure cloud forensic framework. We talk about the architecture of such framework and we emphasize the way in which our research can be applied on top of new or existing cloud infrastructures. Also, for testing purposes, we have applied our findings over our previous developed cloud computing framework. © 2013 IEEE.","cloud computing; cloud computing forensics; cloud computing incident response; KVM; Linux kernel virtualization; secure data forensics; XEN","Incident response; KVM; Secure data; Virtualizations; XEN; Artificial intelligence; Digital storage; Electronic crime countermeasures; Information science; Virtual reality; Cloud computing",Conference Paper,Scopus,2-s2.0-84886473304
"Ramezani F., Lu J., Hussain F.","An online fuzzy Decision Support System for Resource Management in cloud environments",2013,"Proceedings of the 2013 Joint IFSA World Congress and NAFIPS Annual Meeting, IFSA/NAFIPS 2013",14,10.1109/IFSA-NAFIPS.2013.6608495,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886574852&doi=10.1109%2fIFSA-NAFIPS.2013.6608495&partnerID=40&md5=436833a838c9cf35e49c68da18004f7b","Cloud computing is a large-scale distributed computing paradigm driven by economies of scale, in which a pool of abstracted, virtualized, dynamically-scalable, managed computing power, storage, platforms, and services are delivered on demand to external customers over the Internet. Although a significant amount of studies have been developed to optimize resource management and task scheduling in cloud computing, none of them considered the impact of task scheduling patterns on resource management and vice versa. To overcome this drawback, and considering the lack of resources in cloud environments and growing customer demands for cloud services, this paper proposes an Online Resource Management Decision Support System (ORMDSS) that addresses both tasks scheduling and resource management optimization in a unique system. In addition, ORMDSS contains a fuzzy prediction method for predicting VM workload patterns and VM migration time by applying neural networks and fuzzy expert systems. This ORMDSS helps cloud providers to automatically allocate scare resources to the applications and services in an optimal way. It is expected that the ORMDSS not only increases cloud utilization and QoS, but also decreases cost and response time. © 2013 IEEE.",,"Cloud environments; Economies of scale; External customers; Fuzzy decision support system; Fuzzy expert systems; Fuzzy prediction; Resource management; Workload patterns; Artificial intelligence; Cloud computing; Decision support systems; Expert systems; Multitasking; Natural resources management; Optimization; Resource allocation; Scheduling algorithms; Environmental management",Conference Paper,Scopus,2-s2.0-84886574852
"Mizuki T., Asiedu I.K., Sone H.","Voting with a logarithmic number of cards",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,10.1007/978-3-642-39074-6-16,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886001440&doi=10.1007%2f978-3-642-39074-6-16&partnerID=40&md5=8b1c4345a6a0e1b918deab423f641a3b","Consider an election where there are two candidates and several voters. Such an election usually requires the same number of ballot papers as the number of voters. In this paper, we show that such an election can be conducted using only a logarithmic number of cards with two suits - black and red - with identical backs. That is, we can securely compute the summation of a number of inputs (0s and 1s) using a logarithmic number of cards with respect to the number of inputs. © 2013 Springer-Verlag Berlin Heidelberg.",,"Ballot papers; Artificial intelligence; Computer science",Conference Paper,Scopus,2-s2.0-84886001440
"Derrick D.C., Meservy T.O., Jenkins J.L., Burgoon J.K., Nunamaker Jr. J.F.","Detecting deceptive chat-based communication using typing behavior and message cues",2013,"ACM Transactions on Management Information Systems",14,10.1145/2499962.2499967,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885595173&doi=10.1145%2f2499962.2499967&partnerID=40&md5=98ae00b74d2c73bb7cdd023c290a9601","Computer-mediated deception is prevalent and may have serious consequences for individuals, organizations, and society. This article investigates several metrics as predictors of deception in synchronous chatbased environments, where participants must often spontaneously formulate deceptive responses. Based on cognitive load theory, we hypothesize that deception influences response time, word count, lexical diversity, and the number of times a chat message is edited. Using a custom chatbot to conduct interviews in an experiment, we collected 1,572 deceitful and 1,590 truthful chat-based responses. The results of the experiment confirm that deception is positively correlated with response time and the number of edits and negatively correlated to word count. Contrary to our prediction, we found that deception is not significantly correlated with lexical diversity. Furthermore, the age of the participant moderates the influence of deception on response time. Our results have implications for understanding deceit in chat-based communication and building deception-detection decision aids in chat-based systems. © 2013 ACM.","Chat; Deception detection; Decision support system; Typing bahavior","Bahavior; Chat; Chatbot; Cognitive load theory; Deception detection; Decision aids; Experiment confirm; Typing behaviors; Artificial intelligence; Decision support systems; Communication",Article,Scopus,2-s2.0-84885595173
"Basu S., Condron B., Aksel A., Acton S.T.","Segmentation and tracing of single neurons from 3D confocal microscope images",2013,"IEEE Journal of Biomedical and Health Informatics",14,10.1109/TITB.2012.2209670,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885148488&doi=10.1109%2fTITB.2012.2209670&partnerID=40&md5=eb3fd6791c9db03570d6e3b3e202359e","In order to understand the brain, we need to first understand the morphology of neurons. In the neurobiology community, there have been recent pushes to analyze both neuron connectivity and the influence of structure on function. Currently, a technical roadblock that stands in the way of these studies is the inability to automatically trace neuronal structure from microscopy. On the image processing side, proposed tracing algorithms face difficulties in low contrast, indistinct boundaries, clutter, and complex branching structure. To tackle these difficulties, we develop Tree2Tree, a robust automatic neuron segmentation and morphology generation algorithm. Tree2Tree uses a local medial tree generation strategy in combination with a global tree linking to build a maximum likelihood global tree. Recasting the neuron tracing problem in a graph-theoretic context enables Tree2Tree to estimate bifurcations naturally, which is currently a challenge for current neuron tracing algorithms. Tests on cluttered confocal microscopy images of Drosophila neurons give results that correspond to ground truth within a margin of ±2.75% normalized mean absolute error. © 2012 IEEE.","Neuron image analysis; Neuron tracing; Segmentation","Branching structures; Confocal microscope images; Generation algorithm; Mean absolute error; Neuron tracings; Neuronal structure; Tracing algorithm; Tree generation; Algorithms; Forestry; Graph theory; Image segmentation; Morphology; Neurons; Algorithms; Forestry; Image Analysis; Neural Networks; algorithm; animal; artificial intelligence; confocal microscopy; cytology; Drosophila; nerve cell; procedures; three dimensional imaging; article; confocal microscopy; methodology; nerve cell; three dimensional imaging; Algorithms; Animals; Artificial Intelligence; Cytological Techniques; Drosophila; Imaging, Three-Dimensional; Microscopy, Confocal; Neurons; Algorithms; Animals; Artificial Intelligence; Cytological Techniques; Drosophila; Imaging, Three-Dimensional; Microscopy, Confocal; Neurons",Article,Scopus,2-s2.0-84885148488
"Bartoletti M., Scalas A., Tuosto E., Zunino R.","Honesty by typing",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,10.1007/978-3-642-38592-6_21,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885004232&doi=10.1007%2f978-3-642-38592-6_21&partnerID=40&md5=90c8d1d55e1d5044c113e99573f6edcb","We propose a type system for a calculus of contracting processes. Processes may stipulate contracts, and then either behave honestly, by keeping the promises made, or not. Type safety guarantees that a typeable process is honest - that is, the process abides by the contract it has stipulated in all possible contexts, even those containing dishonest adversaries. © 2013 IFIP International Federation for Information Processing.",,"Contracting process; Type safety; Type systems; Artificial intelligence; Computer science",Conference Paper,Scopus,2-s2.0-84885004232
"Robu V., Gerding E.H., Stein S., Parkes D.C., Rogers A., Jennings N.R.","An online mechanism for multi-unit demand and its application to plug-in hybrid electric vehicle charging",2013,"Journal of Artificial Intelligence Research",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888331711&partnerID=40&md5=1670df1286038dcc9de8c7afbeb51009","We develop an online mechanism for the allocation of an expiring resource to a dynamic agent population. Each agent has a non-increasing marginal valuation function for the resource, and an upper limit on the number of units that can be allocated in any period. We propose two versions on a truthful allocation mechanism. Each modifies the decisions of a greedy online assignment algorithm by sometimes cancelling an allocation of resources. One version makes this modification immediately upon an allocation decision while a second waits until the point at which an agent departs the market. Adopting a prior-free framework, we show that the second approach has better worst-case allocative efficiency and is more scalable. On the other hand, the first approach (with immediate cancellation) may be easier in practice because it does not need to reclaim units previously allocated. We consider an application to recharging plug-in hybrid electric vehicles (PHEVs). Using data from a real-world trial of PHEVs in the UK, we demonstrate higher system performance than a fixed price system, performance comparable with a standard, but non-truthful scheduling heuristic, and the ability to support 50% more vehicles at the same fuel cost than a simple randomized policy. © 2013 AI Access Foundation.",,"Allocation decision; Allocation mechanism; Allocative efficiency; Plug-In Hybrid Electric Vehicle; Plug-in hybrid electric vehicles; Randomized policies; Scheduling heuristics; Valuation function; Artificial intelligence; Resource valuation",Article,Scopus,2-s2.0-84888331711
"Silva P.F.B., Marcal A.R.S., Da Silva R.M.A.","Evaluation of features for leaf discrimination",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,10.1007/978-3-642-39094-4_23,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884488199&doi=10.1007%2f978-3-642-39094-4_23&partnerID=40&md5=703448ac70709e772848157e7b8687f9","A number of shape features for automatic plant recognition based on digital image processing have been proposed by Pauwels et al. in 2009. A database with 15 classes and 171 leaf samples was considered for the evaluation of these measures using linear discriminant analysis and hierarchical clustering. The results obtained match the human visual shape perception with an overall accuracy of 87%. © 2013 Springer-Verlag.",,"Hier-archical clustering; Human visual; Linear discriminant analysis; Overall accuracies; Plant recognition; Shape features; Shape perception; Artificial intelligence; Computer science; Image analysis",Conference Paper,Scopus,2-s2.0-84884488199
"Zǎvoianu A.-C., Lughofer E., Koppelstätter W., Weidenholzer G., Amrhein W., Klement E.P.","On the performance of master-slave parallelization methods for multi-objective evolutionary algorithms",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,10.1007/978-3-642-38610-7_12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884401293&doi=10.1007%2f978-3-642-38610-7_12&partnerID=40&md5=97b446ef788a3b50dbc835176c999fee","This paper is focused on a comparative analysis of the performance of two master-slave parallelization methods, the basic generational scheme and the steady-state asynchronous scheme. Both can be used to improve the convergence speed of multi-objective evolutionary algorithms (MOEAs) that rely on time-intensive fitness evaluation functions. The importance of this work stems from the fact that a correct choice for one or the other parallelization method can lead to considerable speed improvements with regards to the overall duration of the optimization. Our main aim is to provide practitioners of MOEAs with a simple but effective method of deciding which master-slave parallelization option is better when dealing with a time-constrained optimization process. © 2013 Springer-Verlag.","evolutionary computation; master-slave parallelization; multi-objective optimization; performance comparison; steady-state evolution","Comparative analysis; Convergence speed; Fitness evaluations; Multi objective evolutionary algorithms; Parallelizations; Performance comparison; Speed improvement; Steady-state evolution; Artificial intelligence; Constrained optimization; Multiobjective optimization; Soft computing; Evolutionary algorithms",Conference Paper,Scopus,2-s2.0-84884401293
"Olimpieri P.P., Chailyan A., Tramontano A., Marcatili P.","Prediction of site-specific interactions in antibody-antigen complexes: The proABC method and server",2013,"Bioinformatics",14,10.1093/bioinformatics/btt369,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883469250&doi=10.1093%2fbioinformatics%2fbtt369&partnerID=40&md5=c02252d4c615c7261dbf5e870dee1ca9","Motivation: Antibodies or immunoglobulins are proteins of paramount importance in the immune system. They are extremely relevant as diagnostic, biotechnological and therapeutic tools. Their modular structure makes it easy to re-engineer them for specific purposes. Short of undergoing a trial and error process, these experiments, as well as others, need to rely on an understanding of the specific determinants of the antibody binding mode. Results: In this article, we present a method to identify, on the basis of the antibody sequence alone, which residues of an antibody directly interact with its cognate antigen. The method, based on the random forest automatic learning techniques, reaches a recall and specificity as high as 80% and is implemented as a free and easy-to-use server, named prediction of Antibody Contacts. We believe that it can be of great help in re-design experiments as well as a guide for molecular docking experiments. The results that we obtained also allowed us to dissect which features of the antibody sequence contribute most to the involvement of specific residues in binding to the antigen. © The Author 2013.",,"antigen antibody complex; antigen antibody complex; artificial intelligence; chemical structure; chemistry; computer program; evaluation study; Internet; sequence analysis; antigen antibody complex; article; chemistry; Antigen-Antibody Complex; Artificial Intelligence; Internet; Models, Molecular; Sequence Analysis, Protein; Software; Antigen-Antibody Complex; Artificial Intelligence; Internet; Models, Molecular; Sequence Analysis, Protein; Software",Article,Scopus,2-s2.0-84883469250
"Kawald B., Lenzner P.","On dynamics in selfish Network Creation",2013,"Annual ACM Symposium on Parallelism in Algorithms and Architectures",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883519759&partnerID=40&md5=b0739a2ffa007b525a4463473c2da498","We consider the dynamic behavior of several variants of the Network Creation Game, introduced by Fabrikant et al. [PODC' 03]. Equilibrium networks in these models have desirable properties like low social cost and small diameter, which makes them attractive for the decentralized creation of overlay-networks. Unfortunately, due to the nonconstructiveness of the Nash equilibrium, no distributed algorithm for finding such networks is known. We treat these games as sequential-move games and analyze if (uncoordinated) selfish play eventually converges to an equilibrium. Thus, we shed light on one of the most natural algorithms for this problem: distributed local search, where in each step some agent performs a myopic selfish improving move. We show that fast convergence is guaranteed for all versions of Swap Games, introduced by Alon et al. [SPAA'10], if the initial network is a tree. Furthermore, we prove that this process can be sped up to an almost optimal number of moves by employing a very natural move policy. Unfortunately, these positive results are no longer true if the initial network has cycles and we show the surprising result that even one non-tree edge suffices to destroy the convergence guarantee. This answers an open problem from Ehsani et al. [SPAA'11] in the negative. Moreover, we show that on non-tree networks no move policy can enforce convergence. We extend our negative results to the well-studied original version, where agents are allowed to buy and delete edges as well. For this model we prove that there is no convergence guarantee - even if all agents play optimally. Even worse, if played on a non-complete host-graph, then there are instances where no sequence of improving moves leads to a stable network. Furthermore, we analyze whether cost-sharing has positive impact on the convergence behavior. For this we consider a version by Corbo and Parkes [PODC' 05] where bilateral consent is needed for the creation of an edge and where edge-costs are shared among the involved agents. We show that employing such a cost-sharing rule yields even worse dynamic behavior. Finally, we contrast our mostly negative theoretical results by a careful empirical study. Our simulations indicate two positive facts: (1) The non-convergent behavior seems to be confined to a small set of pathological instances and is unlikely to show up in practice. (2) In all our simulations we observed a remarkably fast convergence towards a stable network in O(n) steps, where n is the number of agents. © 2013 ACM.","Convergence; Distributed local search; Game dynamics; Network creation games; Stabilization","Convergence; Convergence behaviors; Dynamic behaviors; Empirical studies; Fast convergence; Local search; Nash equilibria; Network creation; Algorithms; Artificial intelligence; Cost effectiveness; Forestry; Network architecture; Stabilization; Computer simulation; Algorithms; Cost Effectiveness; Forestry; Simulation; Stabilization",Conference Paper,Scopus,2-s2.0-84883519759
"Bolourchi P., Uysal S.","Forest fire detection in wireless sensor network using fuzzy logic",2013,"Proceedings - 5th International Conference on Computational Intelligence, Communication Systems, and Networks, CICSyN 2013",14,10.1109/CICSYN.2013.32,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883444310&doi=10.1109%2fCICSYN.2013.32&partnerID=40&md5=1d50045fe2b51622f1c20be399b2f946","The multi-purpose integrated homeland surveillance security systems are usually located in remote areas. Intelligent decision making (IDM) capability emerges as the primary feature in the realization of the T/R architecture. The aim of employing IDM is two-fold. First is to save energy, as the system operation is desired to be autonomous based on the available solar power and the corresponding battery-bank. Second is to activate the necessary action(s) required based on the pre-defined sensitivity levels. The current work is focused on the second aim using the pre-defined sensitivity levels. We propose to use a wireless sensor network (WSN) for data harvesting to be used as raw input data into our control system. Fire detection has been chosen to illustrate the IDM capability of the system. A Fuzzy Logic algorithm is developed using five membership functions as temperature, smoke, light, humidity and distance. Simulation results for the probability of fire based on the fuzzy rules using the status of the membership functions are presented in the paper. © 2013 IEEE.","Fire Detection; Fuzzy Logic; Homeland Security; Intelligent Decision Making System; Natural Hazards; Wireless Sensor Network; Wireless Surveillance","Fire detection; Forest fire detection; Fuzzy logic algorithms; Home land security; Intelligent decision making; Natural hazard; Surveillance securities; System operation; Artificial intelligence; Communication systems; Decision making; Deforestation; Fire detectors; Fuzzy logic; Security systems; Solar energy; Wireless sensor networks; Communication; Decision Making; Deforestation; Fire Detectors; Hazards; Systems",Conference Paper,Scopus,2-s2.0-84883444310
"Massanet S., Torrens J.","On the vertical threshold generation method of fuzzy implication and its properties",2013,"Fuzzy Sets and Systems",14,10.1016/j.fss.2013.03.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887120926&doi=10.1016%2fj.fss.2013.03.003&partnerID=40&md5=be4402696d5b9a9e63ede40f96885866","In recent years, some new construction methods of fuzzy implications from other given ones have been proposed. One of them, the so-called threshold generation method, preserves important properties such as the exchange principle or the law of importation under some minimal conditions. This method is based on an adequate scaling on the second variable of the two initial fuzzy implications. In this paper, we propose a new method to generate fuzzy implications from two given ones in the same spirit of the threshold generation method but now through an adequate scaling on the first variable of the given fuzzy implications. The new implications, called vertical threshold generated implications, are deeply studied focusing on the preservation of the most common properties of fuzzy implications from the initial ones to the generated implication. Moreover, they are fully characterized by means of the e-vertical section of the implication. © 2013 Elsevier B.V.","Contrapositive symmetry; Exchange principle; Fuzzy connectives and aggregation operators; Fuzzy implication; Vertical threshold generation method","Common property; Exchange principles; Fuzzy connectives and aggregation operators; Fuzzy implications; Generated implications; Generation method; New constructions; Artificial intelligence; Fuzzy sets; Fuzzy logic",Article,Scopus,2-s2.0-84887120926
"Candia J., Maunu R., Driscoll M., Biancotto A., Dagur P., McCoy Jr. J.P., Sen H.N., Wei L., Maritan A., Cao K., Nussenblatt R.B., Banavar J.R., Losert W.","From Cellular Characteristics to Disease Diagnosis: Uncovering Phenotypes with Supercells",2013,"PLoS Computational Biology",14,10.1371/journal.pcbi.1003215,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884697284&doi=10.1371%2fjournal.pcbi.1003215&partnerID=40&md5=793712e7ca12b90ac5d46eedd9c55619","Cell heterogeneity and the inherent complexity due to the interplay of multiple molecular processes within the cell pose difficult challenges for current single-cell biology. We introduce an approach that identifies a disease phenotype from multiparameter single-cell measurements, which is based on the concept of ""supercell statistics"", a single-cell-based averaging procedure followed by a machine learning classification scheme. We are able to assess the optimal tradeoff between the number of single cells averaged and the number of measurements needed to capture phenotypic differences between healthy and diseased patients, as well as between different diseases that are difficult to diagnose otherwise. We apply our approach to two kinds of single-cell datasets, addressing the diagnosis of a premature aging disorder using images of cell nuclei, as well as the phenotypes of two non-infectious uveitides (the ocular manifestations of Behçet's disease and sarcoidosis) based on multicolor flow cytometry. In the former case, one nuclear shape measurement taken over a group of 30 cells is sufficient to classify samples as healthy or diseased, in agreement with usual laboratory practice. In the latter, our method is able to identify a minimal set of 5 markers that accurately predict Behçet's disease and sarcoidosis. This is the first time that a quantitative phenotypic distinction between these two diseases has been achieved. To obtain this clear phenotypic signature, about one hundred CD8+ T cells need to be measured. Although the molecular markers identified have been reported to be important players in autoimmune disorders, this is the first report pointing out that CD8+ T cells can be used to distinguish two systemic inflammatory diseases. Beyond these specific cases, the approach proposed here is applicable to datasets generated by other kinds of state-of-the-art and forthcoming single-cell technologies, such as multidimensional mass cytometry, single-cell gene expression, and single-cell full genome sequencing techniques.",,"article; autoimmune disease; Behcet disease; CD8+ T lymphocyte; cell nucleus; cytology; diagnostic accuracy; diagnostic procedure; fibroblast; flow cytometry; gene expression; gene sequence; human; phenotype; premature aging; progeria; sarcoidosis; uveitis; Artificial Intelligence; Diagnosis; Humans",Article,Scopus,2-s2.0-84884697284
"Bagci A.M., Lee S.H., Nagornaya N., Green B.A., Alperin N.","Automated posterior cranial fossa volumetry by MRI: Applications to Chiari malformation type I",2013,"American Journal of Neuroradiology",14,10.3174/ajnr.A3515,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885392889&doi=10.3174%2fajnr.A3515&partnerID=40&md5=3dc9171a89a8b184baad8bfbd6875aaf","BACKGROUND AND PURPOSE: Quantification of PCF volume and the degree of PCF crowdedness were found beneficial for differential diagnosis of tonsillar herniation and prediction of surgical outcome inCMI. However, lack of automated methods limits the clinical use of PCF volumetry. An atlas-based method for automated PCF segmentation tailored for CMI is presented. The method performance is assessed in terms of accuracy and spatial overlap with manual segmentation. The degree of association between PCF volumes and the lengths of previously proposed linear landmarks is reported. MATERIALS AND METHODS: T1-weighted volumetric MR imaging data with 1-mm isotropic resolution obtained with the use of a 3T scanner from 14 patients with CMI and 3 healthy subjects were used for the study. Manually delineated PCF from 9 patients was used to establish a CMI-specific reference for an atlas-based automated PCF parcellation approach. Agreement between manual and automated segmentation of 5 different CMI datasets was verified by means of the t test. Measurement reproducibility was established through the use of 2 repeated scans from 3 healthy subjects. Degree of linear association between PCF volume and 6 linear landmarks was determined by means of Pearson correlation. RESULTS: PCF volumes measured by use of the automated method and with manual delineation were similar, 196.2 ± 8.7 mL versus 196.9 ± 11.0 mL, respectively. The mean relative difference of-0.3 ± 1.9% was not statistically significant. Low measurement variability, with a mean absolute percentage value of 0.6 ± 0.2%, was achieved. None of the PCF linear landmarks were significantly associated with PCF volume. CONCLUSIONS: PCF and tissue content volumes can be reliably measured in patients with CMI by use of an atlas-based automated segmentation method.",,"adult; Arnold Chiari malformation; automation; brain size; clinical article; conference paper; controlled study; female; human; intermethod comparison; male; measurement accuracy; nuclear magnetic resonance imaging; nuclear magnetic resonance scanner; posterior cranial fossa volumetry; posterior fossa; quantitative analysis; reproducibility; volumetry; Adult; Aged; Algorithms; Arnold-Chiari Malformation; Artificial Intelligence; Cranial Fossa, Posterior; Female; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Male; Middle Aged; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Young Adult",Conference Paper,Scopus,2-s2.0-84885392889
"Jang N.R., Kim H.-R., Hou C.T., Kim B.S.","Novel biobased photo-crosslinked polymer networks prepared from vegetable oil and 2,5-furan diacrylate",2013,"Polymers for Advanced Technologies",14,10.1002/pat.3147,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883050693&doi=10.1002%2fpat.3147&partnerID=40&md5=32680bc2b5e27122d261c4be8ec31d13","Novel biobased crosslinked polymer networks were prepared from vegetable oil with 2,5-furan diacrylate as a difunctional stiffener through UV photopolymerization, and the mechanical properties of the resulting films were evaluated. The vegetable oil raw materials used were acrylated epoxidized soybean oil (AESO), acrylated castor oil (ACO), and acrylated 7,10-dihydroxy-8(E)-octadecenoic acid (ADOD). 2,5-Furan dicarboxylic acid (FDCA), which can be synthesized through the oxidative dehydration of C6 sugars, was identified by the US Department of Energy as one of 12 priority chemicals for establishing the green chemistry industry of the future. 2,5-Furan dimethanol (bis-hydroxymethylfuran), which can be derived from FDCA, was used as a starting material to synthesize 2,5-furan diacrylate, which was used as a biobased comonomer along with AESO, ACO, or ADOD to form photo-crosslinked polymer networks. The synthesis of acrylate derivatives was confirmed using FT-IR and 1H-NMR spectroscopic techniques. The composition of the reaction mixture was changed to obtain crosslinked polymer networks with various mechanical properties. The addition of 2,5-furan diacrylate increased the tensile strengths of the polymer films by up to 1.4-4.2 times relative to those obtained without the addition. These fully biobased polymers derived from vegetable oil and sugar can be used as environmentally friendly renewable materials for various applications to replace the existing petroleum-based polymers currently used. © 2013 John Wiley &amp; Sons, Ltd.","2,5-furan diacrylate; Biobased; Environmentally friendly; Photo-crosslinked polymer network; Vegetable oil","Acrylated epoxidized soybean oil; Bio-based; Crosslinked polymer networks; Diacrylates; Environmentally friendly; Polymer networks; Spectroscopic technique; US Department of Energy; Artificial intelligence; Mechanical properties; Organic pollutants; Photopolymerization; Polymer films; Tensile strength; Vegetable oils; Aromatic compounds",Article,Scopus,2-s2.0-84883050693
"Klebanov V., Manthey N., Muise C.","SAT-based analysis and quantification of information flow in programs",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,10.1007/978-3-642-40196-1_16,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882759876&doi=10.1007%2f978-3-642-40196-1_16&partnerID=40&md5=3be62925d3fdce296b4efc25d5400132","Quantitative information flow analysis (QIF) is a portfolio of security techniques quantifying the flow of confidential information to public ports. In this paper, we advance the state of the art in QIF for imperative programs. We present both an abstract formulation of the analysis in terms of verification condition generation, logical projection and model counting, and an efficient concrete implementation targeting ANSI C programs. The implementation combines various novel and existing SAT-based tools for bounded model checking, #SAT solving in presence of projection, and SAT preprocessing. We evaluate the technique on synthetic and semi-realistic benchmarks. © 2013 Springer-Verlag.",,"Bounded model checking; Confidential information; Imperative programs; Information flows; Quantitative information flows; Sat-based analysis; State of the art; Verification condition; Artificial intelligence; Computer science; Model checking",Conference Paper,Scopus,2-s2.0-84882759876
"Roberts K., Rink B., Harabagiu S.M.","A flexible framework for recognizing events, temporal expressions, and temporal relations in clinical text",2013,"Journal of the American Medical Informatics Association",14,10.1136/amiajnl-2013-001619,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882761848&doi=10.1136%2famiajnl-2013-001619&partnerID=40&md5=2a553bd35d9fe2594b8746b41ec5b9b4","Objective: To provide a natural language processing method for the automatic recognition of events, temporal expressions, and temporal relations in clinical records. Materials and Methods: A combination of supervised, unsupervised, and rule-based methods were used. Supervised methods include conditional random fields and support vector machines. A flexible automated feature selection technique was used to select the best subset of features for each supervised task. Unsupervised methods include Brown clustering on several corpora, which result in our method being considered semisupervised. Results: On the 2012 Informatics for Integrating Biology and the Bedside (i2b2) shared task data, we achieved an overall event F1-measure of 0.8045, an overall temporal expression F1-measure of 0.6154, an overall temporal link detection F1-measure of 0.5594, and an end-to-end temporal link detection F1-measure of 0.5258. The most competitive system was our event recognition method, which ranked third out of the 14 participants in the event task. Discussion: Analysis reveals the event recognition method has difficulty determining which modifiers to include/exclude in the event span. The temporal expression recognition method requires significantly more normalization rules, although many of these rules apply only to a small number of cases. Finally, the temporal relation recognition method requires more advanced medical knowledge and could be improved by separating the single discourse relation classifier into multiple, more targeted component classifiers. Conclusions: Recognizing events and temporal expressions can be achieved accurately by combining supervised and unsupervised methods, even when only minimal medical knowledge is available. Temporal normalization and temporal relation recognition, however, are far more dependent on the modeling of medical knowledge.",,"article; book; data extraction; electronic medical record; medical informatics; natural language processing; practice guideline; support vector machine; Clinical Informatics; Medical Records Systems, Computerized; Natural Language Processing; Artificial Intelligence; Electronic Health Records; Humans; Information Storage and Retrieval; Natural Language Processing; Time; Translational Medical Research",Article,Scopus,2-s2.0-84882761848
"Yang Z., Wong P.K., Vong C.M., Zhong J., Liang J.","Simultaneous-fault diagnosis of gas turbine generator systems using a pairwise-coupled probabilistic classifier",2013,"Mathematical Problems in Engineering",14,10.1155/2013/827128,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881450982&doi=10.1155%2f2013%2f827128&partnerID=40&md5=4936e98ab130d0465a3f8b760a7d278a","A reliable fault diagnostic system for gas turbine generator system (GTGS), which is complicated and inherent with many types of component faults, is essential to avoid the interruption of electricity supply. However, the GTGS diagnosis faces challenges in terms of the existence of simultaneous-fault diagnosis and high cost in acquiring the exponentially increased simultaneous-fault vibration signals for constructing the diagnostic system. This research proposes a new diagnostic framework combining feature extraction, pairwise-coupled probabilistic classifier, and decision threshold optimization. The feature extraction module adopts wavelet packet transform and time-domain statistical features to extract vibration signal features. Kernel principal component analysis is then applied to further reduce the redundant features. The features of single faults in a simultaneous-fault pattern are extracted and then detected using a probabilistic classifier, namely, pairwise-coupled relevance vector machine, which is trained with single-fault patterns only. Therefore, the training dataset of simultaneous-fault patterns is unnecessary. To optimize the decision threshold, this research proposes to use grid search method which can ensure a global solution as compared with traditional computational intelligence techniques. Experimental results show that the proposed framework performs well for both single-fault and simultaneous-fault diagnosis and is superior to the frameworks without feature extraction and pairwise coupling. © 2013 Zhixin Yang et al.",,"Computational intelligence techniques; Decision threshold; Fault diagnostic systems; Kernel principal component analyses (KPCA); Probabilistic classifiers; Relevance Vector Machine; Statistical features; Wavelet packet transforms; Artificial intelligence; Principal component analysis; Signal processing; Feature extraction",Article,Scopus,2-s2.0-84881450982
"Henderson J.M., Nuthmann A., Luke S.G.","Eye movement control during scene viewing: Immediate effects of scene luminance on fixation durations",2013,"Journal of Experimental Psychology: Human Perception and Performance",14,10.1037/a0031224,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881013151&doi=10.1037%2fa0031224&partnerID=40&md5=93bdea8e740f649102a9f2d43ccd098d","Recent research on eye movements during scene viewing has primarily focused on where the eyes fixate. But eye fixations also differ in their durations. Here we investigated whether fixation durations in scene viewing are under the direct and immediate control of the current visual input. Subjects freely viewed photographs of scenes in preparation for a later memory test while their eye movements were recorded. Using a novel scene degradation paradigm based on a saccade-contingent display change method, scenes were reduced in luminance during saccades ending in critical fixations. Results from two experiments showed that the durations of the critical fixations were immediately affected by scene luminance, with a monotonic relationship between luminance reduction and fixation duration. The results are the first to demonstrate that fixation durations in scene viewing are immediately influenced by the ease of processing of the image currently in view. These results are consistent with the CRISP (a timer-Controlled Random-Walk with Inhibition for Saccade Planning) computational model of saccade generation in scenes, proposing that difficulty in moment-by-moment visual and cognitive processing of the scene modulates fixation durations. © 2012 American Psychological Association.","Eye movements; Fixation duration; Scene perception","article; artificial intelligence; attention; eye fixation; human; illumination; inhibition (psychology); orientation; pattern recognition; perception; perceptive discrimination; psychophysics; reaction time; saccadic eye movement; Artificial Intelligence; Attention; Discrimination (Psychology); Fixation, Ocular; Humans; Inhibition (Psychology); Lighting; Orientation; Pattern Recognition, Visual; Perceptual Masking; Psychophysics; Reaction Time; Saccades",Article,Scopus,2-s2.0-84881013151
"Buske O.J., Manickaraj A., Mital S., Ray P.N., Brudno M.","Identification of deleterious synonymous variants in human genomes",2013,"Bioinformatics",14,10.1093/bioinformatics/btt308,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880518692&doi=10.1093%2fbioinformatics%2fbtt308&partnerID=40&md5=038081413be0fb214cb3e3325dd751dd","Motivation: The prioritization and identification of disease-causing mutations is one of the most significant challenges in medical genomics. Currently available methods address this problem for non-synonymous single nucleotide variants (SNVs) and variation in promoters/enhancers; however, recent research has implicated synonymous (silent) exonic mutations in a number of disorders.Results: We have curated 33 such variants from literature and developed the Silent Variant Analyzer (SilVA), a machine-learning approach to separate these from among a large set of rare polymorphisms. We evaluate SilVA's performance on in silico 'infection' experiments, in which we implant known disease-causing mutations into a human genome, and show that for 15 of 33 disorders, we rank the implanted mutation among the top five most deleterious ones. Furthermore, we apply the SilVA method to two additional datasets: synonymous variants associated with Meckel syndrome, and a collection of silent variants clinically observed and stratified by a molecular diagnostics laboratory, and show that SilVA is able to accurately predict the harmfulness of silent variants in these datasets.Availability: SilVA is open source and is freely available from the project website: http://compbio.cs.toronto.edu/ silvaContact: Supplementary information: Supplementary data are available at Bioinformatics online. © 2013 The Author 2013. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com.",,"article; artificial intelligence; ciliary dyskinesia; computer simulation; diseases; encephalocele; exon; genetic polymorphism; genetics; genomics; human; human genome; kidney polycystic disease; methodology; mutation; diseases; procedures; Artificial Intelligence; Ciliary Motility Disorders; Computer Simulation; Disease; Encephalocele; Exons; Genome, Human; Genomics; Humans; Mutation; Polycystic Kidney Diseases; Polymorphism, Genetic; Artificial Intelligence; Ciliary Motility Disorders; Computer Simulation; Disease; Encephalocele; Exons; Genome, Human; Genomics; Humans; Mutation; Polycystic Kidney Diseases; Polymorphism, Genetic",Article,Scopus,2-s2.0-84880518692
"Arsanjani R., Xu Y., Dey D., Vahistha V., Shalev A., Nakanishi R., Hayes S., Fish M., Berman D., Germano G., Slomka P.J.","Improved accuracy of myocardial perfusion SPECT for detection of coronary artery disease by machine learning in a large population",2013,"Journal of Nuclear Cardiology",14,10.1007/s12350-013-9706-2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880141012&doi=10.1007%2fs12350-013-9706-2&partnerID=40&md5=7c1c4b6d29e407a7595b3ee40910601a","Objective: We aimed to improve the diagnostic accuracy of myocardial perfusion SPECT (MPS) by integrating clinical data and quantitative image features with machine learning (ML) algorithms. Methods: 1,181 rest 201Tl/stress 99mTc-sestamibi dual-isotope MPS studies [713 consecutive cases with correlating invasive coronary angiography (ICA) and suspected coronary artery disease (CAD) and 468 with low likelihood (LLk) of CAD &lt;5%] were considered. Cases with stenosis &lt;70% by ICA and LLk of CAD were considered normal. Total stress perfusion deficit (TPD) for supine/prone data, stress/rest perfusion change, and transient ischemic dilatation were derived by automated perfusion quantification software and were combined with age, sex, and post-electrocardiogram CAD probability by a boosted ensemble ML algorithm (LogitBoost). The diagnostic accuracy of the model for prediction of obstructive CAD ≥70% was compared to standard prone/supine quantification and to visual analysis by two experienced readers utilizing all imaging, quantitative, and clinical data. Tenfold stratified cross-validation was performed. Results: The diagnostic accuracy of ML (87.3% ± 2.1%) was similar to Expert 1 (86.0% ± 2.1%), but superior to combined supine/prone TPD (82.8% ± 2.2%) and Expert 2 (82.1% ± 2.2%) (P &lt;.01). The receiver operator characteristic areas under curve for ML algorithm (0.94 ± 0.01) were higher than those for TPD and both visual readers (P &lt;.001). The sensitivity of ML algorithm (78.9% ± 4.2%) was similar to TPD (75.6% ± 4.4%) and Expert 1 (76.3% ± 4.3%), but higher than that of Expert 2 (71.1% ± 4.6%), (P &lt;.01). The specificity of ML algorithm (92.1% ± 2.2%) was similar to Expert 1 (91.4% ± 2.2%) and Expert 2 (88.3% ± 2.5%), but higher than TPD (86.8% ± 2.6%), (P &lt;.01). Conclusion: ML significantly improves diagnostic performance of MPS by computational integration of quantitative perfusion and clinical data to the level rivaling expert analysis. © 2013 American Society of Nuclear Cardiology.","automated quantification; coronary artery disease; machine learning; Myocardial perfusion imaging: SPECT; total perfusion deficit","methoxy isobutyl isonitrile technetium tc 99m; thallium 201; adult; age; angiocardiography; area under the curve; article; cardiovascular parameters; clinical assessment; computer program; controlled study; coronary artery disease; coronary artery obstruction; diagnostic accuracy; diagnostic test accuracy study; electrocardiogram; female; human; image processing; machine learning; major clinical study; male; myocardial perfusion imaging; predictive value; priority journal; quantitative analysis; receiver operating characteristic; sensitivity and specificity; sex difference; single photon emission computer tomography; stress rest perfusion change; total perfusion deficit; transient ischemic dilation; Aged; Algorithms; Artifacts; Artificial Intelligence; Coronary Angiography; Coronary Artery Disease; Electrocardiography; Female; Humans; Male; Middle Aged; Myocardial Ischemia; Myocardial Perfusion Imaging; Observer Variation; Pattern Recognition, Automated; Perfusion; Radiopharmaceuticals; Reproducibility of Results; ROC Curve; Technetium Tc 99m Sestamibi; Tomography, Emission-Computed, Single-Photon",Article,Scopus,2-s2.0-84880141012
"Schwarz K., Weathers K.C., Pickett S.T.A., Lathrop Jr. R.G., Pouyat R.V., Cadenasso M.L.","A comparison of three empirically based, spatially explicit predictive models of residential soil Pb concentrations in Baltimore, Maryland, USA: Understanding the variability within cities",2013,"Environmental Geochemistry and Health",14,10.1007/s10653-013-9510-6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879785384&doi=10.1007%2fs10653-013-9510-6&partnerID=40&md5=6f53ee8a4c3ce79cdcbc0203d69ecfdf","In many older US cities, lead (Pb) contamination of residential soil is widespread; however, contamination is not uniform. Empirically based, spatially explicit models can assist city agencies in addressing this important public health concern by identifying areas predicted to exceed public health targets for soil Pb contamination. Sampling of 61 residential properties in Baltimore City using field portable X-ray fluorescence revealed that 53 % had soil Pb that exceeded the USEPA reportable limit of 400 ppm. These data were used as the input to three different spatially explicit models: a traditional general linear model (GLM), and two machine learning techniques: classification and regression trees (CART) and Random Forests (RF). The GLM revealed that housing age, distance to road, distance to building, and the interactions between variables explained 38 % of the variation in the data. The CART model confirmed the importance of these variables, with housing age, distance to building, and distance to major road networks determining the terminal nodes of the CART model. Using the same three predictor variables, the RF model explained 42 % of the variation in the data. The overall accuracy, which is a measure of agreement between the model and an independent dataset, was 90 % for the GLM, 83 % for the CART model, and 72 % for the RF model. A range of spatially explicit models that can be adapted to changing soil Pb guidelines allows managers to select the most appropriate model based on public health targets. © 2013 Springer Science+Business Media Dordrecht.","Classification and regression trees; Pb; Random Forest; Soil; Spatial modeling; Urban","lead; accuracy assessment; classification; comparative study; concentration (composition); guideline; lead; model test; pollution monitoring; regression analysis; soil analysis; soil pollution; urban area; X-ray fluorescence; article; artificial intelligence; comparative study; demography; environmental exposure; environmental monitoring; evaluation study; human; methodology; soil pollutant; spectrometry; statistical model; theoretical model; United States; Artificial Intelligence; Baltimore; Environmental Exposure; Environmental Monitoring; Humans; Lead; Linear Models; Maryland; Models, Theoretical; Residence Characteristics; Soil Pollutants; Spectrometry, X-Ray Emission; Baltimore; Maryland; United States",Article,Scopus,2-s2.0-84879785384
"Zhang F., Zhang Z., Liu Y., Lu H., Leng J.","The quintuple-shape memory effect in electrospun nanofiber membranes",2013,"Smart Materials and Structures",14,10.1088/0964-1726/22/8/085020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881161530&doi=10.1088%2f0964-1726%2f22%2f8%2f085020&partnerID=40&md5=ab016a58690b4810712080c284311f9f","Shape memory fibrous membranes (SMFMs) are an emerging class of active polymers, which are capable of switching from a temporary shape to their permanent shape upon appropriate stimulation. Quintuple-shape memory membranes based on the thermoplastic polymer Nafion, with a stable fibrous structure, are achieved via electrospinning technology, and possess a broad transition temperature. The recovery of multiple temporary shapes of electrospun membranes can be triggered by heat in a single triple-, quadruple-, quintuple-shape memory cycle, respectively. The fiber morphology and nanometer size provide unprecedented design flexibility for the adjustable morphing effect. SMFMs enable complex deformations at need, having a wide potential application field including smart textiles, artificial intelligence robots, bio-medical engineering, aerospace technologies, etc in the future. © 2013 IOP Publishing Ltd.",,"Aerospace technologies; Application fields; Complex deformation; Design flexibility; Electrospun membranes; Electrospun nanofibers; Fibrous structures; Thermoplastic polymer; Artificial intelligence; Biomedical engineering; Fibrous membranes",Article,Scopus,2-s2.0-84881161530
"Tomiyama T., Van Beek T.J., Cabrera A.A.A., Komoto H., D'Amelio V.","Making function modeling practically usable",2013,"Artificial Intelligence for Engineering Design, Analysis and Manufacturing: AIEDAM",14,10.1017/S0890060413000309,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880847997&doi=10.1017%2fS0890060413000309&partnerID=40&md5=fcb36cdceedf93f5fc4ddb2a1403c92a","Function modeling is considered potentially useful in various fields of engineering, including engineering design. However, a close look at practices reveals that practitioners do not use formal function modeling so much, while the concept of function frequently appears in many practical methods without a vigorous definition. This paper tries to understand why formal function modeling is not practically utilized in industry by analyzing usage cases of function. By observing product development activities in industry, the paper identifies three problems that prevent formal function modeling from wider applications in practices, namely, practitioners' neglect of function modeling, the lack of practically useful function reasoning, and the complexity of the methods and tools of formal function modeling that make them impractical. Finally, the paper proposes strategies to tackle these problems and illustrates some research efforts in this regard. Copyright © Cambridge University Press 2013.","Function; Function modeling; Function reasoning; Product development; Systems architecting","Engineering design; Function modeling; Practical method; Product development activities; Research efforts; Systems architecting; Artificial intelligence; Engineering; Functions; Industrial engineering; Product development",Article,Scopus,2-s2.0-84880847997
"Jimenez-Hernandez M., Hughes C., Bassan P., Ball F., Brown M.D., Clarke N.W., Gardner P.","Exploring the spectroscopic differences of caki-2 cells progressing through the cell cycle while proliferating in vitro",2013,"Analyst",14,10.1039/c3an00507k,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891724180&doi=10.1039%2fc3an00507k&partnerID=40&md5=7ba5d843862c69bc8abb13ee5237e0fb","FTIR micro-spectral images of Caki-2 cells cytospun onto calcium fluoride (CaF2) slides were used to build a computational model in order to discriminate between the biochemical events of the continuous cell cycle during proliferation. Multivariate analysis and machine learning techniques such as PCA, PLSR and SVMs were used to highlight the chemical differences among the cell cycle phases and also to point out the need for removing the distortion of the spectra due to the morphology of the cells. Results showed cell cycle dependant scattering profiles that enabled the training of a SVM in order to recognise, with a relative high accuracy, each cell cycle phase purely with the scattering curve removed from the FTIR data after being subject to the RMieS-EMSC algorithm. © The Royal Society of Chemistry 2013.",,"article; artificial intelligence; cell cycle; cell proliferation; cell size; computer simulation; fluorescence microscopy; fluorescent antibody technique; human; in vitro study; infrared spectroscopy; kidney carcinoma; kidney tumor; methodology; pathology; physiology; principal component analysis; tumor cell culture; cell cycle; infrared spectroscopy; kidney carcinoma; kidney tumor; pathology; physiology; procedures; Artificial Intelligence; Carcinoma, Renal Cell; Cell Cycle; Cell Proliferation; Cell Size; Computer Simulation; Fluorescent Antibody Technique; Humans; Kidney Neoplasms; Microscopy, Fluorescence; Principal Component Analysis; Spectroscopy, Fourier Transform Infrared; Tumor Cells, Cultured; Artificial Intelligence; Carcinoma, Renal Cell; Cell Cycle; Cell Proliferation; Cell Size; Computer Simulation; Fluorescent Antibody Technique; Humans; In Vitro Techniques; Kidney Neoplasms; Microscopy, Fluorescence; Principal Component Analysis; Spectroscopy, Fourier Transform Infrared; Tumor Cells, Cultured",Article,Scopus,2-s2.0-84891724180
"Garrido A., Onaindia E.","Assembling learning objects for personalized learning: An ai planning perspective",2013,"IEEE Intelligent Systems",14,10.1109/MIS.2011.36,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880148854&doi=10.1109%2fMIS.2011.36&partnerID=40&md5=2bdc8a240ebe4de9142f48b584201922","The aim of educational systems is to assemble learning objects on a set of topics tailored to the goals and individual students' styles. Given the amount of available Learning Objects, the challenge of e-learning is to select the proper objects, define their relationships, and adapt their sequencing to the specific needs, objectives, and background of the student. This article describes the general requirements for course adaptation, the full potential of applying planning techniques on the construction of personalized e-learning routes, and how to accommodate the temporal and resource constraints to make the course applicable in a real scenario. © 2001-2011 IEEE.","applications and expert knowledge-intensive systems; e-learning; education; personalized learning; planning","AI planning; Applications and Expert Knowledge-Intensive Systems; Educational systems; Learning objects; Personalized e-learning; Personalized learning; Planning techniques; Resource Constraint; Artificial intelligence; Education; Intelligent systems; Planning; E-learning",Article,Scopus,2-s2.0-84880148854
"Mosallam A., Medjaher K., Zerhouni N.","Nonparametric time series modelling for industrial prognostics and health management",2013,"International Journal of Advanced Manufacturing Technology",14,10.1007/s00170-013-5065-z,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887618999&doi=10.1007%2fs00170-013-5065-z&partnerID=40&md5=9d483c810cb9dea1bcc3d16f15464255","Prognostics and health management (PHM) methods aim at detecting the degradation, diagnosing the faults and predicting the time at which a system or a component will no longer perform its desired function. PHM is based on access to a model of a system or a component using one or combination of physical or data-driven models. In physical-based models, one has to gather a lot of knowledge about the desired system and then build an analytical model of the system function of the degradation mechanism that is used as a reference during system operation. On the other hand, data-driven models are based on the exploitation of symptoms or indicators of degradations using statistical or artificial intelligence methods on the monitored system once it is operational and learn the normal behaviour. Trend extraction is one of the methods used to extract important information contained in the sensory signals, which can be used for data-driven models. However, extraction of such information from the collected data in a practical working environment is always a great challenge as sensory signals are usually multidimensional and obscured by noise. Also, the extracted trends should represent the nominal behaviour of the system as well as the health status evolution. This paper presents a method for nonparametric trend modelling from multidimensional sensory data so as to use such trends in machinery health prognostics. The goal of this work is to develop a method that can extract features representing the nominal behaviour of the monitored component, and from these features, smooth trends are extracted to represent the critical component's health evolution over the time. The proposed method starts by multidimensional feature extraction from machinery sensory signals. Then, unsupervised feature selection on the features' domain is applied without making any assumptions concerning the number of the extracted features. The selected features can be used to represent the nominal behaviour of the system and hence detect any deviation. Then, empirical mode decomposition algorithm is applied on the projected features with the purpose of following the evolution of data in a compact representation over time. Finally, ridge regression is applied to the extracted trend for modelling and can be used later for the remaining useful life prediction. The method is demonstrated on accelerated degradation data set of bearings acquired from PRONOSTIA experimental platform and another data set downloaded from NASA repository where it is shown to be able to extract signal trends. © 2013 Springer-Verlag London.","Feature extraction; Health indicator; Health state detection; Prognostics; Trend construction","Artificial intelligence methods; Empirical Mode Decomposition; Health indicators; Health state; Prognostics; Prognostics and health managements; Remaining useful life predictions; Unsupervised feature selection; Artificial intelligence; Degradation; Extraction; Feature extraction; Health; Machinery; Models; NASA; Regression analysis; Systems engineering; Signal detection",Article,Scopus,2-s2.0-84887618999
"Armañanzas R., Bielza C., Chaudhuri K.R., Martinez-Martin P., Larrañaga P.","Unveiling relevant non-motor Parkinson's disease severity symptoms using a machine learning approach",2013,"Artificial Intelligence in Medicine",14,10.1016/j.artmed.2013.04.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880043209&doi=10.1016%2fj.artmed.2013.04.002&partnerID=40&md5=467f01e3c9092021825fd1cad0113933","Objective: Is it possible to predict the severity staging of a Parkinson's disease (PD) patient using scores of non-motor symptoms? This is the kickoff question for a machine learning approach to classify two widely known PD severity indexes using individual tests from a broad set of non-motor PD clinical scales only Methods: The Hoehn & Yahr index and clinical impression of severity index are global measures of PD severity They constitute the labels to be assigned in two supervised classification problems using only non-motor symptom tests as predictor variables Such predictors come from a wide range of PD symptoms, such as cognitive impairment, psychiatric complications, autonomic dysfunction or sleep disturbance The classification was coupled with a feature subset selection task using an advanced evolutionary algorithm, namely an estimation of distribution algorithm Results: Results show how five different classification paradigms using a wrapper feature selection scheme are capable of predicting each of the class variables with estimated accuracy in the range of 72-92% In addition, classification into the main three severity categories (mild, moderate and severe) was split into dichotomic problems where binary classifiers perform better and select different subsets of non-motor symptoms The number of jointly selected symptoms throughout the whole process was low, suggesting a link between the selected non-motor symptoms and the general severity of the disease Conclusion: Quantitative results are discussed from a medical point of view, reflecting a clear translation to the clinical manifestations of PD Moreover, results include a brief panel of non-motor symptoms that could help clinical practitioners to identify patients who are at different stages of the disease from a limited set of symptoms, such as hallucinations, fainting, inability to control body sphincters or believing in unlikely facts © 2013 Elsevier B.V.","Estimation of distribution algorithms; Feature subset selection; Parkinson's disease; Severity indexes","Clinical manifestation; Cognitive impairment; Estimation of distribution algorithms; Feature subset selection; Machine learning approaches; Parkinson's disease; Severity indexes; Supervised classification; Disease control; Evolutionary algorithms; Heuristic algorithms; Learning systems; Medical problems; Neurodegenerative diseases; Feature extraction; amantadine; antiparkinson agent; dopamine receptor stimulating agent; levodopa; rasagiline; selegiline; accuracy; adult; aged; article; autonomic dysfunction; classifier; cognitive defect; disease classification; disease duration; disease severity; evolutionary algorithm; faintness; female; hallucination; human; machine learning; major clinical study; male; mental disease; motor dysfunction; Parkinson disease; predictor variable; priority journal; questionnaire; rating scale; sleep disorder; staging; Estimation of distribution algorithms; Feature subset selection; Parkinson's disease; Severity indexes; Aged; Algorithms; Artificial Intelligence; Decision Trees; Diagnosis, Computer-Assisted; Disease Progression; Female; Humans; Male; Middle Aged; Parkinson Disease; Predictive Value of Tests; Prognosis; Severity of Illness Index; Software Design",Article,Scopus,2-s2.0-84880043209
"Means T., Olson E., Spooner J.","Discovering ways that don't work on the road to success: Strengths and weaknesses revealed by an active learning studio classroom project",2013,"Cases on Educational Technology Planning, Design, and Implementation: A Project Management Perspective",14,10.4018/978-1-4666-4237-9.ch006,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944810356&doi=10.4018%2f978-1-4666-4237-9.ch006&partnerID=40&md5=20c1e10920c4f507478449a41d93a7b9","Educational technology projects undertaken by higher education institutions range in complexity, scope, and impact. The Edison project created a sophisticated studio classroom that supports active learning teaching methods for both local and distant students. The team undertaking this complex project was composed of information technology and instructional design professionals with no real background in formal project management techniques. The team soon discovered that intuition and organic processes for implementing a complex project with increasing scope Audioresulted in risks and challenges that threatened the success and potential impact of the project. The project team learned valuable lessons about the need for a systematic project management process. This case shares the project details, major accomplishments, and lessons learned by the team through the Active Learning Studio classroom (Edison) project. © 2013, IGI Global.",,"Artificial intelligence; Human resource management; Project management; Students; Studios; Teaching; Active Learning; Complex projects; Higher education institutions; Instructional designs; Management techniques; Potential impacts; Project management process; Teaching methods; Education",Book Chapter,Scopus,2-s2.0-84944810356
"Jimeno-Yepes A.J., Plaza L., Mork J.G., Aronson A.R., Díaz A.","MeSH indexing based on automatically generated summaries",2013,"BMC Bioinformatics",14,10.1186/1471-2105-14-208,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879435368&doi=10.1186%2f1471-2105-14-208&partnerID=40&md5=591297c23014bd383436f9b01dd91c65","Background: MEDLINE citations are manually indexed at the U.S. National Library of Medicine (NLM) using as reference the Medical Subject Headings (MeSH) controlled vocabulary. For this task, the human indexers read the full text of the article. Due to the growth of MEDLINE, the NLM Indexing Initiative explores indexing methodologies that can support the task of the indexers. Medical Text Indexer (MTI) is a tool developed by the NLM Indexing Initiative to provide MeSH indexing recommendations to indexers. Currently, the input to MTI is MEDLINE citations, title and abstract only. Previous work has shown that using full text as input to MTI increases recall, but decreases precision sharply. We propose using summaries generated automatically from the full text for the input to MTI to use in the task of suggesting MeSH headings to indexers. Summaries distill the most salient information from the full text, which might increase the coverage of automatic indexing approaches based on MEDLINE. We hypothesize that if the results were good enough, manual indexers could possibly use automatic summaries instead of the full texts, along with the recommendations of MTI, to speed up the process while maintaining high quality of indexing results.Results: We have generated summaries of different lengths using two different summarizers, and evaluated the MTI indexing on the summaries using different algorithms: MTI, individual MTI components, and machine learning. The results are compared to those of full text articles and MEDLINE citations. Our results show that automatically generated summaries achieve similar recall but higher precision compared to full text articles. Compared to MEDLINE citations, summaries achieve higher recall but lower precision.Conclusions: Our results show that automatic summaries produce better indexing than full text articles. Summaries produce similar recall to full text but much better precision, which seems to indicate that automatic summaries can efficiently capture the most important contents within the original articles. The combination of MEDLINE citations and automatically generated summaries could improve the recommendations suggested by MTI. On the other hand, indexing performance might be dependent on the MeSH heading being indexed. Summarization techniques could thus be considered as a feature selection algorithm that might have to be tuned individually for each MeSH heading. © 2013 Jimeno-Yepes et al.; licensee BioMed Central Ltd.",,"Automatically generated; Feature selection algorithm; High quality; Indexing approaches; Indexing methodologies; Medical subject headings; National library of medicines; Speed up; Abstracting; Algorithms; Automatic indexing; Indexing (of information); algorithm; article; artificial intelligence; documentation; Medical Subject Headings; Medline; methodology; Abstracting and Indexing as Topic; Algorithms; Artificial Intelligence; Medical Subject Headings; MEDLINE",Article,Scopus,2-s2.0-84879435368
"Fink A., Homberger J.","An ant-based coordination mechanism for resource-constrained project scheduling with multiple agents and cash flow objectives",2013,"Flexible Services and Manufacturing Journal",14,10.1007/s10696-012-9136-5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872686616&doi=10.1007%2fs10696-012-9136-5&partnerID=40&md5=0f58dfe37a621a3f2062453f951059df","We consider a multi-agent extension of the non-preemptive single-mode resource-constrained project scheduling problem with discounted cash flow objectives. Such a problem setting is related to project scheduling problems which involve different autonomous firms where project activities are uniquely assigned to the project parties (agents). Taking into account opportunistic agents and the resulting information asymmetry we propose a general decentralized negotiation approach which uses ideas from ant colony optimization. In the course of the negotiation the agents iteratively vote on proposed project schedules without disclosing preference information regarding cash flow values. Computational experiments serve to analyze the agent-based coordination mechanism in comparison to other approaches from the literature. The proposed mechanism turns out as an effective method for coordinating self-interested agents with conflicting goals which collaborate in resource-constrained projects. © 2012 Springer Science+Business Media, LLC.","Ant colony optimization; Decentralized coordination; Resource-constrained project scheduling","Agent-based coordination; Ant Colony Optimization (ACO); Cash flow; Computational experiment; Coordination mechanisms; Decentralized coordination; Discounted cash flow; Information asymmetry; Multiple agents; Non-preemptive; Preference information; Project activities; Project schedules; Project scheduling problem; Resource constrained project scheduling; Resource-constrained; Resource-constrained project scheduling problem; Self-interested agents; Single mode; Algorithms; Artificial intelligence; Iterative methods; Scheduling",Article,Scopus,2-s2.0-84872686616
"Koch C.P., Perna A.M., Pillong M., Todoroff N.K., Wrede P., Folkers G., Hiss J.A., Schneider G.","Scrutinizing MHC-I Binding Peptides and Their Limits of Variation",2013,"PLoS Computational Biology",14,10.1371/journal.pcbi.1003088,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879540457&doi=10.1371%2fjournal.pcbi.1003088&partnerID=40&md5=e8fe0e88148295b1a3e370e2d30fd999","Designed peptides that bind to major histocompatibility protein I (MHC-I) allomorphs bear the promise of representing epitopes that stimulate a desired immune response. A rigorous bioinformatical exploration of sequence patterns hidden in peptides that bind to the mouse MHC-I allomorph H-2Kb is presented. We exemplify and validate these motif findings by systematically dissecting the epitope SIINFEKL and analyzing the resulting fragments for their binding potential to H-2Kb in a thermal denaturation assay. The results demonstrate that only fragments exclusively retaining the carboxy- or amino-terminus of the reference peptide exhibit significant binding potential, with the N-terminal pentapeptide SIINF as shortest ligand. This study demonstrates that sophisticated machine-learning algorithms excel at extracting fine-grained patterns from peptide sequence data and predicting MHC-I binding peptides, thereby considerably extending existing linear prediction models and providing a fresh view on the computer-based molecular design of future synthetic vaccines. The server for prediction is available at http://modlab-cadd.ethz.ch (SLiDER tool, MHC-I version 2012). © 2013 Koch et al.",,"major histocompatibility antigen class 1; octapeptide; peptide; serylisoleucylisoleucylasparaginylphenylalanylglutamyllysylleucine; tetrapeptide; unclassified drug; amino terminal sequence; antigen binding; article; carboxy terminal sequence; computer model; controlled study; drug protein binding; drug structure; ligand binding; machine learning; molecular computer; peptide analysis; protein database; protein denaturation; protein motif; structural bioinformatics; Animals; Artificial Intelligence; Computational Biology; Histocompatibility Antigens Class I; Mice; Peptides; Protein Binding",Article,Scopus,2-s2.0-84879540457
"Pullan T.T., Bhasi M., Madhu G.","Decision support tool for lean product and process development",2013,"Production Planning and Control",14,10.1080/09537287.2011.633374,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877988172&doi=10.1080%2f09537287.2011.633374&partnerID=40&md5=72bb5eb91c5d13b46632bb21f21292d3","Lean and concurrent engineering (CE) are widely acknowledged business process improvement strategies. These strategies can improve processes, reduce costs, and cut waste enabling organisations to remain competitive. Lean manufacturing offers an enterprise-wide methodology that improves reliability and flexibility while reducing lead-times and inventory carrying costs. Companies in manufacturing and service sectors are focusing on integrating lean manufacturing methodology with other applications, so that, all their systems and processes are aligned. This article proposes a CE framework based on application of information technology and object-oriented methodology for lean manufacturing. This approach expected to give manufacturing companies an extra edge in today's competitive market. A case study is presented in this article to demonstrate the effectiveness of CE framework in a machine tool manufacturing company. CE practice was adopted for new products, to align the systems and processes of the company. Product development lead-time was found to decrease by more than 50% compared to similar development projects carried out by the company. The need for rework was found to be negligible and the development cost was reduced considerably. © 2013 Copyright Taylor and Francis Group, LLC.","concurrent engineering; decision support system; design for manufacture; lean manufacture","Application of information technologies; Business process improvement strategies; Decision support tools; Design for manufacture; Lean manufactures; Lean product and process development; Manufacturing companies; Object-oriented methodology; Agile manufacturing systems; Artificial intelligence; Concurrent engineering; Decision support systems; Industry; Information technology; Machine design; Product development; Cost reduction",Article,Scopus,2-s2.0-84877988172
"Capozzi D., Lanzola G.","A generic telemedicine infrastructure for monitoring an artificial pancreas trial",2013,"Computer Methods and Programs in Biomedicine",14,10.1016/j.cmpb.2013.01.011,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877019100&doi=10.1016%2fj.cmpb.2013.01.011&partnerID=40&md5=bfc284d29d5ea92c9b07bf01d53fa2f7","Telemedicine systems are seen as a possible solution for the remote monitoring of physiological parameters and can be particularly useful for chronic patients treated at home. Implementing those systems however has always required spending a great effort on the underlying infrastructure instead of focusing on the application cores as perceived by their users. This paper proposes an abstract unifying infrastructure for telemedicine services which is loosely based on the multi-agent paradigm. It provides the capability of transferring to the clinic any remotely acquired information, and possibly sending back updates to the patient. The infrastructure is a layered one, with the bottom layer acting at the data level and implemented in terms of a software library targeting a wide set of hardware devices. On top of this infrastructure several services can be written shaping the functionality of the telemedicine application while at the highest level, adhering to a simple agent model, it is possible to reuse those functional components porting the application to different platforms. The infrastructure has been successfully used for implementing a telemonitoring service for a randomized controlled study aimed at testing the effectiveness of the artificial pancreas as a treatment within the AP@home project funded by the European Union. © 2013 Elsevier Ireland Ltd.","Artificial pancreas; Diabetes; Multi-agent systems; Remote patient monitoring; Telemedicine","Artificial pancreas; Functional components; Multi agent system (MAS); Remote monitoring; Software libraries; Telemedicine application; Telemedicine services; Telemedicine systems; Artificial organs; Medical problems; Multi agent systems; Physiological models; Remote patient monitoring; Telemedicine; article; artificial intelligence; artificial pancreas; automatic speech recognition; chronic patient; clinical effectiveness; computer program; human; information processing; microcomputer; mobile phone; patient monitoring; personal digital assistant; physical activity; randomized controlled trial (topic); teleconsultation; telemedicine; telemonitoring; Algorithms; Diabetes Mellitus; Home Care Services; Humans; Insulin Infusion Systems; Monitoring, Physiologic; Pancreas, Artificial; Randomized Controlled Trials as Topic; Remote Sensing Technology; Software; Telemedicine",Article,Scopus,2-s2.0-84877019100
"Combi C., Hunsberger L., Posenato R.","An algorithm for checking the dynamic controllability of a conditional Simple Temporal Network with Uncertainty",2013,"ICAART 2013 - Proceedings of the 5th International Conference on Agents and Artificial Intelligence",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878000854&partnerID=40&md5=37c8b4b567586c3bc259084961cb18c9","A Simple Temporal Network with Uncertainty (STNU) is a framework for representing and reasoning about temporal problems involving actions whose durations are bounded but uncontrollable. A dynamically controllable STNU is one for which there exists a strategy for executing its time-points that guarantees that all of the temporal constraints in the network will be satisfied no matter how the uncontrollable durations turn out. A Conditional Simple Temporal Network with Uncertainty (CSTNU) augments an STNU to include observation nodes, where the execution of each observation node provides, in real time, the truth value of an associated proposition. Recent work has generalized the notion of dynamic controllability to cover CSTNUs. This paper presents an algorithm - called a DC-checking algorithm - for determining whether arbitrary CSTNUs are dynamically controllable. The algorithm, which is proven to be sound, is the first such algorithm to be presented in the literature. The algorithm extends edge-generation/constraint-propagation rules from an existing STNU algorithm to accommodate propositional labels, while adding new rules required to deal with the observation nodes. The paper also discusses implementation issues associated with the management of propositional labels.","Temporal controllability; Temporal network; Temporal uncertainty; Temporal workflow","Dynamic controllability; Real time; Simple temporal network with uncertainties; Temporal constraints; Temporal networks; Temporal uncertainty; Temporal workflow; Truth values; Artificial intelligence; Controllability; Algorithms",Conference Paper,Scopus,2-s2.0-84878000854
"Liu L., Zhang Q., Wu M., Li W., Shang F.","Adaptive segmentation of magnetic resonance images with intensity inhomogeneity using level set method",2013,"Magnetic Resonance Imaging",14,10.1016/j.mri.2012.10.010,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876152367&doi=10.1016%2fj.mri.2012.10.010&partnerID=40&md5=874dae05180e6a1aede6eba8e118aa66","It is a big challenge to segment magnetic resonance (MR) images with intensity inhomogeneity. The widely used segmentation algorithms are region based, which mostly rely on the intensity homogeneity, and could bring inaccurate results. In this paper, we propose a novel region-based active contour model in a variational level set formulation. Based on the fact that intensities in a relatively small local region are separable, a local intensity clustering criterion function is defined. Then, the local function is integrated around the neighborhood center to formulate a global intensity criterion function, which defines the energy term to drive the evolution of the active contour locally. Simultaneously, an intensity fitting term that drives the motion of the active contour globally is added to the energy. In order to segment the image fast and accurately, we utilize a coefficient to make the segmentation adaptive. Finally, the energy is incorporated into a level set formulation with a level set regularization term, and the energy minimization is conducted by a level set evolution process. Experiments on synthetic and real MR images show the effectiveness of our method. © 2013 Elsevier Inc.","Image segmentation; Intensity inhomogeneity; Level set; Magnetic resonance","adaptive segmentation; article; controlled study; energy; image display; image processing; intensity inhomogeneity; intermethod comparison; level set; mathematical analysis; nuclear magnetic resonance imaging; physical parameters; priority journal; statistical model; Algorithms; Artificial Intelligence; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-84876152367
"Mooney C., Haslam N.J., Holton T.A., Pollastri G., Shields D.C.","PeptideLocator: prediction of bioactive peptides in protein sequences.",2013,"Bioinformatics (Oxford, England)",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880967124&partnerID=40&md5=805db1df8197b718dc6ca00c0e899cab","Peptides play important roles in signalling, regulation and immunity within an organism. Many have successfully been used as therapeutic products often mimicking naturally occurring peptides. Here we present PeptideLocator for the automated prediction of functional peptides in a protein sequence. We have trained a machine learning algorithm to predict bioactive peptides within protein sequences. PeptideLocator performs well on training data achieving an area under the curve of 0.92 when tested in 5-fold cross-validation on a set of 2202 redundancy reduced peptide containing protein sequences. It has predictive power when applied to antimicrobial peptides, cytokines, growth factors, peptide hormones, toxins, venoms and other peptides. It can be applied to refine the choice of experimental investigations in functional studies of proteins. PeptideLocator is freely available for academic users at http://bioware.ucd.ie/.",,"antimicrobial cationic peptide; peptide; protein; algorithm; article; artificial intelligence; chemistry; classification; methodology; sequence analysis; Algorithms; Antimicrobial Cationic Peptides; Artificial Intelligence; Peptides; Proteins; Sequence Analysis, Protein",Article,Scopus,2-s2.0-84880967124
"Romero C., Zafra A., Luna J.M., Ventura S.","Association rule mining using genetic programming to provide feedback to instructors from multiple-choice quiz data",2013,"Expert Systems",14,10.1111/j.1468-0394.2012.00627.x,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877607651&doi=10.1111%2fj.1468-0394.2012.00627.x&partnerID=40&md5=1a616187892311c4062492eac9fd432a","This paper proposes the application of association rule mining to improve quizzes and courses. First, the paper shows how to preprocess quiz data and how to create several data matrices for use in the process of knowledge discovery. Next, the proposed algorithm that uses grammar-guided genetic programming is described and compared with both classical and recent soft-computing association rule mining algorithms. Then, different objective and subjective rule evaluation measures are used to select the most interesting and useful rules. Experiments have been carried out by using real data of university students enrolled on an artificial intelligence practice Moodle's course on the CLIPS programming language. Some examples of these rules are shown, together with the feedback that they provide to instructors making decisions about how to improve quizzes and courses. Finally, starting with the information provided by the rules, the CLIPS quiz and course have been updated. These innovations have been evaluated by comparing the performance achieved by students before and after applying the changes using one control group and two different experimental groups. © 2012 Wiley Publishing Ltd.","association rule mining; computer-based testing; Educational data mining; grammar-guided genetic programming","Computer-based testing; Control groups; Educational data mining; Experimental groups; Making decision; Rule evaluation; Rule mining algorithms; University students; Algorithms; Artificial intelligence; Education; Genetic programming; Data mining",Article,Scopus,2-s2.0-84877607651
"Norousi R., Wickles S., Leidig C., Becker T., Schmid V.J., Beckmann R., Tresch A.","Automatic post-picking using MAPPOS improves particle image detection from cryo-EM micrographs",2013,"Journal of Structural Biology",14,10.1016/j.jsb.2013.02.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876722357&doi=10.1016%2fj.jsb.2013.02.008&partnerID=40&md5=3134a59f2850acb4aa2e9abdb61a7535","Cryo-electron microscopy (cryo-EM) studies using single particle reconstruction are extensively used to reveal structural information on macromolecular complexes. Aiming at the highest achievable resolution, state of the art electron microscopes automatically acquire thousands of high-quality micrographs. Particles are detected on and boxed out from each micrograph using fully- or semi-automated approaches. However, the obtained particles still require laborious manual post-picking classification, which is one major bottleneck for single particle analysis of large datasets. We introduce MAPPOS, a supervised post-picking strategy for the classification of boxed particle images, as additional strategy adding to the already efficient automated particle picking routines. MAPPOS employs machine learning techniques to train a robust classifier from a small number of characteristic image features. In order to accurately quantify the performance of MAPPOS we used simulated particle and non-particle images. In addition, we verified our method by applying it to an experimental cryo-EM dataset and comparing the results to the manual classification of the same dataset. Comparisons between MAPPOS and manual post-picking classification by several human experts demonstrated that merely a few hundred sample images are sufficient for MAPPOS to classify an entire dataset with a human-like performance. MAPPOS was shown to greatly accelerate the throughput of large datasets by reducing the manual workload by orders of magnitude while maintaining a reliable identification of non-particle images. © 2013 Elsevier Inc.","3D cryo-EM density map; Classification ensemble; Electron microscopy; Machine learning; Particle picking","algorithm; article; classifier; cryoelectron microscopy; machine learning; priority journal; workload; Algorithms; Area Under Curve; Artificial Intelligence; Computer Simulation; Cryoelectron Microscopy; Escherichia coli; Image Processing, Computer-Assisted; Macromolecular Substances; Molecular Conformation; Ribosomes; Software",Article,Scopus,2-s2.0-84876722357
"Antić A., Šimunović G., Šarić T., Milošević M., Ficko M.","A model of tool wear monitoring system for turning [Model sustava za klasifikaciju trošenja alata pri obradi tokarenjem]",2013,"Tehnicki Vjesnik",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876388335&partnerID=40&md5=620fc3c9e6ce39874578a1b06d414395","Acquiring high-quality and timely information on the tool wear condition in real time, presents a necessary prerequisite for identification of tool wear degree, which significantly improves the stability and quality of the machining process. Defined in this paper is a model of tool wear monitoring system with special emphasis on the module for acquisition and processing of vibration acceleration signal by applying discrete wavelet transformations (DWT) in signal decomposition. The paper presents a model of the developed fuzzy system for tool wear classification. The system comprises three modules: module for data acquisition and processing, module for tool wear classification, and module for decision-making. The selected method for feature extraction is presented within the module for data classification and processing. The selected model for the fuzzy classifier and classification in experimental laboratory conditions is shown within data classification and clustering. The proposed model has been tested in longitudinal and transversal machining operations.","Artificial intelligence; Feature extraction; Tool wear monitoring","Data classification; Discrete wavelet transformation; Experimental laboratory; Machining operations; Machining Process; Signal decomposition; Tool wear monitoring; Vibration acceleration; Artificial intelligence; Data handling; Discrete wavelet transforms; Feature extraction; Fuzzy systems; Machining centers; Monitoring; Wear of materials",Article,Scopus,2-s2.0-84876388335
"Wan S.-P., Li D.-F., Rui Z.-F.","Possibility mean, variance and covariance of triangular intuitionistic fuzzy numbers",2013,"Journal of Intelligent and Fuzzy Systems",14,10.3233/IFS-2012-0603,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876270496&doi=10.3233%2fIFS-2012-0603&partnerID=40&md5=1052ae5e952dae864a73d633f728247e","Triangular intuitionistic fuzzy numbers (TIFNs) are a special kind of intuitionistic fuzzy sets (IFSs) on the real number set. TIFNs are useful to deal with ill-known quantities in decision data and decision making problems themselves. How to measure the value and uncertainty of a TIFN is of great importance. In this paper, we introduce the concepts of the weighted possibility mean, variance and covariance of TIFNs. Furthermore, we show that the weighted possibility mean and the weighted possibility variance of linear combination of TIFNs can be computed in a similar manner to those in probability theory. The desirable properties for the possibility covariance of TIFNs are also investigated. The concepts of the weighted possibility mean, variance and covariance of TIFNs can be considered as a generalization of those of the triangular fuzzy numbers. © 2013 - IOS Press and the authors. All rights reserved.","possibility covariance; possibility mean; possibility variance; Triangular intuitionistic fuzzy number","Decision-making problem; Intuitionistic fuzzy sets; Linear combinations; possibility covariance; possibility mean; possibility variance; Triangular fuzzy numbers; Triangular intuitionistic fuzzy numbers; Artificial intelligence; Engineering; Fuzzy rules",Article,Scopus,2-s2.0-84876270496
"Busse B., Smith S.","Automated Analysis of a Diverse Synapse Population",2013,"PLoS Computational Biology",14,10.1371/journal.pcbi.1002976,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876000308&doi=10.1371%2fjournal.pcbi.1002976&partnerID=40&md5=96a94c49cccaa0004e9de3526009ba9a","Synapses of the mammalian central nervous system are highly diverse in function and molecular composition. Synapse diversity per se may be critical to brain function, since memory and homeostatic mechanisms are thought to be rooted primarily in activity-dependent plastic changes in specific subsets of individual synapses. Unfortunately, the measurement of synapse diversity has been restricted by the limitations of methods capable of measuring synapse properties at the level of individual synapses. Array tomography is a new high-resolution, high-throughput proteomic imaging method that has the potential to advance the measurement of unit-level synapse diversity across large and diverse synapse populations. Here we present an automated feature extraction and classification algorithm designed to quantify synapses from high-dimensional array tomographic data too voluminous for manual analysis. We demonstrate the use of this method to quantify laminar distributions of synapses in mouse somatosensory cortex and validate the classification process by detecting the presence of known but uncommon proteomic profiles. Such classification and quantification will be highly useful in identifying specific subpopulations of synapses exhibiting plasticity in response to perturbations from the environment or the sensory periphery.",,"article; autoanalysis; classification algorithm; connectome; controlled study; cytoarchitecture; decision making; human; intermethod comparison; mathematical computing; mathematical model; molecular imaging; mouse; nerve cell network; nerve cell plasticity; neuroanatomy; nonhuman; proteomics; somatosensory cortex; synapse; synaptic transmission; validation process; Algorithms; Animals; Artificial Intelligence; Humans; Image Processing, Computer-Assisted; Mice; Molecular Imaging; Observer Variation; Principal Component Analysis; Proteome; Proteomics; Reproducibility of Results; Somatosensory Cortex; Synapses; Tomography; Mammalia",Article,Scopus,2-s2.0-84876000308
"Hamann H.","Towards swarm calculus: Urn models of collective decisions and universal properties of swarm performance",2013,"Swarm Intelligence",14,10.1007/s11721-013-0080-0,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880763810&doi=10.1007%2fs11721-013-0080-0&partnerID=40&md5=fd998353039c861dec24d9f39c286842","Methods of general applicability are searched for in swarm intelligence with the aim of gaining new insights about natural swarms and to develop design methodologies for artificial swarms. An ideal solution could be a 'swarm calculus' that allows to calculate key features of swarms such as expected swarm performance and robustness based on only a few parameters. To work towards this ideal, one needs to find methods and models with high degrees of generality. In this paper, we report two models that might be examples of exceptional generality. First, an abstract model is presented that describes swarm performance depending on swarm density based on the dichotomy between cooperation and interference. Typical swarm experiments are given as examples to show how the model fits to several different results. Second, we give an abstract model of collective decision making that is inspired by urn models. The effects of positive-feedback probability, that is increasing over time in a decision making system, are understood by the help of a parameter that controls the feedback based on the swarm's current consensus. Several applicable methods, such as the description as Markov process, calculation of splitting probabilities, mean first passage times, and measurements of positive feedback, are discussed and applications to artificial and natural swarms are reported. © 2013 Springer Science+Business Media New York.","Collective decision-making; Positive feedback; Swarm performance; Urn model","Collective decision; Collective decision-making; Decision-making systems; Design Methodology; Mean first passage time; Swarm performance; Universal properties; Urn models; Artificial intelligence; Calculations; Decision making; Markov processes; Feedback",Article,Scopus,2-s2.0-84880763810
"Zungeru A.M., Seng K.P., Ang L.-M., Chong Chia W.","Energy efficiency performance improvements for ant-based routing algorithm in wireless sensor networks",2013,"Journal of Sensors",14,10.1155/2013/759654,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875650869&doi=10.1155%2f2013%2f759654&partnerID=40&md5=7a523e030c38e2bd7942b0b073619786","The main problem for event gathering in wireless sensor networks (WSNs) is the restricted communication range for each node. Due to the restricted communication range and high network density, event forwarding in WSNs is very challenging and requires multihop data forwarding. Currently, the energy-efficient ant based routing (EEABR) algorithm, based on the ant colony optimization (ACO) metaheuristic, is one of the state-of-the-art energy-aware routing protocols. In this paper, we propose three improvements to the EEABR algorithm to further improve its energy efficiency. The improvements to the original EEABR are based on the following: (1) a new scheme to intelligently initialize the routing tables giving priority to neighboring nodes that simultaneously could be the destination, (2) intelligent update of routing tables in case of a node or link failure, and (3) reducing the flooding ability of ants for congestion control. The energy efficiency improvements are significant particularly for dynamic routing environments. Experimental results using the RMASE simulation environment show that the proposed method increases the energy efficiency by up to 9% and 64% in converge-cast and target-tracking scenarios, respectively, over the original EEABR without incurring a significant increase in complexity. The method is also compared and found to also outperform other swarm-based routing protocols such as sensor-driven and cost-aware ant routing (SC) and Beesensor. © 2013 Adamu Murtala Zungeru et al.",,"Ant Colony Optimization (ACO); Communication range; Energy efficiency improvements; Energy-aware routing protocol; Neighboring nodes; Performance improvements; Simulation environment; Wireless sensor network (WSNs); Artificial intelligence; Communication; Complex networks; Digital storage; Energy efficiency; Routers; Routing protocols; Sensor nodes; Algorithms",Article,Scopus,2-s2.0-84875650869
"Sadri A.R., Zekri M., Sadri S., Gheissari N., Mokhtari M., Kolahdouzan F.","Segmentation of dermoscopy images using wavelet networks",2013,"IEEE Transactions on Biomedical Engineering",14,10.1109/TBME.2012.2227478,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875503457&doi=10.1109%2fTBME.2012.2227478&partnerID=40&md5=98010940d34cc29824de84f00c3c1624","This paper introduces a new approach for the segmentation of skin lesions in dermoscopic images based on wavelet network (WN). The WN presented here is a member of fixed-grid WNs that is formed with no need of training. In this WN, after formation of wavelet lattice, determining shift and scale parameters of wavelets with two screening stage and selecting effective wavelets, orthogonal least squares algorithm is used to calculate the network weights and to optimize the network structure. The existence of two stages of screening increases globality of the wavelet lattice and provides a better estimation of the function especially for larger scales. R, G, and B values of a dermoscopy image are considered as the network inputs and the network structure formation. Then, the image is segmented and the skin lesions exact boundary is determined accordingly. The segmentation algorithm were applied to 30 dermoscopic images and evaluated with 11 different metrics, using the segmentation result obtained by a skilled pathologist as the ground truth. Experimental results show that our method acts more effectively in comparison with some modern techniques that have been successfully used in many medical imaging problems. © 1964-2012 IEEE.","Dermoscopy; image segmentation; melanoma diagnosis; orthogonal least squares (OLS); wavelet network (WN)","Dermoscopic images; Dermoscopy; Network structures; Orthogonal least squares; Orthogonal least squares algorithm; Segmentation algorithms; Segmentation results; Wavelet network; Dermatology; Medical imaging; Image segmentation; article; artificial intelligence; dermatologist; diagnostic accuracy; epiluminescence microscopy; false negative result; false positive result; image analysis; image quality; image segmentation; melanoma; sensitivity and specificity; skin tumor; wavelet network; Algorithms; Databases, Factual; Dermoscopy; Humans; Image Interpretation, Computer-Assisted; Least-Squares Analysis; Melanoma; Wavelet Analysis",Article,Scopus,2-s2.0-84875503457
"Tseng C.-C., Lin C.-L., Shih B.-Y., Chen C.-Y.","SIP-enabled Surveillance Patrol Robot",2013,"Robotics and Computer-Integrated Manufacturing",14,10.1016/j.rcim.2012.09.009,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868535937&doi=10.1016%2fj.rcim.2012.09.009&partnerID=40&md5=d7e6adc23a4dfc03afea6cde352fb3cd","Robots are gradually being introduced as part of our life. With the advances in computational power and sophisticated sensors, home safety is being ensured with the help of robots. This study proposes SSPR, a SIP-enabled Surveillance Patrol Robot which tracks a moving object actively and informs the householder of security issues for the home environment. Such a robot is equipped with a camera and is aware of session initiation protocol (SIP). When SSPR senses a moving object, it starts tracking and initiates a SIP call to the mobile device of the householder. SIP will establish both audio and video streams between SSPR and the mobile device. Thus, the householder is able to get the whole picture of the home status through his/her mobile device. With the mobility support in SIP, SSPR can switch its wireless link adaptively to another access point (AP) and the audio and video streams will continue after handoff. As a result, SSPR can complement the security routine and make a home smarter and safer. © 2012 Elsevier Ltd. All rights reserved.","Artificial intelligence; Intelligent robot; Object tracking; Session initiation protocol","Access points; Audio and video; Computational power; Home environment; Mobility supports; Moving objects; Object Tracking; Security issues; Session initiation protocol; Session Initiation Protocols; Wireless link; Artificial intelligence; Intelligent robots; Mobile devices; Robots; Video streaming; Internet protocols",Article,Scopus,2-s2.0-84868535937
"Tango F., Botta M.","Real-time detection system of driver distraction using machine learning",2013,"IEEE Transactions on Intelligent Transportation Systems",14,10.1109/TITS.2013.2247760,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878712470&doi=10.1109%2fTITS.2013.2247760&partnerID=40&md5=5781950ec3a71113e09f72534366e9d2","There is accumulating evidence that driver distraction is a leading cause of vehicle crashes and incidents. In particular, increased use of so-called in-vehicle information systems (IVIS) and partially autonomous driving assistance systems (PADAS) have raised important and growing safety concerns. Thus, detecting the driver's state is of paramount importance, to adapt IVIS and PADAS accordingly, therefore avoiding or mitigating their possible negative effects. The purpose of this paper is to show a method for the nonintrusive and real-time detection of visual distraction, using vehicle dynamics data and without using the eye-tracker data as inputs to classifiers. Specifically, we present and compare different models that are based on well-known machine learning (ML) methods. Data for training the models were collected using a static driving simulator, with real human subjects performing a specific secondary task [i.e., a surrogate visual research task (SURT)] while driving. Different training methods, model characteristics, and feature selection criteria have been compared. Based on our results, using a support vector machine (SVM) has outperformed all the other ML methods, providing the highest classification rate for most of the subjects. Potential applications of this paper include the design of an adaptive IVIS and of a ""smarter"" PADAS. © 2011 IEEE.","Accident prevention; artificial intelligence and machine learning (ML); driver distraction and inattention; intelligent supporting systems","Autonomous driving; Classification rates; Driver distractions; Fea-ture selections; In-vehicle information system; Real-time detection; Supporting systems; Visual distractions; Accident prevention; Artificial intelligence; Automobile drivers; Intelligent vehicle highway systems; Signal detection; Support vector machines; Accidents",Article,Scopus,2-s2.0-84878712470
"Insulander Björk K., Lau C.W., Nylén H., Sandberg U.","Study of thorium-plutonium fuel for possible operating cycle extension in PWRs",2013,"Science and Technology of Nuclear Installations",14,10.1155/2013/867561,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874925130&doi=10.1155%2f2013%2f867561&partnerID=40&md5=ab2813b29858c828777826bab7a69638","Computer simulations have been carried out to investigate the possibility of extending operating cycle length in the Pressurised Water Reactor Ringhals 3 by the use of thorium-plutonium oxide fuel. The calculations have been carried out using tools and methods that are normally employed for reload design and safety evaluation in Ringhals 3. The 3-batch reload scheme and the power level have been kept unchanged, and a normal uranium oxide fuel assembly designed for a 12-month operating cycle in this reactor is used as a reference. The use of plutonium as the fissile component reduces the worth of control rods and soluble boron, which makes it necessary to modify the control systems. The delayed neutron fraction is low compared with the reference, but simulations and qualitative assessments of relevant transients indicate that the reactor could still be operated safely. Differences in reactivity coefficients are mainly beneficial for the outcome of transient simulations for the thorium based fuel. A 50% extension of the current 12-month operating cycle length should be possible with thorium-plutonium mixed oxide fuel, given an upgrade of the control systems. More detailed simulations have to be carried out for some transients in order to confirm the qualitative reasoning presented. © 2013 Klara Insulander Björk et al.",,"Fissile components; Pressurised water reactor; Qualitative assessments; Qualitative reasoning; Reactivity coefficients; Thorium based fuels; Transient simulation; Worth of control rods; Artificial intelligence; Computer simulation; Control systems; Fuels; Mixed oxide fuels; Plutonium; Plutonium compounds; Thorium; Transients; Pressurized water reactors",Article,Scopus,2-s2.0-84874925130
"Charguéraud A.","Pretty-big-step semantics",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,10.1007/978-3-642-37036-6_3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874400102&doi=10.1007%2f978-3-642-37036-6_3&partnerID=40&md5=48799a4af6bf3d823441f961fb57e0a8","In spite of the popularity of small-step semantics, big-step semantics remain used by many researchers. However, big-step semantics suffer from a serious duplication problem, which appears as soon as the semantics account for exceptions and/or divergence. In particular, many premises need to be copy-pasted across several evaluation rules. This duplication problem, which is particularly visible when scaling up to full-blown languages, results in formal definitions growing far bigger than necessary. Moreover, it leads to unsatisfactory redundancy in proofs. In this paper, we address the problem by introducing pretty-big-step semantics. Pretty-big-step semantics preserve the spirit of big-step semantics, in the sense that terms are directly related to their results, but they eliminate the duplication associated with big-step semantics. © 2013 Springer-Verlag.",,"Big-step semantics; Evaluation rules; Formal definition; Scaling-up; Small-step semantics; Artificial intelligence; Semantics",Conference Paper,Scopus,2-s2.0-84874400102
"Zhao J., Brubaker M.A., Rubinstein J.L.","TMaCS: A hybrid template matching and classification system for partially-automated particle selection",2013,"Journal of Structural Biology",14,10.1016/j.jsb.2012.12.010,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873999666&doi=10.1016%2fj.jsb.2012.12.010&partnerID=40&md5=fb9bf585598e62d0efad7f96ee8691c6","Selection of particle images from electron micrographs presents a bottleneck in determining the structures of macromolecular assemblies by single particle electron cryomicroscopy (cryo-EM). The problem is particularly important when an experimentalist wants to improve the resolution of a 3D map by increasing by tens or hundreds of thousands of images the size of the dataset used for calculating the map. Although several existing methods for automatic particle image selection work well for large protein complexes that produce high-contrast images, it is well known in the cryo-EM community that small complexes that give low-contrast images are often refractory to existing automated particle image selection schemes. Here we develop a method for partially-automated particle image selection when an initial 3D map of the protein under investigation is already available. Candidate particle images are selected from micrographs by template matching with template images derived from projections of the existing 3D map. The candidate particle images are then used to train a support vector machine, which classifies the candidates as particle images or non-particle images. In a final step in the analysis, the selected particle images are subjected to projection matching against the initial 3D map, with the correlation coefficient between the particle image and the best matching map projection used to assess the reliability of the particle image. We show that this approach is able to rapidly select particle images from micrographs of a rotary ATPase, a type of membrane protein complex involved in many aspects of biology. © 2013 Elsevier Inc.","Automated; Cryo-EM; Machine learning; Particle picking; Particle selection; Single particle; Software; Template matching","article; automation; classification; classifier; computer interface; computer program; cryoelectron microscopy; image analysis; nonhuman; priority journal; protein analysis; reliability; support vector machine; template matching and classification system; Algorithms; Artificial Intelligence; Automatic Data Processing; Cryoelectron Microscopy; Hemocyanin; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Reproducibility of Results; Software",Article,Scopus,2-s2.0-84873999666
"Köpcke F., Kraus S., Scholler A., Nau C., Schüttler J., Prokosch H., Ganslandt T.","Secondary use of routinely collected patient data in a clinical trial: An evaluation of the effects on patient recruitment and data acquisition",2013,"International Journal of Medical Informatics",14,10.1016/j.ijmedinf.2012.11.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874300125&doi=10.1016%2fj.ijmedinf.2012.11.008&partnerID=40&md5=62e404b26ec6077734b7aedd75be0477","Purpose: Clinical trials are time-consuming and require constant focus on data quality. Finding sufficient time for a trial is a challenging task for involved physicians, especially when it is conducted in parallel to patient care. From the point of view of medical informatics, the growing amount of electronically available patient data allows to support two key activities: the recruitment of patients into the study and the documentation of trial data. Methods: The project was carried out at one site of a European multicenter study. The study protocol required eligibility assessment for 510 patients in one week and the documentation of 46-186 data elements per patient. A database query based on routine data from patient care was set up to identify eligible patients and its results were compared to those of manual recruitment. Additionally, routine data was used to pre-populate the paper-based case report forms and the time necessary to fill in the remaining data elements was compared to completely manual data collection. Results: Even though manual recruitment of 327 patients already achieved high sensitivity (88%) and specificity (87%), the subsequent electronic report helped to include 42 (14%) additional patients and identified 21 (7%) patients, who were incorrectly included. Pre-populating the case report forms decreased the time required for documentation from a median of 255 to 30. s. Conclusions: Reuse of routine data can help to improve the quality of patient recruitment and may reduce the time needed for data acquisition. These benefits can exceed the efforts required for development and implementation of the corresponding electronic support systems. © 2012 Elsevier Ireland Ltd.","Clinical decision support systems; Clinical trial; Documentation; Research subject recruitment","Case report forms; Clinical decision support systems; Clinical trial; Data collection; Data elements; Data quality; Database queries; Electronic reports; Electronic support; High sensitivity; Medical informatics; Multicenter study; Patient care; Patient data; Point of views; Research subjects; Secondary use; Artificial intelligence; Data acquisition; Decision support systems; Experiments; Hospital data processing; System program documentation; Medical applications; anesthesist; article; clinical trial (topic); data base; data collection method; human; information processing; major clinical study; medical documentation; medical informatics; operating room; patient care; patient coding; patient participation; patient selection; priority journal; sensitivity and specificity; workflow; Clinical Trials as Topic; Data Interpretation, Statistical; Europe; Patient Selection",Article,Scopus,2-s2.0-84874300125
"Onieva E., Godoy J., Villagrá J., Milanés V., Pérez J.","On-line learning of a fuzzy controller for a precise vehicle cruise control system",2013,"Expert Systems with Applications",14,10.1016/j.eswa.2012.08.036,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870255199&doi=10.1016%2fj.eswa.2012.08.036&partnerID=40&md5=1e5eb499512799a0ceecc73f90773e34","Usually, vehicle applications need to use artificial intelligence techniques to implement control strategies able to deal with the noise in the signals provided by sensors, or with the impossibility of having full knowledge of the dynamics of a vehicle (engine state, wheel pressure, or occupants' weight). This work presents a cruise control system which is able to manage the pedals of a vehicle at low speeds. In this context, small changes in the vehicle or road conditions can occur unpredictably. To solve this problem, a method is proposed to allow the on-line evolution of a zero-order TSK fuzzy controller to adapt its behaviour to uncertain road or vehicle dynamics. Starting from a very simple or even empty configuration, the consequents of the rules are adapted in real time, while the membership functions used to codify the input variables are modified after a certain period of time. Extensive experimentation in both simulated and real vehicles showed the method to be both fast and precise, even when compared with a human driver. © 2012 Elsevier Ltd. All rights reserved.","Autonomous vehicles; Fuzzy control; Intelligent Transportation Systems; On-line learning; Speed control","Artificial intelligence techniques; Autonomous Vehicles; Control strategies; Engine state; Fuzzy controllers; Human drivers; Input variables; Intelligent transportation systems; Low speed; Online learning; Real time; Real vehicles; Road condition; Vehicle applications; Vehicle dynamics; Computer control; Dynamics; E-learning; Fuzzy control; Intelligent systems; Roads and streets; Speed control; Vehicles",Article,Scopus,2-s2.0-84870255199
"Hermans J., Peeters R.","Private yoking proofs: Attacks, models and new provable constructions",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,10.1007/978-3-642-36140-1-7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873386005&doi=10.1007%2f978-3-642-36140-1-7&partnerID=40&md5=cc36eead894dea58f46c3de75ba752f9","We present two attacks on the security of the private grouping proof by Batina et al. [1]. We introduce the first formal models for yoking proofs. One model incorporates the aspect time, ensuring that the grouping proofs were generated at a specific time. A more general variant only provides a proof that tags were together at some time. Based on these models we propose two new protocols to generate sound yoking proofs that can trivially be extended to multiple parties and that attain narrow-strong privacy. © 2013 Springer-Verlag Berlin Heidelberg.",,"Formal model; New protocol; Specific time; Yoking proof; Artificial intelligence; Radio frequency identification (RFID)",Conference Paper,Scopus,2-s2.0-84873386005
"Sánchez-Anguix V., Valero S., Julián V., Botti V., García-Fornes A.","Evolutionary-aided negotiation model for bilateral bargaining in Ambient Intelligence domains with complex utility functions",2013,"Information Sciences",14,10.1016/j.ins.2010.11.018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870065934&doi=10.1016%2fj.ins.2010.11.018&partnerID=40&md5=c898a45248c77a800f968f7c685fc1b1","Ambient Intelligence aims to offer personalized services and easier ways of interaction between people and systems. Since several users and systems may coexist in these environments, it is quite possible that entities with opposing preferences need to cooperate to reach their respective goals. Automated negotiation is pointed as one of the mechanisms that may provide a solution to this kind of problems. In this article, a multi-issue bilateral bargaining model for Ambient Intelligence domains is presented where it is assumed that agents have computational bounded resources and do not know their opponents' preferences. The main goal of this work is to provide negotiation models that obtain efficient agreements while maintaining the computational cost low. A niching genetic algorithm is used before the negotiation process to sample one's own utility function (self-sampling). During the negotiation process, genetic operators are applied over the opponent's and one's own offers in order to sample new offers that are interesting for both parties. Results show that the proposed model is capable of outperforming similarity heuristics which only sample before the negotiation process and of obtaining similar results to similarity heuristics which have access to all of the possible offers. © 2012 Elsevier Inc. All rights reserved.","Agreement technologies; Automated negotiation; Bilateral bargaining; Evolutionary computation; Multi-agent systems","Ambient intelligence; Automated negotiations; Bargaining model; Bilateral bargaining; Computational costs; Genetic operators; Multi agent system (MAS); Multi-issue; Negotiation models; Negotiation process; Niching genetic algorithm; Personalized service; Self-sampling; Utility functions; Artificial intelligence; Evolutionary algorithms; Software engineering; Multi agent systems",Article,Scopus,2-s2.0-84870065934
"Sharma A., Naidu M., Sargaonkar A.","Development of computer automated decision support system for surface water quality assessment",2013,"Computers and Geosciences",14,10.1016/j.cageo.2012.09.007,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870190553&doi=10.1016%2fj.cageo.2012.09.007&partnerID=40&md5=268c21bf2724fb53413f402dd5dcf5a0","The Overall Index of Pollution (OIP) is a single number that expresses the overall water quality by integrating measurements of 14 different physicochemical, toxicological, and bacteriological water quality parameters. It provides a simple and concise method for water quality classification as, 'Excellent', 'Acceptable', 'Slightly Polluted', 'Polluted', and 'Heavily Polluted'. OIP values range from 0 to 16. A high OIP value signals poor water quality, while a low value signals good water quality based on the classification scheme developed for India. In this paper, we present a computer-automated, user-friendly, and standalone Surface Water Quality Assessment Tool (SWQAT), which calculates OIP values and displays it on Google map. The software is developed in VB.Net and SQL database. The software application is demonstrated through water quality assessment of two rivers of India, namely Cauvery and Tungabhadra. OIP values are estimated at 10 sampling stations on the river Cauvery and at eight sampling stations on the river Tungabhadra. The Cauvery river OIP scores in the range 0.85-7.91 while for Tungabhadra river, it is in range 2.08 to 8.97. The results are useful to analyze the variations in the water quality of different sites at different times. SWQAT improves understanding of general water quality issues, communicates water quality status, and draws the need for and effectiveness of protection measures. © 2012 Elsevier Ltd.","Google earth plug-in; Software; Water quality assessment; Water quality index","Classification scheme; Google maps; Integrating measurement; Plug-ins; Protection measures; Quality classification; Sampling stations; Software applications; SQL database; Water quality assessments; Water quality indexes; Water quality issues; Water quality parameters; Artificial intelligence; Computer software; Decision support systems; Rivers; Water quality; River pollution; assessment method; automation; computer simulation; decision support system; GIS; Internet; parameterization; physicochemical property; software; surface water; toxicology; water quality; Cauvery River; India; Tungabhadra River",Article,Scopus,2-s2.0-84870190553
"Bedregal B.C., Santiago R.H.N.","Interval representations, Łukasiewicz implicators and Smets-Magrez axioms",2013,"Information Sciences",14,10.1016/j.ins.2012.09.022,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884209908&doi=10.1016%2fj.ins.2012.09.022&partnerID=40&md5=de6b8997815c4744b22fa9ae693ab278","The Smets-Magrez axiomatic is usually used to define the class of fuzzy continuous implications which are both S and R-implications (Łukasiewicz implications). Another approach is the construction of such class starting from a basic implication and applying automorphisms. Literature has shown that there is a harmony between those approaches, however in this paper we show that the extension of the Łukasiewicz implication defined on [0, 1] for interval values cannot be applied in a direct way. We show that the harmony between the Smets-Magrez axiomatic approach and the one that comes from the generation by automorphisms is not preserved when such extension is done. One of the main consequences lies on the fact that the automorphism approach induces the loss of R-implications from the resulting class of implicators. More precisely, we show that the interval version of such approaches produce two disjunct classes of implicators, meaning that, unlike the usual case, the choice of the respective approach is an important step. © 2012 Elsevier Inc. All rights reserved.","Automorphisms; Fuzzy logic; Interval representations; Interval-valued fuzzy logic; Smets-Magrez axioms; Łukasiewicz implicators","Artificial intelligence; Software engineering; Automorphisms; Implicators; Interval representations; Interval-valued fuzzy logic; Smets-Magrez axioms; Fuzzy logic",Article,Scopus,2-s2.0-84884209908
"Chou Y.-T., Liu C.-W., Wang Y.-J., Wu C.-C., Lin C.-C.","Development of a black start decision supporting system for isolated power systems",2013,"IEEE Transactions on Power Systems",14,10.1109/TPWRS.2013.2237792,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880916864&doi=10.1109%2fTPWRS.2013.2237792&partnerID=40&md5=3dc78b08d4a62fd30c2d4f177cb81ea6","Black start is the primary procedure deployed for rapid recovery from a complete blackout. For isolated power systems such as the Taiwan power system (TPS), a reliable and efficient black start procedure is more important than interconnected power systems. In this paper, a black start decision-supporting system (BSS) with an interactive graphical user interface (GUI) has been developed. BSS can rapidly generate optimal black start strategies according to the updated system data configuration and objective function; furthermore, BSS can automatically simulate the strategies and visualize the results. By applying the BSS to evaluate the black start strategies for TPS, the effectiveness of BSS has been demonstrated. The BSS has been utilized by Taiwan Power Company (TPC) for black start planning with comparison to traditional manual planning. With the aid of BSS, the dispatchers are equipped with more support and the restoration risk can be much alleviated. © 1969-2012 IEEE.","Black start; decision support system; graphical user interfaces (GUI); power systems","Black start; Data configuration; Decision supporting systems; Graphical user interfaces (GUI); Interactive graphical user interface; Isolated power system; Objective functions; Taiwan power companies; Artificial intelligence; Decision support systems; Electric power system interconnection; Standby power systems; Uninterruptible power systems; Graphical user interfaces",Article,Scopus,2-s2.0-84880916864
"Serbanuta T.F., Chen F., Roşu G.","Maximal causal models for sequentially consistent systems",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,10.1007/978-3-642-35632-2-16,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872863719&doi=10.1007%2f978-3-642-35632-2-16&partnerID=40&md5=c455cce77037e707d2a31fc765ece1d6","This paper shows that it is possible to build a maximal and sound causal model for concurrent computations from a given execution trace. It is sound, in the sense that any program which can generate a trace can also generate all traces in its causal model. It is maximal (among sound models), in the sense that by extending the causal model of an observed trace with a new trace, the model becomes unsound: there exists a program generating the original trace which cannot generate the newly introduced trace. Thus, the maximal sound model has the property that it comprises all traces which all programs that can generate the original trace can also generate. The existence of such a model is of great theoretical value as it can be used to prove the soundness of non-maximal, and thus smaller, causal models. © 2013 Springer-Verlag Berlin Heidelberg.",,"Causal model; Concurrent computation; Execution trace; Theoretical values; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84872863719
"Luo J., Li W., Qiu J., Wei D., Liu Y., Zhang Q.","Neural Basis of Scientific Innovation Induced by Heuristic Prototype",2013,"PLoS ONE",14,10.1371/journal.pone.0049231,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872854519&doi=10.1371%2fjournal.pone.0049231&partnerID=40&md5=20fb6b562de2307dadf16a3475a7b2db","A number of major inventions in history have been based on bionic imitation. Heuristics, by applying biological systems to the creation of artificial devices and machines, might be one of the most critical processes in scientific innovation. In particular, prototype heuristics propositions that innovation may engage automatic activation of a prototype such as a biological system to form novel associations between a prototype's function and problem-solving. We speculated that the cortical dissociation between the automatic activation and forming novel associations in innovation is critical point to heuristic creativity. In the present study, novel and old scientific innovations (NSI and OSI) were selected as experimental materials in using learning-testing paradigm to explore the neural basis of scientific innovation induced by heuristic prototype. College students were required to resolve NSI problems (to which they did not know the answers) and OSI problems (to which they knew the answers). From two fMRI experiments, our results showed that the subjects could resolve NSI when provided with heuristic prototypes. In Experiment 1, it was found that the lingual gyrus (LG; BA18) might be related to prototype heuristics in college students resolving NSI after learning a relative prototype. In Experiment 2, the LG (BA18) and precuneus (BA31) were significantly activated for NSI compared to OSI when college students learned all prototypes one day before the test. In addition, the mean beta-values of these brain regions of NSI were all correlated with the behavior accuracy of NSI. As our hypothesis indicated, the findings suggested that the LG might be involved in forming novel associations using heuristic information, while the precuneus might be involved in the automatic activation of heuristic prototype during scientific innovation. © 2013 Luo et al.",,"accuracy; adult; article; behavioral science; college student; controlled study; creativity; decision making; experimental test; female; functional dissociation; functional magnetic resonance imaging; heuristic prototype; human; human experiment; learning test; lingual gyrus; male; novel scientific innovation; old scientific innovation; precuneus; problem solving; scoring system; task performance; Adult; Artificial Intelligence; Behavior; Brain Mapping; Dentate Gyrus; Female; Gyrus Cinguli; Humans; Inventions; Learning; Magnetic Resonance Imaging; Male; Problem Solving; Thalamus",Article,Scopus,2-s2.0-84872854519
"Balbiani P., Van Ditmarsch H., Kudinov A.","Subset space logic with arbitrary announcements",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,10.1007/978-3-642-36039-8-21,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872803856&doi=10.1007%2f978-3-642-36039-8-21&partnerID=40&md5=22733d4e7dd162dc3e40cc74f2f311ce","In this paper we introduce public announcements to Subset Space Logic (SSL). In order to do this we have to change the original semantics for SSL a little and consider a weaker version of SSL without the cross axiom. We present an axiomatization, prove completeness and show that this logic is PSPACE-complete. Finally, we add the arbitrary announcement modality which expresses ""true after any announcement"", prove several semantic results, and show completeness for a Hilbert-style axiomatization of this logic. © 2013 Springer-Verlag Berlin Heidelberg.",,"Axiomatization; Artificial intelligence; Semantics",Conference Paper,Scopus,2-s2.0-84872803856
"Yu J., Wang C.","A max-min ant colony system for assembly sequence planning",2013,"International Journal of Advanced Manufacturing Technology",14,10.1007/s00170-012-4695-x,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888641475&doi=10.1007%2fs00170-012-4695-x&partnerID=40&md5=e5280c762e11a3e76d909abfe8d732bb","An improved ant colony optimization (ACO)-based assembly sequence planning (ASP) method for complex products that combines the advantages of ant colony system (ACS) and max-min ant system (MMAS) and integrates some optimization measures is proposed. The optimization criteria, assembly information models, and components number in case study that reported in the literatures of ACO-based ASP during the past 10 years are reviewed and compared. To reduce tedious manual input of parameters and identify the best sequence easily, the optimization criteria such as directionality, parallelism, continuity, stability, and auxiliary stroke are automatically quantified and integrated into the multi-objective heuristic and fitness functions. On the precondition of geometric feasibility based on interference matrix, several strategies of ACS and MMAS are combined in a max-min ant colony system (MMACS) to improve the convergence speed and sequence quality. Several optimization measures are integrated into the system, among which the performance appraisal method transfers the computing resource from the worst ant to the better one, and the group method makes up the deficiency of solely depending on heuristic searching for all parallel parts in each group. An assembly planning system ""AutoAssem"" is developed based on Siemens NX, and the effectiveness of each optimization measure is testified through case study. Compared with the methods of priority rules screening, genetic algorithm, and particle swarm optimization, MMACS is verified to have superiority in efficiency and sequence performance. © 2013 Springer-Verlag London.","Ant colony optimization algorithm; Assembly sequence planning; Extended interference matrix; Max-min ant system","Ant Colony Optimization algorithms; Assembly information model; Assembly sequence planning; Improved ant colony optimization; Interference matrix; Max min ant system(MMAS); Max-Min Ant System; Optimization measures; Artificial intelligence; Assembly; Heuristic algorithms; Multiobjective optimization; Particle swarm optimization (PSO); Stability criteria; Ant colony optimization",Article,Scopus,2-s2.0-84888641475
"Gao L., Hailu A.","Identifying preferred management options: An integrated agent-based recreational fishing simulation model with an AHP-TOPSIS evaluation method",2013,"Ecological Modelling",14,10.1016/j.ecolmodel.2012.07.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872131505&doi=10.1016%2fj.ecolmodel.2012.07.002&partnerID=40&md5=9db470ddbc7fed1a522a3d2a4a0bc096","In the management of recreational fishing, multiple biophysical and socio-economic objectives need to be considered. The identification of best management options is technically challenging because of the lack of scientific tools to inform resource managers of future responses/impacts of these different options. This paper proposes a decision support system which aims at greatly improving stakeholder dialogue and decision making. The system consists of two main components: an integrated agent-based model for simulating recreational fishing behavior and reef ecosystem dynamics; and an evaluation model based on the analytic hierarchy process (AHP) together with a technique for order preference by similarity to ideal solution (TOPSIS). The evaluation component is responsible for assessing alternative strategies based on the simulation outputs generated by the former. We apply the proposed system to the assessment of management strategies for recreational fishing in the Ningaloo Marine Park, an iconic coral reef system in Western Australia. A set of management strategies, including a ""business-as-usual"" strategy and six alternative site closure strategies are assessed using the proposed solution. The site closure strategies evaluated vary in length and number of popular sites closed. The evaluation results illustrate the usefulness of the proposed system in tackling complex management choices. We also perform a sensitivity analysis on the stakeholder or decision maker outcome preference weights and provide measures for assessing the sensitivity of strategy ranking outcomes to changes in these weights. © 2012 Elsevier B.V.","Agent-based model; AHP; Decision support system; Integrated economic-biophysical modeling; Multiple criteria decision making; TOPSIS","Agent based; Agent-based model; AHP; Business-as-usual; Complex management; Coral reef systems; Decision makers; Ecosystem dynamics; Evaluation models; Evaluation results; Future response; Management options; Management strategies; Marine park; Multiple criteria decision making; Resource managers; Scientific tool; Simulation model; Site closure; Socio-economics; Stakeholder dialogue; Technique for order preference by similarity to ideal solutions; TOPSIS; Western Australia; Artificial intelligence; Computational methods; Computer simulation; Decision making; Decision support systems; Hierarchical systems; Reefs; Fisheries; best management practice; decision making; decision support system; fishing effort; hierarchical system; numerical model; recreational management; reef; resource management; simulation; stakeholder; tracking; Australia; Ningaloo Marine Park; Western Australia; Anthozoa",Article,Scopus,2-s2.0-84872131505
"Samanta S., Acharjee S., Mukherjee A., Das D., Dey N.","Ant Weight Lifting algorithm for image segmentation",2013,"2013 IEEE International Conference on Computational Intelligence and Computing Research, IEEE ICCIC 2013",14,10.1109/ICCIC.2013.6724160,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894282976&doi=10.1109%2fICCIC.2013.6724160&partnerID=40&md5=c91fe028a09595b7c21a61157b5a2c4a","Image segmentation forms a quintessential concept and is one of the most in-demand arenas of research in the field of image processing. Throughout the years several techniques like k-means clustering, watershed segmentation and quad tree segmentation have been devised to properly segment an image into well-defined classes. Segmentation techniques can be broadly classified as thresholding techniques, edge detection techniques, clustering, region based and matching. Image segmentation may be the ultimate output desired and may also be a penultimate step in the algorithm. In either case it becomes essential to get an accurately segmented image which is often not the case with the existing algorithms since each algorithm has its own drawback. In our paper we have proposed a novel segmentation technique that is bio-inspired from the behavioral nature of ants and is hence called the Ant Weight Lifting (AWL) segmentation algorithm. Our segmentation algorithm has generated optimum results on a wide range of test cases imposed by us with a high correlation factor between the original and segmented image and also an added perk in the form of a low time complexity. © 2013 IEEE.","Ant Weight Lifting Algorithm (AWL); Edge-Based Segmentation; Image Segmentation; Quadtree Segmentation; Thresholding; Watershed Segmentation","Algorithms; Artificial intelligence; Edge detection; Image matching; Research; Edge-based; Quadtree segmentation; Thresholding; Watershed segmentation; Weight lifting; Image segmentation",Conference Paper,Scopus,2-s2.0-84894282976
"Augusto J.C., Hornos M.J.","Software simulation and verification to increase the reliability of Intelligent Environments",2013,"Advances in Engineering Software",14,10.1016/j.advengsoft.2012.12.004,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873679248&doi=10.1016%2fj.advengsoft.2012.12.004&partnerID=40&md5=2df9bfe6e973f989a0bda3966a1a9023","This paper explains how the Spin model checker can be used to guide and inform the development of more reliable Intelligent Environments. The idea is to present a methodological guide which provides strategies and suggestions on how to model, simulate and verify these types of systems, as well as to facilitate the use of well-known tools like Spin in the development of Intelligent Environments. These tools, which have been developed by the Software Engineering community, have proven their usefulness for improving the quality of complex software systems, even in the industry field. However, researchers and developers in the area of Intelligent Environments do not usually make use of these tools. Our aim is therefore to encourage their use by colleagues working in this area to increase the reliability of these complex systems, which integrate aspects and elements of networks, sensors/actuators, ubiquitous/pervasive computing, human-computer interaction and artificial intelligence, among other related areas. © 2013 Elsevier Ltd. All rights reserved.","Intelligent Environments; Model checking; Reliability; Simulation; Spin; Verification","Artificial intelligence; Intelligent agents; Model checking; Reliability; Software engineering; Verification; Complex software systems; Engineering community; Intelligent environment; Simulation; Software simulation; Spin; Spin models; Ubiquitous/pervasive computing; Software reliability",Article,Scopus,2-s2.0-84873679248
"Echezarreta-López M.M., Landin M.","Using machine learning for improving knowledge on antibacterial effect of bioactive glass",2013,"International Journal of Pharmaceutics",14,10.1016/j.ijpharm.2013.06.036,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884151691&doi=10.1016%2fj.ijpharm.2013.06.036&partnerID=40&md5=60813eb40d6f1b60e84a4fc5f6bbcee5","The aim of this work was to find relationships between critical bioactive glass characteristics and their antibacterial behaviour using an artificial intelligence tool. A large dataset including ingredients and process variables of the bioactive glasses production, bacterial characteristics and microbiological experimental conditions was generated from literature and analyzed by neurofuzzy logic technology. Our findings allow an explanation on the variability in antibacterial behaviour found by different authors and to obtain general conclusions about critical parameters of bioactive glasses to be considered in order to achieve activity against some of the most common skin and implant surgery pathogens. © 2013 Elsevier B.V. All rights reserved.","Antibacterial behaviour; Artificial intelligence; Bioactive glass; Machine learning; Modelling; Neurofuzzy logic","glass; antibacterial activity; article; artificial intelligence; controlled study; machine learning; nonhuman; priority journal",Article,Scopus,2-s2.0-84884151691
"Singh K.P., Gupta S., Ojha P., Rai P.","Predicting adsorptive removal of chlorophenol from aqueous solution using artificial intelligence based modeling approaches",2013,"Environmental Science and Pollution Research",14,10.1007/s11356-012-1102-y,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875551049&doi=10.1007%2fs11356-012-1102-y&partnerID=40&md5=4feffb13f01f5fbd09c6cea6c2491182","The research aims to develop artificial intelligence (AI)-based model to predict the adsorptive removal of 2-chlorophenol (CP) in aqueous solution by coconut shell carbon (CSC) using four operational variables (pH of solution, adsorbate concentration, temperature, and contact time), and to investigate their effects on the adsorption process. Accordingly, based on a factorial design, 640 batch experiments were conducted. Nonlinearities in experimental data were checked using Brock-Dechert-Scheimkman (BDS) statistics. Five nonlinear models were constructed to predict the adsorptive removal of CP in aqueous solution by CSC using four variables as input. Performances of the constructed models were evaluated and compared using statistical criteria. BDS statistics revealed strong nonlinearity in experimental data. Performance of all the models constructed here was satisfactory. Radial basis function network (RBFN) and multilayer perceptron network (MLPN) models performed better than generalized regression neural network, support vector machines, and gene expression programming models. Sensitivity analysis revealed that the contact time had highest effect on adsorption followed by the solution pH, temperature, and CP concentration. The study concluded that all the models constructed here were capable of capturing the nonlinearity in data. A better generalization and predictive performance of RBFN and MLPN models suggested that these can be used to predict the adsorption of CP in aqueous solution using CSC. © 2012 Springer-Verlag.","Adsorptive removal efficiency; Artificial intelligence; Artificial neural networks; Gene expression programming; Nonlinear models; Support vector machines","2 chlorophenol; 2-chlorophenol; charcoal; chlorophenol; adsorption; aqueous solution; artificial intelligence; artificial neural network; chlorophenol; experimental study; fruit; genetic algorithm; nonlinearity; numerical model; performance assessment; pH; pollutant removal; prediction; regression analysis; shell; adsorption; article; artificial intelligence; artificial neural network; chemical model; chemistry; coconut; evaluation; fruit; kinetics; methodology; nonlinear system; pH; statistical model; temperature; water management; water pollutant; Adsorption; Artificial Intelligence; Charcoal; Chlorophenols; Cocos; Fruit; Hydrogen-Ion Concentration; Kinetics; Models, Chemical; Models, Statistical; Neural Networks (Computer); Nonlinear Dynamics; Temperature; Water Pollutants, Chemical; Water Purification",Article,Scopus,2-s2.0-84875551049
"Trapeznikov K., Saligrama V.","Supervised sequential classification under budget constraints",2013,"Journal of Machine Learning Research",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954200708&partnerID=40&md5=24ca559df158853173e1b8b43e7a48fa","In this paper we develop a framework for a sequential decision making under budget constraints for multi-class classification. In many classification systems, such as medical diagnosis and homeland security, sequential decisions are often warranted. For each instance, a sensor is first chosen for acquiring measurements and then based on the available information one decides (rejects) to seek more measurements from a new sensor/ modality or to terminate by classifying the example based on the available information. Different sensors have varying costs for acquisition, and these costs account for delay, throughput or monetary value. Consequently, we seek methods for maximizing performance of the system subject to budget constraints. We formulate a multi-stage multi-class empirical risk objective and learn sequential decision functions from training data. We show that reject decision at each stage can be posed as supervised binary classification. We derive bounds for the VC dimension of the multi-stage system to quantify the generalization error. We compare our approach to alternative strategies on several multi-class real world datasets. Copyright 2013 by the authors.",,"Artificial intelligence; Budget control; Computer aided diagnosis; Decision making; Diagnosis; Binary classification; Budget constraint; Classification system; Generalization Error; Multi-class classification; Real-world datasets; Sequential decision making; Sequential decisions; Classification (of information)",Conference Paper,Scopus,2-s2.0-84954200708
"Poursalehi N., Zolfaghari A., Minuchehr A.","PWR loading pattern optimization using Harmony Search algorithm",2013,"Annals of Nuclear Energy",14,10.1016/j.anucene.2012.06.037,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870197093&doi=10.1016%2fj.anucene.2012.06.037&partnerID=40&md5=94ac475936b1b5f326246bdcb276ad35","In this paper a core reloading technique using Harmony Search, HS, is presented in the context of finding an optimal configuration of fuel assemblies, FA, in pressurized water reactors. To implement and evaluate the proposed technique a Harmony Search along Nodal Expansion Code for 2-D geometry, HSNEC2D, is developed to obtain nearly optimal arrangement of fuel assemblies in PWR cores. This code consists of two sections including Harmony Search algorithm and Nodal Expansion modules using fourth degree flux expansion which solves two dimensional-multi group diffusion equations with one node per fuel assembly. Two optimization test problems are investigated to demonstrate the HS algorithm capability in converging to near optimal loading pattern in the fuel management field and other subjects. Results, convergence rate and reliability of the method are quite promising and show the HS algorithm performs very well and is comparable to other competitive algorithms such as Genetic Algorithm and Particle Swarm Intelligence. Furthermore, implementation of nodal expansion technique along HS causes considerable reduction of computational time to process and analysis optimization in the core fuel management problems. © 2012 Elsevier Ltd. All rights reserved.","Fitness function; Harmony Search algorithm; Loading pattern optimization; Nodal expansion method","Artificial intelligence; Fuels; Learning algorithms; Optimal systems; Pressurized water reactors; Competitive algorithms; Diffusion equations; Fitness functions; Harmony search algorithms; Loading patterns; Nodal expansion method; Optimal arrangement; Optimization test problems; Optimization",Article,Scopus,2-s2.0-84870197093
"Říha K., Mašek J., Burget R., Beneš R., Závodná E.","Novel Method for Localization of Common Carotid Artery Transverse Section in Ultrasound Images Using Modified Viola-Jones Detector",2013,"Ultrasound in Medicine and Biology",14,10.1016/j.ultrasmedbio.2013.04.013,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883453413&doi=10.1016%2fj.ultrasmedbio.2013.04.013&partnerID=40&md5=2f6c33f8c39d71373e3195b865fd63c1","This article describes a novel method for highly accurate and effective localization of the transverse section of the carotis comunis artery in ultrasound images. The method has a high success rate, approximately 97%. Unlike analytical methods based on geometric descriptions of the object sought, the method proposed here can cover a large area of shape variation of the artery under study, which normally occurs during examinations as a result of the pressure on the examined tissue, tilt of the probe, setup of the sonographic device, and other factors. This method shows great promise in automating the process of determining circulatory system parameters in the non-invasive clinical diagnostics of cardiovascular diseases. The method employs a Viola-Jones detector that has been specially adapted for efficient detection of transverse sections of the carotid artery. This algorithm is trained on a set of labeled images using the AdaBoost algorithm, Haar-like features and the Matthews coefficient. The training algorithm of the artery detector was modified using evolutionary algorithms. The method for training a cascade of classifiers achieves on a small number of positive and negative training data samples (about 500images) a high success rate in a computational time that allows implementation of the detector in real time. Testing was performed on images of different patients for whom different ultrasonic instruments were used under different conditions (settings) so that the algorithm developed is applicable in general radiologic practice. © 2013 World Federation for Ultrasound in Medicine & Biology.","B-mode ultrasound; Carotid artery; Evolutionary algorithms; Image processing; Transverse section; Viola-Jones detector","Adaptive boosting; Algorithms; Cardiovascular system; Classification (of information); Evolutionary algorithms; Image processing; Ultrasonic applications; Ultrasonics; Cardio-vascular disease; Carotid artery; Cascade of classifiers; Clinical diagnostics; Common carotid artery; Geometric description; Transverse section; Viola jones; Detectors; AdaBoost algorithm; adolescent; adult; aged; algorithm; article; automation; cardiovascular disease; cardiovascular parameters; carotid atherosclerosis; circulatory system parameters; classifier; common carotid artery; controlled study; diagnostic accuracy; diagnostic test accuracy study; female; Haar like feature; human; machine learning; male; Matthews coefficient; non invasive procedure; pressure; priority journal; real time echography; sensitivity analysis; tissues; ultrasound scanner; Viola Jones detector; B-mode ultrasound; Carotid artery; Evolutionary algorithms; Image processing; Transverse section; Viola-Jones detector; Algorithms; Artificial Intelligence; Carotid Arteries; Computer Systems; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Software; Ultrasonography",Article,Scopus,2-s2.0-84883453413
"Palacky P., Hudecek P., Havel A.","Real-time estimation of induction motor parameters based on the genetic algorithm",2013,"Advances in Intelligent Systems and Computing",14,10.1007/978-3-642-33018-6_41,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868118217&doi=10.1007%2f978-3-642-33018-6_41&partnerID=40&md5=7fc6f1ca09c2b2b764afebbcf7b216e7","This article shows one of many ways how to identify the parameters of the IM in real time. There is used the theory of genetic algorithms for IM parameters identification. The introduction describes why the problem is discussed. Next chapters show induction motor's dynamic model and the principle and way how to implement the IM's parameters identification. Theory of used genetic algorithm and experimental results are demonstrated in the end of this article. The conclusion describes the potential use of this method and discusses further development in the field of real time estimation of induction motor's parameters. © 2013 Springer-Verlag Berlin Heidelberg.",,"Artificial intelligence; Genetic algorithms; Identification (control systems); Induction motors; Soft computing; Induction motor parameters; Parameters identification; Real time; Real-time estimation; Parameter estimation",Conference Paper,Scopus,2-s2.0-84868118217
"Heras S., Jordán J., Botti V., Julián V.","Argue to agree: A case-based argumentation approach",2013,"International Journal of Approximate Reasoning",14,10.1016/j.ijar.2012.06.005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870066231&doi=10.1016%2fj.ijar.2012.06.005&partnerID=40&md5=c7286c9fe5aa9b659287d6e2deab6a7a","The capability of reaching agreements is a necessary feature that large computer systems where agents interoperate must include. In these systems, agents represent self-motivated entities that have a social context, including dependency relations among them, and different preferences and beliefs. Without agreement there is no cooperation and thus, complex tasks which require the interaction of agents with different points of view cannot be performed. In this work, we propose a case-based argumentation approach for Multi-Agent Systems where agents reach agreements by arguing and improve their argumentation skills from experience. A set of knowledge resources and a reasoning process that agents can use to manage their positions and arguments are presented. These elements are implemented and validated in a customer support application. © 2012 Elsevier Inc. All rights reserved.","Agreement technologies; Argumentation; Case-based reasoning; Multi-agent systems","Argumentation; Complex task; Customer support; Dependency relation; Knowledge resource; Multi agent system (MAS); Reasoning process; Social context; Artificial intelligence; Software engineering; Multi agent systems",Article,Scopus,2-s2.0-84870066231
"Lombaert H., Sporring J., Siddiqi K.","Diffeomorphic spectral matching of cortical surfaces.",2013,"Information processing in medical imaging : proceedings of the ... conference",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901282121&partnerID=40&md5=7dc3e40bf80e2473afdd12bb38993d33","Accurate matching of cortical surfaces is necessary in many neuroscience applications. In this context diffeomorphisms are often sought, because they facilitate further statistical analysis and atlas building. Present methods for computing diffeomorphisms are based on optimizing flows or on inflating surfaces to a common template, but they are often computationally expensive. It typically takes several hours on a conventional desktop computer to match a single pair of cortical surfaces having a few hundred thousand vertices. We propose a very fast alternative based on an application of spectral graph theory on a novel association graph. Our symmetric approach can generate a diffeomorphic correspondence map within a few minutes on high-resolution meshes while avoiding the sign and multiplicity ambiguities of conventional spectral matching methods. The eigenfunctions are shared between surfaces and provide a smooth parameterization of surfaces. These properties are exploited to compute differentials on highly folded cortical surfaces. Diffeomorphisms can thus be verified and invalid surface folding detected. Our method is demonstrated to attain a vertex accuracy that is at least as good as that of FreeSurfer and Spherical Demons but in only a fraction of their processing time. As a practical experiment, we construct an unbiased atlas of cortical surfaces with a speed several orders of magnitude faster than current methods.",,"algorithm; article; artificial intelligence; automated pattern recognition; brain cortex; computer assisted diagnosis; histology; human; image enhancement; image subtraction; methodology; nuclear magnetic resonance imaging; reproducibility; sensitivity and specificity; three dimensional imaging; Algorithms; Artificial Intelligence; Cerebral Cortex; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-84901282121
"Yepes A.J., Mork J.G., Demner-Fushman D., Aronson A.R.","Comparison and combination of several MeSH indexing approaches.",2013,"AMIA ... Annual Symposium proceedings / AMIA Symposium. AMIA Symposium",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901267295&partnerID=40&md5=25fca99dcb8223c91ef6c92fba35803a","MeSH indexing of MEDLINE is becoming a more difficult task for the group of highly qualified indexing staff at the US National Library of Medicine, due to the large yearly growth of MEDLINE and the increasing size of MeSH. Since 2002, this task has been assisted by the Medical Text Indexer or MTI program. We extend previous machine learning analysis by adding a more diverse set of MeSH headings targeting examples where MTI has been shown to perform poorly. Machine learning algorithms exceed MTI's performance on MeSH headings that are used very frequently and headings for which the indexing frequency is very low. We find that when we combine the MTI suggestions and the prediction of the learning algorithms, the performance improves compared to any single method for most of the evaluated MeSH headings.",,"algorithm; article; artificial intelligence; comparative study; documentation; Medical Subject Headings; Medline; methodology; natural language processing; Abstracting and Indexing as Topic; Algorithms; Artificial Intelligence; Medical Subject Headings; MEDLINE; Natural Language Processing",Article,Scopus,2-s2.0-84901267295
"Novak D., Omlin X., Leins-Hess R., Riener R.","Predicting targets of human reaching motions using different sensing technologies",2013,"IEEE Transactions on Biomedical Engineering",14,10.1109/TBME.2013.2262455,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882958228&doi=10.1109%2fTBME.2013.2262455&partnerID=40&md5=fd1f87dcb535113b0600951abdca226a","Rapid recognition of voluntary motions is crucial in human-computer interaction, but few studies compare the predictive abilities of different sensing technologies. This paper thus compares performances of different technologies when predicting targets of human reaching motions: electroencephalography (EEG), electrooculography, camera-based eye tracking, electromyography (EMG), hand position, and the user's preferences. Supervised machine learning is used to make predictions at different points in time (before and during limb motion) with each individual sensing modality. Different modalities are then combined using an algorithm that takes into account the different times at which modalities provide useful information. Results show that EEG can make predictions before limb motion onset, but requires subject-specific training and exhibits decreased performance as the number of possible targets increases. EMG and hand position give high accuracy, but only once the motion has begun. Eye tracking is robust and exhibits high accuracy at the very onset of limb motion. Several advantages of combining different modalities are also shown, including advantages of combining measurements with contextual data. Finally, some recommendations are given for sensing modalities with regard to different criteria and applications. The information could aid human-computer interaction designers in selecting and evaluating appropriate equipment for their applications. © 1964-2012 IEEE.","Human-computer interaction; intention detection; machine learning; physiology; sensor fusion","Electromyography; Electrophysiology; Forecasting; Learning systems; Physiology; Intention detection; Predictive abilities; Sensing modalities; Sensing technology; Sensor fusion; Subject-specific; Supervised machine learning; User's preferences; Human computer interaction; accuracy; adult; article; coffee; drinking behavior; electroencephalogram; electroencephalography; electromyogram; electromyography; electrooculogram; electrooculography; eye movement; eye tracking; female; hand; hand movement; human; human computer interaction; human experiment; jaw movement; machine learning; male; measurement error; motion; muscle contraction; muscle fatigue; normal human; performance; questionnaire; smoking; arm; artificial intelligence; behavior; biomedical engineering; electrodiagnosis; man machine interaction; movement (physiology); physiology; procedures; reproducibility; signal processing; task performance; Adult; Artificial Intelligence; Biomedical Engineering; Electrodiagnosis; Female; Humans; Intention; Male; Man-Machine Systems; Movement; Reproducibility of Results; Signal Processing, Computer-Assisted; Task Performance and Analysis; Upper Extremity",Article,Scopus,2-s2.0-84882958228
"Brosch T., Tam R., Initiative for the Alzheimers Disease Neuroimaging","Manifold learning of brain MRIs by deep learning.",2013,"Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897570416&partnerID=40&md5=d8429a8a10182b6747c452862a750a59","Manifold learning of medical images plays a potentially important role for modeling anatomical variability within a population with pplications that include segmentation, registration, and prediction of clinical parameters. This paper describes a novel method for learning the manifold of 3D brain images that, unlike most existing manifold learning methods, does not require the manifold space to be locally linear, and does not require a predefined similarity measure or a prebuilt proximity graph. Our manifold learning method is based on deep learning, a machine learning approach that uses layered networks (called deep belief networks, or DBNs) and has received much attention recently in the computer vision field due to their success in object recognition tasks. DBNs have traditionally been too computationally expensive for application to 3D images due to the large number of trainable parameters. Our primary contributions are (1) a much more computationally efficient training method for DBNs that makes training on 3D medical images with a resolution of up to 128 x 128 x 128 practical, and (2) the demonstration that DBNs can learn a low-dimensional manifold of brain volumes that detects modes of variations that correlate to demographic and disease parameters.",,"algorithm; article; artificial intelligence; automated pattern recognition; brain; brain disease; computer assisted diagnosis; human; image enhancement; information retrieval; methodology; nuclear magnetic resonance imaging; pathology; reproducibility; sensitivity and specificity; three dimensional imaging; Algorithms; Artificial Intelligence; Brain; Brain Diseases; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Information Storage and Retrieval; Magnetic Resonance Imaging; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84897570416
"Hassas S.","Using swarm intelligence for dynamic Web content organizing",2013,"2003 IEEE Swarm Intelligence Symposium, SIS 2003 - Proceedings",14,10.1109/SIS.2003.1202242,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942120586&doi=10.1109%2fSIS.2003.1202242&partnerID=40&md5=96ea1221f6daaf6b37e39af6b41c8dc2","The World Wide Web contains a huge amount of unstructured, distributed, multi-media data. This content provides a great potential source for knowledge acquisition that needs to be filtered, organized, and maintained in order to permit an efficient use. The wide distribution of the Web, its openness and high dynamics make any tentative of content organization or maintaining, a task very hard to achieve. The WWW is thus, a complex system, for which we have to imagine mechanisms of content maintaining, filtering and organizing, that are able to deal with its content evolving, dynamics and distribution. Integrating mechanisms of self-organization of the Web content is an attractive perspective, to match with these requirements. We present in this paper a new approach, inspired by social insects, to organize dynamically the Web content. Our approach combines foraging behavior and collective sorting behavior, and uses the stigmergy mechanism. We present in this paper some experiments and results that show its effectiveness. © 2003 IEEE.","Chemicals; Filtering; Insects; Knowledge acquisition; Organizing; Particle swarm optimization; Sorting; Web sites; World Wide Web","Artificial intelligence; Chemicals; Filtration; Knowledge acquisition; Particle swarm optimization (PSO); Screening; Social networking (online); Sorting; Websites; Collective sorting; Dynamic web content; Foraging behaviors; Insects; Organizing; Potential sources; Self organizations; Swarm Intelligence; World Wide Web",Conference Paper,Scopus,2-s2.0-84942120586
"Faria B.M., Reis L.P., Lau N., Soares J.C., Vasconcelos S.","Patient classification and automatic configuration of an intelligent wheelchair",2013,"Communications in Computer and Information Science",14,10.1007/978-3-642-36907-0_18,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882280731&doi=10.1007%2f978-3-642-36907-0_18&partnerID=40&md5=13252caffcb1653518ee6b9b56f11e40","The access to instruments that allow higher autonomy to people is increasing and the scientific community is giving special attention on designing and developing such systems. Intelligent Wheelchairs (IW) are an example of how the knowledge on robotics and artificial intelligence may be applied to this field. IWs can have different interfaces and multimodal interfaces enabling several inputs such as head movements, joystick, facial expressions and voice commands. This paper describes the foundations for creating a simple procedure for extracting user profiles, which can be used to adequately select the best IW command mode for each user. The methodology consists on an interactive wizard composed by a flexible set of simple tasks presented to the user, and a method for extracting and analyzing the user's execution of those tasks. The results showed that it is possible to extract simple user profiles, using the proposed method. © Springer-Verlag Berlin Heidelberg 2013.","Classification; Intelligent wheelchair; Knowledge discovery; Patient","Artificial intelligence; Classification (of information); Data mining; Automatic configuration; Facial Expressions; Head movements; Intelligent wheelchair; Multi-modal interfaces; Patient; Scientific community; Voice command; Intelligent robots",Conference Paper,Scopus,2-s2.0-84882280731
"Jurrus E., Watanabe S., Giuly R.J., Paiva A.R.C., Ellisman M.H., Jorgensen E.M., Tasdizen T.","Semi-automated neuron boundary detection and nonbranching process segmentation in electron microscopy images",2013,"Neuroinformatics",14,10.1007/s12021-012-9149-y,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872491808&doi=10.1007%2fs12021-012-9149-y&partnerID=40&md5=ed5364d20530f6930558f989ae36b54c","Neuroscientists are developing new imaging techniques and generating large volumes of data in an effort to understand the complex structure of the nervous system. The complexity and size of this data makes human interpretation a labor-intensive task. To aid in the analysis, new segmentation techniques for identifying neurons in these feature rich datasets are required. This paper presents a method for neuron boundary detection and nonbranching process segmentation in electron microscopy images and visualizing them in three dimensions. It combines both automated segmentation techniques with a graphical user interface for correction of mistakes in the automated process. The automated process first uses machine learning and image processing techniques to identify neuron membranes that deliniate the cells in each two-dimensional section. To segment nonbranching processes, the cell regions in each two-dimensional section are connected in 3D using correlation of regions between sections. The combination of this method with a graphical user interface specially designed for this purpose, enables users to quickly segment cellular processes in large volumes. © 2012 Springer Science+Business Media, LLC.","Artificial neural networks; Connectomics; Contour completion; Filter bank; Machine learning; Membrane detection; Neural circuit reconstruction","article; artificial intelligence; artificial neural network; automated pattern recognition; computer assisted diagnosis; computer interface; connectome; human; image processing; methodology; nerve cell; transmission electron microscopy; ultrastructure; Artificial Intelligence; Connectome; Humans; Image Interpretation, Computer-Assisted; Image Processing, Computer-Assisted; Microscopy, Electron, Transmission; Neural Networks (Computer); Neurons; Pattern Recognition, Automated; User-Computer Interface",Article,Scopus,2-s2.0-84872491808
"Gosztolya G., Busa-Feket R., Tóth L.","Detecting autism, emotions and social signals using AdaBoost",2013,"Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906276482&partnerID=40&md5=0a016565fb3e0fa4b8791a4bb68d6209","In the area of speech technology, tasks that involve the extraction of non-lingustic information have been receiving more attention recently. The Computational Paralinguistics Challenge (ComParE 2013) sought to develop techniques to efficiently detect a number of paralinguistic events, including the detection of non-linguistic events (laughter and fillers) in speech recordings as well as categorizing whole (albeit short) recordings by speaker emotion, conflict or the presence of development disorders (autism). We treated these sub-challenges as general classification tasks and applied the general-purpose machine learning meta-algorithm, AdaBoost.MH, and its recently proposed variant, AdaBoost.MH.BA, to them. The results show that these new algorithms convincingly outperform baseline SVM scores. Copyright © 2013 ISCA.","Adaboost.MH; Adaboost.MH.BA; Emotion detection; Machine learning; Speech recognition; Speech technology","Adaptive boosting; Artificial intelligence; Classification (of information); Diseases; Learning systems; Linguistics; Adaboost.MH; Adaboost.MH.BA; Classification tasks; Development disorders; Emotion detection; Paralinguistics; Speech recording; Speech technology; Speech recognition",Conference Paper,Scopus,2-s2.0-84906276482
"Miao S., Zhang F., Li S., Mu Y.","On security of a certificateless signcryption scheme",2013,"Information Sciences",14,10.1016/j.ins.2011.11.045,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875446221&doi=10.1016%2fj.ins.2011.11.045&partnerID=40&md5=31d08b0f18a8c7d9fc6b577ad21185a1","It would be interesting if a signcryption scheme in the standard model could be made certificateless. One of the interesting attempts is due to Liu et al. [Z. Liu, Y. Hu, X. Zhang, H. Ma, Certificateless signcryption scheme in the standard model, Information Sciences 180 (3) (2010) 452-464]. In this paper, we provide a cryptanalysis on this scheme by depicting two kinds of subtle public key replacement attacks against it. Our analysis reveals that it does not meet the basic requirements of confidentiality and non-repudiation. © 2013 Elsevier Inc. All rights reserved.","Certificateless cryptography; Confidentiality; Non-repudiation; Public key replacement attack; Signcryption","Certificateless cryptographies; Confidentiality; Non-repudiation; Replacement attacks; Signcryption; Artificial intelligence; Software engineering; Public key cryptography",Article,Scopus,2-s2.0-84875446221
"Karlsson I., Zhao J., Asker L., Boström H.","Predicting adverse drug events by analyzing electronic patient records",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,10.1007/978-3-642-38326-7_19,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887287057&doi=10.1007%2f978-3-642-38326-7_19&partnerID=40&md5=4b87ff360b2041df6ea636bd93516728","Diagnosis codes for adverse drug events (ADEs) are sometimes missing from electronic patient records (EPRs). This may not only affect patient safety in the worst case, but also the number of reported ADEs, resulting in incorrect risk estimates of prescribed drugs. Large databases of electronic patient records (EPRs) are potentially valuable sources of information to support the identification of ADEs. This study investigates the use of machine learning for predicting one specific ADE based on information extracted from EPRs, including age, gender, diagnoses and drugs. Several predictive models are developed and evaluated using different learning algorithms and feature sets. The highest observed AUC is 0.87, obtained by the random forest algorithm. The resulting model can be used for screening EPRs that are not, but possibly should be, assigned a diagnosis code for the ADE under consideration. Preliminary results from using the model are presented. © 2013 Springer-Verlag.","adverse drug events; electronic patient records; machine learning","Artificial intelligence; Decision trees; Learning algorithms; Learning systems; adverse drug events; Electronic patient record; Large database; Patient safety; Predictive models; Random forest algorithm; Risk estimates; Sources of informations; Diagnosis",Conference Paper,Scopus,2-s2.0-84887287057
"Ramanathan V., Wechsler H.","Phishing detection and impersonated entity discovery using Conditional Random Field and Latent Dirichlet Allocation",2013,"Computers and Security",14,10.1016/j.cose.2012.12.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891903388&doi=10.1016%2fj.cose.2012.12.002&partnerID=40&md5=a4e4e6ef753262bde3226d87c6f42b8a","Phishing is an attempt to steal users' personal and financial information such as passwords, social security and credit card numbers, via electronic communication such as e-mail and other messaging services. Attackers pretend to be froma legitimate organization and direct users to a fake website that resembles a legitimate website, which is then used to collect users' personal information. In this paper, we propose a novel methodology to detect phishing attacks and to discover the entity/organization that the attackers impersonate during phishing attacks. The proposed multi-stage methodology employs natural language processing and machine learning. The methodology first discovers (i) named entities, which includes names of people, organizations, and locations; and (ii) hidden topics, using (a) Conditional Random Field (CRF) and (b) Latent Dirichlet Allocation (LDA) operating on both phishing and non-phishing data. Utilizing topics and named entities as features, the next stage classifies each message as phishing or non-phishing using AdaBoost. For messages classified as phishing, the final stage discovers the impersonated entity using CRF. Experimental results show that the phishing classifier detects phishing attacks with no misclassification when the proportion of phishing emails is less than 20%. The F-measure obtained was 100%. Our approach also discovers the impersonated entity from messages that are classified as phishing, with a discovery rate of 88.1%. The automatic discovery of impersonated entity from phishing helps the legitimate organization to take down the offending phishing site. This protects their users from falling for phishing attacks, which in turn leads to satisfied customers. Automatic discovery of an impersonated entity also helps email service providers to collaborate with each other to exchange attack information and protect their customers. © 2012 Elsevier Ltd. All rights reserved.","Boosting; Conditional Random Field; Identity theft; Impersonated entity discovery; Latent Dirichlet Allocation; Machine learning; Named entity; Natural language processing; Phishing","Adaptive boosting; Artificial intelligence; Computer crime; Electronic mail; Learning algorithms; Learning systems; Natural language processing systems; Random processes; Statistics; Websites; Boosting; Conditional random field; Identity theft; Impersonated entity discovery; Latent Dirichlet allocation; Named entities; NAtural language processing; Phishing; Information dissemination",Article,Scopus,2-s2.0-84891903388
"Shaker M., Shaker N., Togelius J.","Ropossum: An authoring tool for designing, optimizing and solving cut the Rope levels",2013,"Proceedings of the 9th AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, AIIDE 2013",14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84916891747&partnerID=40&md5=8f557bdde9315f558f9e43e33adb132d","We present a demonstration of Ropossum, an authoring tool for the generation and testing of levels of the physics-based game, Cut the Rope. Ropossum integrates many features: (1) automatic design of complete solvable content, (2) incorporation of designer's input through the creation of complete or partial designs, (3) automatic check for playability and (4) optimization of a given design based on playability. The system includes a physics engine to simulate the game and an evolutionary framework to evolve content as well as an AI reasoning agent to check for playability. The system is optimised to allow on-line feedback and realtime interaction. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Artificial intelligence; Rope; Authoring tool; Automatic design; Evolutionary framework; Partial designs; Physics engine; Physics-based; Playability; Real time interactions; Design",Conference Paper,Scopus,2-s2.0-84916891747
"Nolz R., Kammerer G., Cepuder P.","Calibrating soil water potential sensors integrated into a wireless monitoring network",2013,"Agricultural Water Management",14,10.1016/j.agwat.2012.10.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870262459&doi=10.1016%2fj.agwat.2012.10.002&partnerID=40&md5=4ea28051525fe74425157c44df4f8256","Monitoring of the soil water status is a proper method for optimizing agricultural irrigation. Wireless sensor networks enhance data availability, thus, they can be used as decision support systems. In this study two types of soil water potential sensors were tested. One was the well-established Watermark sensor by Irrometer Co., the other was the relatively new MPS-1 by Decagon Devices, Inc. The goal was to (1) integrate the sensors into a wireless monitoring network, (2) determine and evaluate calibration functions for the integrated sensors, and (3) compare the measuring range and the reaction time of both sensor types in a soil layer during drying. The integration of the sensors into the telemetry network worked well. Data were transmitted over several kilometers and made available via Internet access. Calibration was done for several sensors in a pressure pot. A common calibration function was found for the combination of Watermark sensors with the required interface. Sensor specific calibrations became essential for the MPS-1 due to the very large sensor-to-sensor variation. Four approaches were applied and evaluated: Fitting of a standard power function, fitting of a retention function, using so-called one-point calibrations, and using the factory calibration. The latter was not useful at all. The first two methods performed best. The one-point calibrations turned out to be a sound alternative, because the method is less time consuming. A set of sensors was installed in a thin soil layer in the laboratory in order to compare the water potential measurements during drying. Both sensor types delivered water potential measurements in a range from -10. kPa to -600. kPa. For values <-130. kPa the Watermark sensors reacted significantly more slowly than the MPS-1. © 2012 Elsevier B.V.","Calibration; Irrigation management; MPS-1; Telemetry; Watermark","Agricultural irrigation; Calibration functions; Data availability; Integrated sensors; Internet access; Irrigation management; MPS-1; Power functions; Soil layer; Soil water potential; Soil water status; Water potential; Watermark; Wireless monitoring; Artificial intelligence; Calibration; Decision support systems; Irrigation; Soil moisture; Telemetering; Telemetering equipment; Watermarking; Wireless sensor networks; Sensors; calibration; data set; decision support system; installation; Internet; irrigation system; monitoring system; sensor; soil water; telemetry; water management",Article,Scopus,2-s2.0-84870262459
"Seçkiner S.U., Eroǧlu Y., Emrullah M., Dereli T.","Ant colony optimization for continuous functions by using novel pheromone updating",2013,"Applied Mathematics and Computation",14,10.1016/j.amc.2012.10.097,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871182472&doi=10.1016%2fj.amc.2012.10.097&partnerID=40&md5=71a2bc3a1c38ed2ec27e77aecb4bb4bd","This paper presents an ant colony optimization (ACO) algorithm for continuous functions based on novel pheromone updating. At the end of the each iteration in the proposed algorithm, pheromone is updated according to percentiles which determine the number of ants to track the best candidate solution. It is performed by means of solution archive and information provided by previous solutions. Performance of the proposed algorithm is tested on ten benchmark problems found in the literature and compared with performances of previous methods. The results show that ACO which is based on novel pheromone updating scheme (ACO-NPU) handles different types of continuous functions very well and can be a robust alternative approach to other stochastic search algorithms. © 2012 Elsevier Inc. All rights reserved.","Ant colony optimization; Comparative analysis; Continuous optimization; Global minimum; Novel pheromone updating","Ant Colony Optimization (ACO); Comparative analysis; Continuous optimization; Global minima; Novel pheromone updating; Artificial intelligence; Benchmarking; Functions; Iterative methods; Algorithms",Article,Scopus,2-s2.0-84871182472
"Arenas M., Botoeva E., Calvanese D., Ryzhikov V.","Exchanging OWL 2 QL knowledge bases",2013,"IJCAI International Joint Conference on Artificial Intelligence",13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896061929&partnerID=40&md5=2a39ca37f9dcbaea38923fde0104715c","Knowledge base exchange is an important problem in the area of data exchange and knowledge representation, where one is interested in exchanging information between a source and a target knowledge base connected through a mapping. In this paper, we study this fundamental problem for knowledge bases and mappings expressed in OWL2 QL, the profile of OWL2 based on the description logic DL-LiteR. More specifically, we consider the problem of computing universal solutions, identified as one of the most desirable translations to be materialized, and the problem of computing UCQ- representations, which optimally capture in a target TBox the information that can be extracted from a source TBox and a mapping by means of unions of conjunctive queries. For the former we provide a novel automata-theoretic technique, and complexity results that range from NP to EXPTIME, while for the latter we show NLOGSPACE-completeness.",,"Complexity results; Conjunctive queries; Description logic; Dl-lite; Exptime; Knowledge base; Knowledge basis; Universal solutions; Artificial intelligence; Automata theory; Data description; Electronic data interchange; Knowledge based systems; Knowledge representation; Mapping",Conference Paper,Scopus,2-s2.0-84896061929
"Ma Z., Yang Y., Nie F., Sebe N.","Thinking of images as what they are: Compound matrix regression for image classification",2013,"IJCAI International Joint Conference on Artificial Intelligence",13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896064299&partnerID=40&md5=67c1c155feed56e91904fecb79a5f296","In this paper, we propose a new classification framework for image matrices. The approach is realized by learning two groups of classification vectors for each dimension of the image matrices. One novelty is that we utilize compound regression models in the learning process, which endows the algorithm increased degree of freedom. On top of that, we extend the two-dimensional classification method to a semi-supervised classifier which leverages both labeled and unlabeled data. A fast iterative solution is then proposed to solve the objective function. The proposed method is evaluated by several different applications. The experimental results show that our method outperforms several classification approaches. In addition, we observe that our method attains respectable classification performance even when only few labeled training samples are provided. This advantage is especially desirable for real-world problems since precisely annotated images are scarce.",,"Classification approach; Classification framework; Classification methods; Classification performance; Iterative solutions; Labeled and unlabeled data; Objective functions; Real-world problem; Artificial intelligence; Iterative methods; Learning algorithms; Regression analysis; Image classification",Conference Paper,Scopus,2-s2.0-84896064299
"Ren Y., Li G., Zhang J., Zhou W.","Lazy collaborative filtering for data sets with missing values",2013,"IEEE Transactions on Cybernetics",13,10.1109/TSMCB.2012.2231411,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890016550&doi=10.1109%2fTSMCB.2012.2231411&partnerID=40&md5=cdbb5a3553d16f6852d60673b8c3c901","As one of the biggest challenges in research on recommender systems, the data sparsity issue is mainly caused by the fact that users tend to rate a small proportion of items from the huge number of available items. This issue becomes even more problematic for the neighborhood-based collaborative filtering (CF) methods, as there are even lower numbers of ratings available in the neighborhood of the query item. In this paper, we aim to address the data sparsity issue in the context of neighborhood-based CF. For a given query (user, item), a set of key ratings is first identified by taking the historical information of both the user and the item into account. Then, an auto-adaptive imputation (AutAI) method is proposed to impute the missing values in the set of key ratings. We present a theoretical analysis to show that the proposed imputation method effectively improves the performance of the conventional neighborhood-based CF methods. The experimental results show that our new method of CF with AutAI outperforms six existing recommendation methods in terms of accuracy. © 2013 IEEE.","Imputation; Neighborhood-based collaborative filtering (CF); Recommender systems","Data sparsity; Historical information; Imputation; Imputation methods; Missing values; Neighborhood-based collaborative filtering (CF); Recommendation methods; Recommender systems; Collaborative filtering; algorithm; article; artificial intelligence; automated pattern recognition; data mining; factual database; information retrieval; methodology; sample size; signal processing; Algorithms; Artificial Intelligence; Data Mining; Databases, Factual; Information Storage and Retrieval; Pattern Recognition, Automated; Sample Size; Signal Processing, Computer-Assisted",Article,Scopus,2-s2.0-84890016550
"Badea A.F., Bran S., Tamas-Szora A., Floareş A., Badea R., Baciut G.","Solid parotid tumors: An individual and integrative analysis of various ultrasonographic criteria. A prospective and observational study",2013,"Medical Ultrasonography",13,10.11152/mu.2013.2066.154.afb2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888794392&doi=10.11152%2fmu.2013.2066.154.afb2&partnerID=40&md5=800775882c2ac91c2dd12a90685bee23","Objectives: The purpose of the study is to identify and validate ultrasound criteria for parotid tumors evaluation, as well as to elaborate a multimodal, multi-criteria and integrative ultrasound approach for allowing tumor discrimination in a non-invasive manner. Material and method: Twenty patients with solid parotid tumors (12 benign, 8 malignant) were examined by ultrasound: real-time ""grey scale"" ultrasound, Doppler ultrasound, elastography, harmonic ultrasound imaging with i.v. contrast (CEUS). The study focused on tumor morphology and circulation. The analysis of the results was observational, enhanced by statistical methods and artificial intelligence (decision trees). Results: All malignant tumors showed increased hypoechogenicity, tumoral cervical adenopathies, increased stiffness and ""in block"" mobility with the parotid gland upon palpation with the transducer, uneven distribution of the contrast material during the arterial phase (8/8). To varying degrees, they showed imprecise delineation (7/8), structural heterogeneity (6/8) and disorganized flow pattern (6/8). All cases of benign tumors showed heterogeneous echostructure, clear delineation and no capsule (12). They also showed moderate hypoechogenicity (9/12), no cervical lymph nodes (11/12) and variable rigidity (increased 6/12; low 3/12). A selection and ranking of relevant ultrasound parameters was also made. Some of them were included in a transparent and easy-to-use decision tree model with 100% data accuracy. Conclusions: The characterization and discrimination of solid parotid tumors require a multimodal and multicriteria approach. Ultrasound criteria can be divided into criteria of certainty and criteria of diagnosis probability. CEUS examination of parotid tumors did not reveal significant differences between benign and malignant circulatory bed. Decision trees discovered by artificial intelligence from the data may represent intelligent diagnosis support systems with very high accuracy, up to 100%.","Contrast agent; Elastography; Harmonic; Parotid tumor; Ultrasound","adult; aged; article; benign tumor; cervical lymphadenopathy; clinical article; color ultrasound flowmetry; contrast enhanced ultrasonography; controlled study; decision tree; diagnostic accuracy; diagnostic test accuracy study; differential diagnosis; Doppler echography; echography; elastography; female; Gray scale echography; human; intermethod comparison; male; melanoma; microcirculation; non invasive procedure; observational study; parotid gland carcinoma; parotid gland tumor; prospective study; rigidity; solid tumor; tumor diagnosis; tumor differentiation; tumor vascularization; tumor volume; vascular tumor; Adult; Aged; Aged, 80 and over; Algorithms; Artificial Intelligence; Female; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Male; Middle Aged; Multimodal Imaging; Parotid Neoplasms; Prospective Studies; Reproducibility of Results; Sensitivity and Specificity; Systems Integration; Ultrasonography",Article,Scopus,2-s2.0-84888794392
"Baier H., Winands M.H.M.","Monte-Carlo Tree Search and minimax hybrids",2013,"IEEE Conference on Computatonal Intelligence and Games, CIG",13,10.1109/CIG.2013.6633630,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892422172&doi=10.1109%2fCIG.2013.6633630&partnerID=40&md5=dfe925f64007fdef092a48d00096bba5","Monte-Carlo Tree Search is a sampling-based search algorithm that has been successfully applied to a variety of games. Monte-Carlo rollouts allow it to take distant consequences of moves into account, giving it a strategic advantage in many domains over traditional depth-limited minimax search with alpha-beta pruning. However, MCTS builds a highly selective tree and can therefore miss crucial moves and fall into traps in tactical situations. Full-width minimax search does not suffer from this weakness. This paper proposes MCTS-minimax hybrids that employ shallow minimax searches within the MCTS framework. The three proposed approaches use minimax in the selection/expansion phase, the rollout phase, and the backpropagation phase of MCTS. Without requiring domain knowledge in the form of evaluation functions, these hybrid algorithms are a first step at combining the strategic strength of MCTS and the tactical strength of minimax. We investigate their effectiveness in the test domains of Connect-4 and Breakthrough. © 2013 IEEE.",,"Alpha-beta pruning; Domain knowledge; Evaluation function; Hybrid algorithms; Monte-Carlo tree searches; Sampling-based; Search Algorithms; Strategic advantages; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84892422172
"Storandt S., Funke S.","Enabling E-mobility: Facility location for battery loading stations",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893390903&partnerID=40&md5=e0463e16917ff1b9464140d8130c7f44","The short cruising range due to the limited battery supply of current Electric Vehicles (EVs) is one of the main obstacles for a complete transition to E-mobility. Until batteries of higher energy storage density have been developed, it is of utmost importance to deliberately plan the locations of new loading stations for best possible coverage. Ideally the network of loading stations should allow driving from anywhere to anywhere (and back) without running out of energy. We show that minimizing the number of necessary loading stations to achieve this goal is NP-hard and even worse, we can rule out polynomial-time constant approximation algorithms. Hence algorithms with better approximation guarantees have to make use of the special structure of road networks (which is not obvious how to do it). On the positive side, we show with instance based lower bounds that our heuristic algorithms achieve provably good solutions on real-world problem instances. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Constant approximation algorithms; Electric Vehicles (EVs); Energy storage density; Facility locations; Minimizing the number of; Polynomial-time; Real-world problem; Special structure; Approximation algorithms; Artificial intelligence; Heuristic algorithms; Location; Loading",Conference Paper,Scopus,2-s2.0-84893390903
"Gandy L., Allan N., Atallah M., Frieder O., Howard N., Kanareykin S., Koppel M., Last M., Neuman Y., Argamon S.","Automatic identification of conceptual metaphors with limited knowledge",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893377888&partnerID=40&md5=b67ed23924780082090610e0e209b726","Full natural language understanding requires identifying and analyzing the meanings of metaphors, which are ubiquitous in both text and speech. Over the last thirty years, linguistic metaphors have been shown to be based on more general conceptual metaphors, partial semantic mappings between disparate conceptual domains. Though some achievements have been made in identifying linguistic metaphors over the last decade or so, little work has been done to date on automatically identifying conceptual metaphors. This paper describes research on identifying conceptual metaphors based on corpus data. Our method uses as little background knowledge as possible, to ease transfer to new languages and to minimize any bias introduced by the knowledge base construction process. The method relies on general heuristics for identifying linguistic metaphors and statistical clustering (guided by Wordnet) to form conceptual metaphor candidates. Human experiments show the system effectively finds meaningful conceptual metaphors. © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Automatic identification; Back-ground knowledge; Knowledge-base construction; Natural language understanding; Semantic mapping; Statistical clustering; Wordnet; Artificial intelligence; Automation; Knowledge based systems; Semantics; Linguistics",Conference Paper,Scopus,2-s2.0-84893377888
"Wang J., Zhao Z.-Q., Hu X., Cheung Y.-M., Wang M., Wu X.","Online group feature selection",2013,"IJCAI International Joint Conference on Artificial Intelligence",13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896059103&partnerID=40&md5=a5c5d98901bf4dfdc4bacefb7f02314e","Online feature selection with dynamic features has become an active research area in recent years. However, in some real-world applications such as image analysis and email spam filtering, features may arrive by groups. Existing online feature selection methods evaluate features individually, while existing group feature selection methods cannot handle online processing. Motivated by this, we formulate the online group feature selection problem, and propose a novel selection approach for this problem. Our proposed approach consists of two stages: online intra-group selection and online inter-group selection. In the intra-group selection, we use spectral analysis to select discriminative features in each group when it arrives. In the inter-group selection, we use Lasso to select a globally optimal subset of features. This 2-stage procedure continues until there are no more features to come or some predefined stopping conditions are met. Extensive experiments conducted on benchmark and real-world data sets demonstrate that our proposed approach outperforms other state-of-the-art online feature selection methods.",,"Discriminative features; Dynamic features; Feature selection methods; Feature selection problem; Intra-group; Online feature selection; Online processing; Optimal subsets; Artificial intelligence; Spectrum analysis; Virtual reality; Distributed computer systems",Conference Paper,Scopus,2-s2.0-84896059103
"Weng Y., Negi R., Ilić M.D.","Graphical model for state estimation in electric power systems",2013,"2013 IEEE International Conference on Smart Grid Communications, SmartGridComm 2013",13,10.1109/SmartGridComm.2013.6687941,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893547495&doi=10.1109%2fSmartGridComm.2013.6687941&partnerID=40&md5=d5fa3e3f251b59f0ef6bc8be0ec90b83","This paper is motivated by major needs for fast and accurate on-line state estimation (SE) in the emerging electric energy systems, due to recent penetration of distributed green energy, distributed intelligence, and plug-in electric vehicles. Different from the traditional deterministic approach, this paper uses a probabilistic graphical model to account for these new uncertainties by efficient distributed state estimation. The proposed graphical model is able to discover and analyze unstructured information and it has been successfully deployed in statistical physics, computer vision, error control coding, and artificial intelligence. Specifically, this paper shows how to model the traditional power system state estimation problem in a probabilistic manner. Mature graphical model inference tools, such as belief propagation and variational belief propagation, are subsequently applied. Simulation results demonstrate better performance of SE over the traditional deterministic approach in terms of accuracy and computational time. Notably, the near-linear computational time of the proposed approach enables the scalability of state estimation which is crucial in the operation of future large-scale smart grid. © 2013 IEEE.",,"Deterministic approach; Distributed intelligence; Distributed state estimation; Electric energy systems; Error control coding; Plug-in Electric Vehicles; Probabilistic graphical models; Traditional power system; Artificial intelligence; Graphic methods; Smart power grids; State estimation",Conference Paper,Scopus,2-s2.0-84893547495
"Hajaj C., Hazon N., Sarne D., Elmalech A.","Search more, disclose less",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893394496&partnerID=40&md5=5bb5224340ebe656b34a7216ee7f6078","The blooming of comparison shopping agents (CSAs) in recent years enables buyers in today's markets to query more than a single CSA while shopping, thus substantially expanding the list of sellers whose prices they obtain. From the individual CSA point of view, however, the multi-CSAs querying is definitely non-favorable as most of today's CSAs benefit depends on payments they receive from sellers upon transferring buyers to their websites (and making a purchase). The most straightforward way for the CSA to improve its competence is through spending more resources on getting more sellers' prices, potentially resulting in a more attractive ""best price"". In this paper we suggest a complementary approach that improves the attractiveness of the best price returned to the buyer without having to extend the CSAs' price database. This approach, which we term ""selective price disclosure"" relies on removing some of the prices known to the CSA from the list of results returned to the buyer. The advantage of this approach is in the ability to affect the buyer's beliefs regarding the probability of obtaining more attractive prices if querying additional CSAs. The paper presents two methods for choosing the subset of prices to be presented to a fully-rational buyer, attempting to overcome the computational complexity associated with evaluating all possible subsets. The effectiveness and efficiency of the methods are demonstrated using real data, collected from five CSAs for four products. Furthermore, since people are known to have an inherently bounded rationality, the two methods are also evaluated with human buyers, demonstrating that selective price-disclosing can be highly effective with people, however the subset of prices that needs to be used should be extracted in a different (and more simplistic) manner. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Bounded rationality; Comparison shopping; Effectiveness and efficiencies; Artificial intelligence; Behavioral research; Query processing; Sales; Costs",Conference Paper,Scopus,2-s2.0-84893394496
"Sahu Y., Pateriya R.K., Gupta R.K.","Cloud server optimization with load balancing and green computing techniques using dynamic compare and balance algorithm",2013,"Proceedings - 5th International Conference on Computational Intelligence and Communication Networks, CICN 2013",13,10.1109/CICN.2013.114,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892647864&doi=10.1109%2fCICN.2013.114&partnerID=40&md5=cf288cef6f09a20a7e997e31f7c79c46","Cloud computing is a business oriented concept to provide online IT resources and IT services on demand using pay per use model where main goal of cloud service provider is to use cloud computing resources efficiently and gain profits marginally. One of the challenging areas in cloud computing is frequent optimization of cloud server. It mainly concerns with the load balancing of cloud data centers to improve efficiency of the host machine and minimize number of active host machine to support green computing concept. To balance the load of entire data center, we need to transfer the virtual machines of the overloaded host to the light weighted host using migration techniques. In this paper, we introduce a threshold based Dynamic compare and balance algorithm (DCABA) for cloud server optimization. Unlike the traditional server optimization strategies which consider only load balancing and scheduling of resources based on the usage of CPU, RAM and BW in physical servers, DCABA also minimizes the number of host machines to be powered on, for reducing the cost of cloud services. Our approach can serve the purpose of service cost reduction in cloud industry with effective utilization of available resources. © 2013 IEEE.","Cloud Computing; Dynamic Resource Managememt; Green Computing; Load Balancing; Server Consolidation","Cloud data centers; Cloud service providers; Computing resource; Dynamic resources; Green computing; Migration technique; Optimization strategy; Server consolidation; Algorithms; Artificial intelligence; Cloud computing; Cost reduction; Distributed database systems; Optimization; Parallel architectures; Resource allocation; Information technology",Conference Paper,Scopus,2-s2.0-84892647864
"Le Bras R., Bernstein R., Gomes C.P., Selman B., Van Dover R.B.","Crowdsourcing backdoor identification for combinatorial optimization",2013,"IJCAI International Joint Conference on Artificial Intelligence",13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896061157&partnerID=40&md5=0604c49842ef5e8a2fd6827fa21c6fdb","We will show how human computation insights can be key to identifying so-called backdoor variables in combinatorial optimization problems. Backdoor variables can be used to obtain dramatic speedups in combinatorial search. Our approach leverages the complementary strength of human input, based on a visual identification of problem structure, crowd sourcing, and the power of combinatorial solvers to exploit complex constraints. We describe our work in the context of the domain of materials discovery. The motivation for considering the materials discovery domain comes from the fact that new materials can provide solutions for key challenges in sustainability, e.g., in energy, new catalysts for more efficient fuel cell technology.",,"Combinatorial optimization problems; Combinatorial search; Complex constraints; Crowd sourcing; Fuel cell technologies; Human computation; Problem structure; Visual identification; Artificial intelligence; Combinatorial optimization",Conference Paper,Scopus,2-s2.0-84896061157
"Wu J., Liu H., Xiong H., Cao J.","A theoretic framework of K-means-based Consensus Clustering",2013,"IJCAI International Joint Conference on Artificial Intelligence",13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896061966&partnerID=40&md5=06f8741f39a9a2a1fe16ae6ab420839f","Consensus clustering emerges as a promising solution to find cluster structures from data. As an efficient approach for consensus clustering, the Kmeans based method has garnered attention in the literature, but the existing research is still preliminary and fragmented. In this paper, we provide a systematic study on the framework of K-means based Consensus Clustering (KCC). We first formulate the general definition of KCC, and then reveal a necessary and sufficient condition for utility functions that work for KCC, on both complete and incomplete basic partitionings. Experimental results on various real-world data sets demonstrate that KCC is highly efficient and is comparable to the state-of-the-art methods in terms of clustering quality. In addition, KCC shows high robustness to incomplete basic partitionings with substantial missing values.",,"Cluster structure; Clustering quality; Consensus clustering; High robustness; Missing values; State-of-the-art methods; Systematic study; Utility functions; Artificial intelligence; Virtual reality; Cluster analysis",Conference Paper,Scopus,2-s2.0-84896061966
"Lu T., Boutilier C.","Multi-winner social choice with incomplete preferences",2013,"IJCAI International Joint Conference on Artificial Intelligence",13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062788&partnerID=40&md5=a825b6746792f54ba32b34a1bf233325","Multi-winner social choice considers the problem of selecting a slate of K options to realize some social objective. It has found application in the construction of political legislatures and committees, product recommendation, and related problems, and has recently attracted attention from a computational perspective. We address the multi-winner problem when facing incomplete voter preferences, using the notion of minimax regret to determine a robust slate of options in the presence of preference uncertainty. We analyze the complexity of this problem and develop new exact and greedy robust optimization algorithms for its solution. Using these techniques, we also develop preference elicitation heuristics which, in practice, allow us to find near-optimal slates with considerable savings in the preference information required vis-à-vis complete votes.",,"Minimax regret; Preference elicitation; Preference information; Preference uncertainty; Product recommendation; Robust optimization algorithm; Social choice; Social objectives; Algorithms; Artificial intelligence; Slate",Conference Paper,Scopus,2-s2.0-84896062788
"Husmann M., Nebeling M., Norrie M.C.","MultiMasher: A visual tool for multi-device mashups",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-319-04244-2_4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893740840&doi=10.1007%2f978-3-319-04244-2_4&partnerID=40&md5=35591cc58398ce997901eeef08f2c839","The proliferation of a wide range of computing devices from tablets to large displays has created many situations where we no longer use a single device only. Rather, multiple devices are commonly used together to achieve a task. However, there is still little tool support for such scenarios in which different devices need to be combined to control an interface. Our goal is to enable multiple devices to view and interact with multiple web resources in a coordinated manner based on our new idea of multi-device mashups. In this paper, we present a first, visual tool for mashing up devices to access web sites, and discuss how we addressed the challenges as well as interesting issues for further research. © Springer International Publishing 2013.",,"Computing devices; Large displays; Mashups; Multi-devices; Multiple devices; Tool support; Visual tools; Web resources; Artificial intelligence; Computer science; Computers; Tools",Conference Paper,Scopus,2-s2.0-84893740840
"Zhang P.Y., Romero D.A., Beck J.C., Amon C.H.","Solving wind farm layout optimization with mixed integer programming and constraint programming",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-642-38171-3_19,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892914356&doi=10.1007%2f978-3-642-38171-3_19&partnerID=40&md5=b31e084a44b348ed89314222d58f306a","The wind farm layout optimization problem is concerned with the optimal location of turbines within a fixed geographical area to maximize energy capture under stochastic wind conditions. Previously it has been modelled as a maximum diversity (or p-dispersion-sum) problem, but such a formulation cannot capture the nonlinearity of aerodynamic interactions among multiple wind turbines. We present the first constraint programming (CP) and mixed integer linear programming (MIP) models that incorporate such nonlinearity. Our empirical results indicate that the relative performance between these two models reverses when the wind scenario changes from a simple to a more complex one. We also propose an improvement to the previous maximum diversity model and demonstrate that the improved model solves more problem instances. © Springer-Verlag 2013.",,"Aerodynamic interactions; Constraint programming; Geographical area; Mixed Integer Linear Programming (MIP); Mixed integer programming; Problem instances; Relative performance; Wind farm layout optimizations; Artificial intelligence; Constraint theory; Electric utilities; Linear programming; Operations research; Optimization; Wind power; Computer programming",Conference Paper,Scopus,2-s2.0-84892914356
"Huang J., Nie F., Huang H., Ding C.","Supervised and projected sparse coding for image classification",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893373763&partnerID=40&md5=df0a11e26963c6ad89318ec0274e1cef","Classic sparse representation for classification (SRC) method fails to incorporate the label information of training images, and meanwhile has a poor scalability due to the expensive computation for '1 norm. In this paper, we propose a novel subspace sparse coding method with utilizing label information to effectively classify the images in the subspace. Our new approach unifies the tasks of dimension reduction and supervised sparse vector learning, by simultaneously preserving the data sparse structure and meanwhile seeking the optimal projection direction in the training stage, therefore accelerates the classification process in the test stage. Our method achieves both flat and structured sparsity for the vector representations, therefore making our framework more discriminative during the subspace learning and subsequent classification. The empirical results on 4 benchmark data sets demonstrate the effectiveness of our method. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Classification process; Dimension reduction; Label information; Projection direction; Sparse representation; Structured sparsities; Subspace learning; Vector representations; Artificial intelligence; Image classification; Image coding; Classification (of information)",Conference Paper,Scopus,2-s2.0-84893373763
"Yu Q., Wen X., Liu Y.","Multi-agent epistemic explanatory diagnosis via reasoning about actions",2013,"IJCAI International Joint Conference on Artificial Intelligence",13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896064115&partnerID=40&md5=130e560d57a2e22b92b70323b0615f2f","The task of explanatory diagnosis conjectures actions to explain observations. This is a common task in real life and an essential ability of intelligent agents. It becomes more complicated in multi-agent scenarios, since agents' actions may be partially observable to other agents, and observations might involve agents' knowledge about the world or other agents' knowledge or even common knowledge of a group of agents. For example, we might want to explain the observation that p does not hold, but Ann believes p, or the observation that Ann, Bob, and Carl commonly believe p. In this paper, we formalize the multi-agent explanatory diagnosis task in the framework of dynamic epistemic logic, where Kripke models of actions are used to represent agents' partial observability of actions. Since this task is undecidable in general, we identify important decidable fragments via techniques of reducing the potentially infinite search spaces to finite ones of epistemic states or action sequences.",,"Action sequences; Common knowledge; Dynamic epistemic logic; Epistemic state; Essential abilities; Partial observability; Reasoning about actions; Search spaces; Intelligent agents; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84896064115
"Gan J., An B., Wang H., Sun X., Shi Z.","Optimal pricing for improving efficiency of taxi systems",2013,"IJCAI International Joint Conference on Artificial Intelligence",13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062486&partnerID=40&md5=b75a00f316706f7532443f28ec0016c6","In Beijing, most taxi drivers intentionally avoid working during peak hours despite of the huge customer demand within these peak periods. This dilemma is mainly due to the fact that taxi drivers' congestion costs are not reflected in the current taxi fare structure. To resolve this problem, we propose a new pricing scheme to provide taxi drivers with extra incentives to work during peak hours. This differs from previous studies of taxi market by considering market variance over multiple periods, taxi drivers' profit-driven decisions, and their scheduling constraints regarding the interdependence among different periods. The major challenge of this research is the computational intensiveness to identify optimal strategy due to the exponentially large size of a taxi driver's strategy space and the scheduling constraints. We develop an atom schedule method to overcome these issues. It reduces the magnitude of the problem while satisfying the constraints to filter out infeasible pure strategies. Simulation results based on real data show the effectiveness of the proposed methods, which opens up a new door to improving the efficiency of taxi market in megacities (e.g., Beijing).",,"Computational intensiveness; Congestion costs; Customer demands; Improving efficiency; Optimal strategies; Schedule methods; Scheduling constraints; Taxi fare structure; Artificial intelligence; Commerce; Scheduling; Taxicabs; Traffic congestion; Constraint satisfaction problems",Conference Paper,Scopus,2-s2.0-84896062486
"Caraffini F., Neri F., Passow B.N., Iacca G.","Re-sampled inheritance search: High performance despite the simplicity",2013,"Soft Computing",13,10.1007/s00500-013-1106-7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888050661&doi=10.1007%2fs00500-013-1106-7&partnerID=40&md5=b7d3b3c00f7ff562aaa22facd871aa60","This paper proposes re-sampled inheritance search (RIS), a novel algorithm for solving continuous optimization problems. The proposed method, belonging to the class of Memetic Computing, is very simple and low demanding in terms of memory employment and computational overhead. The RIS algorithm is composed of a stochastic sample mechanism and a deterministic local search. The first operator randomly generates a solution and then recombines it with the best solution detected so far (inheritance) while the second operator searches in an exploitative way within the neighbourhood indicated by the stochastic operator. This extremely simple scheme is shown to display a very good performance on various problems, including hard to solve multi-modal, highly-conditioned, large scale problems. Experimental results show that the proposed RIS is a robust scheme that competitively performs with respect to recent complex algorithms representing the-state-of-the-art in modern continuous optimization. In order to further prove its applicability in real-world cases, RIS has been used to perform the control system tuning for yaw operations on a helicopter robot. Experimental results on this real-world problem confirm the value of the proposed approach. © 2013 Springer-Verlag Berlin Heidelberg.","Autonomous helicopter; Computational intelligence optimization; Control system design; Large scale optimization; Memetic computing; Ockham's Razor","Autonomous helicopters; Computational intelligence optimizations; Large-scale optimization; Memetic computing; Ockham's razor; Algorithms; Artificial intelligence; Control systems; Helicopter services; Helicopters; Interlocking signals; Optimization; Stochastic systems; Flight control systems",Article,Scopus,2-s2.0-84888050661
"Kalinowski T., Narodytska N., Walsh T.","A social welfare optimal sequential allocation procedure",2013,"IJCAI International Joint Conference on Artificial Intelligence",13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896059870&partnerID=40&md5=a18b73d2450a4009f4f104b1ebb11178","We consider a simple sequential allocation procedure for sharing indivisible items between agents in which agents take turns to pick items. Supposing additive utilities and independence between the agents, we show that the expected utility of each agent is computable in polynomial time. Using this result, we prove that the expected utilitarian social welfare is maximized when agents take alternate turns. We also argue that this mechanism remains optimal when agents behave strategically.",,"Expected utility; Optimal sequential; Polynomial-time; Sequential allocations; Social welfare; Polynomial approximation; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84896059870
"Lu F., Yamamoto K., Nomura L.H., Mizuno S., Lee Y., Thawonmas R.","Fighting game artificial intelligence competition platform",2013,"2013 IEEE 2nd Global Conference on Consumer Electronics, GCCE 2013",13,10.1109/GCCE.2013.6664844,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892634565&doi=10.1109%2fGCCE.2013.6664844&partnerID=40&md5=3ef809420d2660deb6f46125fc79a41d","Game playing has provided an interesting framework for developing and testing artificial intelligence (AI) algorithms, with well-known examples such as Go and Chess. Since the first chess programs were written, there has been steady progress in the level of play to the point where current systems can challenge top-class human players. Recently, academia groups have created competitions to evaluate and compare AI methods, such as Ms Pac-Man versus Ghost Team and Student StarCraft AI Tournament. Those tournaments define their rules and goals based on the game they selected, according to which intelligent techniques and technologies are developed. We focus on a genre called fighting game. Examples of fighting games are Street Fighter, Tekken, and Mortal Kombat. To select the winner in the fighting game is simple: the winner is the character with less damage or the last one standing. In this paper we introduce a brand new competition1 based on a 2D fighting game written in Java. © 2013 IEEE.","AI; algorithms; artificial intelligence; contest; fighting game","Chess program; contest; fighting game; Game artificial intelligence; Game playing; Human players; Intelligent techniques; Pac-man; Algorithms; Consumer electronics; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84892634565
"Pozzi F.A., Maccagnola D., Fersini E., Messina E.","Enhance user-level Sentiment Analysis on microblogs with approval relations",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-319-03524-6_12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892712283&doi=10.1007%2f978-3-319-03524-6_12&partnerID=40&md5=eb6bf20bfe673cfbea69898e8e9043e0","Sentiment Analysis for polarity classification on microblogs is generally based on the assumption that texts are independent and identically distributed (i.i.d). Although these methods are aimed at handling the complex characteristics of natural language, usually they do not consider microblogs as networked data. Early approaches for overcoming this limitation consist in exploiting friendship relationships, since connected users may be more likely to hold similar opinions (Homophily and Social Influence). However, the assumption about the friendship relations does not reflect the real world, where two connected users could have different opinions about the same topic. In order to overcome these shortcomings, we propose a semi-supervised framework that estimates user polarities about a given topic by combining post contents and weighted approval relations, which are intended to better represent the contagion on social networks. The experimental investigation reveals that incorporating approval relations can lead to statistically significant improvements over the performance of complex supervised classifiers based only on textual features. © Springer International Publishing Switzerland 2013.",,"Complex characteristics; Experimental investigations; Natural languages; Polarity classification; Sentiment analysis; Social influence; Supervised classifiers; Textual features; Artificial intelligence; Data mining; Natural language processing systems",Conference Paper,Scopus,2-s2.0-84892712283
"Shivashankar V., Alford R., Kuter U., Nau D.","The GoDeL planning system: A more perfect union of domain-independent and hierarchical planning",2013,"IJCAI International Joint Conference on Artificial Intelligence",13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063452&partnerID=40&md5=ed97e2e68f2366fefb7ddc9ed4d3a812","One drawback of Hierarchical Task Network (HTN) planning is the difficulty of providing complete domain knowledge, i.e., a complete and correct set of HTN methods for every task. To provide a principled way to overcome this difficulty, we define a simple formalism that extends classical planning to include problem decomposition using methods, and a planning algorithm based on this formalism. In our formalism, the methods specify ways to achieve goals (rather than tasks as in conventional HTN planning), and goals may be achieved even when no methods are available. Our planning algorithm, GoDeL (Goal Decomposition with Landmarks), is sound and complete irrespective of whether the domain knowledge (i.e., the set of methods given to the planner) is complete. By comparing GoDeL's performance with varying amounts of domain knowledge across three benchmark planning domains, we show experimentally that (1) GoDeL works correctly with partial planning knowledge, (2) GoDeL's performance improves as more planning knowledge is given, and (3) when given full domain knowledge, GoDeL matches the performance of a state-of-the-art hierarchical planner.",,"Classical planning; Hierarchical planners; Hierarchical planning; Hierarchical task network (HTN) planning; Planning algorithms; Planning domains; Problem decomposition; Sound and complete; Algorithms; Artificial intelligence; Benchmarking; Domain decomposition methods",Conference Paper,Scopus,2-s2.0-84896063452
"De Melo G.","Not quite the same: Identity constraints for the Web of Linked Data",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893358659&partnerID=40&md5=192e549f13c45716adc048eafc92dec6","Linked Data is based on the idea that information from different sources can flexibly be connected to enable novel applications that individual datasets do not support on their own. This hinges upon the existence of links between datasets that would otherwise be isolated. The most notable form, sameAs links, are intended to express that two identifiers are equivalent in all respects. Unfortunately, many existing ones do not reflect such genuine identity. This study provides a novel method to analyse this phenomenon, based on a thorough theoretical analysis, as well as a novel graph-based method to resolve such issues to some extent. Our experiments on a representative Web-scale set of sameAs links from the Web of Data show that our method can identify and remove hundreds of thousands of constraint violations. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Constraint violation; Graph-based methods; Identity constraints; Linked datum; Novel applications; Web of datum; Artificial intelligence; Data handling",Conference Paper,Scopus,2-s2.0-84893358659
"Zhang D., Yang G., Hu Y., Jin Z., Cai D., He X.","A unified approximate Nearest Neighbor Search scheme by combining data structure and hashing",2013,"IJCAI International Joint Conference on Artificial Intelligence",13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896061228&partnerID=40&md5=22a37afed7b58935d70d921a3bda787d","Nowadays, Nearest Neighbor Search becomes more and more important when facing the challenge of big data. Traditionally, to solve this problem, researchers mainly focus on building effective data structures such as hierarchical k-means tree or using hashing methods to accelerate the query process. In this paper, we propose a novel unified approximate nearest neighbor search scheme to combine the advantages of both the effective data structure and the fast Hamming distance computation in hashing methods. In this way, the searching procedure can be further accelerated. Computational complexity analysis and extensive experiments have demonstrated the effectiveness of our proposed scheme.",,"Approximate Nearest Neighbor Search; Big datum; Computational complexity analysis; Hamming distance computation; Hashing method; Hierarchical k-means; Nearest Neighbor search; Artificial intelligence; Data structures",Conference Paper,Scopus,2-s2.0-84896061228
"Bartholomew M., Lee J.","Functional stable model semantics and Answer Set Programming Modulo Theories",2013,"IJCAI International Joint Conference on Artificial Intelligence",13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896061846&partnerID=40&md5=464c91cbcd1be2431c80eb02358601e5","Recently there has been an increasing interest in incorporating ""intensional"" functions in answer set programming. Intensional functions are those whose values can be described by other functions and predicates, rather than being pre-defined as in the standard answer set programming. We demonstrate that the functional stable model semantics plays an important role in the framework of ""Answer Set Programming Modulo Theories (ASPMT)"" -a tight integration of answer set programming and satisfiability modulo theories, under which existing integration approaches can be viewed as special cases where the role of functions is limited. We show that ""tight"" ASPMT programs can be translated into SMT instances, which is similar to the known relationship between ASP and SAT.",,"Answer set programming; Integration approach; Intensional functions; Modulo theories; Satisfiability modulo Theories; Stable model semantics; Tight integrations; Artificial intelligence; Semantics; Logic programming",Conference Paper,Scopus,2-s2.0-84896061846
"Saleh S.Q., Hussain M., Muhammad G., Bebis G.","Evaluation of image forgery detection using multi-scale weber local descriptors",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-642-41939-3_40,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888243899&doi=10.1007%2f978-3-642-41939-3_40&partnerID=40&md5=468fd2ad8ba308e2cff41a9f7acf05f7","In this paper, a detailed evaluation of multi-scale Weber local descriptors (WLD) based image forgery detection method is presented. Multi-scale WLD extracts the features from chrominance components of an image, which usually encode the tampering information that escapes the human eyes. The WLD incorporates differential excitation and gradient orientation of a center pixel around a neighborhood. In the multi-scale WLD, three different neighborhoods are chosen. A support vector machine is used for classification purpose. The experiments are conducted on three image databases, namely, CASIA v1.0, CASIA v2.0, and Columbia color. The experimental results show that the accuracy rate of the proposed method are 94.19% for CASIA v1.0, 96.61% for CASIA v2.0, and 94.17% for Columbia dataset. These accuracies are significantly higher than those obtained by some state-of-the-art methods. © 2013 Springer-Verlag.","copy-move forgery; image forgery detection; image splicing; Weber local descriptors","Copy-move forgeries; Differential excitation; Gradient orientations; Image database; Image forgery detections; Image splicing; State-of-the-art methods; Weber local descriptors; Artificial intelligence; Computer science",Conference Paper,Scopus,2-s2.0-84888243899
"Menghal P.M., Jaya Laxmi A.","Adaptive neuro fuzzy based dynamic simulation of induction motor drives",2013,"IEEE International Conference on Fuzzy Systems",13,10.1109/FUZZ-IEEE.2013.6622452,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887844814&doi=10.1109%2fFUZZ-IEEE.2013.6622452&partnerID=40&md5=7290a3e9ba011658e8196b68c5cb6bdc","In the industrial sector especially in the field of electric drives & control, induction motors play a vital role. Without proper controlling of the speed, it is virtually impossible to achieve the desired task for a specific application. Basically AC motors, such as Induction Motors are of Squirrel-Cage type. They are simple, reliable, low cost and virtually maintenance-free electrical drives. Based on the inability of conventional control methods like PI, PID controllers to work under wide range of operation, artificial intelligent based controllers are widely used in the industry like ANN, Fuzzy controller, ANFIS, expert system, genetic algorithm. The main problem with the conventional fuzzy controllers is that the parameters associated with the membership functions and the rules depend broadly on the intuition of the experts. To overcome this problem, Adaptive Neuro-Fuzzy controller is proposed in this paper. The comparison between Conventional PI, Fuzzy Controller and Adaptive neuro fuzzy controller based dynamic performance of induction motor drive has been presented. Adaptive Neuro Fuzzy based control of induction motor will prove to be more reliable than other control methods. © 2013 IEEE.","Adaptive neuro-fuzzy controller; Fuzzy Logic Controller(FLC); Hebbian learning algorithm; PI controller; Sugeno fuzzy controller","Adaptive neuro-fuzzy; Fuzzy controllers; Fuzzy logic controllers; Hebbian learning algorithm; PI Controller; Artificial intelligence; Computer simulation; Controllers; Electric drives; Expert systems; Fuzzy logic; Fuzzy systems; Three term control systems; Induction motors",Conference Paper,Scopus,2-s2.0-84887844814
"Albert E., Flores-Montoya A., Genaim S., Martin-Martin E.","Termination and cost analysis of loops with concurrent interleavings",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-319-02444-8_25,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887420920&doi=10.1007%2f978-3-319-02444-8_25&partnerID=40&md5=29044f192d8756a6e010043f940b3c4d","By following a rely-guarantee style of reasoning, we present a novel termination analysis for concurrent programs that, in order to prove termination of a considered loop, makes the assumption that the ""shared-data that is involved in the termination proof of the loop is modified a finite number of times"". In a subsequent step, it proves that this assumption holds in all code whose execution might interleave with such loop. At the core of the analysis, we use a may-happen-in-parallel analysis to restrict the set of program points whose execution can interleave with the considered loop. Interestingly, the same kind of reasoning can be applied to infer upper bounds on the number of iterations of loops with concurrent interleavings. To the best of our knowledge, this is the first method to automatically bound the cost of such kind of loops. © 2013 Springer International Publishing.",,"Concurrent program; Cost analysis; Interleavings; Number of iterations; Program points; Rely guarantees; Termination analysis; Termination proof; Artificial intelligence; Computer science",Conference Paper,Scopus,2-s2.0-84887420920
"Testylier R., Dang T.","NLTOOLBOX: A library for reachability computation of nonlinear dynamical systems",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-319-02444-8_37,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887497173&doi=10.1007%2f978-3-319-02444-8_37&partnerID=40&md5=05df9ad7db9375bfe4f95f816bbb3fcf","We describe NLTOOLBOX, a library of data structures and algorithms for reachability computation of nonlinear dynamical systems. It provides the users with an easy way to ""program"" their own analysis procedures or to solve other problems beyond verification. We illustrate the use of the library for the analysis of a biological model. © 2013 Springer International Publishing.",,"Biological modeling; Reachability; Artificial intelligence; Computer science; Nonlinear dynamical systems",Conference Paper,Scopus,2-s2.0-84887497173
"Biswas S., Kundu S., Bose D., Das S., Suganthan P.N., Panigrahi B.K.","Migrating forager population in a multi-population Artificial Bee Colony algorithm with modified perturbation schemes",2013,"Proceedings of the 2013 IEEE Symposium on Swarm Intelligence, SIS 2013 - 2013 IEEE Symposium Series on Computational Intelligence, SSCI 2013",13,10.1109/SIS.2013.6615186,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886773921&doi=10.1109%2fSIS.2013.6615186&partnerID=40&md5=20b6358316981061936380ac094d4e40","Swarm Intelligent algorithms focus on imbibing the collective intelligence of a group of simple agents that can work together as a unit. This research article focus on a recently proposed swarm-based metaheuristic called the Artificial Bee Colony (ABC) algorithm and suggests modifications to the algorithmic framework in order to enhance its performance. The proposed ABC variant shall be referred to as MsABC-Fm (Multi swarm Artificial Bee Colony with Forager migration). MsABC-Fm maintains multiple swarm populations that apply different perturbation strategies and gradually migration of the population from worse performing strategy to the better mode of perturbation is promoted. To evaluate the performance of the algorithm, we conduct comparative study involving 8 algorithms and test the problems on 25 benchmark problems proposed in the Special Session on IEEE Congress on Evolutionary Competition 2005. The superiority of the MsABC-Fm approach is also highlighted statistically. © 2013 IEEE.","Artificial Bee Colony; migration; multi-population; strategy; Swarm Intelligence","Artificial bee colonies; migration; Multi population; strategy; Swarm Intelligence; Artificial intelligence; Evolutionary algorithms",Conference Paper,Scopus,2-s2.0-84886773921
"Huang H.-C.","SoPC-based parallel ACO algorithm and its application to optimal motion controller design for intelligent omnidirectional mobile robots",2013,"IEEE Transactions on Industrial Informatics",13,10.1109/TII.2012.2222033,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886667078&doi=10.1109%2fTII.2012.2222033&partnerID=40&md5=f6e32d28b907b9ee2ed25c16b12aff89","This paper presents an embedded system-on-a-programmable-chip (SoPC)-based parallel ant colony optimization (ACO) algorithm and its application to optimal motion controller design for intelligent omnidirectional mobile robots. Both polar-space parallel ACO (PACO) parameter tuner and kinematic motion controller are integrated in one field programmable gate arrays chip to efficiently construct an intelligent omnidirectional mobile robot. The optimal parameters of the motion controllers are obtained by minimizing the performance index using the proposed SoPC-based PACO algorithm. These polar-space optimal parameters are then employed in the PACO-based embedded kinematic motion controller to achieve trajectory tracking and stabilization. Experimental results are conducted to show the effectiveness and merit of the proposed PACO algorithm for embedded controllers of omnidirectional mobile robots. These results indicate that the proposed PACO-based embedded optimal controller outperforms the nonoptimal controllers and the conventional genetic algorithm optimal controllers in polar coordinates. © 2005-2012 IEEE.","Ant colony optimization (ACO); kinematic; mobile robot; optimization; polar-space","Ant Colony Optimization (ACO); Embedded controllers; Omnidirectional mobile robot; Parallel ant colony optimization; Performance indices; polar-space; System on a programmable chips; Trajectory tracking; Ant colony optimization; Artificial intelligence; Field programmable gate arrays (FPGA); Kinematics; Mobile robots; Motion control; Optimization; Tracking (position); Algorithms",Article,Scopus,2-s2.0-84886667078
"Goodswen S.J., Kennedy P.J., Ellis J.T.","A novel strategy for classifying the output from an in silico vaccine discovery pipeline for eukaryotic pathogens using machine learning algorithms",2013,"BMC Bioinformatics",13,10.1186/1471-2105-14-315,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886737285&doi=10.1186%2f1471-2105-14-315&partnerID=40&md5=484f59330fd42dc336fe4fa130f1ad70","Background: An in silico vaccine discovery pipeline for eukaryotic pathogens typically consists of several computational tools to predict protein characteristics. The aim of the in silico approach to discovering subunit vaccines is to use predicted characteristics to identify proteins which are worthy of laboratory investigation. A major challenge is that these predictions are inherent with hidden inaccuracies and contradictions. This study focuses on how to reduce the number of false candidates using machine learning algorithms rather than relying on expensive laboratory validation. Proteins from Toxoplasma gondii, Plasmodium sp., and Caenorhabditis elegans were used as training and test datasets.Results: The results show that machine learning algorithms can effectively distinguish expected true from expected false vaccine candidates (with an average sensitivity and specificity of 0.97 and 0.98 respectively), for proteins observed to induce immune responses experimentally.Conclusions: Vaccine candidates from an in silico approach can only be truly validated in a laboratory. Given any in silico output and appropriate training data, the number of false candidates allocated for validation can be dramatically reduced using a pool of machine learning algorithms. This will ultimately save time and money in the laboratory. © 2013 Goodswen et al.; licensee BioMed Central Ltd.",,"Average sensitivities; Caenorhabditis elegans; Computational tools; Immune response; Laboratory investigations; Novel strategies; Subunit vaccine; Toxoplasma gondii; Laboratories; Learning systems; Pipelines; Proteins; Vaccines; Learning algorithms; Caenorhabditis elegans; Eukaryota; Plasmodium (Apicomplexa); Plasmodium sp.; Toxoplasma gondii; antigen; Caenorhabditis elegans protein; protozoal protein; vaccine; algorithm; animal; article; artificial intelligence; biology; chemistry; computer simulation; drug development; immunology; methodology; sensitivity and specificity; Algorithms; Animals; Antigens; Artificial Intelligence; Caenorhabditis elegans Proteins; Computational Biology; Computer Simulation; Drug Discovery; Protozoan Proteins; Sensitivity and Specificity; Vaccines",Article,Scopus,2-s2.0-84886737285
"Sarnovsky M., Ulbrik Z.","Cloud-based clustering of text documents using the GHSOM algorithm on the GridGain platform",2013,"SACI 2013 - 8th IEEE International Symposium on Applied Computational Intelligence and Informatics, Proceedings",13,10.1109/SACI.2013.6608988,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886464892&doi=10.1109%2fSACI.2013.6608988&partnerID=40&md5=fbc9b15a92fe9a10eadc5ae7f493d1be","This paper provides an overview of our research activities aimed on efficient use of distributed computing concepts for text-mining tasks. Work presented within this paper describes the GHSOM (Growing Hierarchical Self-Organizing Maps) algorithm for clustering of text documents and proposes the design and implementation of distributed version of this approach. Proposed implementation is based on JBOWL framework as a base for text mining. For distribution we used MapReduce paradigm implemented within the GridGain framework, which was used as a cloud application platform. Experiments were performed on standard Reuters dataset and for testing purposes we decided to use a simple private cloud infrastructure. © 2013 IEEE.",,"Cloud applications; Cloud-based; Design and implementations; Growing hierarchical self-organizing maps; Private clouds; Research activities; Text document; Text-mining; Artificial intelligence; Data mining; Information science; Statistical tests; Clustering algorithms",Conference Paper,Scopus,2-s2.0-84886464892
"Mnerie C.A., Preitl S., Duma V.-F.","Performance enhancement of galvanometer scanners using extended control structures",2013,"SACI 2013 - 8th IEEE International Symposium on Applied Computational Intelligence and Informatics, Proceedings",13,10.1109/SACI.2013.6608952,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886521949&doi=10.1109%2fSACI.2013.6608952&partnerID=40&md5=4fbfb3cab1e43fd42f5dda8366765c75","The galvanometer-based scanners (GS) are optomechatronic devices with a wide range of applications, from industrial to biomedical imaging. They are driven with periodical and time variable signals - especially sawtooth, triangular, sine wave or signals with special variations. We approach in this paper the rejection of the effect of disturbances in order to improve the tracking performances of the GS. The control solution proposed in the paper is based on the extension of the existing control structure with an additional loop, with an additional controller PID-Ll type, which will ensure better speed response and good immunity to constant disturbances which can affect the servo system. © 2013 IEEE.","extended control structure; galvanometer scanners; intelligent mechatronic system; performance enhancement; PID-Ll controller","Biomedical imaging; Constant disturbances; Control solutions; Control structure; Galvanometer scanner; Performance enhancements; Speed response; Tracking performance; Artificial intelligence; Information science; Intelligent mechatronics; Medical imaging; Scanning",Conference Paper,Scopus,2-s2.0-84886521949
"Al-Numair N.S., Martin A.C.","The SAAP pipeline and database: tools to analyze the impact and predict the pathogenicity of mutations.",2013,"BMC genomics",13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886315107&partnerID=40&md5=9023b38f5a1181d1b11d85346060f644","Understanding and predicting the effects of mutations on protein structure and phenotype is an increasingly important area. Genes for many genetically linked diseases are now routinely sequenced in the clinic. Previously we focused on understanding the structural effects of mutations, creating the SAAPdb resource. We have updated SAAPdb to include 41% more SNPs and 36% more PDs. Introducing a hydrophobic residue on the surface, or a hydrophilic residue in the core, no longer shows significant differences between SNPs and PDs. We have improved some of the analyses significantly enhancing the analysis of clashes and of mutations to-proline and from-glycine. A new web interface has been developed allowing users to analyze their own mutations. Finally we have developed a machine learning method which gives a cross-validated accuracy of 0.846, considerably out-performing well known methods including SIFT and PolyPhen2 which give accuracies between 0.690 and 0.785. We have updated SAAPdb and improved its analyses, but with the increasing rate with which mutation data are generated, we have created a new analysis pipeline and web interface. Results of machine learning using the structural analysis results to predict pathogenicity considerably outperform other methods.",,"protein; amino acid substitution; article; artificial intelligence; biology; computer program; genetic disorder; genetics; human; Internet; methodology; mutation; phenotype; protein conformation; single nucleotide polymorphism; Amino Acid Substitution; Artificial Intelligence; Computational Biology; Genetic Diseases, Inborn; Humans; Internet; Mutation; Phenotype; Polymorphism, Single Nucleotide; Protein Conformation; Proteins; Software",Article,Scopus,2-s2.0-84886315107
"Yu W., Fu X., Blasch E., Pham K., Shen D., Chen G., Lu C.","On effectiveness of hopping-based spread spectrum techniques for network forensic traceback",2013,"SNPD 2013 - 14th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing",13,10.1109/SNPD.2013.75,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885988528&doi=10.1109%2fSNPD.2013.75&partnerID=40&md5=11288bb825a5eddb651a1a2f3690863f","Network-based crime has been increasing in both extent and severity and network-based forensics encapsulates an essential part of legal surveillance. A key network forensics tool is trace back, which can be used to identify true sources of suspects. Both accuracy and secrecy are essential attributes of a successful forensic trace back. In this paper, we present a class of hopping based spread-spectrum techniques for forensic trace back, which fully use the benefits of the spread spectrum approach and preserves a greater degree of secrecy. Our proposed techniques, including Code Hopping-Direct Sequence Spread Spectrum (CHDSSS), Frequency Hopping-Direct Sequence Spread Spectrum (FH-DSSS), and Time Hopping-Spread Spectrum (TH-DSSS), operate to randomize the effects of marking traffic through both the time and frequency domains. Our simulation study validates these techniques in terms of accuracy and secrecy. © 2013 IEEE.","Accuracy; DSSS; Hopping; Secrecy; Traceback","Accuracy; DSSS; Hopping; Secrecy; Traceback; Artificial intelligence; Frequency hopping; Spectroscopy; Software engineering",Conference Paper,Scopus,2-s2.0-84885988528
"Martin T.P., Azvine B.","The X-mu approach: Fuzzy quantities, fuzzy arithmetic and fuzzy association rules",2013,"Proceedings of the 2013 IEEE Symposium on Foundations of Computational Intelligence, FOCI 2013 - 2013 IEEE Symposium Series on Computational Intelligence, SSCI 2013",13,10.1109/FOCI.2013.6602451,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885229493&doi=10.1109%2fFOCI.2013.6602451&partnerID=40&md5=66fe10e402a9d46a33a1f2f8de258f59","The use of so-called fuzzy numbers for approximate calculations leads to significant problems, because the underlying mathematical structure is weaker than ordinary arithmetic. Many of these problems arise from the fact that the fuzzy quantities are actually fuzzy intervals. Gradual numbers were recently proposed as a better representation for fuzzy quantities. In this paper, we describe the X-μ approach, a new method of visualizing and calculating functions of fuzzy quantities. In particular, we illustrate the calculation of fuzzy association confidence in cases where membership can be represented by a function or a table of values. © 2013 IEEE.","eradual elements; fuzzy numbers; fuzzy quantities; tuzzv association rules; X-mu method","Approximate calculations; Fuzzy arithmetics; Fuzzy association rule; Fuzzy associations; Fuzzy numbers; fuzzy quantities; Mathematical structure; X-mu method; Association rules; Fuzzy rules; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84885229493
"Gottron T., Knauf M., Scheglmann S., Scherp A.","A systematic investigation of explicit and implicit schema information on the linked open data cloud",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-642-38288-8-16,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885004251&doi=10.1007%2f978-3-642-38288-8-16&partnerID=40&md5=59f5e59d2441510eb0b18edd9e3fbc9d","Schema information about resources in the Linked Open Data (LOD) cloud can be provided in a twofold way: it can be explicitly defined by attaching RDF types to the resources. Or it is provided implicitly via the definition of the resources' properties. In this paper, we present a method and metrics to analyse the information theoretic properties and the correlation between the two manifestations of schema information. Furthermore, we actually perform such an analysis on large-scale linked data sets. To this end, we have extracted schema information regarding the types and properties defined in the data set segments provided for the Billion Triples Challenge 2012. We have conducted an in depth analysis and have computed various entropy measures as well as the mutual information encoded in the two types of schema information. Our analysis provides insights into the information encoded in the different schema characteristics. Two major findings are that implicit schema information is far more discriminative and that applications involving schema information based on either types or properties alone will only capture between 63.5% and 88.1% of the schema information contained in the data. Based on these observations, we derive conclusions about the design of future schemas for LOD as well as potential application scenarios. © 2013 Springer-Verlag Berlin Heidelberg.",,"Application scenario; Entropy measure; Implicit schemata; In-depth analysis; Linked open data (LOD); Linked open datum; Mutual informations; Schema information; Artificial intelligence; Computer science",Conference Paper,Scopus,2-s2.0-84885004251
"Bereta K., Smeros P., Koubarakis M.","Representation and querying of valid time of triples in linked geospatial data",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-642-38288-8-18,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881219559&doi=10.1007%2f978-3-642-38288-8-18&partnerID=40&md5=d2c62dea8d0e9dc69beab1acbe48f76f","We introduce the temporal component of the stRDF data model and the stSPARQL query language, which have been recently proposed for the representation and querying of linked geospatial data that changes over time. With this temporal component in place, stSPARQL becomes a very expressive query language for linked geospatial data, going beyond the recent OGC standard GeoSPARQL, which has no support for valid time of triples. We present the implementation of the stSPARQL temporal component in the system Strabon, and study its performance experimentally. Strabon is shown to outperform all the systems it has been compared with. © 2013 Springer-Verlag Berlin Heidelberg.",,"Geo-spatial data; OGC Standards; Artificial intelligence; Computer science; Query languages",Conference Paper,Scopus,2-s2.0-84881219559
"Chen J., Ma Z., Liu Y.","Local coordinates alignment with global preservation for dimensionality reduction",2013,"IEEE Transactions on Neural Networks and Learning Systems",13,10.1109/TNNLS.2012.2225844,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884936397&doi=10.1109%2fTNNLS.2012.2225844&partnerID=40&md5=b9044c84dae1e826afc40c610c4cbbae","Dimensionality reduction is vital in many fields, and alignment-based methods for nonlinear dimensionality reduction have become popular recently because they can map the high-dimensional data into a low-dimensional subspace with the property of local isometry. However, the relationships between patches in original high-dimensional space cannot be ensured to be fully preserved during the alignment process. In this paper, we propose a novel method for nonlinear dimensionality reduction called local coordinates alignment with global preservation. We first introduce a reasonable definition of topology-preserving landmarks (TPLs), which not only contribute to preserving the global structure of datasets and constructing a collection of overlapping linear patches, but they also ensure that the right landmark is allocated to the new test point. Then, an existing method for dimensionality reduction that has good performance in preserving the global structure is used to derive the low-dimensional coordinates of TPLs. Local coordinates of each patch are derived using tangent space of the manifold at the corresponding landmark, and then these local coordinates are aligned into a global coordinate space with the set of landmarks in low-dimensional space as reference points. The proposed alignment method, called landmarks-based alignment, can produce a closed-form solution without any constraints, while most previous alignment-based methods impose the unit covariance constraint, which will result in the deficiency of global metrics and undesired rescaling of the manifold. Experiments on both synthetic and real-world datasets demonstrate the effectiveness of the proposed algorithm. © 2012 IEEE.","Isometric mapping; Manifold learning; Nonlinear dimensionality reduction; Tangent space","Dimensionality reduction; High dimensional spaces; Isometric mapping; Low-dimensional spaces; Low-dimensional subspace; Manifold learning; Nonlinear dimensionality reduction; Tangent space; Artificial intelligence; Computer networks; Nonlinear analysis",Article,Scopus,2-s2.0-84884936397
"Wissel T., Pfeiffer T., Frysch R., Knight R.T., Chang E.F., Hinrichs H., Rieger J.W., Rose G.","Hidden Markov model and support vector machine based decoding of finger movements using electrocorticography",2013,"Journal of Neural Engineering",13,10.1088/1741-2560/10/5/056020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885465701&doi=10.1088%2f1741-2560%2f10%2f5%2f056020&partnerID=40&md5=3728ad5ad4d37217b44a22b9c6a1f953","Objective. Support vector machines (SVM) have developed into a gold standard for accurate classification in brain-computer interfaces (BCI). The choice of the most appropriate classifier for a particular application depends on several characteristics in addition to decoding accuracy. Here we investigate the implementation of hidden Markov models (HMM) for online BCIs and discuss strategies to improve their performance. Approach. We compare the SVM, serving as a reference, and HMMs for classifying discrete finger movements obtained from electrocorticograms of four subjects performing a finger tapping experiment. The classifier decisions are based on a subset of low-frequency time domain and high gamma oscillation features. Main results. We show that decoding optimization between the two approaches is due to the way features are extracted and selected and less dependent on the classifier. An additional gain in HMM performance of up to 6% was obtained by introducing model constraints. Comparable accuracies of up to 90% were achieved with both SVM and HMM with the high gamma cortical response providing the most important decoding information for both techniques. Significance. We discuss technical HMM characteristics and adaptations in the context of the presented data as well as for general BCI applications. Our findings suggest that HMMs and their characteristics are promising for efficient online BCIs. © 2013 IOP Publishing Ltd.",,"Classifier decisions; Cortical response; Decoding information; Electrocorticography; Finger movements; Gamma oscillations; Gold standards; Model constraints; Brain; Brain computer interface; Decoding; Hidden Markov models; Support vector machines; adult; algorithm; article; brain computer interface; electric potential; electrocorticography; epileptic discharge; finger; Fourier transformation; hidden Markov model; human; human experiment; information processing; male; motor cortex; priority journal; reaction time; support vector machine; Adolescent; Adult; Algorithms; Artificial Intelligence; Brain-Computer Interfaces; Data Interpretation, Statistical; Electrodes; Electroencephalography; Fingers; Humans; Male; Markov Chains; Models, Neurological; Movement; Support Vector Machines; Young Adult",Article,Scopus,2-s2.0-84885465701
"Ye G., Liu Y., Deng Y., Hasler N., Ji X., Dai Q., Theobalt C.","Free-viewpoint video of human actors using multiple handheld kinects",2013,"IEEE Transactions on Cybernetics",13,10.1109/TCYB.2013.2272321,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890357691&doi=10.1109%2fTCYB.2013.2272321&partnerID=40&md5=16113ec2ccdcd215cc9ae4fe06403723","We present an algorithm for creating free-viewpoint video of interacting humans using three handheld Kinect cameras. Our method reconstructs deforming surface geometry and temporal varying texture of humans through estimation of human poses and camera poses for every time step of the RGBZ video. Skeletal configurations and camera poses are found by solving a joint energy minimization problem, which optimizes the alignment of RGBZ data from all cameras, as well as the alignment of human shape templates to the Kinect data. The energy function is based on a combination of geometric correspondence finding, implicit scene segmentation, and correspondence finding using image features. Finally, texture recovery is achieved through jointly optimization on spatio-temporal RGB data using matrix completion. As opposed to previous methods, our algorithm succeeds on free-viewpoint video of human actors under general uncontrolled indoor scenes with potentially dynamic background, and it succeeds even if the cameras are moving. © 2013 IEEE.","Free-viewpoint video; Handheld kinect; Markerless performance capture","Dynamic background; Energy minimization problem; Free-viewpoint video; Handhelds; Jointly optimizations; Performance capture; Scene segmentation; Surface geometries; Algorithms; Image segmentation; Textures; Cameras; actimetry; algorithm; article; artificial intelligence; automated pattern recognition; computer; computer simulation; equipment; human; image enhancement; methodology; recreation; three dimensional imaging; transducer; videorecording; whole body imaging; actimetry; automated pattern recognition; devices; procedures; three dimensional imaging; videorecording; whole body imaging; Actigraphy; Algorithms; Artificial Intelligence; Computer Peripherals; Computer Simulation; Humans; Image Enhancement; Imaging, Three-Dimensional; Pattern Recognition, Automated; Transducers; Video Games; Video Recording; Whole Body Imaging; Actigraphy; Algorithms; Artificial Intelligence; Computer Peripherals; Computer Simulation; Humans; Image Enhancement; Imaging, Three-Dimensional; Pattern Recognition, Automated; Transducers; Video Games; Video Recording; Whole Body Imaging",Article,Scopus,2-s2.0-84890357691
"Muduli P.K., Das M.R., Samui P., Kumar Das S.","Uplift Capacity of Suction Caisson in Clay Using Artificial Intelligence Techniques",2013,"Marine Georesources and Geotechnology",13,10.1080/1064119X.2012.690827,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880021029&doi=10.1080%2f1064119X.2012.690827&partnerID=40&md5=35cd36314f3c9761422ad50ce223e0bf","Caisson anchor is the most common structure for the offshore structures. In this study artificial intelligence techniques artificial neural network (ANN), genetic programming (GP), support vector machine (SVM), and relevance vector machine (RVM) have been used to predict the uplift capacity (Q) of suction caisson in clay. The model inputs included the L/d (L is the embedded length of the caisson and d is the diameter of caisson), undrained shear strength of soil at the depth of the caisson tip (Su), D/L (D is the depth of the load application point from the soil surface), inclined angle(θ) and load rate parameter (Tk). A comparative analysis is made with the results of the above listed artificial intelligence techniques based on different statistical performance criteria like correlation coefficient (R), root mean square error (RMSE), Nash-Sutcliff coefficient of efficiency (E), log normal distribution of ratio of predicted to observed load capacity. Model equations based on the above techniques are discussed and a sensitivity analysis is made to identify the important parameters contributing to the uplift load capacity of caisson. © 2013 Copyright Taylor and Francis Group, LLC.","artificial neural network; caisson; genetic programming; relevance vector machine; support vector machine; uplift capacity","Artificial intelligence techniques; Coefficient of efficiencies; Correlation coefficient; Log-normal distribution; Relevance Vector Machine; Statistical performance; Undrained shear strength; Uplift capacity; Caissons; Foundations; Genetic programming; Hydraulic structures; Mean square error; Neural networks; Normal distribution; Offshore structures; Soil mechanics; Support vector machines; Pressure vessels; artificial intelligence; artificial neural network; caisson; clay soil; error analysis; genetic algorithm; loading; offshore structure; shear strength; soil depth; soil mechanics; suction; uplift",Article,Scopus,2-s2.0-84880021029
"Thapaliya K., Pyun J.-Y., Park C.-S., Kwon G.-R.","Level set method with automatic selective local statistics for brain tumor segmentation in MR images",2013,"Computerized Medical Imaging and Graphics",13,10.1016/j.compmedimag.2013.05.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888070479&doi=10.1016%2fj.compmedimag.2013.05.003&partnerID=40&md5=722fb87e8ee3295da7a5ae15e9959013","The level set approach is a powerful tool for segmenting images. This paper proposes a method for segmenting brain tumor images from MR images. A new signed pressure function (SPF) that can efficiently stop the contours at weak or blurred edges is introduced. The local statistics of the different objects present in the MR images were calculated. Using local statistics, the tumor objects were identified among different objects. In this level set method, the calculation of the parameters is a challenging task. The calculations of different parameters for different types of images were automatic. The basic thresholding value was updated and adjusted automatically for different MR images. This thresholding value was used to calculate the different parameters in the proposed algorithm. The proposed algorithm was tested on the magnetic resonance images of the brain for tumor segmentation and its performance was evaluated visually and quantitatively. Numerical experiments on some brain tumor images highlighted the efficiency and robustness of this method. © 2013.","Active contours; Chan-Vese model; Geodesic active contours; Image segmentation; Level set method; MR images","Active contours; Chan-Vese model; Geodesic Active Contour; Level Set method; MR images; Algorithms; Brain; Drop breakup; Image segmentation; Level measurement; Numerical methods; Tumors; Magnetic resonance imaging; algorithm; article; brain tumor; image analysis; image segmentation; imaging and display; information processing; mathematical model; nuclear magnetic resonance; priority journal; radiological parameters; signed pressure function; statistics; Active contours; Chan-Vese model; Geodesic active contours; Image segmentation; Level set method; MR images; Algorithms; Artificial Intelligence; Brain Neoplasms; Computer Simulation; Data Interpretation, Statistical; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Models, Biological; Models, Statistical; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84888070479
"Amer A., Maruyama N., Pericàs M., Taura K., Yokota R., Matsuoka S.","Fork-join and data-driven execution models on multi-core architectures: Case study of the FMM",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-642-38750-0_19,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884470402&doi=10.1007%2f978-3-642-38750-0_19&partnerID=40&md5=436eb961081ae5d04c6f29d3e3cc7348","Extracting maximum performance of multi-core architectures is a difficult task primarily due to bandwidth limitations of the memory subsystem and its complex hierarchy. In this work, we study the implications of fork-join and data-driven execution models on this type of architecture at the level of task parallelism. For this purpose, we use a highly optimized fork-join based implementation of the FMM and extend it to a data-driven implementation using a distributed task scheduling approach. This study exposes some limitations of the conventional fork-join implementation in terms of synchronization overheads. We find that these are not negligible and their elimination by the data-driven method, with a careful data locality strategy, was beneficial. Experimental evaluation of both methods on state-of-the-art multi-socket multi-core architectures showed up to 22% speed-ups of the data-driven approach compared to the original method. We demonstrate that a data-driven execution of FMM not only improves performance by avoiding global synchronization overheads but also reduces the memory-bandwidth pressure caused by memory-intensive computations. © 2013 Springer-Verlag.",,"Bandwidth limitation; Complex hierarchy; Data-driven approach; Data-driven methods; Distributed tasks; Experimental evaluation; Global synchronization; Multicore architectures; Artificial intelligence; Computer science; Computer architecture",Conference Paper,Scopus,2-s2.0-84884470402
"Dudek-Dyduch E., Dutkiewicz L.","Substitution tasks method for discrete optimization",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-642-38610-7_39,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881581976&doi=10.1007%2f978-3-642-38610-7_39&partnerID=40&md5=b6bf4e10217fa547e3b0f67206a296ad","The aim of the paper is to present a novel heuristic optimization method for discrete dynamic optimization problems. The method has been named substitution tasks method (ST method). According to the method, a solution is generated by means of sequence of dynamically created local optimization tasks so-called substitution tasks. The method is based on formal algebraic-logical meta model of multistage decision process (ALMM of MDP), that is given in the paper. The paper presents a formal approach for designing constructive algorithms that are based on the method. A general idea of creating substitution tasks for different optimization problems is given. Then creation of substitution tasks, based on automatic analisys of set of non-admissible states is proposed. To illustrate the presented ideas, a scheduling algorithm for a particular NP-hard problem is given and results of computer experiments are presented. © 2013 Springer-Verlag.","algebraic-logical meta model; discrete dynamic optimization; multistage decision process; scheduling problem; substitution tasks method","Constructive algorithms; Discrete dynamics; Discrete optimization; Heuristic optimization method; Meta model; Multi-stage decision process; Optimization problems; Scheduling problem; Algorithms; Artificial intelligence; Computational complexity; Optimization; Soft computing; Algebra",Conference Paper,Scopus,2-s2.0-84881581976
"Vaz Jr. M., Cardoso E.L., Stahlschmidt J.","Particle swarm optimization and identification of inelastic material parameters",2013,"Engineering Computations (Swansea, Wales)",13,10.1108/EC-10-2011-0118,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884229087&doi=10.1108%2fEC-10-2011-0118&partnerID=40&md5=c4910b6e491b5cc635a1f3e5a3cae19e","Purpose - Parameter identification is a technique which aims at determining material or other process parameters based on a combination of experimental and numerical techniques. In recent years, heuristic approaches, such as genetic algorithms (GAs), have been proposed as possible alternatives to classical identification procedures. The present work shows that particle swarm optimization (PSO), as an example of such methods, is also appropriate to identification of inelastic parameters. The paper aims to discuss these issues. Design/methodology/approach - PSO is a class of swarm intelligence algorithms which attempts to reproduce the social behaviour of a generic population. In parameter identification, each individual particle is associated to hyper-coordinates in the search space, corresponding to a set of material parameters, upon which velocity operators with random components are applied, leading the particles to cluster together at convergence. Findings - PSO has proved to be a viable alternative to identification of inelastic parameters owing to its robustness (achieving the global minimum with high tolerance for variations of the population size and control parameters), and, contrasting to GAs, higher convergence rate and small number of control variables. Originality/value - PSO has been mostly applied to electrical and industrial engineering. This paper extends the field of application of the method to identification of inelastic material parameters. Copyright © 2013 Emerald Group Publishing Limited. All rights reserved.","Genetic algorithms; Optimization techniques; Parameter identification; Particle swarm optimization","Design/methodology/approach; Genetic algorithm (GAs); Identification procedure; Individual particles; Inelastic materials; Numerical techniques; Optimization techniques; Swarm intelligence algorithms; Artificial intelligence; Genetic algorithms; Heuristic methods; Parameter estimation; Particle swarm optimization (PSO); Population statistics; Identification (control systems)",Article,Scopus,2-s2.0-84884229087
"Černý M., Antoch J., Hladík M.","On the possibilistic approach to linear regression models involving uncertain, indeterminate or interval data",2013,"Information Sciences",13,10.1016/j.ins.2013.04.035,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878923441&doi=10.1016%2fj.ins.2013.04.035&partnerID=40&md5=a8c1bf1cc02d138e40cfdce1e2af804a","We consider linear regression models where both input data (the observations of independent variables) and output data (the observations of the dependent variable) are affected by loss of information caused by uncertainty, indeterminacy, rounding or censoring. Instead of real-valued (crisp) data, only intervals are available. We study a possibilistic generalization of the least squares estimator, so called OLS-set for the interval model. Investigation of the OLS-set allows us to quantify whether the replacement of real-valued (crisp) data by interval values can have a significant impact on our knowledge of the value of the OLS estimator. We show that in the general case, very elementary questions about properties of the OLS-set are computationally intractable (assuming P ≠ NP). We also focus on restricted versions of the general interval linear regression model to the crisp input case. Taking the advantage of the fact that in the crisp input - interval output model the OLS-set is a zonotope, we design both exact and approximate methods for its description. We also discuss special cases of the regression model, e.g. a model with repeated observations. © 2013 Elsevier Inc. All rights reserved.","Computational complexity; Interval data; Possibilistic regression; Uncertain data","Approximate methods; Independent variables; Interval data; Least-squares estimator; Linear regression models; Possibilistic approach; Possibilistic regressions; Uncertain datas; Artificial intelligence; Computational complexity; Software engineering; Linear regression",Article,Scopus,2-s2.0-84878923441
"Prasanna P., Jain S., Bhagat N., Madabhushi A.","Decision support system for detection of diabetic retinopathy using smartphones",2013,"Proceedings of the 2013 7th International Conference on Pervasive Computing Technologies for Healthcare and Workshops, PervasiveHealth 2013",13,10.4108/icst.pervasivehealth.2013.252093,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883059965&doi=10.4108%2ficst.pervasivehealth.2013.252093&partnerID=40&md5=9e41d55aaeb49ab995ab048691276ad9","Certain retinal disorders, if not detected in time, can be serious enough to cause blindness in patients. This paper proposes a low-cost and portable smartphone-based decision support system for initial screening of diabetic retinopathy using sophisticated image analysis and machine learning techniques. It requires a smartphone to be attached to a direct hand-held ophthalmoscope. The phone is used to capture fundus images as seen through the direct ophthalmoscope. We deploy pattern recognition on the captured images to develop a classifier that distinguishes normal images from those with retinal abnormalities. The algorithm performance is characterized by testing on an existing database. We were able to diagnose conditions with an average sensitivity of 86%. Our system has been designed to be used by ophthalmologists, general practitioners, emergency room physicians, and other health care personnel alike. The emphasis of this paper is not only on devising a detection algorithm for diabetic retinopathy, but more so on the development and utility of a novel system for diagnosis. Through this mobile eye-examination system, we envision making the early screening of diabetic retinopathy accessible, especially to rural regions in developing countries, where dedicated ophthalmology centers are expensive, and to alleviate detection in early stages. © 2013 ICST.","Diabetic Retinopathy; Image Processing; Mobile System; Pattern Recognition; Retinal Diseases","Algorithm performance; Average sensitivities; Detection algorithm; Diabetic retinopathy; General practitioners; Machine learning techniques; Mobile systems; Retinal disease; Artificial intelligence; Decision support systems; Developing countries; Diagnosis; Health care; Image processing; Learning systems; Ophthalmology; Pattern recognition; Smartphones; Ubiquitous computing; Eye protection",Conference Paper,Scopus,2-s2.0-84883059965
"Borrego D., Eshuis R., Gómez-López M.T., Gasca R.M.","Diagnosing correctness of semantic workflow models",2013,"Data and Knowledge Engineering",13,10.1016/j.datak.2013.04.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884979158&doi=10.1016%2fj.datak.2013.04.008&partnerID=40&md5=43601f69c8b6e677805a7bac5e13c1d8","To model operational business processes in an accurate way, workflow models need to reference both the control flow and dataflow perspectives. Checking the correctness of such workflow models and giving precise feedback in case of errors is challenging due to the interplay between these different perspectives. In this paper, we propose a fully automated approach for diagnosing correctness of semantic workflow models in which the semantics of activities are specified with pre and postconditions. The control flow and dataflow perspectives of a semantic workflow are modeled in an integrated way using Artificial Intelligence techniques (Integer Programming and Constraint Programming). The approach has been implemented in the DiagFlow tool, which reads and diagnoses annotated XPDL models, using a state-of-the-art constraint solver as back end. Using this novel approach, complex semantic workflow models can be verified and diagnosed in an efficient way. © 2013 Elsevier B.V. All rights reserved.","Business process management; Constraint programming; Diagnosis; Integer programming; Workflow","Artificial intelligence techniques; Business process management; Constraint programming; Constraint solvers; Fully automated; Operational business; Workflow; Workflow models; Artificial intelligence; Computer programming; Constraint theory; Diagnosis; Enterprise resource management; Integer programming; Model checking; Semantics; Data flow analysis",Article,Scopus,2-s2.0-84884979158
"Rana H., Thulasiraman P., Thulasiram R.K.","MAZACORNET: Mobility aware zone based ant colony optimization routing for VANET",2013,"2013 IEEE Congress on Evolutionary Computation, CEC 2013",13,10.1109/CEC.2013.6557928,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881599335&doi=10.1109%2fCEC.2013.6557928&partnerID=40&md5=ca65e8e02e40da35e6e70ad5eb58907a","Vehicular Ad hoc Networks (VANET) exhibit highly dynamic behavior with high mobility and random network topologies. The performance of Transmission Control Protocols (TCP) in such wireless ad hoc networks is plagued by a number of problems: frequent link failures, scalability, multi-hop data transmission and data loss. © 2013 IEEE.","Multi-Path; Swarm Intelligence; VANET; Zone","Dynamic behaviors; Frequent link failures; Mobility-aware; Multi-hop data transmissions; Multi-Path; Random network; Swarm Intelligence; VANET; Artificial intelligence; Electric network topology; Transmission control protocol; Wireless ad hoc networks; Zoning; Vehicular ad hoc networks",Conference Paper,Scopus,2-s2.0-84881599335
"Caraffini F., Iacca G., Neri F., Picinali L., Mininno E.","A CMA-ES super-fit scheme for the re-sampled inheritance search",2013,"2013 IEEE Congress on Evolutionary Computation, CEC 2013",13,10.1109/CEC.2013.6557692,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881575468&doi=10.1109%2fCEC.2013.6557692&partnerID=40&md5=c63d8581c5a008755ad68b435a43b650","The super-fit scheme, consisting of injecting an individual with high fitness into the initial population of an algorithm, has shown to be a simple and effective way to enhance the algorithmic performance of the population-based algorithm. Whether the super-fit individual is based on some prior knowledge on the optimization problem or is derived from an initial step of pre-processing, e.g. a local search, this mechanism has been applied successfully in various examples of evolutionary and swarm intelligence algorithms. This paper presents an unconventional application of this super-fit scheme, where the super-fit individual is obtained by means of the Covariance Adaptation Matrix Evolution Strategy (CMA-ES), and fed to a single solution local search which perturbs iteratively each variable. Thus, compared to other super-fit schemes, the roles of super-fit individual generator and global optimizer are switched. To prevent premature convergence, the local search employs a re-sampling mechanism which inherits parts of the best individual while randomly sampling the remaining variables. We refer to such local search as Re-sampled Inheritance Search (RIS). Tested on the CEC 2013 optimization benchmark, the proposed algorithm, named CMA-ES-RIS, displays a respectable performance and a good balance between exploration and exploitation, resulting into a versatile and robust optimization tool. © 2013 IEEE.",,"Evolution strategies; Exploration and exploitation; Initial population; Optimization problems; Population-based algorithm; Pre-mature convergences; Robust optimization; Swarm intelligence algorithms; Artificial intelligence; Benchmarking; Covariance matrix; Evolutionary algorithms; Interlocking signals; Optimization; Iterative methods",Conference Paper,Scopus,2-s2.0-84881575468
"Ganty P., Genaim S.","Proving termination starting from the end",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-642-39799-8_27,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881180102&doi=10.1007%2f978-3-642-39799-8_27&partnerID=40&md5=b70c470db2a7c68b456fb38c69b83b34","We present a novel technique for proving program termination which introduces a new dimension of modularity. Existing techniques use the program to incrementally construct a termination proof. While the proof keeps changing, the program remains the same. Our technique goes a step further. We show how to use the current partial proof to partition the transition relation into those behaviors known to be terminating from the current proof, and those whose status (terminating or not) is not known yet. This partition enables a new and unexplored dimension of incremental reasoning on the program side. In addition, we show that our approach naturally applies to conditional termination which searches for a precondition ensuring termination. We further report on a prototype implementation that advances the state-of-the-art on the grounds of termination and conditional termination. © 2013 Springer-Verlag.",,"Incremental reasoning; New dimensions; Novel techniques; Program termination; Prototype implementations; Termination proof; Transition relations; Artificial intelligence; Computer science; Computer aided analysis",Conference Paper,Scopus,2-s2.0-84881180102
"Sutherland D.J., Póczos B., Schneider J.","Active learning and search on low-rank matrices",2013,"Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",13,10.1145/2487575.2487627,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977886950&doi=10.1145%2f2487575.2487627&partnerID=40&md5=bb34030f2a1fe8a4c11f8d3e4379cae6","Collaborative prediction is a powerful technique, useful in domains from recommender systems to guiding the scientific discovery process. Low-rank matrix factorization is one of the most powerful tools for collaborative prediction. This work presents a general approach for active collaborative prediction with the Probabilistic Matrix Factorization model. Using variational approximations or Markov chain Monte Carlo sampling to estimate the posterior distribution over models, we can choose query points to maximize our understanding of the model, to best predict unknown elements of the data matrix, or to find as many ""positive"" data points as possible. We evaluate our methods on simulated data, and also show their applicability to movie ratings prediction and the discovery of drug-Target interactions. Copyright © 2013 ACM.","Active learning; Active search; Coldstart; Collaborative filtering; Drug discovery; Matrix factorization; Recommender systems","Artificial intelligence; Collaborative filtering; Data mining; Education; Factorization; Forecasting; Markov processes; Monte Carlo methods; Query processing; Recommender systems; Active Learning; Active searches; Cold-start; Drug discovery; Matrix factorizations; Matrix algebra",Conference Paper,Scopus,2-s2.0-84977886950
"Cano A., Zafra A., Ventura S.","An interpretable classification rule mining algorithm",2013,"Information Sciences",13,10.1016/j.ins.2013.03.038,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877703416&doi=10.1016%2fj.ins.2013.03.038&partnerID=40&md5=9f999bdbc9c845bf3dfa20780e8f2791","Obtaining comprehensible classifiers may be as important as achieving high accuracy in many real-life applications such as knowledge discovery tools and decision support systems. This paper introduces an efficient Evolutionary Programming algorithm for solving classification problems by means of very interpretable and comprehensible IF-THEN classification rules. This algorithm, called the Interpretable Classification Rule Mining (ICRM) algorithm, is designed to maximize the comprehensibility of the classifier by minimizing the number of rules and the number of conditions. The evolutionary process is conducted to construct classification rules using only relevant attributes, avoiding noisy and redundant data information. The algorithm is evaluated and compared to nine other well-known classification techniques in 35 varied application domains. Experimental results are validated using several non-parametric statistical tests applied on multiple classification and interpretability metrics. The experiments show that the proposal obtains good results, improving significantly the interpretability measures over the rest of the algorithms, while achieving competitive accuracy. This is a significant advantage over other algorithms as it allows to obtain an accurate and very comprehensible classifier quickly. © 2013 Elsevier Inc. All rights reserved.","Classification; Evolutionary programming; Interpretability; Rule mining","Classification technique; Evolutionary programming algorithms; Interpretability; Minimizing the number of; Multiple Classification; Non-parametric statistical tests; Real-life applications; Rule mining; Artificial intelligence; Classification (of information); Computer programming; Decision support systems; Evolutionary algorithms; Statistical tests; Data mining",Article,Scopus,2-s2.0-84877703416
"Menghal P.M., Laxmi A.J.","Neural network based dynamic simulation of induction motor drive",2013,"Proceedings of 2013 International Conference on Power, Energy and Control, ICPEC 2013",13,10.1109/ICPEC.2013.6527722,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880889068&doi=10.1109%2fICPEC.2013.6527722&partnerID=40&md5=9c9b5ee9ebad0a4ac0ec5547713b26df","With the improvement in the technology of Microprocessor and Power Electronics, Induction motor drives with digital control have become more popular. Artificial intelligent controller (AIC) could be the best candidate for Induction Motor control. Over the last two decades researchers have been working to apply AIC for induction motor drives. This is because that AIC possesses advantages as compared to the conventional PI, PID and their adaptive versions. The main advantages are that the designs of these controllers do not depend on accurate system mathematical model and their performances are robust. In recent years, scientists and researchers have acquired significant development on various sorts of control theories and methods. Among these control technologies, intelligent control methods, which are generally regarded as the aggregation of Fuzzy Logic Control, Neural Network Control, Genetic Algorithm, and Expert System, have exhibited particular superiorities. The artificial neural network controller introduced to the system for keeping the motor speed to be constant when the load varies. The speed control scheme of vector controlle d induction motor drive involves decoupling of the speed and ref speed into torque and flux producing components. The performance of artificial neural network based controller's is compared with that of the conventional proportional integral controller. The dynamic modeling of Induction motor is done and the performance of the Induction motor drive has been analyzed for constant and variable loads. By using neuro controller the transient response of induction machine has been improved greatly and the dynamic response of the same has been made faster. © 2013 IEEE.","Artificial Intelligence (AI); Artificial Neural Network (ANN); Direct; Dynamic Simulation; PI Controller; Vector Control (VC)","Artificial neural network controllers; Control theories and methods; Conventional proportional integrals; Direct; Induction motor control; Neural network control; PI Controller; Vector controls; Computer simulation; Digital control systems; Dynamic response; Electric drives; Expert systems; Induction motors; Intelligent control; Mathematical models; Neural networks; Power electronics; Robust control; Controllers",Conference Paper,Scopus,2-s2.0-84880889068
"Matera M., Picozzi M., Pini M., Tonazzo M.","PEUDOM: A mashup platform for the end user development of common information spaces",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-642-39200-9_43,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880867338&doi=10.1007%2f978-3-642-39200-9_43&partnerID=40&md5=235e7d2021847c86cbd60c2c326a3255","This paper presents a Web platform for the user-driven, service-based creation of Common Information Spaces (CISs). Two composition environments, characterized by intuitive visual notations, enable i) the integration of services to create UI-rich components and ii) the synchronization of components into interactive workspaces. Collaborative features allow multiple users to collaborate, synchronously and asynchronously, to share and co-create CISs. © 2013 Springer-Verlag Berlin Heidelberg.","Collaborative Mashups; Common Information Spaces; End User Development","Common information spaces; End user development; Integration of services; Interactive workspace; Mashup platforms; Mashups; Multiple user; Visual notations; Artificial intelligence; Computer science",Conference Paper,Scopus,2-s2.0-84880867338
"Toossi M.T.B., Pourreza H.R., Zare H., Sigari M.-H., Layegh P., Azimi A.","An effective hair removal algorithm for dermoscopy images",2013,"Skin Research and Technology",13,10.1111/srt.12015,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880347146&doi=10.1111%2fsrt.12015&partnerID=40&md5=2ce26dd5354ccc809ec56e11543dec38","Background/purpose: Dermoscopy is one of the major imaging modalities used in the diagnosis of pigmented skin lesions. Due to the difficulty and subjectivity of human interpretation, computerized image analysis techniques have become important tools in this research area. Hair removal from skin lesion images is one of the key problems for the precise segmentation and analysis of the skin lesions. In this study, we present a new scheme that automatically detects and removes hairs from dermoscopy images. Methods: The proposed algorithm includes two steps: firstly, light and dark hairs and ruler marking are segmented through adaptive canny edge detector and refinement by morphological operators. Secondly, the hairs are repaired based on multi-resolution coherence transport inpainting. Results: The algorithm was applied to 50 dermoscopy images. To estimate the accuracy of the proposed hair detection algorithm, quantitative analysis was performed using TDR, FPR, and DA metrics. Moreover, to evaluate the performance of the proposed hair repaired algorithm, three statistical metrics namely entropy, standard deviation, and co-occurrence matrix were used. Conclusion: The results demonstrate that the proposed algorithm is highly accurate and able to detect and repair the hair pixels with few errors. In addition, the segmentation veracity of the skin lesion is effectively improved after our proposed hair removal algorithm. © 2013 John Wiley & Sons A/S.","Dermoscopy images; Hair detection; Hair removal; Melanoma; Skin lesions","Dermoscopy images; Diagnosis of pigmented skin lesions; Hair removal; Image analysis techniques; Melanoma; Morphological operator; Segmentation veracity; Skin lesion; Dermatology; Image segmentation; Repair; Algorithms; algorithm; article; diagnostic accuracy; diagnostic test accuracy study; entropy; epiluminescence microscopy; esthetic surgery; false positive rate; human; intermethod comparison; medical expert; metric system; quantitative analysis; true detection rate; dermoscopy images; hair detection; hair removal; melanoma; skin lesions; Algorithms; Artificial Intelligence; Dermoscopy; Hair; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Skin; Subtraction Technique",Article,Scopus,2-s2.0-84880347146
"Kreps G.L., Neuhauser L.","Artificial intelligence and immediacy: Designing health communication to personally engage consumers and providers",2013,"Patient Education and Counseling",13,10.1016/j.pec.2013.04.014,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880327010&doi=10.1016%2fj.pec.2013.04.014&partnerID=40&md5=7c0d9942118bfbdeb50cfb32333195e3","Objective: We describe how ehealth communication programs can be improved by using artificial intelligence (AI) to increase immediacy. Methods: We analyzed major deficiencies in ehealth communication programs, illustrating how programs often fail to fully engage audiences and can even have negative consequences by undermining the effective delivery of information intended to guide health decision-making and influence adoption of health-promoting behaviors. We examined the use of AI in ehealth practices to promote immediacy and provided examples from the ChronologyMD project. Results: Strategic use of AI is shown to help enhance immediacy in ehealth programs by making health communication more engaging, relevant, exciting, and actionable. Conclusion: AI can enhance the ""immediacy"" of ehealth by humanizing health promotion efforts, promoting physical and emotional closeness, increasing authenticity and enthusiasm in health promotion efforts, supporting personal involvement in communication interactions, increasing exposure to relevant messages, reducing demands on healthcare staff, improving program efficiency, and minimizing costs. Practice implications: User-centered AI approaches, such as the use of personally involving verbal and nonverbal cues, natural language translation, virtual coaches, and comfortable human-computer interfaces can promote active information processing and adoption of new ideas. Immediacy can improve information access, trust, sharing, motivation, and behavior changes. © 2013 Elsevier Ireland Ltd.","Artificial intelligence; Consumer engagement; Crohn's disease; Health communication; Immediacy; Participatory design","access to information; article; artificial intelligence; behavior change; clinical practice; consumer; cost minimization analysis; health behavior; health care personnel; health education; health program; health promotion; human; human computer interaction; information dissemination; information processing; interpersonal communication; medical decision making; medical information; motivation; nonverbal communication; priority journal; social interaction; telehealth; trust; verbal communication; Artificial intelligence; Consumer engagement; Crohn's disease; Health communication; Immediacy; Participatory design; Artificial Intelligence; Communication; Consumer Health Information; Consumer Participation; Crohn Disease; Health Promotion; Humans; Program Evaluation; Telemedicine; Time Factors",Article,Scopus,2-s2.0-84880327010
"Leung Y.Y., Ryvkin P., Ungar L.H., Gregory B.D., Wang L.-S.","CoRAL: Predicting non-coding RNAs from small RNA-sequencing data",2013,"Nucleic Acids Research",13,10.1093/nar/gkt426,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881516445&doi=10.1093%2fnar%2fgkt426&partnerID=40&md5=e52aeffd8ebf8228c5832da121a29733","The surprising observation that virtually the entire human genome is transcribed means we know little about the function of many emerging classes of RNAs, except their astounding diversities. Traditional RNA function prediction methods rely on sequence or alignment information, which are limited in their abilities to classify the various collections of non-coding RNAs (ncRNAs). To address this, we developed Classification of RNAs by Analysis of Length (CoRAL), a machine learning-based approach for classification of RNA molecules. CoRAL uses biologically interpretable features including fragment length and cleavage specificity to distinguish between different ncRNA populations. We evaluated CoRAL using genome-wide small RNA sequencing data sets from four human tissue types and were able to classify six different types of RNAs with ∼80% cross-validation accuracy. Analysis by CoRAL revealed that microRNAs, small nucleolar and transposon-derived RNAs are highly discernible and consistent across all human tissue types assessed, whereas long intergenic ncRNAs, small cytoplasmic RNAs and small nuclear RNAs show less consistent patterns. The ability to reliably annotate loci across tissue types demonstrates the potential of CoRAL to characterize ncRNAs using small RNA sequencing data in less well-characterized organisms. © The Author(s) 2013. Published by Oxford University Press.",,"long untranslated RNA; small nuclear RNA; untranslated RNA; access to information; accuracy; article; comparative study; computer program; human; human tissue; Internet; machine learning; nucleotide sequence; prediction; priority journal; RNA analysis; RNA cleavage; RNA sequence; RNA structure; transposon; Algorithms; Artificial Intelligence; Classification; Humans; RNA, Small Untranslated; Sequence Analysis, RNA",Article,Scopus,2-s2.0-84881516445
"Neuhauser L., Kreps G.L., Morrison K., Athanasoulis M., Kirienko N., Van Brunt D.","Using design science and artificial intelligence to improve health communication: ChronologyMD case example",2013,"Patient Education and Counseling",13,10.1016/j.pec.2013.04.006,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880330025&doi=10.1016%2fj.pec.2013.04.006&partnerID=40&md5=629f919974cccb58b7dde627a7b50c8c","Objective: This paper describes how design science theory and methods and use of artificial intelligence (AI) components can improve the effectiveness of health communication. Methods: We identified key weaknesses of traditional health communication and features of more successful eHealth/AI communication. We examined characteristics of the design science paradigm and the value of its user-centered methods to develop eHealth/AI communication. We analyzed a case example of the participatory design of AI components in the ChronologyMD project intended to improve management of Crohn's disease. Results: eHealth/AI communication created with user-centered design shows improved relevance to users' needs for personalized, timely and interactive communication and is associated with better health outcomes than traditional approaches. Participatory design was essential to develop ChronologyMD system architecture and software applications that benefitted patients. Conclusion: AI components can greatly improve eHealth/AI communication, if designed with the intended audiences. Design science theory and its iterative, participatory methods linked with traditional health communication theory and methods can create effective AI health communication. Practice implications: eHealth/AI communication researchers, developers and practitioners can benefit from a holistic approach that draws from theory and methods in both design sciences and also human and social sciences to create successful AI health communication. © 2013 Elsevier Ireland Ltd.","Artificial intelligence; Artificial sciences; Crohn's disease; Design sciences; EHealth; Health communication; Observations of daily living; Participatory design; User-centered design","article; artificial intelligence; computer interface; computer program; Crohn disease; design science; disease management; human; medical informatics; medical information; medical technology; priority journal; science; Artificial intelligence; Artificial sciences; Crohn's disease; Design sciences; eHealth; Health communication; Observations of daily living; Participatory design; User-centered design; Adult; Artificial Intelligence; Crohn Disease; Disease Management; Health Communication; Humans; Internet; Patient Participation; Research Design; Telemedicine",Article,Scopus,2-s2.0-84880330025
"Abdullah S., Aslam M., Khan T.A., Naeem M.","A new type of fuzzy normal subgroups and fuzzy cosets",2013,"Journal of Intelligent and Fuzzy Systems",13,10.3233/IFS-2012-0612,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880088843&doi=10.3233%2fIFS-2012-0612&partnerID=40&md5=32ea33408554f612378883889c2e6c1a","Using the notions of belonging (∈) and quasi-k-coincidence (q k) of a fuzzy point with a fuzzy set, we define the concepts of (∈, ∈ vqk̄(qk)-fuzzy normal subgroups and (∈}, ∈{in } vqk̄(qk)-fuzzy cosets which is a generalization of fuzzy normal subgroups, fuzzy coset, (∈, ∈ vqk̄{q})-fuzzy normal subgroups and (∈, ∈ vqk̄{q})-fuzzy cosets. We give characterizations of an (∈, ∈ vqk̄(qk)-fuzzy normal subgroup and (∈, ∈ vqk̄(qk)-fuzzy coset, and deal with several related properties. The important achievement of the study with an (∈, ∈ vqk̄(qk)-fuzzy normal subgroup and (∈, ∈ vqk̄(qk)-fuzzy cosets is the generalization of that the notions of fuzzy normal subgroups, fuzzy coset, (∈ ,∈ vqk̄{q})-fuzzy normal subgroups and (∈, ∈ vqk̄{q})-fuzzy cosets. We prove that the set of all (∈, ∈ vqk̄(qk)-fuzzy cosets of G is a group, where the multiplication is defined by {f-{x}} {f-{y}} = {f→(xy) for all x,y∈ G. If ∼f:F → [0,1] is defined by f∼(f→x) = f(x) for all x∈ G. Then f∼ is a fuzzy normal subgroup of F. © 2013-IOS Press and the authors. All rights reserved.","(∈, ∈ vqk̄(qk)-fuzzy coset; (∈, ∈ vqk̄(qk)-fuzzy normal subgroup; fuzzy coset; fuzzy normal subgroup; Group","fuzzy coset; fuzzy normal subgroup; Fuzzy point; Group; Artificial intelligence; Engineering; Fuzzy sets",Article,Scopus,2-s2.0-84880088843
"Koedinger K.R., Stamper J.C., McLaughlin E.A., Nixon T.","Using data-driven discovery of better student models to improve student learning",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-642-39112-5-43,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880011040&doi=10.1007%2f978-3-642-39112-5-43&partnerID=40&md5=1a5daed5d28b76a2331be582dde9085d","Deep analysis of domain content yields novel insights and can be used to produce better courses. Aspects of such analysis can be performed by applying AI and statistical algorithms to student data collected from educational technology and better cognitive models can be discovered and empirically validated in terms of more accurate predictions of student learning. However, can such improved models yield improved student learning? This paper reports positively on progress in closing this loop. We demonstrate that a tutor unit, redesigned based on data-driven cognitive model improvements, helped students reach mastery more efficiently. In particular, it produced better learning on the problem-decomposition planning skills that were the focus of the cognitive model improvements. © 2013 Springer-Verlag Berlin Heidelberg.","Cognitive modeling; Data mining; Machine learning","Accurate prediction; Cognitive model; Cognitive modeling; Statistical algorithm; Student learning; Student Models; Artificial intelligence; Cognitive systems; Curricula; Data mining; Learning systems; Teaching; Students",Conference Paper,Scopus,2-s2.0-84880011040
"Navoret N., Jacquir S., Laurent G., Binczak S.","Detection of complex fractionated atrial electrograms using recurrence quantification analysis",2013,"IEEE Transactions on Biomedical Engineering",13,10.1109/TBME.2013.2247402,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879967179&doi=10.1109%2fTBME.2013.2247402&partnerID=40&md5=e3527afce052676416d51f1c8c50a8ee","Atrial fibrillation (AF) is the most common cardiac arrhythmia but its proarrhythmic substrate remains unclear. Reentrant electrical activity in the atria may be responsible for AF maintenance. Over the last decade, different catheter ablation strategies targeting the electrical substrate of the left atrium have been developed in order to treat AF. Complex fractionated atrial electrograms (CFAEs) recorded in the atria may represent not only reentry mechanisms, but also a large variety of bystander electrical wave fronts. In order to identify CFAE involved in AF maintenance as a potential target for AF ablation, we have developed an algorithm based on nonlinear data analysis using recurrence quantification analysis (RQA). RQA features make it possible to quantify hidden structures in a signal and offer clear representations of different CFAE types. Five RQA features were used to qualify CFAE areas previously tagged by a trained electrophysiologist. Data from these analyzes were used by two classifiers to detect CFAE periods in a signal. While a single feature is not sufficient to properly detect CFAE periods, the set of five RQA features combined with a classifier were highly reliable for CFAE detection. © 1964-2012 IEEE.","Atrial fibrillation; CFAE detection algorithm; recurrence quantification analysis","Atrial electrograms; Atrial fibrillation; Cardiac arrhythmia; Detection algorithm; Electrical activities; Nonlinear data analysis; Potential targets; Recurrence quantification analysis; Ablation; Diseases; Maintenance; Signal detection; algorithm; article; catheter ablation; clinical article; complex fractionated atrial electrogram; data analysis; electrography; female; heart atrium fibrillation; heart catheter; heart electrophysiology; human; male; nonlinear system; quantitative analysis; recurrence quantification analysis; reliability; sensitivity and specificity; Algorithms; Artificial Intelligence; Atrial Fibrillation; Diagnosis, Computer-Assisted; Electrocardiography; Humans; Oscillometry; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84879967179
"Woods G.J., Kang D., Quintanar D.R., Curley E.F., Davis S.E., Lansey K.E., Arnold R.G.","Centralized versus decentralized wastewater reclamation in the houghton area of Tucson, Arizona",2013,"Journal of Water Resources Planning and Management",13,10.1061/(ASCE)WR.1943-5452.0000249,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876212990&doi=10.1061%2f%28ASCE%29WR.1943-5452.0000249&partnerID=40&md5=c98cceacd14acb883987c79adfa0b4ae","Reclaimed wastewater is increasingly important to satisfaction of water-sustainability objectives in water-short municipalities throughout the United States and particularly in the Southwest. Water reclamation and reuse present new challenges for urban planners, who now tend to consider renewable freshwater and reclaimed wastewater as unique parts of a single water resources portfolio. Efficiency objectives in geographically dispersed communities lead planners to explore the relative merits of centralized versus decentralized wastewater-treatment capacity when new construction is required. However, the complexity of the planning landscape-in which existing water distribution and sewerage capacities; geographic factors; and uncertainty in growth projections, energy cost, and even the sustainability of existing freshwater supplies contribute to plan selection-suggests that decision support methods can usefully supplement engineering judgment to find a near-optimal level of decentralization in facilities planning. In this study, an existing decision support system (DSS) was modified to include costs attributable to infrastructure construction, operation, and maintenance for wastewater collection and transmission of both potable and reclaimed water at the regional (city or city subsection) level to aid water supply planning. The modified DSS was then applied to a study area in southeast Tucson, Arizona. Several scenarios are developed and compared on the basis of cost and energy consumption. A sensitivity analysis is provided. In general, increased peripheral demand, limited existing capacity, greater elevation differences, and lower discount rates favor decentralized design and construction. © 2013 American Society of Civil Engineers.","Decentralized infrastructure; Integrated water management; Urban planning; Wastewater reclamation","Decentralized infrastructure; Decentralized wastewater; Decision support system (dss); Engineering judgments; Infrastructure construction; Integrated water management; Reclamation and reuse; Water supply planning; Artificial intelligence; Cost engineering; Costs; Decision support systems; Energy utilization; Potable water; Sustainable development; Urban planning; Wastewater treatment; Water resources; Water supply; Water supply systems; Wastewater reclamation; decision support system; energy use; integrated approach; sustainability; urban planning; wastewater; water management; water resource; water supply; water treatment; water use; Arizona; Tucson; United States",Article,Scopus,2-s2.0-84876212990
"Valizadeh N., El-Shafie A.","Forecasting the Level of Reservoirs Using Multiple Input Fuzzification in ANFIS",2013,"Water Resources Management",13,10.1007/s11269-013-0349-5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878770211&doi=10.1007%2fs11269-013-0349-5&partnerID=40&md5=741300bfedc279a29dd9deb798ac2c99","Estimation the Level of water is one of the crucial subjects in reservoir management influencing on reservoir operation and decision making. One of the most accurate artificial intelligence model used broadly in water resource aspects is adaptive neuro-fuzzy interface system (ANFIS) taking in to account the membership functions (MF) on the basis of the smoothness characteristics and mathematical components each for set of input data. All researches in hydrological estimation used ANFIS, merely a type of MF has been noticed for all sets of inputs without considering the response of each of them. This study is applying a specified certain MFs for each type of input to improve the accuracy of ANFIS model in forecasting the water level in Klang Gates Dam in Malaysia. On the basis of the previous studies, two most popular MFs, Generalized Bell Shape MF and, Gaussian MF, are employed for examine the new pattern in two inputs ANFIS architecture resulted less stress in error performance, and higher accuracy in estimation, compare to the traditional ANFIS model. The aim is achieved by evaluating the performance in and fitness of the model in daily reservoir estimation. © 2013 Springer Science+Business Media Dordrecht.","Klang Dam; Level estimation; MFs; Neuro-fuzzy","Adaptive neuro-fuzzy; ANFIS architecture; Error performance; Hydrological estimation; Interface system; MFs; Neuro-Fuzzy; Reservoir operation; Artificial intelligence; Reservoirs (water); Water levels; Water resources; Estimation; accuracy assessment; artificial intelligence; estimation method; forecasting method; fuzzy mathematics; reservoir; water level; Klang Basin; Malaysia; Selangor; West Malaysia",Article,Scopus,2-s2.0-84878770211
"Nejad A.B., Madsen K.H., Ebdrup B.H., Siebner H.R., Rasmussen H., Aggernæs B., Glenthoj B.Y., Baaré W.F.C.","Neural markers of negative symptom outcomes in distributed working memory brain activity of antipsychotic-naive schizophrenia patients",2013,"International Journal of Neuropsychopharmacology",13,10.1017/S1461145712001253,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878887293&doi=10.1017%2fS1461145712001253&partnerID=40&md5=6ef565ad51bca7bd37491dee8326c6b8","Since working memory deficits in schizophrenia have been linked to negative symptoms, we tested whether features of the one could predict the treatment outcome in the other. Specifically, we hypothesized that working memory-related functional connectivity at pre-treatment can predict improvement of negative symptoms in antipsychotic-treated patients. Fourteen antipsychotic-naive patients with first-episode schizophrenia were clinically assessed before and after 7 months of quetiapine monotherapy. At baseline, patients underwent functional magnetic resonance imaging while performing a verbal n-back task. Spatial independent component analysis identified task-modulated brain networks. A linear support vector machine was trained with these components to discriminate six patients who showed improvement in negative symptoms from eight non-improvers. Classification accuracy and significance was estimated by leave-one-out cross-validation and permutation tests, respectively. Two frontoparietal and one default mode network components predicted negative symptom improvement with a classification accuracy of 79% (pÂ =Â 0.003). Discriminating features were found in the frontoparietal networks but not the default mode network. These preliminary data suggest that functional patterns at baseline can predict negative symptom treatment-response in schizophrenia. This information may be used to stratify patients into subgroups thereby facilitating personalized treatment. © CINP 2012.","Antipsychotic drugs; functional connectivity; multivariate classification analysis; negative symptoms; schizophrenia","quetiapine; adult; article; behavior; clinical article; electroencephalogram; female; follow up; human; male; monotherapy; nuclear magnetic resonance imaging; pathophysiology; priority journal; schizophrenia; working memory; Adolescent; Adult; Antipsychotic Agents; Artificial Intelligence; Brain; Brain Mapping; Dibenzothiazepines; Female; Humans; Image Processing, Computer-Assisted; Male; Memory Disorders; Memory, Short-Term; Middle Aged; Nerve Net; Oxygen; Prospective Studies; Psychiatric Status Rating Scales; Schizophrenia; Young Adult",Article,Scopus,2-s2.0-84878887293
"Miwa M., Ohta T., Rak R., Rowley A., Kell D.B., Pyysalo S., Ananiadou S.","A method for integrating and ranking the evidence for biochemical pathways by mining reactions from text",2013,"Bioinformatics",13,10.1093/bioinformatics/btt227,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879895059&doi=10.1093%2fbioinformatics%2fbtt227&partnerID=40&md5=5168a8b28720656437306585a92819d3","Motivation: To create, verify and maintain pathway models, curators must discover and assess knowledge distributed over the vast body of biological literature. Methods supporting these tasks must understand both the pathway model representations and the natural language in the literature. These methods should identify and order documents by relevance to any given pathway reaction. No existing system has addressed all aspects of this challenge.Method: We present novel methods for associating pathway model reactions with relevant publications. Our approach extracts the reactions directly from the models and then turns them into queries for three text mining-based MEDLINE literature search systems. These queries are executed, and the resulting documents are combined and ranked according to their relevance to the reactions of interest. We manually annotate document-reaction pairs with the relevance of the document to the reaction and use this annotation to study several ranking methods, using various heuristic and machine-learning approaches.Results: Our evaluation shows that the annotated document-reaction pairs can be used to create a rule-based document ranking system, and that machine learning can be used to rank documents by their relevance to pathway reactions. We find that a Support Vector Machine-based system outperforms several baselines and matches the performance of the rule-based system. The success of the query extraction and ranking methods are used to update our existing pathway search system, PathText. © The Author 2013.",,"algorithm; artificial intelligence; biochemistry; data mining; Medline; procedures; support vector machine; article; data mining; methodology; Algorithms; Artificial Intelligence; Biochemical Processes; Data Mining; MEDLINE; Support Vector Machines; Algorithms; Artificial Intelligence; Biochemical Processes; Data Mining; MEDLINE; Support Vector Machines",Conference Paper,Scopus,2-s2.0-84879895059
"Bresso E., Grisoni R., Marchetti G., Karaboga A.S., Souchet M., Devignes M.-D., Smaïl-Tabbone M.","Integrative relational machine-learning for understanding drug side-effect profiles",2013,"BMC Bioinformatics",13,10.1186/1471-2105-14-207,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879426876&doi=10.1186%2f1471-2105-14-207&partnerID=40&md5=ad337205828c933377b5ab0d54db32bf","Background: Drug side effects represent a common reason for stopping drug development during clinical trials. Improving our ability to understand drug side effects is necessary to reduce attrition rates during drug development as well as the risk of discovering novel side effects in available drugs. Today, most investigations deal with isolated side effects and overlook possible redundancy and their frequent co-occurrence.Results: In this work, drug annotations are collected from SIDER and DrugBank databases. Terms describing individual side effects reported in SIDER are clustered with a semantic similarity measure into term clusters (TCs). Maximal frequent itemsets are extracted from the resulting drug x TC binary table, leading to the identification of what we call side-effect profiles (SEPs). A SEP is defined as the longest combination of TCs which are shared by a significant number of drugs. Frequent SEPs are explored on the basis of integrated drug and target descriptors using two machine learning methods: decision-trees and inductive-logic programming. Although both methods yield explicit models, inductive-logic programming method performs relational learning and is able to exploit not only drug properties but also background knowledge. Learning efficiency is evaluated by cross-validation and direct testing with new molecules. Comparison of the two machine-learning methods shows that the inductive-logic-programming method displays a greater sensitivity than decision trees and successfully exploit background knowledge such as functional annotations and pathways of drug targets, thereby producing rich and expressive rules. All models and theories are available on a dedicated web site.Conclusions: Side effect profiles covering significant number of drugs have been extracted from a drug ×side-effect association table. Integration of background knowledge concerning both chemical and biological spaces has been combined with a relational learning method for discovering rules which explicitly characterize drug-SEP associations. These rules are successfully used for predicting SEPs associated with new drugs. © 2013 Bresso et al.; licensee BioMed Central Ltd.","Data integration; Data mining; Drug discovery; Drug side-effects; Relational machine learning","Back-ground knowledge; Chemical and biologicals; Drug discovery; Drug side-effects; Functional annotation; Learning efficiency; Maximal frequent itemsets; Semantic similarity measures; Data integration; Data mining; Decision trees; Drug products; Forestry; Learning systems; Logic programming; Drug interactions; Data; Decision Making; Forestry; Integration; Trees; adverse drug reaction; article; artificial intelligence; biology; decision tree; drug database; methodology; reproducibility; Artificial Intelligence; Computational Biology; Databases, Pharmaceutical; Decision Trees; Drug-Related Side Effects and Adverse Reactions; Reproducibility of Results",Article,Scopus,2-s2.0-84879426876
"Xiong H., Chen Y., Guan Z., Chen Z.","Finding and fixing vulnerabilities in several three-party password authenticated key exchange protocols without server public keys",2013,"Information Sciences",13,10.1016/j.ins.2013.02.004,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875929826&doi=10.1016%2fj.ins.2013.02.004&partnerID=40&md5=fdcd8d206543a13b0ca01a1f10f1f560","Three-party password-based authenticated key exchange (3PAKE) protocols allow two users (clients) to establish a session key with the support from an authenticated server over an insecure channel. Several 3PAKE protocols, which do not require server public keys, have been proposed recently. In this paper, we use Chang, et al.'s protocol as a case study and demonstrate that all of the 3PAKE protocols without server public keys are not secure against Key Compromise Impersonation (KCI) attack. A detailed analysis of flaw in these protocols has been conducted and we hope that by identifying this design flaw, similar structural mistakes can be avoided in future designs. Furthermore, we propose an improved protocol that remedies the weakness of these protocols and prove its security in a widely accepted model. © 2012 Elsevier Inc. All rights reserved.","Authentication; Key Compromise Impersonation (KCI) attack; Password-based; Three-party key exchange","Authenticated key exchange; Future designs; Key exchange; Key-compromise impersonation; Password authenticated key exchange protocols; Password-based; Session key; Three-party; Artificial intelligence; Software engineering; Authentication",Article,Scopus,2-s2.0-84875929826
"Yang S.-T., Lee J.-D., Chang T.-C., Huang C.-H., Wang J.-J., Hsu W.-C., Chan H.-L., Wai Y.-Y., Li K.-Y.","Discrimination between Alzheimer's disease and mild cognitive impairment using SOM and PSO-SVM",2013,"Computational and Mathematical Methods in Medicine",13,10.1155/2013/253670,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878631954&doi=10.1155%2f2013%2f253670&partnerID=40&md5=37959b38959758936abeb98f1795bbe1","In this study, an MRI-based classification framework was proposed to distinguish the patients with AD and MCI from normal participants by using multiple features and different classifiers. First, we extracted features (volume and shape) from MRI data by using a series of image processing steps. Subsequently, we applied principal component analysis (PCA) to convert a set of features of possibly correlated variables into a smaller set of values of linearly uncorrelated variables, decreasing the dimensions of feature space. Finally, we developed a novel data mining framework in combination with support vector machine (SVM) and particle swarm optimization (PSO) for the AD/MCI classification. In order to compare the hybrid method with traditional classifier, two kinds of classifiers, that is, SVM and a self-organizing map (SOM), were trained for patient classification. With the proposed framework, the classification accuracy is improved up to 82.35% and 77.78% in patients with AD and MCI. The result achieved up to 94.12% and 88.89% in AD and MCI by combining the volumetric features and shape features and using PCA. The present results suggest that novel multivariate methods of pattern matching reach a clinically relevant accuracy for the a priori prediction of the progression from MCI to AD. © 2013 Shih-Ting Yang et al.",,"aged; Alzheimer disease; article; clinical article; disease classification; disease course; female; human; male; mild cognitive impairment; nuclear magnetic resonance imaging; principal component analysis; support vector machine; volumetry; algorithm; Alzheimer disease; artificial intelligence; biology; brain; case control study; classification; comparative study; computer assisted diagnosis; data mining; differential diagnosis; mild cognitive impairment; pathology; procedures; statistics and numerical data; classification; computer assisted diagnosis; methodology; nuclear magnetic resonance imaging; statistics; Aged; Algorithms; Alzheimer Disease; Artificial Intelligence; Brain; Case-Control Studies; Computational Biology; Data Mining; Diagnosis, Differential; Disease Progression; Female; Humans; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Male; Mild Cognitive Impairment; Principal Component Analysis; Support Vector Machines; Aged; Algorithms; Alzheimer Disease; Artificial Intelligence; Brain; Case-Control Studies; Computational Biology; Data Mining; Diagnosis, Differential; Disease Progression; Female; Humans; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Male; Mild Cognitive Impairment; Principal Component Analysis; Support Vector Machines",Article,Scopus,2-s2.0-84878631954
"Zhang S., Xia Z., Wang T.","A real-time interactive simulation framework for watershed decision making using numerical models and virtual environment",2013,"Journal of Hydrology",13,10.1016/j.jhydrol.2013.04.030,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877916278&doi=10.1016%2fj.jhydrol.2013.04.030&partnerID=40&md5=827166a66d8552f098b464b6196082df","Decision support systems based on a virtual environment (VE) are becoming a popular platform in watershed simulation and management. Simulation speed and data visualization is of great significance to decision making, especially in urgent events. Real-time interaction during the simulation process is also very important for dealing with different conditions and for making timely decisions. In this study, a VE-based real-time interactive simulation framework (VERTISF) is developed and applied to simulation and management of the Dujiangyan Project in China. In VERTISF development, a virtual reality platform and numerical models were hosted on different computers and connected by a network to improve simulation speed. Different types of numerical models were generalized in a unified architecture based on time step, and interactive control was realized by modifying model boundary conditions at each time step. The ""instruction-response"" method and data interpolation were used to synchronize virtual environment visualization and numerical model calculation. Implementation of the framework was based on modular software design; various computer languages can be used to develop the appropriate module. Since only slight modification was needed for current numerical model integration in the framework, VERTISF was easy to extend. Results showed that VERTISF could take full advantage of hardware development, and it was a simple and effective solution for complex watershed simulation. © 2013 Elsevier B.V.","Decision support system; Interactive simulation; Numerical model; Real time; Virtual environment","Hardware development; Interactive simulations; Modular software designs; Real time; Real time interactions; Real time interactive simulations; Unified architecture; Watershed simulation; Artificial intelligence; Complex networks; Data visualization; Decision making; Decision support systems; Landforms; Virtual reality; Watersheds; Numerical models; data interpretation; decision support system; hydrological modeling; numerical model; real time; visualization; watershed; China; Dujiangyan; Sichuan",Article,Scopus,2-s2.0-84877916278
"Montiel-Ross O., Sepúlveda R., Castillo O., Melin P.","Ant colony test center for planning autonomous mobile robot navigation",2013,"Computer Applications in Engineering Education",13,10.1002/cae.20463,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876138034&doi=10.1002%2fcae.20463&partnerID=40&md5=e92297d21c9adce9b697e58abd1677ad","This paper presents the navigation software called Ant Colony Test Center designed to teach the different stages involved in mobile robotics. The navigation problem consists of the four subproblems: world perception, path planning, path generation, and path tracking. This software based on ant colonies has two operational modes: virtual and on-line. In virtual mode, is able to achieve path generation, path planning and virtual path tracking at once, since the virtual mobile robots ""ants"" searches for the objective point generating feasible paths, then generate a set of subgoals to obtain the optimal path, since the optimal path was obtained using a cost function that considers the robot architecture, the optimal trajectory for the robot also is obtained. In on-line mode, the robot is able to sense the world using stereoscopic vision, the map is updated using epipolar geometry, and the on-line navigation problem is handled similar to the virtual mode. The software has many educational skills, since students can learn about path generation, optimal planning and path tracking using the heuristic methodology known as Ant Colony Optimization that recently has had good acceptance for solving discrete optimization planning problems. The platform is also a good tool to learn about stereoscopic vision and epipolar geometry that is one of the best sensing methods in mobile robotics. © 2010 Wiley Periodicals, Inc.","Ant Colony Optimization; autonomous mobile robot navigation; Fuzzy Logic; path planning; Simple Tuning Algorithm","Autonomous Mobile Robot; Discrete optimization; Navigation problem; Navigation software; Optimal trajectories; Robot architecture; Stereoscopic vision; Tuning algorithm; Algorithms; Ant colony optimization; Artificial intelligence; Fuzzy logic; Mobile robots; Motion planning; Robotics; Students; Navigation",Article,Scopus,2-s2.0-84876138034
"Yazidi A., Granmo O.-C., Oommen B.J.","Learning-automaton-based online discovery and tracking of spatiotemporal event patterns",2013,"IEEE Transactions on Cybernetics",13,10.1109/TSMCB.2012.2224339,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890377077&doi=10.1109%2fTSMCB.2012.2224339&partnerID=40&md5=5a4018dce4cb9c7c9694332edaa98f5b","Discovering and tracking of spatiotemporal patterns in noisy sequences of events are difficult tasks that have become increasingly pertinent due to recent advances in ubiquitous computing, such as community-based social networking applications. The core activities for applications of this class include the sharing and notification of events, and the importance and usefulness of these functionalities increase as event sharing expands into larger areas of one's life. Ironically, instead of being helpful, an excessive number of event notifications can quickly render the functionality of event sharing to be obtrusive. Indeed, any notification of events that provides redundant information to the application/user can be seen to be an unnecessary distraction. In this paper, we introduce a new scheme for discovering and tracking noisy spatiotemporal event patterns, with the purpose of suppressing reoccurring patterns, while discerning novel events. Our scheme is based on maintaining a collection of hypotheses, each one conjecturing a specific spatiotemporal event pattern. A dedicated learning automaton (LA) - the spatiotemporal pattern LA (STPLA) - is associated with each hypothesis. By processing events as they unfold, we attempt to infer the correctness of each hypothesis through a real-time guided random walk. Consequently, the scheme that we present is computationally efficient, with a minimal memory footprint. Furthermore, it is ergodic, allowing adaptation. Empirical results involving extensive simulations demonstrate the superior convergence and adaptation speed of STPLA, as well as an ability to operate successfully with noise, including both the erroneous inclusion and omission of events. An empirical comparison study was performed and confirms the superiority of our scheme compared to a similar state-of-the-art approach. In particular, the robustness of the STPLA to inclusion as well as to omission noise constitutes a unique property compared to other related approaches. In addition, the results included, which involve the so-called ""presence sharing"" application, are both promising and, in our opinion, impressive. It is thus our opinion that the proposed STPLA scheme is, in general, ideal for improving the usefulness of event notification and sharing systems, since it is capable of significantly, robustly, and adaptively suppressing redundant information. © 2012 IEEE.","Learning automata (LAs); Spatiotemporal pattern recognition","Computationally efficient; Empirical - comparisons; Learning Automata; Social networking applications; Spatio-temporal events; Spatiotemporal pattern recognition; Spatiotemporal patterns; State-of-the-art approach; Particulate emissions; Ubiquitous computing; Automata theory; algorithm; article; artificial intelligence; automated pattern recognition; computer system; human; methodology; online system; social support; spatiotemporal analysis; automated pattern recognition; procedures; Algorithms; Artificial Intelligence; Computer Systems; Humans; Online Systems; Pattern Recognition, Automated; Social Support; Spatio-Temporal Analysis; Algorithms; Artificial Intelligence; Computer Systems; Humans; Online Systems; Pattern Recognition, Automated; Social Support; Spatio-Temporal Analysis",Article,Scopus,2-s2.0-84890377077
"Qu H., Yi Z., Yang S.X.","Efficient shortest-path-tree computation in network routing based on pulse-coupled neural networks",2013,"IEEE Transactions on Cybernetics",13,10.1109/TSMCB.2012.2221695,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890442792&doi=10.1109%2fTSMCB.2012.2221695&partnerID=40&md5=dc459823a42a24650eabdf00a0b16a96","Shortest path tree (SPT) computation is a critical issue for routers using link-state routing protocols, such as the most commonly used open shortest path first and intermediate system to intermediate system. Each router needs to recompute a new SPT rooted from itself whenever a change happens in the link state. Most commercial routers do this computation by deleting the current SPT and building a new one using static algorithms such as the Dijkstra algorithm at the beginning. Such recomputation of an entire SPT is inefficient, which may consume a considerable amount of CPU time and result in a time delay in the network. Some dynamic updating methods using the information in the updated SPT have been proposed in recent years. However, there are still many limitations in those dynamic algorithms. In this paper, a newmodified model of pulse-coupled neural networks (M-PCNNs) is proposed for the SPT computation. It is rigorously proved that the proposed model is capable of solving some optimization problems, such as the SPT. A static algorithm is proposed based on the M-PCNNs to compute the SPT efficiently for large-scale problems. In addition, a dynamic algorithm that makes use of the structure of the previously computed SPT is proposed, which significantly improves the efficiency of the algorithm. Simulation results demonstrate the effective and efficient performance of the proposed approach. © 2012 IEEE.","Autowave; Open shortest path first (OSPF); Pulse-coupled neural networks (PCNNs); Routing; Shortest path tree (SPT)","Autowave; Open shortest path first; Pulse coupled neural network; Routing; Shortest path tree; Forestry; Graph theory; Neural networks; Routers; Algorithms; Algorithms; Forestry; Neural Networks; algorithm; artificial intelligence; artificial neural network; automated pattern recognition; computer assisted diagnosis; computer network; information retrieval; procedures; signal processing; article; automated pattern recognition; computer assisted diagnosis; information retrieval; methodology; Algorithms; Artificial Intelligence; Computer Communication Networks; Image Interpretation, Computer-Assisted; Information Storage and Retrieval; Neural Networks (Computer); Pattern Recognition, Automated; Signal Processing, Computer-Assisted; Algorithms; Artificial Intelligence; Computer Communication Networks; Image Interpretation, Computer-Assisted; Information Storage and Retrieval; Neural Networks (Computer); Pattern Recognition, Automated; Signal Processing, Computer-Assisted",Article,Scopus,2-s2.0-84890442792
"Bhardwaj S., Srivastava S., Hanmandlu M., Gupta J.R.P.","GFM-based methods for speaker identification",2013,"IEEE Transactions on Cybernetics",13,10.1109/TSMCB.2012.2223461,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890439886&doi=10.1109%2fTSMCB.2012.2223461&partnerID=40&md5=fd6fc04052d0e46af67e44d6a59f449b","This paper presents three novel methods for speaker identification of which two methods utilize both the continuous density hidden Markov model (HMM) and the generalized fuzzy model (GFM), which has the advantages of both Mamdani and Takagi-Sugeno models. In the first method, the HMM is utilized for the extraction of shape-based batch feature vector that is fitted with the GFM to identify the speaker. On the other hand, the second method makes use of the Gaussian mixture model (GMM) and the GFM for the identification of speakers. Finally, the third method has been inspired by the way humans cash in on the mutual acquaintances while identifying a speaker. To see the validity of the proposed models [HMM-GFM, GMM-GFM, and HMM-GFM (fusion)] in a real-life scenario, they are tested on VoxForge speech corpus and on the subset of the 2003 National Institute of Standards and Technology evaluation data set. These models are also evaluated on the corrupted VoxForge speech corpus by mixing with different types of noisy signals at different values of signal-to-noise ratios, and their performance is found superior to that of the well-known models. © 2012 IEEE.","Gaussian mixture model (GMM); Generalized fuzzy model (GFM); Hidden Markov model (HMM); Shape-based batching (SBB)","Continuous density hidden Markov models; Feature vectors; Gaussian Mixture Model; Generalized fuzzy models; National Institute of Standards and Technology; Shape based; Speaker identification; Takagi-Sugeno models; Hidden Markov models; Loudspeakers; Modal analysis; Speech recognition; algorithm; artificial intelligence; automated pattern recognition; biometry; fuzzy logic; human; information retrieval; normal distribution; probability; procedures; speech analysis; statistical analysis; article; automated pattern recognition; biometry; methodology; speech analysis; Algorithms; Artificial Intelligence; Biometry; Data Interpretation, Statistical; Fuzzy Logic; Humans; Information Storage and Retrieval; Markov Chains; Normal Distribution; Pattern Recognition, Automated; Speech Production Measurement; Algorithms; Artificial Intelligence; Biometry; Data Interpretation, Statistical; Fuzzy Logic; Humans; Information Storage and Retrieval; Markov Chains; Normal Distribution; Pattern Recognition, Automated; Speech Production Measurement",Article,Scopus,2-s2.0-84890439886
"Chen H., Tang Y., Li L., Yuan Y., Li X., Tang Y.","Error analysis of stochastic gradient descent ranking",2013,"IEEE Transactions on Cybernetics",13,10.1109/TSMCB.2012.2217957,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890428197&doi=10.1109%2fTSMCB.2012.2217957&partnerID=40&md5=947cda2391363dbd085277f2bc1266c6","Ranking is always an important task in machine learning and information retrieval, e.g., collaborative filtering, recommender systems, drug discovery, etc. A kernel-based stochastic gradient descent algorithm with the least squares loss is proposed for ranking in this paper. The implementation of this algorithm is simple, and an expression of the solution is derived via a sampling operator and an integral operator. An explicit convergence rate for leaning a ranking function is given in terms of the suitable choices of the step size and the regularization parameter. The analysis technique used here is capacity independent and is novel in error analysis of ranking learning. Experimental results on real-world data have shown the effectiveness of the proposed algorithm in ranking tasks, which verifies the theoretical analysis in ranking error. © 2013 IEEE.","Error analysis; Integral operator; Ranking; Reproducing kernel Hilbert space; Sampling operator; Stochastic gradient descent","Analysis techniques; Integral operators; Ranking; Ranking functions; Regularization parameters; Reproducing Kernel Hilbert spaces; Stochastic gradient descent; Stochastic gradient descent algorithm; Learning algorithms; Mathematical operators; Time varying networks; Error analysis; algorithm; article; artificial intelligence; automated pattern recognition; computer simulation; information retrieval; methodology; statistical model; statistics; automated pattern recognition; information retrieval; Markov chain; procedures; Algorithms; Artificial Intelligence; Computer Simulation; Information Storage and Retrieval; Models, Statistical; Pattern Recognition, Automated; Stochastic Processes; Algorithms; Artificial Intelligence; Computer Simulation; Information Storage and Retrieval; Models, Statistical; Pattern Recognition, Automated; Stochastic Processes",Article,Scopus,2-s2.0-84890428197
"Chang S.-W., Abdul-Kareem S., Merican A.F., Zain R.B.","Oral cancer prognosis based on clinicopathologic and genomic markers using a hybrid of feature selection and machine learning methods",2013,"BMC Bioinformatics",13,10.1186/1471-2105-14-170,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878374114&doi=10.1186%2f1471-2105-14-170&partnerID=40&md5=d580e88f28e25fbfe77f6e09305b1c5d","Background: Machine learning techniques are becoming useful as an alternative approach to conventional medical diagnosis or prognosis as they are good for handling noisy and incomplete data, and significant results can be attained despite a small sample size. Traditionally, clinicians make prognostic decisions based on clinicopathologic markers. However, it is not easy for the most skilful clinician to come out with an accurate prognosis by using these markers alone. Thus, there is a need to use genomic markers to improve the accuracy of prognosis. The main aim of this research is to apply a hybrid of feature selection and machine learning methods in oral cancer prognosis based on the parameters of the correlation of clinicopathologic and genomic markers.Results: In the first stage of this research, five feature selection methods have been proposed and experimented on the oral cancer prognosis dataset. In the second stage, the model with the features selected from each feature selection methods are tested on the proposed classifiers. Four types of classifiers are chosen; these are namely, ANFIS, artificial neural network, support vector machine and logistic regression. A k-fold cross-validation is implemented on all types of classifiers due to the small sample size. The hybrid model of ReliefF-GA-ANFIS with 3-input features of drink, invasion and p63 achieved the best accuracy (accuracy = 93.81%; AUC = 0.90) for the oral cancer prognosis.Conclusions: The results revealed that the prognosis is superior with the presence of both clinicopathologic and genomic markers. The selected features can be investigated further to validate the potential of becoming as significant prognostic signature in the oral cancer studies. © 2013 Chang et al.; licensee BioMed Central Ltd.","Clinicopathologic; Feature selection; Genomic; Machine learning; Oral cancer prognosis","Clinicopathologic; Feature selection methods; Genomic; K fold cross validations; Machine learning methods; Machine learning techniques; Noisy and incomplete datum; Oral cancer; Data processing; Diagnosis; Diseases; Feature extraction; Genes; Logistics; Neural networks; Learning systems; tumor marker; adult; aged; article; artificial intelligence; artificial neural network; biological model; female; genetics; human; human genome; male; middle aged; mouth tumor; pathology; prognosis; statistical model; support vector machine; Adult; Aged; Artificial Intelligence; Female; Genome, Human; Humans; Logistic Models; Male; Middle Aged; Models, Biological; Mouth Neoplasms; Neural Networks (Computer); Prognosis; Support Vector Machines; Tumor Markers, Biological",Article,Scopus,2-s2.0-84878374114
"Jimeno-Yepes A.J., Sticco J.C., Mork J.G., Aronson A.R.","GeneRIF indexing: Sentence selection based on machine learning",2013,"BMC Bioinformatics",13,10.1186/1471-2105-14-171,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878376462&doi=10.1186%2f1471-2105-14-171&partnerID=40&md5=60264a1fc13b73888cb89aece91d5266","Background: A Gene Reference Into Function (GeneRIF) describes novel functionality of genes. GeneRIFs are available from the National Center for Biotechnology Information (NCBI) Gene database. GeneRIF indexing is performed manually, and the intention of our work is to provide methods to support creating the GeneRIF entries. The creation of GeneRIF entries involves the identification of the genes mentioned in MEDLINE®; citations and the sentences describing a novel function.Results: We have compared several learning algorithms and several features extracted or derived from MEDLINE sentences to determine if a sentence should be selected for GeneRIF indexing. Features are derived from the sentences or using mechanisms to augment the information provided by them: assigning a discourse label using a previously trained model, for example. We show that machine learning approaches with specific feature combinations achieve results close to one of the annotators. We have evaluated different feature sets and learning algorithms. In particular, Naïve Bayes achieves better performance with a selection of features similar to one used in related work, which considers the location of the sentence, the discourse of the sentence and the functional terminology in it.Conclusions: The current performance is at a level similar to human annotation and it shows that machine learning can be used to automate the task of sentence selection for GeneRIF annotation. The current experiments are limited to the human species. We would like to see how the methodology can be extended to other species, specifically the normalization of gene mentions in other species. © 2013 Jimeno-Yepes et al.; licensee BioMed Central Ltd.",,"Better performance; Current performance; Feature combination; Human annotations; Machine learning approaches; National center for biotechnology informations; Related works; Sentence selection; Database systems; Genes; Learning algorithms; Learning systems; Indexing (of information); algorithm; artificial intelligence; Bayes theorem; documentation; gene; genetic database; human; Medline; procedures; article; documentation; methodology; Abstracting and Indexing as Topic; Algorithms; Artificial Intelligence; Bayes Theorem; Databases, Genetic; Genes; Humans; MEDLINE; Abstracting and Indexing as Topic; Algorithms; Artificial Intelligence; Bayes Theorem; Databases, Genetic; Genes; Humans; MEDLINE",Article,Scopus,2-s2.0-84878376462
"Kundu K., Costa F., Huber M., Reth M., Backofen R.","Semi-Supervised Prediction of SH2-Peptide Interactions from Imbalanced High-Throughput Data",2013,"PLoS ONE",13,10.1371/journal.pone.0062732,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877849501&doi=10.1371%2fjournal.pone.0062732&partnerID=40&md5=19797941cfa613547af073cbbdee4f15","Src homology 2 (SH2) domains are the largest family of the peptide-recognition modules (PRMs) that bind to phosphotyrosine containing peptides. Knowledge about binding partners of SH2-domains is key for a deeper understanding of different cellular processes. Given the high binding specificity of SH2, in-silico ligand peptide prediction is of great interest. Currently however, only a few approaches have been published for the prediction of SH2-peptide interactions. Their main shortcomings range from limited coverage, to restrictive modeling assumptions (they are mainly based on position specific scoring matrices and do not take into consideration complex amino acids inter-dependencies) and high computational complexity. We propose a simple yet effective machine learning approach for a large set of known human SH2 domains. We used comprehensive data from micro-array and peptide-array experiments on 51 human SH2 domains. In order to deal with the high data imbalance problem and the high signal-to-noise ration, we casted the problem in a semi-supervised setting. We report competitive predictive performance w.r.t. state-of-the-art. Specifically we obtain 0.83 AUC ROC and 0.93 AUC PR in comparison to 0.71 AUC ROC and 0.87 AUC PR previously achieved by the position specific scoring matrices (PSSMs) based SMALI approach. Our work provides three main contributions. First, we showed that better models can be obtained when the information on the non-interacting peptides (negative examples) is also used. Second, we improve performance when considering high order correlations between the ligand positions employing regularization techniques to effectively avoid overfitting issues. Third, we developed an approach to tackle the data imbalance problem using a semi-supervised strategy. Finally, we performed a genome-wide prediction of human SH2-peptide binding, uncovering several findings of biological relevance. We make our models and genome-wide predictions, for all the 51 SH2-domains, freely available to the scientific community under the following URLs: http://www.bioinf.uni-freiburg.de/Software/SH2PepInt/SH2PepInt.tar.gz and http://www.bioinf.uni-freiburg.de/Software/SH2PepInt/Genome-wide-predictions.tar.gz, respectively. © 2013 Kundu et al.",,"adaptor protein; BCR ABL protein; phosphatidylinositol 3 kinase; protein SH2; protein tyrosine kinase; protein tyrosine phosphatase SHP 1; tyrosine; ligand; protein binding; access to information; accuracy; amino acid sequence; area under the curve; article; computer program; correlation analysis; false positive result; high throughput screening; Internet; machine learning; microarray analysis; nonlinear system; prediction; protein binding; protein phosphorylation; protein protein interaction; receiver operating characteristic; scoring system; sensitivity and specificity; signal noise ratio; support vector machine; artificial intelligence; biological model; evaluation study; genetics; human; physiology; predictive value; procedures; protein analysis; protein domain; protein microarray; Src homology domain; Area Under Curve; Artificial Intelligence; Humans; Ligands; Models, Biological; Predictive Value of Tests; Protein Array Analysis; Protein Binding; Protein Interaction Domains and Motifs; Protein Interaction Mapping; ROC Curve; src Homology Domains",Article,Scopus,2-s2.0-84877849501
"Cai Z., Wen L., Yang J., Lei Z., Li S.Z.","Structured visual tracking with dynamic graph",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-642-37431-9_7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875875622&doi=10.1007%2f978-3-642-37431-9_7&partnerID=40&md5=ea9726f7d5903b4eb1286becef09d98b","Structure information has been increasingly incorporated into computer vision field, whereas only a few tracking methods have employed the inner structure of the target. In this paper, we introduce a dynamic graph with pairwise Markov property to model the structure information between the inner parts of the target. The target tracking is viewed as tracking a dynamic undirected graph whose nodes are the target parts and edges are the interactions between parts. These target parts within the graph waiting for matching are separated from the background with graph cut, and a spectral matching technique is exploited to accomplish the graph tracking. With the help of an intuitive updating mechanism, our dynamic graph can robustly adapt to the variations of target structure. Experimental results demonstrate that our structured tracker outperforms several state-of-the-art trackers in occlusion and structure deformations. © 2013 Springer-Verlag.",,"Inner structure; Spectral matching techniques; Structure deformation; Structure information; Target structure; Tracking method; Undirected graph; Visual Tracking; Artificial intelligence; Target tracking",Conference Paper,Scopus,2-s2.0-84875875622
"Poria S., Gelbukh A., Das D., Bandyopadhyay S.","Fuzzy clustering for semi-supervised learning - Case study: Construction of an emotion lexicon",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-642-37807-2_7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875844919&doi=10.1007%2f978-3-642-37807-2_7&partnerID=40&md5=2a72403e107792f3632af190cdc3da46","We consider the task of semi-supervised classification: extending category labels from a small dataset of labeled examples to a much larger set. We show that, at least on our case study task, unsupervised fuzzy clustering of the unlabeled examples helps in obtaining the hard clusters. Namely, we used the membership values obtained with fuzzy clustering as additional features for hard clustering. We also used these membership values to reduce the confusion set for the hard clustering. As a case study, we use applied the proposed method to the task of constructing a large emotion lexicon by extending the emotion labels from the WordNet Affect lexicon using various features of words. Some of the features were extracted from the emotional statements of the freely available ISEAR dataset; other features were WordNet distance and the similarity measured via the polarity scores in the SenticNet resource. The proposed method classified words by emotion labels with high accuracy. © 2013 Springer-Verlag.",,"Hard clustering; Membership values; Semi-supervised classification; Semi-supervised learning; Unsupervised fuzzy clustering; Wordnet; Artificial intelligence; Fuzzy clustering; Ontology; Research; Supervised learning",Conference Paper,Scopus,2-s2.0-84875844919
"Dong C.-S.J., Srinivasan A.","Agent-enabled service-oriented decision support systems",2013,"Decision Support Systems",13,10.1016/j.dss.2012.05.047,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877755057&doi=10.1016%2fj.dss.2012.05.047&partnerID=40&md5=fd33fcc8fe9e7f8566900ddbbf891cd1","The design of Decision Support Systems have recently emphasized web enablement as the next step in design improvements for this class of applications. We argue that these approaches fail to address the key notion of adaptability in the support for decision makers. Instead of focusing exclusively on automation in decision making, we believe it is also necessary to pay attention to the interplay between decision makers and organizational processes. The service oriented view of organizations recognizes the need to accommodate the changing reality of organizational dynamics. For example, the service science approach focuses on interactions between service providers, their clients, and consumers as important interacting components of a service system. Current approaches to DSS design are constrained in terms of their ability to adapt to changes in user requirements and to provide support for the evolution of systems. This situation worsens when resources are distributed at different locations across organizations, decision making processes are required to be integrated at different points in time, and when collaboration is needed among decision makers. However, this typically characterizes the needs of collaborative decision making in networked organizations as exemplified by systems used for supply chain management. To address these problems we leverage the power of services for designing a framework that explicitly recognizes the need for design based on service delivery. We develop an agent-enabled service-oriented architecture to realize the proposed framework with service and agent paradigms. The architecture is refined and validated with an implementation in the supply chain context. © 2012 Elsevier B.V. All rights reserved.","Decision making processes; Decision support systems; Service delivery; Services; Software agents","Collaborative decision making; Decision making process; Design improvements; Networked organizations; Organizational dynamics; Organizational process; Service delivery; Services; Artificial intelligence; Decision support systems; Design; Information services; Service oriented architecture (SOA); Societies and institutions; Software agents; Supply chain management; Decision making",Article,Scopus,2-s2.0-84877755057
"Pontabry J., Rousseau F., Oubel E., Studholme C., Koob M., Dietemann J.-L.","Probabilistic tractography using Q-ball imaging and particle filtering: Application to adult and in-utero fetal brain studies",2013,"Medical Image Analysis",13,10.1016/j.media.2012.11.004,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875244113&doi=10.1016%2fj.media.2012.11.004&partnerID=40&md5=f1eeb20bcbf64f2fb5a475e770c29820","By assuming that orientation information of brain white matter fibers can be inferred from Diffusion-Weighted Magnetic Resonance Imaging (DW-MRI) measurements, tractography algorithms provide an estimation of the brain connectivity in vivo. The two key ingredients of tractography are the diffusion model (tensor, high-order tensor, Q-ball, etc.) and the means to deal with uncertainty during the tracking process (deterministic vs probabilistic mathematical framework). In this paper, we investigate the use of an analytical Q-ball model for the diffusion data within a well-formalized particle filtering framework. The proposed method is validated and compared to other tracking algorithms on the MICCAI'09 contest Fiber Cup phantom. Tractographies of in vivo adult and fetal brain Diffusion-Weighted Images (DWIs) are also shown to illustrate the robustness of the algorithm. © 2012 Elsevier B.V.","Diffusion; MCMC; MRI; Particle filter; Tractography","Diffusion weighted images; Diffusion-weighted magnetic resonance imaging; Mathematical frameworks; MCMC; Orientation information; Particle filter; Probabilistic tractography; Tractography; Algorithms; Brain; Diffusion; Magnetic resonance imaging; Monte Carlo methods; Tensors; Signal filtering and prediction; adult; algorithm; article; diffusion tensor imaging; diffusion weighted imaging; fetus; human; in vivo study; neuroimaging; priority journal; tractography; white matter; Adult; Algorithms; Artificial Intelligence; Brain; Data Interpretation, Statistical; Diffusion Tensor Imaging; Female; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Male; Pattern Recognition, Automated; Phantoms, Imaging; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84875244113
"Zhang F., Ma Z.M., Yan L.","Construction of fuzzy ontologies from fuzzy XML models",2013,"Knowledge-Based Systems",13,10.1016/j.knosys.2012.12.015,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874654999&doi=10.1016%2fj.knosys.2012.12.015&partnerID=40&md5=572a968f6a81fa76a1f067ca4d817e18","The success and proliferation of the Semantic Web depends heavily on construction of Web ontologies. However, classical ontology construction approaches are not sufficient for handling imprecise and uncertain information that is commonly found in many application domains. Therefore, great efforts on construction of fuzzy ontologies have been made in recent years. In particular, XML is imposing itself as a standard for representing and exchanging information on the Web, topics related to the modeling of fuzzy data have become very interesting in the XML data context. Therefore, constructing fuzzy ontologies from fuzzy XML data resources may make the existing fuzzy XML data upgrade to Semantic Web contents, and the constructed fuzzy ontologies may be useful for improving some fuzzy XML applications. This paper proposes a formal approach and an automated tool for constructing fuzzy ontologies from fuzzy XML data resources. Firstly, we propose a formal definition of fuzzy XML models (including the document structure fuzzy DTDs and the document content fuzzy XML documents). On this basis, we propose a formal approach for constructing fuzzy ontologies from fuzzy XML models, i.e., transforming a fuzzy XML model (including fuzzy DTD and fuzzy XML document) into a fuzzy ontology. Also, we give the proof of correctness of the construction approach, and provide a detailed construction example. Furthermore, we implement a prototype tool called FXML2FOnto, which can automatically construct fuzzy ontologies from fuzzy XML models. Finally, in order to show that the constructed fuzzy ontologies may be useful for improving some fuzzy XML applications, we focus on investigating how to reason on fuzzy XML models (e.g., conformance, inclusion, and equivalence) based on the constructed fuzzy ontologies, and it turns out that the reasoning tasks of fuzzy XML models can be checked by means of the reasoning mechanism of fuzzy ontologies. © 2013 Elsevier B.V. All rights reserved.","Construction; Fuzzy DTD; Fuzzy ontology; Fuzzy XML document; Fuzzy XML model; Reasoning","Fuzzy DTD; Fuzzy ontology; Fuzzy XML document; Fuzzy xml models; Reasoning; Artificial intelligence; Construction; Software engineering; XML",Article,Scopus,2-s2.0-84874654999
"Mahdavi I., Shirazi B., Ghorbani N., Sahebjamnia N.","IMAQCS: Design and implementation of an intelligent multi-agent system for monitoring and controlling quality of cement production processes",2013,"Computers in Industry",13,10.1016/j.compind.2012.11.005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875245342&doi=10.1016%2fj.compind.2012.11.005&partnerID=40&md5=9f254c4fad32a94c6495a8029f5398bd","In cement plant, since all processes are chemical and irreversible, monitoring and control is a critical factor. If the process is not controlled at any stage, the final product can be damaged or lost. Thus, in such environments, considering the quality of the product at each state is essential. Also, to control the process, communication among different parts of production line is essential. The wasted time in production line has a direct effect on process correction time and cement production performance. Here, a model of a new intelligent multi-agent quality control system (IMAQCS) for controlling the quality of cement production processes is suggested. This model, using of rule-based artificial intelligence technique, concentrates on relationship between departments in cement production line to monitor multi-attribute quality factors. With the presence of agents for controlling the quality of cement processes, real-time analyzing and decision making in a fault condition will be provided. In order to validate the proposed model, IMAQCS is deployed in real plants of a cement industries complex in Iran. The ability of the system in the process production environment is assessed. The effectiveness and efficiency of the system are demonstrated by reducing the process correction time and increasing the cement production performance. Finally, this system can effectively impact on factory resources and cost saving. © 2012 Elsevier B.V.","Cement process; Intelligent monitoring and control; Multi-agent system","Artificial intelligence techniques; Cement production lines; Design and implementations; Effectiveness and efficiencies; Intelligent monitoring; Intelligent multi agent systems; Monitoring and controlling; Multi agent system (MAS); Cement industry; Cement plants; Cements; Multi agent systems; Production engineering; Process control",Article,Scopus,2-s2.0-84875245342
"Salvador C., Martins M.R., Vicente H., Neves J., Arteiro J.M., Caldeira A.T.","Modelling molecular and inorganic data of Amanita ponderosa mushrooms using artificial neural networks",2013,"Agroforestry Systems",13,10.1007/s10457-012-9548-y,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874946654&doi=10.1007%2fs10457-012-9548-y&partnerID=40&md5=1c26784b880beba1fcdd1ae694e9e337","Wild edible mushrooms Amanita ponderosa Malençon and Heim are very appreciated in gastronomy, with high export potential. This species grows in some microclimates, namely in the southwest of the Iberian Peninsula. The results obtained demonstrate that A. ponderosa mushrooms showed different inorganic composition according to their habitat and the molecular data, obtained by M13-PCR, allowed to distinguish the mushrooms at species level and to differentiate the A. ponderosa strains according to their location. Taking into account, on the one hand, that the characterisation of different strains is essential in further commercialisation and certification process and, on the other hand, the molecular studies are quite time consuming and an expensive process, the development of formal models to predict the molecular profile based on inorganic composition comes to be something essential. In the present work, Artificial Neural Networks (ANNs) were used to solve this problem. The ANN selected to predict molecular profile based on inorganic composition has a 6-7-14 topology. A good match between the observed and predicted values was observed. The present findings are wide potential application and both health and economical benefits arise from this study. © 2012 Springer Science+Business Media B.V.","Artificial intelligence based tools; Ectomycorrhizal macrofungi; Inorganic composition; M13-PCR; Wild edible mushrooms","artificial neural network; chemical composition; commercialization; data set; ecological modeling; economic analysis; ectomycorrhiza; habitat type; microclimate; molecular analysis; mushroom; polymerase chain reaction; Iberian Peninsula; Amanita; Amanita ponderosa; Basidiomycota",Article,Scopus,2-s2.0-84874946654
"Elnahal S.M., Kerstiens J., Helsper R.S., Zietman A.L., Johnstone P.A.S.","Proton beam therapy and accountable care: The challenges ahead",2013,"International Journal of Radiation Oncology Biology Physics",13,10.1016/j.ijrobp.2012.10.038,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875214088&doi=10.1016%2fj.ijrobp.2012.10.038&partnerID=40&md5=71207bb734cc02eba1e656e98f7f43c7","Purpose: Proton beam therapy (PBT) centers have drawn increasing public scrutiny for their high cost. The behavior of such facilities is likely to change under the Affordable Care Act. We modeled how accountable care reform may affect the financial standing of PBT centers and their incentives to treat complex patient cases. Methods and Materials: We used operational data and publicly listed Medicare rates to model the relationship between financial metrics for PBT center performance and case mix (defined as the percentage of complex cases, such as pediatric central nervous system tumors). Financial metrics included total daily revenues and debt coverage (daily revenues - daily debt payments). Fee-for-service (FFS) and accountable care (ACO) reimbursement scenarios were modeled. Sensitivity analyses were performed around the room time required to treat noncomplex cases: simple (30 minutes), prostate (24 minutes), and short prostate (15 minutes). Sensitivity analyses were also performed for total machine operating time (14, 16, and 18 h/d). Results: Reimbursement under ACOs could reduce daily revenues in PBT centers by up to 32%. The incremental revenue gained by replacing 1 complex case with noncomplex cases was lowest for simple cases and highest for short prostate cases. ACO rates reduced this incremental incentive by 53.2% for simple cases and 41.7% for short prostate cases. To cover daily debt payments after ACO rates were imposed, 26% fewer complex patients were allowable at varying capital costs and interest rates. Only facilities with total machine operating times of 18 hours per day would cover debt payments in all scenarios. Conclusions: Debt-financed PBT centers will face steep challenges to remain financially viable after ACO implementation. Paradoxically, reduced reimbursement for noncomplex cases will require PBT centers to treat more such cases over cases for which PBT has demonstrated superior outcomes. Relative losses will be highest for those facilities focused primarily on treating noncomplex cases. © 2013 Elsevier Inc. All rights reserved.",,"Central nervous systems; Financial metrics; Interest rates; Machine operating; Methods and materials; Operational data; Proton beam therapy; Public scrutiny; Health insurance; Proton beams; Sensitivity analysis; Urology; Artificial intelligence; article; central nervous system tumor; childhood cancer; financial deficit; financial management; health care cost; human; medicare; priority journal; prostate cancer; proton therapy; reimbursement; sensitivity analysis; Accountable Care Organizations; Cancer Care Facilities; Capital Financing; Central Nervous System Neoplasms; Child; Costs and Cost Analysis; Diagnosis-Related Groups; Fee-for-Service Plans; Humans; Income; Male; Patient Protection and Affordable Care Act; Prostatic Neoplasms; Proton Therapy; Reimbursement Mechanisms; Time Factors; United States",Article,Scopus,2-s2.0-84875214088
"Kusiak A., Zeng Y., Xu G.","Minimizing energy consumption of an air handling unit with a computational intelligence approach",2013,"Energy and Buildings",13,10.1016/j.enbuild.2013.02.006,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874697758&doi=10.1016%2fj.enbuild.2013.02.006&partnerID=40&md5=885d1fb73f7f6b6a95e757fc22f69507","A data-mining approach is applied to optimize the energy consumption of an air handling unit. A multi-perceptron ensemble algorithm is used to model a chiller, a pump, and the supply and return fans. A non-linear model is developed to minimize the total energy consumption of the air-handling unit while maintaining the temperature of the supply air and the static pressure in a predetermined range. A dynamic, penalty-based, electromagnetism-like algorithm is designed to solve the proposed model. In all, 200 test data points are used to validate the proposed algorithm. The computational results show that the energy consumed by the air-handling unit is reduced by almost 23%. © 2013 Elsevier B.V.","Air handling unit; Data mining; Dynamic penalty function; Electromagnetism-like algorithm; MLP ensemble","Air handling units; Computational results; Electromagnetism-like algorithm; Ensemble algorithms; MLP ensemble; Non-linear model; Penalty function; Static pressure; Test data; Total energy consumption; Algorithms; Artificial intelligence; Electromagnetism; Energy utilization; Data mining",Article,Scopus,2-s2.0-84874697758
"Berrichi A., Yalaoui F.","Efficient bi-objective ant colony approach to minimize total tardiness and system unavailability for a parallel machine scheduling problem",2013,"International Journal of Advanced Manufacturing Technology",13,10.1007/s00170-013-4841-0,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887624737&doi=10.1007%2fs00170-013-4841-0&partnerID=40&md5=7f4cd35368fe617e0430fec72c24eac9","In recent years, decision makers give more importance to the maintenance function, viewing its substantial contribution to business productivity. However, most literature on scheduling studies does not take into account maintenance planning when implementing production schedules. The achievement of production plan without taking into account maintenance activities increases the probability of machine breakdowns, and inversely, considering maintenance actions in production planning elongates the achievement dates of orders and affects deadlines. In this paper, we propose a bi-objective model to deal with production scheduling and maintenance planning problems simultaneously. The performance criteria considered for production and maintenance are, respectively, the total tardiness and the unavailability of the production system. The start times of preventive maintenance actions and their number are not fixed in advance but considered, with the execution dates of production tasks, as decisions variables of the problem. The solution of the integrated model is based on multi-objective ant colony optimization approach. The proposed algorithm (Pareto ant colony optimization) is compared, on the basis of several metrics, with well-known multi-objective genetic algorithms, namely NSGA-II and SPEA 2, and a hybrid particle swarm optimization algorithm. Interesting results are obtained via empirical study. © 2013 Springer-Verlag London.","Ant colony; Multi-objective optimization; Preventive maintenance (PM); Production scheduling; Reliability; Total tardiness","Ant colonies; Hybrid particle swarm optimization algorithm; Multi-objective genetic algorithm; Parallel machine-scheduling problems; Performance criterion; Preventive maintenance (pm); Production Scheduling; Total tardiness; Algorithms; Ant colony optimization; Artificial intelligence; Multiobjective optimization; Particle swarm optimization (PSO); Planning; Production control; Production engineering; Reliability; Preventive maintenance",Article,Scopus,2-s2.0-84887624737
"Park G., Kim S.","Hand biometric recognition based on fused hand geometry and vascular patterns",2013,"Sensors (Switzerland)",13,10.3390/s130302895,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875139997&doi=10.3390%2fs130302895&partnerID=40&md5=c389a6b99215a31ff2314b426f27e5d2","A hand biometric authentication method based on measurements of the user's hand geometry and vascular pattern is proposed. To acquire the hand geometry, the thickness of the side view of the hand, the K-curvature with a hand-shaped chain code, the lengths and angles of the finger valleys, and the lengths and profiles of the fingers were used, and for the vascular pattern, the direction-based vascular-pattern extraction method was used, and thus, a new multimodal biometric approach is proposed. The proposed multimodal biometric system uses only one image to extract the feature points. This system can be configured for low-cost devices. Our multimodal biometric-approach hand-geometry (the side view of the hand and the back of hand) and vascular-pattern recognition method performs at the score level. The results of our study showed that the equal error rate of the proposed system was 0.06%. © 2013 by the authors; licensee MDPI, Basel, Switzerland.","Hand biometric; Hand geometry; Multimodal biometric; Vascular-pattern recognition","Biometric authentication; Biometric recognition; Extraction method; Hand geometry; Multi-modal biometrics; Multimodal biometric systems; Recognition methods; Vascular-pattern recognition; Geometry; Biometrics; algorithm; article; artificial intelligence; biometry; blood vessel; hand; histology; human; image processing; vascularization; Algorithms; Artificial Intelligence; Biometric Identification; Blood Vessels; Hand; Humans; Image Processing, Computer-Assisted",Article,Scopus,2-s2.0-84875139997
"Troffaes M.C.M., Miranda E., Destercke S.","On the connection between probability boxes and possibility measures",2013,"Information Sciences",13,10.1016/j.ins.2012.09.033,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871000672&doi=10.1016%2fj.ins.2012.09.033&partnerID=40&md5=124d22cd52d84909a8e393ff775aeaa5","We explore the relationship between possibility measures (supremum preserving normed measures) and p-boxes (pairs of cumulative distribution functions) on totally preordered spaces, extending earlier work in this direction by De Cooman and Aeyels, among others. We start by demonstrating that only those p-boxes who have 0-1-valued lower or upper cumulative distribution function can be possibility measures, and we derive expressions for their natural extension in this case. Next, we establish necessary and sufficient conditions for a p-box to be a possibility measure. Finally, we show that almost every possibility measure can be modelled by a p-box, simply by ordering elements by increasing possibility. Whence, any techniques for p-boxes can be readily applied to possibility measures. We demonstrate this by deriving joint possibility measures from marginals, under varying assumptions of independence, using a technique known for p-boxes. Doing so, we arrive at a new rule of combination for possibility measures, for the independent case. © 2012 Elsevier Inc. All rights reserved.","Coherent lower and upper probabilities; Maxitive measures; Natural extension; Possibility measures; Probability boxes","Lower and upper probabilities; Maxitive; Natural extension; Possibility measure; Probability box; Artificial intelligence; Software engineering; Probability",Article,Scopus,2-s2.0-84871000672
"Santos R.S., Malheiros S.M.F., Cavalheiro S., de Oliveira J.M.P.","A data mining system for providing analytical information on brain tumors to public health decision makers",2013,"Computer Methods and Programs in Biomedicine",13,10.1016/j.cmpb.2012.10.010,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874238467&doi=10.1016%2fj.cmpb.2012.10.010&partnerID=40&md5=d5f7c8ea3bdee6854c8b5a9cc2c90325","Cancer is the leading cause of death in economically developed countries and the second leading cause of death in developing countries. Malignant brain neoplasms are among the most devastating and incurable forms of cancer, and their treatment may be excessively complex and costly. Public health decision makers require significant amounts of analytical information to manage public treatment programs for these patients. Data mining, a technology that is used to produce analytically useful information, has been employed successfully with medical data. However, the large-scale adoption of this technique has been limited thus far because it is difficult to use, especially for non-expert users. One way to facilitate data mining by non-expert users is to automate the process. Our aim is to present an automated data mining system that allows public health decision makers to access analytical information regarding brain tumors. The emphasis in this study is the use of ontology in an automated data mining process. The non-experts who tried the system obtained useful information about the treatment of brain tumors. These results suggest that future work should be conducted in this area. © 2012 Elsevier Ireland Ltd.","Brain neoplasms; Data analysis; Decision support system; Health services administration; Information system; Ontology","Automated data mining; Brain neoplasms; Brain tumors; Data mining system; Decision makers; Developed countries; Health services; Medical data; Artificial intelligence; Brain; Data reduction; Decision making; Decision support systems; Developing countries; Diseases; Information systems; Information use; Ontology; Patient treatment; Public health; Tumors; Data mining; access to information; article; brain cancer; data analysis; data mining; decision support system; human; information processing; public health; Algorithms; Brain Neoplasms; Computer Simulation; Data Mining; Databases, Factual; Decision Making; Decision Support Systems, Clinical; Humans; International Classification of Diseases; Public Health; Systems Integration",Article,Scopus,2-s2.0-84874238467
"Akin H.L., Ito N., Jacoff A., Kleiner A., Pellenz J., Visser A.","RoboCup rescue robot and simulation leagues",2013,"AI Magazine",13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876167322&partnerID=40&md5=546443a3ae589715d32474d2ca6cdfc1","The RoboCup Rescue Robot and Simulation competitions have been held since 2000. The experience gained during these competitions has increased the maturity level of the field, which allowed deploying robots after real disasters (for example, the Fukushima Daiichi nuclear disaster). This article provides an overview of these competitions and highlights the state of the art and the lessons learned. Copyright © 2013, Association for the Advancement of Artificial Intelligence.",,"Fukushima daiichi; Lessons learned; Maturity levels; RoboCup rescue; Simulation league; State of the art; Artificial intelligence; Robots",Conference Paper,Scopus,2-s2.0-84876167322
"Derhami V., Khodadadian E., Ghasemzadeh M., Zareh Bidoki A.M.","Applying reinforcement learning for web pages ranking algorithms",2013,"Applied Soft Computing Journal",13,10.1016/j.asoc.2012.12.023,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873694050&doi=10.1016%2fj.asoc.2012.12.023&partnerID=40&md5=32bc041de9e0c6ece8bf3c29b16a51db","Ranking web pages for presenting the most relevant web pages to user's queries is one of the main issues in any search engine. In this paper, two new ranking algorithms are offered, using Reinforcement Learning (RL) concepts. RL is a powerful technique of modern artificial intelligence that tunes agent's parameters, interactively. In the first step, with formulation of ranking as an RL problem, a new connectivity-based ranking algorithm, called RL-Rank, is proposed. In RL-Rank, agent is considered as a surfer who travels between web pages by clicking randomly on a link in the current page. Each web page is considered as a state and value function of state is used to determine the score of that state (page). Reward is corresponded to number of out links from the current page. Rank scores in RL-Rank are computed in a recursive way. Convergence of these scores is proved. In the next step, we introduce a new hybrid approach using combination of BM25 as a content-based algorithm and RL-Rank. Both proposed algorithms are evaluated by well known benchmark datasets and analyzed according to concerning criteria. Experimental results show using RL concepts leads significant improvements in raking algorithms. © 2013 Elsevier B.V.","Agent; Artificial intelligence; Ranking; Reinforcement Learning; Search engine; Value function","Benchmark datasets; Content-based algorithm; Hybrid approach; Out links; Rank scores; Ranking; Ranking algorithm; Value functions; Agents; Algorithms; Artificial intelligence; Reinforcement learning; Search engines; Websites",Article,Scopus,2-s2.0-84873694050
"Akhoondzadeh M.","Support vector machines for TEC seismo-ionospheric anomalies detection",2013,"Annales Geophysicae",13,10.5194/angeo-31-173-2013,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873646706&doi=10.5194%2fangeo-31-173-2013&partnerID=40&md5=64647216c7562f7377b30ac0879a2ed9","Using time series prediction methods, it is possible to pursue the behaviors of earthquake precursors in the future and to announce early warnings when the differences between the predicted value and the observed value exceed the predefined threshold value. Support Vector Machines (SVMs) are widely used due to their many advantages for classification and regression tasks. This study is concerned with investigating the Total Electron Content (TEC) time series by using a SVM to detect seismo-ionospheric anomalous variations induced by the three powerful earthquakes of Tohoku (11 March 2011), Haiti (12 January 2010) and Samoa (29 September 2009). The duration of TEC time series dataset is 49, 46 and 71 days, for Tohoku, Haiti and Samoa earthquakes, respectively, with each at time resolution of 2 h. In the case of Tohoku earthquake, the results show that the difference between the predicted value obtained from the SVM method and the observed value reaches the maximum value (i.e., 129.31 TECU) at earthquake time in a period of high geomagnetic activities. The SVM method detected a considerable number of anomalous occurrences 1 and 2 days prior to the Haiti earthquake and also 1 and 5 days before the Samoa earthquake in a period of low geomagnetic activities. In order to show that the method is acting sensibly with regard to the results extracted during nonevent and event TEC data, i.e., to perform some null-hypothesis tests in which the methods would also be calibrated, the same period of data from the previous year of the Samoa earthquake date has been taken into the account. Further to this, in this study, the detected TEC anomalies using the SVM method were compared to the previous results (Akhoondzadeh and Saradjian, 2011; Akhoondzadeh, 2012) obtained from the mean, median, wavelet and Kalman filter methods. The SVM detected anomalies are similar to those detected using the previous methods. It can be concluded that SVM can be a suitable learning method to detect the novelty changes of a nonlinear time series such as variations of earthquake precursors. © Author(s) 2013.","Ionosphere (Ionospheric irregularities)","artificial intelligence; early warning system; earthquake precursor; earthquake prediction; electron; geomagnetic field; Haiti earthquake 2010; ionosphere; Kalman filter; Tohoku earthquake 2011; wavelet analysis; Haiti; Honshu; Japan; Samoa; Tohoku",Article,Scopus,2-s2.0-84873646706
"Kumar A., Hanmandlu M., Gupta H.M.","Ant colony optimization based fuzzy binary decision tree for bimodal hand knuckle verification system",2013,"Expert Systems with Applications",13,10.1016/j.eswa.2012.07.042,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867665596&doi=10.1016%2fj.eswa.2012.07.042&partnerID=40&md5=67c8e3ae07180c8189d9886331536d10","In the recent trends of touch-less biometric authentication systems, hand knuckles from dorsal part of the hand is gaining popularity as a potential candidate for verification/recognition in variety of security applications. However, most of the available knuckle verification systems offer fixed security achieved for desired level of accuracy which cannot meet the varying levels of security requirements. This paper presents a bimodal knuckle verification system which is designed to meet a wide range of applications varying from civilian to high security regions. We use ant colony optimization (ACO) to choose the optimal fusion parameters corresponding to each level of security. The developed verification system utilizes fuzzy binary decision tree (FBDT) which is aimed at decision making in two classes: genuine (accept) and imposter (reject) using matching scores computed from the knuckle database. The FBDT is implemented using fuzzy Gini index for the selection of the tree nodes. The experiments are carried out on four publicly available HongKong PolyU knuckle databases named as: left index, right index, left middle and right middle with four bimodal systems: left-right index, left-right middle, left index-middle and right index-middle. The experimental results from these four bimodal knuckle databases validate the contributions of the proposed work. © 2012 Elsevier Ltd. All rights reserved.","Ant colony optimization; Fuzzy binary decision tree; Knuckle based biometric verification; Score level fusion","Ant Colony Optimization (ACO); Bimodal systems; Binary decision trees; Biometric authentication system; Biometric verification; Gini Index; Hong-kong; Matching score; Optimal fusion; Recent trends; Score-level fusion; Security application; Security region; Security requirements; Tree nodes; Verification systems; Algorithms; Artificial intelligence; Biometrics; Database systems; Decision making; Decision trees; Binary trees",Article,Scopus,2-s2.0-84867665596
"Shahwan A., Léon J.-C., Foucault G., Trlin M., Palombi O.","Qualitative behavioral reasoning from components' interfaces to components' functions for DMU adaption to FE analyses",2013,"CAD Computer Aided Design",13,10.1016/j.cad.2012.10.021,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868210000&doi=10.1016%2fj.cad.2012.10.021&partnerID=40&md5=23c32e417abe447776caafdc722abd94","A digital mock-up (DMU), with its B-Rep model of product components, is a standard industrial representation that lacks geometric information about interfaces between components. Component shapes reflect common engineering practices that influence component interfaces with interferences and not only contacts. The proposed approach builds upon relationships between function, behavior, and shape to derive functional information from the geometry of component interfaces. Among these concepts, the concept of behavior is more difficult to set up and connect to the geometry of interfaces and functions. Indeed, states and design rules are introduced to express the behavior of components through a qualitative reasoning process. This reasoning process, in turn, takes advantage of domain knowledge rules and facts, checking the validity of certain hypotheses that must hold true all along a specific state of the product's lifecycle, such as operational, stand-by or relaxed states. Eliminating configurations that contradict one or more of those hypotheses in their corresponding reference state reduces ambiguity, subsequently producing functional information in a bottom-up manner. This bottom-up process starts with the generation of a conventional interfaces graph (CIG) with components as nodes, and conventional interfaces (CIs) as arcs. A CI is initially defined by a geometric interaction that can be a contact or an interference between two components. CIs are then populated with functional interpretations (FIs) according to their geometric properties, producing potentially many combinations. A first step of the reasoning process, the validation against reference states, reduces the number of FIs per CI. Domain knowledge rules are then applied again to group semantics of component interfaces into one functional designation per component to connect together geometric entities of its boundary with its function. © 2012 Elsevier Ltd. All rights reserved.","DMUs; Functional designation; Geometric model; Product design; Reasoning and knowledge management; Semantics","B-rep models; Behavioral reasoning; Bottom-up manner; Certain hypothesis; Component interfaces; Component shape; Design rules; Digital mock-ups; DMUs; Domain knowledge; Engineering practices; FE analysis; Functional designation; Functional information; Functional interpretations; Geometric entities; Geometric information; Geometric models; Geometric properties; Qualitative reasoning; Reasoning process; Reference state; Relaxed state; Specific state; Two-component; Artificial intelligence; Geometry; Knowledge management; Product design; Semantics; Contacts (fluid mechanics)",Conference Paper,Scopus,2-s2.0-84868210000
"Nichols E., McDaid L.J., Siddique N.","Biologically inspired SNN for robot control",2013,"IEEE Transactions on Cybernetics",13,10.1109/TSMCB.2012.2200674,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890433115&doi=10.1109%2fTSMCB.2012.2200674&partnerID=40&md5=27c4e29eb71141fc1056b3d60d5df4e9","This paper proposes a spiking-neural-network-based robot controller inspired by the control structures of biological systems. Information is routed through the network using facilitating dynamic synapses with short-term plasticity. Learning occurs through long-term synaptic plasticity which is implemented using the temporal difference learning rule to enable the robot to learn to associate the correct movement with the appropriate input conditions. The network self-organizes to provide memories of environments that the robot encounters. A Pioneer robot simulator with laser and sonar proximity sensors is used to verify the performance of the network with a wall-following task, and the results are presented. © 2012 IEEE.","Dynamic synapses; Self-organization; Spiking neural network (SNN); Temporal difference (TD) learning rule","Biologically inspired; Control structure; Dynamic synapsis; Self organizations; Short term plasticity; Spiking neural network(SNN); Synaptic plasticity; Temporal difference learning; Neural networks; Robots; artificial intelligence; artificial neural network; biological model; computer simulation; nerve cell plasticity; procedures; robotics; Artificial Intelligence; Computer Simulation; Models, Neurological; Neural Networks (Computer); Neuronal Plasticity; Robotics",Article,Scopus,2-s2.0-84890433115
"Song L., Langfelder P., Horvath S.","Random generalized linear model: A highly accurate and interpretable ensemble predictor",2013,"BMC Bioinformatics",13,10.1186/1471-2105-14-5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872177797&doi=10.1186%2f1471-2105-14-5&partnerID=40&md5=55f0ef232b907de3dc48ff09c1b5844c","Background: Ensemble predictors such as the random forest are known to have superior accuracy but their black-box predictions are difficult to interpret. In contrast, a generalized linear model (GLM) is very interpretable especially when forward feature selection is used to construct the model. However, forward feature selection tends to overfit the data and leads to low predictive accuracy. Therefore, it remains an important research goal to combine the advantages of ensemble predictors (high accuracy) with the advantages of forward regression modeling (interpretability). To address this goal several articles have explored GLM based ensemble predictors. Since limited evaluations suggested that these ensemble predictors were less accurate than alternative predictors, they have found little attention in the literature.Results: Comprehensive evaluations involving hundreds of genomic data sets, the UCI machine learning benchmark data, and simulations are used to give GLM based ensemble predictors a new and careful look. A novel bootstrap aggregated (bagged) GLM predictor that incorporates several elements of randomness and instability (random subspace method, optional interaction terms, forward variable selection) often outperforms a host of alternative prediction methods including random forests and penalized regression models (ridge regression, elastic net, lasso). This random generalized linear model (RGLM) predictor provides variable importance measures that can be used to define a "" thinned"" ensemble predictor (involving few features) that retains excellent predictive accuracy.Conclusion: RGLM is a state of the art predictor that shares the advantages of a random forest (excellent predictive accuracy, feature importance measures, out-of-bag estimates of accuracy) with those of a forward selected generalized linear model (interpretability). These methods are implemented in the freely available R software package randomGLM. © 2013 Song et al.; licensee BioMed Central Ltd.",,"Comprehensive evaluation; Forward feature selections; Forward regression; Generalized linear model; Prediction methods; Predictive accuracy; Random subspace method; Variable importances; Regression analysis; Decision trees; animal; article; artificial intelligence; computer program; diseases; gene expression profiling; genetics; genomics; human; methodology; mouse; regression analysis; statistical model; Animals; Artificial Intelligence; Disease; Gene Expression Profiling; Genomics; Humans; Linear Models; Mice; Regression Analysis; Software",Article,Scopus,2-s2.0-84872177797
"Hadded R., Nouiri I., Alshihabi O., Maßmann J., Huber M., Laghouane A., Yahiaoui H., Tarhouni J.","A Decision Support System to Manage the Groundwater of the Zeuss Koutine Aquifer Using the WEAP-MODFLOW Framework",2013,"Water Resources Management",13,10.1007/s11269-013-0266-7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876441355&doi=10.1007%2fs11269-013-0266-7&partnerID=40&md5=cd4feae7f55129dc2039c03dd920a98b","This paper describes the development of a Decision Support System (DSS) for groundwater management of the 'Zeuss Koutine' aquifer in southeastern Tunisia using the WEAP-MODFLOW framework. First, a monthly MODFLOW model was developed to simulate the behaviour of the studied aquifer. A conceptual model of the study area was designed and a WEAP schematic representing the real hydraulic system was developed. In addition to the studied aquifer, other water resources available in the region, such as desalination plants and groundwater, were taken into consideration in this DSS. Inputs to the hydrogeological model include natural recharge and inflow from higher neighbouring aquifers. Outputs are mainly agricultural, touristic and urban water consumption. It was shown that the DSS developed was able to evaluate water management scenarios up to 2030, especially future water consumption, transmission link flow and active cell heads of the MODFLOW model for each time step. Results for the Zeuss Koutine aquifer demonstrated that desalination plants already built in the cities of Jerba and Zarzis have contributed to decreasing the continuous drawdown observed before 1999. The use of a sea water desalination plant to supply Jerba and Zarzis in the future is a solution for reducing the Zeuss Koutine aquifer drawdown. Defining its optimal capacity over time poses a new research question. © 2013 Springer Science+Business Media Dordrecht.","DSS; Groundwater management; MODFLOW; Tunisia; WEAP; Zeuss Koutine","DSS; Groundwater management; MODFLOW; Tunisia; WEAP; Zeuss Koutine; Aquifers; Artificial intelligence; Decision support systems; Desalination; Hydraulic equipment; Hydrogeology; Water filtration; Water levels; Water management; Water supply; Groundwater resources; aquifer; conceptual framework; decision support system; desalination; flow modeling; groundwater; hydraulic property; hydrogeology; water management; water use; Tunisia",Article,Scopus,2-s2.0-84876441355
"Wu S.-L., Wu C.-W., Pal N.R., Chen C.-Y., Chen S.-A., Lin C.-T.","Common spatial pattern and linear discriminant analysis for motor imagery classification",2013,"Proceedings of the 2013 IEEE Symposium on Computational Intelligence, Cognitive Algorithms, Mind, and Brain, CCMB 2013 - 2013 IEEE Symposium Series on Computational Intelligence, SSCI 2013",13,10.1109/CCMB.2013.6609178,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886580074&doi=10.1109%2fCCMB.2013.6609178&partnerID=40&md5=6d2e21ee888aa94f36b14ef9e8100e0c","A Brain-Computer Interface (BCI) system provides a convenient way of communication for healthy subjects and subjects who suffer from severe diseases such as amyotrophic lateral sclerosis (ALS). Motor imagery (MI) is one of the popular ways of designing BCI systems. The architecture of many BCI system is quite complex and they involve time consuming processing. The electroencephalography (EEG) signal is the most commonly used inputs for BCI applications but EEG is often contaminated with noise. To overcome such drawbacks, in this paper we use the common spatial pattern (CSP) for feature extraction from EEG and the linear discriminant analysis (LDA) for motor imagery classification. In this study, CSP and LDA have been used to reduce the artifact and classify MI-based EEG signal. We have used two-level cross validation scheme to determine the subject specific best time window and number of CSP features. We have compared the performance of our system with BCI competition results. We have also experimented with MI data generated in our lab. The proposed system is found to produce good results. In particular, using our EEG data for MI movements, we have obtained an average classification accuracy of 80% for two subjects using only 9 channels, without any feature selection. This proposed MI-based BCI system may be used in real life applications. © 2013 IEEE.","Brain-Computer Interface (BCI); common spatial pattern (CSP); electroencephalography (EEG); linear discriminant analysis (LDA); Motor imagery (MI)","Algorithms; Artificial intelligence; Electroencephalography; Electrophysiology; Feature extraction; Neurodegenerative diseases; Amyotrophic lateral sclerosis; Classification accuracy; Common spatial patterns; Healthy subjects; Linear discriminant analysis; Motor imagery; Motor imagery classification; Real-life applications; Brain computer interface",Conference Paper,Scopus,2-s2.0-84886580074
"Mookiah M.R.K., Acharya U.R., Chua C.K., Min L.C., Ng E.Y.K., Mushrif M.M., Laude A.","Automated detection of optic disk in retinal fundus images using intuitionistic fuzzy histon segmentation",2013,"Proceedings of the Institution of Mechanical Engineers, Part H: Journal of Engineering in Medicine",13,10.1177/0954411912458740,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877139261&doi=10.1177%2f0954411912458740&partnerID=40&md5=5204f1beaac8ad8fde6ee29ee0363f52","The human eye is one of the most sophisticated organs, with perfectly interrelated retina, pupil, iris cornea, lens, and optic nerve. Automatic retinal image analysis is emerging as an important screening tool for early detection of eye diseases. Uncontrolled diabetic retinopathy (DR) and glaucoma may lead to blindness. The identification of retinal anatomical regions is a prerequisite for the computer-aided diagnosis of several retinal diseases. The manual examination of optic disk (OD) is a standard procedure used for detecting different stages of DR and glaucoma. In this article, a novel automated, reliable, and efficient OD localization and segmentation method using digital fundus images is proposed. General-purpose edge detection algorithms often fail to segment the OD due to fuzzy boundaries, inconsistent image contrast, or missing edge features. This article proposes a novel and probably the first method using the Attanassov intuitionistic fuzzy histon (A-IFSH)-based segmentation to detect OD in retinal fundus images. OD pixel intensity and column-wise neighborhood operation are employed to locate and isolate the OD. The method has been evaluated on 100 images comprising 30 normal, 39 glaucomatous, and 31 DR images. Our proposed method has yielded precision of 0.93, recall of 0.91, F-score of 0.92, and mean segmentation accuracy of 93.4%. We have also compared the performance of our proposed method with the Otsu and gradient vector flow (GVF) snake methods. Overall, our result shows the superiority of proposed fuzzy segmentation technique over other two segmentation methods. © IMechE 2012.","Glaucoma; Gradient vector flow; Histon; Intuitionistic fuzzy set; Retina","Glaucoma; Gradient vector flow; Histon; Intuitionistic fuzzy sets; Retina; Glaucoma; Gradient vector flow; Histon; Intuitionistic fuzzy sets; Retina; Computer aided diagnosis; Edge detection; Eye protection; Fuzzy sets; Ophthalmology; Computer aided diagnosis; Diagnosis; Edge detection; Eye protection; Fuzzy sets; Ophthalmology; Image segmentation; Image segmentation; adult; aged; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; diabetic retinopathy; female; fluorescence angiography; fuzzy logic; glaucoma; human; male; methodology; middle aged; optic disk; pathology; reproducibility; retinoscopy; sensitivity and specificity; Adult; Aged; Aged, 80 and over; Artificial Intelligence; Diabetic Retinopathy; Female; Fluorescein Angiography; Fuzzy Logic; Glaucoma; Humans; Image Interpretation, Computer-Assisted; Male; Middle Aged; Optic Disk; Pattern Recognition, Automated; Reproducibility of Results; Retinoscopy; Sensitivity and Specificity; Young Adult",Article,Scopus,2-s2.0-84877139261
"Portela F., Aguiar J., Santos M.F., Silva Á., Rua F.","Pervasive intelligent decision support system - Technology acceptance in Intensive Care Units",2013,"Advances in Intelligent Systems and Computing",13,10.1007/978-3-642-36981-0_27,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876229382&doi=10.1007%2f978-3-642-36981-0_27&partnerID=40&md5=23d6d0afa959a66cb6ace8be93253b1e","Intensive Care Units are considered a critical environment where the decision needs to be carefully taken. The real-time recognition of the condition of the patient is important to drive the decision process efficiently. In order to help the decision process, a Pervasive Intelligent Decision Support System (PIDSS) was developed. To provide a better comprehension of the acceptance of the PIDSS it is very important to assess how the users accept the system at level of usability and their importance in the Decision Making Process. This assessment was made using the four constructs proposed by the Technology Acceptance Methodology and a questionnaire-based approach guided by the Delphi Methodology. The results obtained so far show that although the users are satisfied with the offered information recognizing its importance, they demand for a faster system. © 2013 Springer-Verlag.","Decision Support System; INTCare; Intensive Care; Pervasive; TAM; Technology Acceptance; Technology Assessment","Artificial intelligence; Decision support systems; Information systems; Intensive care units; INTCare; Intensive care; Pervasive; TAM; Technology acceptance; Technology assessments; Technology",Conference Paper,Scopus,2-s2.0-84876229382
"Clifton A., Kilcher L., Lundquist J.K., Fleming P.","Using machine learning to predict wind turbine power output",2013,"Environmental Research Letters",13,10.1088/1748-9326/8/2/024009,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880911595&doi=10.1088%2f1748-9326%2f8%2f2%2f024009&partnerID=40&md5=da84a5a0b5382f0ec854735adeaed81a","Wind turbine power output is known to be a strong function of wind speed, but is also affected by turbulence and shear. In this work, new aerostructural simulations of a generic 1.5 MW turbine are used to rank atmospheric influences on power output. Most significant is the hub height wind speed, followed by hub height turbulence intensity and then wind speed shear across the rotor disk. These simulation data are used to train regression trees that predict the turbine response for any combination of wind speed, turbulence intensity, and wind shear that might be expected at a turbine site. For a randomly selected atmospheric condition, the accuracy of the regression tree power predictions is three times higher than that from the traditional power curve methodology. The regression tree method can also be applied to turbine test data and used to predict turbine performance at a new site. No new data are required in comparison to the data that are usually collected for a wind resource assessment. Implementing the method requires turbine manufacturers to create a turbine regression tree model from test site data. Such an approach could significantly reduce bias in power predictions that arise because of the different turbulence and shear at the new site, compared to the test site. © 2013 IOP Publishing Ltd.","classification and regression trees; machine learning; wind energy; wind turbine","Atmospheric thermodynamics; Forecasting; Forestry; Learning systems; Regression analysis; Thermoelectric power; Turbulence; Wind effects; Wind power; Wind turbines; Atmospheric conditions; Classification and regression tree; Regression tree models; Turbine performance; Turbulence and shears; Turbulence intensity; Wind resource assessment; Wind turbine power; Trees (mathematics); accuracy assessment; artificial intelligence; equipment component; numerical model; performance assessment; prediction; productivity; regression analysis; resource assessment; turbulence; wind power; wind shear; wind turbine; wind velocity; Classification; Forestry; Mathematics; Regression Analysis; Trees; Turbulence",Article,Scopus,2-s2.0-84880911595
"Lai L.L., Zhang H.T., Lai C.S., Xu F.Y., Mishra S.","Investigation on July 2012 Indian blackout",2013,"Proceedings - International Conference on Machine Learning and Cybernetics",13,10.1109/ICMLC.2013.6890450,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907260883&doi=10.1109%2fICMLC.2013.6890450&partnerID=40&md5=fb509fbbf1fe87b198476927f9f7199a","Twice Indian blackouts occurred at the end of July in 2012 left over 600 million of people in the dark for several hours. In these two-day, Indian grid disturbances were regarded as the most serious and large-scale blackout in the world in history. A report has been generated by the enquiry committee which was organized by the Ministry of Power, Government of India, to investigate the factors which led to the initiation of the grid disturbance. Recommendations were also generated by the committee in order to provide the plan for Indian grid enhancement. Further to the recommendations by Enquiry Committee, this paper will give further suggestions to minimize blackouts in future. An insight into decision support requirement for power network operation will be made. © 2013 IEEE.","blackout; decision support; Indian grid; strategic planning","Artificial intelligence; Decision support systems; Learning systems; Strategic planning; blackout; Decision supports; Government of India; Grid disturbance; Indian grids; Large-scale blackout; Ministry of power; Power networks; Outages",Conference Paper,Scopus,2-s2.0-84907260883
"Voss M.S.","Social programming using functional swarm optimization",2013,"2003 IEEE Swarm Intelligence Symposium, SIS 2003 - Proceedings",13,10.1109/SIS.2003.1202254,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942089494&doi=10.1109%2fSIS.2003.1202254&partnerID=40&md5=225102ea1ab842ab5ef2428ab78310e2","The development of mathematical neural networks was based on an analogy with biological neural networks found in nature. Recently there has been a resurgence in research and understanding in self-organizing networks that are based on other metaphors: genetics, immune systems etc. In this paper a new methodology is presented for creating complex adaptive functional networks (CAFN) that are based on the particle swarm social-psychological metaphor. The proposed social programming methodology is based on combining the particle swarm methodology with the group method of data handling and Cartesian programming. © 2003 IEEE.","Adaptive systems; Equations; Evolutionary computation; Functional programming; Genetics; Immune system; Neural networks; Particle swarm optimization; Psychology; Self-organizing networks","Adaptive systems; Artificial intelligence; Chromosomes; Complex networks; Data handling; Evolutionary algorithms; Immune system; Neural networks; Particle swarm optimization (PSO); Biological neural networks; Equations; Genetics; Group method of data handling; Mathematical neural networks; Psychology; Self-organizing network; Social programming; Functional programming",Conference Paper,Scopus,2-s2.0-84942089494
"Sun L., Chen W.","The improved ChinaCCS decision support system: A case study for Beijing-Tianjin-Hebei Region of China",2013,"Applied Energy",13,10.1016/j.apenergy.2013.05.016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884205638&doi=10.1016%2fj.apenergy.2013.05.016&partnerID=40&md5=1726c7f3e61415d464185226c69d06a9","Wide employment of CO2 capture and storage (CCS) technology is of great importance to the sustainable development of China thanks to its contributions to climate change mitigation. Considering the fact that coal accounts for over 65% in the primary energy consumption mix of China, commensurate infrastructure should be built up to transport CO2 to storage sinks from emission sources. The paper presents the improved ChinaCCS Decision Support System (DSS), a tool developed by the General Algebraic Modeling System (GAMS) for optimizing CO2 transportation network given a set of sources and sinks by generating a fully integrated, cost-minimizing CCS system. The improved ChinaCCS DSS assists in determining the sources, sinks and amounts for CO2 capture and sequestration, as well as the paths and sizes of pipelines, while minimizing the net present value for sequestrating a given amount of CO2. Pipeline construction costs take into account topographic conditions such as slopes, heavily populated areas, rivers, railway, highway, etc. A case study for Beijing-Tianjin-Hebei Region of China was demonstrated in the paper. The results highlighted the importance of systematic planning for CCS infrastructure. © 2013 Elsevier Ltd.","Carbon capture and storage; Pipeline network; Source-sink matching","Artificial intelligence; Carbon capture; Coal storage; Decision support systems; Energy utilization; Pipelines; Beijing-tianjin-hebei regions; Carbon capture and storage; Climate change mitigation; Decision support system (dss); Pipeline networks; Primary energy consumption; Source-sink; Transportation network; Carbon dioxide; carbon emission; cost-benefit analysis; decision support system; environmental planning; optimization; pipeline; pollutant transport; sustainable development; topography; Beijing [China]; China",Article,Scopus,2-s2.0-84884205638
"Palmer D.W., Kirschenbaum M., Murton J.P., Kovacina M.A., Steinberg D.H., Calabrese S.N., Zajac K.M., Hantak C.M., Schatz J.E.","Using a collection of humans as an execution testbed for swarm algorithms",2013,"2003 IEEE Swarm Intelligence Symposium, SIS 2003 - Proceedings",13,10.1109/SIS.2003.1202248,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942090200&doi=10.1109%2fSIS.2003.1202248&partnerID=40&md5=f54e6d8dae03b88f742f56bc9a44ca10","To gain insight into swarm algorithms, researchers can study insect societies and other natural collectives, program multi-agent software simulations or build groups of cooperating robots. In our research, we consider another resource: swarms of humans. Human swarms provide three primary benefits: quick feedback and evaluation of swarm algorithms, experience with high-level swarm directives instead of low-level agent programs, and a source of swarm algorithms that can potentially be reverse-engineered for use in other applications. Planning is a human's preferred problem solving methodology because we are intelligent creatures with high-level communication skills. Due to the intelligence of the agents, human swarms can be quickly programmed, for subsequent observation and analysis. This paper describes human swarm experiments designed for gathering information on swarm algorithms. At these events 100 volunteers, wearing data-encoded T-shirts, work together to perform tasks of differing degrees of complexity. Researchers provide simple instructions for each task (programming the swarm), record the swarm's behavior (videotaped observation) and analyze the results (problem identification and algorithm-mining). We demonstrate the viability of this research by presenting the quick identification of a swarm algorithm ""bug"" and by producing a software implementation of a swarm algorithm gleaned from our observations. © 2003 IEEE.","Algorithm design and analysis; Feedback; Humans; Insects; Intelligent agent; Probability; Problem-solving; Robots; Software algorithms; Testing","Algorithms; Application programs; Artificial intelligence; Feedback; Intelligent agents; Intelligent robots; Machine design; Multi agent systems; Probability; Problem solving; Robots; Software testing; Testing; Video recording; Algorithm design and analysis; Degrees of complexity; Humans; Insects; Multi-agent softwares; Problem identification; Software algorithms; Software implementation; Software agents",Conference Paper,Scopus,2-s2.0-84942090200
"Van Erp M., Rizzo G., Troncy R.","Learning with the web: Spotting named entities on the intersection of NERD and machine learning",2013,"CEUR Workshop Proceedings",13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922327143&partnerID=40&md5=67e5088badb0a56118fc0bcae3cf3e1b","Microposts shared on social platforms instantaneously report facts, opinions or emotions. In these posts, entities are often used but they are continuously changing depending on what is currently trending. In such a scenario, recognising these named entities is a challenging task, for which off-the-shelf approaches are not well equipped. We propose NERD-ML, an approach that unifies the benefits of a crowd entity recognizer through Web entity extractors combined with the linguistic strengths of a machine learning classifier. Copyright © 2013 held by author(s)/owner(s).","Machine Learning; Named Entity Recognition; NERD","Artificial intelligence; Natural language processing systems; Social networking (online); Text processing; World Wide Web; Named entities; Named entity recognition; NERD; Web entities; Learning systems",Conference Paper,Scopus,2-s2.0-84922327143
"López M., Iglesias G.","Artificial Intelligence for estimating infragravity energy in a harbour",2013,"Ocean Engineering",13,10.1016/j.oceaneng.2012.08.009,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867431319&doi=10.1016%2fj.oceaneng.2012.08.009&partnerID=40&md5=1b633183a0b8fb88bfbdc8d953b8b395","The estimation of long waves inside a harbour is a matter of great importance for port management. The objective of this work is to apply Artificial Intelligence to estimate the significant infragravity wave height inside a harbour. Two Artificial Neural Network (ANN) models with the same input (the short wave parameters outside the harbour and the tidal level) are developed and compared. The first is a one-step model that estimates the significant infragravity wave height inside the harbour directly. The second is a two-step model that computes the infragravity wave height first outside, then inside the harbour. The two models are trained and successfully validated based on observations at the Port of Ferrol (NW Spain), where seiching is known to occur. The network architecture that performs best for each model is selected using a k-fold cross-validation method. The estimation of the infragravity wave height outside the harbour with the two-step model is shown to be more accurate than that from a widely used empirical expression. As regards the all-important estimation inside the harbour, the one-step model is found to perform better than its two-step counterpart. © 2012 Elsevier Ltd.","Artificial Intelligence; Artificial Neural Networks; Harbour resonance; Infragravity waves; Long waves; Seiche","Artificial neural network models; Cross-validation methods; Empirical expression; Infragravity; Infragravity waves; Long waves; NW Spain; One-step model; Port management; Seiche; Short waves; Tidal level; TWo-step model; Artificial intelligence; Estimation; Gravity waves; Network architecture; Neural networks; Water waves; Ports and harbors; artificial intelligence; artificial neural network; gravity wave; harbor; ocean wave; port operation; seiche; wave height; El Ferrol; Galicia [Spain]; La Coruna [Galicia]; Spain",Article,Scopus,2-s2.0-84867431319
"Lele C., Nganou J.B.","MV-algebras derived from ideals in BL-algebras",2013,"Fuzzy Sets and Systems",13,10.1016/j.fss.2012.09.014,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875472151&doi=10.1016%2fj.fss.2012.09.014&partnerID=40&md5=09c5c279f974431c2d3e68f384ff18eb","We introduce the notion of ideal in BL-algebras as a natural generalization of that of ideal in MV-algebras. We show that from a purely algebraic point of view, the introduced notion has the proper meaning. Among other things, we establish that an ideal is prime if and only if the quotient BL-algebra is an MV-chain, and also that the MV-center of a BL-algebra as treated by Turunen and Sessa is simply the factor of the BL-algebra by the trivial ideal. We also analyze the relationship between ideals and deductive systems using the set of complement elements. It is our hope that this work will settle once and for all the existence of ideals in BL-algebras settings. © 2012 Elsevier B.V.","BL-algebra; Ideal; MV-algebra; MV-center; MV-chain; Prime ideal","BL-algebra; Ideal; MV-algebras; MV-center; MV-chain; Prime ideal; Artificial intelligence; Fuzzy sets; Algebra",Article,Scopus,2-s2.0-84875472151
"Cong Z., De Schutter B., Babuška R.","Ant Colony Routing algorithm for freeway networks",2013,"Transportation Research Part C: Emerging Technologies",13,10.1016/j.trc.2013.09.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885911607&doi=10.1016%2fj.trc.2013.09.008&partnerID=40&md5=ade9033af140ebd15f6b0a8b6eebe34d","Dynamic traffic routing refers to the process of (re)directing vehicles at junctions in a traffic network according to the evolving traffic conditions. The traffic management center can determine desired routes for drivers in order to optimize the performance of the traffic network by dynamic traffic routing. However, a traffic network may have thousands of links and nodes, resulting in a large-scale and computationally complex non-linear, non-convex optimization problem. To solve this problem, Ant Colony Optimization (ACO) is chosen as the optimization method in this paper because of its powerful optimization heuristic for combinatorial optimization problems. ACO is implemented online to determine the control signal - i.e., the splitting rates at each node. However, using standard ACO for traffic routing is characterized by four main disadvantages: 1. traffic flows for different origins and destinations cannot be distinguished; 2. all ants may converge to one route, causing congestion; 3. constraints cannot be taken into account; and 4. neither can dynamic link costs. These problems are addressed by adopting a novel ACO algorithm with stench pheromone and with colored ants, called Ant Colony Routing (ACR). Using the stench pheromone, the ACR algorithm can distribute the vehicles over the traffic network with less or no traffic congestion, as well as reduce the number of vehicles near some sensitive zones, such as hospitals and schools. With colored ants, the traffic flows for multiple origins and destinations can be represented. The proposed approach is also implemented in a simulation-based case study in the Walcheren area, the Netherlands, illustrating the effectiveness of the approach. © 2013 Elsevier Ltd.","Ant Colony Optimization; Dynamic traffic routing; Model Predictive Control; Stench pheromone","Ant colony optimization; Artificial intelligence; Combinatorial optimization; Convex optimization; Heuristic methods; Highway traffic control; Model predictive control; Motor transportation; Network routing; Optimization; Problem solving; Routing algorithms; Traffic control; Transportation; Vehicles; Algorithms; Complex networks; Ant Colony Optimization (ACO); Combinatorial optimization problems; Dynamic traffic routing; Nonconvex optimization; Number of vehicles; Optimization method; Stench pheromone; Traffic management centers; Traffic congestion; algorithm; computer simulation; control system; dynamic analysis; implementation process; motorway; network analysis; optimization; pheromone; traffic management; traffic congestion; Netherlands; Formicidae",Article,Scopus,2-s2.0-84885911607
"Cracknell M.J., Reading A.M.","The upside of uncertainty: Identification of lithology contact zones from airborne geophysics and satellite data using random forests and support vector machines",2013,"Geophysics",13,10.1190/GEO2012-0411.1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888028156&doi=10.1190%2fGEO2012-0411.1&partnerID=40&md5=96127d6dd58d2d5c53266203b5c44f2d","Inductive machine learning algorithms attempt to recognize patterns in, and generalize from empirical data. They provide a practical means of predicting lithology, or other spatially varying physical features, from multidimensional geophysical data sets. It is for this reason machine learning approaches are increasing in popularity for geophysical data inference. A key motivation for their use is the ease with which uncertainty measures can be estimated for nonprobabilistic algorithms. We have compared and evaluated the abilities of two nonprobabilistic machine learning algorithms, random forests (RF) and support vector machines (SVM), to recognize ambiguous supervised classification predictions using uncertainty calculated from estimates of class membership probabilities. We formulated a method to establish optimal uncertainty threshold values to identify and isolate the maximum number of incorrect predictions while preserving most of the correct classifications. This is illustrated using a case example of the supervised classification of surface lithologies in a folded, structurally complex, metamorphic terrain. We found that (1) the use of optimal uncertainty thresholds significantly improves overall classification accuracy of RF predictions, but not those of SVM, by eliminating the maximum number of incorrectly classified samples while preserving the maximum number of correctly classified samples; (2) RF, unlike SVM, was able to exploit dependencies and structures contained within spatially varying input data; and (3) high RF prediction uncertainty is spatially coincident with transitions in lithology and associated contact zones, and regions of intense deformation. Uncertainty has its upside in the identification of areas of key geologic interest and has wide application across the geosciences, where transition zones are important classes in their own right. The techniques used in this study are of practical value in prioritizing subsequent geologic field activities, which, with the aid of this analysis, may be focused on key lithology contacts and problematic localities. © 2013 Society of Exploration Geophysicists.",,"Algorithms; Artificial intelligence; Decision trees; Forecasting; Geophysics; Learning systems; Lithology; Support vector machines; Uncertainty analysis; Class membership probabilities; Classification accuracy; Inductive machine learning; Machine learning approaches; Metamorphic terrains; Prediction uncertainty; Supervised classification; Uncertainty measures; Learning algorithms; algorithm; contact zone; data set; forest; lithology; satellite data; transition zone; uncertainty analysis",Article,Scopus,2-s2.0-84888028156
"Riaño D., Collado A.","Model-based combination of treatments for the management of chronic comorbid patients",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-642-38326-7_2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887298108&doi=10.1007%2f978-3-642-38326-7_2&partnerID=40&md5=180dcccaa3e050cab526938c302a365b","The prevalence of chronic diseases is growing year after year. This implies that health care systems must deal with an increasing number of patients with several simultaneous pathologies (i.e., comorbid patients), which involves interventions combining primary, specialist, and hospital cares. Clinical practice guidelines provide evidence-based information on these interventions, but only on individual pathologies. This sets up the urgent need of developing ways of merging multiple single-disease interventions to provide professional assistance to comorbid patients. Here, we propose an integrated care model formalizing the treatment of chronic comorbid patients across primary, specialist and hospital cares. The model establishes the baseline of a divide-and-conquer approach to the complex task of multiple therapy combination that was tested on the comorbidity of hypertension and chronic heart failure. © 2013 Springer-Verlag.","Clinical Decision Support Systems; Computerized Clinical Practice Guidelines; Health Care Modeling; Therapy Combination","Artificial intelligence; Decision support systems; Hospitals; Medical applications; Pathology; Chronic heart failures; Clinical decision support systems; Clinical practice guidelines; Divide-and-conquer approach; Health care model; Health-care system; Professional assistance; Therapy Combination; Patient treatment",Conference Paper,Scopus,2-s2.0-84887298108
"Elghazel H., Aussem A.","Unsupervised feature selection with ensemble learning",2013,"Machine Learning",13,10.1007/s10994-013-5337-8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920709599&doi=10.1007%2fs10994-013-5337-8&partnerID=40&md5=8b41390f80fb57e0c36f6c26a33fa31f","In this paper, we show that the way internal estimates are used to measure variable importance in Random Forests are also applicable to feature selection in unsupervised learning. We propose a new method called Random Cluster Ensemble (RCE for short), that estimates the out-of-bag feature importance from an ensemble of partitions. Each partition is constructed using a different bootstrap sample and a random subset of the features. We provide empirical results on nineteen benchmark data sets indicating that RCE, boosted with a recursive feature elimination scheme (RFE) (Guyon and Elisseeff, Journal of Machine Learning Research, 3:1157–1182, 2003), can lead to significant improvement in terms of clustering accuracy, over several state-of-the-art supervised and unsupervised algorithms, with a very limited subset of features. The method shows promise to deal with very large domains. All results, datasets and algorithms are available on line (http://perso.univ-lyon1.fr/haytham.elghazel/RCE.zip). © 2013, The Author(s).","Ensemble methods; Feature selection; Random forest; Unsupervised learning","Artificial intelligence; Decision trees; Learning systems; Unsupervised learning; Clustering accuracy; Ensemble methods; Machine learning research; Random forests; Recursive feature elimination; Unsupervised algorithms; Unsupervised feature selection; Variable importances; Feature extraction",Article,Scopus,2-s2.0-84920709599
"Wu G., Kim M., Wang Q., Gao Y., Liao S., Shen D.","Unsupervised deep feature learning for deformable registration of MR brain images.",2013,"Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention",13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892841517&partnerID=40&md5=c0dec09508b8b4aa4f66aa2f1328be4e","Establishing accurate anatomical correspondences is critical for medical image registration. Although many hand-engineered features have been proposed for correspondence detection in various registration applications, no features are general enough to work well for all image data. Although many learning-based methods have been developed to help selection of best features for guiding correspondence detection across subjects with large anatomical variations, they are often limited by requiring the known correspondences (often presumably estimated by certain registration methods) as the ground truth for training. To address this limitation, we propose using an unsupervised deep learning approach to directly learn the basis filters that can effectively represent all observed image patches. Then, the coefficients by these learnt basis filters in representing the particular image patch can be regarded as the morphological signature for correspondence detection during image registration. Specifically, a stacked two-layer convolutional network is constructed to seek for the hierarchical representations for each image patch, where the high-level features are inferred from the responses of the low-level network. By replacing the hand-engineered features with our learnt data-adaptive features for image registration, we achieve promising registration results, which demonstrates that a general approach can be built to improve image registration by using data-adaptive features through unsupervised deep learning.",,"algorithm; article; artificial intelligence; automated pattern recognition; brain; computer assisted diagnosis; histology; human; image enhancement; image subtraction; information retrieval; methodology; nuclear magnetic resonance imaging; reproducibility; sensitivity and specificity; Algorithms; Artificial Intelligence; Brain; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Information Storage and Retrieval; Magnetic Resonance Imaging; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-84892841517
"De Assis M.V.O., Carvalho L.F., Rodrigues J.J.P.C., Proenca M.L.","Holt-Winters statistical forecasting and ACO metaheuristic for traffic characterization",2013,"IEEE International Conference on Communications",13,10.1109/ICC.2013.6654913,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891348090&doi=10.1109%2fICC.2013.6654913&partnerID=40&md5=8ad78fb5f64ebc356bd761ecfa4298fa","Due to modernization, expansion of computer networks has become an inevitable process. However, this growth is also accompanied by increased complexity, which makes it necessary to use resources that assist the management of these networks. In this paper, we propose a traffic characterization using two-dimensional flow analysis for modeling the behavior traffic pattern, here called Digital Signature of Network Segment Using Flow Analysis (DSNSF). To accomplish this task we have used the improved Holt-Winters forecasting and Ant Colony Optimization metaheuristic methods. The DSNSF obtained by each model are compared to a real traffic of packets and bits and then subjected to specific evaluations in order to measure its accuracy. © 2013 IEEE.","Ant Colony Optimization; DSNSF; Holt-Winters; NetFlow; Network Management; Traffic Characterization","Algorithms; Ant colony optimization; Artificial intelligence; Complex networks; Network management; DSNSF; Holt-Winters; NetFlows; Network segment; Statistical forecasting; Traffic characterization; Traffic pattern; Two-dimensional flow; Characterization",Conference Paper,Scopus,2-s2.0-84891348090
"Bottomley C., Van Belle V., Kirk E., Van Huffel S., Timmerman D., Bourne T.","Accurate prediction of pregnancy viability by means of a simple scoring system",2013,"Human Reproduction",13,10.1093/humrep/des352,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871222519&doi=10.1093%2fhumrep%2fdes352&partnerID=40&md5=461f602dc03f6ee04f91c0dc0c61cb75","Study Question What is the performance of a simple scoring system to predict whether women will have an ongoing viable intrauterine pregnancy beyond the first trimester? Summary Answer A simple scoring system using demographic and initial ultrasound variables accurately predicts pregnancy viability beyond the first trimester with an area under the curve (AUC) in a receiver operating characteristic curve of 0.924 [95% confidence interval (CI) 0.900-0.947] on an independent test set. What is Known Already Individual demographic and ultrasound factors, such as maternal age, vaginal bleeding and gestational sac size, are strong predictors of miscarriage. Previous mathematical models have combined individual risk factors with reasonable performance. A simple scoring system derived from a mathematical model that can be easily implemented in clinical practice has not previously been described for the prediction of ongoing viability.STUDY DESIGN, SIZEAND DURATIONThis was a prospective observational study in a single early pregnancy assessment centre during a 9-month period.PARTICIPANTS/MATERIALS, SETTINGAND Methods A cohort of 1881 consecutive women undergoing transvaginal ultrasound scan at a gestational age <84 days were included. Women were excluded if the first trimester outcome was not known. Demographic features, symptoms and ultrasound variables were tested for their influence on ongoing viability. Logistic regression was used to determine the influence on first trimester viability from demographics and symptoms alone, ultrasound findings alone and then from all the variables combined. Each model was developed on a training data set, and a simple scoring system was derived from this. This scoring system was tested on an independent test data set. Main Results AND THE ROLE OF CHANCEThe final outcome based on a total of 1435 participants was an ongoing viable pregnancy in 885 (61.7%) and early pregnancy loss in 550 (38.3%) women. The scoring system using significant demographic variables alone (maternal age and amount of bleeding) to predict ongoing viability gave an AUC of 0.724 (95% CI = 0.692-0.756) in the training set and 0.729 (95% CI = 0.684-0.774) in the test set. The scoring system using significant ultrasound variables alone (mean gestation sac diameter, mean yolk sac diameter and the presence of fetal heart beat) gave an AUC of 0.873 (95% CI = 0.850-0.897) and 0.900 (95% CI = 0.871-0.928) in the training and the test sets, respectively. The final scoring system using demographic and ultrasound variables together gave an AUC of 0.901 (95% CI = 0.881-0.920) and 0.924 (CI = 0.900-0.947) in the training and the test sets, respectively. After defining the cut-off at which the sensitivity is 0.90 on the training set, this model performed with a sensitivity of 0.92, specificity of 0.73, positive predictive value of 84.7% and negative predictive value of 85.4% in the test set. Limitations, Reasons for Caution BMI and smoking variables were a potential omission in the data collection and might further improve the model performance if included. A further limitation is the absence of information on either bleeding or pain in 18% of women. Caution should be exercised before implementation of this scoring system prior to further external validation studies Wider Implications of the Findings This simple scoring system incorporates readily available data that are routinely collected in clinical practice and does not rely on complex data entry. As such it could, unlike most mathematical models, be easily incorporated into normal early pregnancy care, where women may appreciate an individualized calculation of the likelihood of ongoing pregnancy viability. Study Funding/Competing Interest (S)Research by V.V.B. supported by Research Council KUL: GOA MaNet, PFV/10/002 (OPTEC), several PhD/postdoc & fellow grants; IWT: TBM070706-IOTA3, PhD Grants; IBBT; Belgian Federal Science Policy Office: IUAP P7/(DYSCO, 'Dynamical systems, control and optimization', 2012-2017). T.B. is supported by the Imperial Healthcare NHS Trust NIHR Biomedical Research Centre. Trial Registration Number Not applicable. © 2012 The Author.","first trimester; miscarriage; pregnancy; transvaginal ultrasound; viability","adolescent; adult; area under the curve; article; cohort analysis; confidence interval; female; fetus heart rate; first trimester pregnancy; gestational sac; human; logistic regression analysis; major clinical study; maternal age; mathematical model; observational study; predictive value; pregnancy; prospective study; receiver operating characteristic; risk factor; scoring system; sensitivity and specificity; spontaneous abortion; transvaginal echography; vagina bleeding; yolk sac; Adolescent; Adult; Artificial Intelligence; Cohort Studies; Embryo Loss; Female; Humans; London; Models, Biological; Pregnancy; Pregnancy Complications; Pregnancy Maintenance; Pregnancy Trimester, First; Prospective Studies; Risk; Sensitivity and Specificity; Severity of Illness Index; Ultrasonography, Prenatal; Young Adult",Article,Scopus,2-s2.0-84871222519
"Gibson B.R., Rogers T.T., Zhu X.","Human Semi-Supervised Learning",2013,"Topics in Cognitive Science",13,10.1111/tops.12010,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872794303&doi=10.1111%2ftops.12010&partnerID=40&md5=1eab5733fbc160a6bbe2926fc294f55f","Most empirical work in human categorization has studied learning in either fully supervised or fully unsupervised scenarios. Most real-world learning scenarios, however, are semi-supervised: Learners receive a great deal of unlabeled information from the world, coupled with occasional experiences in which items are directly labeled by a knowledgeable source. A large body of work in machine learning has investigated how learning can exploit both labeled and unlabeled data provided to a learner. Using equivalences between models found in human categorization and machine learning research, we explain how these semi-supervised techniques can be applied to human learning. A series of experiments are described which show that semi-supervised learning models prove useful for explaining human behavior when exposed to both labeled and unlabeled data. We then discuss some machine learning models that do not have familiar human categorization counterparts. Finally, we discuss some challenges yet to be addressed in the use of semi-supervised models for modeling human categorization. © 2013 Cognitive Science Society, Inc.","Category learning; Machine learning; Semi-supervised learning","article; artificial intelligence; classification; concept formation; decision making; empirical research; human; learning; probability; psychological model; reinforcement; statistical distribution; statistical model; Artificial Intelligence; Classification; Concept Formation; Decision Making; Empirical Research; Humans; Knowledge of Results (Psychology); Learning; Models, Psychological; Models, Statistical; Probability; Statistical Distributions",Article,Scopus,2-s2.0-84872794303
"Michalowski M., Wilk S., Michalowski W., Lin D., Farion K., Mohapatra S.","Using constraint logic programming to implement iterative actions and numerical measures during mitigation of concurrently applied clinical practice guidelines",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-642-38326-7_3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887298052&doi=10.1007%2f978-3-642-38326-7_3&partnerID=40&md5=8a2ed20d107b980f9fdfd92e26138ad2","There is a pressing need in clinical practice to mitigate (identify and address) adverse interactions that occur when a comorbid patient is managed according to multiple concurrently applied disease-specific clinical practice guidelines (CPGs). In our previous work we described an automatic algorithm for mitigating pairs of CPGs. The algorithm constructs logical models of processed CPGs and employs constraint logic programming to solve them. However, the original algorithm was unable to handle two important issues frequently occurring in CPGs - iterative actions forming a cycle and numerical measurements. Dealing with these two issues in practice relies on a physician's knowledge and the manual analysis of CPGs. Yet for guidelines to be considered stand-alone and an easy to use clinical decision support tool this process needs to be automated. In this paper we take an additional step towards building such a tool by extending the original mitigation algorithm to handle cycles and numerical measurements present in CPGs. © 2013 Springer-Verlag.","Clinical Decision Support Systems; Comorbidity; Computerized Clinical Practice Guidelines; Constraint Logic Programming","Algorithms; Artificial intelligence; Decision support systems; Iterative methods; Medical applications; Automatic algorithms; Clinical decision support; Clinical decision support systems; Clinical practice guidelines; Clinical practices; Co morbidities; Constraint Logic Programming; Original algorithms; Logic programming",Conference Paper,Scopus,2-s2.0-84887298052
"Sánchez-Garzón I., Fdez-Olivares J., Onaindía E., Milla G., Jordán J., Castejón P.","A multi-agent planning approach for the generation of personalized treatment plans of comorbid patients",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,10.1007/978-3-642-38326-7_4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887293121&doi=10.1007%2f978-3-642-38326-7_4&partnerID=40&md5=3e9c25e2745ed2d88fd40817658a21d4","This work addresses the generation of a personalized treatment plan from multiple clinical guidelines, for a patient with multiple diseases (comorbid patient), as a multi-agent cooperative planning process that provides support to collaborative medical decision-making. The proposal is based on a multi-agent planning architecture in which each agent is capable of (1) planning a personalized treatment from a temporal Hierarchical Task Network (HTN) representation of a single-disease guideline, and (2) coordinating with other planning agents by both sharing disease specific knowledge, and resolving the eventual conflicts that may arise when conciliating different guidelines by merging single-disease treatment plans. The architecture follows a life cycle that starting from a common specification of the main high-level steps of a treatment for a given comorbid patient, results in a detailed treatment plan without harmful interactions among the single-disease personalized treatments. © 2013 Springer-Verlag.","comorbidity; guideline conciliation; multi-agent planning","Artificial intelligence; Network architecture; Patient treatment; Clinical guideline; Co morbidities; Co-operative planning; Common specification; guideline conciliation; Hierarchical task networks; Medical decision making; Multi-agent planning; Diseases",Conference Paper,Scopus,2-s2.0-84887293121
"Chaves-González J.M., Vega-Rodríguez M.A., Granado-Criado J.M.","A multiobjective swarm intelligence approach based on artificial bee colony for reliable DNA sequence design",2013,"Engineering Applications of Artificial Intelligence",13,10.1016/j.engappai.2013.04.011,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888343199&doi=10.1016%2fj.engappai.2013.04.011&partnerID=40&md5=4ad2aa46c43b8042ef2beca6987c021c","The design of reliable DNA sequences is crucial in many engineering applications which depend on DNA-based technologies, such as nanotechnology or DNA computing. In these cases, two of the most important properties that must be controlled to obtain reliable sequences are self-assembly and self-complementary hybridization. These processes have to be restricted to avoid undesirable reactions, because in the specific case of DNA computing, undesirable reactions usually lead to incorrect computations. Therefore, it is important to design robust sets of sequences which provide efficient and reliable computations. The design of reliable DNA sequences involves heterogeneous and conflicting design criteria that do not fit traditional optimization methods. In this paper, DNA sequence design has been formulated as a multiobjective optimization problem and a novel multiobjective approach based on swarm intelligence has been proposed to solve it. Specifically, a multiobjective version of the Artificial Bee Colony metaheuristics (MO-ABC) is developed to tackle the problem. MO-ABC takes in consideration six different conflicting design criteria to generate reliable DNA sequences that can be used for biomolecular computing. Moreover, in order to verify the effectiveness of the novel multiobjective proposal, formal comparisons with the well-known multiobjective standard NSGA-II (fast non-dominated sorting genetic algorithm) were performed. After a detailed study, results indicate that our artificial swarm intelligence approach obtains satisfactory reliable DNA sequences. Two multiobjective indicators were used in order to compare the developed algorithms: hypervolume and set coverage. Finally, other relevant works published in the literature were also studied to validate our results. To this respect the conclusion that can be drawn is that the novel approach proposed in this paper obtains very promising DNA sequences that significantly surpass other results previously published. © 2013 Elsevier Ltd. All rights reserved.","Artificial Bee Colony; DNA sequence design; Molecular computing; Multiobjective evolutionary algorithm; Multiobjective optimization; Swarm intelligence","Artificial intelligence; Bioinformatics; Calculations; DNA; Evolutionary algorithms; Genetic algorithms; Multiobjective optimization; Optimization; Self assembly; Swarm intelligence; Artificial bee colonies; DNA sequence design; Engineering applications; Molecular computing; Multi objective evolutionary algorithms; Multi-objective optimization problem; Multiobjective approach; Non- dominated sorting genetic algorithms; DNA sequences",Article,Scopus,2-s2.0-84888343199
"Ning X., Lam K.C.","Cost-safety trade-off in unequal-area construction site layout planning",2013,"Automation in Construction",13,10.1016/j.autcon.2013.01.011,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878359454&doi=10.1016%2fj.autcon.2013.01.011&partnerID=40&md5=f40ce6e04b93ad26ffe80a306d9dfd41","Cost and safety are two key elements when designing a good construction site layout planning (CSLP). Previous research works always considered CSLP from the aspect of reducing cost and treated SCLPasasingle objective optimization problem. In the paper, CSLP was designed by a multi-objective optimization (MOO) model using modified Pareto-based ant colony optimization (ACO) algorithm, which could find a Pareto solution (trade-off layout) to fulfill the requirement of reducing cost and improve the site safety level simultaneously. Furthermore, in order to apply MOO model to solve unequal-area problem, the random grids-recognition strategy was employed in the proposed MOO model to solve the unequal-area site layout problems without increasing the computational complexities. A case study of a residential building project isused tovalidate the proposed MOO model and the results are very positive. © 2013 Elsevier B.V. All rights reserved.","Construction site layout planning; Cost-safety trade-off; Multi-objective optimization; Pareto-based ACO algorithm","Ant colony optimization; Artificial intelligence; Cost reduction; Costs; Economic and social effects; Optimization; Problem solving; ACO algorithms; Ant Colony Optimization algorithms; Construction site layouts; Objective optimization; Pareto solution; Recognition strategies; Residential building; Trade off; Multiobjective optimization",Article,Scopus,2-s2.0-84878359454
"Sanchez E., Toro C., Artetxe A., Graña M., Sanin C., Szczerbicki E., Carrasco E., Guijarro F.","Bridging challenges of clinical decision support systems with a semantic approach. A case study on breast cancer",2013,"Pattern Recognition Letters",13,10.1016/j.patrec.2013.04.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885677354&doi=10.1016%2fj.patrec.2013.04.003&partnerID=40&md5=380d67e1cfdc3f9eaba45a3055f2aa25","The integration of Clinical Decision Support Systems (CDSS) in nowadays clinical environments has not been fully achieved yet. Although numerous approaches and technologies have been proposed since 1960, there are still open gaps that need to be bridged. In this work we present advances from the established state of the art, overcoming some of the most notorious reported difficulties in: (i) automating CDSS, (ii) clinical workflow integration, (iii) maintainability and extensibility of the system, (iv) timely advice, (v) evaluation of the costs and effects of clinical decision support, and (vi) the need of architectures that allow the sharing and reusing of CDSS modules and services. In order to do so, we introduce a new clinical task model oriented to clinical workflow integration, which follows a federated approach. Our work makes use of the reported benefits of semantics in order to fully take advantage of the knowledge present in every stage of clinical tasks and the experience acquired by physicians. In order to introduce a feasible extension of classical CDSS, we present a generic architecture that permits a semantic enhancement, namely Semantic CDSS (S-CDSS). A case study of the proposed architecture in the domain of breast cancer is also presented, pointing some highlights of our methodology. © 2013 Elsevier B.V. All rights reserved.","Breast cancer; Clinical decision support system; Clinical task model; General architecture; Implementation; Semantic technologies","Architecture; Artificial intelligence; Computer architecture; Diseases; Integration; Semantics; Breast Cancer; Clinical decision support systems; Clinical tasks; General architectures; Implementation; Semantic technologies; Decision support systems",Article,Scopus,2-s2.0-84885677354
"Yilmaz E., Kilikçier Ç.","Determination of fetal state from cardiotocogram using LS-SVM with particle swarm optimization and binary decision tree",2013,"Computational and Mathematical Methods in Medicine",13,10.1155/2013/487179,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888869975&doi=10.1155%2f2013%2f487179&partnerID=40&md5=b8a97840561d5d83e95ed36270569e06","We use least squares support vector machine (LS-SVM) utilizing a binary decision tree for classification of cardiotocogram to determine the fetal state. The parameters of LS-SVM are optimized by particle swarm optimization. The robustness of the method is examined by running 10-fold cross-validation. The performance of the method is evaluated in terms of overall classification accuracy. Additionally, receiver operation characteristic analysis and cobweb representation are presented in order to analyze and visualize the performance of the method. Experimental results demonstrate that the proposed method achieves a remarkable classification accuracy rate of 91.62%. © 2013 Ersen Yilmaz and Çaǧlar Kilikçier.",,"article; cardiotocograph; cardiotocography; classification algorithm; clinical evaluation; decision tree; diagnostic accuracy; fetus; fetus development; human; image analysis; intelligence; learning algorithm; least square support vector machine; machine learning; nonhuman; parameters; particle swarm optimization; process optimization; receiver operating characteristic; support vector machine; artificial intelligence; cardiotocography; decision support system; decision tree; evaluation study; female; pregnancy; regression analysis; statistics and numerical data; support vector machine; validation study; statistics; Artificial Intelligence; Cardiotocography; Decision Support Systems, Clinical; Decision Trees; Female; Humans; Least-Squares Analysis; Pregnancy; ROC Curve; Support Vector Machines; Artificial Intelligence; Cardiotocography; Decision Support Systems, Clinical; Decision Trees; Female; Humans; Least-Squares Analysis; Pregnancy; ROC Curve; Support Vector Machines",Article,Scopus,2-s2.0-84888869975
"Zhang Z.F., Liu M.L.","Rational secret sharing as extensive games",2013,"Science China Information Sciences",13,10.1007/s11432-011-4382-9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874118911&doi=10.1007%2fs11432-011-4382-9&partnerID=40&md5=520c6bf17a7fd8159bc378a4721d7ae5","The threat that comes from previously used punishment strategies in rational secret sharing is weakened because the punishment somtimes also causes loss to the punisher himself. In this paper, we first model 2-out-of-2 rational secret sharing in an extensive game with imperfect information, and then provide a strategy for achieving secret recovery in this game. Moreover, we prove that the strategy is a sequential equilibrium which means after any history of the game no player can benefit from deviations so long as the other players stick to the strategy. In particular, when a deviation is detected, the punishment executed by the punisher is still his optimal option. Therefor, by considering rational secret sharing as an extensive game, we design punishment strategies that effectively punish the deviants and meanwhile guarantee punishers' benefit. Hence, these punishments are more credible than previous ones. Except assuming the existence of simultaneous channels, our scheme can have dealer off-line and extend to the t-out-of-n setting, and also satisfies computational equilibria in some sense. © 2011 Science China Press and Springer-Verlag Berlin Heidelberg.","extensive game; rational secret sharing; sequential equilibrium","extensive game; Game with imperfect information; Rational secret sharing; Sequential equilibrium; Natural sciences; Software engineering; Artificial intelligence",Article,Scopus,2-s2.0-84874118911
"Parnafes O., Disessa A.A.","Microgenetic learning analysis: A methodology for studying knowledge in transition",2013,"Human Development",13,10.1159/000342945,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873652305&doi=10.1159%2f000342945&partnerID=40&md5=806237ed1ef635710770a619eaeb9dd0","This paper introduces and exemplifies a qualitative method for studying learning, microgenetic learning analysis (MLA), which is aimed jointly at developing theory and at establishing useful empirical results. Among modern methodologies, the focus on theory is somewhat distinctive. We use two strategies to describe MLA. First, we develop a framework for comparing the focus and means of different methods, particularly qualitative methods, aimed at studying learning. Using the framework, we compare and contrast MLA with two better-known methods, microgenetic analysis and grounded theory. Second, we aim to schematize elements of MLA-from large-scale patterns of work to detailed analytical strategies-and to exemplify some of them in a case study using data collected some years prior to this elaboration of MLA. Copyright © 2013 S. Karger AG, Basel.","Conceptual change; Knowledge analysis; Learning theory; Microgenetic analysis; Qualitative methods; Theory building","Article; artificial intelligence; conceptual framework; data analysis; grounded theory; information processing; knowledge; learning theory; microgenetic analysis; priority journal; problem solving; skill",Article,Scopus,2-s2.0-84873652305
"De Maturana E.L., Ye Y., Calle M.L., Rothman N., Urrea V., Kogevinas M., Petrus S., Chanock S.J., Tardón A., García-Closas M., González-Neira A., Vellalta G., Carrato A., Navarro A., Lorente-Galdós B., Silverman D.T., Real F.X., Wu X., Malats N.","Application of multi-SNP approaches Bayesian LASSO and AUC-RF to detect main effects of inflammatory-gene variants associated with bladder cancer risk",2013,"PLoS ONE",12,10.1371/journal.pone.0083745,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894280659&doi=10.1371%2fjournal.pone.0083745&partnerID=40&md5=aaeb803174219c1a50fe86748923df8f","The relationship between inflammation and cancer is well established in several tumor types, including bladder cancer. We performed an association study between 886 inflammatory-gene variants and bladder cancer risk in 1,047 cases and 988 controls from the Spanish Bladder Cancer (SBC)/EPICURO Study. A preliminary exploration with the widely used univariate logistic regression approach did not identify any significant SNP after correcting for multiple testing. We further applied two more comprehensive methods to capture the complexity of bladder cancer genetic susceptibility: Bayesian Threshold LASSO (BTL), a regularized regression method, and AUC-Random Forest, a machine-learning algorithm. Both approaches explore the joint effect of markers. BTL analysis identified a signature of 37 SNPs in 34 genes showing an association with bladder cancer. AUC-RF detected an optimal predictive subset of 56 SNPs. 13 SNPs were identified by both methods in the total population. Using resources from the Texas Bladder Cancer study we were able to replicate 30% of the SNPs assessed. The associations between inflammatory SNPs and bladder cancer were reexamined among non-smokers to eliminate the effect of tobacco, one of the strongest and most prevalent environmental risk factor for this tumor. A 9 SNP-signature was detected by BTL. Here we report, for the first time, a set of SNP in inflammatory genes jointly associated with bladder cancer risk. These results highlight the importance of the complex structure of genetic susceptibility associated with cancer risk. © 2013 de Maturana et al.",,"Adult; Aged; Aged, 80 and over; Algorithms; Artificial Intelligence; Bayes Theorem; Case-Control Studies; Female; Follow-Up Studies; Genetic Predisposition to Disease; Humans; Inflammation; Inflammation Mediators; Male; Middle Aged; Polymorphism, Single Nucleotide; Prognosis; Risk Factors; Smoking; Texas; Tumor Markers, Biological; Urinary Bladder Neoplasms; Young Adult",Article,Scopus,2-s2.0-84894280659
"Squartini S., Fuselli D., Boaro M., De Angelis F., Piazza F.","Home energy resource scheduling algorithms and their dependency on the battery model",2013,"IEEE Symposium on Computational Intelligence Applications in Smart Grid, CIASG",12,10.1109/CIASG.2013.6611508,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890035590&doi=10.1109%2fCIASG.2013.6611508&partnerID=40&md5=fa8a3241174336976d07cfb52c489dfb","Smart Home Energy Management is a very hot topic within the scientific community and some interesting solutions are also available on the market. One key issue is represented by the capability of planning the usage of energy resources in order to reduce the overall energy costs. This means that, considering the dynamic electricity price and the availability of adequately sized storage system, the expert system is supposed to automatically decide the more convenient policy to administer the electrical energy flux from and towards the grid. In this work a comparison between different methods for home energy resource scheduling is proposed and analyzed from the perspective of the dependency of their performance on the employed battery model, with special focus on its capacity and charge/discharge rates. A typical grid-connected residential energy system is considered for performed computer simulations, in which a system storage and renewable resources are available and exploitable to match the user load demand. Obtained results allows the authors to provide interesting guidelines for the selection of the battery features. © 2013 IEEE.","Adaptive Critic Design; Battery Capacity; Battery Charge/Discharge Rate; Computational Intelligence; Energy Resource Scheduling; Particle Swarm Optimization","Adaptive critic designs; Battery capacity; Charge/discharge; Renewable resource; Residential energy systems; Resource scheduling algorithms; Resource-scheduling; Scientific community; Automation; Computer simulation; Energy management; Energy resources; Expert systems; Intelligent buildings; Particle swarm optimization (PSO); Scheduling; Smart power grids; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84890035590
"Badrzadeh H., Sarukkalige R., Jayawardena A.W.","Impact of multi-resolution analysis of artificial intelligence models inputs on multi-step ahead river flow forecasting",2013,"Journal of Hydrology",12,10.1016/j.jhydrol.2013.10.017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887146337&doi=10.1016%2fj.jhydrol.2013.10.017&partnerID=40&md5=09c1707188f7b6e5066fc91d04e88b5c","In this paper an attempt is made to show that the performance of daily river flow forecasting is improved when data-preprocessing techniques are used with computational intelligence methods. Especially for forecasting longer lead-times, one of the inherent problems in all forecasting methods is that the reliability of forecasting decreases with increasing the lead-time. Therefore wavelet multi-resolution analysis is coupled with artificial neural networks (ANN) and adaptive neuro-fuzzy interface system (ANFIS) which are two promising methods for forecasting nonlinear and non-stationary time series. Different models with a combination of the different input data sets are developed for 1, 2, 3, 4 and 5. days ahead forecasting in Harvey River, Western Australia. Daubechies and Symlet wavelets are used to decompose river flow time series to different levels. Hybrid wavelet neural networks (WNN) models are trained using Levenberg-Marquart (LM) algorithm and the wavelet neuro-fuzzy (WNF) models with subtractive clustering method to find the optimum number of fuzzy rules. Comparing the results with those of the original ANN and ANFIS models indicates that the hybrid models produce significantly better results, especially for the peak values and longer lead-times. © 2013 Elsevier B.V.","Forecast; Fuzzy clustering; Multi-resolution; Neural network; River flow; Wavelet","Computational intelligence methods; Multi-resolution; Non-stationary time series; River flow; River flow time series; Wavelet; Wavelet multi-resolution analysis; Wavelet neural networks; Discrete wavelet transforms; Fuzzy clustering; Neural networks; Stream flow; Time series; Forecasting; algorithm; artificial intelligence; artificial neural network; data processing; flow modeling; forecasting method; fuzzy mathematics; numerical model; river flow; time series; wavelet analysis; Australia; Harvey River; Western Australia",Article,Scopus,2-s2.0-84887146337
"Yager R.R.","Exponential smoothing with credibility weighted observations",2013,"Information Sciences",12,10.1016/j.ins.2013.07.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884289798&doi=10.1016%2fj.ins.2013.07.008&partnerID=40&md5=5043277ad99166bdb13ef3b56b41876a","Our interest is in time series data smoothing. We view this process as an aggregation of previously observed values. We first discuss the features desired of a good smoothing operator. We particularly note the conflict that exists between our desire for minimal variance and desire to use the freshest data. We describe a number of commonly used smoothing techniques, moving average and exponential smoothing. We then consider the extension of these methods to the case where the observations can have different credibility or importances. Specifically we develop an extension of the exponential smoothing method to the case where the observations can have different importance weights in the smoothing process. © 2012 Published by Elsevier Inc.","Aggregation; Exponential smoothing; Forecasting; Importance; Time series","Exponential smoothing; Exponential smoothing method; Importance; Importance weights; Smoothing operator; Smoothing process; Smoothing techniques; Time-series data; Agglomeration; Artificial intelligence; Forecasting; Software engineering; Time series",Article,Scopus,2-s2.0-84884289798
"Freeman D.M.","Using Naive Bayes to detect spammy names in social networks",2013,"Proceedings of the ACM Conference on Computer and Communications Security",12,10.1145/2517312.2517314,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888989407&doi=10.1145%2f2517312.2517314&partnerID=40&md5=8f1a737da37088085e58e7102f17e541","Many social networks are predicated on the assumption that a member's online information reflects his or her real identity. In such networks, members who fill their name fields with fictitious identities, company names, phone numbers, or just gibberish are violating the terms of service, polluting search results, and degrading the value of the site to real members. Finding and removing these accounts on the basis of their spammy names can both improve the site experience for real members and prevent further abusive activity. In this paper we describe a set of features that can be used by a Naive Bayes classifier to find accounts whose names do not represent real people. The model can detect both automated and human abusers and can be used at registration time, before other signals such as social graph or clickstream history are present. We use member data from LinkedIn to train and validate our model and to choose parameters. Our best-scoring model achieves AUC 0.85 on a sequestered test set. We ran the algorithm on live LinkedIn data for one month in parallel with our previous name scoring algorithm based on regular expressions. The false positive rate of our new algorithm (3.3%) was less than half that of the previous algorithm (7.0%). When the algorithm is run on email usernames as well as user-entered first and last names, it provides an effective way to catch not only bad human actors but also bots that have poor name and email generation algorithms. © 2013 ACM.","naive bayes classifier; social networks; spam detection","False positive rates; Generation algorithm; Naive Bayes classifiers; On-line information; Regular expressions; Scoring algorithms; Spam detection; Terms of services; Algorithms; Artificial intelligence; Classifiers; Electronic mail; Learning systems; Pattern matching; Telephone systems; Social networking (online)",Conference Paper,Scopus,2-s2.0-84888989407
"Oren J., Filmus Y., Boutilier C.","Efficient vote elicitation under candidate uncertainty",2013,"IJCAI International Joint Conference on Artificial Intelligence",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063108&partnerID=40&md5=c484488fc38da2668deacb86f98c200f","Top-k voting is an especially natural form of partial vote elicitation in which only length k prefixes of rankings are elicited. We analyze the ability of top-k vote elicitation to correctly determine true winners, with high probability, given probabilistic models of voter preferences and candidate availability. We provide bounds on the minimal value of k required to determine the correct winner under the plurality and Borda voting rules, considering both worst-case preference profiles and profiles drawn from the impartial culture and Mallows probabilistic models. We also derive conditions under which the special case of zero-elicitation (i.e., k = 0) produces the correct winner. We provide empirical results that confirm the value of top-k voting.",,"High probability; Minimal value; Probabilistic models; Voting rules; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84896063108
"Zhang W., Pan G., Wu Z., Li S.","Online community detection for large complex networks",2013,"IJCAI International Joint Conference on Artificial Intelligence",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896061771&partnerID=40&md5=09ee65bf2a53f8b858d0d249fcfbd184","Complex networks describe a wide range of systems in nature and society. To understand the complex networks, it is crucial to investigate their internal structure. In this paper, we propose an online community detection method for large complex networks, which make it possible to process networks edge-by-edge in a serial fashion. We investigate the generative mechanism of complex networks and propose a split mechanism based on the degree of the nodes to create new community. Our method has linear time complexity. The method has been applied to six real-world network datasets and the experimental results show that it is comparable to existing methods in modularity with much less running time.",,"Generative mechanism; Internal structure; Linear time complexity; On-line communities; Process networks; Real-world networks; Serial fashion; Split mechanisms; Artificial intelligence; Online systems; Population dynamics",Conference Paper,Scopus,2-s2.0-84896061771
"Ruvolo P., Eaton E.","Active task selection for lifelong machine learning",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893368602&partnerID=40&md5=18dbc75601153c4c08a62aa824214b00","In a lifelong learning framework, an agent acquires knowledge incrementally over consecutive learning tasks, continually building upon its experience. Recent lifelong learning algorithms have achieved nearly identical performance to batch multi-task learning methods while reducing learning time by three orders of magnitude. In this paper, we further improve the scalability of lifelong learning by developing curriculum selection methods that enable an agent to actively select the next task to learn in order to maximize performance on future learning tasks. We demonstrate that active task selection is highly reliable and effective, allowing an agent to learn high performance models using up to 50% fewer tasks than when the agent has no control over the task order. We also explore a variant of transfer learning in the lifelong learning setting in which the agent can focus knowledge acquisition toward a particular target task. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Learning tasks; Learning time; Life long learning; Multitask learning; Performance Model; Selection methods; Three orders of magnitude; Transfer learning; Artificial intelligence; Curricula; Learning algorithms; Learning systems",Conference Paper,Scopus,2-s2.0-84893368602
"Bucchiarone A., Marconi A., Mezzina C.A., Pistore M., Raik H.","On-the-fly adaptation of dynamic service-based systems: Incrementality, reduction and reuse",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-45005-1_11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892409899&doi=10.1007%2f978-3-642-45005-1_11&partnerID=40&md5=8507c793bdebfee629b6deba95fae7a3","On-the-fly adaptation is where adaptation activities are not explicitly represented at design time but are discovered and managed at run time considering all aspect of the execution environments. In this paper we present a comprehensive framework for the on-the-fly adaptation of highly dynamic service-based systems. The framework relies on advanced context-aware adaptation techniques that allow for i) incremental handling of complex adaptation problems by interleaving problem solving and solution execution, ii) reduction in the complexity of each adaptation problem by minimizing the search space according to the specific execution context, and iii) reuse of adaptation solutions by learning from past executions. We evaluate the applicability of the proposed approach on a real world scenario based on the operation of the Bremen sea port. © 2013 Springer-Verlag.",,"Context-aware adaptation; Design time; Execution context; Execution environments; On-the-fly; Real-world scenario; Search spaces; Service-based systems; Computer science; Computers; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84892409899
"Alessandro A., Corani G., Mauá D., Gabaglio S.","An ensemble of Bayesian networks for multilabel classification",2013,"IJCAI International Joint Conference on Artificial Intelligence",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896061697&partnerID=40&md5=2aa1cea4037b4e6b5be1c700394c9cbb","We present a novel approach for multilabel classification based on an ensemble of Bayesian networks. The class variables are connected by a tree; each model of the ensemble uses a different class as root of the tree. We assume the features to be conditionally independent given the classes, thus generalizing the naive Bayes assumption to the multiclass case. This assumption allows us to optimally identify the correlations between classes and features; such correlations are moreover shared across all models of the ensemble. Inferences are drawn from the ensemble via logarithmic opinion pooling. To minimize Hamming loss, we compute the marginal probability of the classes by running standard inference on each Bayesian network in the ensemble, and then pooling the inferences. To instead minimize the subset 0/1 loss, we pool the joint distributions of each model and cast the problem as a MAP inference in the corresponding graphical model. Experiments show that the approach is competitive with state-of-the-art methods for multilabel classification.",,"Different class; GraphicaL model; Joint distributions; MAP inferences; Marginal probability; Multi-label classifications; Naive bayes; State-of-the-art methods; Artificial intelligence; Forestry; Bayesian networks; Algorithms; Artificial Intelligence; Forestry",Conference Paper,Scopus,2-s2.0-84896061697
"Ciancarini P., Di Iorio A., Nuzzolese A.G., Peroni S., Vitali F.","Semantic annotation of scholarly documents and citations",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-319-03524-6_29,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892700221&doi=10.1007%2f978-3-319-03524-6_29&partnerID=40&md5=ddae0d50fb8e059c624b0cf796813511","Scholarly publishing is in the middle of a revolution based on the use of Web-related technologies as medium of communication. In this paper we describe our ongoing study of semantic publishing and automatic annotation of scholarly documents, presenting several models and tools for the automatic annotation of structural and semantic components of documents. In particular, we focus on citations and their automatic classification obtained by CiTalO, a framework that combines ontology learning techniques with NLP techniques. © Springer International Publishing Switzerland 2013.","Citation networks; Citation patterns; CiTO; PDF jailbreaking; Semantic annotations; Semantic publishing; Semantic Web","Citation networks; Citation patterns; CiTO; PDF jailbreaking; Semantic annotations; Semantic publishing; Artificial intelligence; Semantic Web; Publishing",Conference Paper,Scopus,2-s2.0-84892700221
"Giakkoupis G., Helmi M., Higham L., Woelfel P.","An O(√n) space bound for obstruction-free leader election",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-41527-2_4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893789350&doi=10.1007%2f978-3-642-41527-2_4&partnerID=40&md5=71106df9f893e96b0e9623e99eff218c","We present a deterministic obstruction-free implementation of leader election from O(√n) atomic O(log n)-bit registers in the standard asynchronous shared memory system with n processes. We provide also a technique to transform any deterministic obstruction-free algorithm, in which any process can finish if it runs for b steps without interference, into a randomized wait-free algorithm for the oblivious adversary, in which the expected step complexity is polynomial in n and b. This transformation allows us to combine our obstruction-free algorithm with the leader election algorithm by Giakkoupis and Woelfel [21], to obtain a fast randomized leader election (and thus test-and-set) implementation from O(√n) O(log n)-bit registers, that has expected step complexity O(log* n) against the oblivious adversary. Our algorithm provides the first sub-linear space upper bound for obstruction-free leader election. A lower bound of Ω(log n) has been known since 1989 [29]. Our research is also motivated by the longstanding open problem whether there is an obstruction-free consensus algorithm which uses fewer than n registers. © Springer-Verlag Berlin Heidelberg 2013.","Leader election; Obstruction-free algorithms; Randomized algorithms; Shared memory model; Test-and-set","Leader election; Obstruction-free; Randomized Algorithms; Shared memory model; Test and sets; Artificial intelligence; Computer science; Computers; Algorithms",Conference Paper,Scopus,2-s2.0-84893789350
"Borgwardt S., Peñaloza R.","Positive subsumption in fuzzy EL with general t-norms",2013,"IJCAI International Joint Conference on Artificial Intelligence",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063934&partnerID=40&md5=ccce6240d6eb9432336c002780faf136","The Description Logic EL is used to formulate several large biomedical ontologies. Fuzzy extensions of EL can express the vagueness inherent in many biomedical concepts. We study the reasoning problem of deciding positive subsumption in fuzzy EL with semantics based on general t-norms. We show that the complexity of this problem depends on the specific t-norm chosen. More precisely, if the t-norm has zero divisors, then the problem is co-NP-hard; otherwise, it can be decided in polynomial time. We also show that the best subsumption degree cannot be computed in polynomial time if the t-norm contains the Łukasiewicz t-norm.",,"Biomedical ontologies; Description logic; Fuzzy extension; Polynomial-time; Reasoning problems; T - Norm; Zero divisors; Artificial intelligence; Data description; Polynomial approximation; Semantics; Mathematical operators",Conference Paper,Scopus,2-s2.0-84896063934
"Kazakov Y., Klinov P.","Incremental reasoning in OWL EL without bookkeeping",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-41335-3_15,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891954904&doi=10.1007%2f978-3-642-41335-3_15&partnerID=40&md5=c41860c7639b7d4328f85ccf7a4dacd2","We describe a method for updating the classification of ontologies expressed in the EL family of Description Logics after some axioms have been added or deleted. While incremental classification modulo additions is relatively straightforward, handling deletions is more problematic since it requires retracting logical consequences that are no longer valid. Known algorithms address this problem using various forms of bookkeeping to trace the consequences back to premises. But such additional data can consume memory and place an extra burden on the reasoner during application of inferences. In this paper, we present a technique, which avoids this extra cost while being very efficient for small incremental changes in ontologies. The technique is freely available as a part of the open-source EL reasoner ELK and its efficiency is demonstrated on naturally occurring and synthetic data. © 2013 Springer-Verlag.",,"Additional datum; Description logic; Incremental changes; Incremental reasoning; Its efficiencies; Logical consequences; Naturally occurring; Synthetic data; Artificial intelligence; Computer science; Computers; Data description",Conference Paper,Scopus,2-s2.0-84891954904
"Wang Y., Wang K., Zhang M.","Forgetting for answer set programs revisited",2013,"IJCAI International Joint Conference on Artificial Intelligence",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063413&partnerID=40&md5=f26888f24bfb5d9385ca1a7ec57a2d04","A new semantic forgetting for answer set programs (ASP), called SM-forgetting, is proposed in the paper. It distinguishes itself from the others in that it preserves not only skeptical and credulous consequences on unforgotten variables, but also strong equivalence - forgetting same variables in strongly equivalent logic programs has strongly equivalent results. The forgetting presents a positive answer to Gabbay, Pearce and Valverde's open question - if ASP has uniform interpolation property. We also investigate some properties, algorithm and computational complexities for the forgetting. It shows that computing the forgetting result is generally intractable even for Horn logic programs.",,"Answer set; Horn logic; Logic programs; Uniform interpolations; Logic programming; Semantics; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84896063413
"Belle V., Levesque H.J.","Reasoning about continuous uncertainty in the situation calculus",2013,"IJCAI International Joint Conference on Artificial Intelligence",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062261&partnerID=40&md5=fd738c16d8902a7fe5a3b37ce5dd850d","Among the many approaches for reasoning about degrees of belief in the presence of noisy sensing and acting, the logical account proposed by Bacchus, Halpern, and Levesque is perhaps the most expressive. While their formalism is quite general, it is restricted to fluents whose values are drawn from discrete countable domains, as opposed to the continuous domains seen in many robotic applications. In this paper, we show how this limitation in their approach can be lifted. By dealing seamlessly with both discrete distributions and continuous densities within a rich theory of action, we provide a very general logical specification of how belief should change after acting and sensing in complex noisy domains. The authors would like to thank Tim Capes and Steven Shapiro for valuable discussions, Fahiem Bacchus for feedback on a previous version of the paper, and the Natural Sciences and Engineering Research Council of Canada for financial support.",,"Continuous domain; Continuous uncertainties; Discrete distribution; Financial support; Logical specifications; Natural sciences and engineerings; Robotic applications; Situation calculus; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84896062261
"Niinimäki T., Koivisto M.","Annealed importance sampling for structure learning in Bayesian networks",2013,"IJCAI International Joint Conference on Artificial Intelligence",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896064141&partnerID=40&md5=ea0e9d45cd335441f59dd72eb7de6ea8","We present a new sampling approach to Bayesian learning of the Bayesian network structure. Like some earlier sampling methods, we sample linear orders on nodes rather than directed acyclic graphs (DAGs). The key difference is that we replace the usual Markov chain Monte Carlo (MCMC) methodby the method of annealed importance sampling (AIS). We show that AIS is not only competitiveto MCMC in exploring the posterior, but also superior to MCMC in two ways: it enables easy and efficient parallelization, due to the independence of the samples, and lower-bounding of the marginal likelihood of the model with good probabilistic guarantees. We also provide a principled way to correct the bias due to order-based sampling, by implementing a fast algorithm for counting the linear extensions of a given partial order.",,"Bayesian learning; Bayesian network structure; Directed acyclic graphs; Linear extensions; Marginal likelihood; Markov chain Monte Carlo; Probabilistic guarantees; Structure-learning; Artificial intelligence; Importance sampling; Bayesian networks",Conference Paper,Scopus,2-s2.0-84896064141
"Frénay B., Doquire G., Verleysen M.","Is mutual information adequate for feature selection in regression?",2013,"Neural Networks",12,10.1016/j.neunet.2013.07.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880930604&doi=10.1016%2fj.neunet.2013.07.003&partnerID=40&md5=9b196a5d31903c2e1aef27253925b8ca","Feature selection is an important preprocessing step for many high-dimensional regression problems. One of the most common strategies is to select a relevant feature subset based on the mutual information criterion. However, no connection has been established yet between the use of mutual information and a regression error criterion in the machine learning literature. This is obviously an important lack, since minimising such a criterion is eventually the objective one is interested in. This paper demonstrates that under some reasonable assumptions, features selected with the mutual information criterion are the ones minimising the mean squared error and the mean absolute error. On the contrary, it is also shown that the mutual information criterion can fail in selecting optimal features in some situations that we characterise. The theoretical developments presented in this work are expected to lead in practice to a critical and efficient use of the mutual information for feature selection. © 2013 Elsevier Ltd.","Feature selection; MAE; MSE; Mutual information; Regression","High-dimensional regression problems; Machine learning literature; MAE; Mean absolute error; MSE; Mutual informations; Regression; Theoretical development; Feature extraction; Learning algorithms; Regression analysis; analytical error; analytical parameters; article; artificial neural network; entropy; feature selection; mathematical analysis; mean absolute error; mean squared error; mutual information; priority journal; regression analysis; statistical parameters; theoretical study; Feature selection; MAE; MSE; Mutual information; Regression; Algorithms; Artificial Intelligence; Entropy; Image Processing, Computer-Assisted; Informatics; Information Storage and Retrieval; Neural Networks (Computer); Normal Distribution; Regression Analysis; Signal-To-Noise Ratio",Article,Scopus,2-s2.0-84880930604
"Yan J., Wang Y., Zhou K., Huang J., Tian C., Zha H., Dong W.","Towards effective prioritizing water pipe replacement and rehabilitation",2013,"IJCAI International Joint Conference on Artificial Intelligence",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063468&partnerID=40&md5=bf7e768a147a48662a9586d110f0e5df","Water pipe failures can not only have a great impact on people's daily life but also cause significant waste of water which is an essential and precious resource to human beings. As a result, preventative maintenance for water pipes, particularly in urbanscale networks, is of great importance for a sustainable society. To achieve effective replacement and rehabilitation, failure prediction aims to proactively find those 'most-likely-to-fail' pipes becomes vital and has been attracting more attention from both academia and industry, especially from the civil engineering field. This paper presents an alreadydeployed industrial computational system for pipe failure prediction. As an alternative to risk matrix methods often depending on ad-hoc domain heuristics, learning based methods are adopted using the attributes with respect to physical, environmental, operational conditions and etc. Further challenge arises in practice when lacking of profile attributes. A dive into the failure records shows that the failure event sequences typically exhibit temporal clustering patterns, which motivates us to use the stochastic process to tackle the failure prediction task. Specifically, the failure sequence is formulated as a self-exciting stochastic process which is, to our best knowledge, a novel formulation for pipe failure prediction. And we show that it outperforms a baseline assuming the failure risk grows linearly with aging. Broad new problems and research points for the machine learning community are also introduced for future work.",,"Computational system; Failure prediction; Learning-based methods; Machine learning communities; Operational conditions; Preventative maintenance; Sustainable society; Temporal clustering; Artificial intelligence; Civil engineering; Joint prostheses; Random processes; Water piping systems; Water pipelines",Conference Paper,Scopus,2-s2.0-84896063468
"Keraudren K., Kyriakopoulou V., Rutherford M., Hajnal J.V., Rueckert D.","Localisation of the brain in fetal MRI using bundled SIFT features.",2013,"Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894617275&partnerID=40&md5=c6045840fd434adbfb8e470ea8f8a019","Fetal MRI is a rapidly emerging diagnostic imaging tool. Its main focus is currently on brain imaging, but there is a huge potential for whole body studies. We propose a method for accurate and robust localisation of the fetal brain in MRI when the image data is acquired as a stack of 2D slices misaligned due to fetal motion. We first detect possible brain locations in 2D images with a Bag-of-Words model using SIFT features aggregated within Maximally Stable Extremal Regions (called bundled SIFT), followed by a robust fitting of an axis-aligned 3D box to the selected regions. We rely on prior knowledge of the fetal brain development to define size and shape constraints. In a cross-validation experiment, we obtained a median error distance of 5.7mm from the ground truth and no missed detection on a database of 59 fetuses. This 2D approach thus allows a robust detection even in the presence of substantial fetal motion.",,"algorithm; article; artificial intelligence; automated pattern recognition; brain; computer assisted diagnosis; histology; human; image enhancement; methodology; nuclear magnetic resonance imaging; prenatal development; prenatal diagnosis; reproducibility; sensitivity and specificity; Algorithms; Artificial Intelligence; Brain; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Pattern Recognition, Automated; Prenatal Diagnosis; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84894617275
"Tallevi-Diotallevi S., Kotoulas S., Foschini L., Lécué F., Corradi A.","Real-time urban monitoring in Dublin using semantic and stream technologies",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-41338-4_12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891920781&doi=10.1007%2f978-3-642-41338-4_12&partnerID=40&md5=30e7517e0fbfbcb8144dadc4f3d09b7c","Several sources of information, from people, systems, things, are already available in most modern cities. Processing these continuous flows of information and capturing insight poses unique technical challenges that span from response time constraints to data heterogeneity, in terms of format and throughput. To tackle these problems, we focus on a novel prototype to ease real-time monitoring and decision-making processes for the City of Dublin with three main original technical aspects: (i) an extension to SPARQL to support efficient querying of heterogeneous streams; (ii) a query execution framework and runtime environment based on IBM InfoSphere Streams, a high-performance, industrial strength, stream processing engine; (iii) a hybrid RDFS reasoner, optimized for our stream processing execution framework. Our approach has been validated with real data collected on the field, as shown in our Dublin City video demonstration. Results indicate that real-time processing of city information streams based on semantic technologies is indeed not only possible, but also efficient, scalable and low-latency. © 2013 Springer-Verlag.",,"Decision making process; Ibm infosphere streams; Response-time constraint; Runtime environments; Semantic technologies; Sources of informations; Stream processing engines; Technical challenges; Computer science; Computers; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84891920781
"Han T.A., Pereira L.M., Santos F.C., Lenaerts T.","Why is it so hard to say sorry? Evolution of apology with commitments in the iterated Prisoner's Dilemma",2013,"IJCAI International Joint Conference on Artificial Intelligence",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896058987&partnerID=40&md5=39f6089b02dd1a1b249458866a009486","When making a mistake, individuals can apologize to secure further cooperation, even if the apology is costly. Similarly, individuals arrange commitments to guarantee that an action such as a cooperative one is in the others' best interest, and thus will be carried out to avoid eventual penalties for commitment failure. Hence, both apology and commitment should go side by side in behavioral evolution. Here we provide a computational model showing that apologizing acts are rare in non-committed interactions, especially whenever cooperation is very costly, and that arranging prior commitments can considerably increase the frequency of such behavior. In addition, we show that in both cases, with or without commitments, apology works only if it is sincere, i.e. costly enough. Most interestingly, our model predicts that individuals tend to use much costlier apology in committed relationships than otherwise, because it helps better identify free-riders such as fake committers: 'commitments bring about sincerity'. Furthermore, we show that this strategy of apology supported by commitments outperforms the famous existent strategies of the iterated Prisoner's Dilemma.",,"Computational model; Free-riders; Iterated prisoner's dilemma; Side by sides; Artificial intelligence; Game theory",Conference Paper,Scopus,2-s2.0-84896058987
"Alhejali A.M., Lucas S.M.","Using genetic programming to evolve heuristics for a Monte Carlo Tree Search Ms Pac-Man agent",2013,"IEEE Conference on Computatonal Intelligence and Games, CIG",12,10.1109/CIG.2013.6633639,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892418158&doi=10.1109%2fCIG.2013.6633639&partnerID=40&md5=1b8e863bc9afc0abb948acede0dfd2e8","Ms Pac-Man is one of the most challenging test beds in game artificial intelligence (AI). Genetic programming and Monte Carlo Tree Search (MCTS) have already been successful applied to several games including Pac-Man. In this paper, we use Monte Carlo Tree Search to create a Ms Pac-Man playing agent before using genetic programming to enhance its performance by evolving a new default policy to replace the random agent used in the simulations. The new agent with the evolved default policy was able to achieve an 18% increase on its average score over the agent with random default policy. © 2013 IEEE.","genetic programming; Monte Carlo Tree Search; Pac-Man","Game artificial intelligence; Monte Carlo tree search (MCTS); Monte-Carlo tree searches; Pac-man; Equipment testing; Forestry; Genetic programming; Artificial intelligence; Forestry; Random Processes",Conference Paper,Scopus,2-s2.0-84892418158
"Ghiass R.S., Arandjelović O., Bendada H., Maldague X.","Vesselness features and the inverse compositional AAM for robust face recognition sing thermal IR",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884882946&partnerID=40&md5=6a53b33f1234e66138a4ada7b0bd6e12","Over the course of the last decade, infrared (IR) and particularly thermal IR imaging based face recognition has emerged as a promising complement to conventional, visible spectrum based approaches which continue to struggle when applied in the real world. While inherently insensitive to visible spectrum illumination changes, IR images introduce specific challenges of their own, most notably sensitivity to factors which affect facial heat emission patterns, e.g. emotional state, ambient temperature, and alcohol intake. In addition, facial expression and pose changes are more difficult to correct in IR images because the y are less rich in high frequency detail which is an important cue for fitting any deformable model. In this paper we describe a novel method which addresses the se major challenges. Specifically, to normalize for pose and facial expression changes we generate a synthetic frontal image of a face in a canonical, neutral facial expression from an image of the face in an arbitrary pose and facial expression. This is achieved by piecewise affine warping which follows active appearance model (AAM) fitting. This is the first publication which explores the use of an AAM on thermal IR images; we propose a pre-processing step which enhances detail in thermal images, making AAM convergence faster and more accurate. To overcome the problem of thermal IR image sensitivity to the exact pattern of facial temperature emissions we describe a representation based on reliable anatomical features. In contrast to previous approaches, our representation is not binary; rather, our method accounts for the reliability of the extracted features. This makes the proposed representation much more robust both to pose and scale changes. The effectiveness of the proposed approach is demonstrated on the largest public database of thermal IR images of faces on which it achieved 100% identification rate, significantly outperforming previously described methods. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Active appearance models; Anatomical features; Deformable modeling; Facial Expressions; Identification rates; Piecewise affines; Pre-processing step; Public database; Artificial intelligence; Infrared imaging; Face recognition",Conference Paper,Scopus,2-s2.0-84884882946
"Huang J., Nie F., Huang H.","Spectral rotation versus K-Means in spectral clustering",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893395024&partnerID=40&md5=d621120ed5068bccd5284c67bd30b906","Spectral clustering has been a popular data clustering algorithm. This category of approaches often resort to other clustering methods, such as K-Means, to get the final cluster. The potential flaw of such common practice is that the obtained relaxed continuous spectral solution could severely deviate from the true discrete solution. In this paper, we propose to impose an additional orthonormal constraint to better approximate the optimal continuous solution to the graph cut objective functions. Such a method, called spectral rotation in literature, optimizes the spectral clustering objective functions better than K-Means, and improves the clustering accuracy. We would provide efficient algorithm to solve the new problem rigorously, which is not significantly more costly than K-Means. We also establish the connection between our method and K-Means to provide theoretical motivation of our method. Experimental results show that our algorithm consistently reaches better cut and meanwhile outperforms in clustering metrics than classic spectral clustering methods. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Clustering accuracy; Clustering methods; Data clustering algorithm; Objective functions; Orthonormal; Spectral clustering; Spectral clustering methods; Spectral solutions; Artificial intelligence; Cluster analysis; Clustering algorithms",Conference Paper,Scopus,2-s2.0-84893395024
"Song Y., Wen Z., Lin C.-Y., Davis R.","One-Class Conditional Random Fields for sequential anomaly detection",2013,"IJCAI International Joint Conference on Artificial Intelligence",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896061362&partnerID=40&md5=ee78bdbf7e566c22afae372bf1ed2584","Sequential anomaly detection is a challenging problem due to the one-class nature of the data (i.e., data is collected from only one class) and the temporal dependence in sequential data. We present One-Class Conditional Random Fields (OCCRF) for sequential anomaly detection that learn from a one-class dataset and capture the temporal dependence structure, in an unsupervised fashion. We propose a hinge loss in a regularized risk minimization framework that maximizes the margin between each sequence being classified as ""normal"" and ""abnormal."" This allows our model to accept most (but not all) of the training data as normal, yet keeps the solution space tight. Experimental results on a number of real-world datasets show our model outperforming several baselines. We also report an exploratory study on detecting abnormal organizational behavior in enterprise social networks.",,"Anomaly detection; Conditional random field; Exploratory studies; Organizational behavior; Real-world datasets; Risk minimization; Sequential data; Temporal dependence; Artificial intelligence; Random processes",Conference Paper,Scopus,2-s2.0-84896061362
"Zhu G., Wang Q., Yuan Y., Yan P.","Learning saliency by MRF and differential threshold",2013,"IEEE Transactions on Cybernetics",12,10.1109/TSMCB.2013.2238927,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890065984&doi=10.1109%2fTSMCB.2013.2238927&partnerID=40&md5=f3a1da52dec9c346dc832d444ca6705d","Saliency detection has been an attractive topic in recent years. The reliable detection of saliency can help a lot of useful processing without prior knowledge about the scene, such as content-aware image compression, segmentation, etc. Although many efforts have been spent in this subject, the feature expression and model construction are far from perfect. The obtained saliency maps are therefore not satisfying enough. In order to overcome these challenges, this paper presents a new psychologic visual feature based on differential threshold and applies it in a supervised Markov-random-field framework. Experiments on two public data sets and an image retargeting application demonstrate the effectiveness, robustness, and practicability of the proposed method. © 2013 IEEE.","Computer vision; Differential threshold; Machine learning; Markov random field (MRF); Saliency detection; Visual attention","Differential threshold; Feature expression; Image retargeting; Markov Random Fields; Model construction; Reliable detection; Saliency detection; Visual Attention; Computer vision; Learning systems; Markov processes; Image segmentation; algorithm; article; artificial intelligence; automated pattern recognition; biomimetics; computer assisted diagnosis; decision support system; human; methodology; Algorithms; Artificial Intelligence; Biomimetics; Decision Support Techniques; Humans; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84890065984
"Hazon N., Lin R., Kraus S.","How to change a group's collective decision?",2013,"IJCAI International Joint Conference on Artificial Intelligence",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063016&partnerID=40&md5=c6fb38fc60f2cd7e243bc195595d94e5","Persuasion is a common social and economic activity. It usually arises when conflicting interests among agents exist, and one of the agents wishes to sway the opinions of others. This paper considers the problem of an automated agent that needs to influence the decision of a group of self-interested agents that must reach an agreement on a joint action. For example, consider an automated agent that aims to reduce the energy consumption of a nonresidential building, by convincing a group of people who share an office to agree on an economy mode of the air-conditioning and low light intensity. In this paper we present four problems that address issues of minimality and safety of the persuasion process. We discuss the relationships to similar problems from social choice, and show that if the agents are using Plurality or Veto as their voting rule all of our problems are in P. We also show that with K-Approval, Bucklin and Borda voting rules some problems become intractable. We thus present heuristics for efficient persuasion with Borda, and evaluate them through simulations.",,"Automated agents; Collective decision; Economic activities; Economy modes; Joint actions; Nonresidential buildings; Self-interested agents; Social choice; Artificial intelligence; Decision theory; Energy utilization; Economics",Conference Paper,Scopus,2-s2.0-84896063016
"Greco S., Molinaro C., Trubitsyna I.","Bounded programs: A new decidable class of logic programs with function symbols",2013,"IJCAI International Joint Conference on Artificial Intelligence",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062817&partnerID=40&md5=e4d387976b8b515c3fae3d9f0d5de246","While function symbols are widely acknowledged as an important feature in logic programming, they make common inference tasks undecidable. To cope with this problem, recent research has focused on identifying classes of logic programs imposing restrictions on the use of function symbols, but guaranteeing decidability of common inference tasks. This has led to several criteria, called termination criteria, providing sufficient conditions for a program to have finitely many stable models, each of finite size. This paper introduces the new class of bounded programs which guarantees the aforementioned property and strictly includes the classes of programs determined by current termination criteria. Different results on the correctness, the expressiveness, and the complexity of the class of bounded programs are presented.",,"Finite size; Function symbols; Important features; Logic programs; Recent researches; Stable model; Termination criteria; Artificial intelligence; Logic programming; Computability and decidability",Conference Paper,Scopus,2-s2.0-84896062817
"Sokeh H.S., Gould S., Renz J.","Efficient extraction and representation of spatial information from video data",2013,"IJCAI International Joint Conference on Artificial Intelligence",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062599&partnerID=40&md5=906190af5a652c143fc2fb019dae383d","Vast amounts of video data are available on the web and are being generated daily using surveillance cameras or other sources. Being able to efficiently analyse and process this data is essential for a number of different applications. We want to be able to efficiently detect activities in these videos or be able to extract and store essential information contained in these videos for future use and easy search and access. Cohn et al. (2012) proposed a comprehensive representation of spatial features that can be efficiently extracted from video and used for these purposes. In this paper, we present a modified version of this approach that is equally efficient and allows us to extract spatial information with much higher accuracy than previously possible. We present efficient algorithms both for extracting and storing spatial information from video, as well as for processing this information in order to obtain useful spatial features. We evaluate our approach and demonstrate that the extracted spatial information is considerably more accurate than that obtained from existing approaches.",,"Spatial features; Spatial informations; Surveillance cameras; Video data; Algorithms; Artificial intelligence; Video recording; Video signal processing; Security systems",Conference Paper,Scopus,2-s2.0-84896062599
"Gupta S., Dhillon S.S., Khera P., Marwaha A.","Dual band u-slotted microstrip patch antenna for c band and x band radar applications",2013,"Proceedings - 5th International Conference on Computational Intelligence and Communication Networks, CICN 2013",12,10.1109/CICN.2013.18,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892649197&doi=10.1109%2fCICN.2013.18&partnerID=40&md5=a026054ab3b8295327903a99117c7d83","A compact dual band micro strip patch antenna is designed for C (4-8 GHz) band and X (8-12 GHz) band applications. The proposed antenna consists of a rectangular patch having four U-slots and one I-slot with H-shaped DGS (Defected Ground Structure). The antenna has overall size of 25mm by 23 mm and gives bandwidth of about 140 MHz from 5.85 GHz to 6 GHz and of about 1.21 GHz from 7.87 to 9 GHz at resonating frequency of 5.9 GHz and 8.8 GHz respectively with DGS. The antenna without DGS mainly resonates at 6 GHz and 8.7 GHz. The antenna with DGS has return losses-16.29dB at 5.9 GHz and -18.28 dB at 8.8 GHz, gain 1.2 dBi for 5.9 GHz and 4.4 dBi for 8.8 GHz. This antenna has been analyzed using IE3D electromagnetic solver. © 2013 IEEE.","DGS; Dual Band; Hshape; Microstrip Patch Antenna","DGS; Dual Band; Electromagnetic solvers; Hshape; Micro-strip patch antennas; Rectangular patch; Resonating frequency; X-band Radars; Artificial intelligence; Microstrip antennas; Slot antennas",Conference Paper,Scopus,2-s2.0-84892649197
"Lu X., Song L., Shen S., He K., Yu S., Ling N.","Parallel Hough Transform-based straight line detection and its FPGA implementation in embedded vision.",2013,"Sensors (Basel, Switzerland)",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891427698&partnerID=40&md5=d5c219c044db51cac3a6dff42437c4e1","Hough Transform has been widely used for straight line detection in low-definition and still images, but it suffers from execution time and resource requirements. Field Programmable Gate Arrays (FPGA) provide a competitive alternative for hardware acceleration to reap tremendous computing performance. In this paper, we propose a novel parallel Hough Transform (PHT) and FPGA architecture-associated framework for real-time straight line detection in high-definition videos. A resource-optimized Canny edge detection method with enhanced non-maximum suppression conditions is presented to suppress most possible false edges and obtain more accurate candidate edge pixels for subsequent accelerated computation. Then, a novel PHT algorithm exploiting spatial angle-level parallelism is proposed to upgrade computational accuracy by improving the minimum computational step. Moreover, the FPGA based multi-level pipelined PHT architecture optimized by spatial parallelism ensures real-time computation for 1,024 × 768 resolution videos without any off-chip memory consumption. This framework is evaluated on ALTERA DE2-115 FPGA evaluation platform at a maximum frequency of 200 MHz, and it can calculate straight line parameters in 15.59 ms on the average for one frame. Qualitative and quantitative evaluation results have validated the system performance regarding data throughput, memory bandwidth, resource, speed and robustness.",,"algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; image enhancement; methodology; automated pattern recognition; computer assisted diagnosis; image enhancement; procedures; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84891427698
"Wu F., Tan X., Yang Y., Tao D., Tang S., Zhuang Y.","Supervised nonnegative tensor factorization with Maximum-Margin Constraint",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893378823&partnerID=40&md5=448b78b887db0db1250b857602c73b8c","Non-negative tensor factorization (NTF) has attracted great attention in the machine learning community. In this paper, we extend traditional non-negative tensor factorization into a supervised discriminative decomposition, referred as Supervised Non-negative Tensor Factorization with Maximum-Margin Constraint (SNTFM2). SNTFM2 formulates the optimal discriminative factorization of non-negative tensorial data as a coupled least-squares optimization problem via a maximum-margin method. As a result, SNTFM2 not only faithfully approximates the tensorial data by additive combinations of the basis, but also obtains a strong generalization power to discriminative analysis (in particular for classification in this paper). The experimental results show the superiority of our proposed model over state-of-the-art techniques on both toy and real world data sets. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Least-squares optimization; Machine learning communities; Non negatives; Nonnegative tensor factorizations; State-of-the-art techniques; Tensor factorization; Artificial intelligence; Least squares approximations; Optimization; Tensors; Factorization",Conference Paper,Scopus,2-s2.0-84893378823
"Rospocher M., Serafini L.","An ontological framework for decision support",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-37996-3_16,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892982627&doi=10.1007%2f978-3-642-37996-3_16&partnerID=40&md5=f26aa417c1212b686dd970a1a6fbc432","In the last few years, ontologies have been successfully exploited by Decision Support Systems (DSSs) to support some phases of the decision-making process. In this paper, we propose to employ an ontological representation for all the content both processed and produced by a DSS in answering requests. This semantic representation supports the DSS in the whole decision-making process, and it is capable of encoding (i) the request, (ii) the data relevant for it, and (iii) the conclusions/suggestions/decisions produced by the DSS. The advantages of using an ontology-based representation of the main data structure of a DSS are many: (i) it enables the integration of heterogeneous sources of data available in the web, and to be processed by the DSS, (ii) it allows to track, and to expose in a structured form to additional services (e.g., explanation or case reuse services), all the content processed and produced by the DSS for each request, and (iii) it enables to exploit logical reasoning for some of the inference steps of the DSS decision-making process. The proposed approach have been successfully implemented and exploited in a DSS for personalized environmental information, developed in the context of the PESCaDO EU project. © Springer-Verlag 2013.",,"Decision making process; Decision support system (DSSs); Environmental information; Heterogeneous sources; Ontological frameworks; Ontological representation; Ontology-based representation; Semantic representation; Artificial intelligence; Decision making; Decision support systems; Semantic Web; Semantics; Ontology",Conference Paper,Scopus,2-s2.0-84892982627
"Long W., Yuqing L., Qingxin X.","Using cloudsim to model and simulate cloud computing environment",2013,"Proceedings - 9th International Conference on Computational Intelligence and Security, CIS 2013",12,10.1109/CIS.2013.75,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900640372&doi=10.1109%2fCIS.2013.75&partnerID=40&md5=85b4774eec9e384c844f3196003e6c8d","Cloud computing will be a major technology in the development of the future Internet of Services. Service providers want to remove the bottle neck of the cloud computing system in order to satisfy user requirement. And in order to save energy consumption they also need to apply new energy-efficiency mechanism and observe its effects on a data center. As it is difficult to test new mechanism in real cloud computing environment and researchers often cannot reach the real cloud environment, simulation to model the mechanism and evaluate the results is necessary. Simulating a data center avoids spending time and effort to configure a real testing environment. Moreover, as real machines are not used for testing purposes, their computational power can be allocated to profit-making applications. This paper introduces a simulation framework called CloudSim which provides simulation, power to manage services and modeling of cloud infrastructure. And we have also discussed about how to extend it to simulation your own mechanism in cloud computing. © 2013 IEEE.","Cloud computing; Cloudsim; Energy effective; Green computing; Simulation","Cloud computing environments; Cloud infrastructures; Cloudsim; Energy effective; Green computing; Simulation; Simulation framework; Testing environment; Artificial intelligence; Bottles; Cloud computing; Computer simulation; Energy utilization; Computer systems",Conference Paper,Scopus,2-s2.0-84900640372
"Giagkiozis I., Purshouse R.C., Fleming P.J.","Towards understanding the cost of adaptation in decomposition-based optimization algorithms",2013,"Proceedings - 2013 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2013",12,10.1109/SMC.2013.110,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893548977&doi=10.1109%2fSMC.2013.110&partnerID=40&md5=2c4898d838cbdfb7c76b77bf8f533525","Decomposition-based methods are an increasingly popular choice for a posteriori multi-objective optimization. However the ability of such methods to describe a trade-off surface depends on the choice of weighting vectors defining the set of subproblems to be solved. Recent adaptive approaches have sought to progressively modify the weighting vectors to obtain a desirable distribution of solutions. This paper argues that adaptation imposes a non-negligible cost - in terms of convergence - on decomposition-based algorithms. To test this hypothesis, the process of adaptation is abstracted and then subjected to experimentation on established problems involving between three and 11 conflicting objectives. The results show that adaptive approaches require longer traversals through objectivespace than fixed-weight approaches. Since fixed weights cannot, in general, be specified in advance, it is concluded that the new wave of decomposition-based methods offer no immediate panacea to the well-known conflict between convergence and distribution afflicting Pareto-based a posteriori methods. © 2013 IEEE.","Adaptation; Decision support systems; Decomposition; Multi-objective optimization","Adaptation; Adaptive approach; Conflicting objectives; Decomposition-based optimization; Posteriori; Sub-problems; Weighting vector; Artificial intelligence; Decision support systems; Decomposition; Multiobjective optimization; Algorithms",Conference Paper,Scopus,2-s2.0-84893548977
"Felfernig A., Schubert M., Reiterer S.","Personalized diagnosis for over-constrained problems",2013,"IJCAI International Joint Conference on Artificial Intelligence",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063670&partnerID=40&md5=1eb0dc59edaafec587e0d76169d7ae57","Constraint-based applications such as configurators, recommenders, and scheduling systems support users in complex decision making scenarios. Typically, these systems try to identify a solution that satisfies all articulated user requirements. If the requirements are inconsistent with the underlying constraint set, users have to be actively supported in finding a way out from the no solution could be found dilemma. In this paper we introduce techniques that support the calculation of personalized diagnoses for inconsistent constraint sets. These techniques significantly improve the diagnosis prediction quality compared to approaches based on the calculation of minimal cardinality diagnoses. In order to show the applicability of our approach we present the results of an empirical study and a corresponding performance analysis.",,"Complex decision; Constraint-based; Empirical studies; Over-constrained problem; Performance analysis; Prediction quality; Scheduling systems; User requirements; Artificial intelligence; Discrete cosine transforms",Conference Paper,Scopus,2-s2.0-84896063670
"Zhao S., Yao H., Sun X., Jiang X., Xu P.","Flexible presentation of videos based on affective content analysis",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-35725-1_34,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892874193&doi=10.1007%2f978-3-642-35725-1_34&partnerID=40&md5=28fd20d078cf7cbeeb4472f5bc68246a","The explosion of multimedia contents has resulted in a great demand of video presentation. While most previous works focused on presenting certain type of videos or summarizing videos by event detection, we propose a novel method to present general videos of different genres based on affective content analysis. We first extract rich audio-visual affective features and select discriminative ones. Then we map effective features into corresponding affective states in an improved categorical emotion space using hidden conditional random fields (HCRFs). Finally we draw affective curves which tell the types and intensities of emotions. With the curves and related affective visualization techniques, we select the most affective shots and concatenate them to construct affective video presentation with a flexible and changeable type and length. Experiments on representative video database from the web demonstrate the effectiveness of the proposed method. © Springer-Verlag 2013.","Affective analysis; Emotion space; HCRFs.; Video presentation","Affective analysis; Content analysis; Emotion spaces; HCRFs; Hidden conditional random fields; Multimedia contents; Video presentations; Visualization technique; Computer science; Computers; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84892874193
"Lee L.-M., Jean F.-R.","Adaptation of hidden markov models for recognizing speech of reduced frame rate",2013,"IEEE Transactions on Cybernetics",12,10.1109/TCYB.2013.2240450,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890049050&doi=10.1109%2fTCYB.2013.2240450&partnerID=40&md5=baf3b40c9e2e22c06a8d71ad228f396c","The frame rate of the observation sequence in distributed speech recognition applications may be reduced to suit a resource-limited front-end device. In order to use models trained using full-frame-rate data in the recognition of reduced-frame-rate (RFR) data, we propose a method for adapting the transition probabilities of hidden Markov models (HMMs) to match the frame rate of the observation. Experiments on the recognition of clean and noisy connected digits are conducted to evaluate the proposed method. Experimental results show that the proposed method can effectively compensate for the frame-rate mismatch between the training and the test data. Using our adapted model to recognize the RFR speech data, one can significantly reduce the computation time and achieve the same level of accuracy as that of a method, which restores the frame rate using data interpolation. © 2013 IEEE.","Adaptation; Distributed speech recognition (DSR); HiddenMarkov model (HMM); Reduced frame rate (RFR)","Adaptation; Connected digits; Data interpolation; Distributed speech recognition; Frame rate; Front-end devices; Hidden markov models (HMMs); Transition probabilities; Hidden Markov models; Speech recognition; algorithm; article; artificial intelligence; automated pattern recognition; automatic speech recognition; computer simulation; human; information retrieval; methodology; probability; speech analysis; statistical model; Algorithms; Artificial Intelligence; Computer Simulation; Humans; Information Storage and Retrieval; Markov Chains; Models, Statistical; Pattern Recognition, Automated; Speech Production Measurement; Speech Recognition Software",Article,Scopus,2-s2.0-84890049050
"Khan Z.H., Gu I.Y.-H.","Nonlinear dynamic model for visual object tracking on grassmann manifolds with partial occlusion handling",2013,"IEEE Transactions on Cybernetics",12,10.1109/TSMCB.2013.2237900,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890107873&doi=10.1109%2fTSMCB.2013.2237900&partnerID=40&md5=a6821948a0617ae7c8229c2a9eabdc98","This paper proposes a novel Bayesian online learning and tracking scheme for video objects on Grassmann manifolds. Although manifold visual object tracking is promising, large and fast nonplanar (or out-of-plane) pose changes and long-term partial occlusions of deformable objects in video remain a challenge that limits the tracking performance. The proposed method tackles these problems with the main novelties on: 1) online estimation of object appearances on Grassmann manifolds; 2) optimal criterion-based occlusion handling for online updating of object appearances; 3) a nonlinear dynamic model for both the appearance basis matrix and its velocity; and 4) Bayesian formulations, separately for the tracking process and the online learning process, that are realized by employing two particle filters: one is on the manifold for generating appearance particles and another on the linear space for generating affine box particles. Tracking and online updating are performed in an alternating fashion to mitigate the tracking drift. Experiments using the proposed tracker on videos captured by a single dynamic/static camera have shown robust tracking performance, particularly for scenarios when target objects contain significant nonplanar pose changes and long-term partial occlusions. Comparisons with eight existing state-of-the-art/most relevant manifold/nonmanifold trackers with evaluations have provided further support to the proposed scheme. © 2013 IEEE.","Bayesian tracking; Grassmann manifolds; Manifold tracking; Nonlinear state-space modeling; Online manifold learning; Particle filters (PFs); Piecewise geodesics; Visual object tracking","Bayesian tracking; Grassmann manifold; Manifold learning; Manifold tracking; Nonlinear state space models; Particle filters (PFs); Piecewise geodesics; Visual object tracking; E-learning; Monte Carlo methods; Target tracking; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; computer simulation; methodology; nonlinear system; theoretical model; videorecording; Algorithms; Artificial Intelligence; Computer Simulation; Image Interpretation, Computer-Assisted; Models, Theoretical; Nonlinear Dynamics; Pattern Recognition, Automated; Video Recording",Article,Scopus,2-s2.0-84890107873
"Chen J., Cao N., Low K.H., Ouyang R., Tan C.K.-Y., Jaillet P.","Parallel Gaussian process regression with low-rank covariance matrix approximations",2013,"Uncertainty in Artificial Intelligence - Proceedings of the 29th Conference, UAI 2013",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888182286&partnerID=40&md5=f1b3c9279fd0917c6e0b58a50c6941b8","Gaussian processes (GP) are Bayesian non-parametric models that are widely used for probabilistic regression. Unfortunately, it cannot scale well with large data nor perform real-time predictions due to its cubic time cost in the data size. This paper presents two parallel GP regression methods that exploit low-rank covariance matrix approximations for distributing the computational load among parallel machines to achieve time efficiency and scalability. We theoretically guarantee the predictive performances of our proposed parallel GPs to be equivalent to that of some centralized approximate GP regression methods: The computation of their centralized counterparts can be distributed among parallel machines, hence achieving greater time efficiency and scalability. We analytically compare the properties of our parallel GPs such as time, space, and communication complexity. Empirical evaluation on two real-world datasets in a cluster of 20 computing nodes shows that our parallel GPs are significantly more time-efficient and scalable than their centralized counterparts and exact/full GP while achieving predictive performances comparable to full GP.",,"Communication complexity; Empirical evaluations; Gaussian process regression; Matrix approximation; Non-parametric model; Predictive performance; Probabilistic regression; Real-time prediction; Artificial intelligence; Cluster computing; Covariance matrix; Gaussian noise (electronic); Regression analysis; Scalability; Gaussian distribution",Conference Paper,Scopus,2-s2.0-84888182286
"Clark P., Harrison P., Balasubramanian N.","A study of the knowledge base requirements for passing an elementary science test",2013,"AKBC 2013 - Proceedings of the 2013 Workshop on Automated Knowledge Base Construction, Co-located with CIKM 2013",12,10.1145/2509558.2509565,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888177808&doi=10.1145%2f2509558.2509565&partnerID=40&md5=aa5eea7bd8b9994f6847764892ddd201","Our long-term interest is in machines that contain large amounts of general and scientific knowledge, stored in a ""computable"" form that supports reasoning and explanation. As a medium-term focus for this, our goal is to have the computer pass a fourth-grade science test, anticipating that much of the required knowledge will need to be acquired semi-automatically. This paper presents the first step towards this goal, namely a blueprint of the knowledge requirements for an early science exam, and a brief description of the resources, methods, and challenges involved in the semi-automatic acquisition of that knowledge. The result of our analysis suggests that as well as fact extraction from text and statistically driven rule extraction, three other styles of automatic knowledge base construction (AKBC) would be useful: acquiring definitional knowledge, direct 'reading' of rules from texts that state them, and, given a particular representational framework (e.g., qualitative reasoning), acquisition of specific instances of those models from text (e..g, specific qualitative models). © 2013 ACM.","knowledge acquisition; knowledge base construction","Elementary science; Fact extraction; Knowledge requirements; Knowledge-base construction; Long-term interests; Qualitative model; Qualitative reasoning; Scientific knowledge; Artificial intelligence; Automation; Extraction; Knowledge acquisition; Mergers and acquisitions; Knowledge based systems",Conference Paper,Scopus,2-s2.0-84888177808
"Mooij J.M., Heskes T.","Cyclic causal discovery from continuous equilibrium data",2013,"Uncertainty in Artificial Intelligence - Proceedings of the 29th Conference, UAI 2013",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888153835&partnerID=40&md5=fba7ba3481661b9624850da03c1aba43","We propose a method for learning cyclic causal models from a combination of observational and interventional equilibrium data. Novel aspects of the proposed method are its ability to work with continuous data (without assuming linearity) and to deal with feedback loops. Within the context of biochemical reactions, we also propose a novel way of modeling interventions that modify the activity of compounds instead of their abundance. For computational reasons, we approximate the nonlinear causal mechanisms by (coupled) local linearizations, one for each experimental condition. We apply the method to reconstruct a cellular signaling network from the flow cytometry data measured by Sachs et al. (2005). We show that our method finds evidence in the data for feedback loops and that it gives a more accurate quantitative description of the data at comparable model complexity.",,"Biochemical reactions; Cellular signaling networks; Continuous data; Equilibrium data; Experimental conditions; Local linearization; Model complexity; Quantitative description; Signaling; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84888153835
"Malone B., Yuan C.","Evaluating anytime algorithms for learning optimal Bayesian networks",2013,"Uncertainty in Artificial Intelligence - Proceedings of the 29th Conference, UAI 2013",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888180844&partnerID=40&md5=41f14a60f54835f531de9a8a024edb42","Exact algorithms for learning Bayesian networks guarantee to find provably optimal networks. However, they may fail in difficult learning tasks due to limited time or memory. In this research we adapt several anytime heuristic search-based algorithms to learn Bayesian networks. These algorithms find high-quality solutions quickly, and continually improve the incumbent solution or prove its optimality before resources are exhausted. Empirical results show that the anytime window A* algorithm usually finds higher-quality, often optimal, networks more quickly than other approaches. The results also show that, surprisingly, while generating networks with few parents per variable are structurally simpler, they are harder to learn than complex generating networks with more parents per variable.",,"Any-time algorithms; Exact algorithms; High-quality solutions; Learning Bayesian networks; Learning tasks; Optimal networks; Optimality; Search-based algorithms; Artificial intelligence; Bayesian networks; Complex networks; Heuristic algorithms; Optimization; Learning algorithms",Conference Paper,Scopus,2-s2.0-84888180844
"Ermon S., Gomes C.P., Sabharwal A., Selman B.","Optimization with parity constraints: From binary codes to discrete integration",2013,"Uncertainty in Artificial Intelligence - Proceedings of the 29th Conference, UAI 2013",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888163238&partnerID=40&md5=78397da74f597531e55ea23f656cf925","Many probabilistic inference tasks involve summations over exponentially large sets. Recently, it has been shown that these problems can be reduced to solving a polynomial number of MAP inference queries for a model augmented with randomly generated parity constraints. By exploiting a connection with max-likelihood decoding of binary codes, we show that these optimizations are computationally hard. Inspired by iterative message passing decoding algorithms, we propose an Integer Linear Programming (ILP) formulation for the problem, enhanced with new sparsification techniques to improve decoding performance. By solving the ILP through a sequence of LP relaxations, we get both lower and upper bounds on the partition function, which hold with high probability and are much tighter than those obtained with variational methods.",,"Integer Linear Programming; Iterative message passing; Lower and upper bounds; Parity constraint; Partition functions; Polynomial number; Probabilistic inference; Variational methods; Artificial intelligence; Binary codes; Inductive logic programming (ILP); Integer programming; Optimization; Iterative decoding",Conference Paper,Scopus,2-s2.0-84888163238
"Urdiales C., Perez E.J., Peinado G., Fdez-Carmona M., Peula J.M., Annicchiarico R., Sandoval F., Caltagirone C.","On the construction of a skill-based wheelchair navigation profile",2013,"IEEE Transactions on Neural Systems and Rehabilitation Engineering",12,10.1109/TNSRE.2013.2241454,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888102847&doi=10.1109%2fTNSRE.2013.2241454&partnerID=40&md5=185ff53c0b430ab5bc8e508c06b1c9af","Assisted wheelchair navigation is of key importance for persons with severe disabilities. The problem has been solved in different ways, usually based on the shared control paradigm. This paradigm consists of giving the user more or less control on a need basis. Naturally, these approaches require personalization: each wheelchair user has different skills and needs and it is hard to know a priori from diagnosis how much assistance must be provided. Furthermore, since there is no such thing as an average user, sometimes it is difficult to quantify the benefits of these systems. This paper proposes a new method to extract a prototype user profile using real traces based on more than 70 volunteers presenting different physical and cognitive skills. These traces are clustered to determine the average behavior that can be expected from a wheelchair user in order to cope with significant situations. Processed traces provide a prototype user model for comparison purposes, plus a simple method to obtain without supervision a skill-based navigation profile for any user while he/she is driving. This profile is useful for benchmarking but also to determine the situations in which a given user might require more assistance after evaluating how well he/she compares to the benchmark. Profile-based shared control has been successfully tested by 18 volunteers affected by left or right brain stroke at Fondazione Santa Lucia, in Rome, Italy. © 2013 IEEE.","Benchmark; data mining; rehabilitation robotics; robot control; user centered design","Average behavior; Personalizations; Rehabilitation robotics; Robot controls; Severe disabilities; User centered designs; Wheelchair navigation; Wheelchair users; Benchmarking; Data mining; Navigation; Robotics; Robots; Wheelchairs; Handicapped persons; algorithm; article; artificial intelligence; cerebrovascular accident; computer assisted therapy; equipment; human; man machine interaction; methodology; motor performance; pathophysiology; robotics; wheelchair; Algorithms; Artificial Intelligence; Humans; Man-Machine Systems; Motor Skills; Robotics; Stroke; Therapy, Computer-Assisted; Wheelchairs",Article,Scopus,2-s2.0-84888102847
"Sutantyo D., Levi P., Moslinger C., Read M.","Collective-adaptive Lévy flight for underwater multi-robot exploration",2013,"2013 IEEE International Conference on Mechatronics and Automation, IEEE ICMA 2013",12,10.1109/ICMA.2013.6617961,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887910962&doi=10.1109%2fICMA.2013.6617961&partnerID=40&md5=baa7df82f6e37435a387eaff9d58fc72","This paper presents the use of Lévy flight, a bio-inspired algorithm, to efficiently and effectively locate targets in underwater search scenarios. We demonstrate how a novel adaptation strategy, building on the Firefly optimization algorithm, substantially improves Lévy flight performance. The adaptation strategy represents a swarm intelligence approach, the distribution patterns governing robot motion are optimized in accordance with the distribution of targets in the environment, as detected by and communicated between the robots themselves. Simulation experiments contrasting the performance of the present Lévy flight and two other search strategies in both sparse and clustered distributions of targets are conducted. We identify Lévy flight as exhibiting the best performance, and this is improved with our adaptation strategy, particularly when targets are clustered. Finally, Lévy flight's superior performance over the alternative strategies examined here is empirically confirmed through deployment on real-world underwater swarm robotic platforms. © 2013 IEEE.",,"Adaptation strategies; Bio-inspired algorithms; Distribution patterns; Multi-robot exploration; Optimization algorithms; Search strategies; Swarm Intelligence; Underwater searches; Algorithms; Artificial intelligence; Flight simulators",Conference Paper,Scopus,2-s2.0-84887910962
"Ekins S., Freundlich J.S., Reynolds R.C.","Fusing dual-event data sets for mycobacterium tuberculosis machine learning models and their evaluation",2013,"Journal of Chemical Information and Modeling",12,10.1021/ci400480s,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888586878&doi=10.1021%2fci400480s&partnerID=40&md5=2212bd818dbf6f09e74fee5d92226839","The search for new tuberculosis treatments continues as we need to find molecules that can act more quickly, be accommodated in multidrug regimens, and overcome ever increasing levels of drug resistance. Multiple large scale phenotypic high-throughput screens against Mycobacterium tuberculosis (Mtb) have generated dose response data, enabling the generation of machine learning models. These models also incorporated cytotoxicity data and were recently validated with a large external data set. A cheminformatics data-fusion approach followed by Bayesian machine learning, Support Vector Machine, or Recursive Partitioning model development (based on publicly available Mtb screening data) was used to compare individual data sets and subsequent combined models. A set of 1924 commercially available molecules with promising antitubercular activity (and lack of relative cytotoxicity to Vero cells) were used to evaluate the predictive nature of the models. We demonstrate that combining three data sets incorporating antitubercular and cytotoxicity data in Vero cells from our previous screens results in external validation receiver operator curve (ROC) of 0.83 (Bayesian or RP Forest). Models that do not have the highest 5-fold cross-validation ROC scores can outperform other models in a test set dependent manner. We demonstrate with predictions for a recently published set of Mtb leads from GlaxoSmithKline that no single machine learning model may be enough to identify compounds of interest. Data set fusion represents a further useful strategy for machine learning construction as illustrated with Mtb. Coverage of chemistry and Mtb target spaces may also be limiting factors for the whole-cell screening data generated to date. © 2013 American Chemical Society.",,"Antitubercular activities; Glaxo smithkline; High-throughput screens; Machine learning models; Mycobacterium tuberculosis; Receiver operator curves; Recursive partitioning model; Single- machines; Cytotoxicity; Molecules; Pharmacodynamics; Regression analysis; Learning systems; cytotoxin; tuberculostatic agent; animal; article; artificial intelligence; Bayes theorem; cell survival; chemistry; Chlorocebus aethiops; computer interface; decision tree; drug effect; human; Mycobacterium tuberculosis; receiver operating characteristic; support vector machine; Vero cell line; Animals; Antitubercular Agents; Artificial Intelligence; Bayes Theorem; Cell Survival; Cercopithecus aethiops; Cytotoxins; Decision Trees; Humans; Mycobacterium tuberculosis; ROC Curve; Support Vector Machines; User-Computer Interface; Vero Cells",Article,Scopus,2-s2.0-84888586878
"Alreshoodi M., Woods J.","An empirical study based on a fuzzy logic system to assess the QoS/QoE correlation for layered video streaming",2013,"2013 IEEE International Conference on Computational Intelligence and Virtual Environments for Measurement Systems and Applications, CIVEMSA 2013 - Proceedings",12,10.1109/CIVEMSA.2013.6617417,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886858063&doi=10.1109%2fCIVEMSA.2013.6617417&partnerID=40&md5=c599e19d5a09e4947b16ac860bdc52b2","A model that can predict an end user satisfaction or QoE (Quality of Experience) directly from the network QoS (Quality of Service) is still illusive in the field of image processing and is completely absent in multi-layered video. This motivates the derivation of a meaningful QoS to QoE mapping function to allow one to be predicted in the absence of the other. This paper presents an affine fuzzy logic based system that can map the QoS to QoE and can be extended to layered video streaming. The proposed methodology employs a learning system which optimizes the coded layered video for best QoE. Four QoS parameters are chosen as the inputs of the designed model, while the output is the Peak Signal-to-Noise Ratio (PSNR). The designed membership functions and the fuzzy rules extracted from the input and the output enable the proposed model to identify and learn the video QoE. © 2013 IEEE.","fuzzy logic; QoE; QoS; video quality","Empirical studies; End-user satisfactions; Fuzzy logic system; Logic based systems; Peak signal-to-noise ratio; QoE; Quality of experience (QoE); Video quality; Artificial intelligence; Fuzzy logic; Human computer interaction; Image processing; Measurements; Video streaming; Virtual reality; Quality of service",Conference Paper,Scopus,2-s2.0-84886858063
"Chen S.-M., Lee L.-W., Shen V.R.L.","Weighted fuzzy interpolative reasoning systems based on interval type-2 fuzzy sets",2013,"Information Sciences",12,10.1016/j.ins.2013.05.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882824653&doi=10.1016%2fj.ins.2013.05.002&partnerID=40&md5=ca7cdf4014f9061f99c3051ad66eaa98","In this paper, we present a weighted fuzzy interpolative reasoning method for sparse fuzzy rule-based systems based on interval type-2 fuzzy sets. We also apply the proposed weighted fuzzy interpolative reasoning method to deal with the truck backer-upper control problem. The proposed method satisfies the seven evaluation indices for fuzzy interpolative reasoning. The experimental results show that the proposed method outperforms the existing methods. It provides us with a useful way for dealing with fuzzy interpolative reasoning in sparse fuzzy rule-based systems. © 2013 Elsevier Inc. All rights reserved.","Fuzzy interpolative reasoning; Interval type-2 fuzzy sets; Sparse fuzzy rule-based systems; Weighted fuzzy interpolative reasoning","Control problems; Evaluation index; Fuzzy interpolative reasoning; Fuzzy rule-based systems; Interval type-2 fuzzy sets; Weighted fuzzy interpolative reasoning; Artificial intelligence; Software engineering; Fuzzy sets",Article,Scopus,2-s2.0-84882824653
"Tantithamthavorn C., Ihara A., Matsumoto K.-I.","Using co-change histories to improve bug localization performance",2013,"SNPD 2013 - 14th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing",12,10.1109/SNPD.2013.92,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886043776&doi=10.1109%2fSNPD.2013.92&partnerID=40&md5=cee3bba731af59a4e56feaf4b8a11f83","A large open source software (OSS) project receives many bug reports on a daily basis. Bug localization techniques automatically pinpoint source code fragments that are relevant to a bug report, thus enabling faster correction. Even though many bug localization methods have been introduced, their performance is still not efficient. In this research, we improved on existing bug localization methods by taking into account co-change histories. We conducted experiments on two OSS datasets, the Eclipse SWT 3.1 project and the Android ZXing project. We validated our approach by evaluating effectiveness compared to the state-of-the-art approach Bug Locator. In the Eclipse SWT 3.1 project, our approach reliably identified source code that should be fixed for a bug in 72.46% of the total bugs, while Bug Locator identified only 51.02%. In the Android ZXing project, our approach identified 85.71%, while Bug Locator identified 60%. © 2013 IEEE.","Bug Localization; Co-Change Histories; Information Retrieval; Software Maintenance","Bug localizations; Bug reports; Open source software projects; Source codes; State-of-the-art approach; Artificial intelligence; Computer software maintenance; Information retrieval; Robots; Software engineering",Conference Paper,Scopus,2-s2.0-84886043776
"Wu G., Kim M., Wang Q., Gao Y., Liao S., Shen D.","Unsupervised deep feature learning for deformable registration of MR brain images",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-40763-5_80,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885932240&doi=10.1007%2f978-3-642-40763-5_80&partnerID=40&md5=5c49270f9e4a1d210b0dc9b30d0c6e32","Establishing accurate anatomical correspondences is critical for medical image registration. Although many hand-engineered features have been proposed for correspondence detection in various registration applications, no features are general enough to work well for all image data. Although many learning-based methods have been developed to help selection of best features for guiding correspondence detection across subjects with large anatomical variations, they are often limited by requiring the known correspondences (often presumably estimated by certain registration methods) as the ground truth for training. To address this limitation, we propose using an unsupervised deep learning approach to directly learn the basis filters that can effectively represent all observed image patches. Then, the coefficients by these learnt basis filters in representing the particular image patch can be regarded as the morphological signature for correspondence detection during image registration. Specifically, a stacked two-layer convolutional network is constructed to seek for the hierarchical representations for each image patch, where the high-level features are inferred from the responses of the low-level network. By replacing the hand-engineered features with our learnt data-adaptive features for image registration, we achieve promising registration results, which demonstrates that a general approach can be built to improve image registration by using data-adaptive features through unsupervised deep learning. © 2013 Springer-Verlag.",,"Anatomical variations; Convolutional networks; Correspondence detection; Deep feature learning; Deformable registration; Hierarchical representation; Learning-based methods; Morphological signatures; Artificial intelligence; Computer science; Image registration",Conference Paper,Scopus,2-s2.0-84885932240
"Modi C.N., Patel D.","A novel hybrid-network intrusion detection system (H-NIDS) in cloud computing",2013,"Proceedings of the 2013 IEEE Symposium on Computational Intelligence in Cyber Security, CICS 2013 - 2013 IEEE Symposium Series on Computational Intelligence, SSCI 2013",12,10.1109/CICYBS.2013.6597201,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885610256&doi=10.1109%2fCICYBS.2013.6597201&partnerID=40&md5=ad4b8c7f42b17d6162a48998fdbb2246","To detect and prevent network intrusions in Cloud computing environment, we propose a novel security framework hybrid-network intrusion detection system (H-NIDS). We use different classifiers (Bayesian, Associative and Decision tree) and Snort to implement this framework. This framework aims to detect network attacks in Cloud by monitoring network traffic, while ensuring performance and service quality. We evaluate the performance and detection efficiency of H-NIDS for ensuring its feasibility in Cloud. The results show that the proposed framework has higher detection rate and low false positives at an affordable computational cost. © 2013 IEEE.","Classifier; Cloud computing; Network Intrusion detection; Snort; Virtualization","Cloud computing environments; Computational costs; Detection efficiency; Intrusion Detection Systems; Network intrusion detection; Security frameworks; Snort; Virtualizations; Artificial intelligence; Classifiers; Cloud computing; Computer systems; Decision trees; Intrusion detection; Computer crime",Conference Paper,Scopus,2-s2.0-84885610256
"Romero M., Usart M.","Serious games integration in an entrepreneurship Massive Online Open Course (MOOC)",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-40790-1_21,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885223302&doi=10.1007%2f978-3-642-40790-1_21&partnerID=40&md5=528de050b2b9114b4de1454ed95bdcd5","The current crisis in Europe has raised the need to increase the entrepreneurship orientation of students and adult citizens. At the same time, Massive Online Open Course (MOOC) has appeared as a disruptive innovation that permits to engage a large number of persons in an online open course available through Internet to anyone aiming to enrol. MOOC has been deployed based on basic technologies such text-based materials, video-lectures and forum based interactions. In this study we introduce the design of a MOOC for Entrepreneurship education that aims to go one step further by integrating the use of Serious Games as a key part of the methodology for teaching and learning entrepreneurship basics in the context of a MOOC. © 2013 Springer-Verlag.","Entrepreneurship; Game Based Learning; Massive Online Open Course; Serious Games","Disruptive innovations; Entrepreneurship; Entrepreneurship education; Game-based Learning; Key parts; Massive Online Open Course; Serious games; Teaching and learning; Artificial intelligence; Computer science; E-learning",Conference Paper,Scopus,2-s2.0-84885223302
"Clifton D.A., Wong D., Clifton L., Wilson S., Way R., Pullinger R., Tarassenko L.","A large-scale clinical validation of an integrated monitoring system in the Emergency Department",2013,"IEEE Journal of Biomedical and Health Informatics",12,10.1109/JBHI.2012.2234130,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885161339&doi=10.1109%2fJBHI.2012.2234130&partnerID=40&md5=b3b8fe22e5406fa750ba5dccff1f6cee","We consider an integrated patient monitoring system, combining electronic patient records with high-rate acquisition of patient physiological data. There remain many challenges in increasing the robustness of ""e-health"" applications to a level at which they are clinically useful, particularly in the use of automated algorithms used to detect and cope with artifact in data contained within the electronic patient record, and in analyzing and communicating the resultant data for reporting to clinicians. There is a consequential ""plague of pilots,"" in which engineering prototype systems do not enter into clinical use. This paper describes an approach in which, for the first time, the Emergency Department (ED) of a major research hospital has adopted such systems for use during a large clinical trial. We describe the disadvantages of existing evaluation metrics when applied to such large trials, and propose a solution suitable for large-scale validation. We demonstrate that machine learning technologies embedded within healthcare information systems can provide clinical benefit, with the potential to improve patient outcomes in the busy environment of a major ED and other high-dependence areas of patient care. © 2013 IEEE.","Biomedical informatics; Biomedical signal processing; Machine learning","Biomedical informatics; Biomedical signal processing; Electronic patient record; Emergency departments; Health care information system; Integrated monitoring systems; Machine learning technology; Patient monitoring systems; Learning systems; Medical computing; Patient monitoring; Signal processing; Emergency rooms; adolescent; adult; aged; algorithm; artificial intelligence; electronic medical record; emergency health service; female; human; male; medical informatics; middle aged; physiologic monitoring; procedures; signal processing; system analysis; very elderly; young adult; Adolescent; Adult; Aged; Aged, 80 and over; Algorithms; Artificial Intelligence; Electronic Health Records; Emergency Service, Hospital; Female; Humans; Male; Medical Informatics Computing; Middle Aged; Monitoring, Physiologic; Signal Processing, Computer-Assisted; Systems Integration; Young Adult",Article,Scopus,2-s2.0-84885161339
"Carral D., Scheider S., Janowicz K., Vardeman C., Krisnadhi A.A., Hitzler P.","An ontology design pattern for cartographic map scaling",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-38288-8-6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885008832&doi=10.1007%2f978-3-642-38288-8-6&partnerID=40&md5=f0fae6197356077e354f1acd9592968d","The concepts of scale is at the core of cartographic abstraction and mapping. It defines which geographic phenomena should be displayed, which type of geometry and map symbol to use, which measures can be taken, as well as the degree to which features need to be exaggerated or spatially displaced. In this work, we present an ontology design pattern for map scaling using the Web Ontology Language (OWL) within a particular extension of the OWL RL profile. We explain how it can be used to describe scaling applications, to reason over scale levels, and geometric representations. We propose an axiomatization that allows us to impose meaningful constraints on the pattern, and, thus, to go beyond simple surface semantics. Interestingly, this includes several functional constraints currently not expressible in any of the OWL profiles. We show that for this specific scenario, the addition of such constraints does not increase the reasoning complexity which remains tractable. © 2013 Springer-Verlag Berlin Heidelberg.",,"Axiomatization; Functional constraints; Geographic phenomena; Geometric representation; Ontology design; Scale levels; Web ontology language; Artificial intelligence; Computer science; Birds",Conference Paper,Scopus,2-s2.0-84885008832
"Straková J., Straka M., Hajič J.","A new state-of-the-art Czech named entity recognizer",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-40585-3_10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884935076&doi=10.1007%2f978-3-642-40585-3_10&partnerID=40&md5=6f65debaa5f272c57ce779aebb7918ad","We present a new named entity recognizer for the Czech language. It reaches 82.82 F-measure on the Czech Named Entity Corpus 1.0 and significantly outperforms previously published Czech named entity recognizers. On the English CoNLL-2003 shared task, we achieved 89.16 F-measure, reaching comparable results to the English state of the art. The recognizer is based on Maximum Entropy Markov Model and a Viterbi algorithm decodes an optimal sequence labeling using probabilities estimated by a maximum entropy classifier. The classification features utilize morphological analysis, two-stage prediction, word clustering and gazetteers. © 2013 Springer-Verlag.","Czech; named entities; named entity recognition","Classification features; Czech; Entropy classifiers; Maximum entropy Markov model; Morphological analysis; Named entities; Named entity recognition; Using probabilities; Artificial intelligence; Computer science; Natural language processing systems",Conference Paper,Scopus,2-s2.0-84884935076
"Lobanov M.Y., Sokolovskiy I.V., Galzitskaya O.V.","IsUnstruct: Prediction of the residue status to be ordered or disordered in the protein chain by a method based on the Ising model",2013,"Journal of Biomolecular Structure and Dynamics",12,10.1080/07391102.2012.718529,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884568065&doi=10.1080%2f07391102.2012.718529&partnerID=40&md5=18226bb2b89254734e0e6a385884f157",[No abstract available],"disordered residues; intrinsically disordered proteins; Ising model; library of disordered pattern; structured PDB","algorithm; article; computer program; Internet; machine learning; prediction; priority journal; protein conformation; Protein Data Bank; protein database; protein structure; Amino Acids; Artificial Intelligence; Computational Biology; Databases, Protein; Internet; Proteins; Reproducibility of Results; Software",Article,Scopus,2-s2.0-84884568065
"Lin B., He X., Zhang C., Ji M.","Parallel vector field embedding",2013,"Journal of Machine Learning Research",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887423857&partnerID=40&md5=55233fea459fe3b771e1f46da42e1c3f","We propose a novel local isometry based dimensionality reduction method from the perspective of vector fields, which is called parallel vector field embedding (PFE). We first give a discussion on local isometry and global isometry to show the intrinsic connection between parallel vector fields and isometry. The problem of finding an isometry turns out to be equivalent to finding orthonormal parallel vector fields on the data manifold. Therefore, we first find orthonormal parallel vector fields by solving a variational problem on the manifold. Then each embedding function can be obtained by requiring its gradient field to be as close to the corresponding parallel vector field as possible. Theoretical results show that our method can precisely recover the manifold if it is isometric to a connected open subset of Euclidean space. Both synthetic and real data examples demonstrate the effectiveness of our method even if there is heavy noise and high curvature. © 2013 Binbin Lin, Xiaofei He, Chiyuan Zhang and Ming Ji.","Covariant derivative; Isometry; Manifold learning; Out-of-sample extension; Vector field","Covariant derivatives; Isometry; Manifold learning; Out-of-sample extension; Vector fields; Artificial intelligence; Software engineering; Vectors",Article,Scopus,2-s2.0-84887423857
"Preziosi E., Del Bon A., Romano E., Petrangeli A.B., Casadei S.","Vulnerability to Drought of a Complex Water Supply System. The Upper Tiber Basin Case Study (Central Italy)",2013,"Water Resources Management",12,10.1007/s11269-013-0434-9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883753290&doi=10.1007%2fs11269-013-0434-9&partnerID=40&md5=25701815ea877f36bb539ce7bb3455d8","The importance of simulation models to assess the impacts of droughts and the effects of mitigation options on water supply systems is well known. However a common procedure about the exploitation of model results is not established yet. Vulnerability is used to characterize the performance of the system, and it can be a helpful indicator in the evaluation of the most likely failures. In this paper a water allocation model is applied to the water supply system of the upper Tiber Basin (Central Italy) in which both surface waters (rivers, reservoirs) and ground waters (wells, springs) are exploited to feed mainly irrigation and civil users. Drought vulnerability indices are calculated to analyze the performance of the supply system under different climate and management conditions. Water shortage scenarios are simulated as a progressive reduction of mean precipitation, an increase in its standard deviation or a combination of both. The model shows that the safety of the water supply system mainly relies on the reservoirs and that the foreseen increased exploitation of the springs to replace contaminated wells, could be seriously limited by discharge decrease during fall. The vulnerability reduction obtained by a hypothetical augmentation of the storage capacity through additional small reservoirs was positively tested by the model. In conclusion vulnerability indices and synoptic risk maps demonstrated to be useful tools to analyze the model outputs. They provide easy-to-read scenarios to be used in a decision making framework considering negotiating among the main users. © 2013 Springer Science+Business Media Dordrecht.","Climate change; Decision support system; Drought management; Integrated water resources management; Water allocation model; Water shortage","Decision-making frameworks; Drought management; Integrated Water Resources Management; Mean precipitation; Vulnerability index; Vulnerability reductions; Water allocation models; Water shortages; Artificial intelligence; Climate change; Computer simulation; Decision support systems; Groundwater; Maps; Water resources; Water supply; Water supply systems; Drought; climate change; decision support system; drought; groundwater; integrated approach; negotiation process; numerical model; reservoir; resource allocation; risk assessment; surface water; vulnerability; water management; water supply; Italy; Tiber Basin",Article,Scopus,2-s2.0-84883753290
"Castro L., Lefebvre E., Lefebvre L.A.","Adding intelligence to mobile asset management in hospitals: The true value of RFID",2013,"Journal of Medical Systems",12,10.1007/s10916-013-9963-2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881302166&doi=10.1007%2fs10916-013-9963-2&partnerID=40&md5=0e48ba295a1400e220d52d770a385343","RFID (Radio Frequency Identification) technology is expected to play a vital role in the healthcare arena, especially in times when cost containments are at the top of the priorities of healthcare management authorities. Medical equipment represents a significant share of yearly healthcare operational costs; hence, ensuring an effective and efficient management of such key assets is critical to promptly and reliably deliver a diversity of clinical services at the patient bedside. Empirical evidence from a phased-out RFID implementation in one European hospital demonstrates that RFID has the potential to transform asset management by improving inventorymanagement, enhancing asset utilization, increasing staff productivity, improving care services, enhancing maintenance compliance, and increasing information visibility. Most importantly, RFID allows the emergence of intelligent asset management processes, which is, undoubtedly, the most important benefit that could be derived from the RFID system. Results show that the added intelligence can be rather basic (auto-status change) or a bit more advanced (personalized automatic triggers). More importantly, adding intelligence improves planning and decision-making processes. © Springer Science+Business Media New York 2013.","Added-intelligence; Hospitals; Inefficiencies; Intelligent processes; Mobile assetsmanagement; RFID","article; artificial intelligence; health care management; health care planning; health care utilization; hospital management; hospital personnel management; human; medical decision making; medical device; medical information; medical technology; radiofrequency identification; health care delivery; health service; hospital; intelligence; patient identification; radio frequency identification device; radiofrequency radiation; Delivery of Health Care; Health Services Administration; Hospitals; Humans; Intelligence; Patient Identification Systems; Radio Frequency Identification Device; Radio Waves",Article,Scopus,2-s2.0-84881302166
"Tamis-Lemonda C.S., Kuchirko Y., Tafuro L.","From action to interaction: Infant object exploration and mothers' contingent responsiveness",2013,"IEEE Transactions on Autonomous Mental Development",12,10.1109/TAMD.2013.2269905,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884504450&doi=10.1109%2fTAMD.2013.2269905&partnerID=40&md5=414790613d14b7926a3648b7848e4709","We examined maternal contingent responsiveness to infant object exploration in 190 mother-infant pairs from diverse cultural communities. Dyads were video-recorded during book-sharing and play when infants were 14 mo. Researchers coded the temporal onsets and offsets of infant and mother object exploration and mothers' referential (e.g., 'That's a bead') and regulatory (e.g., 'Stop it') language. The times when infant or mother were neither exploring objects nor communicating were classified as 'off task.' Sequential analysis was used to examine whether certain maternal behaviors were more (or less) likely to follow infant object exploration relative to chance, to one another, and to times when infants were off task. Mothers were more likely to explore objects and use referential language in response to infant object exploration than to use regulatory language or be off task, and maternal behaviors were reduced in the context of infants being off task. Additionally, mothers coordinated their object exploration with referential language specifically; thus, mothers' responses to infants were didactic and multimodal. Infant object exploration elicits reciprocal object exploration and informative verbal input from mothers, illustrating the active role infants play in their social experiences. © 2013 IEEE.","Infant object exploration; infant word learning; language development; maternal responsiveness","Language development; maternal responsiveness; Multi-modal; Object exploration; Sequential analysis; Word learning; Software engineering; Artificial intelligence",Article,Scopus,2-s2.0-84884504450
"Patterson M.K., Poole S.W., Hsu C.-H., Maxwell D., Tschudi W., Coles H., Martinez D.J., Bates N.","TUE, a new energy-efficiency metric applied at ORNL's Jaguar",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-38750-0_28,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884470441&doi=10.1007%2f978-3-642-38750-0_28&partnerID=40&md5=8f75bafa17e9418cb64857966f871e4d","The metric, Power Usage Effectiveness (PUE), has been successful in improving energy efficiency of data centers, but it is not perfect. One challenge is that PUE does not account for the power distribution and cooling losses inside IT equipment. This is particularly problematic in the HPC (high performance computing) space where system suppliers are moving cooling and power subsystems into or out of the cluster. This paper proposes two new metrics: ITUE (IT-power usage effectiveness), similar to PUE but ""inside"" the IT and TUE (total-power usage effectiveness), which combines the two for a total efficiency picture. We conclude with a demonstration of the method, and a case study of measurements at ORNL's Jaguar system. TUE provides a ratio of total energy, (internal and external support energy uses) and the specific energy used in the HPC. TUE can also be a means for comparing HPC site to HPC site. © 2013 Springer-Verlag.","data center; energy-efficiency; HPC; metrics","Data centers; High performance computing; HPC; metrics; Power distributions; Power Usage Effectiveness (PUE); System suppliers; Total efficiency; Artificial intelligence; Computer science; Energy efficiency",Conference Paper,Scopus,2-s2.0-84884470441
"Gałkowski T.","Kernel estimation of regression functions in the boundary regions",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-38610-7_15,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884401002&doi=10.1007%2f978-3-642-38610-7_15&partnerID=40&md5=203f224cce84a3cc1e3f548de7d2de21","The article refers to the problem of regression functions estimation in the points near the edges of their domain. We investigate the model yi = R (xi) + εi, i = 1, 2, ... n, where xi is assumed to be the set of deterministic inputs, xi ∈ D, y i is the set of probabilistic outputs, and εi is a measurement noise with zero mean and bounded variance. is a completely unknown function. The possible clue of finding unknown function is to apply the algorithms based on Parzen kernel [5], [12]. The commonly known inconvenience of these algorithms is that the error of estimation dramatically increases if the point of estimation x is coming up to the left or right bound of interval D. The main result of this paper is a new, original algorithm (named NMS) based on integral version of Parzen methods for estimation of edge values of a function R. The cross-validation-like technique is used in this procedure. The results of numerical experiments are presented. © 2013 Springer-Verlag.",,"Boundary regions; Kernel estimation; Measurement Noise; Numerical experiments; Original algorithms; Probabilistic output; Regression function; Algorithms; Artificial intelligence; Soft computing; Estimation",Conference Paper,Scopus,2-s2.0-84884401002
"Toda T.","Hypergraph transversal computation with binary decision diagrams",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-38527-8_10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884368181&doi=10.1007%2f978-3-642-38527-8_10&partnerID=40&md5=d8c6b1e6f877fab6c7e7316546682ad2","We study a hypergraph transversal computation: given a hypergraph, the problem is to generate all minimal transversals. This problem is related to many applications in computer science and various algorithms have been proposed. We present a new efficient algorithm using the compressed data structures BDDs and ZDDs, and we analyze the time complexity for it. By conducting computational experiments, we show that our algorithm is highly competitive with existing algorithms. © 2013 Springer-Verlag.","artificial intelligence; BDD; Boolean function; data mining; hitting set; logic; monotone dualization; transversal hypergraph; ZDD","BDD; Hitting sets; Hypergraph; logic; monotone dualization; ZDD; Artificial intelligence; Binary decision diagrams; Boolean functions; Data mining; Algorithms",Conference Paper,Scopus,2-s2.0-84884368181
"Hoexter M.Q., Miguel E.C., Diniz J.B., Shavitt R.G., Busatto G.F., Sato J.R.","Predicting obsessive-compulsive disorder severity combining neuroimaging and machine learning methods",2013,"Journal of Affective Disorders",12,10.1016/j.jad.2013.05.041,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888352564&doi=10.1016%2fj.jad.2013.05.041&partnerID=40&md5=a50c3e6178b463d9d2e73595ea8f6d13","Background: Recently, machine learning methods have been used to discriminate, on an individual basis, patients from healthy controls through brain structural magnetic resonance imaging (MRI). However, the application of these methods to predict the severity of psychiatric symptoms is less common. Methods: Herein, support vector regression (SVR) was employed to evaluate whether gray matter volumes encompassing cortical-subcortical loops contain discriminative information to predict obses-sive-compulsive disorder (OCD) symptom severity in 37 treatment-naïve adult OCD patients. Results: The Pearson correlation coefficient between predicted and observed symptom severity scores was 0.49 (p=0.002) for total Dimensional Yale-Brown Obsessive-Compulsive Scale (DY-BOCS) and 0.44 (p=0.006) for total Yale-Brown Obsessive-Compulsive Scale (Y-BOCS). The regions that contained the most discriminative information were the left medial orbitofrontal cortex and the left putamen for both scales. Limitations: Our sample is relatively small and our results must be replicated with independent and larger samples. Conclusions: These results indicate that machine learning methods such as SVR analysis may identify neurobiological markers to predict OCD symptom severity based on individual structural MRI datasets. © 2013 Elsevier B.V. All rights reserved.","Machine learning; Magnetic resonance imaging; Neuroimaging; Obsessive-compulsive disorder; Support vector regression; Symptom severity","adult; aged; article; clinical article; disease severity; gray matter; human; machine learning; neuroimaging; nuclear magnetic resonance imaging; obsessive compulsive disorder; orbital cortex; predictive value; priority journal; putamen; scoring system; support vector machine; Machine learning; Magnetic resonance imaging; Neuroimaging; Obsessive-compulsive disorder; Support vector regression; Symptom severity; Adolescent; Adult; Artificial Intelligence; Cerebral Cortex; Female; Humans; Magnetic Resonance Imaging; Male; Middle Aged; Neuroimaging; Obsessive-Compulsive Disorder; Prognosis; Putamen; Severity of Illness Index; Young Adult",Article,Scopus,2-s2.0-84888352564
"Gagie T., Karhu K., Navarro G., Puglisi S.J., Sirén J.","Document listing on repetitive collections",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-38905-4_12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884295366&doi=10.1007%2f978-3-642-38905-4_12&partnerID=40&md5=88d26e5601d72719aa6ba60657d4b145","Many document collections consist largely of repeated material, and several indexes have been designed to take advantage of this. There has been only preliminary work, however, on document retrieval for repetitive collections. In this paper we show how one of those indexes, the run-length compressed suffix array (RLCSA), can be extended to support document listing. In our experiments, our additional structures on top of the RLCSA can reduce the query time for document listing by an order of magnitude while still using total space that is only a fraction of the raw collection size. As a byproduct, we develop a new document listing technique for general collections that is of independent interest. © 2013 Springer-Verlag.",,"Additional structures; Compressed suffix array; Document collection; Document Retrieval; Query time; Run length; Artificial intelligence; Computer science; Pattern matching",Conference Paper,Scopus,2-s2.0-84884295366
"Liarokapis M.V., Artemiadis P.K., Kyriakopoulos K.J., Manolakos E.S.","A learning scheme for reach to grasp movements: On emg-based interfaces using task specific motion decoding models",2013,"IEEE Journal of Biomedical and Health Informatics",12,10.1109/JBHI.2013.2259594,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883887265&doi=10.1109%2fJBHI.2013.2259594&partnerID=40&md5=e3a893b37f8efb22d333622f73ca13ea","A learning scheme based on random forests is used to discriminate between different reach to grasp movements in 3-D space, based on the myoelectric activity of human muscles of the upper-arm and the forearm. Task specificity for motion decoding is introduced in two different levels: Subspace to move toward and object to be grasped. The discrimination between the different reach to grasp strategies is accomplished with machine learning techniques for classification. The classification decision is then used in order to trigger an EMG-based task-specific motion decoding model. Task specific models manage to outperform 'general' models providing better estimation accuracy. Thus, the proposed scheme takes advantage of a framework incorporating both a classifier and a regressor that cooperate advantageously in order to split the task space. The proposed learning scheme can be easily used to a series of EMG-based interfaces that must operate in real time, providing data-driven capabilities for multiclass problems, that occur in everyday life complex environments. © 2013 IEEE.","Electromyography (EMG); learning scheme; random forests; task specificity","Classification decision; Complex environments; Learning schemes; Machine learning techniques; Random forests; Reach-to-grasp movements; task specificity; Task-specific models; Decision trees; Electromyography; Learning systems; Decoding; adult; artificial intelligence; decision tree; devices; electromyography; female; hand strength; human; limb prosthesis; male; physiology; procedures; robotics; signal processing; young adult; Adult; Artificial Intelligence; Artificial Limbs; Decision Trees; Electromyography; Female; Hand Strength; Humans; Male; Robotics; Signal Processing, Computer-Assisted; Young Adult",Article,Scopus,2-s2.0-84883887265
"Saha S., Ghosh S., Konar A., Nagar A.K.","Gesture recognition from Indian classical dance using kinect sensor",2013,"Proceedings - 5th International Conference on Computational Intelligence, Communication Systems, and Networks, CICSyN 2013",12,10.1109/CICSYN.2013.11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883426067&doi=10.1109%2fCICSYN.2013.11&partnerID=40&md5=a514407a611bf35d91f7a212aa721116","This work proposes gesture recognition algorithm for Indian Classical Dance Style using Kinect sensor. This device generates the skeleton of human body from which twenty different junction 3-dimensional coordinates are obtained. Here we require only eleven coordinates for the proposed work. Basically six joints coordinates about right and left hands and five upper body joint coordinates are processed. A unique system of feature extraction have been used to distinguish between 'Anger', 'Fear', 'Happiness', 'Sadness' and 'Relaxation'. This system checks whether the emotion is positive or negative with its intensity information. A total of twenty three features have been extracted based on the distance between different parts of the upper human body, the velocity and acceleration generated along with the angle between different joints. The proposed algorithm gives a high recognition rate of 86.8% using SVM. © 2013 IEEE.","angle; feature extraction; gesture recognition; Kinect sensor; skeleton","3-dimensional; angle; Gesture recognition algorithm; Intensity information; Joint coordinates; Kinect sensors; skeleton; Upper bodies; Artificial intelligence; Communication systems; Feature extraction; Musculoskeletal system; Sensors; Gesture recognition",Conference Paper,Scopus,2-s2.0-84883426067
"Bachrach Y., Parkes D.C., Rosenschein J.S.","Computing cooperative solution concepts in coalitional skill games",2013,"Artificial Intelligence",12,10.1016/j.artint.2013.07.005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883396551&doi=10.1016%2fj.artint.2013.07.005&partnerID=40&md5=21374bba134c0f741583a75745b5f502","We consider a simple model of cooperation among agents called Coalitional Skill Games (CSGs). This is a restricted form of coalitional games, where each agent has a set of skills that are required to complete various tasks. Each task requires a set of skills in order to be completed, and a coalition can accomplish the task only if the coalition's agents cover the set of required skills for the task. The gain for a coalition depends only on the subset of tasks it can complete. We consider the computational complexity of several problems in CSGs, such as testing if an agent is a dummy or veto agent, computing the core and core-related solution concepts, and computing power indices such as the Shapley value and Banzhaf power index. © 2013 Published by Elsevier B.V.","Coalitional game theory; Core; Power indices","Coalitional game; Coalitional game theory; Computing power; Core; Power indices; Shapley value; Simple modeling; Solution concepts; Artificial intelligence; Computer games",Article,Scopus,2-s2.0-84883396551
"Andrist S., Mutlu B., Gleicher M.","Conversational gaze aversion for virtual agents",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-40415-3_22,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883434789&doi=10.1007%2f978-3-642-40415-3_22&partnerID=40&md5=4a44c249efdf6fa0a4a62b59ab21acb8","In conversation, people avert their gaze from one another to achieve a number of conversational functions, including turn-taking, regulating intimacy, and indicating that cognitive effort is being put into planning an utterance. In this work, we enable virtual agents to effectively use gaze aversions to achieve these same functions in conversations with people. We extend existing social science knowledge of gaze aversion by analyzing video data of human dyadic conversations. This analysis yielded precise timings of speaker and listener gaze aversions, enabling us to design gaze aversion behaviors for virtual agents. We evaluated these behaviors for their ability to achieve positive conversational functions in a laboratory experiment with 24 participants. Results show that virtual agents employing gaze aversion are perceived as thinking, are able to elicit more disclosure from human interlocutors, and are able to regulate conversational turn-taking. © 2013 Springer-Verlag.","conversational behavior; disclosure; Gaze aversion; intimacy; turn-taking; virtual agents","conversational behavior; disclosure; Gaze aversion; intimacy; Turn-taking; Virtual agent; Artificial intelligence; Computer science; Intelligent virtual agents",Conference Paper,Scopus,2-s2.0-84883434789
"Decker N., Leucker M., Thoma D.","jUnitRV - Adding runtime verification to jUnit",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-38088-4_34,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883423004&doi=10.1007%2f978-3-642-38088-4_34&partnerID=40&md5=c88b62d975af05d2785ed27a96aebec8","This paper presents jUnitRV as a tool extending the unit testing framework jUnit by runtime verification capabilities. Roughly, jUnit RV provides a new annotation @Monitors listing monitors that are synthesized from temporal specifications. The monitors check whether the currently executed tests satisfy the correctness properties underlying the monitors. As such, jUnit's concept of plain assert-based verification limited to checking properties of single states of a program is extended significantly towards checking properties of complete execution paths. © 2013 Springer-Verlag.",,"Correctness properties; Execution paths; Run-time verification; Single state; Temporal specification; Unit testing frameworks; Artificial intelligence; Computer science; NASA",Conference Paper,Scopus,2-s2.0-84883423004
"Anvar S.M.H., Yau W.-Y., Teoh E.K.","Multiview face detection and registration requiring minimal manual intervention",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",12,10.1109/TPAMI.2013.37,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883157036&doi=10.1109%2fTPAMI.2013.37&partnerID=40&md5=c10f45e155f322b628746711a1517af5","Most face recognition systems require faces to be detected and localized a priori. In this paper, an approach to simultaneously detect and localize multiple faces having arbitrary views and different scales is proposed. The main contribution of this paper is the introduction of a face constellation, which enables multiview face detection and localization. In contrast to other multiview approaches that require many manually labeled images for training, the proposed face constellation requires only a single reference image of a face containing two manually indicated reference points for initialization. Subsequent training face images from arbitrary views are automatically added to the constellation (registered to the reference image) based on finding the correspondences between distinctive local features. Thus, the key advantage of the proposed scheme is the minimal manual intervention required to train the face constellation. We also propose an approach to identify distinctive correspondence points between pairs of face images in the presence of a large amount of false matches. To detect and localize multiple faces with arbitrary views, we then propose a probabilistic classifier-based formulation to evaluate whether a local feature cluster corresponds to a face. Experimental results conducted on the FERET, CMU, and FDDB datasets show that our proposed approach has better performance compared to the state-of-the-art approaches for detecting faces with arbitrary pose. © 1979-2012 IEEE.","face constellation; image registration; Multiview; simultaneous face detection and localization","Better performance; Detection and localization; face constellation; Face recognition systems; Manual intervention; Multi-view face detection; Multi-views; State-of-the-art approach; Artificial intelligence; Computer vision; Image registration; Face recognition; algorithm; anatomy and histology; artificial intelligence; automated pattern recognition; biometry; computer assisted diagnosis; computer interface; face; image enhancement; photography; procedures; article; automated pattern recognition; biometry; computer assisted diagnosis; face; histology; methodology; photography; Algorithms; Artificial Intelligence; Biometry; Face; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Photography; User-Computer Interface; Algorithms; Artificial Intelligence; Biometry; Face; Image Enhancement; Image Interpretation, Computer-Assisted; Pattern Recognition, Automated; Photography; User-Computer Interface",Article,Scopus,2-s2.0-84883157036
"Olson R.S., Knoester D.B., Adami C.","Critical interplay between density-dependent predation and evolution of the selfish herd",2013,"GECCO 2013 - Proceedings of the 2013 Genetic and Evolutionary Computation Conference",12,10.1145/2463372.2463394,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883106691&doi=10.1145%2f2463372.2463394&partnerID=40&md5=53712719cf46d430841a63c64b1a149a","Animal grouping behaviors have been widely studied due to their implications for understanding social intelligence, collective cognition, and potential applications in engineering, artificial intelligence, and robotics. An important biological aspect of these studies is discerning which selection pressures favor the evolution of grouping behavior. The selfish herd hypothesis states that concentrated groups arise because prey selfishly attempt to place their conspecifics between themselves and the predator, thus causing an endless cycle of movement toward the center of the group. Using an evolutionary model of a predator-prey system, we show that the predator attack mode plays a critical role in the evolution of the selfish herd. Following this discovery, we show that density-dependent predation provides an abstraction of Hamilton's original formulation of ""domains of danger."" Finally, we verify that density-dependent predation provides a sufficient selective advantage for prey to evolve the selfish herd in response to predation by coevolving predators. Thus, our work verifies Hamilton's selfish herd hypothesis in a digital evolutionary model, refines the assumptions of the selfish herd hypothesis, and generalizes the domain of danger concept to density-dependent predation. Copyright © 2013 ACM.","Density-dependent predation; Digital evolutionary model; Group behavior; Predator-prey co-evolution; Selfish herd hypothesis","Co-evolution; Density-dependent; Digital evolutionary models; Group behavior; Selfish herd hypothesis; Artificial intelligence; Predator prey systems; Biology",Conference Paper,Scopus,2-s2.0-84883106691
"Bourguet J.-R., Thomopoulos R., Mugnier M.-L., Abécassis J.","An artificial intelligence-based approach to deal with argumentation applied to food quality in a public health policy",2013,"Expert Systems with Applications",12,10.1016/j.eswa.2013.01.059,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876039511&doi=10.1016%2fj.eswa.2013.01.059&partnerID=40&md5=398aec2f59ca75add83862c00fd5db7b","Argumentation is a relatively new research area in Artificial Intelligence. Since the early 1980s, its use has been investigated in various frameworks. We propose a general model for recommendation-based argumentation by extending Dung's seminal argumentation system. This approach is applied to analyse argumentation on food quality in a public health policy. Cereal products, and more specifically bread, are used by decision makers as a healthy lever to fight against diseases such as obesity or diabetes. Our model outputs new recommendations based on stakeholder's argumentation by targeting some specific audiences. © 2013 Elsevier Ltd. All rights reserved.","Arbitration; Argumentation; Artificial intelligence; Benefit-risk analysis; Case study; Decision support; Knowledge representation; Nutrition; Preferences","Arbitration; Argumentation; Benefit-risk analysis; Decision supports; Preferences; Decision support systems; Disease control; Knowledge representation; Nutrition; Public health; Artificial intelligence",Article,Scopus,2-s2.0-84876039511
"Buer T., Homberger J., Gehring H.","A collaborative ant colony metaheuristic for distributed multi-level uncapacitated lot-sizing",2013,"International Journal of Production Research",12,10.1080/00207543.2013.802822,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884903361&doi=10.1080%2f00207543.2013.802822&partnerID=40&md5=4054344459557dec9d684ae5ce4e69be","The paper presents an ant colony optimization metaheuristic for collaborative planning. Collaborative planning is used to coordinate individual plans of self-interested decision-makers with private information in order to increase the overall benefit of the coalition. The method consists of a new search graph based on encoded solutions. Distributed and private information are integrated via voting mechanisms and via a simple but effective collaborative local search procedure. The approach is applied to a distributed variant of the multi-level lot-sizing problem and evaluated by means of 352 benchmark instances from the literature. The proposed approach clearly outperforms existing approaches on the sets of medium-and large-sized instances. While the best method in the literature so far achieves an average deviation from the best-known non-distributed solutions of 75% for the set of the largest instances, for example, the presented approach reduces the average deviation to 7%. © 2013 Taylor and Francis.","ant colony optimization; collaborative planning; group decisions and negotiations; lot-sizing; metaheuristics","Average deviation; Collaborative planning; Group decisions and negotiations; Lot sizing; Lot sizing problems; Meta heuristics; Private information; Voting mechanism; Algorithms; Artificial intelligence; Benchmarking; Ant colony optimization",Article,Scopus,2-s2.0-84884903361
"Bleik S., Mishra M., Huan J., Song M.","Text categorization of biomedical data sets using graph kernels and a controlled Vocabulary",2013,"IEEE/ACM Transactions on Computational Biology and Bioinformatics",12,10.1109/TCBB.2013.16,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894553853&doi=10.1109%2fTCBB.2013.16&partnerID=40&md5=b88c51b75f07c3b6c654c82ba221042a","Recently, graph representations of text have been showing improved performance over conventional bag-of-words representations in text categorization applications. In this paper, we present a graph-based representation for biomedical articles and use graph kernels to classify those articles into high-level categories. In our representation, common biomedical concepts and semantic relationships are identified with the help of an existing ontology and are used to build a rich graph structure that provides a consistent feature set and preserves additional semantic information that could improve a classifier's performance. We attempt to classify the graphs using both a set-based graph kernel that is capable of dealing with the disconnected nature of the graphs and a simple linear kernel. Finally, we report the results comparing the classification performance of the kernel classifiers to common text-based classifiers. © 2004-2012 IEEE.","biomedical ontologies; classifier design and evaluation; graph kernels; graph representations; mining methods and algorithms; modeling structured; Text categorization; text mining; textual and multimedia data","algorithm; article; artificial intelligence; controlled vocabulary; information retrieval; methodology; natural language processing; nomenclature; publication; semantics; Algorithms; Artificial Intelligence; Information Storage and Retrieval; Manuscripts as Topic; Natural Language Processing; Semantics; Terminology as Topic; Vocabulary, Controlled",Article,Scopus,2-s2.0-84894553853
"Cottone P., Lo Re G., Maida G., Morana M.","Motion sensors for activity recognition in an ambient-intelligence scenario",2013,"2013 IEEE International Conference on Pervasive Computing and Communications Workshops, PerCom Workshops 2013",12,10.1109/PerComW.2013.6529573,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881491326&doi=10.1109%2fPerComW.2013.6529573&partnerID=40&md5=8a3a55b32afe9c14ee8e84ceb4bca4c9","In recent years, Ambient Intelligence (AmI) has attracted a number of researchers due to the widespread diffusion of unobtrusive sensing devices. The availability of such a great amount of acquired data has driven the interest of the scientific community in producing novel methods for combining raw measurements in order to understand what is happening in the monitored scenario. Moreover, due the primary role of the end user, an additional requirement of any AmI system is to maintain a high level of pervasiveness. In this paper we propose a method for recognizing human activities by means of a time of flight (ToF) depth and RGB camera device, namely Microsoft Kinect. The proposed approach is based on the estimation of some relevant joints of the human body by using Kinect depth information. The most significative configurations of joints positions are combined by a clustering approach and classified by means of a multi-class Support Vector Machine. Then, Hidden Markov Models (HMMs) are applied to model each activity as a sequence of known postures. The proposed solution has been tested on a public dataset while considering four different configurations corresponding to some state-of-the-art approaches and results are very promising. Moreover, in order to maintain a high level of pervasiveness, we implemented a real prototype by connecting Kinect sensor to a miniature computer capable of real-time processing. © 2013 IEEE.","Activity Recognition; Ambient Intelligence","Activity recognition; Ambient intelligence; Clustering approach; Hidden markov models (HMMs); Multi-class support vector machines; Realtime processing; Scientific community; State-of-the-art approach; Artificial intelligence; Hidden Markov models; Minicomputers; Pattern recognition; Ubiquitous computing; Sensors",Conference Paper,Scopus,2-s2.0-84881491326
"Huang V.S.-M., Huang R., Chiang M.","A DDoS mitigation system with multi-stage detection and text-based turing testing in cloud computing",2013,"Proceedings - 27th International Conference on Advanced Information Networking and Applications Workshops, WAINA 2013",12,10.1109/WAINA.2013.94,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881424101&doi=10.1109%2fWAINA.2013.94&partnerID=40&md5=41627adf8ef9a16ed0e270c05173edc0","An important trend in the computer science is towards Cloud Computing and we can see that many cloud services are proposed and developed in the Internet. An important cloud service like the IaaS as AWS EC2 can help many companies to build data centers with high performance computing resources and reduce the cost of maintaining the computing hardware. A data center which provides internet service may suffer from many security risks including Distributed Denial of Service (DDOS) attack. We believe that most of the cloud services, like Gmail, Drop box, Google Document, and etc., are based on HTTP connection. Hence, we aim at HTTP-based connection and propose a low reflection ratio mitigation system against the DDoS attacks. Our system is in the front of an IaaS that all of the virtual data centers in the IaaS are our protection targets. Our system consists of Source Checking, Counting, Attack Detection, Turing Test, and Question Generation modules. We provide a multi-stage detection to more precisely detect the possible attackers and a text-based turing test with question generation module to challenge the suspected requesters who are detected by the detection module. We implemented the proposed system and evaluated the performance to show that our system works efficiently to mitigate the DDoS traffic from the Internet. © 2013 IEEE.","CAPTCHA; Cloud Computing; DDoS; Multi-Stage Detection; Text-based Question; Turing Testing","CAPTCHAs; DDoS; Distributed denial of service attack; High-performance computing resources; Mitigation systems; Multi-stage; Text-based Question; Virtual data centers; Artificial intelligence; Cloud computing; Distributed database systems; HTTP; Internet; Network security; Web services; Infrastructure as a service (IaaS)",Conference Paper,Scopus,2-s2.0-84881424101
"Dong C., Chen L., Camenisch J., Russello G.","Fair private set intersection with a semi-trusted arbiter",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-39256-6_9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881161658&doi=10.1007%2f978-3-642-39256-6_9&partnerID=40&md5=96a705b9e2c8a3ee217391c78984d00b","A private set intersection (PSI) protocol allows two parties to compute the intersection of their input sets privately. Most of the previous PSI protocols only output the result to one party and the other party gets nothing from running the protocols. However, a mutual PSI protocol in which both parties can get the output is highly desirable in many applications. A major obstacle in designing a mutual PSI protocol is how to ensure fairness. In this paper we present the first fair mutual PSI protocol which is efficient and secure. Fairness of the protocol is obtained in an optimistic fashion, i.e. by using an offline third party arbiter. In contrast to many optimistic protocols which require a fully trusted arbiter, in our protocol the arbiter is only required to be semi-trusted, in the sense that we consider it to be a potential threat to both parties' privacy but believe it will follow the protocol. The arbiter can resolve disputes without knowing any private information belongs to the two parties. This feature is appealing for a PSI protocol in which privacy may be of ultimate importance. © 2013 IFIP International Federation for Information Processing.",,"Input set; Offline; Optimistic protocols; Potential threats; Private information; Set intersection; Third parties; Artificial intelligence; Computer science; Asynchronous sequential logic",Conference Paper,Scopus,2-s2.0-84881161658
"Crawford B., Soto R., Monfroy E., Castro C., Palma W., Paredes F.","A hybrid soft computing approach for subset problems",2013,"Mathematical Problems in Engineering",12,10.1155/2013/716069,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880829097&doi=10.1155%2f2013%2f716069&partnerID=40&md5=27e548a84235d093dcb5b716cc802214","Subset problems (set partitioning, packing, and covering) are formal models for many practical optimization problems. A set partitioning problem determines how the items in one set (S) can be partitioned into smaller subsets. All items in S must be contained in one and only one partition. Related problems are set packing (all items must be contained in zero or one partitions) and set covering (all items must be contained in at least one partition). Here, we present a hybrid solver based on ant colony optimization (ACO) combined with arc consistency for solving this kind of problems. ACO is a swarm intelligence metaheuristic inspired on ants behavior when they search for food. It allows to solve complex combinatorial problems for which traditional mathematical techniques may fail. By other side, in constraint programming, the solving process of Constraint Satisfaction Problems can dramatically reduce the search space by means of arc consistency enforcing constraint consistencies either prior to or during search. Our hybrid approach was tested with set covering and set partitioning dataset benchmarks. It was observed that the performance of ACO had been improved embedding this filtering technique in its constructive phase. © 2013 Broderick Crawford et al.",,"Ant Colony Optimization (ACO); Complex combinatorial problem; Constraint consistency; Constraint programming; Filtering technique; Optimization problems; Set partitioning problem; Soft computing approaches; Ant colony optimization; Computer programming; Constraint theory; Soft computing; Artificial intelligence",Article,Scopus,2-s2.0-84880829097
"Yadav K., Sarioglu E., Smith M., Choi H.-A.","Automated outcome classification of emergency department computed tomography imaging reports",2013,"Academic Emergency Medicine",12,10.1111/acem.12174,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882294932&doi=10.1111%2facem.12174&partnerID=40&md5=74542fb2d002e3b1a7fffada515574e4","Background Reliably abstracting outcomes from free-text electronic health records remains a challenge. While automated classification of free text has been a popular medical informatics topic, performance validation using real-world clinical data has been limited. The two main approaches are linguistic (natural language processing [NLP]) and statistical (machine learning). The authors have developed a hybrid system for abstracting computed tomography (CT) reports for specified outcomes. Objectives The objective was to measure performance of a hybrid NLP and machine learning system for automated outcome classification of emergency department (ED) CT imaging reports. The hypothesis was that such a system is comparable to medical personnel doing the data abstraction. Methods A secondary analysis was performed on a prior diagnostic imaging study on 3,710 blunt facial trauma victims. Staff radiologists dictated CT reports as free text, which were then deidentified. A trained data abstractor manually coded the reference standard outcome of acute orbital fracture, with a random subset double-coded for reliability. The data set was randomly split evenly into training and testing sets. Training patient reports were used as input to the Medical Language Extraction and Encoding (MedLEE) NLP tool to create structured output containing standardized medical terms and modifiers for certainty and temporal status. Findings were filtered for low certainty and past/future modifiers and then combined with the manual reference standard to generate decision tree classifiers using data mining tools Waikato Environment for Knowledge Analysis (WEKA) 3.7.5 and Salford Predictive Miner 6.6. Performance of decision tree classifiers was evaluated on the testing set with or without NLP processing. Results The performance of machine learning alone was comparable to prior NLP studies (sensitivity = 0.92, specificity = 0.93, precision = 0.95, recall = 0.93, f-score = 0.94), and the combined use of NLP and machine learning showed further improvement (sensitivity = 0.93, specificity = 0.97, precision = 0.97, recall = 0.96, f-score = 0.97). This performance is similar to, or better than, that of medical personnel in previous studies. Conclusions A hybrid NLP and machine learning automated classification system shows promise in coding free-text electronic clinical data. © 2013 by the Society for Academic Emergency Medicine.",,"computer assisted tomography; conference paper; decision tree; emergency care; emergency ward; human; information processing; machine learning; medical personnel; natural language processing; orbit fracture; predictive value; priority journal; sensitivity and specificity; Artificial Intelligence; Cohort Studies; Electronic Health Records; Emergency Medical Services; Emergency Service, Hospital; Humans; Natural Language Processing; Retrospective Studies; Sensitivity and Specificity; Tomography, X-Ray Computed",Conference Paper,Scopus,2-s2.0-84882294932
"El-Shafie A., Alsulami H.M., Jahanbani H., Najah A.","Multi-lead ahead prediction model of reference evapotranspiration utilizing ANN with ensemble procedure",2013,"Stochastic Environmental Research and Risk Assessment",12,10.1007/s00477-012-0678-6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880082142&doi=10.1007%2fs00477-012-0678-6&partnerID=40&md5=3500e1f09a09827c03aee07a5124573f","Obtaining an accurate estimate of the reference evapotranspiration (ETo) can be difficult, especially when there is insufficient data to utilize the Penman-Monteith method. Artificial intelligence-based methods may provide reliable prediction models for several applications in engineering. However, time-series prediction based on artificial neural network (ANN) learning algorithms is fundamentally problematic. For example, the ANN model can experience over-fitting during training and, in consequence, lose its generalization. In this research, several over-fitting procedures have been augmented with the classical ANN model, are proposed. This model was applied to the prediction of the daily ETo at Rasht city, located in the north part of Iran, by using the minimum and maximum daily temperature of the region collected from 1975-1988. In addition, three different scenarios have been developed in order to achieve better prediction accuracy. The results showed that the proposed ENN model successfully predicted the daily ETo with a significant level of accuracy using only the maximum and minimum temperatures. The model also outperformed the classical ANN method. In addition, the proposed ENN compared with Hargreaves and Samani (Appl Eng Agric 1:96-99, 1985) (HGS) model and showed the ENN provides more accurate prediction for ETo. Furthermore, the proposed model could provide relatively good level of accuracy when examined for multi-lead predictions, which could not be afford by HGS model. © 2012 Springer-Verlag Berlin Heidelberg.","Ensemble neural network; Evapotranspiration; Neural network; Over-fitting; Rasht City (Iran)","Accurate prediction; Ensemble neural network; Maximum and minimum temperatures; Overfitting; Penman-Monteith method; Rasht City (Iran); Reference evapotranspiration; Time series prediction; Forecasting; Mathematical models; Neural networks; Water supply; Evapotranspiration; artificial intelligence; artificial neural network; ensemble forecasting; evapotranspiration; numerical model; prediction; Gilan; Iran; Rasht",Article,Scopus,2-s2.0-84880082142
"Rabiej M.","Application of immune and genetic algorithms to the identification of a polymer based on its X-ray diffraction curve",2013,"Journal of Applied Crystallography",12,10.1107/S0021889813015987,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880559806&doi=10.1107%2fS0021889813015987&partnerID=40&md5=f6c118b861f61e915b3fbb5c227f906f","This paper describes how a combination of two methods of artificial intelligence, an immune algorithm and a genetic algorithm, can be used to recognize a polymer by the shape of its X-ray diffraction curve. To this end, the hybrid algorithm uses a database which contains theoretical functions describing wide-angle X-ray diffraction curves of different polymers. These curves are compared by the algorithm with the experimental diffraction curve and the most similar are chosen. Such theoretical curves are kept in the immunological memory, and their parameters can be set as the starting ones in the optimization methods used for decomposition of the experimental curve into crystalline peaks and amorphous component. Using this algorithm, the preparation of the starting parameters is much easier and faster. Decomposition is the most important step in polymer crystallinity determination. © 2013 International Union of Crystallography Printed in Singapore - all rights reserved.","deconvolution; diffraction curves; genetic algorithms; immune algorithms; starting parameters","Amorphous component; Crystalline peaks; Experimental curves; Immune algorithms; Immunological memory; Optimization method; Polymer crystallinity; Wide-angle x-ray diffraction; Amorphous materials; Artificial intelligence; Deconvolution; Genetic algorithms; Polymers; X ray diffraction; Parameter estimation",Article,Scopus,2-s2.0-84880559806
"Prade H., Richard G.","Analogical proportions and multiple-valued logics",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-39091-3-42,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880720938&doi=10.1007%2f978-3-642-39091-3-42&partnerID=40&md5=de5664501b70350a141ea453ddd644ed","Recently, a propositional logic modeling of analogical proportions, i.e., statements of the form ""A is to B as C is to D"", has been proposed, and has then led to introduce new related proportions in a general setting. This framework is well-suited for analogical reasoning and classification tasks about situations described by means of Boolean properties. There is a clear need for extending this approach to deal with the cases where i) properties are gradual; ii) properties may not apply to some situations; iii) the truth status of a property is unknown. The paper investigates the appropriate extension in each of these three cases. © 2013 Springer-Verlag Berlin Heidelberg.","analogical proportion; multiple-valued logic; three-valued logics","Analogical proportions; Analogical reasoning; Classification tasks; Multiple valued logic; Propositional logic; Three-valued logic; Artificial intelligence; Computer science; Many valued logics",Conference Paper,Scopus,2-s2.0-84880720938
"Tripathy B.C., Sen M.","On fuzzy I-convergent difference sequence spaces",2013,"Journal of Intelligent and Fuzzy Systems",12,10.3233/IFS-120671,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880517513&doi=10.3233%2fIFS-120671&partnerID=40&md5=f95c6f87b99cfc3b1f8e9562cceef27d","In this article we introduce the notions of I-convergent and I-bounded difference sequences of fuzzy real numbers. We study different properties of I-convergent, I-null and I-bounded difference sequence spaces of fuzzy real numbers like completeness, solidness, monotone, symmetricity, sequence algebra, convergence free, nowhere denseness. We prove some inclusion results also. © 2013 - IOS Press and the authors. All rights reserved.","difference sequence; Fuzzy real numbers; I-convergence; solid space; symmetric space","Difference sequences; Fuzzy real numbers; I-convergence; Symmetric spaces; Artificial intelligence; Engineering",Article,Scopus,2-s2.0-84880517513
"Azimi R., Esmaeili S.","Multiobjective daily Volt/VAr control in distribution systems with distributed generation using binary ant colony optimization",2013,"Turkish Journal of Electrical Engineering and Computer Sciences",12,10.3906/elk-1110-16,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880094140&doi=10.3906%2felk-1110-16&partnerID=40&md5=985790e3d147a3fe30483735647644d8","This paper presents a multiobjective daily voltage and reactive power control (Volt/VAr) in radial distribution systems, including distributed generation units. The main purpose is to determine optimum dispatch schedules for onload tap changer (OLTC) settings at substations, substation-switched capacitors, and feeder-switched capacitors based on the day-ahead load forecast. The objectives are selected to minimize the voltage deviation on the secondary bus of the main transformer, total electrical energy losses, the number of OLTCs, and capacitor operation and voltage fluctuations in distribution systems for the next day. Since this model is the weighted sum of individual objective functions, an analytic hierarchy process is adopted to determine the weights. In order to simplify the control actions for OLTC at substations, a time interval-based control strategy is used for decomposition of a daily load forecast into several sequential load levels. A binary ant colony optimization (BACO) method is used to solve the daily voltage and reactive control, which is a nonlinear mixed-integer problem. To illustrate the effectiveness of the proposed method, the Volt/VAr control is performed in IEEE 33-bus and 69-bus distribution systems and its performance is compared with the genetic, hybrid binary genetic, and particle swarm optimization algorithms. The simulation results verify that the BACO algorithm gives better performances than other algorithms. © Tübi̇tak;.","Binary ant colony optimization; Distributed generators; Multiobjective; Reactive power and voltage control","Binary ant colony optimizations; Distributed generation units; Distributed generators; Multi objective; Particle swarm optimization algorithm; Radial distribution systems; Voltage and reactive controls; Voltage and reactive power control; Algorithms; Ant colony optimization; Artificial intelligence; Barium compounds; Constrained optimization; Energy dissipation; Particle swarm optimization (PSO); Reactive power; Distributed power generation",Article,Scopus,2-s2.0-84880094140
"Bull S., Johnson M.D., Alotaibi M., Byrne W., Cierniak G.","Visualising multiple data sources in an independent open learner model",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-39112-5-21,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880024196&doi=10.1007%2f978-3-642-39112-5-21&partnerID=40&md5=2eb2f791b4479f013b7252dece354169","This paper introduces the Next-TELL independent open learner model which is constructed based on data from a range of sources. An example is presented for a university course, with the learner model built from the main activities undertaken during the course. Use of the Next-TELL open learner model over a five week period is described for this group of students, suggesting that independent open learner models built from multiple sources of data may have much to offer in supporting students' understanding of their learning, and could potentially be used to encourage greater peer interaction. © 2013 Springer-Verlag Berlin Heidelberg.","Multiple data sources; Open learner model; Visualisation","Learner model; Multiple data sources; Multiple source; Open learner models; Peer interactions; University course; Computer science; Visualization; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84880024196
"Shi L., Gkotsis G., Stepanyan K., Al Qudah D., Cristea A.I.","Social personalized adaptive e-learning environment: Topolor-implementation and evaluation",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-39112-5-94,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880028552&doi=10.1007%2f978-3-642-39112-5-94&partnerID=40&md5=da5d81965b9d08022a5d8465ee34b66a","This paper presents a quantitative study on the use of Topolor-a prototype that introduces Web 2.0 tools and Facebook-like appearance into an adaptive educational hypermedia system. We present the system design and its evaluation using system usability scale questionnaire and learning behavior data analysis. The results indicate high level of student satisfaction with the learning experience and the diversity of learning activities. © 2013 Springer-Verlag Berlin Heidelberg.","Adaptive educational hypermedia; E-learning system; Evaluation; Learning behavior analysis; Social learning","Adaptive educational hypermedia; E-learning systems; Evaluation; Learning behavior; Social learning; Artificial intelligence; Behavioral research; E-learning; Hypermedia systems; Computer aided instruction",Conference Paper,Scopus,2-s2.0-84880028552
"Bosch N., D'Mello S., Mills C.","What emotions do novices experience during their first computer programming learning session?",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-39112-5-2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880013525&doi=10.1007%2f978-3-642-39112-5-2&partnerID=40&md5=f5c999626fd56d669885eae4397f3118","We conducted a study to track the emotions, their behavioral correlates, and relationship with performance when novice programmers learned the basics of computer programming in the Python language. Twenty-nine participants without prior programming experience completed the study, which consisted of a 25 minute scaffolding phase (with explanations and hints) and a 15 minute fadeout phase (no explanations or hints) with a computerized learning environment. Emotional states were tracked via retrospective self-reports in which learners viewed videos of their faces and computer screens recorded during the learning session and made judgments about their emotions at approximately 100 points. The results indicated that flow/engaged (23%), confusion (22%), frustration (14%), and boredom (12%) were the major emotions students experienced, while curiosity, happiness, anxiety, surprise, anger, disgust, fear, and sadness were comparatively rare. The emotions varied as a function of instructional scaffolds and were systematically linked to different student behaviors (idling, constructing code, running code). Boredom, flow/engaged, and confusion were also correlated with performance outcomes. Implications of our findings for affect-sensitive learning interventions are discussed. © 2013 Springer-Verlag Berlin Heidelberg.",,"Constructing codes; Learning environments; Learning sessions; Novice programmer; Performance outcome; Programming experience; Programming learning; Student behavior; Artificial intelligence; Computer aided instruction; Scaffolds; Computer programming",Conference Paper,Scopus,2-s2.0-84880013525
"Santos O.C., Salmeron-Majadas S., Boticario J.G.","Emotions detection from math exercises by combining several data sources",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-39112-5-102,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880001083&doi=10.1007%2f978-3-642-39112-5-102&partnerID=40&md5=dd2ea5007f75c5faf7357d1c72c1798a","Emotions detection and their management are key issues to provide personalize support in educational scenarios. Literature suggests that combining several input sources can improve the performance of affect recognition. To gain a better understanding of this issue, we carried out a large scale experiment in our laboratory where about 100 participants performed several mathematical exercises while emotional information was gathered from different input sources, including a written emotional report. As a first step, we have explored emotions detection from traditional methods by combining analysis of user behavior when typing this report with sentiment analysis on the text. Moreover, an expert labeled these reports. All these data were used to feed several machine learning algorithms to infer user's emotions. Preliminary results are not conclusive, but lead some light on how to proceed with the analysis. © 2013 Springer-Verlag Berlin Heidelberg.","Emotions; Machine learning; Mathematics; Sentiment analysis","Affect recognition; Data-sources; Emotional information; Emotions; Input sources; Large scale experiments; Sentiment analysis; User behaviors; Artificial intelligence; Data mining; Learning algorithms; Learning systems; Mathematical techniques; Behavioral research",Conference Paper,Scopus,2-s2.0-84880001083
"Xie L., Yang G., Zeng J., Cui Z.","Swarm robots search based on artificial physics optimisation algorithm",2013,"International Journal of Computing Science and Mathematics",12,10.1504/IJCSM.2013.054684,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879581091&doi=10.1504%2fIJCSM.2013.054684&partnerID=40&md5=bcf25d5197b1cf017ced4144f1d8c685","Swarm robotic is an important research area of swarm intelligence. Swarm robots searching potential target provides a better way in victim search or rescue operations in some disaster scenarios. This paper uses artificial physics optimisation (APO) algorithm as the modelling tool for solving swarm robots searching problem. Firstly, viewed as an individual moving in a closed two-dimensional workspace, each robot is abstracted as one order inertial element. Further, the model of swarm robots search for target with global sense based on APO algorithm is given, in which the virtual force law among robots is constructed through referring the attraction-repulsion rule of APO. Simulation results under an ideal environment show that APO is a feasible and effective when applied to swarm robotic search for target with global sense. Copyright © 2013 Inderscience Enterprises Ltd.","APO; Artificial physics optimization; Physicomimetics; Swarm robotics; Target search; Virtual force","APO; Artificial physics optimizations; Physicomimetics; Swarm robotics; Target search; Virtual forces; Algorithms; Artificial intelligence; Optimization; Robotics; Data communication equipment",Article,Scopus,2-s2.0-84879581091
"Mora A.M., García-Sánchez P., Merelo J.J., Castillo P.A.","Pareto-based multi-colony multi-objective ant colony optimization algorithms: An island model proposal",2013,"Soft Computing",12,10.1007/s00500-013-0993-y,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878784991&doi=10.1007%2fs00500-013-0993-y&partnerID=40&md5=70ef0c6c1f00a049f16615ab7a71a081","Multi-objective algorithms are aimed to obtain a set of solutions, called Pareto set, covering the whole Pareto front, i.e. the representation of the optimal set of solutions. To this end, the algorithms should yield a wide amount of near-optimal solutions with a good diversity or spread along this front. This work presents a study on different coarse-grained distribution schemes dealing with Multi-Objective Ant Colony Optimization Algorithms (MOACOs). Two of them are a variation of independent multi-colony structures, respectively having a fixed number of ants in every subset or distributing the whole amount of ants into small sub-colonies. We introduce in this paper a third method: an island-based model where the colonies communicate by migrating ants, following a neighbourhood topology which fits to the search space. All the methods are aimed to cover the whole PF, thus each sub-colony or island tries to search for solutions in a limited area, complemented by the rest of colonies, in order to obtain a more diverse high-quality set of solutions. The models have been tested by implementing them considering three different MOACOs: two well-known and CHAC, an algorithm previously proposed by the authors. Three different instances of the bi-Criteria travelling salesman problem have been considered. The experiments have been performed in a parallel environment (inside a cluster platform), in order to get a time improvement. Moreover, the system scaling factor with respect to the number of processors will be also analysed. The results show that the proposed Pareto-island model and its novel neighbourhood topology performs better than the other models, yielding a more diverse and more optimized set of solutions. Moreover, from the algorithmic point of view, the proposed algorithm, named CHAC, yields the best results on average. © 2013 Springer-Verlag Berlin Heidelberg.","Ant colony optimization; Bi-criteria TSP; Coarse-grained; Distributed algorithms; Island model; MOACO; Multi-colony; Multi-objective; Pareto-based topology","Bi-criteria; Coarse-grained; Island model; MOACO; Multi objective; Multi-colony; Ant colony optimization; Artificial intelligence; Constrained optimization; Parallel algorithms; Pareto principle; Topology; Traveling salesman problem; Algorithms",Article,Scopus,2-s2.0-84878784991
"Hirel J., Gaussier P., Quoy M., Banquet J.P., Save E., Poucet B.","The hippocampo-cortical loop: Spatio-temporal learning and goal-oriented planning in navigation",2013,"Neural Networks",12,10.1016/j.neunet.2013.01.023,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875272738&doi=10.1016%2fj.neunet.2013.01.023&partnerID=40&md5=e2dc36a43afea6a08d736c1a14b29019","We present a neural network model where the spatial and temporal components of a task are merged and learned in the hippocampus as chains of associations between sensory events. The prefrontal cortex integrates this information to build a cognitive map representing the environment. The cognitive map can be used after latent learning to select optimal actions to fulfill the goals of the animal. A simulation of the architecture is made and applied to learning and solving tasks that involve both spatial and temporal knowledge. We show how this model can be used to solve the continuous place navigation task, where a rat has to navigate to an unmarked goal and wait for 2 seconds without moving to receive a reward. The results emphasize the role of the hippocampus for both spatial and timing prediction, and the prefrontal cortex in the learning of goals related to the task. © 2013 Elsevier Ltd.","Hippocampus; Navigation; Prefrontal cortex; Timing","Hippocampus; Navigation tasks; Neural network model; Optimal actions; Prefrontal cortex; Spatio-temporal learning; Temporal knowledge; Timing; Artificial intelligence; Cognitive systems; Navigation; article; brain function; cognition; cognitive map; entorhinal cortex; goal oriented planning; hippocampus; learning; multisensory integration; navigation; nerve cell network; nonhuman; prefrontal cortex; priority journal; reward; simulation; spatial learning; task performance; temporal learning; Animals; Goals; Hippocampus; Learning; Models, Neurological; Neural Networks (Computer); Neurons; Prefrontal Cortex; Rats; Reward; Space Perception",Article,Scopus,2-s2.0-84875272738
"Nitta T.","Local minima in hierarchical structures of complex-valued neural networks",2013,"Neural Networks",12,10.1016/j.neunet.2013.02.002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874675045&doi=10.1016%2fj.neunet.2013.02.002&partnerID=40&md5=e10064a9763e4e077885359789f551d8","Most of local minima caused by the hierarchical structure can be resolved by extending the real-valued neural network to complex numbers. It was proved in 2000 that a critical point of the real-valued neural network with H - 1 hidden neurons always gives many critical points of the real-valued neural network with H hidden neurons. These critical points consist of many lines in the parameter space which could be local minima or saddle points. Local minima cause plateaus which have a strong negative influence on learning. However, most of the critical points of complex-valued neural network are saddle points unlike those of the real-valued neural network. This is a prominent property of the complex-valued neural network. © 2013 Elsevier Ltd.","Complex number; Local minimum; Redundancy; Saddle point; Singular point","Complex number; Complex-valued neural networks; Critical points; Hidden neurons; Hierarchical structures; Local minimums; Negative influence; Parameter spaces; Prominent property; Saddle point; Singular points; Artificial intelligence; Cognitive systems; Redundancy; Neural networks; article; complex formation; computer simulation; learning; mathematical computing; mathematical model; nerve cell network; priority journal; structural equation modeling; structure analysis; Algorithms; Learning; Neural Networks (Computer); Neurons; Signal Processing, Computer-Assisted",Article,Scopus,2-s2.0-84874675045
"Huang T., Cai Y.-D.","An Information-Theoretic Machine Learning Approach to Expression QTL Analysis",2013,"PLoS ONE",12,10.1371/journal.pone.0067899,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879386452&doi=10.1371%2fjournal.pone.0067899&partnerID=40&md5=090fe14deb220ee60cfc31ebf078bfde","Expression Quantitative Trait Locus (eQTL) analysis is a powerful tool to study the biological mechanisms linking the genotype with gene expression. Such analyses can identify genomic locations where genotypic variants influence the expression of genes, both in close proximity to the variant (cis-eQTL), and on other chromosomes (trans-eQTL). Many traditional eQTL methods are based on a linear regression model. In this study, we propose a novel method by which to identify eQTL associations with information theory and machine learning approaches. Mutual Information (MI) is used to describe the association between genetic marker and gene expression. MI can detect both linear and non-linear associations. What's more, it can capture the heterogeneity of the population. Advanced feature selection methods, Maximum Relevance Minimum Redundancy (mRMR) and Incremental Feature Selection (IFS), were applied to optimize the selection of the affected genes by the genetic marker. When we applied our method to a study of apoE-deficient mice, it was found that the cis-acting eQTLs are stronger than trans-acting eQTLs but there are more trans-acting eQTLs than cis-acting eQTLs. We compared our results (mRMR.eQTL) with R/qtl, and MatrixEQTL (modelLINEAR and modelANOVA). In female mice, 67.9% of mRMR.eQTL results can be confirmed by at least two other methods while only 14.4% of R/qtl result can be confirmed by at least two other methods. In male mice, 74.1% of mRMR.eQTL results can be confirmed by at least two other methods while only 18.2% of R/qtl result can be confirmed by at least two other methods. Our methods provide a new way to identify the association between genetic markers and gene expression. Our software is available from supporting information. © 2013 Huang, Cai.",,"analytic method; animal experiment; article; calculation; controlled study; female; gene expression; gene frequency; genetic marker; genotype; heterozygosity; homozygosity; incremental feature selection; information processing; information retrieval; intermethod comparison; machine learning; male; Maximum Relevance Minimum Redundancy; mouse; nonhuman; quantitative trait locus; simulation; single nucleotide polymorphism; Animals; Artificial Intelligence; Female; Gene Expression Regulation; Information Theory; Male; Mice; Polymorphism, Single Nucleotide; Quantitative Trait Loci; Mus",Article,Scopus,2-s2.0-84879386452
"Singhaputtangkul N., Low S.P., Teo A.L., Hwang B.-G.","Knowledge-based decision support system quality function deployment (KBDSS-QFD) tool for assessment of building envelopes",2013,"Automation in Construction",12,10.1016/j.autcon.2013.05.017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884503293&doi=10.1016%2fj.autcon.2013.05.017&partnerID=40&md5=f71687f0b68c191e0328f27941613946","A building design team has faced several decision-making problems when assessing building envelope materials and designs for a private high-rise residential building in the early design stage. This study developed an automated fuzzy Knowledge-based Decision Support System Quality Function Deployment (KBDSS-QFD) tool to facilitate the team to mitigate such problems. A case study of the design team comprising an architect, a civil and structural (C&S) engineer and a mechanical and electrical (M&E) engineer was selected as the research design of this study. Results from the qualitative data analysis showed that the tool has the potential to mitigate the decision-making problems. The contributions of using this automated tool include not only achieving better design management but also raising the level of productivity in the construction industry. © 2013 Elsevier B.V.","Building envelopes; Decision support system; Design management; Design team; Knowledge-based system; Quality Function Deployment","Building envelopes; Decision-making problem; Design management; Design team; High rise residential building; Knowledge based decision support systems; Mechanical and electrical; Qualitative data analysis; Artificial intelligence; Construction industry; Decision making; Decision support systems; Design; Knowledge based systems; Quality function deployment; Solar buildings; Tall buildings; Tools; Human resource management",Article,Scopus,2-s2.0-84884503293
"Kotta J., Kutser T., Teeveer K., Vahtmäe E., Pärnoja M.","Predicting Species Cover of Marine Macrophyte and Invertebrate Species Combining Hyperspectral Remote Sensing, Machine Learning and Regression Techniques",2013,"PLoS ONE",12,10.1371/journal.pone.0063946,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878640040&doi=10.1371%2fjournal.pone.0063946&partnerID=40&md5=bba0cedb64f45ad1ffea757277d6b52c","In order to understand biotic patterns and their changes in nature there is an obvious need for high-quality seamless measurements of such patterns. If remote sensing methods have been applied with reasonable success in terrestrial environment, their use in aquatic ecosystems still remained challenging. In the present study we combined hyperspectral remote sensing and boosted regression tree modelling (BTR), an ensemble method for statistical techniques and machine learning, in order to test their applicability in predicting macrophyte and invertebrate species cover in the optically complex seawater of the Baltic Sea. The BRT technique combined with remote sensing and traditional spatial modelling succeeded in identifying, constructing and testing functionality of abiotic environmental predictors on the coverage of benthic macrophyte and invertebrate species. Our models easily predicted a large quantity of macrophyte and invertebrate species cover and recaptured multitude of interactions between environment and biota indicating a strong potential of the method in the modelling of aquatic species in the large variety of ecosystems. © 2013 Kotta et al.",,"sea water; abiotic stress; aquatic species; article; Baltic Sea; benthos; boosted regression tree modelling; environmental factor; environmental monitoring; invertebrate; machine learning; macrophyte; marine environment; nonhuman; prediction; remote sensing; spatial analysis; species cover; statistical analysis; animal; artificial intelligence; ecosystem; Estonia; geography; invertebrate; physiology; principal component analysis; regression analysis; sea; species difference; Invertebrata; Animals; Aquatic Organisms; Artificial Intelligence; Ecosystem; Estonia; Geography; Invertebrates; Oceans and Seas; Principal Component Analysis; Regression Analysis; Remote Sensing Technology; Species Specificity",Article,Scopus,2-s2.0-84878640040
"Kazemi S., Abghari S., Lavesson N., Johnson H., Ryman P.","Open data for anomaly detection in maritime surveillance",2013,"Expert Systems with Applications",12,10.1016/j.eswa.2013.04.029,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878341926&doi=10.1016%2fj.eswa.2013.04.029&partnerID=40&md5=c1fdeabec63543461c7c9195e2c6b974","Maritime surveillance has received increased attention from a civilian perspective in recent years. Anomaly detection is one of many techniques available for improving the safety and security in this domain. Maritime authorities use confidential data sources for monitoring the maritime activities; however, a paradigm shift on the Internet has created new open sources of data. We investigate the potential of using open data as a complementary resource for anomaly detection in maritime surveillance. We present and evaluate a decision support system based on open data and expert rules for this purpose. We conduct a case study in which experts from the Swedish coastguard participate to conduct a real-world validation of the system. We conclude that the exploitation of open data as a complementary resource is feasible since our results indicate improvements in the efficiency and effectiveness of the existing surveillance systems by increasing the accuracy and covering unseen aspects of maritime activities. © 2013 Elsevier Ltd. All rights reserved.","Anomaly detection; Maritime domain awareness; Maritime security; Open data","Anomaly detection; Efficiency and effectiveness; Maritime domain awareness; Maritime security; Maritime surveillance; Open datum; Safety and securities; Surveillance systems; Artificial intelligence; Decision support systems; Monitoring; Network security",Article,Scopus,2-s2.0-84878341926
"Hung C.-Y., Liu P., Lian K.-Y.","Fuzzy virtual reference model sensorless tracking control for linear induction motors",2013,"IEEE Transactions on Cybernetics",12,10.1109/TSMCB.2012.2220347,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890430860&doi=10.1109%2fTSMCB.2012.2220347&partnerID=40&md5=57353dfea44210fcf3fbc53ae963b613","This paper introduces a fuzzy virtual reference model (FVRM) synthesis method for linear induction motor (LIM) speed sensorless tracking control. First, we represent the LIM as a Takagi-Sugeno fuzzy model. Second, we estimate the immeasurable mover speed and secondary flux by a fuzzy observer. Third, to convert the speed tracking control into a stabilization problem, we define the internal desired states for state tracking via an FVRM. Finally, by solving a set of linear matrix inequalities (LMIs), we obtain the observer gains and the control gains where exponential convergence is guaranteed. The contributions of the approach in this paper are threefold: 1) simplified approach - speed tracking problem converted into stabilization problem; 2) omit need of actual reference model - FVRM generates internal desired states; and 3) unification of controller and observer design - control objectives are formulated into an LMI problem where powerful numerical toolboxes solve controller and observer gains. Finally, experiments are carried out to verify the theoretical results and show satisfactory performance both in transient response and robustness. © 2012 IEEE.","Linear induction motors (LIMs); Sensorless control; Takagi-Sugeno (TS) fuzzy model","Control objectives; Exponential convergence; Linear induction motors; Reference modeling; Sensor less control; Speed tracking control; Stabilization problems; Takagi-sugeno fuzzy models; Induction motors; Linear matrix inequalities; Models; Navigation; Stabilization; Controllers; artificial intelligence; automated pattern recognition; computer simulation; feedback system; fuzzy logic; procedures; statistical model; transducer; article; automated pattern recognition; methodology; Artificial Intelligence; Computer Simulation; Feedback; Fuzzy Logic; Linear Models; Pattern Recognition, Automated; Transducers; Artificial Intelligence; Computer Simulation; Feedback; Fuzzy Logic; Linear Models; Pattern Recognition, Automated; Transducers",Article,Scopus,2-s2.0-84890430860
"Jafarzadeh S., Sami Fadali M.","On the stability and control of continuous-time TSK fuzzy systems",2013,"IEEE Transactions on Cybernetics",12,10.1109/TSMCB.2012.2223672,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890331562&doi=10.1109%2fTSMCB.2012.2223672&partnerID=40&md5=b3113e6ca8262b8c3c8a09c37be49d0d","This paper introduces a new stability test and control design methodology for type-1 and type-2 continuous-time (CT) Takagi-Sugeno-Kang systems. Unlike methods based on a common Lyapunov function, our stability results apply for systems with unstable consequents, and our controllers can be designed for systems with unstabilizable consequents. The stability results are derived using the comparison principle with a discontinuous function and the upper right-hand derivative. The control results include CT fuzzy proportional controllers and fuzzy proportional-integral controllers that can be obtained by solving linear matrix inequalities. We provide several examples to demonstrate our stability testing and controller design and compare our results to available methods in the literature. Our results compare favorably with results available in the literature and provide stability tests and controllers where earlier approaches fail. © 2012 IEEE.","Comparison principle; Fuzzy control; Stability; Takagi-Sugeno-Kang (TSK) systems; Type-2 fuzzy systems","Common Lyapunov functions; Comparison principle; Discontinuous functions; Proportional controller; Proportional integral controllers; Stability and control; Takagi-sugeno-kang systems; Type-2 fuzzy systems; Continuous time systems; Convergence of numerical methods; Fuzzy control; Fuzzy systems; Linear matrix inequalities; Lyapunov functions; System stability; algorithm; article; artificial intelligence; automated pattern recognition; feedback system; fuzzy logic; methodology; statistical model; automated pattern recognition; procedures; Algorithms; Artificial Intelligence; Feedback; Fuzzy Logic; Models, Statistical; Pattern Recognition, Automated; Algorithms; Artificial Intelligence; Feedback; Fuzzy Logic; Models, Statistical; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84890331562
"Krüger F., Yordanova K., Hein A., Kirste T.","Plan synthesis for probabilistic activity recognition",2013,"ICAART 2013 - Proceedings of the 5th International Conference on Agents and Artificial Intelligence",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877938418&partnerID=40&md5=3b423a58988d58b9514380cf47a05db9","We analyze the applicability of model-based approaches to the task of inferring activities in smart environments. We introduce a symbolic approach to representing human behavior that enables the use of prior knowledge on the causality of human action and outline its probabilistic semantics. Based on an experimental analysis of a real world scenario from the smart meeting room domain, we show that such a symbolic approach allows to build reusable behavior models that compete with data-driven models at the performance level and that are able to track human behavior across a wide range of scenarios.","Activity recognition; Causal models; Plan synthesis; Probabilistic reasoning","Activity recognition; Causal model; Experimental analysis; Inferring activities; Model based approach; Plan synthesis; Probabilistic reasoning; Probabilistic semantics; Artificial intelligence; Pattern recognition; Semantics; Social sciences; Motion estimation",Conference Paper,Scopus,2-s2.0-84877938418
"Yoo K.-S., Han S.-Y.","A modified ant colony optimization algorithm for dynamic topology optimization",2013,"Computers and Structures",12,10.1016/j.compstruc.2013.04.012,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877847661&doi=10.1016%2fj.compstruc.2013.04.012&partnerID=40&md5=f7e01121532a3f78574a97405d985a81","A modified ant colony optimization (MACO) algorithm implementing a new definition of pheromone and a new cooperation mechanism between ants is presented in this paper. The sensitivity of structural response to the presence of each element included in the finite element (FE) model is evaluated. The study aims to improve the suitability and computational efficiency of the ant colony optimization algorithm in dynamic topology optimization problems. The natural frequencies of the structure must be maximized yet satisfying a constraint on the final volume. Optimization results obtained in three test cases indicate that MACO is more efficient and robust than standard ACO in solving dynamic topology optimization problems. © 2013 Elsevier Ltd. All rights reserved.","Ant colony optimization (ACO); Dynamic topology optimization for natural frequencies; Finite element method; Modified ant colony optimization (MACO)","Ant Colony Optimization (ACO); Ant Colony Optimization algorithms; Cooperation mechanism; Dynamic topologies; Finite element models; Modified ant colony optimization (MACO); Optimization problems; Structural response; Ant colony optimization; Artificial intelligence; Finite element method; Natural frequencies; Algorithms",Article,Scopus,2-s2.0-84877847661
"Chen Z., Zhang W.","Integrative Analysis Using Module-Guided Random Forests Reveals Correlated Genetic Factors Related to Mouse Weight",2013,"PLoS Computational Biology",12,10.1371/journal.pcbi.1002956,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875987456&doi=10.1371%2fjournal.pcbi.1002956&partnerID=40&md5=7c89cedf9d7e1cd2a2cca7a8ffe21b66","Complex traits such as obesity are manifestations of intricate interactions of multiple genetic factors. However, such relationships are difficult to identify. Thanks to the recent advance in high-throughput technology, a large amount of data has been collected for various complex traits, including obesity. These data often measure different biological aspects of the traits of interest, including genotypic variations at the DNA level and gene expression alterations at the RNA level. Integration of such heterogeneous data provides promising opportunities to understand the genetic components and possibly genetic architecture of complex traits. In this paper, we propose a machine learning based method, module-guided Random Forests (mgRF), to integrate genotypic and gene expression data to investigate genetic factors and molecular mechanism underlying complex traits. mgRF is an augmented Random Forests method enhanced by a network analysis for identifying multiple correlated variables of different types. We applied mgRF to genetic markers and gene expression data from a cohort of F2 female mouse intercross. mgRF outperformed several existing methods in our extensive comparison. Our new approach has an improved performance when combining both genotypic and gene expression data compared to using either one of the two types of data alone. The resulting predictive variables identified by mgRF provide information of perturbed pathways that are related to body weight. More importantly, the results uncovered intricate interactions among genetic markers and genes that have been overlooked if only one type of data was examined. Our results shed light on genetic mechanisms of obesity and our approach provides a promising complementary framework to the ""genetics of gene expression"" analysis for integrating genotypic and gene expression information for analyzing complex traits. © 2013 Chen, Zhang.",,"article; body weight; controlled study; female; gene expression; genetic analysis; genetic association; genetic marker; genetic trait; genotype; intermethod comparison; module guided random forest; mouse; nonhuman; obesity; quantitative trait locus mapping; random forest; Algorithms; Animals; Artificial Intelligence; Body Weight; Computational Biology; Decision Trees; Female; Mice; Models, Genetic; Obesity",Article,Scopus,2-s2.0-84875987456
"Rubio J.C., Serrat J., López A.","Video co-segmentation",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-37444-9_2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875897672&doi=10.1007%2f978-3-642-37444-9_2&partnerID=40&md5=223c51a4fcc0b9c4565c4a731e86bc65","Segmentation of a single image is in general a highly underconstrained problem. A frequent approach to solve it is to somehow provide prior knowledge or constraints on how the objects of interest look like (in terms of their shape, size, color, location or structure). Image co-segmentation trades the need for such knowledge for something much easier to obtain, namely, additional images showing the object from other viewpoints. Now the segmentation problem is posed as one of differentiating the similar object regions in all the images from the more varying background. In this paper, for the first time, we extend this approach to video segmentation: given two or more video sequences showing the same object (or objects belonging to the same class) moving in a similar manner, we aim to outline its region in all the frames. In addition, the method works in an unsupervised manner, by learning to segment at testing time. We compare favorably with two state-of-the-art methods on video segmentation and report results on benchmark videos. © 2013 Springer-Verlag.",,"Co segmentations; Prior knowledge; Single images; State-of-the-art methods; Under-constrained; Varying background; Video segmentation; Video sequences; Artificial intelligence; Image segmentation",Conference Paper,Scopus,2-s2.0-84875897672
"Sidorov G., Velasquez F., Stamatatos E., Gelbukh A., Chanona-Hernández L.","Syntactic dependency-based n-grams as classification features",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-37798-3_1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875865567&doi=10.1007%2f978-3-642-37798-3_1&partnerID=40&md5=cf60a5c215ea0a989a1996197e003db1","In this paper we introduce a concept of syntactic n-grams (sn-grams). Sn-grams differ from traditional n-grams in the manner of what elements are considered neighbors. In case of sn-grams, the neighbors are taken by following syntactic relations in syntactic trees, and not by taking the words as they appear in the text. Dependency trees fit directly into this idea, while in case of constituency trees some simple additional steps should be made. Sn-grams can be applied in any NLP task where traditional n-grams are used. We describe how sn-grams were applied to authorship attribution. SVM classifier for several profile sizes was used. We used as baseline traditional n-grams of words, POS tags and characters. Obtained results are better when applying sn-grams. © 2013 Springer-Verlag.","authorship attribution; classification features; parsing; sn-grams; syntactic n-grams; syntactic paths","Authorship attribution; Classification features; N-grams; parsing; sn-grams; Artificial intelligence; Forestry; Syntactics; Tin; Classification (of information); Classification; Forestry; Information Retrieval; Tin",Conference Paper,Scopus,2-s2.0-84875865567
"Reading B.J., Williams V.N., Chapman R.W., Williams T.I., Sullivan C.V.","Dynamics of the striped bass (Morone saxatilis) ovary proteome reveal a complex network of the translasome",2013,"Journal of Proteome Research",12,10.1021/pr3010293,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875926652&doi=10.1021%2fpr3010293&partnerID=40&md5=c0cd6c1a210c292c0b682517948aea7f","We evaluated changes in the striped bass (Morone saxatilis) ovary proteome during the annual reproductive cycle using label-free quantitative mass spectrometry and a novel machine learning analysis based on K-means clustering and support vector machines. Modulated modularity clustering was used to group co-variable proteins into expression modules and Gene Ontology (GO) biological process and KEGG pathway enrichment analyses were conducted for proteins within those modules. We discovered that components of the ribosome along with translation initiation and elongation factors generally decrease as the annual ovarian cycle progresses toward ovulation, concomitant with a slight increase in components of the 26S-proteasome. Co-variation within more than one expression module of components from these two multi-protein complexes suggests that they are not only co-regulated, but that co-regulation occurs through more than one sub-network. These components also co-vary with subunits of the TCP-1 chaperonin system and enzymes of intermediary metabolic pathways, suggesting that protein folding and cellular bioenergetic state play important roles in protein synthesis and degradation. We provide further evidence to suggest that protein synthesis and degradation are intimately linked, and our results support function of a proteasome-ribosome supercomplex known as the translasome. © 2013 American Chemical Society.","fish; mass-spectrometry; ovary; proteasome; reproduction; ribosome; support vector machines; teleost; translasome","chaperonin containing TCP1; article; bioenergy; biological phenomena and functions concerning the entire organism; complex formation; female; gluconeogenesis; glycolysis; machine learning; mass spectrometry; molecular dynamics; nonhuman; ovary; ovary cycle; priority journal; protein degradation; protein expression; protein folding; protein synthesis; quantitative analysis; ribosome; striped bass; support vector machine; transcription elongation; translation initiation; Animals; Artificial Intelligence; Bass; Cluster Analysis; Female; Fish Proteins; Gene Ontology; Mass Spectrometry; Menstrual Cycle; Ovary; Proteasome Endopeptidase Complex; Proteome; Ribosomes; Morone; Morone saxatilis; Teleostei",Article,Scopus,2-s2.0-84875926652
"Llamazares B.","An analysis of some functions that generalizes weighted means and OWA operators",2013,"International Journal of Intelligent Systems",12,10.1002/int.21581,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874115161&doi=10.1002%2fint.21581&partnerID=40&md5=da3f02ab5582b0b023c90d0689cbbbba","In this paper, we analyze several classes of functions proposed in the literature to simultaneously generalize weighted means and ordered weighted averaging (OWA) operators: weighted OWA (WOWA) operators, hybrid weighted averaging (HWA) operators, and ordered weighted averaging-weighted average (OWAWA) operators. Since, in some cases, the results provided by these operators may be questionable, we introduce functions that also generalize both operators and characterize those satisfying a condition imposed to maintain the relationship among the weights. © 2013 Wiley Periodicals, Inc.",,"Ordered weighted averaging operator; OWA operators; Weighted averaging; Weighted mean; Artificial intelligence; Software engineering; Statistical methods",Article,Scopus,2-s2.0-84874115161
"Renna P.","Decision model to support the SMEs decision to participate or leave a collaborative network",2013,"International Journal of Production Research",12,10.1080/00207543.2012.701773,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873445461&doi=10.1080%2f00207543.2012.701773&partnerID=40&md5=baae00e3b3f12a45e095db2eec5c46e6","Enterprises need to adopt new business paradigms in order to make a rapid response to market changes and improve the competitiveness. The development of information and communication technology allows the support of new business paradigms such as extended enterprises, virtual organisations, regional clusters, etc. The research proposed concerns the study of a dynamic network in which the partners change in short term periods. The model proposed supports the enterprises in the decisions to participate or exit in a network of enterprises. The model is based on the definition of a set of rules that operate with local information to take the decisions. The local information is the output of the collaboration process; this means that the approach proposed integrates the collaboration methodology and the decision model. This environment is related to independent plants that cooperate with reduced information sharing. A simulation environment is developed to test the approach proposed. The simulation results show that the proposed approach is a very promising tool to support the enterprise's participation decisions. © 2013 Taylor & Francis Group, LLC.","decision support system; dynamic network; information sharing; network enterprises; simulation environment","Collaboration process; Collaborative network; Decision models; Dynamic network; Extended enterprise; Information and Communication Technologies; Information sharing; Local information; Market changes; Rapid response; Regional clusters; Set of rules; Short term; Simulation environment; Virtual organisations; Artificial intelligence; Decision support systems; Industry; Information analysis; Information technology; Models; Plants (botany); Competition",Article,Scopus,2-s2.0-84873445461
"Jamalipour M., Gharib M., Sayareh R., Khoshahval F.","PWR power distribution flattening using Quantum Particle Swarm intelligence",2013,"Annals of Nuclear Energy",12,10.1016/j.anucene.2013.01.026,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874591995&doi=10.1016%2fj.anucene.2013.01.026&partnerID=40&md5=b0a3c96087e94d7256596cd45db7da0e","In-core fuel management optimization (ICFMO) is one of the most challenging concepts of nuclear engineering. Most of the strategies implemented for optimizing fuel loading pattern in nuclear power reactors are based on maximizing core multiplication factor in order to extract maximum energy and reducing power peaking factor from a predetermined value to maintain fuel integrity. In this investigation a new method using Quantum Particle Swarm Optimization (QPSO) algorithm has been developed in order to flatten power density distribution in WWER-1000 Bushehr Nuclear Power Plant (BNPP) and thereby provide a better safety margin. The result and convergence of this method show that QPSO performs very well and is comparable to PSO. Furthermore, an operator has been added to QPSO as a mutation operator. This algorithm, called QPSO-DM, shows a better performance on ICFMO than PSO and QPSO. MATLAB software was used to map PSO, QPSO and QPSO-DM for loading pattern optimization. Multi-group constants generated by WIMS for different fuel configurations were fed into CITATION to obtain the power density distribution. © 2013 Elsevier Ltd. All rights reserved.","In-core fuel management optimization; Loading pattern; Mutation; Particle Swarm Optimization; Quantum Particle Swarm Optimization; WWER","In-Core fuel Management optimizations; Loading patterns; Mutation; Quantum particle swarm optimization; WWER; Algorithms; Artificial intelligence; Fuels; Nuclear engineering; Nuclear power plants; Pressurized water reactors; Particle swarm optimization (PSO)",Article,Scopus,2-s2.0-84874591995
"Marozzo F., Talia D., Trunfio P.","Using clouds for scalable knowledge discovery applications",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-36949-0_25,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874408577&doi=10.1007%2f978-3-642-36949-0_25&partnerID=40&md5=6588a2b76376e5331113c43e98193f11","Cloud platforms provide scalable processing and data storage and access services that can be exploited for implementing high-performance knowledge discovery systems and applications. This paper discusses the use of Clouds for the development of scalable distributed knowledge discovery applications. Service-oriented knowledge discovery concepts are introduced, and a framework for supporting high-performance data mining applications on Clouds is presented. The system architecture, its implementation, and current work aimed at supporting the design and execution of knowledge discovery applications modeled as workflows are described. © 2013 Springer-Verlag.",,"Cloud platforms; Data mining applications; Data storage; Distributed knowledge discovery; Knowledge discovery systems; Knowledge-discovery application; Service Oriented; System architectures; Work-flows; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84874408577
"Klammler G., Kupfersberger H., Rock G., Fank J.","Modeling coupled unsaturated and saturated nitrate distribution of the aquifer Westliches Leibnitzer Feld, Austria",2013,"Environmental Earth Sciences",12,10.1007/s12665-013-2302-6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876893043&doi=10.1007%2fs12665-013-2302-6&partnerID=40&md5=71b2ed9bd2f3ca957aece48d06761e85","The aquifer Westliches Leibnitzer Feld, Austria, is a significant resource for regional and supraregional drinking water supply for more than 100,000 inhabitants, but the region also provides excellent agricultural conditions. This dual use implicates conflicts (e. g., non-point source groundwater pollution by nitrogen leaching), which have to be harmonized for a sustainable coexistence. At the aquifer scale, numerical models are state-of-the-art tools to simulate the behavior of groundwater quantity and quality and serve as decision support system for implementing groundwater protecting measures. While fully and iteratively coupled simulation models consider feedback between the saturated and unsaturated zone, sandy soil conditions and groundwater depths beneath the root zone allow the use of a unidirectional sequential coupling of the unsaturated water flow and nitrate transport model SIMWASER/STOTRASIM with FEFLOW for the investigation area. Considering separated inputs of water and nitrogen into groundwater out of surface water bodies, agricultural, residential and forested areas, first simulation results match observed groundwater tables, but underestimate nitrate concentrations in general. Thus, multiple scenarios assuming higher nitrogen inputs at the surface are simulated to converge with measured nitrate concentrations. Preliminary results indicate that N-input into the groundwater is strongly dominated by contributions of agricultural land. © 2013 Springer-Verlag Berlin Heidelberg.","Aquifer scale; Coupled model; Modeling/modelling; Nitrogen/nitrate; Saturated zone; Unsaturated/vadose zone","Agricultural conditions; Coupled models; Nitrate concentration; Nitrate distribution; Saturated zone; Sequential coupling; Unsaturated water flow; Unsaturated/vadose zone; Agriculture; Aquifers; Artificial intelligence; Computer simulation; Decision support systems; Nitrogen; Surface waters; Groundwater resources; agricultural land; aquifer; coexistence; coupling; decision support system; drinking water; groundwater pollution; leaching; nitrate; nitrogen; phreatic zone; sandy soil; simulation; surface water; vadose zone; water flow; water supply; Austria",Article,Scopus,2-s2.0-84876893043
"Nikoo M.R., Karimi A., Kerachian R., Poorsepahy-Samian H., Daneshmand F.","Rules for Optimal Operation of Reservoir-River-Groundwater Systems Considering Water Quality Targets: Application of M5P Model",2013,"Water Resources Management",12,10.1007/s11269-013-0314-3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877005002&doi=10.1007%2fs11269-013-0314-3&partnerID=40&md5=df809670424087441bbab183542898ba","The aim of this paper is to develop rules for optimal reservoir operation and water withdrawal from river and aquifer considering water supply and pollution control targets. The general approach is making use of an integrated water quantity-quality management (IWQM) modeling in conjunction with accurate data mining techniques. The IWQM model generates data, including; optimal releases and water withdrawal from river and aquifer for different conditions, and M5P and Support Vector Regression (SVR) data mining models utilize the results of the IWQM model for the derivation of rules. The IWQM model minimizes the deviation from water supply and water quality targets during the planning horizon. This method for derivation of operating rules is applied to a real world case study, Zayandehrood system, in Iran, with serious water supply and water pollution problems. The IWQM model is analyzed for different hydrologic and water demands scenarios with total dissolved solids (TDS) as the water quality indicator. Results show that an integrated approach to reservoir-river-aquifer operation in the study area can reduce the TDS by 43 % in the downstream river. © 2013 Springer Science+Business Media Dordrecht.","Integrated water and waste load allocation; Regression tree Induction; Support Vector Regression; Water quantity-quality management; Zayandehrood dam sub-basin","Integrated approach; Optimal reservoir operations; Regression trees; Subbasins; Support vector regression (SVR); Total dissolved solids; Waste load allocations; Water quality indicators; Aquifers; Data mining; Groundwater resources; Optimization; Quality management; Regression analysis; Reservoirs (water); Rivers; Runoff; Water quality; Water supply; Water pollution; artificial intelligence; dam; data mining; groundwater; pollution control; reservoir; river; water management; water quality; water supply; Esfahan [Iran]; Iran; Zayandeh River",Article,Scopus,2-s2.0-84877005002
"Weber M., Neri F., Tirronen V.","A study on scale factor/crossover interaction in distributed differential evolution",2013,"Artificial Intelligence Review",12,10.1007/s10462-011-9267-1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876157565&doi=10.1007%2fs10462-011-9267-1&partnerID=40&md5=f36943c7318cd5d04b085650890387c9","This paper studies the use of multiple scale factor values within distributed Differential Evolution structures employing the so-called exponential crossover. Four different scale factor schemes are proposed, tested, compared and analyzed. Two schemes simply employ multiple scale factor values and two also include an update logic during the evolution. The four schemes have been integrated for comparison within three recently proposed distributed Differential Evolution structures and tested on several various test problems. The results are then compared to those of a previous study where the so-called binomial crossover was employed. Numerical results show that, when associated to the exponential crossover, the employment of multiple scale factors is not systematically beneficial and in some cases even detrimental to the performance of the algorithm. The exponential crossover accentuates the exploitative character of the Differential Evolution, which cannot always be counterbalanced by the increase in the explorative aspect of the algorithm introduced by the employment of multiple scale factor values. © 2011 Springer Science+Business Media B.V.","Differential evolution; Distributed algorithms; Evolutionary algorithms; Scale factor","Different scale; Differential Evolution; Multiple scale factors; Numerical results; Scale Factor; Test problem; Artificial intelligence; Parallel algorithms; Evolutionary algorithms",Article,Scopus,2-s2.0-84876157565
"Dechesne F., Di Tosto G., Dignum V., Dignum F.","No smoking here: Values, norms and culture in multi-agent systems",2013,"Artificial Intelligence and Law",12,10.1007/s10506-012-9128-5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873723607&doi=10.1007%2fs10506-012-9128-5&partnerID=40&md5=dcbe479b50a641111e6a27e743624b30","We use the example of the introduction of the anti-smoking legislation to model the relationship between the cultural make-up, in terms of values, of societies and the acceptance of and compliance with norms. We present two agent-based simulations and discuss the challenge of modeling sanctions and their relation to values and culture. © 2012 Springer Science+Business Media B.V.","Culture; Multi-agent simulation; Norm acceptance; Values","Agent based simulation; Anti-smoking; Multi agent simulation; Multi agent system (MAS); Norm acceptance; Values; Artificial intelligence; Cell culture; Management; Multi agent systems",Article,Scopus,2-s2.0-84873723607
"Biswas S., Dey D., Chatterjee B., Chakravorti S.","An approach based on rough set theory for identification of single and multiple partial discharge source",2013,"International Journal of Electrical Power and Energy Systems",12,10.1016/j.ijepes.2012.10.050,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869864483&doi=10.1016%2fj.ijepes.2012.10.050&partnerID=40&md5=445eb55e2aa15bca3666b6554298a180","This paper describes a methodology to detect the location of single as well as multiple partial discharge sources by sensing the optical radiation from the source. To establish the methodology, an experimental setup has been arranged in the laboratory for generation of partial discharge inside a steel tank provided with five optical sensors placed at the centre of all its five inside walls excepting the top. Analyzing the data by comparing the results from the five sensors give estimation about the position(s) of the partial discharge occurring inside the tank. For successful analysis in the present work, auto-correlation, an extension of correlation based feature extraction technique, is used to extract the features from the recorded signal of the sensors. To classify the extracted features, a rough set theory (RST) based decision support system is used in this work. The novelty of this present work is in locating single as well as multiple sources of partial discharges that emit optical radiation simultaneously. Results show that the auto-correlation based feature extraction technique in conjunction with RST based classifier can localize the sources of partial discharge inside the tank with reasonable degree of accuracy. © 2012 Elsevier Ltd. All rights reserved.","Auto-correlation; Multiple PD source; Optical PD detection; Partial discharge; PD source locations; Rough set theory (RST)","Degree of accuracy; Feature extraction techniques; Multiple PD source; Multiple source; Optical radiations; Partial discharge sources; Recorded signals; Source location; Artificial intelligence; Decision support systems; Feature extraction; Rough set theory; Sensors; Steel tanks; Partial discharges",Article,Scopus,2-s2.0-84869864483
"Perri S., Ricca F., Sirianni M.","Parallel instantiation of ASP programs: Techniques and experiments",2013,"Theory and Practice of Logic Programming",12,10.1017/S1471068411000652,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874601210&doi=10.1017%2fS1471068411000652&partnerID=40&md5=4b087afdd1453ab96896c8fb924cf919","Answer-Set Programming (ASP) is a powerful logic-based programming language, which is enjoying increasing interest within the scientific community and (very recently) in industry. The evaluation of Answer-Set Programs is traditionally carried out in two steps. At the first step, an input program undergoes the so-called instantiation (or grounding) process, which produces a program ′ semantically equivalent to , but not containing any variable; in turn, ′ is evaluated by using a backtracking search algorithm in the second step. It is well-known that instantiation is important for the efficiency of the whole evaluation, might become a bottleneck in common situations, is crucial in several real-world applications, and is particularly relevant when huge input data have to be dealt with. At the time of this writing, the available instantiator modules are not able to exploit satisfactorily the latest hardware, featuring multi-core/multi-processor Symmetric MultiProcessing technologies. This paper presents some parallel instantiation techniques, including load-balancing and granularity control heuristics, which allow for the effective exploitation of the processing power offered by modern Symmetric MultiProcessing machines. This is confirmed by an extensive experimental analysis reported herein. © 2012 Cambridge University Press.","Answer-Set Programming; Heuristics; Instantiation; Parallelism","Artificial intelligence; Answer-set programming; Backtracking search algorithms; Experimental analysis; Granularity controls; Heuristics; Input datas; Input programs; Instantiation; Load-Balancing; Multi core; Parallelism; Processing power; Real-world application; Scientific community; Symmetric multi processing; Symmetric multiprocessing machines; Software engineering",Article,Scopus,2-s2.0-84874601210
"Canetti R., Lin H., Paneth O.","Public-coin concurrent zero-knowledge in the global hash model",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-36594-2_5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873942901&doi=10.1007%2f978-3-642-36594-2_5&partnerID=40&md5=a012f69a716cf3d663f89703a33d1e6b","Public-coin zero-knowledge and concurrent zero-knowledge ( cZK ) are two classes of zero knowledge protocols that guarantee some additional desirable properties. Still, to this date no protocol is known that is both public-coin and cZK for a language outside BPP. Furthermore, it is known that no such protocol can be black-box ZK [Pass et.al, Crypto 09]. We present a public-coin concurrent ZK protocol for any NP language. The protocol assumes that all verifiers have access to a globally specified function, drawn from a collision resistant hash function family. (This model, which we call the Global Hash Function, or GHF model, can be seen as a restricted case of the non-programmable reference string model.) We also show that the impossibility of black-box public-coin cZK extends also to the GHF model. Our protocol assumes CRH functions against quasi-polynomial adversaries and takes O(log 1 + ε n) rounds for any ε &gt; 0, where n is the security parameter. Our techniques combine those for (non-public-coin) black-box cZK with Barak's non-black-box technique for public-coin constant-round ZK. As a corollary we obtain the first simultaneously resettable zero-knowledge protocol with O(log1 + ε n) rounds, in the GHF model. © 2013 International Association for Cryptologic Research.",,"Black boxes; Collision-resistant hash functions; Non black boxes; Protocol cans; Quasi-poly-nomial; Security parameters; String models; Zero knowledge; Zero-knowledge protocols; Artificial intelligence; Hash functions",Conference Paper,Scopus,2-s2.0-84873942901
"Backes A.R., Bruno O.M.","Polygonal approximation of digital planar curves through vertex betweenness",2013,"Information Sciences",12,10.1016/j.ins.2012.07.062,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870054655&doi=10.1016%2fj.ins.2012.07.062&partnerID=40&md5=e5c0c365ddaf7585a6e9576755896ef7","Contour polygonal approximation is usually defined as a set of selected points, which describes a polygon and best represents the original contour. This paper presents a novel graph based approach to compute a polygonal approximation of a shape contour. In a graph, such points correspond to a high transitivity region of the graph. We use the vertex betweenness to measure the importance of each vertice in a graph according to the number of shortest paths where each vertice occurs. By selecting the vertices with higher vertex betweenness, a polygon which retains the main characteristics of the contour is achieved. By using benchmark curves, a comparative experiment with other commonly used algorithms is presented. Results indicate that the proposed approach produced efficient and effective polygonal approximations for digital planar curves. © 2012 Elsevier Inc. All rights reserved.","Digital planar curves; Polygonal approximation; Shape representation; Vertex betweenness","Betweenness; Comparative experiments; Graph-based; Planar curves; Polygonal approximations; Shape contours; Shape representation; Shortest path; Artificial intelligence; Software engineering; Graph theory",Article,Scopus,2-s2.0-84870054655
"Kumbasar T., Eksin I., Guzelkaya M., Yesil E.","Exact inversion of decomposable interval type-2 fuzzy logic systems",2013,"International Journal of Approximate Reasoning",12,10.1016/j.ijar.2012.11.005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873318832&doi=10.1016%2fj.ijar.2012.11.005&partnerID=40&md5=6cef804d7bfea4888f46c3c3d55c1188","It has been demonstrated that type-2 fuzzy logic systems are much more powerful tools than ordinary (type-1) fuzzy logic systems to represent highly nonlinear and/or uncertain systems. As a consequence, type-2 fuzzy logic systems have been applied in various areas especially in control system design and modelling. In this study, an exact inversion methodology is developed for decomposable interval type-2 fuzzy logic system. In this context, the decomposition property is extended and generalized to interval type-2 fuzzy logic sets. Based on this property, the interval type-2 fuzzy logic system is decomposed into several interval type-2 fuzzy logic subsystems under a certain condition on the input space of the fuzzy logic system. Then, the analytical formulation of the inverse interval type-2 fuzzy logic subsystem output is explicitly driven for certain switching points of the Karnik-Mendel type reduction method. The proposed exact inversion methodology driven for the interval type-2 fuzzy logic subsystem is generalized to the overall interval type-2 fuzzy logic system via the decomposition property. In order to demonstrate the feasibility of the proposed methodology, a simulation study is given where the beneficial sides of the proposed exact inversion methodology are shown clearly. © 2012 Elsevier Inc. All rights reserved.","Decomposition property; Interval type-2 fuzzy logic systems; System inversion","Analytical formulation; Decomposition property; Fuzzy logic system; In-control; Input space; Inversion methodology; Simulation studies; Switching points; System inversion; Type reduction; Type-2 fuzzy logic; Type-2 fuzzy logic system; Artificial intelligence; Software engineering; Fuzzy logic",Article,Scopus,2-s2.0-84873318832
"Mayernik M.S., Wallis J.C., Borgman C.L.","Unearthing the infrastructure: Humans and sensors in field-based scientific research",2013,"Computer Supported Cooperative Work",12,10.1007/s10606-012-9178-y,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872614100&doi=10.1007%2fs10606-012-9178-y&partnerID=40&md5=0b39bc97511496662cfee429826fbc50","Distributed sensing systems for studying scientific phenomena are critical applications of information technologies. By embedding computational intelligence in the environment of study, sensing systems allow researchers to study phenomena at spatial and temporal scales that were previously impossible to achieve. We present an ethnographic study of field research practices among researchers in the Center for Embedded Networked Sensing (CENS), a National Science Foundation Science & Technology Center devoted to developing wireless sensing systems for scientific and social applications. Using the concepts of boundary objects and trading zones, we trace the processes of collaborative research around sensor technology development and adoption within CENS. Over the 10-year lifespan of CENS, sensor technologies, sensor data, field research methods, and statistical expertise each emerged as boundary objects that were understood differently by the science and technology partners. We illustrate how sensing technologies were incompatible with field-based environmental research until researchers ""unearthed"" their infrastructures, explicitly reintroducing human skill and expertise into the data collection process and developing new collaborative languages that emphasized building dynamic sensing systems that addressed human needs. In collaborating around a dynamic sensing model, the sensing systems became embedded not in the environment of study, but in the practices of the scientists. © 2012 The Author(s).","boundary objects; collaboration; ecology; environmental science; infrastructure; scientific data; seismology; sensors; technology driven research; trading zones","Boundary objects; collaboration; Environmental science; infrastructure; Scientific data; trading zones; Artificial intelligence; Commerce; Digital storage; Ecology; Environmental technology; Information technology; Research; Seismology; Sensors",Article,Scopus,2-s2.0-84872614100
"Yu H., Chen J., Wang X.","The boomerang attacks on the round-reduced skein-512",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-35999-6_19,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872592865&doi=10.1007%2f978-3-642-35999-6_19&partnerID=40&md5=2f99417df68aeba1415af7557590b2b1","The hash function Skein is one of the five finalists of the NIST SHA-3 competition. It is based on the block cipher Threefish which only uses three primitive operations: modular addition, rotation and bitwise XOR (ARX). This paper studies the boomerang attacks on Skein-512. Boomerang distinguishers on the compression function reduced to 32 and 36 rounds are proposed, with time complexities 2104.5 and 2454 hash computations respectively. Examples of the distinguishers on 28 and 31 rounds are also given. In addition, the boomerang distinguishers are applicable to the key-recovery attacks on reduced Threefish-512. The time complexities for key-recovery attacks reduced to 32-/33-/34-round are about 2181, 2305 and 2424 encryptions. Because the previous boomerang distinguishers for Threefish-512 are in fact not compatible [14], our attacks are the first valid boomerang attacks for the reduced-round Skein-512. © 2013 Springer-Verlag Berlin Heidelberg.","Boomerang attack; Hash function; Skein; Threefish","Block ciphers; Boomerang attack; Compression functions; Distinguishers; Hash computation; Key-recovery; Modular addition; Primitive operations; Sha-3 competitions; Skein; Threefish; Time complexity; Artificial intelligence; Hash functions",Conference Paper,Scopus,2-s2.0-84872592865
"Ryan P.Y.A., Teague V.","Pretty good democracy",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-36213-2-15,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872435183&doi=10.1007%2f978-3-642-36213-2-15&partnerID=40&md5=8a893e19330f9798fb0d96bf3c3c26f7","Code voting seeks to address the issues of privacy and integrity for Remote Internet Voting. It sidesteps many of the inherent vulnerabilities of the Internet and client platforms but it does not provide end-to-end verification that votes are counted as cast. In this paper, we propose a simple technique to enhance the verifiability of code voting by ensuring that the Vote Server can only access the acknowledgement codes if the vote code is correctly registered by a threshold set of Trustees. The mechanism proposed here therefore adds an extra level of verifiability in registering and counting the vote. Voter-verification is simple and direct: the voters need only check that the acknowledgement code returned to them agrees with the value on their code sheet. To ensure receipt-freeness we propose the use of a single acknowledgement code per code sheet, rather than individual acknowledgement codes for each candidate with usual code voting. © Springer-Verlag Berlin Heidelberg 2013.",,"As-cast; Internet voting; Receipt-freeness; Verifiability; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84872435183
"Babaei S., Hulsman M., Reinders M., Ridder J.D.","Detecting recurrent gene mutation in interaction network context using multi-scale graph diffusion",2013,"BMC Bioinformatics",12,10.1186/1471-2105-14-29,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872522601&doi=10.1186%2f1471-2105-14-29&partnerID=40&md5=1d7c68cad3e56912d4fa0e9dd4e4b764","Background: Delineating the molecular drivers of cancer, i.e. determining cancer genes and the pathways which they deregulate, is an important challenge in cancer research. In this study, we aim to identify pathways of frequently mutated genes by exploiting their network neighborhood encoded in the protein-protein interaction network. To this end, we introduce a multi-scale diffusion kernel and apply it to a large collection of murine retroviral insertional mutagenesis data. The diffusion strength plays the role of scale parameter, determining the size of the network neighborhood that is taken into account. As a result, in addition to detecting genes with frequent mutations in their genomic vicinity, we find genes that harbor frequent mutations in their interaction network context.Results: We identify densely connected components of known and putatively novel cancer genes and demonstrate that they are strongly enriched for cancer related pathways across the diffusion scales. Moreover, the mutations in the clusters exhibit a significant pattern of mutual exclusion, supporting the conjecture that such genes are functionally linked. Using multi-scale diffusion kernel, various infrequently mutated genes are found to harbor significant numbers of mutations in their interaction network neighborhood. Many of them are well-known cancer genes.Conclusions: The results demonstrate the importance of defining recurrent mutations while taking into account the interaction network context. Importantly, the putative cancer genes and networks detected in this study are found to be significant at different diffusion scales, confirming the necessity of a multi-scale analysis. © 2013 Babaei et al.; licensee BioMed Central Ltd.",,"Connected component; Insertional mutagenesis; Interaction networks; Multi scale analysis; Multiscale diffusion; Protein-protein interaction networks; Recurrent mutation; Significant patterns; Active networks; Diffusion; Diseases; Proteins; Genes; Murinae; algorithm; animal; article; artificial intelligence; cluster analysis; computer program; gene expression regulation; genetics; genomics; human; leukemia; methodology; mouse; mutation; mutation rate; neoplasm; protein analysis; tumor gene; Algorithms; Animals; Artificial Intelligence; Cluster Analysis; Genes, Neoplasm; Genomics; Humans; Leukemia; Mice; Mutagenesis, Insertional; Mutation; Mutation Rate; Neoplasms; Protein Interaction Mapping; Software",Article,Scopus,2-s2.0-84872522601
"Ouyang Y.","Generalizing the migrativity of continuous t-norms",2013,"Fuzzy Sets and Systems",12,10.1016/j.fss.2012.03.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867899354&doi=10.1016%2fj.fss.2012.03.008&partnerID=40&md5=602385dcf02bc22038ccdb60c140590a","This paper generalizes the migrativity of triangular norms (t-norms for short). Precisely, we find those continuous t-norms T which satisfy T(λ 1x, λ 2y)= λ 3T(x,y), where λ i∈(0,1],i=1,2,3 are given. It is worth pointing out that this paper provides a new characterization of the product t-norm. © 2012 Elsevier B.V.","Additive generator; Continuous t-norms; Migrativity; Strict t-norms","Additive generators; Continuous t-norms; Migrativity; T - Norm; Triangular norms; Artificial intelligence; Fuzzy sets; Mathematical operators",Article,Scopus,2-s2.0-84867899354
"Bosc P., Pivert O.","On a fuzzy bipolar relational algebra",2013,"Information Sciences",12,10.1016/j.ins.2012.07.018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867579126&doi=10.1016%2fj.ins.2012.07.018&partnerID=40&md5=07b5720ad7c8a362d120b74ff1145712","This paper presents an extension of relational algebra suitable for the handling of bipolar concepts. The type of queries considered involves two parts: a first one which expresses a (possibly flexible) constraint, and a second one that corresponds to a (possibly flexible) wish. The framework considered is that of bipolar fuzzy relations where each tuple is associated with a pair of satisfaction degrees. © 2012 Elsevier Inc. All rights reserved.","Bipolarity; Database fuzzy querying; Relational algebra","Bipolarity; Fuzzy querying; Fuzzy relations; Relational algebra; Satisfaction degrees; Artificial intelligence; Software engineering; Algebra",Article,Scopus,2-s2.0-84867579126
"Moreno-Noguer F., Fua P.","Stochastic exploration of ambiguities for nonrigid shape recovery",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",12,10.1109/TPAMI.2012.102,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871751677&doi=10.1109%2fTPAMI.2012.102&partnerID=40&md5=5464cc845030dfea246f892c60917314","Recovering the 3D shape of deformable surfaces from single images is known to be a highly ambiguous problem because many different shapes may have very similar projections. This is commonly addressed by restricting the set of possible shapes to linear combinations of deformation modes and by imposing additional geometric constraints. Unfortunately, because image measurements are noisy, such constraints do not always guarantee that the correct shape will be recovered. To overcome this limitation, we introduce a stochastic sampling approach to efficiently explore the set of solutions of an objective function based on point correspondences. This allows us to propose a small set of ambiguous candidate 3D shapes and then use additional image information to choose the best one. As a proof of concept, we use either motion or shading cues to this end and show that we can handle a complex objective function without having to solve a difficult nonlinear minimization problem. The advantages of our method are demonstrated on a variety of problems including both real and synthetic data. © 2012 IEEE.","Deformable surfaces; monocular shape estimation","3-D shape; Deformable surfaces; Deformation modes; Different shapes; Geometric constraint; Image information; Image measurements; Linear combinations; Nonlinear minimization; Objective functions; Point correspondence; Proof of concept; Shape estimation; Shape recovery; Single images; Stochastic sampling; Synthetic data; Shape optimization; Stochastic systems; Deformation; algorithm; article; artificial intelligence; automated pattern recognition; biological model; computer assisted diagnosis; image enhancement; methodology; reproducibility; sensitivity and specificity; statistical analysis; statistical model; three dimensional imaging; Algorithms; Artificial Intelligence; Data Interpretation, Statistical; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Models, Biological; Models, Statistical; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84871751677
"Escudero J., Ifeachor E., Zajicek J.P., Green C., Shearer J., Pearson S.","Machine learning-based method for personalized and cost-effective detection of alzheimer's disease",2013,"IEEE Transactions on Biomedical Engineering",12,10.1109/TBME.2012.2212278,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871813342&doi=10.1109%2fTBME.2012.2212278&partnerID=40&md5=fa37a2a6e119d8e420e3a45c8e39cca4","Diagnosis of Alzheimer's disease (AD) is often difficult, especially early in the disease process at the stage of mild cognitive impairment (MCI). Yet, it is at this stage that treatment is most likely to be effective, so there would be great advantages in improving the diagnosis process. We describe and test a machine learning approach for personalized and cost-effective diagnosis of AD. It uses locally weighted learning to tailor a classifier model to each patient and computes the sequence of biomarkers most informative or cost-effective to diagnose patients. Using ADNI data, we classified AD versus controls and MCI patients who progressed to AD within a year, against those who did not. The approach performed similarly to considering all data at once, while significantly reducing the number (and cost) of the biomarkers needed to achieve a confident diagnosis for each patient. Thus, it may contribute to a personalized and effective detection of AD, and may prove useful in clinical settings. © 1964-2012 IEEE.","Alzheimer's disease (AD); classification; cost; machine learning; mild cognitive impairment (MCI); personalization","Alzheimer's disease; Classifier models; Clinical settings; Cognitive impairment; Disease process; Learning approach; Learning-based methods; Locally weighted learning; Personalizations; Classification (of information); Cost effectiveness; Costs; Learning systems; Diagnosis; amyloid beta protein[1-40]; amyloid beta protein[1-42]; apolipoprotein E4; homocysteine; tau protein; Alzheimer disease; article; cerebrospinal fluid analysis; computer assisted diagnosis; controlled study; cost effectiveness analysis; follow up; functional neuroimaging; human; machine learning; major clinical study; mild cognitive impairment; nuclear magnetic resonance imaging; personalized medicine; positron emission tomography; protein blood level; protein phosphorylation; sensitivity and specificity; Alzheimer Disease; Artificial Intelligence; Biological Markers; Cost-Benefit Analysis; Databases, Factual; Disease Progression; Female; Humans; Individualized Medicine; Magnetic Resonance Imaging; Male; Mild Cognitive Impairment; Reproducibility of Results",Article,Scopus,2-s2.0-84871813342
"Esmalifalak M., Nguyen N.T., Zheng R., Han Z.","Detecting stealthy false data injection using machine learning in smart grid",2013,"GLOBECOM - IEEE Global Telecommunications Conference",12,10.1109/GLOCOM.2013.6831172,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904095002&doi=10.1109%2fGLOCOM.2013.6831172&partnerID=40&md5=1d3618d45b35ea53e05ba545b6a8f13c","Aging power industries together with increase in the demand from industrial and residential customers are the main incentive for policy makers to define a road map to the next generation power system called smart grid. In smart grid, the overall monitoring costs will be decreased but at the same time, the risk of cyber attacks might be increased. Recently a new type of attacks (called the stealth attack) has been introduced, which cannot be detected by the bad data detection using state estimation. In this paper, we show how normal operations of power networks can be statistically distinguished from the case under stealthy attacks. We devise two machine learning based techniques for stealthy attack detection. The first method utilizes supervised learning over labeled data and trains a support vector machine. The second method requires no training data and detects the deviation in measurement In both methods, principle component analysis is used to reduce the dimensionality of the data to be processed, and thus leads to lower computation complexities. The results of the proposed detection methods on the IEEE standard test systems demonstrate effectiveness of both schemes. © 2013 IEEE.",,"Artificial intelligence; Communication; Learning systems; Principal component analysis; Smart power grids; Bad data detections; Computation complexity; Detection methods; False data injection; Monitoring costs; Normal operations; Principle component analysis; Residential customers; Electric power transmission networks",Conference Paper,Scopus,2-s2.0-84904095002
"Leony D., Muñoz-Merino P.J., Pardo A., Kloos C.D.","Provision of awareness of learners' emotions through visualizations in a computer interaction-based environment",2013,"Expert Systems with Applications",12,10.1016/j.eswa.2013.03.030,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878319805&doi=10.1016%2fj.eswa.2013.03.030&partnerID=40&md5=5bb46556562fd90c2026b91cd874cb46","One of the challenges of intelligent systems for education is to use low-level data collected in computer environments in the form of events or interactions to infer information with high-level significance using artificial intelligence techniques, and present it through visualizations in a meaningful and effective way. Among this information, emotional data is gaining track in by instructors in their educational activities. Many benefits can be obtained if an intelligent systems can bring teachers with knowledge about their learner's emotions, learning causes, and learning relationships with emotions. In this paper, we propose and justify a set of visualizations for an intelligent system to provide awareness about the emotions of the learners to the instructor based on the learners' interactions in their computers. We apply these learner's affective visualizations in a programming course at University level with more than 300 students, and analyze and interpret the student's emotional results in connection with the learning process. © 2013 Elsevier Ltd. All rights reserved.","Affective learning; Intelligent systems emotion awareness; Learning analytics; Visualization","Education; Flow visualization; Intelligent systems; Students; Teaching; Visualization; Affective learning; Artificial intelligence techniques; Computer environments; Computer interaction; Educational activities; Learning analytics; Programming course; University levels; Learning systems",Article,Scopus,2-s2.0-84878319805
"Neuman U.M., Atkin J.A.D.","Airport gate assignment considering ground movement",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,10.1007/978-3-642-41019-2_14,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886387299&doi=10.1007%2f978-3-642-41019-2_14&partnerID=40&md5=b809bd4f640db856025e377a568d3ed7","Airports all over the world are becoming busier and many of them are facing capacity problems. The actual airport capacity strongly depends on the efficiency of the resource utilisation. Although simultaneously handling all of the problems may result in more effective resource utilisation, historically different types of airport resources have been handled independently. Despite introducing new support systems the historical separation has often remained. This may increase congestion, which has a negative impact on both the passengers' comfort and the environment. This paper focuses on modelling the gate allocation problem taking into consideration possible conflicts at taxiways around gates. Introducing the taxiway information in the early stage of the allocation planning is a step forward in integration of the two airport operations. Various configurations of the model have been tested using a real data set to evaluate how the new anti-conflict and novel towing constraints influence the final allocation. © 2013 Springer-Verlag.","Airport Gate Assignment; Mathematical Modelling; Mixed Integer Programming","Artificial intelligence; Computer science; Mathematical models; Airport capacity; Airport operations; Allocation problems; Capacity problems; Ground movement; Mixed integer programming; Resource utilisation; Support systems; Airports",Conference Paper,Scopus,2-s2.0-84886387299
"Meydan C., Otu H.H., Sezerman O.U.","Prediction of peptides binding to MHC class I and II alleles by temporal motif mining",2013,"BMC Bioinformatics",12,10.1186/1471-2105-14-S2-S13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884193260&doi=10.1186%2f1471-2105-14-S2-S13&partnerID=40&md5=220f98c535f10d72a1ce3f5f41e5b082","Background: MHC (Major Histocompatibility Complex) is a key player in the immune response of most vertebrates. The computational prediction of whether a given antigenic peptide will bind to a specific MHC allele is important in the development of vaccines for emerging pathogens, the creation of possibilities for controlling immune response, and for the applications of immunotherapy. One of the problems that make this computational prediction difficult is the detection of the binding core region in peptides, coupled with the presence of bulges and loops causing variations in the total sequence length. Most machine learning methods require the sequences to be of the same length to successfully discover the binding motifs, ignoring the length variance in both motif mining and prediction steps. In order to overcome this limitation, we propose the use of time-based motif mining methods that work position-independently. Results: The prediction method was tested on a benchmark set of 28 different alleles for MHC class I and 27 different alleles for MHC class II. The obtained results are comparable to the state of the art methods for both MHC classes, surpassing the published results for some alleles. The average prediction AUC values are 0.897 for class I, and 0.858 for class II. Conclusions: Temporal motif mining using partial periodic patterns can capture information about the sequences well enough to predict the binding of the peptides and is comparable to state of the art methods in the literature. Unlike neural networks or matrix based predictors, our proposed method does not depend on peptide length and can work with both short and long fragments. This advantage allows better use of the available training data and the prediction of peptides of uncommon lengths. © 2013 Meydan et al.",,"Artificial intelligence; Complex networks; DNA sequences; Genes; Immune system; Learning systems; Mining; Peptides; Antigenic peptides; Computational predictions; Machine learning methods; Major histocompatibility complex; Periodic pattern; Prediction methods; Sequence lengths; State-of-the-art methods; Forecasting; Vertebrata; peptide; algorithm; allele; area under the curve; article; artificial intelligence; artificial neural network; data mining; gene; human; metabolism; methodology; protein binding; protein database; protein domain; Algorithms; Alleles; Area Under Curve; Artificial Intelligence; Data Mining; Databases, Protein; Genes, MHC Class I; Genes, MHC Class II; Humans; Neural Networks (Computer); Peptides; Protein Binding; Protein Interaction Domains and Motifs",Article,Scopus,2-s2.0-84884193260
"Cheng C.-W., Chanani N., Venugopalan J., Maher K., Wang M.D.","IcuARM-An ICU Clinical Decision Support System Using Association Rule Mining",2013,"IEEE Journal of Translational Engineering in Health and Medicine",12,10.1109/JTEHM.2013.2290113,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904125956&doi=10.1109%2fJTEHM.2013.2290113&partnerID=40&md5=82e11ae8e4ac50a2d64e2b41f178ab4a","The rapid development of biomedical monitoring technologies has enabled modern intensive care units (ICUs) to gather vast amounts of multimodal measurement data about their patients. However, processing large volumes of complex data in real-time has become a big challenge. Together with ICU physicians, we have designed and developed an ICU clinical decision support system icuARM based on associate rule mining (ARM), and a publicly available research database MIMIC-II (Multi-parameter Intelligent Monitoring in Intensive Care II) that contains more than 40,000 ICU records for 30,000+patients. icuARM is constructed with multiple association rules and an easy-to-use graphical user interface (GUI) for care providers to perform real-time data and information mining in the ICU setting. To validate icuARM, we have investigated the associations between patients' conditions such as comorbidities, demographics, and medications and their ICU outcomes such as ICU length of stay. Coagulopathy surfaced as the most dangerous co-morbidity that leads to the highest possibility (54.1%) of prolonged ICU stay. In addition, women who are older than 50 years have the highest possibility (38.8%) of prolonged ICU stay. For clinical conditions treatable with multiple drugs, icuARM suggests that medication choice can be optimized based on patient-specific characteristics. Overall, icuARM can provide valuable insights for ICU physicians to tailor a patient's treatment based on his or her clinical status in real time. © 2013 IEEE.","association rule mining; clinical risk prediction models; Intensive care units (ICUs); personalized clinical decision support system","Artificial intelligence; Association rules; Data mining; Decision support systems; Graphical user interfaces; Patient treatment; User interfaces; Biomedical monitoring; Clinical decision support systems; Clinical risks; Graphical user interfaces (GUI); Intelligent monitoring; Multi-modal measurement; Multiple-association; Patients' conditions; Intensive care units",Article,Scopus,2-s2.0-84904125956
"Nguyen J.S., Su S.W., Nguyen H.T.","Experimental study on a smart wheelchair system using a combination of stereoscopic and spherical vision",2013,"Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS",12,10.1109/EMBC.2013.6610571,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886509748&doi=10.1109%2fEMBC.2013.6610571&partnerID=40&md5=d79a657315522718122ddf1ff5786b02","This paper is concerned with the experimental study performance of a smart wheelchair system named TIM (Thought-controlled Intelligent Machine), which uses a unique camera configuration for vision. Included in this configuration are stereoscopic cameras for 3-Dimensional (3D) depth perception and mapping ahead of the wheelchair, and a spherical camera system for 360-degrees of monocular vision. The camera combination provides obstacle detection and mapping in unknown environments during real-time autonomous navigation of the wheelchair. With the integration of hands-free wheelchair control technology, designed as control methods for people with severe physical disability, the smart wheelchair system can assist the user with automated guidance during navigation. An experimental study on this system was conducted with a total of 10 participants, consisting of 8 able-bodied subjects and 2 tetraplegic (C-6 to C-7) subjects. The hands-free control technologies utilized for this testing were a head-movement controller (HMC) and a brain-computer interface (BCI). The results showed the assistance of TIM's automated guidance system had a statistically significant reduction effect (p-value = 0.000533) on the completion times of the obstacle course presented in the experimental study, as compared to the test runs conducted without the assistance of TIM. © 2013 IEEE.",,"Brain computer interface; Cameras; Depth perception; Obstacle detectors; Automated guidance systems; Autonomous navigation; Camera configuration; Control technologies; Intelligent machine; Physical disability; Stereoscopic camera; Wheelchair control; Wheelchairs; adult; artificial intelligence; brain computer interface; clinical trial; depth perception; equipment design; female; hand; human; male; middle aged; quadriplegia; vision; wheelchair; young adult; Adult; Artificial Intelligence; Brain-Computer Interfaces; Depth Perception; Equipment Design; Female; Hand; Humans; Male; Middle Aged; Quadriplegia; Vision, Ocular; Wheelchairs; Young Adult",Conference Paper,Scopus,2-s2.0-84886509748
"Kadiyala A., Kaur D., Kumar A.","Development of hybrid genetic-algorithm-based neural networks using regression trees for modeling air quality inside a public transportation bus",2013,"Journal of the Air and Waste Management Association",12,10.1080/10962247.2012.741054,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873114370&doi=10.1080%2f10962247.2012.741054&partnerID=40&md5=eeafb1f5a868444b88d4a200403bcb91","The present study developed a novel approach to modeling indoor air quality (IAQ) of a public transportation bus by the development of hybrid genetic-algorithm-based neural networks (also known as evolutionary neural networks) with input variables optimized from using the regression trees, referred as the GART approach. This study validated the applicability of the GART modeling approach in solving complex nonlinear systems by accurately predicting the monitored contaminants of carbon dioxide (CO2), carbon monoxide (CO), nitric oxide (NO), sulfur dioxide (SO2), 0.3-0.4 μm sized particle numbers, 0.4-0.5 μm sized particle numbers, particulate matter (PM) concentrations less than 1.0 μm (PM1.0), and PM concentrations less than 2.5 μm (PM2.5) inside a public transportation bus operating on 20% grade biodiesel in Toledo, OH.First, the important variables affecting each monitored in-bus contaminant were determined using regression trees. Second, the analysis of variance was used as a complimentary sensitivity analysis to the regression tree results to determine a subset of statistically significant variables affecting each monitored in-bus contaminant. Finally, the identified subsets of statistically significant variables were used as inputs to develop three artificial neural network (ANN) models. The models developed were regression tree-based back-propagation network (BPN-RT), regression tree-based radial basis function network (RBFN-RT), and GART models. Performance measures were used to validate the predictive capacity of the developed IAQ models. The results from this approach were compared with the results obtained from using a theoretical approach and a generalized practicable approach to modeling IAQ that included the consideration of additional independent variables when developing the aforementioned ANN models. The hybrid GART models were able to capture majority of the variance in the monitored in-bus contaminants. The genetic-algorithm-based neural network IAQ models outperformed the traditional ANN methods of the back-propagation and the radial basis function networks.Implications: The novelty of this research is the development of a novel approach to modeling vehicular indoor air quality by integration of the advanced methods of genetic algorithms, regression trees, and the analysis of variance for the monitored in-vehicle gaseous and particulate matter contaminants, and comparing the results obtained from using the developed approach with conventional artificial intelligence techniques of back propagation networks and radial basis function networks. This study validated the newly developed approach using holdout and threefold cross-validation methods. These results are of great interest to scientists, researchers, and the public in understanding the various aspects of modeling an indoor microenvironment. This methodology can easily be extended to other fields of study also.Supplemental Materials: Supplemental materials are available for this paper. Go to the publisher's online edition of the Journal of the Air &amp; Waste Management Association. © 2013 Copyright 2013 A&amp;WMA.",,"Air quality; Backpropagation algorithms; Buses; Carbon dioxide; Carbon monoxide; Contamination; Forestry; Impurities; Indoor air pollution; Neural networks; Nitric oxide; Particles (particulate matter); Radial basis function networks; Regression analysis; Sulfur; Sulfur dioxide; Waste management; Artificial intelligence techniques; Artificial neural network models; Backpropagation network; Complex nonlinear system; Cross-validation methods; Evolutionary neural network; In-vehicle; Independent variables; Indoor air quality; Input variables; Microenvironments; Modeling approach; Online editions; Particle numbers; Particulate Matter; Performance measure; Predictive capacity; Public transportation; Radial basis functions; Regression trees; Theoretical approach; Tree-based; Waste Management Association; Trees (mathematics); carbon dioxide; carbon monoxide; nitric oxide; sulfur dioxide; air monitoring; air pollutant; air quality; air quality standard; article; artificial neural network; concentration (parameters); controlled study; GART model; genetic algorithm; hybrid genetic algorithm based neural network; indoor air quality; nonlinear system; particle size; particulate matter; predictive validity; priority journal; public transportation bus; regression analysis; regression tree based back propagation network; regression tree based radial basis function network; sensitivity analysis; statistical analysis; statistical model; theoretical sample; traffic and transport; Air Pollution; Air Quality; Algorithms; Carbon Dioxide; Carbon Monoxide; Forestry; Impurities; Mathematics; Neural Networks; Particles; Pollution; Regression Analysis; Sulfur; Sulfur Dioxide; Trees; Waste Management",Article,Scopus,2-s2.0-84873114370
"Harmancioglu N.B., Barbaros F., Cetinkaya C.P.","Sustainability Issues in Water Management",2013,"Water Resources Management",12,10.1007/s11269-012-0172-4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874977847&doi=10.1007%2fs11269-012-0172-4&partnerID=40&md5=81ba8fddc478507881d18022f6aa51eb","In the 1992 Rio World Summit on Environment and Development/UNCED), water resources are indicated to remain at the core of sustainable development and, thus, they are to be managed and developed on a sustainable basis. Sustainability is a philosophical concept and thus difficult to measure. Yet, we need to describe it on rather precise terms to assess whether our water management practices are sustainable and to ensure sustainability in decision making for management. To this end, a number of sustainability criteria have been defined, based on quantifiable measures, without overlooking immeasurable aspects of sustainable development. This paper considers economic, social and environmental dimensions of sustainability as the basic criteria to be pursued in evaluating how effective our water management plans are in achieving sustainability. On the other hand, actual case studies are needed to test the usefulness of selected criteria by using computer-based interactive optimization and simulation models with associated databases embedded into a decision support system (DSS). The study herein intends to present such a case study based on economic, social and environmental criteria to assess sustainability in management of the Gediz River Basin in Turkey. Various management scenarios developed for the basin are evaluated within a DSS while ensuring multi-stakeholder involvement in defining the three sustainability criteria. The case study is a result of the analyses carried out in SMART (Sustainable Management of Scarce Resources in the Coastal Zone) and OPTIMA (Optimization for Sustainable Water Resources Management) projects funded respectively by the 5th and 6th Framework Programmes of the European Union. © 2012 Springer Science+Business Media Dordrecht.","Decision support systems; Indicators; Optimization; Scenario development; Sustainability; Sustainability criteria; Water management","Decision support system (dss); Environment and development; Interactive optimization; Scenario development; Social and environmental; Sustainability criteria; Sustainable management; Sustainable water resources; Artificial intelligence; Computer simulation; Decision making; Decision support systems; Economic and social effects; Environmental management; Indicators (instruments); Optimization; Philosophical aspects; Planning; Research; Water conservation; Water management; Sustainable development; decision making; decision support system; optimization; stakeholder; sustainability; sustainable development; water management; Gediz Basin; Turkey",Article,Scopus,2-s2.0-84874977847
"Medeiros H., Moniz H., Batista F., Trancoso I., Nunes L.","Disfluency detection based on prosodic features for university lectures",2013,"Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906221162&partnerID=40&md5=091c5087c15ab17a82a7ff4fd4cb4d41","This paper focuses on the identification of disfluent sequences and their distinct structural regions, based on acoustic and prosodic features. Reported experiments are based on a corpus of university lectures in European Portuguese, with roughly 32h, and a relatively high percentage of disfluencies (7.6%). The set of features automatically extracted from the corpus proved to be discriminant of the regions contained in the production of a disfluency. Several machine learning methods have been applied, but the best results were achieved using Classification and Regression Trees (CART). The set of features which was most informative for cross-region identification encompasses word duration ratios, word confidence score, silent ratios, and pitch and energy slopes. Features such as the number of phones and syllables per word proved to be more useful for the identification of the interregnum, whereas energy slopes were most suited for identifying the interruption point. Copyright © 2013 ISCA.","Automatic disfluency detection; Corpus of university lectures; Machine learning; Prosodic features","Artificial intelligence; Classification and regression tree; Confidence score; Corpus of university lectures; Disfluencies; Disfluency detections; Machine learning methods; Prosodic features; Learning systems",Conference Paper,Scopus,2-s2.0-84906221162
"Soualhi A., Clerc G., Razik H., Rivas F.","Long-term prediction of bearing condition by the neo-fuzzy neuron",2013,"Proceedings - 2013 9th IEEE International Symposium on Diagnostics for Electric Machines, Power Electronics and Drives, SDEMPED 2013",12,10.1109/DEMPED.2013.6645774,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891049656&doi=10.1109%2fDEMPED.2013.6645774&partnerID=40&md5=afafec21550416efb675e248dd070ce0","Rolling element bearings are devices used in almost every electrical machine. Therefore, it is important to monitor and track the degradation of bearings. This paper presents a new approach to predict the degradation of bearings by a time series forecasting model called the neo-fuzzy neuron. The proposed approach uses the root mean square extracted from vibration signals as a health indicator. The root mean square is used here as an input of the neo-fuzzy neuron in order to estimate the evolution of bearing's degradation in time. Experimental degradation data provided by the University of Cincinnati is used to validate the proposed approach. A comparative study between the neo-fuzzy neuron and the adaptive neuro-fuzzy inference system is carried out to appraise their prediction capabilities. The experimental results show that the neo-fuzzy model can track the degradation of bearings. © 2013 IEEE.","Artificial intelligence; Feature extraction; Fuzzy neural networks; Prognosis; Time domain analysis; Vibration measurement","Artificial intelligence; Digital storage; Electric machinery; Feature extraction; Forecasting; Fuzzy neural networks; Neurons; Power electronics; Time domain analysis; Vibration measurement; Adaptive neuro-fuzzy inference system; Comparative studies; Electrical machine; Long-term prediction; Prediction capability; Prognosis; Rolling Element Bearing; Time series forecasting models; Electric drives",Conference Paper,Scopus,2-s2.0-84891049656
"Al-Nawayseh M.K., Alnabhan M.M., Al-Debei M.M., Balachandran W.","An adaptive decision support system for last mile logistics in e-commerce: A study on online grocery shopping",2013,"International Journal of Decision Support System Technology",12,10.4018/jdsst.2013010103,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891769052&doi=10.4018%2fjdsst.2013010103&partnerID=40&md5=41a2d652418f6344995617d3a98d668e","Last mile logistics represent one of the most important challenging issues in online grocery shopping. Online customers are expecting high logistical services, demanding convenience, high reliable and on-time delivery services. As such, online retailers have to respond to these expectations by providing convenient logistical services while keeping this process cost efficient as much as possible. This research aims to design an e-commerce logistical decision support system for online grocery shopping in Jordan as a case study from the developing countries. Online grocery retailers are supposed to use this model in order to select the most suitable delivery operating system in the future. To implement and evaluate this model, one of the available routing and scheduling online solutions (i.e. ""My Route Online"") is used to identify, analyse, and compare the cost efficiencies of the available alternative delivery solutions. The system is tested using real data over three different delivery alternatives (i.e. home delivery, delivery point and pickup point) in order to evaluate and compare their cost efficiencies. The findings from the experiments show that there are significant differences amongst the three delivery alternatives on the basis of three KPIs: cost, distance and time. The findings also indicate that the time indicator has more powerful change effect on cost than distance for all delivery alternatives. According to the level of investments online grocery retailers are willing to offer, customer preferences, and the experimental results, it is concluded that pickup point solution is the best logistical strategy for online grocery retailers to start with. Copyright © 2013, IGI Global.","Decision support systems; Developing countries; E-commerce; Last mile logistics; Modelling; Online grocery shopping; Small-to-medium enterprises","Adaptive decision support system; Customer preferences; Last mile; Logistical services; On-time delivery; Online customers; Online grocery shopping; Routing and scheduling; Artificial intelligence; Cost effectiveness; Costs; Decision support systems; Developing countries; Electronic commerce; Models; Pickups; Sales; Logistics",Article,Scopus,2-s2.0-84891769052
"Iacoban R., Reynolds R.G., Brewster J.","Cultural swarms: Modeling the impact of culture on social interaction and problem solving",2013,"2003 IEEE Swarm Intelligence Symposium, SIS 2003 - Proceedings",12,10.1109/SIS.2003.1202270,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942134353&doi=10.1109%2fSIS.2003.1202270&partnerID=40&md5=80547c15b2bba9c9c90e881cb7ea1f40","In this paper we investigate how diverse knowledge sources interact to direct individuals in a swarm population. We identify three basic phases of problem solving that are generated by the swarm population in the solution of real valued function optimization problems. The question that we are interested in answering is how these phases derive from the interaction of various sources of cultural knowledge present in the belief space of a population. We map the central tendency of the subset of individuals that are influences by each knowledge source over time at the meta-level. The resultant patterns suggest the presence of ""cultural swarms"" where various knowledge sources take turns at leading and following in the exploration and exploitation of the problem space. This suggests that the social interaction of individuals coupled with their interaction with a culture within which they are embedded provides a powerful vehicle for the solution of complex problems. © 2003 IEEE.","Character generation; Computer science; Cultural differences; Decision making; Genetic programming; History; Humans; Power system modeling; Problem-solving; Vehicles","Amphibious vehicles; Artificial intelligence; Computer programming; Computer science; Computer systems programming; Decision making; Economic and social effects; Genetic algorithms; Genetic programming; History; Optimization; Social networking (online); Social sciences; Vehicles; Central tendencies; Character generation; Cultural difference; Exploration and exploitation; Humans; Power system model; Real-valued functions; Social interactions; Problem solving",Conference Paper,Scopus,2-s2.0-84942134353
"Gao D., Wei F., Li W., Liu X., Zhou M.","Cotraining based bilingual sentiment lexicon learning",2013,"AAAI Workshop - Technical Report",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898874845&partnerID=40&md5=fab04da4542143408080be038dee6f54","In this paper, we address the issue of bilingual sentiment lexicon learning(BSLL) which aims to automatically and simultaneously generate sentiment words for two languages. The underlying motivation is that sentiment information from two languages can perform iterative mutual-teaching in the learning procedure. We propose to develop two classifiers to determine the sentiment polarities of words under a co-training framework, which makes full use of the two-view sentiment information from the two languages. The word alignment derived from the parallel corpus is leveraged to design effective features and to bridge the learning of the two classifiers. The experimental results on English and Chinese languages show the effectiveness of our approach in BSLL. Copyright © 2013, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Bridges; Chinese language; Co-training; Learning procedures; Parallel corpora; Sentiment lexicons; Two views; Word alignment; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84898874845
"Bareinboim E., Pearl J.","Meta-transportability of causal effects: A formal approach",2013,"Journal of Machine Learning Research",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921451964&partnerID=40&md5=f54a1dd12095a7976a21feeab6d56cf1","This paper considers the problem of transferring experimental findings learned from multiple heterogeneous domains to a different environment, in which only passive observations can be collected. Pearl and Bareinboim (2011) established a complete characterization for such transfer between two domains, a source and a target, and this paper generalizes their results to multiple heterogeneous domains. It establishes a necessary and sufficient condition for deciding when effects in the target domain are estimable from both statistical and causal information transferred from the experiments in the source domains. The paper further provides a complete algorithm for computing the transport formula, that is, a way of fusing observational and experimental information to synthesize an unbiased estimate of the desired effects. Copyright 2013 by the authors.",,"Software engineering; Formal approach; Heterogeneous domains; Target domain; Two domains; Unbiased estimates; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84921451964
"Ruiz-Zafra Á., Benghazi K., Noguera M., Garrido J.L.","Zappa: An Open Mobile Platform to Build Cloud-Based m-Health Systems",2013,"Advances in Intelligent Systems and Computing",12,10.1007/978-3-319-00566-9_12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883022872&doi=10.1007%2f978-3-319-00566-9_12&partnerID=40&md5=4cfb828500e0078d3f9de3d807a9d8f7","Cloud computing and associated services are changing the way in which we manage information and access data. E-health services are not impermeable to novel technologies, especially those that involve mobile devices. At present, many patient monitoring m-health (mobile-health) platforms consist of close, vendor-dependent solutions based on particular architectures and technologies offering a limited set of interfaces to interoperate with. This fact hinders to advance in quality attributes such as customization, adaptation, extension, interoperability and even transparency of cloud infrastructure of existing solutions according to the specific needs of their users (patients and physicians). This paper presents an extensible, scalable, highly-interoperable and customizable platform called Zappa, designed to support e-Health/m-Health systems and that is able to operate in the cloud. The platform is based on components and services architecture, as well as on open and close source hardware and opensource software that reduces its acquisition and operation costs. The platform has been used to develop several remote mobile monitoring m-health systems. © Springer International Publishing Switzerland 2013.","cloud computing; e-Health; m-Health; mobile applications; open source; patient monitoring; SOA","Application programs; Artificial intelligence; Cloud computing; Health; Mobile devices; Patient monitoring; Ehealth; mHealth; Mobile applications; Open sources; SOA; Interoperability",Conference Paper,Scopus,2-s2.0-84883022872
"Wong K.I., Wong P.K., Cheung C.S., Vong C.M.","Modelling of diesel engine performance using advanced machine learning methods under scarce and exponential data set",2013,"Applied Soft Computing Journal",12,10.1016/j.asoc.2013.06.006,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885381962&doi=10.1016%2fj.asoc.2013.06.006&partnerID=40&md5=f133ab07fa4ce26de52e2d5c6d2343ef","Traditional methods on creating diesel engine models include the analytical methods like multi-zone models and the intelligent based models like artificial neural network (ANN) based models. However, those analytical models require excessive assumptions while those ANN models have many drawbacks such as the tendency to overfitting and the difficulties to determine the optimal network structure. In this paper, several emerging advanced machine learning techniques, including least squares support vector machine (LS-SVM), relevance vector machine (RVM), basic extreme learning machine (ELM) and kernel based ELM, are newly applied to the modelling of diesel engine performance. Experiments were carried out to collect sample data for model training and verification. Limited by the experiment conditions, only 24 sample data sets were acquired, resulting in data scarcity. Six-fold cross-validation is therefore adopted to address this issue. Some of the sample data are also found to suffer from the problem of data exponentiality, where the engine performance output grows up exponentially along the engine speed and engine torque. This seriously deteriorates the prediction accuracy. Thus, logarithmic transformation of dependent variables is utilized to pre-process the data. Besides, a hybrid of leave-one-out cross-validation and Bayesian inference is, for the first time, proposed for the selection of hyperparameters of kernel based ELM. A comparison among the advanced machine learning techniques, along with two traditional types of ANN models, namely back propagation neural network (BPNN) and radial basis function neural network (RBFNN), is conducted. The model evaluation is made based on the time complexity, space complexity, and prediction accuracy. The evaluation results show that kernel based ELM with the logarithmic transformation and hybrid inference is far better than basic ELM, LS-SVM, RVM, BPNN and RBFNN, in terms of prediction accuracy and training time. © 2012 Elsevier B.V. All rights reserved.","Data exponentiality; Data scarcity; Diesel engine modelling; Engine performance; Hybrid inference; Kernel based extreme learning machine","Artificial intelligence; Backpropagation; Bayesian networks; Complex networks; Deep neural networks; Diesel engines; Engines; Forecasting; Inference engines; Knowledge acquisition; Learning algorithms; Metadata; Neural networks; Radial basis function networks; Statistical methods; Structural optimization; Support vector machines; Two phase flow; Data exponentiality; Data scarcity; Diesel engine modelling; Engine performance; Extreme learning machine; Hybrid inference; Learning systems",Article,Scopus,2-s2.0-84885381962
"Shen X.-J., Liu Y.-Y., Huang Y.-P., Xu T., He X.-W.","Fast ant colony algorithm for solving traveling salesman problem",2013,"Jilin Daxue Xuebao (Gongxueban)/Journal of Jilin University (Engineering and Technology Edition)",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873254868&partnerID=40&md5=ffc54dd2c16af0dec971a4e9bb911bd7","Using ant colony algorithm to solve Traveling Salesman Problem (TSP) has certain disadvantages, such plunging into local minimum, slower convergence speed and so on. In order to find the optimal path accurately and rapidly, an improved ant colony algorithm is proposed. The improved algorithm uses adaptive adjusting mechanism of pheromone decay parameter to adjust the convergence speed and ensure the global search ability. With the help of common path, the computation time is reduced and the better solution can be obtained. Experiments show that, in a relatively few iterations, the percentage deviation of the average solution to the best known solution is 0.46%, and the percentage deviation of the best solution to the best known solution is 0.23%. The improved algorithm has high accuracy and convergence speed.","Adaptive; Ant colony algorithm; Artificial intelligence; Common path; Traveling salesman problem","Adaptive; Adjusting mechanism; Ant colony algorithms; Common path; Computation time; Convergence speed; Decay parameters; Global search ability; Improved ant colony algorithm; Local minimums; Optimal paths; Percentage deviation; Artificial intelligence; Traveling salesman problem; Algorithms",Article,Scopus,2-s2.0-84873254868
"Kadri F., Pach C., Chaabane S., Berger T., Trentesaux D., Tahon C., Sallez Y.","Modelling and management of strain situations in hospital systems using an ORCA approach",2013,"Proceedings of 2013 International Conference on Industrial Engineering and Systems Management, IEEE - IESM 2013",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899051674&partnerID=40&md5=da33b4c85a3f0aa38f126f13eae46d56","Management of patient flow, especially the flow resulting from health crises (exceptional situations) in emergency departments (ED), is one of the most important problems ED managers have to deal with. Emergency departments require significant human and material resources to handle this influx of patients, but these are limited and so the medical and paramedical staff are often confronted with strain situations. To deal with these situations, emergency departments have no choice but to adapt. The main purpose of this paper is to characterize these strain situations and introduce a new framework to model these situations in an emergency department (ED) in order to improve their management by the hospital system. The proposed framework integrates an ORCA (Architecture for an Optimized and Reactive Control) approach to proactively schedule (where possible) the behaviour of a hospital system and to handle strain situations reactively. © 2013 International Institute for Innovation, Industrial Engineering and Entrepreneurship - I4e2.","corrective actions; decision support system; Emergency department; ORCA architecture; Strain situations","Artificial intelligence; Decision support systems; Industrial engineering; Patient monitoring; Corrective actions; Emergency departments; Health crisis; Material resources; Patient flow; Reactive control; Emergency rooms",Conference Paper,Scopus,2-s2.0-84899051674
"Liu F., Suk H.I., Wee C.Y., Chen H., Shen D.","High-order graph matching based feature selection for Alzheimer's disease identification.",2013,"Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention",12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897573822&partnerID=40&md5=6e6e6ce7e4269abbb63063657c646509","One of the main limitations of l1-norm feature selection is that it focuses on estimating the target vector for each sample individually without considering relations with other samples. However, it's believed that the geometrical relation among target vectors in the training set may provide useful information, and it would be natural to expect that the predicted vectors have similar geometric relations as the target vectors. To overcome these limitations, we formulate this as a graph-matching feature selection problem between a predicted graph and a target graph. In the predicted graph a node is represented by predicted vector that may describe regional gray matter volume or cortical thickness features, and in the target graph a node is represented by target vector that include class label and clinical scores. In particular, we devise new regularization terms in sparse representation to impose high-order graph matching between the target vectors and the predicted ones. Finally, the selected regional gray matter volume and cortical thickness features are fused in kernel space for classification. Using the ADNI dataset, we evaluate the effectiveness of the proposed method and obtain the accuracies of 92.17% and 81.57% in AD and MCI classification, respectively.",,"algorithm; Alzheimer disease; article; artificial intelligence; automated pattern recognition; brain; computer assisted diagnosis; human; image enhancement; methodology; nuclear magnetic resonance imaging; pathology; reproducibility; sensitivity and specificity; Algorithms; Alzheimer Disease; Artificial Intelligence; Brain; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84897573822
"Aguirre F., Sallak M., Vanderhaegen F., Berdjag D.","An evidential network approach to support uncertain multiviewpoint abductive reasoning",2013,"Information Sciences",11,10.1016/j.ins.2013.07.014,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885062828&doi=10.1016%2fj.ins.2013.07.014&partnerID=40&md5=c453a83045a9715090d11c89cb97504e","The paper proposes an approach to support human abductive reasoning in the diagnosis of a multiviewpoint system. The novelty of this work lies on the capability of the approach to treat the uncertainty held by the agent performing the diagnosis. To do so, we make use of evidential networks to represent and propagate the uncertain evidence gathered by the agent. Using forward and backward propagation of the information, the impact of the evidence over the different symptoms and causes of failure is quantified. The agent can then make use of this information as additional hints in an iterative diagnosis process until a desired degree of certainty is obtained. The model is compared with a deterministic one in which evidence is represented by binary states, that is, a symptom is either observed or not. © 2013 Elsevier Inc. All rights reserved.","Abductive reasoning; Belief functions theory; Evidential network; Model-based diagnosis; Uncertainty; Valuation based system","Abductive reasoning; Belief functions theory; Evidential networks; Model based diagnosis; Uncertainty; Valuation based systems; Artificial intelligence; Software engineering; Uncertainty analysis",Article,Scopus,2-s2.0-84885062828
"Chang F.-J., Wang K.-W.","A systematical water allocation scheme for drought mitigation",2013,"Journal of Hydrology",11,10.1016/j.jhydrol.2013.10.027,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887517081&doi=10.1016%2fj.jhydrol.2013.10.027&partnerID=40&md5=60022feb2f097340a69c9b4f4c9244f9","The severe drought events worldwide have increased the awareness of serious impacts to various social and economic sectors. It is a challenge to make efficient water resources management that optimizes economic and social well-beings under great uncertainty of hydro-meteorology. Artificial intelligence techniques possess an outstanding ability to handle non-linear complex systems. This study proposes a systematical water allocation scheme, which integrates system analysis with artificial intelligence techniques, for decision makers to mitigate drought threats. We first derive evaluation diagrams through a large number of interactive evaluations based on long-term hydrological data to provide a clear perspective of all possible drought conditions and their corresponding water shortages, and then configure neural-fuzzy networks to learning the associations between events and outcomes for estimating water deficiency levels under various hydrological conditions. The adaptive neuro-fuzzy inference system (ANFIS) is adopted to construct the mechanism between designed inputs (water discount rate and the exceedence probabilities of hydrological conditions) and simulated outputs (water deficiency levels). The water allocation in the Shihmen Reservoir watershed of northern Taiwan is used as a case study. The results suggest that the drought thresholds of reservoir storage in the beginning of the first paddy crops can be recommended as: Q50, Q60, Q70 and Q90 for precautionary, preliminary, moderate and severe drought conditions, respectively. The inference system further indicates reservoir storage is identified as the most influential variable that significantly affects water shortage. We demonstrate the proposed water allocation scheme significantly avails water managers of reliably recommending drought thresholds and determining a suitable discount rate on irrigation water supply. This study has direct bearing on more intelligent and effectual water allocation management, which is expected to substantially benefit water managers. © 2013 Elsevier B.V.","Adaptive neuro-fuzzy inference system (ANFIS); Drought mitigation; Reservoir operation; Surface water; System analysis; Water allocation","Artificial intelligence; Digital storage; Drought; Fuzzy neural networks; Fuzzy systems; Irrigation; Management; Managers; Surface waters; Systems analysis; Tracking (position); Water supply; Adaptive neuro-fuzzy inference system; Artificial intelligence techniques; Drought mitigation; Hydrological condition; Irrigation water supply; Reservoir operation; Water allocations; Water resources management; Reservoirs (water); artificial intelligence; decision making; drought; fuzzy mathematics; hydrological modeling; optimization; simulation; socioeconomic conditions; uncertainty analysis; water management; water storage; Shihmen Reservoir; Taiwan",Article,Scopus,2-s2.0-84887517081
"Varshney L.R., Pinel F., Varshney K.R., Schörgendorfer A., Chee Y.-M.","Cognition as a part of computational creativity",2013,"Proceedings of the 12th IEEE International Conference on Cognitive Informatics and Cognitive Computing, ICCI*CC 2013",11,10.1109/ICCI-CC.2013.6622223,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889064719&doi=10.1109%2fICCI-CC.2013.6622223&partnerID=40&md5=e7193fdc3b900e6844306d03e2455406","Computational creativity and cognitive computing are distinct fields that have developed in a parallel fashion. In this paper, we examine the relationship between the two, concluding that the two fields overlap in one precise way: the evaluation or assessment of artifacts with respect to creativity. Furthermore, we discuss a particular instance of computational creativity, culinary recipe design, and how cognitive informatics and cognitive computation enter into the domain. © 2013 IEEE.",,"Cognitive Computing; Cognitive informatics; Computational creativities; Information science; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84889064719
"Horalek J., Matyska J., Sobeslav V.","Communication protocols in substation automation and IEC 61850 based proposal",2013,"CINTI 2013 - 14th IEEE International Symposium on Computational Intelligence and Informatics, Proceedings",11,10.1109/CINTI.2013.6705214,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893769065&doi=10.1109%2fCINTI.2013.6705214&partnerID=40&md5=dc5669a7e3fd77ae49004a34ce902303","The automation of control systems of substations in the energy industry uses a variety of specialized standards, technologies and protocols. Among the most frequently used belong MODBUS, IEC60870, DNP3 and IEC61850 protocols. The present paper analyses and compares approaches to data communication among the abovementioned protocols with the main focus on modern standard IEC61850. The paper was created on the basis of experience while implementing data communications for ČEPS a.s., the national distributor in the Czech Republic. © 2013 IEEE.","ethernet networks; IEC standards; IP networks; network topology; power system control; substation automation","Ethernet networks; IEC standards; IP networks; Network topology; Power system controls; Substation automation; Artificial intelligence; Automation; Convolutional codes; Electric network topology; Information science; Standards; Electric substations",Conference Paper,Scopus,2-s2.0-84893769065
"Niepert M.","Symmetry-aware marginal density estimation",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893401593&partnerID=40&md5=d0426cd87fa0f5a1007acc5b12c30dbe","The Rao-Blackwell theorem is utilized to analyze and improve the scalability of inference in large probabilistic models that exhibit symmetries. A novel marginal density estimator is introduced and shown both analytically and empirically to outperform standard estimators by several orders of magnitude. The developed theory and algorithms apply to a broad class of probabilistic models including statistical relational models considered not susceptible to lifted probabilistic inference Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Marginal densities; Orders of magnitude; Probabilistic inference; Probabilistic models; Relational Model; Artificial intelligence; Inference engines",Conference Paper,Scopus,2-s2.0-84893401593
"Lelis L.H.S., Otten L., Dechter R.","Predicting the size of Depth-first Branch and Bound search trees",2013,"IJCAI International Joint Conference on Artificial Intelligence",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062564&partnerID=40&md5=65d077f0c4b60d5cc73549920b7e857c","This paper provides algorithms for predicting the size of the Expanded Search Tree (EST) of Depthfirst Branch and Bound algorithms (DFBnB) for optimization tasks. The prediction algorithm is implemented and evaluated in the context of solving combinatorial optimization problems over graphical models such as Bayesian and Markov networks. Our methods extend to DFBnB the approaches provided by Knuth-Chen schemes that were designed and applied for predicting the EST size of backtracking search algorithms. Our empirical results demonstrate good predictions which are superior to competing schemes.",,"Backtracking search algorithms; Branch and bound search; Branch-and-bound algorithms; Combinatorial optimization problems; GraphicaL model; Markov networks; Optimization task; Prediction algorithms; Artificial intelligence; Forecasting; Forestry; Linear programming; Algorithms; Algorithms; Artificial Intelligence; Forecasts; Forestry",Conference Paper,Scopus,2-s2.0-84896062564
"Yang P., Gao W.","Multi-view Discriminant Transfer learning",2013,"IJCAI International Joint Conference on Artificial Intelligence",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896061553&partnerID=40&md5=77f62e6bc95e6c9e271a8272bffb7a33","We study to incorporate multiple views of data in a perceptive transfer learning framework and propose a Multi-view Discriminant Transfer (MDT) learning approach for domain adaptation. The main idea is to find the optimal discriminant weight vectors for each view such that the correlation between the two-view projected data is maximized, while both the domain discrepancy and the view disagreement are minimized simultaneously. Furthermore, we analyze MDT theoretically from discriminant analysis perspective to explain the condition and reason, under which the proposed method is not applicable. The analytical results allow us to investigate whether there exist within-view and/or betweenview conflicts, and thus provides a deep insight into whether the transfer learning algorithm work properly or not in the view-based problems and the combined learning problem. Experiments show that MDT significantly outperforms the state-of-the-art baselines including some typical multi-view learning approaches in single- or cross-domain.",,"Analytical results; Combined Learning; Domain adaptation; Learning approach; Multi-view learning; Multiple views; Transfer learning; Weight vector; Discriminant analysis; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84896061553
"Zhang T., Ji R., Liu W., Tao D., Hua G.","Semi-supervised learning with manifold fitted graphs",2013,"IJCAI International Joint Conference on Artificial Intelligence",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062944&partnerID=40&md5=3a34ea70388549340cc1c39b49ad8b01","In this paper, we propose a locality-constrained and sparsity-encouraged manifold fitting approach, aiming at capturing the locally sparse manifold structure into neighborhood graph construction by exploiting a principled optimization model. The proposed model formulates neighborhood graph construction as a sparse coding problem with the locality constraint, therefore achieving simultaneous neighbor selection and edge weight optimization. The core idea underlying our model is to perform a sparse manifold fitting task for each data point so that close-by points lying on the same local manifold are automatically chosen to connect and meanwhile the connection weights are acquired by simple geometric reconstruction. We term the novel neighborhood graph generated by our proposed optimization model M-Fitted Graph since such a graph stems from sparse manifold fitting. To evaluate the robustness and effectiveness of M-fitted graphs, we leverage graph-based semisupervised learning as the testbed. Extensive experiments carried out on six benchmark datasets validate that the proposed M-fitted graph is superior to state-of-the-art neighborhood graphs in terms of classification accuracy using popular graph-based semi-supervised learning methods.",,"Classification accuracy; Connection weights; Manifold structures; Neighbor selection; Neighborhood graphs; Optimization modeling; Semi- supervised learning; Semi-supervised learning methods; Artificial intelligence; Classification (of information); Graphic methods; Mathematical models; Supervised learning; Graph theory",Conference Paper,Scopus,2-s2.0-84896062944
"Kabanza F., Filion J., Benaskeur A.R., Irandoust H.","Controlling the hypothesis space in probabilistic plan recognition",2013,"IJCAI International Joint Conference on Artificial Intelligence",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063299&partnerID=40&md5=d6ed72ec5503f52d14b429db32850f4b","The ability to understand the goals and plans of other agents is an important characteristic of intelligent behaviours in many contexts. One of the approaches used to endow agents with this capability is the weighted model counting approach. Given a plan library and a sequence of observations, this approach exhaustively enumerates plan execution models that are consistent with the observed behaviour. The probability that the agent might be pursuing a particular goal is then computed as a proportion of plan execution models satisfying the goal. The approach allows to recognize multiple interleaved plans, but suffers from a combinatorial explosion of plan execution models, which impedes its application to real-world domains. This paper presents a heuristic weighted model counting algorithm that limits the number of generated plan execution models in order to recognize goals quickly by computing their lower and upper bound likelihoods.",,"Combinatorial explosion; Hypothesis space; ITS applications; Lower and upper bounds; Plan libraries; Plan recognition; Real world domain; Weighted models; Artificial intelligence; Intelligent agents",Conference Paper,Scopus,2-s2.0-84896063299
"Papageorgiou E.I., Huszka C., De Roo J., Douali N., Jaulent M.-C., Colaert D.","Application of probabilistic and fuzzy cognitive approaches in semantic web framework for medical decision support",2013,"Computer Methods and Programs in Biomedicine",11,10.1016/j.cmpb.2013.07.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885431976&doi=10.1016%2fj.cmpb.2013.07.008&partnerID=40&md5=8da4504531fb5ea4e9a8ab37cfa60bef","•Bayesian belief networks (BBNs) and fuzzy cognitive maps (FCMs), as dynamic influence graphs, were applied to handle the task of medical knowledge formalization for decision support.•Both approaches were implemented in semantic web framework.•For reasoning on these knowledge models, a general purpose reasoning engine, EYE, with the necessary plug-ins was developed in the semantic web.•To assess the formal models' performance, the UTI therapy problem was selected as a proof of concept example.•The validation results showed that using probabilistic networks and fuzzy cognitive maps in semantic web framework is reliable and efficient for decision support tasks. This study aimed to focus on medical knowledge representation and reasoning using the probabilistic and fuzzy influence processes, implemented in the semantic web, for decision support tasks. Bayesian belief networks (BBNs) and fuzzy cognitive maps (FCMs), as dynamic influence graphs, were applied to handle the task of medical knowledge formalization for decision support. In order to perform reasoning on these knowledge models, a general purpose reasoning engine, EYE, with the necessary plug-ins was developed in the semantic web. The two formal approaches constitute the proposed decision support system (DSS) aiming to recognize the appropriate guidelines of a medical problem, and to propose easily understandable course of actions to guide the practitioners. The urinary tract infection (UTI) problem was selected as the proof-of-concept example to examine the proposed formalization techniques implemented in the semantic web. The medical guidelines for UTI treatment were formalized into BBN and FCM knowledge models. To assess the formal models' performance, 55 patient cases were extracted from a database and analyzed. The results showed that the suggested approaches formalized medical knowledge efficiently in the semantic web, and gave a front-end decision on antibiotics' suggestion for UTI. © 2013 Elsevier Ireland Ltd.","Bayesian networks; Decision support; Fuzzy cognitive maps; Knowledge representation","Decision support system (dss); Decision support task; Decision supports; Fuzzy cognitive map; Fuzzy cognitive maps (FCMs); Medical decision supports; Probabilistic network; Urinary tract infections; Artificial intelligence; Decision support systems; Engines; Fuzzy rules; Fuzzy systems; Knowledge representation; Bayesian networks; amoxicillin; ciprofloxacin; cotrimoxazole; fosfomycin; lomefloxacin; nitrofurantoin; norfloxacin; ofloxacin; antibiotic therapy; article; Bayesian belief network; Bayesian learning; conceptual framework; data base; decision support system; drug cost; drug hypersensitivity; drug information; Escherichia coli; fuzzy cognitive map; fuzzy system; glucose absorption; human; Human immunodeficiency virus infected patient; Internet; knowledge; leukopenia; medical decision making; medical information system; medical practice; neuritis; online system; practice guideline; pregnancy; probability; recommended drug dose; search engine; semantic web; semantics; sensitivity and specificity; single drug dose; teratogenicity; thinking; treatment duration; treatment indication; urinary tract infection; Bayesian networks; Decision support; Fuzzy cognitive maps; Knowledge representation; Bayes Theorem; Decision Support Systems, Clinical; Fuzzy Logic; Internet; Probability",Article,Scopus,2-s2.0-84885431976
"Briggs G., Scheutz M.","A hybrid architectural approach to understanding and appropriately generating indirect speech acts",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893350175&partnerID=40&md5=ed94aa9051e0a94ad30367af0df28523","Current approaches to handling indirect speech acts (ISAs) do not account for their sociolinguistic underpinnings (i.e., politeness strategies). Deeper understanding and appropriate generation of indirect acts will require mechanisms that integrate natural language (NL) understanding and generation with social information about agent roles and obligations, which we introduce in this paper. Additionally, we tackle the problem of understanding and handling indirect answers that take the form of either speech acts or physical actions, which requires an inferential, plan-reasoning approach. In order to enable artificial agents to handle an even wider-variety of ISAs, we present a hybrid approach, utilizing both the idiomatic and inferential strategies. We the n demonstrate our system successfully generating indirect requests and handling indirect answers, and discuss avenues of future research. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Agent roles; Architectural approach; Artificial agents; Hybrid approach; Natural languages; Physical action; Social information; Speech acts; Artificial intelligence; Linguistics",Conference Paper,Scopus,2-s2.0-84893350175
"Liu Y., Lu Y., Shi Q., Ding J.","Optical flow based urban road vehicle tracking",2013,"Proceedings - 9th International Conference on Computational Intelligence and Security, CIS 2013",11,10.1109/CIS.2013.89,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900656838&doi=10.1109%2fCIS.2013.89&partnerID=40&md5=4536eba6823c0157ad2552e059522d80","Vehicle tracking is an important part in intelligent transportation surveillance. But now vehicle tracking faces with the problems such as scale change, the interference of similar color, low resolution video data and so on. In this paper an improved Markov chain Monte Carlo(MCMC) named optical flow MCMC(OF-MCMC) sampling tracking algorithm is proposed for vehicle tracking. First, we use the optical flow method to get the moving direction of the vehicle in initial frames, which can solve the problem of scale change, what's more the optical flow method can get the moving speed of the vehicle which replaces the second-order autoregressive motion model owing to the non-parameter characteristic. Second, when calculating whether one particle is accepted or not, a distance factor is considered, which can relieve the interference of similar vehicle nearby. Finally, to deal with vehicle tracking in low resolution of the video data, we generate a more accurate feature template with different features weighted to get better tracking results. Experimental results show that the proposed tracking algorithm has better performance than some traditional ones. © 2013 IEEE.","Feature template; Markov chain Monte Carlo; Optical flow; Vehicle tracking","Better performance; Feature template; Improved Markov chains; Intelligent transportation; Low resolution video; Markov Chain Monte-Carlo; Optical flow methods; Tracking algorithm; Algorithms; Artificial intelligence; Motion estimation; Optical flows; Video recording; Tracking (position)",Conference Paper,Scopus,2-s2.0-84900656838
"Bühmann L., Lehmann J.","Pattern based knowledge base enrichment",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-41335-3_3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891958225&doi=10.1007%2f978-3-642-41335-3_3&partnerID=40&md5=cf97481690c83b307ff8f5253f4cf7bb","Although an increasing number of RDF knowledge bases are published, many of those consist primarily of instance data and lack sophisticated schemata. Having such schemata allows more powerful querying, consistency checking and debugging as well as improved inference. One of the reasons why schemata are still rare is the effort required to create them. In this article, we propose a semi-automatic schemata construction approach addressing this problem: First, the frequency of axiom patterns in existing knowledge bases is discovered. Afterwards, those patterns are converted to SPARQL based pattern detection algorithms, which allow to enrich knowledge base schemata. We argue that we present the first scalable knowledge base enrichment approach based on real schema usage patterns. The approach is evaluated on a large set of knowledge bases with a quantitative and qualitative result analysis. © 2013 Springer-Verlag.",,"Consistency checking; Construction approaches; Enrichment approaches; Knowledge basis; Pattern detection algorithms; Result analysis; Semi-automatics; Usage patterns; Artificial intelligence; Computer science; Computers; Knowledge based systems",Conference Paper,Scopus,2-s2.0-84891958225
"Sanker S., Cirio M.C., Vollmer L.L., Goldberg N.D., McDermott L.A., Hukriede N.A., Vogt A.","Development of high-content assays for kidney progenitor cell expansion in transgenic zebrafish",2013,"Journal of Biomolecular Screening",11,10.1177/1087057113495296,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887937302&doi=10.1177%2f1087057113495296&partnerID=40&md5=260a5d941b3cf676d4c184337161df17","Reactivation of genes normally expressed during organogenesis is a characteristic of kidney regeneration. Enhancing this reactivation could potentially be a therapeutic target to augment kidney regeneration. The inductive events that drive kidney organogenesis in zebrafish are similar to the initial steps in mammalian kidney organogenesis. Therefore, quantifying embryonic signals that drive zebrafish kidney development is an attractive strategy for the discovery of potential novel therapeutic modalities that accelerate kidney regeneration. The Lim1 homeobox protein, Lhx1, is a marker of kidney development that is also expressed in the regenerating kidneys after injury. Using a fluorescent Lhx1a-EGFP transgene whose phenotype faithfully recapitulates that of the endogenous protein, we developed a high-content assay for Lhx1a-EGFP expression in transgenic zebrafish embryos employing an artificial intelligence-based image analysis method termed cognition network technology (CNT). Implementation of the CNT assay on high-content readers enabled automated real-time in vivo time-course, dose-response, and variability studies in the developing embryo. The Lhx1a assay was complemented with a kidney-specific secondary CNT assay that enables direct measurements of the embryonic renal tubule cell population. The integration of fluorescent transgenic zebrafish embryos with automated imaging and artificial intelligence-based image analysis provides an in vivo analysis system for structure-activity relationship studies and de novo discovery of novel agents that augment innate regenerative processes. © 2013 Society for Laboratory Automation and Screening.","high-content screening; image analysis; in vivo screening; phenotypic drug discovery; Zebrafish","enhanced green fluorescent protein; histone deacetylase inhibitor; homeodomain protein; Lhx1 protein; Lhx1a protein; LIM homeodomain protein; methyl 4 (phenylthio)butanoic acid; protein; unclassified drug; uphd 25; acute kidney failure; animal cell; animal experiment; article; artificial intelligence; automation; biotechnology; cell population; cell regeneration; cognition network technology; controlled study; embryo; fluorescence imaging; gene expression; image analysis; in vitro study; in vivo study; kidney development; kidney fibrosis; kidney tubule cell; mouse; network learning; nonhuman; organogenesis; phenotype; priority journal; stem cell; stem cell expansion; structure activity relation; transgenic animal; zebra fish; high-content screening; image analysis; in vivo screening; phenotypic drug discovery; Zebrafish; Animals; Biological Assay; Cell Proliferation; Drug Evaluation, Preclinical; Embryo, Nonmammalian; Gene Expression; Green Fluorescent Proteins; Histone Deacetylase Inhibitors; Kidney; LIM-Homeodomain Proteins; Phenylbutyrates; Recombinant Fusion Proteins; Regeneration; Stem Cells; Transcription Factors; Zebrafish; Zebrafish Proteins",Article,Scopus,2-s2.0-84887937302
"Mamun M.A.A., Hannan M.A., Hussain A., Basri H.","Wireless sensor network prototype for solid waste bin monitoring with energy efficient sensing algorithm",2013,"Proceedings - 16th IEEE International Conference on Computational Science and Engineering, CSE 2013",11,10.1109/CSE.2013.65,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900362281&doi=10.1109%2fCSE.2013.65&partnerID=40&md5=1ac6371c3eab63476a35fe341bafa8cc","This paper presents a novel prototype of solid waste bin monitoring system using wireless sensor network which can respond as soon as someone throw waste insight the bins. The system architecture uses ZigBee and GSM/GPRS communication technologies and a set of carefully chosen sensors to monitor the status of solid waste bins in real time. The system is composed of three tier structure such as lower, middle and upper tier. The lower tier contains bin with sensor node installed in it to measure and transmit bin status to the next tier, the middle tier contains the gateway that stores and transmits bin data to control station and control station resides in the upper tier that stores and analyze the data for further using. An energy efficient sensing algorithm is also used in the first tier to collect the bin parameters. In this way, the system can help to minimize the operation costs and emissions by feeding the collected data to a decision support system for route optimization. © 2013 IEEE.","GSM/GPRS; Municipal solid waste; Wireless sensor network; ZigBee","Communication technologies; Control station; Energy-efficient sensing; GSM/GPRS; Monitoring system; Operation cost; Route optimization; System architectures; Algorithms; Artificial intelligence; Decision support systems; Energy efficiency; Municipal solid waste; Sensor nodes; Solid wastes; Wireless sensor networks; Zigbee; Bins",Conference Paper,Scopus,2-s2.0-84900362281
"Arantes L., Greve F., Sens P., Simon V.","Eventual leader election in evolving mobile networks",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-319-03850-6_3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893137143&doi=10.1007%2f978-3-319-03850-6_3&partnerID=40&md5=6d397badaefaac31efc497f24350a54a","Many reliable distributed services rely on an eventual leader election to coordinate actions. The eventual leader detector has been proposed as a way to implement such an abstraction. It ensures that, eventually, each process in the system will be provided by an unique leader, elected among the set of correct processes in spite of crashes and uncertainties. A number of eventual leader election protocols were suggested. Nonetheless, as far as we are aware of, no one of these protocols tolerates a free pattern of node mobility. This paper proposes a new protocol for this scenario of dynamic and mobile unknown networks. © 2013 Springer International Publishing.","asynchronous systems; dynamic networks; Fault-tolerant leader election; process mobility","Asynchronous system; Distributed service; Dynamic network; Eventual leader; Leader election; Node mobility; Process mobility; Unknown networks; Artificial intelligence; Computer science; Computers; Fault tolerant computer systems",Conference Paper,Scopus,2-s2.0-84893137143
"Kuznets R., Studer T.","Update as evidence: Belief expansion",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-35722-0_19,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892919237&doi=10.1007%2f978-3-642-35722-0_19&partnerID=40&md5=aa18375aafbfad1783fb3a84024315c9","We introduce a justification logic with a novel constructor for evidence terms, according to which the new information itself serves as evidence for believing it. We provide a sound and complete axiomatization for belief expansion and minimal change and explain how the minimality can be graded according to the strength of reasoning. We also provide an evidential analog of the Ramsey axiom. © Springer-Verlag 2013.",,"Justification logic; Minimality; Sound and complete; Artificial intelligence; Computers; Computer science",Conference Paper,Scopus,2-s2.0-84892919237
"Zhong J., Wang J., Peng W., Zhang Z., Pan Y.","Prediction of essential proteins based on gene expression programming.",2013,"BMC genomics",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903872226&partnerID=40&md5=4b91c9125077385716f1f73d4a3f737f","Essential proteins are indispensable for cell survive. Identifying essential proteins is very important for improving our understanding the way of a cell working. There are various types of features related to the essentiality of proteins. Many methods have been proposed to combine some of them to predict essential proteins. However, it is still a big challenge for designing an effective method to predict them by integrating different features, and explaining how these selected features decide the essentiality of protein. Gene expression programming (GEP) is a learning algorithm and what it learns specifically is about relationships between variables in sets of data and then builds models to explain these relationships. In this work, we propose a GEP-based method to predict essential protein by combing some biological features and topological features. We carry out experiments on S. cerevisiae data. The experimental results show that the our method achieves better prediction performance than those methods using individual features. Moreover, our method outperforms some machine learning methods and performs as well as a method which is obtained by combining the outputs of eight machine learning methods. The accuracy of predicting essential proteins can been improved by using GEP method to combine some topological features and biological features.",,"protein; Saccharomyces cerevisiae protein; algorithm; article; artificial intelligence; biological model; biology; cell survival; computer program; essential gene; gene expression; genetics; metabolism; methodology; Saccharomyces cerevisiae; Algorithms; Artificial Intelligence; Cell Survival; Computational Biology; Gene Expression; Genes, Essential; Models, Genetic; Proteins; Saccharomyces cerevisiae; Saccharomyces cerevisiae Proteins; Software",Article,Scopus,2-s2.0-84903872226
"Dang D.-C., El-Hajj R., Moukrim A.","A branch-and-cut algorithm for solving the Team Orienteering Problem",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-38171-3_23,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892921438&doi=10.1007%2f978-3-642-38171-3_23&partnerID=40&md5=c4974bc9a3afb049231585d4b0699816","This paper describes a branch-and-cut algorithm to solve the Team Orienteering Problem (TOP). TOP is a variant of the Vehicle Routing Problem (VRP) in which the aim is to maximize the total amount of collected profits from visiting customers while not exceeding the predefined travel time limit of each vehicle. In contrast to the exact solving methods in the literature, our algorithm is based on a linear formulation with a polynomial number of binary variables. The algorithm features a new set of useful dominance properties and valid inequalities. The set includes symmetric breaking inequalities, boundaries on profits, generalized subtour eliminations and clique cuts from graphs of incompatibilities. Experiments conducted on the standard benchmark for TOP clearly show that our branch-and-cut is competitive with the other methods in the literature and allows us to close 29 open instances. © Springer-Verlag 2013.","Branch-and-cut; Clique cut; Dominance property; Incompatibility","Branch-and-cut; Branch-and-cut algorithms; Clique cut; Dominance properties; Incompatibility; Linear formulation; Team orienteering problems; Vehicle routing problem; Algorithms; Artificial intelligence; Computer programming; Constraint theory; Operations research; Profitability; Combinatorial optimization",Conference Paper,Scopus,2-s2.0-84892921438
"Robak S., Franczyk B., Robak M.","Applying big data and linked data concepts in supply chains management",2013,"2013 Federated Conference on Computer Science and Information Systems, FedCSIS 2013",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892501524&partnerID=40&md5=ab45d4f7af7b90503e6f544979a9506f","One of the contemporary problems, and at the same time a big opportunity, in business networks of supply chains are the issues associated with the vast amounts of data arising there. The data may be utilized by the decision support systems in logistics; nevertheless, often there is an information integration problem. The problems with information interchange are related to issues with exchange between independently designed data systems. The networked supply chains will need appropriate IT architectures to support the cooperating business units utilizing structured and unstructured big data and the mechanisms to integrate data in heterogeneous supply chains. In this paper we analyze the capabilities of the big data technology architectures with cloud computing under usage of Linked Data in business process management in supply chains to cope with unstructured near-time data and data silos problems. We present our approach on a 4PL (Fourth-party Logistics) integrator business process example. © 2013 Polish Information Processing Society.",,"Business networks; Business Process; Business process management; Business units; Information integration; Information interchange; IT architecture; Linked datum; Artificial intelligence; Computer science; Data handling; Decision support systems; Enterprise resource management; Information systems; Network architecture; Supply chains; Information management",Conference Paper,Scopus,2-s2.0-84892501524
"Xu G., Ding Y., Zhao J., Hu L., Fu X.","A novel artificial bee colony approach of live virtual machine migration policy using bayes theorem",2013,"The Scientific World Journal",11,10.1155/2013/369209,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893862697&doi=10.1155%2f2013%2f369209&partnerID=40&md5=902f065e5324c6310b0ccf7fc526f0fd","Green cloud data center has become a research hotspot of virtualized cloud computing architecture. Since live virtual machine (VM) migration technology is widely used and studied in cloud computing, we have focused on the VM placement selection of live migration for power saving. We present a novel heuristic approach which is called PS-ABC. Its algorithm includes two parts. One is that it combines the artificial bee colony (ABC) idea with the uniform random initialization idea, the binary search idea, and Boltzmann selection policy to achieve an improved ABC-based approach with better global exploration's ability and local exploitation's ability. The other one is that it uses the Bayes theorem to further optimize the improved ABC-based process to faster get the final optimal solution. As a result, the whole approach achieves a longer-term efficient optimization for power saving. The experimental results demonstrate that PS-ABC evidently reduces the total incremental power consumption and better protects the performance of VM running and migrating compared with the existing research. It makes the result of live VM migration more high-effective and meaningful. © 2013 Gaochao Xu et al.",,"animal; article; artificial intelligence; automated pattern recognition; Bayes theorem; bee; biomimetics; computer interface; information retrieval; Internet; methodology; physiology; Animals; Artificial Intelligence; Bayes Theorem; Bees; Biomimetics; Information Storage and Retrieval; Internet; Pattern Recognition, Automated; User-Computer Interface",Article,Scopus,2-s2.0-84893862697
"Tudorache T., Nyulas C.I., Noy N.F., Musen M.A.","Using semantic web in ICD-11: Three years down the road",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-41338-4_13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891926890&doi=10.1007%2f978-3-642-41338-4_13&partnerID=40&md5=4bd183852c4c3a29d8ca3c3fa0182373","The World Health Organization is using Semantic Web technologies in the development of the 11th revision of the International Classification of Diseases (ICD-11). Health officials use ICD in all United Nations member countries to compile basic health statistics, to monitor health-related spending, and to inform policy makers. In 2010, we published a paper in the ISWC In Use track reporting on our experience in the first six months with building and deploying iCAT, a Semantic Web platform to support the collaborative authoring of ICD-11. Three years since our original publication, 270 domain experts around the world have used iCAT to author more than 45,000 classes, to perform more than 260,000 changes, and to create more than 17,000 links to external medical terminologies. During the last three years, the collaboration processes, modeling and tooling have evolved significantly, and we have learned important lessons, which we will report in this paper. We describe the benefits of using semantic technologies as an infrastructure, which proved to be critical in making support for this rapid evolution possible. To our knowledge, this effort is the only real-world project supporting the collaborative authoring of ontologies at this scale, and which, at the same time, has a high visibility and impact for the health care around the world. We believe that the insights that we gained and the lessons that we learned after four years into this large-scale project will be useful to others who need to support similar collaborative projects. © 2013 Springer-Verlag.",,"Collaboration process; Collaborative authoring; Collaborative projects; International classification of disease; Medical terminologies; Semantic technologies; Semantic Web technology; World Health Organization; Computer science; Computers; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84891926890
"Sifa R., Bauckhage C.","Archetypical motion: Supervised game behavior learning with Archetypal Analysis",2013,"IEEE Conference on Computatonal Intelligence and Games, CIG",11,10.1109/CIG.2013.6633609,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892426072&doi=10.1109%2fCIG.2013.6633609&partnerID=40&md5=11cb7d3de569e2a3cdf7dc44f9ca9694","The problem of creating believable game AI poses numerous challenges for computational intelligence research. A particular challenge consists in creating human-like behaving game bots by means of applying machine learning to game-play data recorded by human players. In this paper, we propose a novel, biologically inspired approach to behavior learning for video games. Our model is based on the idea of movement primitives and we use Archetypal Analysis to determine elementary movements from data in order to represent any player action in terms of convex combinations of archetypal motions. Given these representations, we use supervised learning in order to create a system that is able to synthesize appropriate motion behavior during a game. We apply our model to teach a first person shooter game bot how to navigate in a game environment. Our results indicate that the model is able to simulate human-like behavior at lower computational costs than previous approaches. © 2013 IEEE.",,"Archetypal analysis; Behavior learning; Biologically inspired; Computational costs; Convex combinations; First person shooter games; Game environment; Movement primitives; Artificial intelligence; Human computer interaction; Interactive computer graphics; Behavioral research",Conference Paper,Scopus,2-s2.0-84892426072
"Weng P., Zanuttini B.","Interactive value iteration for Markov Decision Processes with unknown rewards",2013,"IJCAI International Joint Conference on Artificial Intelligence",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063638&partnerID=40&md5=e87222a60edeedf2c9b29618b7a8e1c4","To tackle the potentially hard task of defining the reward function in a Markov Decision Process, we propose a new approach, based on Value Iteration, which interweaves the elicitation and optimization phases. We assume that rewards whose numeric values are unknown can only be ordered, and that a tutor is present to help comparing sequences of rewards. We first show how the set of possible reward functions for a given preference relation can be represented as a polytope. Then our algorithm, called Interactive Value Iteration, searches for an optimal policy while refining its knowledge about the possible reward functions, by querying a tutor when necessary. We prove that the number of queries needed before finding an optimal policy is upperbounded by a polynomial in the size of the problem, and we present experimental results which demonstrate that our approach is efficient in practice.",,"Hard task; Markov Decision Processes; New approaches; Numeric values; Optimal policies; Preference relation; Reward function; Value iteration; Artificial intelligence; Learning algorithms; Markov processes; Optimization; Iterative methods",Conference Paper,Scopus,2-s2.0-84896063638
"Li J., Qian X., Tang Y.Y., Yang L., Liu C.","GPS estimation from users' photos",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-35725-1_11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888371894&doi=10.1007%2f978-3-642-35725-1_11&partnerID=40&md5=0a325889a3708703698d0f9238b25375","Nowadays social media are very popular for people to share their photos with their friends. Many of the photos are geo-tagged (with GPS information) whether automatically or manually. Social media management websites such as Flickr allow users manually labeling their uploaded photos with GPS with the interface of dragging them into the map. However, manually dragging the photos to the map will bring more error and very boring for users to labeling their photos. Thus in this paper, a GPS location estimation approach is proposed. For an uploaded image, its GPS information is estimated by both hierarchical global feature classification and local feature refinement to guarantee the accuracy and computational cost. To guarantee the estimation performances, k-nearest neighbors are selected in global feature classification stage. Experiments show the effectiveness of our proposed approach. © Springer-Verlag 2013.","BoW; Geo-tag; GPS estimation; Hierarchical structure; K-NN","BoW; Computational costs; Estimation performance; Geo-tags; Global feature; Hierarchical structures; K-nearest neighbors; K-NN; Artificial intelligence; Computer science; Computers; Estimation",Conference Paper,Scopus,2-s2.0-84888371894
"Zhuo H.H., Kambhampati S.","Action-model acquisition from noisy plan traces",2013,"IJCAI International Joint Conference on Artificial Intelligence",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896060986&partnerID=40&md5=a40ac7261057b608353d17dadf8dfc74","There is increasing awareness in the planning community that the burden of specifying complete domain models is too high, which impedes the applicability of planning technology in many real-world domains. Although there have been many learning approaches that help automatically creating domain models, they all assume plan traces (training data) are correct. In this paper, we aim to remove this assumption, allowing plan traces to be with noise. Compared to collecting large amount of correct plan traces, it is much easier to collect noisy plan traces, e.g., we can directly exploit sensors to help collect noisy plan traces. We consider a novel solution for this challenge that can learn action models from noisy plan traces. We create a set of random variables to capture the possible correct plan traces behind the observed noisy ones, and build a graphical model to describe the physics of the domain. We then learn the parameters of the graphical model and acquire the domain model based on the learnt parameters. In the experiment, we empirically show that our approach is effective in several planning domains.",,"Action models; GraphicaL model; Large amounts; Learning approach; Planning community; Planning domains; Real world domain; Training data; Artificial intelligence; Graphic methods; Query languages",Conference Paper,Scopus,2-s2.0-84896060986
"Nguyen K.-H., Ock C.-Y.","Word sense disambiguation as a traveling salesman problem",2013,"Artificial Intelligence Review",11,10.1007/s10462-011-9288-9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890425797&doi=10.1007%2fs10462-011-9288-9&partnerID=40&md5=fae48f20c14c6054532d0e5c6fa0d6a0","Word sense disambiguation (WSD) is a difficult problem in Computational Linguistics, mostly because of the use of a fixed sense inventory and the deep level of granularity. This paper formulates WSD as a variant of the traveling salesman problem (TSP) to maximize the overall semantic relatedness of the context to be disambiguated. Ant colony optimization, a robust nature-inspired algorithm, was used in a reinforcement learning manner to solve the formulated TSP. We propose a novel measure based on the Lesk algorithm and Vector Space Model to calculate semantic relatedness. Our approach to WSD is comparable to state-of-the-art knowledge-based and unsupervised methods for benchmark datasets. In addition, we show that the combination of knowledge-based methods is superior to the most frequent sense heuristic and significantly reduces the difference between knowledge-based and supervised methods. The proposed approach could be customized for other lexical disambiguation tasks, such as Lexical Substitution or Word Domain Disambiguation. © 2011 Springer Science+Business Media B.V.","Ant colony optimization; Lesk algorithm; Semantic relatedness; Traveling salesman problem; Word sense disambiguation; WordNet","Knowledge-based methods; Lexical disambiguation; Nature-inspired algorithms; Semantic relatedness; Unsupervised method; Vector space models; Word Sense Disambiguation; Wordnet; Algorithms; Ant colony optimization; Artificial intelligence; Computational linguistics; Knowledge based systems; Natural language processing systems; Reinforcement learning; Traveling salesman problem; Vector spaces; Heuristic methods",Article,Scopus,2-s2.0-84890425797
"Xiong D., Zhang M.","A topic-based coherence model for statistical machine translation",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893384644&partnerID=40&md5=cd1285ef39ac9a7b094ae1b404cc9a81","Coherence that ties sentences of a text into a meaningfully connected structure is of great importance to text generation and translation. In this paper, we propose a topic-based coherence model to produce coherence for document translation, in terms of the continuity of sentence topics in a text. We automatically extract a coherence chain for each source text to be translated. Based on the extracted source coherence chain, we adopt a maximum entropy classifier to predict the target coherence chain that defines a linear topic structure for the target document. The proposed topic-based coherence model then uses the predicted target coherence chain to help decoder select coherent word/phrase translations. Our experiments show that incorporating the topic-based coherence model into machine translation achieves substantial improvement over both the baseline and previous methods that integrate document topics rather than coherence chains into machine translation. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Connected structures; Document translation; Entropy classifiers; Machine translations; Source coherence; Statistical machine translation; Text generations; Topic structures; Artificial intelligence; Computer aided language translation; Chains",Conference Paper,Scopus,2-s2.0-84893384644
"De Giacomo G., Lespérance Y., Patrizi F.","Bounded epistemic situation calculus theories",2013,"IJCAI International Joint Conference on Artificial Intelligence",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062726&partnerID=40&md5=282477eba270dc6478af362a340aab83","We define the class of e-bounded theories in the epistemic situation calculus, where the number of fluent atoms that the agent thinks may be true is bounded by a constant. Such theories can still have an infinite domain and an infinite set of states. We show that for them verification of an expressive class of first-order μ-calculus temporal epistemic properties is decidable. We also show that if the agent's knowledge in the initial situation is e-bounded and the objective part of an action theory maintains boundedness, then the entire epistemic theory is e-bounded.",,"Action theory; Boundedness; First-order; Infinite domains; Initial situation; Situation calculus; Artificial intelligence; Calculations; Formal logic",Conference Paper,Scopus,2-s2.0-84896062726
"Shaker M., Sarhan M.H., Naameh O.A., Shaker N., Togelius J.","Automatic generation and analysis of physics-based puzzle games",2013,"IEEE Conference on Computatonal Intelligence and Games, CIG",11,10.1109/CIG.2013.6633633,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892397916&doi=10.1109%2fCIG.2013.6633633&partnerID=40&md5=1cb88d241ac9206d7544c8f5bad71ec6","In this paper we present a method for the automatic generation of content for the physics-based puzzle game Cut The Rope. An evolutionary game generator is implemented which evolves the design of levels based on a context-free grammar. We present various measures for analyzing the expressivity of the generator and visualizing the space of content covered. We further perform an experiment on evolving playable content of the game and present and analyze the results obtained. © 2013 IEEE.",,"Automatic Generation; Evolutionary games; Physics-based; Puzzle games; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84892397916
"Oliehoek F.A.","Sufficient plan-time statistics for decentralized POMDPs",2013,"IJCAI International Joint Conference on Artificial Intelligence",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896061004&partnerID=40&md5=9c1fea9652327ac95eb139c31cd44fb9","Optimal decentralized decision making in a team of cooperative agents as formalized by decentralized POMDPs is a notoriously hard problem. A major obstacle is that the agents do not have access to a sufficient statistic during execution, which means that they need to base their actions on their histories of observations. A consequence is that even during off-line planning the choice of decision rules for different stages is tightly interwoven: decisions of earlier stages affect how to act optimally at later stages, and the optimal value function for a stage is known to have a dependence on the decisions made up to that point. This paper makes a contribution to the theory of decentralized POMDPs by showing how this dependence on the 'past joint policy' can be replaced by a sufficient statistic. These results are extended to the case of k-step delayed communication. The paper investigates the practical implications, as well as the effectiveness of a new pruning technique for MAA* methods, in a number of benchmark problems and discusses future avenues of research opened by these contributions.",,"Bench-mark problems; Cooperative agents; Decentralized decision making; Different stages; Off-line planning; Optimal value functions; Pruning techniques; Sufficient statistics; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84896061004
"Liang X.-D., Li L.-Y., Wu J.-G., Chen H.-N.","Mobile robot path planning based on adaptive bacterial foraging algorithm",2013,"Journal of Central South University",11,10.1007/s11771-013-1864-5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891611494&doi=10.1007%2fs11771-013-1864-5&partnerID=40&md5=33d525239e36ba54a266ae4be70f8758","The utilization of biomimicry of bacterial foraging strategy was considered to develop an adaptive control strategy for mobile robot, and a bacterial foraging approach was proposed for robot path planning. In the proposed model, robot that mimics the behavior of bacteria is able to determine an optimal collision-free path between a start and a target point in the environment surrounded by obstacles. In the simulation, two test scenarios of static environment with different number obstacles were adopted to evaluate the performance of the proposed method. Simulation results show that the robot which reflects the bacterial foraging behavior can adapt to complex environments in the planned trajectories with both satisfactory accuracy and stability. © 2013 Central South University Press and Springer-Verlag Berlin Heidelberg.","adaptation; bacterial foraging behaviors; robot path planning; swarm intelligence","adaptation; Adaptive control strategy; Bacterial foraging; Bacterial foraging algorithm; Collision-free paths; Complex environments; Robot path-planning; Swarm Intelligence; Artificial intelligence; Biomimetics; Mobile robots; Motion planning",Article,Scopus,2-s2.0-84891611494
"Zhuo H.H., Nguyen T., Kambhampati S.","Refining incomplete planning domain models through plan traces",2013,"IJCAI International Joint Conference on Artificial Intelligence",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062946&partnerID=40&md5=473c97e7ec056ba225a6a7515cef850f","Most existing work on learning planning models assumes that the entire model needs to be learned from scratch. A more realistic situation is that the planning agent has an incomplete model which it needs to refine through learning. In this paper we propose and evaluate a method for doing this. Our method takes as input an incomplete model (with missing preconditions and effects in the actions), as well as a set of plan traces that are known to be correct. It outputs a ""refined"" model that not only captures additional precondition/effect knowledge about the given actions, but also ""macro actions"". We use a MAX-SAT framework for learning, where the constraints are derived from the executability of the given plan traces, as well as the preconditions/ effects of the given incomplete model. Unlike traditional macro-action learners which use macros to increase the efficiency of planning (in the context of a complete model), our motivation for learning macros is to increase the accuracy (robustness) of the plans generated with the refined model. We demonstrate the effectiveness of our approach through a systematic empirical evaluation.",,"Empirical evaluations; Incomplete model; Max-SAT; Motivation for learning; Planning agents; Planning domains; Planning models; Refined model; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84896062946
"Le Bras R., Dilkina B., Xue Y., Gomes C.P., McKelvey K.S., Schwartz M.K., Montgomery C.A.","Robust network design for multispecies conservation",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893399067&partnerID=40&md5=f9a11bb9a4493a31b3a37997c4169e47","Our work is motivated by an important network design application in computational sustainability concerning wildlife conservation. In the face of human development and climate change, it is important that conservation plans for protecting landscape connectivity exhibit certain level of robustness. While previous work has focused on conservation strategies that result in a connected network of habitat reserves, the robustness of the proposed solutions has not been taken into account. In order to address this important aspect, we formalize the problem as a node-weighted bi-criteria network design problem with connectivity requirements on the number of disjoint paths between pairs of nodes. While in most previous work on survivable network design the objective is to minimize the cost of the selected network, our goal is to optimize the quality of the selected paths within a specified budget, while meeting the connectivity requirements. We characterize the complexity of the problem under different restrictions. We provide a mixed-integer programming encoding that allows for finding solutions with optimality guarantees, as well as a hybrid local search method with better scaling behavior but no guarantees. We evaluate the typical-case performance of our approaches using a synthetic benchmark, and apply them to a large-scale real-world network design problem concerning the conservation of wolverine and lynx populations in the U.S. Rocky Mountains (Montana). Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Computational sustainability; Conservation strategies; Landscape connectivities; Mixed-Integer Programming; Network design problems; Survivable network design; Synthetic benchmark; Wildlife conservation; Artificial intelligence; Benchmarking; Climate change; Conservation; Design; Integer programming; Optimization; Complex networks",Conference Paper,Scopus,2-s2.0-84893399067
"Kajino H., Tsuboi Y., Kashima H.","Clustering crowds",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893382910&partnerID=40&md5=0a6701e320e88c9f96e4da2679436e9c","We present a clustered personal classifier method (CPC method) that jointly estimates a classifier and clusters of workers in order to address the learning from crowds problem. Crowdsourcing allows us to create a large but low-quality data set at very low cost. The learning from crowds problem is to learn a classifier from such a lowquality data set. From some observations, we notice that workers form clusters according to their abilities. Although such a fact was pointed out several times, no method has applied it to the learning from crowds problem. We propose a CPC method that utilizes the clusters of the workers to improve the performance of the obtained classifier, where both the classifier and the clusters of the workers are estimated. The proposed method has two advantages. One is that it realizes robust estimation of the classifier because it utilizes prior knowledge about the workers that they tend to form clusters. The other is that we can obtain the clusters of the workers, which help us analyze the properties of the workers. Experimental results on synthetic and real data sets indicate that the proposed method can estimate the classifier robustly. In addition, clustering workers is shown to work well. Especially in the real data set, an outlier worker was found by applying the proposed method. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Crowdsourcing; Form clusters; Low costs; Low qualities; Prior knowledge; Real data sets; Robust estimation; Synthetic and real data; Artificial intelligence; Estimation",Conference Paper,Scopus,2-s2.0-84893382910
"Kucukkoc I., Zhang D.Z., Keedwell E.C.","Balancing parallel two-sided assembly lines with ant colony optimisation algorithm",2013,"2nd Symposium on Nature-Inspired Computing and Applications, NICA 2013 - AISB Convention 2013",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894177434&partnerID=40&md5=6fd1fd8b196bdac6bad9b157c49dce4c","Assembly lines are one of the most frequently used flow oriented production systems in industry. Although only a few researchers have studied them, two-sided assembly lines are usually utilised to produce high-volume large-sized products such as trucks and buses. In this study, more than one two-sided assembly line constructed in parallel are balanced simultaneously using a newly developed ant colony optimisation algorithm. To the best knowledge of the authors, the proposed method is the first attempt to solve the parallel two-sided assembly line balancing problem using an ant colony optimisation based algorithm. The proposed approach is also illustrated with examples from the literature to show the procedures of the algorithm.","Ant colony optimisation; Artificial intelligence; Assembly line balancing; Meta-heuristics; Parallel two-sided assembly lines",,Conference Paper,Scopus,2-s2.0-84894177434
"Rekaby A.","Directed Artificial Bat Algorithm (DABA) - A new bio-inspired algorithm",2013,"Proceedings of the 2013 International Conference on Advances in Computing, Communications and Informatics, ICACCI 2013",11,10.1109/ICACCI.2013.6637355,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891922213&doi=10.1109%2fICACCI.2013.6637355&partnerID=40&md5=a2ec29308a20c209dc8cedafbca8c508","Bio-inspired artificial algorithms topic is a relatively modern part of metaheuristic track in artificial intelligence field. All these algorithms are inspired from natural biological animals' behaviors like ants, bees, and others. The importance of these algorithms is increased by time due to the complex scientific applications, and other optimization problems demand. In this paper, we introduce a new innovative algorithm. It is titled as 'Directed Artificial Bat Algorithm' (DABA) which is inspired from bats' echo system and how they use it in prey finding. Previous initial contributions related to bat behaviors are mentioned in this paper, but they are truly abstract trials. That is why the comparisons are done versus artificial bee colony algorithm due to its maturity. According to the comparisons, proposed DABA achieves better results than ABC with efficiency enhancements between 5% and 10%. © 2013 IEEE.","Bat Algorithm; Bio-inspired Algorithms; Directed Artificial Bat Algorithm; Swarm Algorithms","Artificial bee colony algorithms; Bat algorithms; Bio-inspired algorithms; Efficiency enhancement; Innovative algorithms; Optimization problems; Scientific applications; Swarm algorithms; Artificial intelligence; Information science; Algorithms",Conference Paper,Scopus,2-s2.0-84891922213
"Garrido A.L., Buey M.G., Escudero S., Ilarri S., Mena E., Silveira S.B.","TM-Gen: A topic map generator from text documents",2013,"Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI",11,10.1109/ICTAI.2013.113,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897677714&doi=10.1109%2fICTAI.2013.113&partnerID=40&md5=d139b004d7c36d6c0d03e651d77bb4e0","The vast amount of text documents stored in digital format is growing at a frantic rhythm each day. Therefore, tools able to find accurate information searching in natural language information repositories are gaining great interest in recent years. In this context, there are especially interesting tools capable of dealing with large amounts of text information and deriving human-readable summaries. However, one step further is to be able not only to summarize, but to extract the knowledge stored in those texts, and even represent it graphically. In this paper we present an architecture to generate automatically a conceptual representation of knowledge stored in a set of text-based documents. For this purpose we have used the topic maps standard and we have developed a method that combines text mining, statistics, linguistic tools, and semantics to obtain a graphical representation of the information contained therein, which can be coded using a knowledge representation language such as RDF or OWL. The procedure is language-independent, fully automatic, self-adjusting, and it does not need manual configuration by the user. Although the validation of a graphic knowledge representation system is very subjective, we have been able to take advantage of an intermediate product of the process to make a experimental validation of our proposals. © 2013 IEEE.","Knowledge acquisition; Linguistics; Ontologies; Semantics; Text mining; Topic maps","Experimental validations; Graphical representations; Information searching; Intermediate product; Knowledge representation language; Text mining; Text-based documents; Topic Maps; Artificial intelligence; Knowledge acquisition; Knowledge representation; Linguistics; Natural language processing systems; Ontology; Semantics; Tools; Data mining",Conference Paper,Scopus,2-s2.0-84897677714
"Benamrane Y., Wybo J.-L., Armand P.","Chernobyl and Fukushima nuclear accidents: What has changed in the use of atmospheric dispersion modeling?",2013,"Journal of Environmental Radioactivity",11,10.1016/j.jenvrad.2013.07.009,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884561836&doi=10.1016%2fj.jenvrad.2013.07.009&partnerID=40&md5=a2d7d03573af0e3ab88c35ef8c0793df","The threat of a major accidental or deliberate event that would lead to hazardous materials emission in the atmosphere is a great cause of concern to societies. This is due to the potential large scale of casualties and damages that could result from the release of explosive, flammable or toxic gases from industrial plants or transport accidents, radioactive material from nuclear power plants (NPPs), and chemical, biological, radiological or nuclear (CBRN) terrorist attacks.In order to respond efficiently to such events, emergency services and authorities resort to appropriate planning and organizational patterns. This paper focuses on the use of atmospheric dispersion modeling (ADM) as a support tool for emergency planning and response, to assess the propagation of the hazardous cloud and thereby, take adequate counter measures.This paper intends to illustrate the noticeable evolution in the operational use of ADM tools over 25 y and especially in emergency situations. This study is based on data available in scientific publications and exemplified using the two most severe nuclear accidents: Chernobyl (1986) and Fukushima (2011).It appears that during the Chernobyl accident, ADM were used few days after the beginning of the accident mainly in a diagnosis approach trying to reconstruct what happened, whereas 25 y later, ADM was also used during the first days and weeks of the Fukushima accident to anticipate the potentially threatened areas.We argue that the recent developments in ADM tools play an increasing role in emergencies and crises management, by supporting stakeholders in anticipating, monitoring and assessing post-event damages. However, despite technological evolutions, its prognostic and diagnostic use in emergency situations still arise many issues. © 2013 Elsevier Ltd.","Atmospheric dispersion modeling; Decision support system; Emergency management; Nuclear accident","Atmospheric dispersion modeling; Emergency management; Fukushima nuclear accidents; Nuclear accidents; Organizational pattern; Scientific publications; Severe nuclear accident; Technological evolution; Artificial intelligence; Atmospheric movements; Biology; Decision support systems; Emergency services; Hazardous materials; Industrial plants; Nuclear power plants; Radioactive materials; Risk management; Tools; Nuclear reactor accidents; atmospheric modeling; atmospheric plume; bioterrorism; Chernobyl accident; crisis management; damage; decision support system; dispersion; explosive; hazardous waste; nuclear power plant; radioactive pollution; stakeholder; toxic material; article; atmospheric dispersion; Chernobyl accident; decision support system; emergency health service; environmental radioactivity; Fukushima nuclear accident; human; nuclear safety; plume dispersion; radiation hazard; radiation protection; Chernobyl; Fukushima; Honshu; Japan; Kiev [Ukraine]; Tohoku; Ukraine; Atmospheric dispersion modeling; Decision support system; Emergency management; Nuclear accident; Chernobyl Nuclear Accident; Fukushima Nuclear Accident; Models, Theoretical; Radiation Monitoring",Article,Scopus,2-s2.0-84884561836
"Sui X., Francois-Nienaber A., Boutilier C.","Multi-dimensional single-peaked consistency and its approximations",2013,"IJCAI International Joint Conference on Artificial Intelligence",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063114&partnerID=40&md5=931a16b4fbf81b1bdbb3b82f82134275","Single-peakedness is one of the most commonly used domain restrictions in social choice. However, the extent to which agent preferences are single-peaked in practice, and the extent to which recent proposals for approximate single-peakedness can further help explain voter preferences, is unclear. In this article, we assess the ability of both single-dimensional and multi-dimensional approximations to explain preference profiles drawn from several real-world elections. We develop a simple branch-and-bound algorithm that finds multi-dimensional, single-peaked axes that best fit a given profile, and which works with several forms of approximation. Empirical results on two election data sets show that preferences in these elections are far from single-peaked in any one-dimensional space, but are nearly single-peaked in two dimensions. Our algorithms are reasonably efficient in practice, and also show excellent anytime performance.",,"Agent preferences; Best fit; Branch-and-bound algorithms; Real-world; Social choice; Two-dimension; Algorithms; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84896063114
"Zhou Y., Nenov Y., Cuenca Grau B., Horrocks I.","Complete query answering over horn ontologies using a triple store",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-41335-3_45,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891939769&doi=10.1007%2f978-3-642-41335-3_45&partnerID=40&md5=6f5f0c3fd2c70a38e93579aec560714b","In our previous work, we showed how a scalable OWL 2 RL reasoner can be used to compute both lower and upper bound query answers over very large datasets and arbitrary OWL 2 ontologies. However, when these bounds do not coincide, there still remain a number of possible answer tuples whose status is not determined. In this paper, we show how in the case of Horn ontologies one can exploit the lower and upper bounds computed by the RL reasoner to efficiently identify a subset of the data and ontology that is large enough to resolve the status of these tuples, yet small enough so that the status can be computed using a fully-fledged OWL 2 reasoner. The resulting hybrid approach has enabled us to compute exact answers to queries over datasets and ontologies where previously only approximate query answering was possible. © 2013 Springer-Verlag.",,"Approximate query answering; Hybrid approach; Large datasets; Lower and upper bounds; Query answering; Reasoner; Triple store; Artificial intelligence; Computer science; Computers; Birds",Conference Paper,Scopus,2-s2.0-84891939769
"Pandey B., Singh D., Baghel D., Yadav J., Pattanaik M.","Clock gated low power memory implementation on virtex-6 FPGA",2013,"Proceedings - 5th International Conference on Computational Intelligence and Communication Networks, CICN 2013",11,10.1109/CICN.2013.90,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892661568&doi=10.1109%2fCICN.2013.90&partnerID=40&md5=21f9fdabb1a6f9793173af082cbdf781","In this work, Virtex-6 is Target 40nm FPGA Device. Xilinx ISE 14.1 is an ISE Design tool. RAM is a target design. Clock Gating is a technique which decreases clock power but increases Logic Power due to added Logic in Design. Irrespective of increase in number of Signal and IO buffer due to Clock Gating, there is significant decrease in IO Power and Dynamic Power due to decrease in number of frequency of device operating. The increase in Logic Power and Signal Power is relatively small in magnitude than decrease in clock power that translates to decrease in overall dynamic power. The clock power consumption of Clock Gated 65536x16-bit dual-port RAM is 38.89%(on 1GHz) and 41.3%(on 10GHz) lesser than the clock power consumption of 65536x16-bit dual-port RAM without using clock gating Techniques. © 2013 IEEE.","Clock Gating; Clock Power; Dual Port RAM; Dynamic Current; Dynamic Power; Logic Power; Signal Power","Clock gating; Dual-port RAM; Dynamic current; Dynamic Power; Logic Power; Signal power; Artificial intelligence; Design; Microprocessor chips; Random access storage; Clocks",Conference Paper,Scopus,2-s2.0-84892661568
"Sturtevant N.R., Rutherford M.J.","Minimizing writes in parallel external memory search",2013,"IJCAI International Joint Conference on Artificial Intelligence",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896060401&partnerID=40&md5=c27911076cc82d9364a7b9ab1b857bf9","Recent research on external-memory search has shown that disks can be effectively used as secondary storage when performing large breadth-first searches. We introduce the Write-Minimizing Breadth-First Search (WMBFS) algorithm which is designed to minimize the number of writes performed in an external-memory BFS. WMBFS is also designed to store the results of the BFS for later use. We present the results of a BFS on a single-agent version of Chinese Checkers and the Rubik's Cube edge cubes, state spaces with about 1 trillion states each. In evaluating against a comparable approach, WMBFS reduces the I/O for the Chinese Checkers domain by over an order of magnitude. In Rubik's cube, in addition to reducing I/O, the search is also 3.5 times faster. Analysis of the results suggests the machine and state-space properties necessary for WMBFS to perform well.",,"Breadth-first search; External memory; Recent researches; Rubik's cubes; Secondary storage; Single-agent; Artificial intelligence; Geometry",Conference Paper,Scopus,2-s2.0-84896060401
"Huang J., Nie F., Huang H.","Robust discrete matrix completion",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893418200&partnerID=40&md5=a671d5dd8b6e5199d4650931a5009b6e","Most existing matrix completion methods seek the matrix global structure in the real number domain and produce predictions that are inappropriate for applications retaining discrete structure, where an additional step is required to post-process prediction results with either heuristic threshold parameters or complicated mappings. Such an ad-hoc process is inefficient and impractical. In this paper, we propose a novel robust discrete matrix completion algorithm that produces the prediction from the collection of user specified discrete values by introducing a new discrete constraint to the matrix completion model. Our method achieves a high prediction accuracy, very close to the most optimal value of competitive methods with threshold values tuning. We solve the difficult integer programming problem via incorporating augmented Lagrangian method in an elegant way, which greatly accelerates the converge process of our method and provides the asymptotic convergence in theory. The proposed discrete matrix completion model is applied to solve three real-world applications, and all empirical results demonstrate the effectiveness of our method. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Asymptotic convergence; Augmented Lagrangian methods; Discrete constraints; Discrete structure; Integer programming problems; Matrix completion; Prediction accuracy; Threshold parameters; Artificial intelligence; Forecasting; Integer programming; Lagrange multipliers; Heuristic methods",Conference Paper,Scopus,2-s2.0-84893418200
"Poria S., Gelbukh A., Hussain A., Bandyopadhyay S., Howard N.","Music genre classification: A semi-supervised approach",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-38989-4_26,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888273177&doi=10.1007%2f978-3-642-38989-4_26&partnerID=40&md5=0904d4ebf899eb641037552e3653520c","Music genres can be seen as categorical descriptions used to classify music basing on various characteristics such as instrumentation, pitch, rhythmic structure, and harmonic contents. Automatic music genre classification is important for music retrieval in large music collections on the web. We build a classifier that learns from very few labeled examples plus a large quantity of unlabeled data, and show that our methodology outperforms existing supervised and unsupervised approaches. We also identify salient features useful for music genre classification. We achieve 97.1% accuracy of 10-way classification on real-world audio collections. © 2013 Springer-Verlag Berlin Heidelberg.",,"Categorical descriptions; Harmonic contents; Large music collections; Music genre classification; Rhythmic structures; Salient features; Semi-supervised; Unsupervised approaches; Artificial intelligence; Computer science; Pattern recognition",Conference Paper,Scopus,2-s2.0-84888273177
"Jamal S., Scaria V.","Cheminformatic models based on machine learning for pyruvate kinase inhibitors of Leishmania mexicana",2013,"BMC Bioinformatics",11,10.1186/1471-2105-14-329,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887695206&doi=10.1186%2f1471-2105-14-329&partnerID=40&md5=b7324d8db3dfe112b6db02a6654f95e1","Background: Leishmaniasis is a neglected tropical disease which affects approx. 12 million individuals worldwide and caused by parasite Leishmania. The current drugs used in the treatment of Leishmaniasis are highly toxic and has seen widespread emergence of drug resistant strains which necessitates the need for the development of new therapeutic options. The high throughput screen data available has made it possible to generate computational predictive models which have the ability to assess the active scaffolds in a chemical library followed by its ADME/toxicity properties in the biological trials. Results: In the present study, we have used publicly available, high-throughput screen datasets of chemical moieties which have been adjudged to target the pyruvate kinase enzyme of L. mexicana (LmPK). The machine learning approach was used to create computational models capable of predicting the biological activity of novel antileishmanial compounds. Further, we evaluated the molecules using the substructure based approach to identify the common substructures contributing to their activity.Conclusion: We generated computational models based on machine learning methods and evaluated the performance of these models based on various statistical figures of merit. Random forest based approach was determined to be the most sensitive, better accuracy as well as ROC. We further added a substructure based approach to analyze the molecules to identify potentially enriched substructures in the active dataset. We believe that the models developed in the present study would lead to reduction in cost and length of clinical studies and hence newer drugs would appear faster in the market providing better healthcare options to the patients. © 2013 Jamal and Scaria; licensee BioMed Central Ltd.",,"Chemical libraries; Computational model; Drug-resistant strains; Figures of merits; High-throughput screens; Machine learning approaches; Neglected tropical disease; Predictive models; Bioactivity; Computational methods; Decision trees; Enzymes; Molecules; Scaffolds; Throughput; Learning systems; Leishmania mexicana; antiprotozoal agent; molecular library; pyruvate kinase; antiprotozoal agent; pyruvate kinase; algorithm; antagonists and inhibitors; artificial intelligence; chemistry; computer simulation; drug development; economics; enzymology; human; Leishmania mexicana; leishmaniasis; molecular library; predictive value; therapeutic use; article; chemistry; drug antagonism; enzymology; Leishmania mexicana; leishmaniasis; molecular library; Algorithms; Antiprotozoal Agents; Artificial Intelligence; Computer Simulation; Drug Discovery; Humans; Leishmania mexicana; Leishmaniasis; Predictive Value of Tests; Pyruvate Kinase; Small Molecule Libraries; Algorithms; Antiprotozoal Agents; Artificial Intelligence; Computer Simulation; Drug Discovery; Humans; Leishmania mexicana; Leishmaniasis; Predictive Value of Tests; Pyruvate Kinase; Small Molecule Libraries",Article,Scopus,2-s2.0-84887695206
"Labati R.D., Genovese A., Piuri V., Scotti F.","Wildfire smoke detection using computational intelligence techniques enhanced with synthetic smoke plume generation",2013,"IEEE Transactions on Systems, Man, and Cybernetics Part A:Systems and Humans",11,10.1109/TSMCA.2012.2224335,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887103434&doi=10.1109%2fTSMCA.2012.2224335&partnerID=40&md5=da53fd1889c3bc5e60204aac4b2fd8e4","An early wildfire detection is essential in order to assess an effective response to emergencies and damages. In this paper, we propose a low-cost approach based on image processing and computational intelligence techniques, capable to adapt and identify wildfire smoke from heterogeneous sequences taken from a long distance. Since the collection of frame sequences can be difficult and expensive, we propose a virtual environment, based on a cellular model, for the computation of synthetic wildfire smoke sequences. The proposed detection method is tested on both real and simulated frame sequences. The results show that the proposed approach obtains accurate results. © 2013 IEEE.","Computer vision; Lattice Boltzmann; Neural networks; Simulation; Smoke detection; Virtual environment; Wildfire","Computational intelligence techniques; Detection methods; Early wildfire detections; Frame sequences; Lattice boltzmann; Simulation; Smoke detection; Wildfire; Artificial intelligence; Computer vision; Damage detection; Image processing; Neural networks; Smoke detectors; Virtual reality; Fires",Article,Scopus,2-s2.0-84887103434
"Carletti V., Foggia P., Percannella G., Saggese A., Vento M.","Recognition of human actions from RGB-D videos using a reject option",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-41190-8_47,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887029098&doi=10.1007%2f978-3-642-41190-8_47&partnerID=40&md5=9dda259338c0944c52a024f88e990aba","In this paper we propose a method for recognizing human actions by using depth images acquired through a Kinect sensor. The depth images are represented through the combination of three sets of well-known features, respectively based on Hu moments, depth variations and the transform, an enhanced version of the Radon transform. A GMM classifier is adopted and finally a reject option is introduced in order to improve the overall reliability of the system. The proposed approach has been tested over two datasets, the Mivia and the MHAD, showing very promising results. © 2013 Springer-Verlag.",,"Depth image; Depth variation; Hu moments; Human actions; Kinect sensors; Radon Transform; Artificial intelligence; Computer science; Image analysis",Conference Paper,Scopus,2-s2.0-84887029098
"Jannach D., Baharloo A., Williamson D.","Toward an integrated framework for declarative and interactive spreadsheet debugging",2013,"ENASE 2013 - Proceedings of the 8th International Conference on Evaluation of Novel Approaches to Software Engineering",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887109938&partnerID=40&md5=37b6dc8fdd5a657840b3e23e057f6fc6","Spreadsheet applications can nowadays be found nearly everywhere in companies and are used for a variety of purposes. Because of the high risk that arises when business decisions are based on faulty spreadsheets, in recent years new approaches for spreadsheet quality assurance have been proposed. Among them are techniques that allow for more intelligent tool support during the spreadsheet test and debugging process. The design and evaluation of such new methods and tools, which are for example based on model-based techniques is however challenging. On the one hand, real-world spreadsheets can be large and complex, thus requiring highly efficient and scalable error-location algorithms. On the other hand, as spreadsheets are usually developed by non-programmers, special care has to be taken when designing the debugging user interface. In this paper, we discuss these challenges and present the design and architecture of an integrated framework for spreadsheet debugging called EXQUISITE. Furthermore, we report details and first experimental results of a constraint-based debugging approach implemented in the framework, which supports the automated identification of possible spreadsheet errors based on user-provided test cases and model-based diagnosis techniques. Copyright © 2013 SCITEPRESS.","Artificial intelligence; Debugging; Software risk; Spreadsheets","Automated identification; Business decisions; Design and evaluations; Integrated frameworks; Model based diagnosis; Software risks; Spreadsheet applications; Spreadsheet debugging; Artificial intelligence; Computer debugging; Program diagnostics; Quality assurance; Software engineering; Spreadsheets; Tools; User interfaces; Program debugging",Conference Paper,Scopus,2-s2.0-84887109938
"Demuth A., Lopez-Herrejon R.E., Egyed A.","Supporting the co-evolution of metamodels and constraints through incremental constraint management",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-41533-3_18,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886826790&doi=10.1007%2f978-3-642-41533-3_18&partnerID=40&md5=47e98f258bbf211aa22eaf95a5da6d58","Design models must abide by constraints that can come from diverse sources, like metamodels, requirements, or the problem domain. Modelers intent to live by these constraints and thus desire automated mechanism that provide instant feedback on constraint violations. However, typical approaches assume that constraints do not evolve over time, which, unfortunately, is becoming increasingly unrealistic. For example, the co-evolution of metamodels and models requires corresponding constraints to be co-evolved continuously. This demands efficient constraint adaptation mechanisms to ensure that validated constraints are up-to-date. This paper presents an approach based on constraint templates that tackles this evolution scenario by automatically updating constraints. We developed the Cross-Layer Modeler (XLM) approach which relies on incremental consistency-checking. As a case study, we performed evolutions of the UML-metamodel and 21 design models. Our approach is sound and the empirical evaluation shows that it is near instant and scales with increasing model sizes. © 2013 Springer-Verlag.","Co-evolution; consistency-checking; metamodeling","Adaptation mechanism; Co-evolution; Consistency checking; Constraint management; Constraint violation; Empirical evaluations; Metamodeling; Problem domain; Artificial intelligence; Computer science; Models",Conference Paper,Scopus,2-s2.0-84886826790
"Baur T., Damian I., Lingenfelser F., Wagner J., André E.","NovA: Automated analysis of nonverbal signals in social interactions",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-319-02714-2_14,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886863959&doi=10.1007%2f978-3-319-02714-2_14&partnerID=40&md5=85c400629f1d4df8a3165181611fc050","Previous studies have shown that the success of interpersonal interaction depends not only on the contents we communicate explicitly, but also on the social signals that are conveyed implicitly. In this paper, we present NovA (NOnVerbal behavior Analyzer), a system that analyzes and facilitates the interpretation of social signals conveyed by gestures, facial expressions and others automatically as a basis for computer-enhanced social coaching. NovA records data of human interactions, automatically detects relevant behavioral cues as a measurement for the quality of an interaction and creates descriptive statistics for the recorded data. This enables us to give a user online generated feedback on strengths and weaknesses concerning his social behavior, as well as elaborate tools for offline analysis and annotation. © 2013 Springer International Publishing.",,"Automated analysis; Descriptive statistics; Facial Expressions; Human interactions; Non-verbal signals; Nonverbal behavior; Off-line analysis; Social interactions; Artificial intelligence; Computer science; Social sciences",Conference Paper,Scopus,2-s2.0-84886863959
"Rubio E., Castillo O.","Interval type-2 fuzzy clustering for membership function generation",2013,"Proceedings of the 2013 IEEE Workshop on Hybrid Intelligent Models and Applications, HIMA 2013 - 2013 IEEE Symposium Series on Computational Intelligence, SSCI 2013",11,10.1109/HIMA.2013.6615017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886706960&doi=10.1109%2fHIMA.2013.6615017&partnerID=40&md5=3c1225c58cbdd81d21b1fc73ca2e781c","This paper presents the basic theory of the Fuzzy C-Means (FCM) algorithm, as well as the proposed IT2 FCM algorithm, which is an extension of the FCM algorithm, that implements techniques of type-2 fuzzy sets, this in order to improve fuzzy data clustering, being able to handle this algorithm with higher degree of uncertainty and be less prone to noise. The approach is illustrated with plots of clusters generated by the IT2 FCM algorithm and memberships functions of type-2, this was done to observe if the Type-2 membership functions generated by the membership matrices produced by the IT2 FCM algorithm for lower and upper limits of the range, present a significant footprint of uncertainty. © 2013 IEEE.","fuzzy clustering; interval type-2 fuzzy clustering; type-2 fuzzy logic","Degree of uncertainty; FCM algorithm; Footprint of uncertainties; Fuzzy C-means algorithms; Interval type-2 fuzzy; Memberships function; Type-2 fuzzy logic; Type-2 fuzzy set; Artificial intelligence; Fuzzy clustering; Fuzzy logic; Membership functions; Clustering algorithms",Conference Paper,Scopus,2-s2.0-84886706960
"Cambria E., Howard N., Hsu J., Hussain A.","Sentic blending: Scalable multimodal fusion for the continuous interpretation of semantics and sentics",2013,"Proceedings of the 2013 IEEE Symposium on Computational Intelligence for Human-Like Intelligence, CIHLI 2013 - 2013 IEEE Symposium Series on Computational Intelligence, SSCI 2013",11,10.1109/CIHLI.2013.6613272,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886665707&doi=10.1109%2fCIHLI.2013.6613272&partnerID=40&md5=407674d615041792d3fd5ffd2399912f","The capability of interpreting the conceptual and affective information associated with natural language through different modalities is a key issue for the enhancement of human-agent interaction. The proposed methodology, termed sentic blending, enables the continuous interpretation of semantics and sentics (i.e., the conceptual and affective information associated with natural language) based on the integration of an affective common-sense knowledge base with any multimodal signal-processing module. In this work, in particular, sentic blending is interfaced with a facial emotional classifier and an opinion mining engine. One of the main distinguishing features of the proposed technique is that it does not simply perform cognitive and affective classification in terms of discrete labels, but it operates in a multidimensional space that enables the generation of a continuous stream characterising user's semantic and sentic progress over time, despite the outputs of the unimodal categorical modules have very different time-scales and output labels. © 2013 IEEE.","Affective common-sense; Emotion recognition; Facial expression analysis; Multimodal fusion; SenticNet","Affective common-sense; Emotion recognition; Facial expression analysis; Multi-modal fusion; SenticNet; Artificial intelligence; Blending; Knowledge based systems; Signal processing; Semantics",Conference Paper,Scopus,2-s2.0-84886665707
"Liu R., Hu J.","DNABind: A hybrid algorithm for structure-based prediction of DNA-binding residues by combining machine learning- and template-based approaches",2013,"Proteins: Structure, Function and Bioinformatics",11,10.1002/prot.24330,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885856027&doi=10.1002%2fprot.24330&partnerID=40&md5=0944417f3b8a8afd86af2bf8060c1940","Accurate prediction of DNA-binding residues has become a problem of increasing importance in structural bioinformatics. Here, we presented DNABind, a novel hybrid algorithm for identifying these crucial residues by exploiting the complementarity between machine learning- and template-based methods. Our machine learning-based method was based on the probabilistic combination of a structure-based and a sequence-based predictor, both of which were implemented using support vector machines algorithms. The former included our well-designed structural features, such as solvent accessibility, local geometry, topological features, and relative positions, which can effectively quantify the difference between DNA-binding and nonbinding residues. The latter combined evolutionary conservation features with three other sequence attributes. Our template-based method depended on structural alignment and utilized the template structure from known protein-DNA complexes to infer DNA-binding residues. We showed that the template method had excellent performance when reliable templates were found for the query proteins but tended to be strongly influenced by the template quality as well as the conformational changes upon DNA binding. In contrast, the machine learning approach yielded better performance when high-quality templates were not available (about 1/3 cases in our dataset) or the query protein was subject to intensive transformation changes upon DNA binding. Our extensive experiments indicated that the hybrid approach can distinctly improve the performance of the individual methods for both bound and unbound structures. DNABind also significantly outperformed the state-of-art algorithms by around 10% in terms of Matthews's correlation coefficient. The proposed methodology could also have wide application in various protein functional site annotations. DNABind is freely available at http://mleg.cse.sc.edu/DNABind/. © 2013 Wiley Periodicals, Inc.","Conformational change; DNA-binding residue; Machine learning; Protein-DNA interaction; Structural analysis; Template","solvent; algorithm; article; conformational transition; DNA binding; DNA protein complex; DNA template; genetic conservation; geometry; hybrid; learning algorithm; machine learning; prediction; priority journal; protein conformation; support vector machine; conformational change; DNA-binding residue; machine learning; protein-DNA interaction; structural analysis; template; Algorithms; Artificial Intelligence; DNA; Protein Binding; Protein Conformation; Proteins",Article,Scopus,2-s2.0-84885856027
"Duan H., Wei X., Dong Z.","Multiple UCAVs cooperative air combat simulation platform based on PSO, ACO, and game theory",2013,"IEEE Aerospace and Electronic Systems Magazine",11,10.1109/MAES.2013.6678487,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890448604&doi=10.1109%2fMAES.2013.6678487&partnerID=40&md5=ee9151f06ab57aaf969b8dd976653ecd","In this work, we have developed a multiple UCAVs cooperative air combat simulation platform, which is based on PSO, ACO, and game theory. The Matlab program is used as the developing tool. In this platform, the practitioners can investigate the inherent mechanism by applying game theory to solve the mission decision-making problem of multiple UCAVs in attacking multiple objects. This simulation platform is friendly, easy to use, and easy to modify © 2013 IEEE.",,"Cooperative air combats; Decision-making problem; MATLAB program; Multiple objects; Simulation platform; Artificial intelligence; Game theory; MATLAB; Decision making",Article,Scopus,2-s2.0-84890448604
"Delgrande J.P., Wassermann R.","Horn clause contraction functions",2013,"Journal of Artificial Intelligence Research",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888372777&partnerID=40&md5=dc61872753f392ce23b2fcde261e11f8","In classical, AGM-style belief change, it is assumed that the underlying logic contains classical propositional logic. This is clearly a limiting assumption, particularly in Artificial Intelligence. Consequently there has been recent interest in studying belief change in approaches where the full expressivity of classical propositional logic is not obtained. In this paper we investigate belief contraction in Horn knowledge bases. We point out that the obvious extension to the Horn case, involving Horn remainder sets as a starting point, is problematic. Not only do Horn remainder sets have undesirable properties, but also some desirable Horn contraction functions are not captured by this approach. For Horn belief set contraction, we develop an account in terms of a model-theoretic characterisation involving weak remainder sets. Maxichoice and partial meet Horn contraction is specified, and we show that the problems arising with earlier work are resolved by these approaches. As well, constructions of the specific operators and sets of postulates are provided, and representation results are obtained. We also examine Horn package contraction, or contraction by a set of formulas. Again, we give a construction and postulate set, linking them via a representation result. Last, we investigate the closely-related notion of forgetting in Horn clauses. This work is arguably interesting since Horn clauses have found widespread use in AI; as well, the results given here may potentially be extended to other areas which make use of Horn-like reasoning, such as logic programming, rule-based systems, and description logics. Finally, since Horn reasoning is weaker than classical reasoning, this work sheds light on the foundations of belief change. © 2013 AI Access Foundation.",,"Belief change; Belief contraction; Classical propositional logic; Description logic; Knowledge basis; Model-theoretic; Rule-based system; Specific operators; Artificial intelligence; Data description; Knowledge representation; Logic programming; Formal logic",Article,Scopus,2-s2.0-84888372777
"Jakobsen M.R., Hornbaek K.","Interactive visualizations on large and small displays: The interrelation of display size, information space, and scale",2013,"IEEE Transactions on Visualization and Computer Graphics",11,10.1109/TVCG.2013.170,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886452177&doi=10.1109%2fTVCG.2013.170&partnerID=40&md5=e32e40bc33f0c0ef1f14e415ae48ecc8","In controlled experiments on the relation of display size (i.e., the number of pixels) and the usability of visualizations, the size of the information space can either be kept constant or varied relative to display size. Both experimental approaches have limitations. If the information space is kept constant then the scale ratio between an overview of the entire information space and the lowest zoom level varies, which can impact performance; if the information space is varied then the scale ratio is kept constant, but performance cannot be directly compared. In other words, display size, information space, and scale ratio are interrelated variables. We investigate this relation in two experiments with interfaces that implement classic information visualization techniques-focus+context, overview+detail, and zooming-for multi-scale navigation in maps. Display size varied between 0.17, 1.5, and 13.8 megapixels. Information space varied relative to display size in one experiment and was constant in the other. Results suggest that for tasks where users navigate targets that are visible at all map scales the interfaces do not benefit from a large display: With a constant map size, a larger display does not improve performance with the interfaces; with map size varied relative to display size, participants found interfaces harder to use with a larger display and task completion times decrease only when they are normalized to compensate for the increase in map size. The two experimental approaches show different interaction effects between display size and interface. In particular, focus+context performs relatively worse at a large display size with variable map size, and relatively worse at a small display size with a fixed map size. Based on a theoretical analysis of the interaction with the visualization techniques, we examine individual task actions empirically so as to understand the relative impact of display size and scale ratio on the visualization techniques' performance and to discuss differences between the two experimental approaches. © 1995-2012 IEEE.","experimental method; Information visualization; interaction techniques; multi-scale navigation; user studies","Controlled experiment; Experimental approaches; Experimental methods; Information visualization; Interaction techniques; Interactive visualizations; User study; Visualization technique; Experiments; Information analysis; Information systems; Visualization; algorithm; artificial intelligence; computer interface; human; physiology; task performance; vision; article; physiology; vision; Algorithms; Artificial Intelligence; Humans; Task Performance and Analysis; User-Computer Interface; Visual Perception; Algorithms; Artificial Intelligence; Humans; Task Performance and Analysis; User-Computer Interface; Visual Perception",Article,Scopus,2-s2.0-84886452177
"Sugaya N.","Training based on ligand efficiency improves prediction of bioactivities of ligands and drug target proteins in a machine learning approach",2013,"Journal of Chemical Information and Modeling",11,10.1021/ci400240u,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887036726&doi=10.1021%2fci400240u&partnerID=40&md5=15edcb89aa7c9f2ecb9291e294c3635d","Machine learning methods based on ligand-protein interaction data in bioactivity databases are one of the current strategies for efficiently finding novel lead compounds as the first step in the drug discovery process. Although previous machine learning studies have succeeded in predicting novel ligand-protein interactions with high performance, all of the previous studies to date have been heavily dependent on the simple use of raw bioactivity data of ligand potencies measured by IC50, EC50, Ki, and Kd deposited in databases. ChEMBL provides us with a unique opportunity to investigate whether a machine-learning-based classifier created by reflecting ligand efficiency other than the IC50, EC50, Ki, and Kd values can also offer high predictive performance. Here we report that classifiers created from training data based on ligand efficiency show higher performance than those from data based on IC 50 or Ki values. Utilizing GPCRSARfari and KinaseSARfari databases in ChEMBL, we created IC50- or Ki-based training data and binding efficiency index (BEI) based training data then constructed classifiers using support vector machines (SVMs). The SVM classifiers from the BEI-based training data showed slightly higher area under curve (AUC), accuracy, sensitivity, and specificity in the cross-validation tests. Application of the classifiers to the validation data demonstrated that the AUCs and specificities of the BEI-based classifiers dramatically increased in comparison with the IC50- or Ki-based classifiers. The improvement of the predictive power by the BEI-based classifiers can be attributed to (i) the more separated distributions of positives and negatives, (ii) the higher diversity of negatives in the BEI-based training data in a feature space of SVMs, and (iii) a more balanced number of positives and negatives in the BEI-based training data. These results strongly suggest that training data based on ligand efficiency as well as data based on classical IC50, EC50, Kd, and Ki values are important when creating a classifier using a machine learning approach based on bioactivity data. © 2013 American Chemical Society.",,"Binding efficiency; Cross-validation tests; Drug discovery process; Ligand-protein interaction; Machine learning approaches; Machine learning methods; Predictive performance; Support vector machine (SVMs); Bioactivity; Database systems; Efficiency; Ligands; Proteins; Support vector machines; Classification (of information); G protein coupled receptor; ligand; protein kinase; area under the curve; article; artificial intelligence; chemical database; chemistry; data mining; drug antagonism; drug database; drug development; drug potentiation; human; IC 50; molecular library; principal component analysis; sensitivity and specificity; support vector machine; Area Under Curve; Artificial Intelligence; Data Mining; Databases, Chemical; Databases, Pharmaceutical; Drug Discovery; Humans; Inhibitory Concentration 50; Ligands; Principal Component Analysis; Protein Kinases; Receptors, G-Protein-Coupled; Sensitivity and Specificity; Small Molecule Libraries; Support Vector Machines",Article,Scopus,2-s2.0-84887036726
"Yokota F., Okada T., Takao M., Sugano N., Tada Y., Tomiyama N., Sato Y.","Automated CT segmentation of diseased hip using hierarchical and conditional statistical shape models",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-40763-5_24,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885901033&doi=10.1007%2f978-3-642-40763-5_24&partnerID=40&md5=ee4bdc27db24423d67163daf5e927ff0","Segmentation of the femur and pelvis is a prerequisite for patient-specific planning and simulation for hip surgery. Accurate boundary determination of the femoral head and acetabulum is the primary challenge in diseased hip joints because of deformed shapes and extreme narrowness of the joint space. To overcome this difficulty, we investigated a multi-stage method in which the hierarchical hip statistical shape model (SSM) is initially utilized to complete segmentation of the pelvis and distal femur, and then the conditional femoral head SSM is used under the condition that the regions segmented during the previous stage are known. CT data from 100 diseased patients categorized on the basis of their disease type and severity, which included 200 hemi-hips, were used to validate the method, which delivered significantly increased segmentation accuracy for the femoral head. © 2013 Springer-Verlag.",,"Boundary determination; CT segmentation; Deformed shape; Multi-stage methods; Patient specific; Planning and simulations; Segmentation accuracy; Statistical shape model; Artificial intelligence; Computer science",Conference Paper,Scopus,2-s2.0-84885901033
"Holzinger A., Bruschi M., Eder W.","On interactive data visualization of physiological low-cost-sensor data with focus on mental stress",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-40511-2_34,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885805089&doi=10.1007%2f978-3-642-40511-2_34&partnerID=40&md5=8292f47c012852876c9b8e8ab9adcec0","Emotions are important mental and physiological states influencing perception and cognition and have been a topic of interest in Human-Computer Interaction (HCI) for some time. Popular examples include stress detection or affective computing. The use of emotional effects for various applications in decision support systems is of increasing interest. Emotional and affective states represent very personal data and could be used for burn-out prevention. In this paper we report on first results and experiences of our EMOMES project, where the goal was to design and develop an end-user centered mobile software for interactive visualization of physiological data. Our solution was a star-plot visualization, which has been tested with data from N=50 managers (aged 25-55) taken during a burn-out prevention seminar. The results demonstrate that the leading psychologist could obtain insight into the data appropriately, thereby providing support in the prevention of stress and burnout syndromes. © 2013 IFIP International Federation for Information Processing.","BVP; Data visualization; EDA; HRV; Knowledge Discovery; low-cost sensor; Stress","Affective Computing; BVP; EDA; HRV; Human computer interaction (HCI); Interactive visualizations; Low-cost sensors; Perception and cognition; Artificial intelligence; Behavioral research; Data mining; Decision support systems; Human computer interaction; Information systems; Physiology; Sensors; Stresses; Visualization; Data visualization",Conference Paper,Scopus,2-s2.0-84885805089
"Griol D., Carbó J., Molina J.M.","An automatic dialog simulation technique to develop and evaluate interactive conversational agents",2013,"Applied Artificial Intelligence",11,10.1080/08839514.2013.835230,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886054143&doi=10.1080%2f08839514.2013.835230&partnerID=40&md5=0f3142050cb5db3208db2d1bee74d803","During recent years, conversational agents have become a solution to provide straightforward and more natural ways of retrieving information in the digital domain. In this article, we present an agent-based dialog simulation technique for learning new dialog strategies and evaluating conversational agents. Using this technique, the effort necessary to acquire data required to train the dialog model and then explore new dialog strategies is considerably reduced. A set of measures has also been defined to evaluate the dialog strategy that is automatically learned and to compare different dialog corpora. We have applied this technique to explore the space of possible dialog strategies and evaluate the dialogs acquired for a conversational agent that collects monitored data from patients suffering from diabetes. The results of the comparison of these measures for an initial corpus and a corpus acquired using the dialog simulation technique show that the conversational agent reduces the time needed to complete the dialogs and improve their quality, thereby allowing the conversational agent to tackle new situations and generate new coherent answers for the situations already present in an initial model. © 2013 Copyright Taylor and Francis Group, LLC.",,"Agent based; Conversational agents; Dialog modeling; Digital domain; Simulation technique; Artificial intelligence; Computer simulation",Article,Scopus,2-s2.0-84886054143
"Wang H.-J., Zhang G.-Q., Qi H.-N.","Wood Recognition Using Image Texture Features",2013,"PLoS ONE",11,10.1371/journal.pone.0076101,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885395589&doi=10.1371%2fjournal.pone.0076101&partnerID=40&md5=99ea9b7281386f160c6ff5bdac9c9f79","Inspired by theories of higher local order autocorrelation (HLAC), this paper presents a simple, novel, yet very powerful approach for wood recognition. The method is suitable for wood database applications, which are of great importance in wood related industries and administrations. At the feature extraction stage, a set of features is extracted from Mask Matching Image (MMI). The MMI features preserve the mask matching information gathered from the HLAC methods. The texture information in the image can then be accurately extracted from the statistical and geometrical features. In particular, richer information and enhanced discriminative power is achieved through the length histogram, a new histogram that embodies the width and height histograms. The performance of the proposed approach is compared to the state-of-the-art HLAC approaches using the wood stereogram dataset ZAFU WS 24. By conducting extensive experiments on ZAFU WS 24, we show that our approach significantly improves the classification accuracy. © 2013 Wang et al.",,"analytic method; article; extraction; geometry; height; higher local order autocorrelation; histogram; image analysis; recognition; stereoradiography; wood; Algorithms; Artificial Intelligence; Databases, Factual; Humans; Image Processing, Computer-Assisted; Pattern Recognition, Automated; Wood",Article,Scopus,2-s2.0-84885395589
"Lambrix P., Kaliyaperumal R.","A session-based approach for aligning large ontologies",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-38288-8-4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885002404&doi=10.1007%2f978-3-642-38288-8-4&partnerID=40&md5=828f5f4f9bbf7fb50cfeebffcf1128c4","There are a number of challenges that need to be addressed when aligning large ontologies. Previous work has pointed out scalability and efficiency of matching techniques, matching with background knowledge, support for matcher selection, combination and tuning, and user involvement as major requirements. In this paper we address these challenges. Our first contribution is an ontology alignment framework that enables solutions to each of the challenges. This is achieved by introducing different kinds of interruptable sessions. The framework allows partial computations for generating mapping suggestions, partial validations of mappings suggestions and use of validation decisions in (re)computation of mapping suggestions and the recommendation of alignment strategies to use. Further, we describe an implemented system providing solutions to each of the challenges and show through experiments the advantages of the session-based approach. © 2013 Springer-Verlag Berlin Heidelberg.",,"Back-ground knowledge; Matching techniques; Ontology alignment; Partial computation; Show through; User involvement; Artificial intelligence; Computer science; Mapping",Conference Paper,Scopus,2-s2.0-84885002404
"Torres Vieira H., Thudichum Vasconcelos V.","Typing progress in communication-centred systems",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-38493-6_17,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885011159&doi=10.1007%2f978-3-642-38493-6_17&partnerID=40&md5=b2f38a9bbb3ed14ff4cd1b2b3065710b","We present a type system for the analysis of progress in session-based communication centred systems. Our development is carried out in a minimal setting considering classic (binary) sessions, but building on and generalising previous work on progress analysis in the context of conversation types. Our contributions aim at underpinning forthcoming works on progress for session-typed systems, so as to support richer verification procedures based on a more foundational approach. Although this work does not target expressiveness, our approach already addresses challenging scenarios which are unaccounted for elsewhere in the literature, in particular systems that interleave communications on received session channels. © 2013 IFIP International Federation for Information Processing.",,"Type systems; Artificial intelligence; Computer science; Communication",Conference Paper,Scopus,2-s2.0-84885011159
"Peska L., Vojtas P.","Enhancing recommender system with linked open data",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-40769-7_42,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884973056&doi=10.1007%2f978-3-642-40769-7_42&partnerID=40&md5=4b22c2cbee30bea6b3672fd76bd80c9e","In this paper, we present an innovative method to use Linked Open Data (LOD) to improve content based recommender systems. We have selected the domain of secondhand bookshops, where recommending is extraordinary difficult because of high ratio of objects/users, lack of significant attributes and small number of the same items in stock. Those difficulties prevents us from successfully apply both collaborative and common content based recommenders. We have queried Czech language mutation of DBPedia in order to receive additional attributes of objects (books) to reveal nontrivial connections between them. Our approach is general and can be applied on other domains as well. Experiments show that enhancing recommender system with LOD can significantly improve its results in terms of object similarity computation and top-k objects recommendation. The main drawback hindering widespread of such systems is probably missing data about considerable portion of objects, which can however vary across domains and improve over time. © 2013 Springer-Verlag Berlin Heidelberg.","content based similarity; implicit user preference; Linked Open Data; Recommender systems","Content-based; Content-based recommender systems; Czech language; implicit user preference; Innovative method; Linked open data (LOD); Linked open datum; Similarity computation; Artificial intelligence; Computer science; Recommender systems",Conference Paper,Scopus,2-s2.0-84884973056
"Barata G., Gama S., Jorge J., Gonçalves D.","So fun it hurts - Gamifying an engineering course",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-39454-6_68,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884831739&doi=10.1007%2f978-3-642-39454-6_68&partnerID=40&md5=3cef927154750b4c0de1377a6c206064","Good games are good motivators by nature, as they make players feel rewarded and fulfilled, which pushes them forward to persist and resist frustration. Gamification is a novel technique that uses game elements like points and badges, to motivated and engage users into embracing new behaviors, such as improving one's health condition, finances or productivity. In this paper, we present an experiment in which an MSc college course was gamified to improve student interest and engagement. The gamified course led to better learning results and participation. However, there were several negative side effects that detracted from the overall experience. We will describe them, identifying their causes and describe possible alternatives to better tailor the gamified experience, stemming from the analysis of the data gathered so far. © 2013 Springer-Verlag Berlin Heidelberg.","Education Gamification; Motivation; Perils; Student engagement","College course; Engineering course; Gamification; Health condition; Negative side effects; Novel techniques; Perils; Student engagement; Artificial intelligence; Computer science; Motivation; Students",Conference Paper,Scopus,2-s2.0-84884831739
"Castelli M., Castaldi D., Giordani I., Silva S., Vanneschi L., Archetti F., Maccagnola D.","An efficient implementation of geometric semantic genetic programming for anticoagulation level prediction in pharmacogenetics",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-40669-0_8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884729122&doi=10.1007%2f978-3-642-40669-0_8&partnerID=40&md5=913a906790ebfff04fdbb54d8bed12c2","The purpose of this study is to develop an innovative system for Coumarin-derived drug dosing, suitable for elderly patients. Recent research highlights that the pharmacological response of the patient is often affected by many exogenous factors other than the dosage prescribed and these factors could form a very complex relationship with the drug dosage. For this reason, new powerful computational tools are needed for approaching this problem. The system we propose is called Geometric Semantic Genetic Programming, and it is based on the use of recently defined geometric semantic genetic operators. In this paper, we present a new implementation of this Genetic Programming system, that allow us to use it for real-life applications in an efficient way, something that was impossible using the original definition. Experimental results show the suitability of the proposed system for managing anticoagulation therapy. In particular, results obtained with Geometric Semantic Genetic Programming are significantly better than the ones produced by standard Genetic Programming both on training and on out-of-sample test data. © 2013 Springer-Verlag.",,"Anticoagulation therapy; Complex relationships; Computational tools; Efficient implementation; Genetic programming system; Geometric semantics; Pharmacological response; Real-life applications; Artificial intelligence; Drug dosage; Geometry; Semantics; Genetic programming",Conference Paper,Scopus,2-s2.0-84884729122
"Vizzari M., Modica G.","Environmental effectiveness of swine sewage management: A multicriteria ahp-based model for a reliable quick assessment",2013,"Environmental Management",11,10.1007/s00267-013-0149-y,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885484531&doi=10.1007%2fs00267-013-0149-y&partnerID=40&md5=b0c0bd3d80fa03b10f37d4dadbdd75df","Environmental issues related to swine production are still a major concern for the general public and represent a key challenge for the swine industry. The environmental impact of higher livestock concentration is particularly significant where it coincides with weaker policy standards and poor manure management. Effective tools for environmental monitoring of the swine sewage management process become essential for verifying the environmental compatibility of farming facilities and for defining suitable policies aimed at increasing swine production sustainability. This research aims at the development and application of a model for a quick assessment of the environmental effectiveness of the pig farming sewage management process. In order to define the model, multicriteria techniques, and in particular, Saaty's analytic hierarchy process, were used to develop an iterative process in which the various key factors influencing the process under investigation were analyzed. The model, named EASE (Environmental Assessment of Sewages management Effectiveness), was optimized and applied to the Lake Trasimeno basin (Umbria, Italy), an area of high natural, environmental and aesthetic value. In this context, inadequate disposal of pig sewage represents a potential source of very considerable pollution. The results have demonstrated how the multicriteria model can represent a very effective and adaptable tool also in those decision-making processes aimed at the sustainable management of livestock production. © 2013 Springer Science+Business Media New York.","Analytic hierarchy process (AHP); Decision support systems (DSS); Environmental assessment and modelling; Livestock sewage management; Multicriteria decision analysis (MCDA)","Analytic hierarchy process (ahp); Decision support system (dss); Environmental assessment; Multi-criteria decision analysis; Sewage management; Agriculture; Analytic hierarchy process; Artificial intelligence; Decision support systems; Environmental impact; Iterative methods; Research and development management; Sewage; Sustainable development; Tools; Waste disposal; Environmental management; analytical hierarchy process; decision analysis; decision support system; environmental impact assessment; environmental monitoring; livestock farming; multicriteria analysis; numerical model; sewage disposal; waste management; article; composting; environmental impact assessment; environmental management; environmental sustainability; fertigation; Italy; livestock; manure; pig farming; policy; pollution monitoring; purification; sewage disposal; Italy; Lake Trasimeno; Perugia; Umbria; Suidae; Animals; Environment; Italy; Sewage; Swine; Waste Management",Article,Scopus,2-s2.0-84885484531
"Pérez M., Bueno M.A., Escalona M., Toorop P., Rodríguez R., Cañal M.J.","Temporary immersion systems (RITA®) for the improvement of cork oak somatic embryogenic culture proliferation and somatic embryo production",2013,"Trees - Structure and Function",11,10.1007/s00468-013-0876-y,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884204406&doi=10.1007%2fs00468-013-0876-y&partnerID=40&md5=30bcb318d61b4e8368d872b78f2fa964","Somatic embryogenesis in cork oak (Quercus suber L.) is an efficient tool that allows the production of large number of embryos from selected quality and productive trees. Temporary immersion systems (TIS) are an alternative to semi-solid or liquid culture that combine the advantages of liquid culture and avoid the associated problems. Parameters that affect the TIS multiplication efficiency of Q. suber L. embryogenic cultures were evaluated. Immersion frequencies of 1 min every 6 or 4 h increased the fresh weight 3.7 or 7.5-fold compared with an immersion frequency of 1 min every 12 h or cultures on semi-solid medium, respectively. The cellular fate of embryogenic cultures was also affected by the immersion frequency, 1 min every 6 h was the best for mass propagation of proliferative developmental stages (embryogenic calli and embryo clusters) while 1 min every 4 h promoted the formation of single, fully developed cotyledonary embryos. An initial amount of 1.5 g fresh weight of proliferative tissues produced the best results in RITA® containers while 0.5 g of embryogenic callus was the best for semi-solid cultures. © 2013 Springer-Verlag Berlin Heidelberg.","Culture parameters; Embryogenic callus; Immersion frequency; Liquid medium; Quercus suber; Repetitive somatic embryogenesis; Semi-solid medium","Culture parameters; Embryogenic callus; Immersion frequency; Liquid medium; Quercus; Semi-solids; Somatic embryogenesis; Animal cell culture; Artificial intelligence; Liquids; Plants (botany); Quercus suber",Article,Scopus,2-s2.0-84884204406
"Smarsly K., Law K.H.","A migration-based approach towards resource-efficient wireless structural health monitoring",2013,"Advanced Engineering Informatics",11,10.1016/j.aei.2013.08.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888006658&doi=10.1016%2fj.aei.2013.08.003&partnerID=40&md5=b328f154a42aa1f8fa9f7d784157fa1e","Wireless sensor networks have emerged as a complementary technology to conventional, cable-based systems for structural health monitoring. However, the wireless transmission of sensor data and the on-board execution of engineering analyses directly on the sensor nodes can consume a significant amount of the inherently restricted node resources. This paper presents an agent migration approach towards resource-efficient wireless sensor networks. Autonomous software agents, referred to as ""on-board agents"", are embedded into the wireless sensor nodes employed for structural health monitoring performing simple resource-efficient routines to continuously analyze, aggregate, and communicate the sensor data to a central server. Once potential anomalies are detected in the observed structural system, the on-board agents autonomously request for specialized software programs (""migrating agents"") that physically migrate to the sensor nodes to analyze the suspected anomaly on demand. In addition to the localized data analyses, a central information pool available on the central server is accessible by the software agents (and by human users), facilitating a distributed-cooperative assessment of the global condition of the monitored structure. As a result of this study, a 95% reduction of memory utilization and a 96% reduction of power consumption of the wireless sensor nodes have been achieved as compared with traditional approaches. © 2013 Elsevier Ltd. All rights reserved.","Distributed artificial intelligence; Dynamic wireless code migration; Mobile multi-agent systems; Smart structures; Structural health monitoring; Wireless sensor networks","Autonomous software agents; Code migration; Distributed Artificial Intelligence; Engineering analysis; Mobile multi-agent systems; Traditional approaches; Wireless structural health monitoring; Wireless transmissions; Autonomous agents; Data processing; Intelligent structures; Multi agent systems; Sensors; Structural health monitoring; Wireless sensor networks; Sensor nodes",Article,Scopus,2-s2.0-84888006658
"Lötsch J., Ultsch A.","A machine-learned knowledge discovery method for associating complex phenotypes with complex genotypes. Application to pain",2013,"Journal of Biomedical Informatics",11,10.1016/j.jbi.2013.07.010,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883752835&doi=10.1016%2fj.jbi.2013.07.010&partnerID=40&md5=187e7f06115ba99121761d14ca5b10c2","Background: The association of genotyping information with common traits is not satisfactorily solved. One of the most complex traits is pain and association studies have failed so far to provide reproducible predictions of pain phenotypes from genotypes in the general population despite a well-established genetic basis of pain. We therefore aimed at developing a method able to prospectively and highly accurately predict pain phenotype from the underlying genotype. Methods: Complex phenotypes and genotypes were obtained from experimental pain data including four different pain stimuli and genotypes with respect to 30 reportedly pain relevant variants in 10 genes. The training data set was obtained in 125 healthy volunteers and the independent prospective test data set was obtained in 89 subjects. The approach involved supervised machine learning. Results: The phenotype-genotype association was reached in three major steps. First, the pain phenotype data was projected and clustered by means of emergent self-organizing map (ESOM) analysis and subsequent U-matrix visualization. Second, pain sub-phenotypes were identified by interpreting the cluster structure using classification and regression tree classifiers. Third, a supervised machine learning algorithm (Unweighted Label Rule generation) was applied to genetic markers reportedly modulating pain to obtain a complex genotype underlying the identified subgroups of subjects with homogenous pain response. This procedure correctly identified 80% of the subjects as belonging to an extreme pain phenotype in an independently and prospectively assessed cohort. Conclusion: The developed methodology is a suitable basis for complex genotype-phenotype associations in pain. It may provide personalized treatments of complex traits. Due to its generality, this new method should also be applicable to other association tasks except pain. © 2013 Elsevier Inc.","Genetics; Knowledge-generation; Machine-learning","Classification and regression tree; Genetics; Genotype-phenotype associations; Healthy volunteers; Knowledge discovery method; Knowledge-generation; Machine-learning; Supervised machine learning; Conformal mapping; Data visualization; Supervised learning; Health; article; genotype; nociception; pain; phenotype; priority journal; probability; Genetics; Knowledge-generation; Machine-learning; Artificial Intelligence; Genotype; Humans; Pain Management; Phenotype",Article,Scopus,2-s2.0-84883752835
"Jalali V., Leake D.","Extending case adaptation with automatically-generated ensembles of adaptation rules",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-39056-2_14,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884505902&doi=10.1007%2f978-3-642-39056-2_14&partnerID=40&md5=17086fedc6ebd9ccafe1ee4786f360e3","Case-based regression often relies on simple case adaptation methods. This paper investigates new approaches to enriching the adaptation capabilities of case-based regression systems, based on the use of ensembles of adaptation rules generated from the case base. The paper explores both local and global methods for generating adaptation rules from the case base, and presents methods for ranking the generated rules and combining the resulting ensemble of adaptation rules to generate new solutions. It tests these methods in five standard domains, evaluating their performance compared to four baseline methods, standard k-NN, linear regression, locally weighted linear regression, and an ensemble of k-NN predictors with different feature subsets. The results demonstrate that the proposed method generally outperforms the baselines and that the accuracy of adaptation based on locally-generated rules is highly competitive with that of global rule-generation methods with much greater computational cost. © 2013 Springer-Verlag.",,"Adaptation rules; Baseline methods; Case adaptation; Computational costs; Feature subset; Global methods; New approaches; Standard domains; Artificial intelligence; Computer science",Conference Paper,Scopus,2-s2.0-84884505902
"Wang X., Matwin S., Japkowicz N., Liu X.","Cost-sensitive boosting algorithms for imbalanced multi-instance datasets",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-38457-8_15,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884493455&doi=10.1007%2f978-3-642-38457-8_15&partnerID=40&md5=fd1c99d5270eb91504cc009d3602785a","Multi-instance learning is different than standard propositional classification, because it uses a set of bags containing many instances as input. The instances in each bag are not labeled, but the bags themselves are labeled positive or negative. Our research shows that classification of multi-instance data with imbalanced class distributions significantly decreases the performance normally achievable by most multi-instance algorithms, which is the same as the performance of most standard, single-instance classifier learning algorithms. In this paper, we present and analyze this multi-instance class imbalance problem, and propose a novel solution framework. We focus on how to utilize the extended AdaBoost techniques applicable to most multi-instance classifier learning algorithms. Cost-sensitive boosting algorithms are developed by introducing cost items into the learning framework of AdaBoost, to enable classification of imbalanced multi-instance datasets. © 2013 Springer-Verlag.","AdaBoost; class imbalance problem; cost-sensitive learning; multi-instance classification","Boosting algorithm; Class imbalance problems; Classifier learning; Cost-sensitive; Cost-sensitive learning; Imbalanced class; Learning frameworks; Multi-instance learning; Adaptive boosting; Algorithms; Artificial intelligence; Costs; Classification (of information)",Conference Paper,Scopus,2-s2.0-84884493455
"Delling D., Goldberg A.V., Werneck R.F.","Hub label compression",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-38527-8_4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884400100&doi=10.1007%2f978-3-642-38527-8_4&partnerID=40&md5=e9593f95fc1e77b1b0710c5bb6727ebc","The hub labels (HL) algorithm is the fastest known technique for computing driving times on road networks, but its practical applicability can be limited by high space requirements relative to the best competing methods. We develop compression techniques that substantially reduce HL space requirements with a small performance penalty. © 2013 Springer-Verlag.",,"Compression techniques; Performance penalties; Road network; Space requirements; Artificial intelligence; Computer science; Algorithms",Conference Paper,Scopus,2-s2.0-84884400100
"Poongavanam V., Kongsted J.","Virtual Screening Models for Prediction of HIV-1 RT Associated RNase H Inhibition",2013,"PLoS ONE",11,10.1371/journal.pone.0073478,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884186947&doi=10.1371%2fjournal.pone.0073478&partnerID=40&md5=321b3d6e03c9fd598e406759fa0e7d94","The increasing resistance to current therapeutic agents for HIV drug regiment remains a major problem for effective acquired immune deficiency syndrome (AIDS) therapy. Many potential inhibitors have today been developed which inhibits key cellular pathways in the HIV cycle. Inhibition of HIV-1 reverse transcriptase associated ribonuclease H (RNase H) function provides a novel target for anti-HIV chemotherapy. Here we report on the applicability of conceptually different in silico approaches as virtual screening (VS) tools in order to efficiently identify RNase H inhibitors from large chemical databases. The methods used here include machine-learning algorithms (e.g. support vector machine, random forest and kappa nearest neighbor), shape similarity (rapid overlay of chemical structures), pharmacophore, molecular interaction fields-based fingerprints for ligands and protein (FLAP) and flexible ligand docking methods. The results show that receptor-based flexible docking experiments provides good enrichment (80-90%) compared to ligand-based approaches such as FLAP (74%), shape similarity (75%) and random forest (72%). Thus, this study suggests that flexible docking experiments is the model of choice in terms of best retrieval of active from inactive compounds and efficiency and efficacy schemes. Moreover, shape similarity, machine learning and FLAP models could also be used for further validation or filtration in virtual screening processes. The best models could potentially be use for identifying structurally diverse and selective RNase H inhibitors from large chemical databases. In addition, pharmacophore models suggest that the inter-distance between hydrogen bond acceptors play a key role in inhibition of the RNase H domain through metal chelation. © 2013 Poongavanam, Kongsted.",,"anti human immunodeficiency virus agent; capravirine; efavirenz; etraviridine; gw 8248; illimaquinone; naphthyridine derivative; nevirapine; nsc 727447; pd 126338; ribonuclease H; RNA directed DNA polymerase; RNA directed DNA polymerase inhibitor; triazole; unclassified drug; antiviral therapy; article; chelation; chemical database; chemical structure; computer model; computer program; drug efficacy; drug identification; enzyme activity; enzyme inhibition; Human immunodeficiency virus 1; Human immunodeficiency virus infection; hydrogen bond; k nearest neighbor; molecular docking; molecular interaction; nonhuman; pharmacophore; protein targeting; random forest; structure activity relation; support vector machine; validation process; virtual reality; Artificial Intelligence; Enzyme Inhibitors; HIV Reverse Transcriptase; Models, Theoretical; Protein Binding; Protein Structure, Secondary; Ribonuclease H",Article,Scopus,2-s2.0-84884186947
"Perhinschi M.G., Moncayo H., Al Azzawi D.","Development of immunity-based framework for aircraft abnormal conditions detection, identification, evaluation, and accommodation",2013,"AIAA Guidance, Navigation, and Control (GNC) Conference",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883699249&partnerID=40&md5=367ffa41db8c3f928a918a554b3e25be","This paper presents the development of a biologically inspired generalized conceptual framework for the detection, identification, evaluation, and accommodation of aircraft sub-system abnormal conditions. The artificial immune system paradigm in conjunction with other artificial intelligence techniques, analytical tools, and heuristics are used in an attempt to provide a comprehensive solution to the problem of safely operating aircraft under abnormal flight conditions. The main concepts and foundations are established and methodologies and algorithms for implementation are outlined. The approach addresses directly the complexity and multi-dimensionality of aircraft dynamic response in the context of abnormal conditions and is expected to facilitate the design of on-board augmentation systems to increase aircraft survivability, improve operation safety, and optimize performance at both normal and abnormal/upset conditions.",,"Abnormal conditions; Aircraft survivability; Artificial Immune System; Artificial intelligence techniques; Augmentation systems; Biologically inspired; Conceptual frameworks; Multidimensionality; Aircraft; Artificial intelligence; Dynamic response; Identification (control systems); Aircraft detection",Conference Paper,Scopus,2-s2.0-84883699249
"Wardeh M., Wyner A., Atkinson K., Bench-Capon T.","Argumentation based tools for policy-making",2013,"Proceedings of the International Conference on Artificial Intelligence and Law",11,10.1145/2514601.2514640,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883531427&doi=10.1145%2f2514601.2514640&partnerID=40&md5=970c32b90e824357ea3521a36fcd1d01","Citizens have a variety of ways to consult with their representatives about policy proposals, seeking justifications, objecting to all or part of it, or making a counter-proposal. For the first, the representative needs only to state a justification. For the second, the representative would want to understand the objections, which may involve asking some questions. For the third, the citizen would have to provide a well formulated proposal that can then be critiqued from the standpoint of the government's own policy proposal. At the end of such a consultation, users will have aired their proposals, understood the implications, and received feedback on how their proposals contrast to that of the government.",,"Policy making; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84883531427
"Bergmann K., Macedonia M.","A virtual agent as vocabulary trainer: Iconic gestures help to improve learners' memory performance",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-40415-3_12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883418892&doi=10.1007%2f978-3-642-40415-3_12&partnerID=40&md5=42bc2f08eb3894a83093223a82e98123","An important and often laborious task in foreign language acquisition is vocabulary learning. Research has repeatedly demonstrated that performing iconic gestures together with novel words has a beneficial effect on learning performance. Can these findings be transferred onto virtual agents applied in gesture-supported vocabulary training? We present a study investigating whether iconic gestures performed by a virtual agent and imitated by learners have an impact on verbal memory for words in a foreign language. In a within-subject design we compared participants' memory performance achieved with the help of a virtual agent and those achieved with the help of a human trainer regarding both short-term learning effects and long-term decay effects. The overall results demonstrate improved memory scores when participants learned with a virtual agent. Especially high performers could profit from gesture-supported training with a virtual agent. © 2013 Springer-Verlag.","iconic gestures; memory performance; Vocabulary acquisition","Beneficial effects; Foreign language; Iconic gestures; Learning performance; Memory performance; Short term learning; Vocabulary acquisition; Vocabulary learning; Artificial intelligence; Computer science; Intelligent virtual agents",Conference Paper,Scopus,2-s2.0-84883418892
"Ososky S., Schuster D., Phillips E., Jentsch F.","Building appropriate trust in human-robot teams",2013,"AAAI Spring Symposium - Technical Report",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883411398&partnerID=40&md5=3f0816e32c9657ddf45a2935a01349a9","Future robotic systems are expected to transition from tools to teammates, characterized by increasingly autonomous, intelligent robots interacting with humans in a more naturalistic manner, approaching a relationship more akin to human-human teamwork. Given the impact of trust observed in other systems, trust in the robot team member will likely be critical to effective and safe performance. Our thesis for this paper is that trust in a robot team member must be appropriately calibrated rather than simply maximized. We describe how the human team member's understanding of the system contributes to trust in human-robot teaming, by evoking mental model theory. We discuss how mental models are related to physical and behavioral characteristics of the robot, on the one hand, and affective and behavioral outcomes, such as trust and system use/disuse/misuse, on the other. We expand upon our discussion by providing recommendations for best practices in human-robot team research and design and other systems using artificial intelligence. © 2013, Association for the Advancement of artificial intelligence.",,"Behavioral characteristics; Behavioral outcomes; Best practices; Human-robot-team; Mental model; Robot teams; Robotic systems; Team members; Artificial intelligence; Models; Robots",Conference Paper,Scopus,2-s2.0-84883411398
"Hilscher M., Linker S., Olderog E.-R.","Proving safety of traffic manoeuvres on country roads",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-39698-4_12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883246869&doi=10.1007%2f978-3-642-39698-4_12&partnerID=40&md5=ad7504e1c7571774365fb8293807352e","We adapt the Multi-lane Spatial Logic MLSL, introduced in [1] for proving the safety (collision freedom) of traffic manoeuvres on multi-lane motorways, where all cars drive in one direction, to the setting of country roads with two-way traffic. To this end, we need suitably refined sensor functions and length measurement in MLSL. Our main contribution is to show that also here we can separate the purely spatial reasoning from the underlying car dynamics in the safety proof. © 2013 Springer-Verlag.",,"Country roads; Length measurement; Sensor function; Spatial logic; Spatial reasoning; Artificial intelligence; Computer science; Rural roads",Conference Paper,Scopus,2-s2.0-84883246869
"Bermbach D., Kuhlenkamp J.","Consistency in distributed storage systems: An overview of models, metrics and measurement approaches",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-40148-0_13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883155191&doi=10.1007%2f978-3-642-40148-0_13&partnerID=40&md5=cf319febf2734b5940fe37ef856b04c0","Due to the advent of eventually consistent storage systems, consistency has become a focus of research. Still, a clear overview of consistency in distributed systems is missing. In this work, we define and describe consistency, show how different consistency models and perspectives are related and briefly discuss how concrete consistency guarantees of a distributed storage system can be measured. © 2013 Springer-Verlag.","Consistency; Distributed Systems","Consistency; Consistency model; Distributed storage system; Distributed systems; Focus of researches; Storage systems; Artificial intelligence; Computer science; Multiprocessing systems",Conference Paper,Scopus,2-s2.0-84883155191
"Grzegorzewski P.","Probabilistic implications",2013,"Fuzzy Sets and Systems",11,10.1016/j.fss.2013.01.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887066785&doi=10.1016%2fj.fss.2013.01.003&partnerID=40&md5=fff60c277f650287e7ad5e0a16dc0626","A new family of implication operators, called probabilistic implications, is discussed. The suggested implications are based on conditional copulas and make a bridge between probability theory and fuzzy logic. It is shown that probabilistic fuzzy implications have some interesting properties, especially those connected with the dependence structure of the underlying environment. Moreover, some other approaches for combining probability and theory of fuzzy implications are considered. In particular, it is shown that in probabilistic environment S-implications coincide with QL-implications. © 2013 Elsevier B.V.","Approximate reasoning; Conditional probability; Copula; Dependence; Fuzzy implication; S-implication","Approximate reasoning; Conditional probabilities; Copula; Dependence; Fuzzy implications; S implications; Artificial intelligence; Fuzzy sets; Fuzzy logic",Article,Scopus,2-s2.0-84887066785
"Leydesdorff L., Strand O.","The Swedish system of innovation: Regional synergies in a knowledge-based economy",2013,"Journal of the American Society for Information Science and Technology",11,10.1002/asi.22895,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882448837&doi=10.1002%2fasi.22895&partnerID=40&md5=bb196d00f3d1ffb3af75115c2330e32a","Based on the complete set of firm data for Sweden (N = 1,187,421; November 2011), we analyze the mutual information among the geographical, technological, and organizational distributions in terms of synergies at regional and national levels. Using this measure, the interaction among three dimensions can become negative and thus indicate a net export of uncertainty by a system or, in other words, synergy in how knowledge functions are distributed over the carriers. Aggregation at the regional level (NUTS3) of the data organized at the municipal level (NUTS5) shows that 48.5% of the regional synergy is provided by the 3 metropolitan regions of Stockholm, Gothenburg, and Malmö/Lund. Sweden can be considered a centralized and hierarchically organized system. Our results accord with other statistics, but this triple helix indicator measures synergy more specifically and quantitatively. The analysis also provides us with validation for using this measure in previous studies of more regionalized systems of innovation (such as Hungary and Norway). © 2013 ASIS&T.","innovation","Knowledge based economy; Metropolitan regions; Municipal levels; Mutual informations; Other statistics; Regional levels; Three dimensions; Triple helixes; Artificial intelligence; Innovation; Software engineering",Article,Scopus,2-s2.0-84882448837
"Bosman P.A., Grahl J., Thierens D.","Benchmarking parameter-free AMaLGaM on functions with and without noise.",2013,"Evolutionary computation",11,10.1162/EVCO_a_00094,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887646546&doi=10.1162%2fEVCO_a_00094&partnerID=40&md5=3f62243c6698e44fb0fc0ad35a12edc6","We describe a parameter-free estimation-of-distribution algorithm (EDA) called the adapted maximum-likelihood Gaussian model iterated density-estimation evolutionary algorithm (AMaLGaM-ID[Formula: see text]A, or AMaLGaM for short) for numerical optimization. AMaLGaM is benchmarked within the 2009 black box optimization benchmarking (BBOB) framework and compared to a variant with incremental model building (iAMaLGaM). We study the implications of factorizing the covariance matrix in the Gaussian distribution, to use only a few or no covariances. Further, AMaLGaM and iAMaLGaM are also evaluated on the noisy BBOB problems and we assess how well multiple evaluations per solution can average out noise. Experimental evidence suggests that parameter-free AMaLGaM can solve a wide range of problems efficiently with perceived polynomial scalability, including multimodal problems, obtaining the best or near-best results among all algorithms tested in 2009 on functions such as the step-ellipsoid and Katsuuras, but failing to locate the optimum within the time limit on skew Rastrigin-Bueche separable and Lunacek bi-Rastrigin in higher dimensions. AMaLGaM is found to be more robust to noise than iAMaLGaM due to the larger required population size. Using few or no covariances hinders the EDA from dealing with rotations of the search space. Finally, the use of noise averaging is found to be less efficient than the direct application of the EDA unless the noise is uniformly distributed. AMaLGaM was among the best performing algorithms submitted to the BBOB workshop in 2009.",,"algorithm; article; artificial intelligence; biology; computer program; computer simulation; methodology; normal distribution; statistical model; Algorithms; Artificial Intelligence; Computational Biology; Computer Simulation; Likelihood Functions; Normal Distribution; Software",Article,Scopus,2-s2.0-84887646546
"Zhang X., Yuen S.Y.","Improving artificial bee colony with one-position inheritance mechanism",2013,"Memetic Computing",11,10.1007/s12293-013-0117-3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881609736&doi=10.1007%2fs12293-013-0117-3&partnerID=40&md5=1c89e0aa86c5ca6898071c2d79e8f64d","Artificial bee colony (ABC) algorithm simulates the foraging behavior of honey bees. It shows good performance in many application problems and large scale optimization problems. However, variation of a solution in the ABC algorithm is only employed on one dimension of the solution. This would sometimes hamper the convergence speed of the ABC algorithm, especially for large scale optimization. This paper proposes a one-position inheritance (OPI) mechanism to overcome this drawback. The OPI mechanism aims to promote information exchange amongst employed bees of the ABC algorithm. For separable function, OPIABC has a higher probability resulting in function value improvement of the worst positions than ABC. Through one-position information exchange, the OPI mechanism can assist the ABC algorithm to find promising solutions. This mechanism has been tested on a set of 25 test functions with D= 30 and on CEC 2008 test suite with D= 100 and 1,000. Experimental results show that the OPI mechanism can speed up the convergence of the ABC algorithm. After the use of OPI, the performance of the ABC algorithm is significantly improved for both rotated problems and large scale problems. OPIABC is also competitive on both test suites comparing with other recently proposed swarm intelligence metaheuristics (e.g. SaDE and PSO2011). Furthermore, the OPI mechanism can greatly enhance the performance of other improved ABC algorithms. © 2013 Springer-Verlag Berlin Heidelberg.","Artificial bee colony; Global optimization; Information exchange; Large scale optimization; Real parameter optimization; Swarm intelligence","Artificial bee colonies; Information exchanges; Large-scale optimization; Real-parameter optimization; Swarm Intelligence; Artificial intelligence; Global optimization; Information dissemination; Optimization; Algorithms",Article,Scopus,2-s2.0-84881609736
"Lian J., He L., Ma B., Li H., Peng W.","Optimal sensor placement for large structures using the nearest neighbour index and a hybrid swarm intelligence algorithm",2013,"Smart Materials and Structures",11,10.1088/0964-1726/22/9/095015,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884541233&doi=10.1088%2f0964-1726%2f22%2f9%2f095015&partnerID=40&md5=9de1662869be57057eeb5ba2cc87b554","Research on optimal sensor placement (OSP) has become very important due to the need to obtain effective testing results with limited testing resources in health monitoring. In this study, a new methodology is proposed to select the best sensor locations for large structures. First, a novel fitness function derived from the nearest neighbour index is proposed to overcome the drawbacks of the effective independence method for OSP for large structures. This method maximizes the contribution of each sensor to modal observability and simultaneously avoids the redundancy of information between the selected degrees of freedom. A hybrid algorithm combining the improved discrete particle swarm optimization (DPSO) with the clonal selection algorithm is then implemented to optimize the proposed fitness function effectively. Finally, the proposed method is applied to an arch dam for performance verification. The results show that the proposed hybrid swarm intelligence algorithm outperforms a genetic algorithm with decimal two-dimension array encoding and DPSO in the capability of global optimization. The new fitness function is advantageous in terms of sensor distribution and ensuring a well-conditioned information matrix and orthogonality of modes, indicating that this method may be used to provide guidance for OSP in various large structures. © 2013 IOP Publishing Ltd.",,"Clonal selection algorithms; Discrete particle swarm optimization; Effective independence methods; Information matrix; Optimal sensor placement; Performance verification; Sensor distributions; Swarm intelligence algorithms; Algorithms; Artificial intelligence; Global optimization; Health; Particle swarm optimization (PSO); Sensors",Article,Scopus,2-s2.0-84884541233
"Gretz F., Katoen J.-P., McIver A.","Prinsys - On a quest for probabilistic loop invariants",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-40196-1_17,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882748646&doi=10.1007%2f978-3-642-40196-1_17&partnerID=40&md5=fef3b1b5ea82f6ed000ebfb36652ead9","Prinsys (pronounced ""princess"") is a new software-tool for probabilistic invariant synthesis. In this paper we discuss its implementation and improvements of the methodology which was set out in previous work. In particular we have substantially simplified the method and generalised it to non-linear programs and invariants. Prinsys follows a constraint-based approach. A given parameterised loop annotation is speculatively placed in the program. The tool returns a formula that captures precisely the invariant instances of the given candidate. Our approach is sound and complete. Prinsys's applicability is evaluated on several examples. We believe the tool contributes to the successful analysis of sequential probabilistic programs with infinite-domain variables and parameters. © 2013 Springer-Verlag.","invariant generation; non-linear constraint solving; probabilistic programs","Constraint-based; Invariant generations; Loop invariants; Non-linear constraints; Probabilistic programs; Sound and complete; Artificial intelligence; Computer science; Tools",Conference Paper,Scopus,2-s2.0-84882748646
"Garla V.N., Brandt C.","Knowledge-based biomedical word sense disambiguation: An evaluation and application to clinical document classification",2013,"Journal of the American Medical Informatics Association",11,10.1136/amiajnl-2012-001350,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882749026&doi=10.1136%2famiajnl-2012-001350&partnerID=40&md5=86c7c8cad302ddccc9b04a003ca78e92","Background: Word sense disambiguation (WSD) methods automatically assign an unambiguous concept to an ambiguous term based on context, and are important to many text-processing tasks. In this study we developed and evaluated a knowledge-based WSD method that uses semantic similarity measures derived from the Unified Medical Language System (UMLS) and evaluated the contribution of WSD to clinical text classification. Methods: We evaluated our system on biomedical WSD datasets and determined the contribution of our WSD system to clinical document classification on the 2007 Computational Medicine Challenge corpus. Results: Our system compared favorably with other knowledge-based methods. Machine learning classifiers trained on disambiguated concepts significantly outperformed those trained using all concepts. Conclusions: We developed a WSD system that achieves high disambiguation accuracy on standard biomedical WSD datasets and showed that our WSD system improves clinical document classification. Data sharing: We integrated our WSD system with MetaMap and the clinical Text Analysis and Knowledge Extraction System, two popular biomedical natural language processing systems. All codes required to reproduce our results and all tools developed as part of this study are released as open source, available under http://code.google.com/p/ytex.",,"article; automation; clinical classification; machine learning; medical documentation; word sense disambiguation; Natural Language Processing; Semantic similarity; Word Sense Disambiguation; Artificial Intelligence; Data Mining; Knowledge Bases; Literature; Medical Subject Headings; Natural Language Processing; Semantics; Unified Medical Language System",Article,Scopus,2-s2.0-84882749026
"Cruz D.P.F., Maia R.D., Szabo A., De Castro L.N.","A bee-inspired algorithm for optimal data clustering",2013,"2013 IEEE Congress on Evolutionary Computation, CEC 2013",11,10.1109/CEC.2013.6557953,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881576389&doi=10.1109%2fCEC.2013.6557953&partnerID=40&md5=35791dd8691bd40cf57642e4e05d881d","The amount of data generated in different knowledge areas has made it necessary the use of data mining tools capable of automatically analyzing and extracting knowledge from datasets. Clustering is one of the most important tasks in data mining and can be defined as the process of partitioning objects into groups or clusters, such that objects in the same group are more similar to one another than to objects belonging to other groups. In this context, this paper aims to propose an adaptation of a bee-inspired optimization algorithm so that it is able to solve data clustering problems. The algorithm was run for different datasets and the results obtained showed high quality clusters and diversity of solutions, whilst a suitable number of clusters was automatically determined. © 2013 IEEE.","bee-inspired algorithms; dynamic size population; optimal data clustering; swarm intelligence","Bee-inspired algorithms; Data-mining tools; Diversity of solutions; Dynamic sizes; Number of clusters; Optimal data; Optimization algorithms; Swarm Intelligence; Artificial intelligence; Cluster analysis; Data mining; Clustering algorithms",Conference Paper,Scopus,2-s2.0-84881576389
"Liang X., Li W., Zhang Y., Zhong Y., Zhou M.","Recent advances in particle swarm optimization via population structuring and individual behavior control",2013,"2013 10th IEEE International Conference on Networking, Sensing and Control, ICNSC 2013",11,10.1109/ICNSC.2013.6548790,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881295400&doi=10.1109%2fICNSC.2013.6548790&partnerID=40&md5=73129a0fd4a8c3671b658e86117c8006","Particle swarm optimization (PSO) is an import bionic algorithm, inspired by the behaviors of gregarious colony such as bees, birds and fish. Since PSO was proposed in 1995 as a kind of swarm intelligence, many improved versions have been developed from different angles. In a swarm, population structures and individual behavior are the key elements for it to evolve. Therefore, in this paper we classify recent PSOs according to their development. Population structures are the foundation of a swarm. Thus some developments are discussed in accordance with the classification of single population and multiple sub-populations. Then the researches on static and dynamic topologies are also reviewed. After that, the improvements on individual behavior control are shown. Finally, some research directions to advance PSO are pointed out. © 2013 IEEE.","ant colony optimization; genetic algorithm; group search optimizer; particle swarm optimization; swarm intelligence component","Bionic algorithm; Dynamic topologies; Group search optimizer (GSO); Individual behavior; Key elements; Population structures; Sub-populations; Swarm Intelligence; Ant colony optimization; Artificial intelligence; Genetic algorithms; Particle swarm optimization (PSO)",Conference Paper,Scopus,2-s2.0-84881295400
"Lourakis M., Zabulis X.","Model-based pose estimation for rigid objects",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-39402-7_9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881255816&doi=10.1007%2f978-3-642-39402-7_9&partnerID=40&md5=f82377af42495fc8e2f77030f27463b9","Determining the pose of objects appearing in images is a problem encountered often in several practical applications. The most effective strategy for dealing with this challenge is to proceed according to the model-based paradigm, which involves building 3D models of objects and then determining object poses by fitting their models to new images with the aid of detected features. This paper proposes a model-based approach for estimating the full pose of known objects from natural point features. The method employs a projective imaging model and incorporates reliable automatic mechanisms for pose initialization and convergence. Furthermore, it is extendable to multiple cameras without the need to perform multi-view matching and relies on sparse structure from motion techniques for the construction of object models offline. Experimental results demonstrate its accuracy and robustness. © 2013 Springer-Verlag.","feature matching; object detection & recognition; Pose estimation","Automatic mechanisms; Feature matching; Model based approach; Model-based pose estimation; Multiple cameras; Object Detection; Pose estimation; Structure from motion; Artificial intelligence; Computer science; Computer vision",Conference Paper,Scopus,2-s2.0-84881255816
"Trany T., Phung D., Luo W., Harvey R., Berk M., Venkatesh S.","An integrated framework for suicide risk prediction",2013,"Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",11,10.1145/2487575.2488196,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945566410&doi=10.1145%2f2487575.2488196&partnerID=40&md5=907326d1490a688c17fc318ffa8fc137","Suicide is a major concern in society. Despite of great attention paid by the community with very substantive medico-legal implications, there has been no satisfying method that can reliably predict the future attempted or completed suicide. We present an integrated machine learning framework to tackle this challenge. Our proposed framework consists of a novel feature extraction scheme, an embedded feature selection process, a set of risk classifiers and finally, a risk calibration procedure. For temporal feature extraction, we cast the patient's clinical history into a temporal image to which a bank of one-side filters are applied. The responses are then partly transformed into mid-level features and then selected in 1-norm framework under the extreme value theory. A set of probabilistic ordinal risk classifiers are then applied to compute the risk probabilities and further re-rank the features. Finally, the predicted risks are calibrated. Together with our Australian partner, we perform comprehensive study on data collected for the mental health cohort, and the experiments validate that our proposed framework outperforms risk assessment instruments by medical practitioners. Copyright © 2013 ACM.","Filter bank; Machine learning; Medical data analysis; Oneside convolutional kernels; Risk modelling; Risk prediction; Suicide","Artificial intelligence; Calibration; Data mining; Education; Extraction; Feature extraction; Filter banks; Filtration; Forecasting; Health risks; Learning systems; Risks; Video streaming; Convolutional kernel; Medical data analysis; Risk modelling; Risk predictions; Suicide; Risk assessment",Conference Paper,Scopus,2-s2.0-84945566410
"Liang P., Shen D., Blasch E., Pham K., Wang Z., Chen G., Ling H.","Spatial context for moving vehicle detection in wide area motion imagery with multiple kernel learning",2013,"Proceedings of SPIE - The International Society for Optical Engineering",11,10.1117/12.2015967,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881110919&doi=10.1117%2f12.2015967&partnerID=40&md5=85fa41d7e74e1e39a88950e90179a0e9","Moving vehicle detection in wide area motion imagery is a challenging task due to the large motion of the camera and the small number of pixels on the target. At the same time, this task is very important for surveillance applications, and the result can be used for urban traffic management, accident and emergency responder routing. Also, the effectiveness of the context in object detection task can be further explored to increase target tracking accuracy. In this paper, we propose to use Spatial Context(SC) to improve the performance of the vehicle detection task. We first model the background of 8 consecutive frames with median filter, and get candidates by using background subtraction. The SC is built based on the candidates that have been classified as positive by Histograms of Oriented Gradient(HOG) with Multiple Kernel Learning(MKL). The region around each positive candidate is divided into m subregions with a fixed length l, then, the SC, a histogram, is built based on the number of positive candidates in each region. We use the publicly available CLIF 2006 dataset to evaluate the effect of SC. The experiments demonstrate that SC is useful to remove false positives, around which there are few positive candidates, and the combination of SC and HOG with multiple kernel learning outperforms the use of SC or HOG only. © 2013 SPIE.","Multiple kernel learning; Spatial context; Vehicle detection","Background subtraction; Histograms of oriented gradients (HoG); Multiple Kernel Learning; Spatial context; Surveillance applications; Urban traffic management; Vehicle detection; Wide-area motion imageries; Artificial intelligence; Data processing; Graphic methods; Statistical methods; Target tracking; Computation theory",Conference Paper,Scopus,2-s2.0-84881110919
"Mehta A.K., Sharma R.N., Chauhan S., Saho S.","Transformer diagnostics under dissolved gas analysis using Support Vector Machine",2013,"Proceedings of 2013 International Conference on Power, Energy and Control, ICPEC 2013",11,10.1109/ICPEC.2013.6527647,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880868155&doi=10.1109%2fICPEC.2013.6527647&partnerID=40&md5=5fa8b80aa68a7d1301df29fdf118008d","Power transformer is vital equipment in any electrical power system. So any fault in the power transformer may lead to the interruption of power supply and accordingly the financial losses will also be great. So it is important to detect the incipient faults of transformer as early as possible. Among the existing methods for identifying the incipient faults, dissolved gas analysis (DGA) is the most popular and successful method. Any kind of fault inside transformer gives rise to overheating and will produce characteristics amount of gases in transformer oil. In this paper classical methods of DGA such as Key Gas Method, Rogers Ratio Method and Duval Triangle Method are reviewed first and the need to integrate with the artificial intelligence (AI) methods for improving the performance of diagnosis is justified. Reported work presents a new and efficient artificial intelligence technique that is support vector machine (SVM) for transformer fault diagnosis using dissolved gas analysis. The proposed method i.e. Support Vector Machine is a classification tool based on statistical learning theory. Here 3 types of multiclass SVM method that is One - against-One, One-against-All and binary decision tree have been used for the fault diagnosis. Each SVM method has been trained and tested with many practical fault data of power transformers. © 2013 IEEE.","Dissolved Gas Analysis etc; Power Transformer; Support Vector Machine","Artificial intelligence techniques; Dissolved gas analyses (DGA); Dissolved gas analysis; Duval triangle methods; Electrical power system; Statistical learning theory; Transformer diagnostics; Transformer fault diagnosis; Artificial intelligence; Binary trees; Decision trees; Electric power systems; Gas chromatography; Losses; Oil filled transformers; Power transformers; Support vector machines",Conference Paper,Scopus,2-s2.0-84880868155
"Laudani A., Fulginei F.R., Salvini A.","Closed forms for the fully-connected continuous flock of starlings optimization algorithm",2013,"Proceedings - UKSim 15th International Conference on Computer Modelling and Simulation, UKSim 2013",11,10.1109/UKSim.2013.25,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880902453&doi=10.1109%2fUKSim.2013.25&partnerID=40&md5=8f9aba242599646ec757bc6aa655da9d","A new typology of swarm-based algorithms which employ analytical closed-forms written in the continuum is presented. The continuous algorithms are firstly introduced by making a simple translation of the numerical swarm-based algorithms into differential equations in the time domain (state equations). The integration of these state equations by using a time windowing approach makes available functions of time that are closed-forms suitable for describing the trajectories of the swarm members for a single time-window. The whole trajectory of a swarm member is then obtained by means of the union of all the paths which have been followed by that member. The proposed continuous algorithms have been validated on famous benchmark functions and the obtained results have been compared with those coming from the corresponding numerical algorithms. © 2013 IEEE.","Dynamic systems; Flock-of-starling optimization; Particle swarm optimization; Swarm intelligence","Benchmark functions; Continuous algorithms; Flock-of-starlings optimizations; Numerical algorithms; State equations; Swarm Intelligence; Swarm members; Time windows; Artificial intelligence; Differential equations; Dynamical systems; Equations of state; Particle swarm optimization (PSO); Algorithms",Conference Paper,Scopus,2-s2.0-84880902453
"Yao C., Zhang X., Bai X., Liu W., Ma Y., Tu Z.","Rotation-Invariant Features for Multi-Oriented Text Detection in Natural Images",2013,"PLoS ONE",11,10.1371/journal.pone.0070173,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881169489&doi=10.1371%2fjournal.pone.0070173&partnerID=40&md5=3487bd16bc603fbc828347e1ff4d3899","Texts in natural scenes carry rich semantic information, which can be used to assist a wide range of applications, such as object recognition, image/video retrieval, mapping/navigation, and human computer interaction. However, most existing systems are designed to detect and recognize horizontal (or near-horizontal) texts. Due to the increasing popularity of mobile-computing devices and applications, detecting texts of varying orientations from natural images under less controlled conditions has become an important but challenging task. In this paper, we propose a new algorithm to detect texts of varying orientations. Our algorithm is based on a two-level classification scheme and two sets of features specially designed for capturing the intrinsic characteristics of texts. To better evaluate the proposed method and compare it with the competing algorithms, we generate a comprehensive dataset with various types of texts in diverse real-world scenes. We also propose a new evaluation protocol, which is more suitable for benchmarking algorithms for detecting texts in varying orientations. Experiments on benchmark datasets demonstrate that our system compares favorably with the state-of-the-art algorithms when handling horizontal texts and achieves significantly enhanced performance on variant texts in complex natural scenes. © 2013 Yao et al.",,"accuracy; algorithm; article; computer analysis; data base; false positive result; geometry; human computer interaction; image analysis; image retrieval; methodology; object recognition; principal component analysis; recall; recognition; semantics; text; Algorithms; Artificial Intelligence; Information Storage and Retrieval; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84881169489
"Fan K., You W., Li Y.","An effective modified binary particle swarm optimization (mBPSO) algorithm for multi-objective resource allocation problem (MORAP)",2013,"Applied Mathematics and Computation",11,10.1016/j.amc.2013.06.039,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880754760&doi=10.1016%2fj.amc.2013.06.039&partnerID=40&md5=161f5069870f0261396407ee0027f910","A modified binary particle swarm optimization (mBPSO) algorithm is proposed for solving the multi-objective resource allocation problem (MORAP). First, the generation mechanism for initial particles is established to guarantee that the algorithm can begin to search optimal particle in the feasible solution space. Second, we develop the update mechanism for iterative particles which includes setting up the memory array, modifying Sig function and verifying the constraint condition to assure that the regenerated particles meet the constraint and algorithm can quickly converge. Third, the selection mechanism for pbest i and gbest is proposed which uses the dynamic neighborhood strategy to ensure that the algorithm to find Pareto optimal solutions. Through comparing the example simulation results of our mBPSO with hGA and ACO published in references, we find that proposed mBPSO outperforms hGA and ACO. Finally, the effectiveness of the different improved methods is analyzed, and the synergism effect and the convergence behavior of the mBPSO algorithm show its good performances. © 2013 Elsevier Inc. All rights reserved.","Algorithm; Binary particle swarm optimization (BPSO); Example simulation; Multi-objective resource allocation problem (MORAP); Pareto optimal solutions","Binary particle swarm optimization; Constraint conditions; Convergence behaviors; Dynamic neighborhood; Example simulation; Feasible solution spaces; Pareto optimal solutions; Resource allocation problem; Artificial intelligence; Iterative methods; Optimal systems; Resource allocation; Algorithms",Article,Scopus,2-s2.0-84880754760
"Agarwal M., Hanmandlu M., Biswas K.K.","A probabilistic and decision attitude aggregation operator for intuitionistic fuzzy environment",2013,"International Journal of Intelligent Systems",11,10.1002/int.21603,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879237406&doi=10.1002%2fint.21603&partnerID=40&md5=00c227f6699b104a1d596bcc0c35b663","In this paper, a new operator for aggregation of uncertain information under intuitionistic fuzzy environment is proposed. A novel approach is proposed for the selection of best alternative action in the face of the imprecise probabilities and the complex attitudinal character of the decision makers (DMs). This approach is distinguished with its capacity to accommodate the linguistic specification of probabilities as provided by human experts directly without the need to determine the fuzzy membership grades. The focus is to compute the net payoff for each alternative in the face of uncertain states of nature and DM's attitude. The proposed operator and the approach are illustrated through two real case studies. © 2013 Wiley Periodicals, Inc.",,"Aggregation operator; Decision attitudes; Decision makers; Fuzzy membership; Human expert; Imprecise probabilities; Intuitionistic fuzzy; Uncertain informations; Artificial intelligence; Software engineering; Fuzzy sets",Article,Scopus,2-s2.0-84879237406
"Latorre J.I., Jiménez E.","Petri nets with exclusive entities for decision making",2013,"International Journal of Simulation and Process Modelling",11,10.1504/IJSPM.2013.055178,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880660752&doi=10.1504%2fIJSPM.2013.055178&partnerID=40&md5=badc25ef0c57d247f4c3dae04942ded7","The design of Discrete Event Systems (DES) can be seen as a sequence of decisions leading to a final product that complies with a set of specifications and operates with efficiency. These decisions usually include the choice among a set of alternative structural configurations for the DES. This paper discusses the formalisation of a decision problem based on a DES into an optimisation problem, stressing and making explicit the exclusiveness between alternative structural configurations. This approach broadens and improves the classical methodology for solving the mentioned problems with new ideas and techniques. A significant advantage achieved consists of increasing the efficiency of the solving techniques by removing redundant information in the Petri net model of the DES and by unifying the solution space. Copyright © 2013 Inderscience Enterprises Ltd.","Alternative Petri nets; Compound Petri nets; Decision making; Decision support system; DES; Discrete event system; Exclusive entities; Optimisation; Parametric Petri nets; Petri nets; PN; Process modelling; Simulation; Undefined parameters; Undefined Petri nets","DES; Exclusive entities; Optimisations; PN; Process modelling; Simulation; Undefined parameters; Artificial intelligence; Decision making; Decision support systems; Optimization; Petri nets; Discrete event simulation",Article,Scopus,2-s2.0-84880660752
"Eshkalak M.O., Mohaghegh S.D., Esmaili S.","Synthetic, geomechanical logs for Marcellus Shale",2013,"Society of Petroleum Engineers - SPE Digital Energy Conference and Exhibition 2013",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880485785&partnerID=40&md5=5fcd60d22ab96f0d7c6a8833f9e9261d","Successful recovery of hydrocarbons from the reservoirs, notably shale, is attributed to realizing the key fundamentals of reservoir rock properties. Having adequate and sufficient information regarding the variable lithology and mineralogy is crucial in order to identify the ""right"" pay-zone intervals for shale gas production. Also, contribution of mechanical properties (Principal stress profiles) of shale to hydraulic fracturing strategies is a well understood concept. It may also contribute to better, more accurate simulation models of production from shale gas reservoirs. In this study, synthetic geomechanical logs (Including following properties: Poisson's Ratio, Total Minimum Horizontal Stress, Bulk and Shear Modulus, etc.) are developed for more than 50 Marcellus Shale wells. Using Artificial Intelligence and Data Mining (AI&DM), data-driven models are developed that are capable of generating synthetic geomechanical logs from conventional logs such as Gamma Ray and Density Porosity. The data-driven models are validated using wells with actual geomechanical logs that have been removed from the database to serve as blind validation wells. In addition, having access to necessary data to building a geomechanical distribution (Map and Volume) model can assist in understanding the rock mechanical behavior and consequently creating effective hydraulic fractures which is considered to be an essential step in economically development of Shale assets. Moreover, running geomechanical logs on a subset of wells, but having the luxury of generating logs of similar quality for all the existing wells in a Shale asset can prove to be a sound reservoir management tool for better reservoir characterization, modeling and efficient production of Marcellus Shale reservoir. Copyright 2013, Society of Petroleum Engineers.",,"Blind validations; Conventional logs; Data-driven model; Efficient production; Mechanical behavior; Reservoir characterization; Reservoir rock properties; Shale gas reservoirs; Artificial intelligence; Computer simulation; Exhibitions; Gamma rays; Geomechanics; Horizontal wells; Hydraulic fracturing; Lithology; Mechanical properties; Minerals; Petroleum reservoir evaluation; Petroleum reservoirs; Shale",Conference Paper,Scopus,2-s2.0-84880485785
"Danciu D.","A CNN based approach for solving a hyperbolic PDE arising from a system of conservation laws - The case of the overhead crane",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-38682-4_39,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880078194&doi=10.1007%2f978-3-642-38682-4_39&partnerID=40&md5=13e16095e1da9b65e2056f02260e56e7","The paper proposes a neurocomputing approach for numerical solving of a hyperbolic partial differential equation (PDE) arising from a system of conservation laws. The main idea is to combine the method of lines (transforming the mixed initial boundary value problem for PDE into a high dimensional system of ordinary differential equations (ODEs)) with a cellular neural network (CNN) optimal structure which exploits the inherent parallelism of the new problem in order to reduce the computational effort and storage. The method ensure from the beginning the convergence of the approximation and preserve the stability of the initial problem. © 2013 Springer-Verlag Berlin Heidelberg.","CNN; conservation laws; Courant-Isaacson-Rees rule; hyperbolic PDE; method of lines; neurocomputing; neuromathematics; ODE","CNN; Conservation law; Courant-Isaacson-Rees rule; Hyperbolic PDEs; Method of lines; Neurocomputing; neuromathematics; ODE; Artificial intelligence; Cellular neural networks; Ordinary differential equations; Partial differential equations; Structural optimization; Physical properties",Conference Paper,Scopus,2-s2.0-84880078194
"Martin S., Ouelhadj D., Smet P., Vanden Berghe G., Özcan E.","Cooperative search for fair nurse rosters",2013,"Expert Systems with Applications",11,10.1016/j.eswa.2013.06.019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880066140&doi=10.1016%2fj.eswa.2013.06.019&partnerID=40&md5=5c30aaff5e1aa83a5885358d3ac63baf","The development of decision support systems acceptable for nurse rostering practitioners still presents a daunting challenge. Building on an existing nurse rostering problem, a set of fairness-based objective functions recently introduced in the literature has been extended. To this end, a generic agent-based cooperative search framework utilising new mechanisms is described, aiming to combine the strengths of multiple metaheuristics. These different metaheuristics represent individual planners' implicit procedures for improving rosters. The framework enables to explore different ways of assessing nurse rosters in terms of fairness objectives. Computational experiments have been conducted across a set of benchmark instances. The overall results indicate that the proposed cooperative search for fair nurse rosters outperforms each metaheuristic run individually. © 2013 Elsevier Ltd. All rights reserved.","Agent-based systems; Cooperative search; Fairness; Nurse rostering","Agent-based systems; Computational experiment; Cooperative search; Fairness; Meta heuristics; New mechanisms; Nurse rostering; Objective functions; Artificial intelligence; Benchmarking; Decision support systems; Heuristic algorithms; Nursing",Article,Scopus,2-s2.0-84880066140
"Crossley S.A., Varner L.K., Roscoe R.D., McNamara D.S.","Using automated indices of cohesion to evaluate an intelligent tutoring system and an automated writing evaluation system",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-39112-5-28,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880012839&doi=10.1007%2f978-3-642-39112-5-28&partnerID=40&md5=bf9a9253a01b899646796e72e84ebc4e","We present an evaluation of the Writing Pal (W-Pal) intelligent tutoring system (ITS) and the W-Pal automated writing evaluation (AWE) system through the use ofcomputational indices related to text cohesion. Sixty-four students participated in this study. Each student was assigned to either the W-Pal ITS condition or the W-Pal AWE condition. The W-Pal ITS includes strategy instruction, game-based practice, and essay-based practice with automated feedback. In the ITS condition, students received strategy training and wrote and revised one essay in each of the 8 training sessions. In the AWE condition, students only interacted with the essay writing and feedback tools. These students wrote and revised two essays in each of the 8 sessions. Indices of local and global cohesion reported by the computational tools Coh-Metrix and the Writing Assessment Tool (WAT) were used to investigate pretest and posttest writing gains. For both the ITS and the AWE systems, training led to the increased use of global cohesion features in essay writing. This study demonstrates that automated indices of text cohesion can be used to evaluate the effects of ITSs and AWE systems and further demonstrates how text cohesion develops as a result of instruction, writing, and automated feedback. © 2013 Springer-Verlag Berlin Heidelberg.","Cohesion; Computational Linguistics; Corpus Linguistics; Intelligent Tutoring Systems; Natural Language Processing; Writing Pedagogy","Assessment tool; Automated feedback; Cohesion; Computational tools; Corpus linguistics; Intelligent tutoring system; NAtural language processing; Training sessions; Adhesion; Artificial intelligence; Computational linguistics; Computer aided instruction; Education computing; Natural language processing systems; Students; Automation",Conference Paper,Scopus,2-s2.0-84880012839
"Desmarais M.C., Naceur R.","A matrix factorization method for mapping items to skills and for enhancing expert-based q-matrices",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-39112-5-45,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879996742&doi=10.1007%2f978-3-642-39112-5-45&partnerID=40&md5=3e011b3132dac07125df6f1745bdf878","Uncovering the right skills behind question items is a difficult task. It requires a thorough understanding of the subject matter and of the cognitive factors that determine student performance. The skills definition, and the mapping of item to skills, require the involvement of experts. We investigate means to assist experts for this task by using a data driven, matrix factorization approach. The two mappings of items to skills, the expert on one side and the matrix factorization on the other, are compared in terms of discrepancies, and in terms of their performance when used in a linear model of skills assessment and item outcome prediction. Visual analysis shows a relatively similar pattern between the expert and the factorized mappings, although differences arise. The prediction comparison shows the factorization approach performs slightly better than the original expert Q-matrix, giving supporting evidence to the belief that the factorization mapping is valid. Implications for the use of the factorization to design better item to skills mapping are discussed. © 2013 Springer-Verlag Berlin Heidelberg.","Alternating least squares matrix factorization; Cognitive modeling; Latent skills; Skills assessment; Student models","Cognitive modeling; Latent skills; Matrix factorizations; Skills assessment; Student Models; Artificial intelligence; Factorization; Mapping; Matrix algebra; Students",Conference Paper,Scopus,2-s2.0-84879996742
"Harley J.M., Bouchet F., Azevedo R.","Aligning and comparing data on emotions experienced during learning with metatutor",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-39112-5-7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880004292&doi=10.1007%2f978-3-642-39112-5-7&partnerID=40&md5=fa0fd200800a285c01fb31ec1f24496d","In this study we aligned and compared self-report and on-line emotions data on 67 college students' emotions at five different points in time over the course of their interactions with MetaTutor. Self-reported emotion data as well as facial expression data were converged and analyzed. Results across channels revealed that neutral and positively-valenced basic and learner-centered emotional states represented the majority of emotional states experienced with MetaTutor. The self-report results revealed a decline in the intensity of positively-valenced and neutral states across the learning session. The facial expression results revealed a substantial decrease in the number of learners' with neutral facial expressions from time one to time two, but a fairly stable pattern for the remainder of the session, with participants who experienced other basic emotional states, transitioning back to a state of neutral between self-reports. Agreement between channels was 75.6%. © 2013 Springer-Verlag Berlin Heidelberg.","Affect; Emotions; Intelligent tutoring systems; Pedagogical agents","Affect; Emotions; Facial expression data; Facial Expressions; Intelligent tutoring system; Learner-centered; Learning sessions; Pedagogical agents; Artificial intelligence; Computer aided instruction; Students; Teaching",Conference Paper,Scopus,2-s2.0-84880004292
"Finkelstein S., Yarzebinski E., Vaughn C., Ogan A., Cassell J.","The effects of culturally congruent educational technologies on student achievement",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-39112-5-50,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880000297&doi=10.1007%2f978-3-642-39112-5-50&partnerID=40&md5=12ba88efdc8d9103ba46c52734830292","Dialectal differences are one explanation for the systematically reduced test scores of children of color compared to their Euro-American peers. In this work, we explore the relationship between academic performance and dialect differences exhibited in a learning environment by assessing 3rd grade students' science performance after interacting with a distant peer technology that employed one of three dialect use patterns. We found that our participants, all native speakers of African American Vernacular English (AAVE), demonstrated the strongest science performance when the technology used AAVE features consistently throughout the interaction. These results call for a re-examination of the cultural assumptions underlying the design of educational technologies, with a specific emphasis on the way in which we present information to culturally-underrepresented groups. © 2013 Springer-Verlag Berlin Heidelberg.","Culture; Dialect; Peer models","Academic performance; African American; Dialect; Learning environments; Peer model; Student achievement; Artificial intelligence; Cell culture; Educational technology",Conference Paper,Scopus,2-s2.0-84880000297
"Song M., Tao D., Sun S., Chen C., Bu J.","Joint sparse learning for 3-D facial expression generation",2013,"IEEE Transactions on Image Processing",11,10.1109/TIP.2013.2261307,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879906937&doi=10.1109%2fTIP.2013.2261307&partnerID=40&md5=8a211467652cba007aa0a38ac8fcbc60","3-D facial expression generation, including synthesis and retargeting, has received intensive attentions in recent years, because it is important to produce realistic 3-D faces with specific expressions in modern film production and computer games. In this paper, we present joint sparse learning (JSL) to learn mapping functions and their respective inverses to model the relationship between the high-dimensional 3-D faces (of different expressions and identities) and their corresponding low-dimensional representations. Based on JSL, we can effectively and efficiently generate various expressions of a 3-D face by either synthesizing or retargeting. Furthermore, JSL is able to restore 3-D faces with holes by learning a mapping function between incomplete and intact data. Experimental results on a wide range of 3-D faces demonstrate the effectiveness of the proposed approach by comparing with representative ones in terms of quality, time cost, and robustness. © 1992-2012 IEEE.","3D facial expression generation; facial expression retargeting; sparse learning","3-d facial expressions; Facial Expressions; Film production; High-dimensional; Low-dimensional representation; Mapping functions; sparse learning; Time cost; Mathematical models; Image processing; algorithm; article; artificial intelligence; automated pattern recognition; biometry; computer assisted diagnosis; face; histology; human; image enhancement; image subtraction; methodology; reproducibility; sensitivity and specificity; three dimensional imaging; anatomy and histology; automated pattern recognition; biometry; computer assisted diagnosis; face; procedures; three dimensional imaging; Algorithms; Artificial Intelligence; Biometry; Face; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique; Algorithms; Artificial Intelligence; Biometry; Face; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-84879906937
"Mendel J.M., Korjani M.M.","Theoretical aspects of Fuzzy Set Qualitative Comparative Analysis (fsQCA)",2013,"Information Sciences",11,10.1016/j.ins.2013.02.048,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877006492&doi=10.1016%2fj.ins.2013.02.048&partnerID=40&md5=73a96e024f34b974b8486bf03d347677","Fuzzy set Qualitative Comparative Analysis (fsQCA) is a methodology for obtaining linguistic summarizations from data that are associated with cases. It has recently been described as a collection of 13 steps [7]. In this paper we focus on how to greatly speed up some of the computationally intensive steps of fsQCA and show how to use the speed-up equations to obtain some interesting and important properties of fsQCA. These properties not only provide additional understanding about fsQCA, but also lead to different ways to implement fsQCA. One of the properties is so important (Section 8) that unless its results are adopted, when a variable is described by more than one term (e.g., Low and High), fsQCA will provide incorrect results. © 2013 Elsevier Inc. All rights reserved.","Causal combination; Causal condition; Consistency; Fast fsQCA fsQCA; Fuzzy set","Causal combination; Causal conditions; Consistency; Fast fsQCA; Fuzzy Set Qualitative Comparative Analysis; Linguistic summarization; Speed up; Theoretical aspects; Artificial intelligence; Software engineering; Fuzzy sets",Conference Paper,Scopus,2-s2.0-84877006492
"Ellens W., Zuraniewski P., Sperotto A., Schotanus H., Mandjes M., Meeuwissen E.","Flow-based detection of DNS tunnels",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-38998-6_16,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879642111&doi=10.1007%2f978-3-642-38998-6_16&partnerID=40&md5=fb821bd5707b9f8af6c30264617d8c8b","DNS tunnels allow circumventing access and security policies in firewalled networks. Such a security breach can be misused for activities like free web browsing, but also for command & control traffic or cyber espionage, thus motivating the search for effective automated DNS tunnel detection techniques. In this paper we develop such a technique, based on the monitoring and analysis of network flows. Our methodology combines flow information with statistical methods for anomaly detection. The contribution of our paper is twofold. Firstly, based on flow-derived variables that we identified as indicative of DNS tunnelling activities, we identify and evaluate a set of non-parametrical statistical tests that are particularly useful in this context. Secondly, the efficacy of the resulting tests is demonstrated by extensive validation experiments in an operational environment, covering many different usage scenarios. © 2013 IFIP International Federation for Information Processing.","anomaly detection; cyber security; DNS tunneling; network flows","Anomaly detection; Cyber security; Flow informations; Monitoring and analysis; Network flows; Operational environments; Security breaches; Tunnel detection; Artificial intelligence; Computer science; Internet protocols",Conference Paper,Scopus,2-s2.0-84879642111
"Maji P., Paul S.","Robust rough-fuzzy C-means algorithm: Design and applications in coding and non-coding RNA expression data clustering",2013,"Fundamenta Informaticae",11,10.3233/FI-2013-829,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879482850&doi=10.3233%2fFI-2013-829&partnerID=40&md5=28fb45866a73f4cea0102b41607bfc88","Cluster analysis is a technique that divides a given data set into a set of clusters in such a way that two objects from the same cluster are as similar as possible and the objects from different clusters are as dissimilar as possible. In this background, different rough-fuzzy clustering algorithms have been shown to be successful for finding overlapping and vaguely defined clusters. However, the crisp lower approximation of a cluster in existing rough-fuzzy clustering algorithms is usually assumed to be spherical in shape, which restricts to find arbitrary shapes of clusters. In this regard, this paper presents a new rough-fuzzy clustering algorithm, termed as robust rough-fuzzy c-means. Each cluster in the proposed clustering algorithm is represented by a set of three parameters, namely, cluster prototype, a possibilistic fuzzy lower approximation, and a probabilistic fuzzy boundary. The possibilistic lower approximation helps in discovering clusters of various shapes. The cluster prototype depends on the weighting average of the possibilistic lower approximation and probabilistic boundary. The proposed algorithm is robust in the sense that it can find overlapping and vaguely defined clusters with arbitrary shapes in noisy environment. An efficient method is presented, based on Pearson's correlation coefficient, to select initial prototypes of different clusters. A method is also introduced based on cluster validity index to identify optimum values of different parameters of the initialization method and the proposed clustering algorithm. The effectiveness of the proposed algorithm, along with a comparison with other clustering algorithms, is demonstrated on synthetic as well as coding and non-coding RNA expression data sets using some cluster validity indices.","Clustering; Fuzzy sets; Pattern recognition; Rough sets; Rough-fuzzy clustering","Cluster validity indices; Clustering; Design and application; Initialization methods; Lower approximation; Pearson's correlation coefficients; Rough-fuzzy c-means algorithms; Rough-fuzzy clustering; Artificial intelligence; Cluster analysis; Copying; Correlation methods; Fuzzy clustering; Fuzzy sets; Pattern recognition; RNA; Rough set theory; Clustering algorithms",Article,Scopus,2-s2.0-84879482850
"Seah C.-W., Tsang I.W., Ong Y.-S.","Transfer ordinal label learning",2013,"IEEE Transactions on Neural Networks and Learning Systems",11,10.1109/TNNLS.2013.2268541,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886952451&doi=10.1109%2fTNNLS.2013.2268541&partnerID=40&md5=ab7fa5443b4d6017e692ab642ffcd364","Designing a classifier in the absence of labeled data is becoming a common encounter as the acquisition of informative labels is often difficult or expensive, particularly on new uncharted target domains. The feasibility of attaining a reliable classifier for the task of interest is embarked by some in transfer learning, where label information from relevant source domains is considered for complimenting the design process. The core challenge arising from such endeavors, however, is the induction of source sample selection bias, such that the trained classifier has the tendency of steering toward the distribution of the source domain. In addition, this bias is deemed to become more severe on data involving multiple classes. Considering this cue, our interest in this paper is to address such a challenge in the target domain, where ordinal labeled data are unavailable. In contrast to the previous works, we propose a transfer ordinal label learning paradigm to predict the ordinal labels of target unlabeled data by spanning the feasible solution space with ensemble of ordinal classifiers from the multiple relevant source domains. Specifically, the maximum margin criterion is considered here for the construction of the target classifier from an ensemble of source ordinal classifiers. Theoretical analysis and extensive empirical studies on real-world data sets are presented to study the benefits of the proposed method. © 2012 IEEE.","Classifier selection; domain adaptation; ordinal regression; sentiment analysis; source sample selection bias; transfer learning","Classifier selection; Domain adaptation; Ordinal regression; Sample selection bias; Sentiment analysis; Transfer learning; Artificial intelligence; Computer networks; Virtual reality",Article,Scopus,2-s2.0-84886952451
"Lin C.-W., Lin Y.-K., Hsieh H.-T.","Ant colony optimization for unrelated parallel machine scheduling",2013,"International Journal of Advanced Manufacturing Technology",11,10.1007/s00170-013-4766-7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888391224&doi=10.1007%2fs00170-013-4766-7&partnerID=40&md5=e74c4867dc76857f6f63ffce8f23e478","Meeting due dates is a major issue in most manufacturing systems, and one effective measure for due dates is total weighted tardiness. In this research, we consider an ant colony optimization (ACO) algorithm incorporating a number of new ideas (heuristic initial solution, machine reselection step, and local search procedure) to solve the problem of scheduling unrelated parallel machines to minimize total weighted tardiness. The problem is NP-hard in the strong sense, because the single machine case is already NP-hard in the strong sense. Computational results show that the proposed ACO algorithm outperforms other existing algorithms in terms of total weighted tardiness. © 2013 Springer-Verlag London.","Ant colony optimization; Total weighted tardiness; Unrelated parallel machines","ACO algorithms; Ant Colony Optimization algorithms; Computational results; Effective measures; Initial solution; Single- machines; Total-weighted tardiness; Unrelated parallel machines; Ant colony optimization; Artificial intelligence; Algorithms",Article,Scopus,2-s2.0-84888391224
"Wang Z., Zineddin B., Liang J., Zeng N., Li Y., Du M., Cao J., Liu X.","A novel neural network approach to cDNA microarray image segmentation",2013,"Computer Methods and Programs in Biomedicine",11,10.1016/j.cmpb.2013.03.013,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878477855&doi=10.1016%2fj.cmpb.2013.03.013&partnerID=40&md5=c8ed93f3e7e1cadafd8a76186d6c69a8","Microarray technology has become a great source of information for biologists to understand the workings of DNA which is one of the most complex codes in nature. Microarray images typically contain several thousands of small spots, each of which represents a different gene in the experiment. One of the key steps in extracting information from a microarray image is the segmentation whose aim is to identify which pixels within an image represent which gene. This task is greatly complicated by noise within the image and a wide degree of variation in the values of the pixels belonging to a typical spot. In the past there have been many methods proposed for the segmentation of microarray image. In this paper, a new method utilizing a series of artificial neural networks, which are based on multi-layer perceptron (MLP) and Kohonen networks, is proposed. The proposed method is applied to a set of real-world cDNA images. Quantitative comparisons between the proposed method and commercial software GenePix® are carried out in terms of the peak signal-to-noise ratio (PSNR). This method is shown to not only deliver results comparable and even superior to existing techniques but also have a faster run time. © 2013 Elsevier Ireland Ltd.","Adaptive segmentation; Artificial neural networks; Kohonen neural networks; Microarray image","Adaptive segmentation; Extracting information; Kohonen neural networks; Microarray images; Microarray technologies; Multi layer perceptron; Peak signal-to-noise ratio; Quantitative comparison; Complex networks; Gene encoding; Neural networks; Pixels; Image segmentation; complementary DNA; algorithm; article; artificial neural network; computer program; DNA microarray; Kohonen neural network; machine learning; perceptron; signal noise ratio; Algorithms; Artificial Intelligence; Image Processing, Computer-Assisted; Neural Networks (Computer); Oligonucleotide Array Sequence Analysis; Pattern Recognition, Automated; Signal-To-Noise Ratio; Software",Article,Scopus,2-s2.0-84878477855
"Tai S.-C., Chang C.-Y., Chen B.-J., Hu J.-F.","Speeding up the decisions of Quad-Tree structures and coding modes for HEVC coding units",2013,"Smart Innovation, Systems and Technologies",11,10.1007/978-3-642-35473-1_40,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879317172&doi=10.1007%2f978-3-642-35473-1_40&partnerID=40&md5=c70ad695c6cb7a1592035de8859f8969","High Efficiency Video Coding (HEVC) is being developed by the joint development of ISO/IEC MPEG and ITU-T Video Coding Experts Group (VCEG) and is expected to be a popular next-generation video codec in the fu-ture. HEVC can provide higher compression ratio compared to H.264/AVC standard; however, the coding complexity is dramatically increased as well. In this thesis, a fast algorithm for coding unit decision is proposed to reduce the burden of the encoding time in HEVC. The proposed algorithm exploits the temporal correlation in the neigh-boring frames of a video sequence to avoid the unnecessary examinations on CU quad-trees. In addition, based on an adap-tive threshold, the best prediction mode is early determined to SKIP mode for reducing the exhaustive evaluations at prediction stage. The performance of the proposed algorithm is verified through the test model for HEVC, HM 5.0. The experimental results show that the proposed algorithm can averagely achieve about 27%, 33%, 20%, and 21% total time encoding time reduction under Low-Delay High Efficiency, Low-Delay Low Complexity, Random-Access High Ef-ficiency, and Random-Access Low Complexity configurations respectively with a negligible degradation of coding performance. The rest of this thesis is organized as follows. Section 1 gives a brief intro-duction to the HEVC encoder, includes overview of HEVC coding standard. Simultaneously, some previously proposed methods for fast CU decision are al-so investigated in this chapter. Section 2 proposes a new early termination algo-rithm for CU decision. Section 3 demonstrates the experimental results verified through the test model for HEVC, HM 5.0 [4]. Section 4 concludes the studies presented in this thesis. © 2013 Springer-Verlag Berlin Heidelberg.","Coding Mode; Coding Unit; Fast Algorithm; High Efficiency Video Coding; Quad-Tree Structure","Coding mode; Coding Unit; Fast algorithms; High-efficiency video coding; Quadtree structure; Algorithms; Compression ratio (machinery); Efficiency; Encoding (symbols); Forestry; Image coding; Intelligent systems; Motion Picture Experts Group standards; Video signal processing; Trees (mathematics); Algorithms; Artificial Intelligence; Communication; Image Analysis; Mathematical Analysis",Article,Scopus,2-s2.0-84879317172
"Farahmand A., Aghakouchak A.","A satellite-based global landslide model",2013,"Natural Hazards and Earth System Science",11,10.5194/nhess-13-1259-2013,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879222352&doi=10.5194%2fnhess-13-1259-2013&partnerID=40&md5=1e204f53862c819beb96dd9e6db4f44a","Landslides are devastating phenomena that cause huge damage around the world. This paper presents a quasi-global landslide model derived using satellite precipitation data, land-use land cover maps, and 250 m topography information. This suggested landslide model is based on the Support Vector Machines (SVM), a machine learning algorithm. The National Aeronautics and Space Administration (NASA) Goddard Space Flight Center (GSFC) landslide inventory data is used as observations and reference data. In all, 70% of the data are used for model development and training, whereas 30% are used for validation and verification. The results of 100 random subsamples of available landslide observations revealed that the suggested landslide model can predict historical landslides reliably. The average error of 100 iterations of landslide prediction is estimated to be approximately 7%, while approximately 2% false landslide events are observed. © 2013 Author(s).",,"algorithm; artificial intelligence; land cover; land use; landslide; model validation; numerical model; prediction; satellite data",Article,Scopus,2-s2.0-84879222352
"Aplak H.S., Türkbey O.","Fuzzy logic based game theory applications in multi-criteria decision making process",2013,"Journal of Intelligent and Fuzzy Systems",11,10.3233/IFS-2012-0642,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878836611&doi=10.3233%2fIFS-2012-0642&partnerID=40&md5=2ca4de30fc4fe78134f4ef5428fd7cad","Decision process can be summarized as a process that helps for choosing the optimal alternatives according to suggested objectives by evaluating all environmental effects in problem solving. Nowadays, since environmental effects are more complex, imprecise and multilateral, fuzzy set theory and game theory are widely preferred instruments in decision making process. The aim of this study is to present a hybrid multi-criteria decision making approach which uses artificial intelligence techniques such as fuzzy logic and game theory. This process is considered in two person non-constant sum game theory perspective. The methods in literature about related topics (such as scenario planning and fuzzy TOPSIS) are examined and a hybrid decision making methodology that comprises many decision methods is formed. All phases of this approach are executed in game theory perspective by evaluating mutual strategies of players. In the study, the methodology is explained and a fictitious international disagreement case is used as a numerical example to demonstrate the validity and applicability. © 2013 - IOS Press and the authors. All rights reserved.","fuzzy sets; Game theory; multi criteria analysis","Artificial intelligence techniques; Decision making methodology; Decision making process; Game theory applications; Multi Criteria Analysis; Multi-criteria decision making; Numerical example; Optimal alternative; Artificial intelligence; Fuzzy logic; Fuzzy set theory; Fuzzy sets; Game theory; Decision making",Article,Scopus,2-s2.0-84878836611
"Prescilla K., Immanuel Selvakumar A.","Modified Binary Particle Swarm optimization algorithm application to real-time task assignment in heterogeneous multiprocessor",2013,"Microprocessors and Microsystems",11,10.1016/j.micpro.2013.05.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885948914&doi=10.1016%2fj.micpro.2013.05.003&partnerID=40&md5=26da449d1ee72ed995205cde2cd3b8ab","Task assignment in a heterogeneous multiprocessor is a NP-hard problem, so approximate methods are used to solve the problem. In this paper the Modified Binary Particle Swarm Optimization (Modified BPSO) algorithm and Novel Binary Particle Swarm (Novel BPSO) Optimization are applied to solve the real-time task assignment in heterogeneous multiprocessor. The problem consists of a set of independent periodic task, which has to be assigned to a heterogeneous multiprocessor without exceeding the utilization bound. The objective is to schedule maximum number of tasks with minimum energy consumption. The execution times and deadlines of the tasks are assumed to be known. Here Modified BPSO performance is compared with Novel BPSO and Ant Colony Optimization algorithm (ACO). Experimental results show that Modified BPSO performs better than Novel BPSO and ACO for consistent utilization matrix and ACO performs better than Modified BPSO and Novel BPSO for inconsistent utilization matrix. © 2013 Elsevier B.V. All rights reserved.","Heterogeneous processors; Modified BPSO; Multiprocessors; Novel BPSO; Periodic tasks; Real-time Systems; Scheduling; Task assignment","Heterogeneous processors; Modified BPSO; Multiprocessors; Novel BPSO; Periodic tasks; Task assignment; Ant colony optimization; Artificial intelligence; Computational complexity; Energy utilization; Multiprocessing systems; Real time systems; Scheduling; Algorithms",Article,Scopus,2-s2.0-84885948914
"Gao J., Wang Z., Yang Y., Zhang W., Tao C., Guan J., Rao N.","A Novel Approach for Lie Detection Based on F-Score and Extreme Learning Machine",2013,"PLoS ONE",11,10.1371/journal.pone.0064704,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878633736&doi=10.1371%2fjournal.pone.0064704&partnerID=40&md5=9b95640ea441a59ba83eede0d6e3371d","A new machine learning method referred to as F-score_ELM was proposed to classify the lying and truth-telling using the electroencephalogram (EEG) signals from 28 guilty and innocent subjects. Thirty-one features were extracted from the probe responses from these subjects. Then, a recently-developed classifier called extreme learning machine (ELM) was combined with F-score, a simple but effective feature selection method, to jointly optimize the number of the hidden nodes of ELM and the feature subset by a grid-searching training procedure. The method was compared to two classification models combining principal component analysis with back-propagation network and support vector machine classifiers. We thoroughly assessed the performance of these classification models including the training and testing time, sensitivity and specificity from the training and testing sets, as well as network size. The experimental results showed that the number of the hidden nodes can be effectively optimized by the proposed method. Also, F-score_ELM obtained the best classification accuracy and required the shortest training and testing time. © 2013 Gao et al.",,"article; classifier; electrode; electroencephalogram; event related potential; F score; forensic medicine; functional magnetic resonance imaging; human; machine learning; principal component analysis; scoring system; sensitivity and specificity; support vector machine; algorithm; artificial intelligence; electroencephalography; female; guilt; male; theoretical model; young adult; Algorithms; Artificial Intelligence; Electroencephalography; Female; Guilt; Humans; Lie Detection; Male; Models, Theoretical; Principal Component Analysis; Young Adult",Article,Scopus,2-s2.0-84878633736
"Zheng Q., Li M., Li Y., Tang Q.","Station ant colony optimization for the type 2 assembly line balancing problem",2013,"International Journal of Advanced Manufacturing Technology",11,10.1007/s00170-012-4465-9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879095097&doi=10.1007%2fs00170-012-4465-9&partnerID=40&md5=e01fdf5798f703e1fda245dc3e52a57b","An improved ant colony optimization (ACO), namely, station ant colony optimization (SACO), is proposed to solve the type 2 assembly line balancing problem (ALBP-2). In the algorithm, ACO is employed to search different better combinations of tasks (component solutions) for each station; an iteration compress mechanism is proposed to reduce the searching space of feasible solutions of ALBP-2. Three heuristic factors [i.e., (1) task time, (2) number of successors, and (3) number of releasable successors], two pheromones, and a task assignment mechanism are proposed to search better component solutions for every station. Finally, the effectiveness and stability of SACO are confirmed through comparison with literatures in 23 instances included in nine examples. © 2012 Springer-Verlag London Limited.","Assembly line balancing; Heuristic factors; Station ant colony optimization; Task assignment mechanism","Assembly line balancing; Assembly line balancing problems; Feasible solution; Heuristic factors; Improved ant colony optimization; Searching spaces; Task assignment; Ant colony optimization; Artificial intelligence; Iterative methods; Algorithms",Article,Scopus,2-s2.0-84879095097
"Ericson K., Pallickara S.","On the performance of high dimensional data clustering and classification algorithms",2013,"Future Generation Computer Systems",11,10.1016/j.future.2012.05.026,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863770788&doi=10.1016%2fj.future.2012.05.026&partnerID=40&md5=b93e93576811f56b7330db8ff41195ed","There is often a need to perform machine learning tasks on voluminous amounts of data. These tasks have application in fields such as pattern recognition, data mining, bioinformatics, and recommendation systems. Here we evaluate the performance of 4 clustering algorithms and 2 classification algorithms supported by Mahout within two different cloud runtimes, Hadoop and Granules. Our benchmarks use the same Mahout backend code, ensuring a fair comparison. The differences between these implementations stem from how the Hadoop and Granules runtimes (1) support and manage the lifecycle of individual computations, and (2) how they orchestrate exchange of data between different stages of the computational pipeline during successive iterations of the clustering algorithm. We include an analysis of our results for each of these algorithms in a distributed setting, as well as a discussion on measures for failure recovery. © 2012 Elsevier B.V.","Classification; Clustering; Distributed stream processing; Granules; Hadoop; Machine learning; Mahout","Artificial intelligence; Classification (of information); Data mining; Distributed parameter control systems; Granulation; Learning systems; Pattern recognition; Pattern recognition systems; Clustering; Distributed stream processing; Granules; Hadoop; Mahout; Clustering algorithms",Article,Scopus,2-s2.0-84863770788
"Ji S.-Y., Belle A., Ward K.R., Ryan K.L., Rickards C.A., Convertino V.A., Najarian K.","Heart rate variability analysis during central hypovolemia using wavelet transformation",2013,"Journal of Clinical Monitoring and Computing",11,10.1007/s10877-013-9434-9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878324123&doi=10.1007%2fs10877-013-9434-9&partnerID=40&md5=7eec9c544b33b1640a1dfbd78d1db1e7","Detection of hypovolemia prior to overt hemodynamic decompensation remains an elusive goal in the treatment of critically injured patients in both civilian and combat settings. Monitoring of heart rate variability has been advocated as a potential means to monitor the rapid changes in the physiological state of hemorrhaging patients, with the most popular methods involving calculation of the R-R interval signal's power spectral density (PSD) or use of fractal dimensions (FD). However, the latter method poses technical challenges, while the former is best suited to stationary signals rather than the non-stationary R-R interval. Both approaches are also limited by high inter- and intra-individual variability, a serious issue when applying these indices to the clinical setting. We propose an approach which applies the discrete wavelet transform (DWT) to the R-R interval signal to extract information at both 500 and 125 Hz sampling rates. The utility of machine learning models based on these features were tested in assessing electrocardiogram signals from volunteers subjected to lower body negative pressure induced central hypovolemia as a surrogate of hemorrhage. These machine learning models based on DWT features were compared against those based on the traditional PSD and FD, at both sampling rates and their performance was evaluated based on leave-one-subject- out fold cross-validation. Results demonstrate that the proposed DWT-based model outperforms individual PSD and FD methods as well as the combination of these two traditional methods at both sample rates of 500 Hz (p value <0.0001) and 125 Hz (p value <0.0001) in detecting the degree of hypovolemia. These findings indicate the potential of the proposed DWT approach in monitoring the physiological changes caused by hemorrhage. The speed and relatively low computational costs in deriving these features may make it particularly suited for implementation in portable devices for remote monitoring. © 2013 Springer Science+Business Media New York.","Discrete wavelet transformation; Heart rate variability (HRV); Higuchi fractal dimension; Lower body negative pressure (LBNP); Power spectral density; RR interval","Discrete wavelet transformation; Heart rate variability; Higuchi fractal dimension; Negative pressures; RR intervals; Finite difference method; Fractal dimension; Heart; Learning systems; Patient treatment; Physiology; Power spectral density; Signal detection; Discrete wavelet transforms; article; calculation; controlled study; discrete wavelet transform; disease severity; electrocardiogram; fractal analysis; heart rate variability; human; hypovolemia; intermethod comparison; lower body negative pressure; major clinical study; mathematical computing; mathematical model; mathematical parameters; power spectral density; priority journal; signal processing; Algorithms; Analysis of Variance; Artificial Intelligence; Diagnosis, Computer-Assisted; Electrocardiography; Fractals; Heart Rate; Humans; Hypovolemia; Lower Body Negative Pressure; Monitoring, Physiologic; Retrospective Studies; Severity of Illness Index; Wavelet Analysis",Article,Scopus,2-s2.0-84878324123
"Carmona C.J., Chrysostomou C., Seker H., Del Jesus M.J.","Fuzzy rules for describing subgroups from Influenza A virus using a multi-objective evolutionary algorithm",2013,"Applied Soft Computing Journal",11,10.1016/j.asoc.2013.04.011,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878157695&doi=10.1016%2fj.asoc.2013.04.011&partnerID=40&md5=bd7b8e44baf7c6bd5a08907489e3da8f","Extraction of biologically-meaningful knowledge is one of the important and challenging tasks in bioinformatics, in particular computational analysis of DNA and protein sequences, in order to identify biological function(s) and behaviour(s) of newly-extracted sequences. Computational intelligence techniques in corporation with sequence-driven features have been applied to tackle the problem and help classify different functional classes of the sequences. In order to study this problem, subgroup discovery algorithms together with a signal processing-based feature extraction method are applied, where the sequences are represented as a signal. The applicability of this method has been studied through four different Neuraminidase genes of Influenza A subtypes, H1N1, H2N2, H3N2 and H5N1. The results yielded not only higher predictive accuracy over these four classes of the proteins but also interpretable rule-based representation of the descriptive model with a significantly reduced feature set driven by means of the signal processing method. Subgroup discovery technique based on evolutionary fuzzy systems is expected to open new areas of research in bioinformatics and further help identify and understand more focused therapeutic protein targets. © 2013 Elsevier B.V.","Evolutionary fuzzy system; Influenza A virus; Multi-objective evolutionary algorithm; Neuraminidase Subgroup discovery","Computational analysis; Computational intelligence techniques; Evolutionary Fuzzy systems; Feature extraction methods; Influenza A virus; Multi objective evolutionary algorithms; Rule-based representations; Subgroup discovery; Artificial intelligence; Bioinformatics; Feature extraction; Fuzzy systems; Proteins; Signal processing; Viruses; DNA sequences",Article,Scopus,2-s2.0-84878157695
"Lo Re G., Morana M., Ortolani M.","Improving user experience via motion sensors in an Ambient Intelligence scenario",2013,"PECCS 2013 - Proceedings of the 3rd International Conference on Pervasive Embedded Computing and Communication Systems",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878082186&partnerID=40&md5=f1f241f06c328e1c6f78a0152b7955ea","Ambient Intelligence (AmI) is a new paradigm in Artificial Intelligence that aims at exploiting the information about the environment state in order to adapt it to the user preferences. AmI systems are usually based on several cheap and unobtrusive sensing devices that allow for continuous monitoring in different scenarios. In this work we present a gesture recognition module for the management of an office environment using a motion sensor device, namely Microsoft Kinect, as the primary interface between the user and the AmI system. The proposed gesture recognition method is based on both RGB and depth information for detecting the hand of the user and a fuzzy rule for determining the state of the detected hand. The shape of the hand is interpreted as one of the basic symbols of a grammar expressing a set of commands for the actuators of the AmI system. In order to maintain a high level of pervasiveness, the Kinect sensor is connected to a miniature computer capable of real-time processing.","Ambient Intelligence; Embedded systems; HCI; Sensor networks","Ambient intelligence; Continuous monitoring; Depth information; Environment state; Microsoft kinect; Office environments; Realtime processing; Sensing devices; Communication systems; Embedded systems; Gesture recognition; Human computer interaction; Minicomputers; Sensor networks; Sensors; User interfaces; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84878082186
"Chou J.-S., Cheng M.-Y., Wu Y.-W.","Improving classification accuracy of project dispute resolution using hybrid artificial intelligence and support vector machine models",2013,"Expert Systems with Applications",11,10.1016/j.eswa.2012.10.036,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872870175&doi=10.1016%2fj.eswa.2012.10.036&partnerID=40&md5=3f691688402009a5c2143f212aa7fd40","Support vector machines (SVMs) have been applied successfully to construction knowledge domains. However, SVMs, as a baseline model, still have a potential improvement space by integrating hybrid intelligence. This work compares the performance of various classification models using the combination of fuzzy logic, a fast and messy genetic algorithm, and SVMs. A set of public-private partnership projects was collected as a real case study in construction management. The data were split into mutually independent folds for cross validation. Experimental results demonstrate that the proposed hybrid artificial intelligence system has the best and most reliable classification accuracy at 77.04%, a 24.76% improvement compared with that of SVMs in predicting project dispute resolution (PDR) outcomes (i.e., mediation, arbitration, litigation, negotiation, and administrative appeals) when the dispute category and phase in which a dispute occurs are known during project execution. This work demonstrates the improvement capability of hybrid intelligence in classifying PDR predictions related to public infrastructure projects. © 2012 Elsevier Ltd. All rights reserved.","Classification; Construction management; Dispute resolutions; Fuzzy logic; Genetic algorithm; Hybrid intelligence; Support vector machines","Administrative appeals; Artificial intelligence systems; Baseline models; Classification accuracy; Classification models; Construction knowledge; Construction management; Cross validation; Dispute resolution; Hybrid intelligence; Messy genetic algorithms; Mutually independents; Project execution; Public infrastructure project; Public private partnerships; Support vector; Artificial intelligence; Classification (of information); Fuzzy logic; Genetic algorithms; Laws and legislation; Project management; Support vector machines",Article,Scopus,2-s2.0-84872870175
"Youngs N., Penfold-Brown D., Drew K., Shasha D., Bonneau R.","Parametric Bayesian priors and better choice of negative examples improve protein function prediction.",2013,"Bioinformatics (Oxford, England)",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886411294&partnerID=40&md5=c9a0a79601ec49935e2f96cd882ea47e","Computational biologists have demonstrated the utility of using machine learning methods to predict protein function from an integration of multiple genome-wide data types. Yet, even the best performing function prediction algorithms rely on heuristics for important components of the algorithm, such as choosing negative examples (proteins without a given function) or determining key parameters. The improper choice of negative examples, in particular, can hamper the accuracy of protein function prediction. We present a novel approach for choosing negative examples, using a parameterizable Bayesian prior computed from all observed annotation data, which also generates priors used during function prediction. We incorporate this new method into the GeneMANIA function prediction algorithm and demonstrate improved accuracy of our algorithm over current top-performing function prediction methods on the yeast and mouse proteomes across all metrics tested. Code and Data are available at: http://bonneaulab.bio.nyu.edu/funcprop.html",,"protein; proteome; algorithm; animal; article; artificial intelligence; Bayes theorem; gene regulatory network; genetics; genome; metabolism; molecular genetics; mouse; physiology; protein analysis; yeast; Algorithms; Animals; Artificial Intelligence; Bayes Theorem; Gene Regulatory Networks; Genome; Mice; Molecular Sequence Annotation; Protein Interaction Mapping; Proteins; Proteome; Yeasts",Article,Scopus,2-s2.0-84886411294
"Seixas J.M., Faria J., Souza Filho J.B.O., Vieira A.F.M., Kritski A., Trajman A.","Artificial neural network models to support the diagnosis of pleural tuberculosis in adult patients",2013,"International Journal of Tuberculosis and Lung Disease",11,10.5588/ijtld.12.0829,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876848484&doi=10.5588%2fijtld.12.0829&partnerID=40&md5=7cbf9d989f10be79a58106770f36a080","BACKGROUND: Clinicians in countries with high tuberculosis (TB) prevalence often treat pleural TB based on clinical grounds, as the availability and sensitivity of diagnostic tests are poor. OBJECTIVE: To evaluate the role of artificial neural networks (ANN) as an aid for the non-invasive diagnosis of pleural TB. These tools can be used in simple computer devices (tablets) without remote internet connection. METHODS: The clinical history and human immunodeficiency virus (HIV) status of 137 patients were prospectively entered in a database. Both non-linear ANN and the linear Fisher discriminant were used to calculate performance indexes based on clinical grounds. The same procedure was performed including pleural fluid test results (smear, culture, adenosine deaminase, serology and nucleic acid amplification test). The gold standard was any positive test for TB. RESULTS: In pre-test modelling, the neural model reached >90% accuracy (Fisher discriminant 74.5%). Under pre-test conditions, ANN had better accuracy compared to each test considered separately. CONCLUSIONS: ANN are highly reliable for diagnosing pleural TB based on clinical grounds and HIV status only, and are useful even in remote conditions lacking access to sophisticated medical or computer infrastructure. In other better-equipped scenarios, these tools should be evaluated as substitutes for thoracocentesis and pleural biopsy. © 2013 The Union.","Accuracy; Artificial intelligence; Diagnosis; Pleurisy; Tuberculosis","adenosine deaminase; adult; article; artificial neural network; comparative study; diagnostic accuracy; diagnostic test accuracy study; female; human; Human immunodeficiency virus; information processing; major clinical study; male; medical history; nucleic acid amplification; outcome assessment; pleura biopsy; pleura fluid; priority journal; sensitivity and specificity; serology; thoracocentesis; tuberculous pleurisy; Adult; Bacteriological Techniques; Biopsy; Coinfection; Diagnosis, Computer-Assisted; Discriminant Analysis; Disease Progression; Early Diagnosis; HIV Infections; Humans; Linear Models; Mycobacterium tuberculosis; Neural Networks (Computer); Nonlinear Dynamics; Paracentesis; Pleural Effusion; Predictive Value of Tests; Prognosis; Prospective Studies; Tuberculosis, Pleural",Article,Scopus,2-s2.0-84876848484
"Al-Helo S., Alomari R.S., Ghosh S., Chaudhary V., Dhillon G., Al-Zoubi M.B., Hiary H., Hamtini T.M.","Compression fracture diagnosis in lumbar: A clinical CAD system",2013,"International Journal of Computer Assisted Radiology and Surgery",11,10.1007/s11548-012-0796-0,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878015966&doi=10.1007%2fs11548-012-0796-0&partnerID=40&md5=04d2debbaf61d1bdd246ee897a1380a9","Purpose Lower back pain affects 80-90 % of all people at some point during their life time, and it is considered as the second most neurological ailment after headache. It is caused by defects in the discs, vertebrae, or the soft tissues. Radiologists perform diagnosis mainly from X-ray radiographs, MRI, or CT depending on the target organ. Vertebra fracture is usually diagnosed from X-ray radiographs or CT depending on the available technology. In this paper, we propose a fully automated Computer-Aided Diagnosis System (CAD) for the diagnosis of vertebra wedge compression fracture from CT images that integrates within the clinical routine. Methods We perform vertebrae localization and labeling, segment the vertebrae, and then diagnose each vertebra. We perform labeling and segmentation via coordinated system that consists of an Active Shape Model and a Gradient Vector Flow Active Contours (GVF-Snake). We propose a set of clinically motivated features that distinguish the fractured vertebra. We provide two machine learning solutions that utilize our features including a supervised learner (Neural Networks (NN)) and an unsupervised learner (K-Means). Results We validate our method on a set of fifty (thirty abnormal) Computed Tomography (CT) cases obtained from our collaborating radiology center. Our diagnosis detection accuracy using NN is 93.2 % on average while we obtained 98 % diagnosis accuracy using K-Means. Our K-Means resulted in a specificity of 87.5 % and sensitivity over 99 %. Conclusions We presented a fully automated CAD system that seamlessly integrates within the clinical work flow of the radiologist. Our clinically motivated features resulted in a great performance of both the supervised and unsupervised learners that we utilize to validate our CAD system. Our CAD system results are promising to serve in clinical applications after extensive validation. © 2012 CARS.","Active Shape Model (ASM); Computed Tomography (CT); K-Means; Neural Network; Vertebrae fracture","article; compression fracture; computer assisted diagnosis; computer assisted tomography; controlled study; diagnostic accuracy; image analysis; image reconstruction; intervertebral disk disease; low back pain; machine learning; nuclear magnetic resonance imaging; priority journal; radiography; radiologist; sensitivity and specificity; soft tissue defect; spine fracture; spine radiography; vertebra malformation; Algorithms; Artificial Intelligence; Cohort Studies; Diagnosis, Computer-Assisted; Fractures, Compression; Humans; Lumbar Vertebrae; Sensitivity and Specificity; Spinal Fractures; Tomography, X-Ray Computed",Article,Scopus,2-s2.0-84878015966
"Warwick K., Shah H., Moor J.","Some implications of a sample of practical turing tests",2013,"Minds and Machines",11,10.1007/s11023-013-9301-y,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877764135&doi=10.1007%2fs11023-013-9301-y&partnerID=40&md5=7755af67c233218f1930705c6229f50a","A series of imitation games involving 3-participant (simultaneous comparison of two hidden entities) and 2-participant (direct interrogation of a hidden entity) were conducted at Bletchley Park on the 100th anniversary of Alan Turing's birth: 23 June 2012. From the ongoing analysis of over 150 games involving (expert and non-expert, males and females, adults and child) judges, machines and hidden humans (foils for the machines), we present six particular conversations that took place between human judges and a hidden entity that produced unexpected results. From this sample we focus on features of Turing's machine intelligence test that the mathematician/code breaker did not consider in his examination for machine thinking: the subjective nature of attributing intelligence to another mind. © 2013 Springer Science+Business Media Dordrecht.","Chatbots; Imitation game; Intelligence; Nature of thought; Philosophy of mind; Practical Turing tests; Understanding","Chatbots; Imitation games; Intelligence; Nature of thought; Philosophy of mind; Turing tests; Understanding; Philosophical aspects; Artificial intelligence",Article,Scopus,2-s2.0-84877764135
"Dietl W., Müller P.","Object ownership in program verification",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-36946-9-11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875973848&doi=10.1007%2f978-3-642-36946-9-11&partnerID=40&md5=2a0a02fa282a1197901a28e3dec9957d","Dealing with aliasing is one of the key challenges for the verification of imperative programs. For instance, aliases make it difficult to determine which abstractions are potentially affected by a heap update and to determine which locks need to be acquired to avoid data races. Object ownership was one of the first approaches that allowed programmers to control aliasing and to restrict the operations that can be applied to a reference. It thus enabled sound, modular, and automatic verification of heap-manipulating programs. In this paper, we present two ownership systems that have been designed specifically to support program verification - Universe Types and Spec#'s Dynamic Ownership - and explain their applications in program verification, illustrated through a series of Spec# examples. © Springer-Verlag Berlin Heidelberg 2013.",,"Aliasing; Automatic verification; Data races; Heap-manipulating programs; Imperative programs; Program Verification; Support programs; Universe types; Artificial intelligence; Object oriented programming",Article,Scopus,2-s2.0-84875973848
"Dhall A., Joshi J., Radwan I., Goecke R.","Finding happiest moments in a social context",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-37444-9_48,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875911112&doi=10.1007%2f978-3-642-37444-9_48&partnerID=40&md5=ea0baca29ce5d94c327600a4dfd03a81","We study the problem of expression analysis for a group of people. Automatic facial expression analysis has seen much research in recent times. However, little attention has been given to the estimation of the overall expression theme conveyed by an image of a group of people. Specifically, this work focuses on formulating a framework for happiness intensity estimation for groups based on social context information. The main contributions of this paper are: a) defining automatic frameworks for group expressions; b) social features, which compute weights on expression intensities; c) an automatic face occlusion intensity detection method; and d) an 'in the wild' labelled database containing images having multiple subjects from different scenarios. The experiments show that the global and local contexts provide useful information for theme expression analysis, with results similar to human perception results. © 2013 Springer-Verlag.",,"Automatic facial expression analysis; Expression analysis; Expression intensities; Face occlusion; Human perception; Intensity detection; Intensity estimation; Local contexts; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84875911112
"Rajagopal A.K., Subramanian R., Vieriu R.L., Ricci E., Lanz O., Ramakrishnan K., Sebe N.","An adaptation framework for head-pose classification in dynamic multi-view scenarios",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-37444-9_51,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875909775&doi=10.1007%2f978-3-642-37444-9_51&partnerID=40&md5=65f39635cf15ad2261eadfc7082743a7","Multi-view head-pose estimation in low-resolution, dynamic scenes is difficult due to blurred facial appearance and perspective changes as targets move around freely in the environment. Under these conditions, acquiring sufficient training examples to learn the dynamic relationship between position, face appearance and head-pose can be very expensive. Instead, a transfer learning approach is proposed in this work. Upon learning a weighted-distance function from many examples where the target position is fixed, we adapt these weights to the scenario where target positions are varying. The adaptation framework incorporates reliability of the different face regions for pose estimation under positional variation, by transforming the target appearance to a canonical appearance corresponding to a reference scene location. Experimental results confirm effectiveness of the proposed approach, which outperforms state-of-the-art by 9.5% under relevant conditions. To aid further research on this topic, we also make DPOSE- a dynamic, multi-view head-pose dataset with ground-truth publicly available with this paper. © 2013 Springer-Verlag.",,"Adaptation framework; Dynamic scenes; Facial appearance; Low resolution; Pose estimation; Target position; Training example; Transfer learning; Artificial intelligence; Data processing",Conference Paper,Scopus,2-s2.0-84875909775
"Alba A., Aguilar-Ponce R.M., Vigueras-Gómez J.F., Arce-Santana E.","Phase correlation based image alignment with subpixel accuracy",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-37807-2_15,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875866969&doi=10.1007%2f978-3-642-37807-2_15&partnerID=40&md5=cc95c1f4650b7dce0448c6a8109ba11c","The phase correlation method is a well-known image alignment technique with broad applications in medical image processing, image stitching, and computer vision. This method relies on estimating the maximum of the phase-only correlation (POC) function, which is defined as the inverse Fourier transform of the normalized cross-spectrum between two images. The coordinates of the maximum correspond to the translation between the two images. One of the main drawbacks of this method, in its basic form, is that the location of the maximum can only be obtained with integer accuracy. In this paper, we propose a new technique to estimate the location with subpixel accuracy, by minimizing the magnitude of gradient of the POC function around a point near the maximum. We also present some experimental results where the proposed method shows an increased accuracy of at least one order of magnitude with respect to the base method. Finally, we illustrate the application of the proposed algorithm to the rigid registration of digital images. © 2013 Springer-Verlag.",,"Broad application; Image alignment; Inverse Fourier transforms; Phase correlation; Phase correlation method; Phase-only correlation; Rigid registration; Subpixel accuracy; Alignment; Image processing; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84875866969
"Nápoles G., Grau I., León M., Grau R.","Modelling, aggregation and simulation of a dynamic biological system through fuzzy cognitive maps",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-37798-3_17,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875823792&doi=10.1007%2f978-3-642-37798-3_17&partnerID=40&md5=ffef4400e3ddf4ef5009f82a8f935de3","The complex dynamics of Human Immunodeficiency Virus leads to serious problems on predicting the drug resistance. Several machine learning techniques have been proposed for modelling this classification problem, but most of them are difficult to aggregate and interpret. In fact, in last years the protein modelling of this virus has become, from diverse points of view, an open problem for researchers. This paper presents a modelling of the protease protein as a dynamic system through Fuzzy Cognitive Maps, using the amino acids contact energies for the sequence description. In addition, a learning scheme based on swarm intelligence called PSO-RSVN is used to estimate the causal weight matrix that characterizes these structures. Finally, an aggregation procedure with previously adjusted maps is applied for obtaining a prototype map, in order to discover knowledge in the causal influences, and simulate the system behaviour when a single (or multiple) mutation takes place. © 2013 Springer-Verlag.","Aggregation Procedure; Fuzzy Cognitive Maps; Human Immunodeficiency Virus; Knowledge Discovery; Learning Scheme; Simulation","Causal influences; Contact energies; Fuzzy cognitive map; Human immunodeficiency virus; Learning schemes; Machine learning techniques; Simulation; Swarm Intelligence; Amino acids; Artificial intelligence; Data mining; Diseases; Fuzzy rules; Fuzzy systems; Learning systems; Proteins; Viruses; Biological systems",Conference Paper,Scopus,2-s2.0-84875823792
"Ismail M.N., Aborujilah A., Musa S., Shahzad A.","Detecting flooding based DoS attack in cloud computing environment using covariance matrix approach",2013,"Proceedings of the 7th International Conference on Ubiquitous Information Management and Communication, ICUIMC 2013",11,10.1145/2448556.2448592,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875838637&doi=10.1145%2f2448556.2448592&partnerID=40&md5=112b9c87e788073c77c5110f4deebe0f","The internet is gaining a lot of importance day by day, especially with the emergence of cloud technology. This new technology has made a new computing service to end users that include, PaaS, SaaS. On the other hand, this technology was accompanied with some shortages. The most serious obstacle is the security challenges because of the cloud is characterized by computing resource sharing and multi-tenancy features and as a result flooding based denial of service attack has been observed. This effect on performance and quality of service on cloud. To overcome this security challenge, there are several methods to detect and prevent this kind of attack. Most of these approaches are using statistical and/or artificial intelligence methods. In this research paper a new model to detect flooding based DoS attack in cloud environment has been suggested consisting three phases. (1) The first-phase is to model the normal traffic pattern for baseline profiling and (2) the second phase is the intrusion detection processes and (3) finally prevention phase. The covariance Matrix mathematical model is used as detecting method. The phase (1) and (2) have been implemented in real test bed. From the result, it is proven that we can detect the flooding attack effectively. Copyright © 2013 ACM.","Cloud computing; Covariance matrix; DoS flooding-based attack; Intrusion detecting","Artificial intelligence methods; Cloud computing environments; Computing resource; Denial of service attacks; Flooding-based attacks; Intrusion detecting; Matrix mathematical models; Security challenges; Artificial intelligence; Cloud computing; Communication; Computer crime; Computer systems; Covariance matrix; Equipment testing; Information management; Intrusion detection; Mathematical models; Quality of service; Floods",Conference Paper,Scopus,2-s2.0-84875838637
"Mavrovouniotis M., Yang S.","Adapting the pheromone evaporation rate in dynamic routing problems",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-37192-9-61,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875637343&doi=10.1007%2f978-3-642-37192-9-61&partnerID=40&md5=d23bffb66f51f56326f39d21cbf1b454","Ant colony optimization (ACO) algorithms have proved to be able to adapt to dynamic optimization problems (DOPs) when stagnation behaviour is avoided. Several approaches have been integrated with ACO to improve its performance for DOPs. The adaptation capabilities of ACO rely on the pheromone evaporation mechanism, where the rate is usually fixed. Pheromone evaporation may eliminate pheromone trails that represent bad solutions from previous environments. In this paper, an adaptive scheme is proposed to vary the evaporation rate in different periods of the optimization process. The experimental results show that ACO with an adaptive pheromone evaporation rate achieves promising results, when compared with an ACO with a fixed pheromone evaporation rate, for different DOPs. © Springer-Verlag Berlin Heidelberg 2013.",,"Adaptive scheme; Ant Colony Optimization algorithms; Dynamic optimization problem (DOP); Dynamic routing; Evaporation rate; Optimization process; Pheromone trails; Ant colony optimization; Artificial intelligence; Evaporation; Evolutionary algorithms; Phase transitions",Conference Paper,Scopus,2-s2.0-84875637343
"Tang F., Wang B., Zha X., Ma Z., Shao Y.","Power system transient stability assessment based on two-stage parallel hidden Markov model",2013,"Zhongguo Dianji Gongcheng Xuebao/Proceedings of the Chinese Society of Electrical Engineering",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877648931&partnerID=40&md5=5491fcf971f9d0cd73c44e964079db84","The transient stability assessment (TSA) of power system based on the artificial intelligence and machine learning method has become more popular. This paper proposed a precise pattern recognition method for TSA based on a two-stage parallel hidden Markov model (TS-PHMM). In the first stage, the sensitive feature subset was selected from the original feature set based on the relative sensitivity principle; in the second stage, the principal component analysis (PCA) method was used to decrease the subset dimension to obtain an optimized feature set. Then the optimized subset was adopted to train PHMM with a serial weight factors for TSA. Finally, in the CEPRI 8-generator 36-bus test system and a real large power system, the simulation results proved the validity and effectiveness of the feature selection approach and PHMM pattern recognition. Meanwhile, this new method needs less training samples compared with some of the common methods such as SVM and ANN to reach an equivalent accurate rate. © 2013 Chin. Soc. for Elec. Eng.","Machine learning method; Pattern recognition; Transient stability assessment (TSA); Two stage hidden Markov model (TS-PHMM)","Artificial intelligence; Learning systems; Optimization; Pattern recognition; Principal component analysis; System stability; Large power systems; Machine learning methods; Parallel hidden markov models; Pattern recognition method; Power system transient stability assessment; Relative sensitivity; Transient stability assessment; Two stage hidden Markov model (TS-PHMM); Hidden Markov models",Article,Scopus,2-s2.0-84877648931
"Casas D., Tejera M., Guillemaut J.-Y., Hilton A.","Interactive animation of 4D performance capture",2013,"IEEE Transactions on Visualization and Computer Graphics",11,10.1109/TVCG.2012.314,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875612327&doi=10.1109%2fTVCG.2012.314&partnerID=40&md5=888493fd319e8fc43884f1f1859b1941","A 4D parametric motion graph representation is presented for interactive animation from actor performance capture in a multiple camera studio. The representation is based on a 4D model database of temporally aligned mesh sequence reconstructions for multiple motions. High-level movement controls such as speed and direction are achieved by blending multiple mesh sequences of related motions. A real-time mesh sequence blending approach is introduced, which combines the realistic deformation of previous nonlinear solutions with efficient online computation. Transitions between different parametric motion spaces are evaluated in real time based on surface shape and motion similarity. Four-dimensional parametric motion graphs allow real-time interactive character animation while preserving the natural dynamics of the captured performance. © 1995-2012 IEEE.","3D video; 4D modeling; 4D performance capture; Character animation; multiview reconstruction; real-time animation; video-based animation","3-D videos; Character animation; Multi-view reconstruction; Performance capture; Real-time animations; Blending; Animation; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; computer graphics; computer interface; human; image enhancement; image subtraction; locomotion; mathematical computing; methodology; physiology; reproducibility; sensitivity and specificity; signal processing; three dimensional imaging; Algorithms; Artificial Intelligence; Computer Graphics; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Locomotion; Numerical Analysis, Computer-Assisted; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Signal Processing, Computer-Assisted; Subtraction Technique; User-Computer Interface",Article,Scopus,2-s2.0-84875612327
"Mai H., Pek E., Xue H., King S.T., Madhusudan P.","Verifying security invariants in expressoS",2013,"ACM SIGPLAN Notices",11,10.1145/2499368.2451148,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880117726&doi=10.1145%2f2499368.2451148&partnerID=40&md5=3b9b9fa400647023cf3adf92753a8069","Security for applications running on mobile devices is important. In this paper we present ExpressOS, a new OS for enabling highassurance applications to run on commodity mobile devices securely. Our main contributions are a new OS architecture and our use of formal methods for proving key security invariants about our implementation. In our use of formal methods, we focus solely on proving that our OS implements our security invariants correctly, rather than striving for full functional correctness, requiring significantly less verification effort while still proving the security relevant aspects of our system. We built ExpressOS, analyzed its security, and tested its performance. Our evaluation shows that the performance of ExpressOS is comparable to an Android-based system. In one test, we ran the same web browser on ExpressOS and on an Android-based system, and found that ExpressOS adds 16% overhead on average to the page load latency time for nine popular web sites.","Automatic theorem proving; Microkernel; Mobile security; Programming by contracts","Automatic theorem proving; Functional correctness; Latency time; Microkernel; Mobile security; Artificial intelligence; Robots; Mobile devices",Conference Paper,Scopus,2-s2.0-84880117726
"Liang C., Peng L.","An automated diagnosis system of liver disease using artificial immune and genetic algorithms",2013,"Journal of Medical Systems",11,10.1007/s10916-013-9932-9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874382643&doi=10.1007%2fs10916-013-9932-9&partnerID=40&md5=1e42208b807272fc20a4592e93fec0a9","The rise of health care cost is one of the world's most important problems. Disease prediction is also a vibrant research area. Researchers have approached this problem using various techniques such as support vector machine, artificial neural network, etc. This study typically exploits the immune system's characteristics of learning and memory to solve the problem of liver disease diagnosis. The proposed system applies a combination of two methods of artificial immune and genetic algorithm to diagnose the liver disease. The system architecture is based on artificial immune system. The learning procedure of system adopts genetic algorithm to interfere the evolution of antibody population. The experiments use two benchmark datasets in our study, which are acquired from the famous UCI machine learning repository. The obtained diagnosis accuracies are very promising with regard to the other diagnosis system in the literatures. These results suggest that this system may be a useful automatic diagnosis tool for liver disease. © 2013 Springer Science+Business Media New York.","Artificial immune; Genetic algorithm; Liver disease diagnosis; Machine learning","antibody; article; computer assisted diagnosis; controlled study; diagnostic accuracy; genetic algorithm; human; immune system; immunological memory; liver disease; machine learning; sensitivity and specificity; algorithm; artificial intelligence; biology; computer assisted diagnosis; computer simulation; evaluation study; genetics; immunology; Liver Diseases; reproducibility; support vector machine; Algorithms; Artificial Intelligence; Computational Biology; Computer Simulation; Diagnosis, Computer-Assisted; Genetics; Humans; Immune System; Liver Diseases; Reproducibility of Results; Support Vector Machines",Article,Scopus,2-s2.0-84874382643
"Elkins A.C., Dunbar N.E., Adame B., Nunamaker Jr. J.F.","Are users threatened by credibility assessment systems?",2013,"Journal of Management Information Systems",11,10.2753/MIS0742-1222290409,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880218457&doi=10.2753%2fMIS0742-1222290409&partnerID=40&md5=fb7216575bbbcb71c188b26a4e1ecf04","Despite the improving accuracy of agent-based expert systems, human expert users aided by these systems have not improved their accuracy. Self-affirmation theory suggests that human expert users could be experiencing threat, causing them to act defensively and ignore the system's conflicting recommendations. Previous research has demonstrated that affirming an individual in an unrelated area reduces defensiveness and increases objectivity to conflicting information. Using an affirmation manipulation prior to a credibility assessment task, this study investigated if experts are threatened by counterattitudinal expert system recommendations. For our study, 178 credibility assessment experts from the American Polygraph Association (n = 134) and the European Union's border security agency Frontex (n = 44) interacted with a deception detection expert system to make a deception judgment that was immediately contradicted. Reducing the threat prior to making their judgments did not improve accuracy, but did improve objectivity toward the system. This study demonstrates that human experts are threatened by advanced expert systems that contradict their expertise. As more and more systems increase integration of artificial intelligence and inadvertently assail the expertise and abilities of users, threat and self-evaluative concerns will become an impediment to technology acceptance. © 2013 M.E. Sharpe, Inc. All rights reserved.","credibility assessment systems; deception detection; expert systems; user anxiety","Agent based; Border security; Credibility assessment; Deception detection; European union; Human expert; Technology acceptance; user anxiety; Artificial intelligence; Expert systems",Article,Scopus,2-s2.0-84880218457
"Oliveira L., Ladouceur C.D., Phillips M.L., Brammer M., Mourao-Miranda J.","What Does Brain Response to Neutral Faces Tell Us about Major Depression? Evidence from Machine Learning and fMRI",2013,"PLoS ONE",11,10.1371/journal.pone.0060121,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875678697&doi=10.1371%2fjournal.pone.0060121&partnerID=40&md5=97959d971245c09f914e6e3bf1c25900","Introduction: A considerable number of previous studies have shown abnormalities in the processing of emotional faces in major depression. Fewer studies, however, have focused specifically on abnormal processing of neutral faces despite evidence that depressed patients are slow and less accurate at recognizing neutral expressions in comparison with healthy controls. The current study aimed to investigate whether this misclassification described behaviourally for neutral faces also occurred when classifying patterns of brain activation to neutral faces for these patients. Methods: Two independent depressed samples: (1) Nineteen medication-free patients with depression and 19 healthy volunteers and (2) Eighteen depressed individuals and 18 age and gender-ratio-matched healthy volunteers viewed emotional faces (sad/neutral; happy/neutral) during an fMRI experiment. We used a new pattern recognition framework: first, we trained the classifier to discriminate between two brain states (e.g. viewing happy faces vs. viewing neutral faces) using data only from healthy controls (HC). Second, we tested the classifier using patterns of brain activation of a patient and a healthy control for the same stimuli. Finally, we tested if the classifier's predictions (predictive probabilities) for emotional and neutral face classification were different for healthy controls and depressed patients. Results: Predictive probabilities to patterns of brain activation to neutral faces in both groups of patients were significantly lower in comparison to the healthy controls. This difference was specific to neutral faces. There were no significant differences in predictive probabilities to patterns of brain activation to sad faces (sample 1) and happy faces (samples 2) between depressed patients and healthy controls. Conclusions: Our results suggest that the pattern of brain activation to neutral faces in depressed patients is not consistent with the pattern observed in healthy controls subject to the same stimuli. This difference in brain activation might underlie the behavioural misinterpretation of the neutral faces content by the depressed patients. © 2013 Oliveira et al.",,"accuracy; adult; article; brain function; classifier; clinical article; controlled study; evidence based medicine; face profile; facial expression; female; functional magnetic resonance imaging; Gaussian process classifier; human; major depression; male; neuroimaging; nuclear magnetic resonance scanner; pattern recognition; predictive value; task performance; visual discrimination; Adolescent; Adult; Artificial Intelligence; Brain; Case-Control Studies; Depressive Disorder, Major; Face; Facial Expression; Female; Grief; Happiness; Humans; Magnetic Resonance Imaging; Male; Middle Aged; Pattern Recognition, Visual; Psychotropic Drugs",Article,Scopus,2-s2.0-84875678697
"Xu Y., Li D., Chen Q., Fan Y.","Full supervised learning for osteoporosis diagnosis using micro-CT images",2013,"Microscopy Research and Technique",11,10.1002/jemt.22171,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875604996&doi=10.1002%2fjemt.22171&partnerID=40&md5=c725aa9a2f7fdddba6ee19769a76d1ae","Early osteoporosis diagnosis is of important significance for reducing fracture risk. Image analysis provides a new perspective for noninvasive diagnosis in recent years. In this article, we propose a novel method based on machine-learning method performed on micro-CT images todiagnose osteoporosis. The aim of this work is to find a way to more effectively and accurately diagnose osteoporosis on which many methods have been proposed and practiced. In this method, in contrast to the previously proposed methods in which features are analyzed individually, several features are combined to build a classifier for distinguishing osteoporosis group and normal group. Twelve features consisting of two groups are involved in our research, including bone volume/total volume (BV/TV), bone surface/bone volume (BS/BV), trabecular number (Tb.N), obtained from the software of micro-CT, and other four features from volumetric topological analysis (VTA). Support vector machine (SVM) method and k-nearest neighbor (kNN) method are introduced to create classifiers with these features due to their excellent performances on classification. In the experiment, 200 micro-CT images are used in which half are from osteoporosis patients and the rest are from normal people. The performance of the obtained classifiers is evaluated by precision, recall, and F-measure. The best performance with precision of 100%, recall of 100%, and F-measure of 100% is acquired when all the features are included. The satisfying result demonstrates that SVM and kNN are effective for diagnosing osteoporosis with micro-CT images. © 2013 Wiley Periodicals, Inc.","Micro-CT images; Osteoporosis; Trabecular bone; Volumetric topological analysis","article; artificial intelligence; bone; evaluation; human; methodology; micro-computed tomography; osteoporosis; radiography; support vector machine; Artificial Intelligence; Bone and Bones; Humans; Osteoporosis; Support Vector Machines; X-Ray Microtomography",Article,Scopus,2-s2.0-84875604996
"Bhowal K., Bhattacharyya D., Jyoti Pal A., Kim T.-H.","A GA based audio steganography with enhanced security",2013,"Telecommunication Systems",11,10.1007/s11235-011-9542-0,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879991755&doi=10.1007%2fs11235-011-9542-0&partnerID=40&md5=f2aa383813b3ca5b710d6e8cf1469dcc","In the current internet community, secure data transfer is limited due to its attack made on data communication. So more robust methods are chosen so that they ensure secured data transfer. One of the solutions which came to the rescue is the audio Steganography. ""A GA Based Audio Steganography with enhanced security"" is one propose system which is based on audio Steganography and cryptography, ensures secure data transfer between the source and destination. Here we present a novel, principled approach to resolve the remained problems of substitution technique of audio Steganography. We use most powerful encryption algorithm (RSA) to encrypt message in the first level of security, which is very complex to break. In the second level, we use a more powerful GA based LSB (Least Significant Bit) Algorithm to encode the encrypted message into audio data. Here encrypted message bits are embedded into random and higher LSB layers, resulting in increased robustness against noise addition. The robustness specially would be increased against those intentional attacks which try to reveal the hidden message and also some unintentional attacks like noise addition as well. On the other hand, to reduce the distortion, GA operators are used. The basic idea behind this paper is maintained randomness in message bit insertion into audio data for hiding the data from hackers and multi-objective GA is used to reduce distortion. © 2011 Springer Science+Business Media, LLC.","Artificial intelligence; Audio steganography; Data hiding; Genetic algorithm; Substitution techniques","Audio steganography; Data hiding; Encryption algorithms; Internet communities; Least significant bits; Robustness against noise; Secure data transfer; Substitution techniques; Acoustic noise; Artificial intelligence; Cryptography; Data transfer; Personal computing; Steganography; Genetic algorithms",Article,Scopus,2-s2.0-84879991755
"Zhu L., Gong H., Li X., Li Y., Su X., Guo G.","Comprehensive analysis and artificial intelligent simulation of land subsidence of Beijing, China",2013,"Chinese Geographical Science",11,10.1007/s11769-013-0589-6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880539695&doi=10.1007%2fs11769-013-0589-6&partnerID=40&md5=1eb3287ac6c2ce5c4ae300f845c49f1a","Mechanism and modeling of the land subsidence are complex because of the complicate geological background in Beijing, China. This paper analyzed the spatial relationship between land subsidence and three factors, including the change of groundwater level, the thickness of compressible sediments and the building area by using remote sensing and GIS tools in the upper-middle part of alluvial-proluvial plain fan of the Chaobai River in Beijing. Based on the spatial analysis of the land subsidence and three factors, there exist significant non-linear relationship between the vertical displacement and three factors. The Back Propagation Neural Network (BPN) model combined with Genetic Algorithm (GA) was used to simulate regional distribution of the land subsidence. Results showed that at field scale, the groundwater level and land subsidence showed a significant linear relationship. However, at regional scale, the spatial distribution of groundwater depletion funnel did not overlap with the land subsidence funnel. As to the factor of compressible strata, the places with the biggest compressible strata thickness did not have the largest vertical displacement. The distributions of building area and land subsidence have no obvious spatial relationships. The BPN-GA model simulation results illustrated that the accuracy of the trained model during fifty years is acceptable with an error of 51% of verification data less than 20 mm and the average of the absolute error about 32 mm. The BPN model could be utilized to simulate the general distribution of land subsidence in the study area. Overall, this work contributes to better understand the complex relationship between the land subsidence and three influencing factors. And the distribution of the land subsidence can be simulated by the trained BPN-GA model with the limited available dada and acceptable accuracy. © 2013 Science Press, Northeast Institute of Geography and Agricultural Ecology, CAS and Springer-Verlag Berlin Heidelberg.","Back Propagation Neural Network and Genetic Algorithm (BPN-GA) model; building area; compressible sediments thickness; groundwater level change; land subsidence","alluvial plain; artificial intelligence; artificial neural network; back propagation; displacement; genetic algorithm; GIS; groundwater; remote sensing; sediment thickness; spatial analysis; subsidence; water level; Beijing [China]; China",Article,Scopus,2-s2.0-84880539695
"Dolan M.J., Davidson E.M., Ault G.W., Bell K.R.W., McArthur S.D.J.","Distribution power flow management utilizing an online constraint programming method",2013,"IEEE Transactions on Smart Grid",11,10.1109/TSG.2012.2234148,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878333536&doi=10.1109%2fTSG.2012.2234148&partnerID=40&md5=1a89ccab191cbe8e446dcdb7b79909ad","This paper presents a novel active power flow management (PFM) method for managing multiple distributed generator (DG) units connected to medium voltage distribution networks. The method uses the artificial intelligence technique of constraint programming to autonomously manage DG real power outputs and offers flexible and network agnostic characteristics. The method is assessed using multiple scenarios on two real case study networks to examine simulated closed-loop control actions under certain thermal excursions. The test cases are explored with algorithms implemented, in software, on commercially available substation computing hardware to identify computation timescales and investigate algorithm robustness when presented with measurement error. The archival value of this paper is in the specification and evaluation of a novel application of the constraint programming technique for online control of DG in thermally constrained distribution networks. © 2010-2012 IEEE.","Active network management; distributed control; distributed generation; smart grids","Artificial intelligence techniques; Closed-loop control; Constraint programming; Distributed control; Distribution power flow; Medium-voltage distribution networks; Multiple distributed generators; Smart grid; Active networks; Algorithms; Computer programming; Constraint theory; Network management; Software testing; Distributed power generation",Article,Scopus,2-s2.0-84878333536
"Straßburger L.","Cut elimination in nested sequents for intuitionistic modal logics",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-37075-5_14,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874429388&doi=10.1007%2f978-3-642-37075-5_14&partnerID=40&md5=b11cf9bdb94c6812d80a1d4c8c02b3a0","We present cut-free deductive systems without labels for the intuitionistic variants of the modal logics obtained by extending IK with a subset of the axioms d, t, b, 4, and 5. For this, we use the formalism of nested sequents, which allows us to give a uniform cut elimination argument for all 15 logic in the intuitionistic S5 cube. © 2013 Springer-Verlag.",,"Cut elimination; Deductive systems; Modal logic; Nested sequents; Artificial intelligence; Formal logic",Conference Paper,Scopus,2-s2.0-84874429388
"Arapinis M., Cortier V., Kremer S., Ryan M.","Practical everlasting privacy",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-36830-1_2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874414638&doi=10.1007%2f978-3-642-36830-1_2&partnerID=40&md5=8bd70d5d2cb9f22d371ca898e6b4302c","Will my vote remain secret in 20 years? This is a natural question in the context of electronic voting, where encrypted votes may be published on a bulletin board for verifiability purposes, but the strength of the encryption is eroded with the passage of time. The question has been addressed through a property referred to as everlasting privacy. Perfect everlasting privacy may be difficult or even impossible to achieve, in particular in remote electronic elections. In this paper, we propose a definition of practical everlasting privacy. The key idea is that in the future, an attacker will be more powerful in terms of computation (he may be able to break the cryptography) but less powerful in terms of the data he can operate on (transactions between a vote client and the vote server may not have been stored). We formalize our definition of everlasting privacy in the applied-pi calculus. We provide the means to characterize what an attacker can break in the future in several cases. In particular, we model this for perfectly hiding and computationally binding primitives (or the converse), such as Pedersen commitments, and for symmetric and asymmetric encryption primitives. We adapt existing tools, in order to allow us to automatically prove everlasting privacy. As an illustration, we show that several variants of Helios (including Helios with Pedersen commitments) and a protocol by Moran and Naor achieve practical everlasting privacy, using the ProVerif and the AKiSs tools. © 2013 Springer-Verlag.",,"Asymmetric encryption; Electronic election; Electronic voting; Proverif; Verifiability; Artificial intelligence; Cryptography",Conference Paper,Scopus,2-s2.0-84874414638
"Bartoletti M., Cimoli T., Zunino R.","A theory of agreements and protection",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-36830-1_10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874416879&doi=10.1007%2f978-3-642-36830-1_10&partnerID=40&md5=384a6877bf2ac6283bac9bc39f36949a","We present a theory of contracts. Contracts are interacting processes with an explicit notion of obligations and objectives. We model processes and their obligations as event structures. We define a general notion of agreement, by interpreting contracts as multi-player concurrent games. A participant agrees on a contract if she has a strategy to reach her objectives (or make another participant chargeable for a violation), whatever the moves of her adversaries. We then tackle the problem of protection. A participant is protected by a contract when she has a strategy to defend herself in all possible contexts, even in those where she has not reached an agreement. We show that, in a relevant class of contracts, agreements and protection mutually exclude each other. We then propose a novel formalism for modelling contractual obligations: event structures with circular causality. Using this model, we show how to construct contracts which guarantee both agreements and protection. © 2013 Springer-Verlag.",,"Concurrent games; Contractual obligations; Event structures; Interacting process; Artificial intelligence; Contracts",Conference Paper,Scopus,2-s2.0-84874416879
"Gamez N., Fuentes L.","Architectural evolution of FamiWare using cardinality-based feature models",2013,"Information and Software Technology",11,10.1016/j.infsof.2012.06.012,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872980906&doi=10.1016%2fj.infsof.2012.06.012&partnerID=40&md5=f896a99a2d17fb0a14e016f00516af2c","Context: Ambient Intelligence systems domain is an outstanding example of modern systems that are in permanent evolution, as new devices, technologies or facilities are continuously appearing. This means it would be desirable to have a mechanism that helps with the propagation of evolution changes in deployed systems. Objective: We present a software product line engineering process to manage the evolution of FamiWare, a family of middleware for ambient intelligence environments. This process drives the evolution of FamiWare middleware configurations using cardinality-based feature models, which are especially well suited to express the structural variability of ambient intelligence systems. Method: FamiWare uses cardinality-based feature models and clonable features to model the structural variability present in ambient intelligence systems, composed of a large variety of heterogeneous devices. Since the management evolution of configurations with clonable features is manually untreatable due to the high number of features, our process automates it and propagates changes made at feature level to the architectural components of the FamiWare middleware. This is a model driven development process as the evolution management, the propagation of evolution changes and the code generation are performed using some kind of model mappings and transformations. Concretely we present a variability modelling language to map the selection of features to the corresponding FamiWare middleware architectural components. Results: Our process is able to manage the evolution of cardinality-based feature models with thousands of features, something which is not possible to tackle manually. Thanks to the use of the variability language and the automatic code generation it is possible to propagate and maintain a correspondence between the FamiWare architectural model and the code. The process is then able to calculate the architectural differences between the evolved configuration and the previous one. Checking these differences, our process helps to calculate the effort needed to perform the evolution changes in the customized products. To perform those tasks we have defined two operators, one to calculate the differences between two feature model configurations and another to create a new configuration from a previous one. Conclusion: Our process automatically propagates the evolution changes of the middleware family into the existing configurations where the middleware is already deployed and also helps us to calculate the effort in performing the changes in every configuration. Finally, we validated our approach, demonstrating the functioning of the defined operators and showing that by using our tool we can generate evolved configurations for FamiWare with thousands of cloned features, for several case studies. © 2012 Elsevier B.V. All rights reserved.","Evolution; Feature Models; Middleware family; Software Product Lines","Ambient intelligence; Ambient intelligence systems; Architectural components; Architectural evolution; Architectural models; Automatic code generations; Cardinality-based feature; Code Generation; Customized products; Deployed systems; Evolution; Feature level; Feature models; Heterogeneous devices; Middleware configurations; Model driven development; Model mappings; Modelling language; New devices; Process drives; Software Product Line; Software product line engineerings; Structural variability; Artificial intelligence; Cloning; Middleware; Software design",Conference Paper,Scopus,2-s2.0-84872980906
"Novatchkov H., Baca A.","Artificial intelligence in sports on the example of weight training",2013,"Journal of Sports Science and Medicine",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877339598&partnerID=40&md5=c3b745041add05a42b157c40c8b0a6f8","The overall goal of the present study was to illustrate the potential of artificial intelligence (AI) techniques in sports on the example of weight training. The research focused in particular on the implementation of pattern recognition methods for the evaluation of performed exercises on training machines. The data acquisition was carried out using way and cable force sensors attached to various weight machines, thereby enabling the measurement of essential displacement and force determinants during training. On the basis of the gathered data, it was consequently possible to deduce other significant characteristics like time periods or movement velocities. These parameters were applied for the development of intelligent methods adapted from conventional machine learning concepts, allowing an automatic assessment of the exercise technique and providing individuals with appropriate feedback. In practice, the implementation of such techniques could be crucial for the investigation of the quality of the execution, the assistance of athletes but also coaches, the training optimization and for prevention purposes. For the current study, the data was based on measurements from 15 rather inexperienced participants, performing 3-5 sets of 10- 12 repetitions on a leg press machine. The initially preprocessed data was used for the extraction of significant features, on which supervised modeling methods were applied. Professional trainers were involved in the assessment and classification processes by analyzing the video recorded executions. The so far obtained modeling results showed good performance and prediction outcomes, indicating the feasibility and potency of AI techniques in assessing performances on weight training equipment automatically and providing sportsmen with prompt advice. © Journal of Sports Science and Medicine.","Artificial intelligence; Feedback; Machine learning; Pattern recognition; Weight training",,Article,Scopus,2-s2.0-84877339598
"Franco C., Montero J., Rodríguez J.T.","A fuzzy and bipolar approach to preference modeling with application to need and desire",2013,"Fuzzy Sets and Systems",11,10.1016/j.fss.2012.06.006,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872145723&doi=10.1016%2fj.fss.2012.06.006&partnerID=40&md5=dd0c8322280b3fd5f33855b47d70fbe6","Fuzziness and bipolarity allow representing human knowledge, taking into account the gradual and the dialectic properties of language, focusing on the meaning of concepts. Under this cognitive and linguistic approach, we explore preference relations, examining their semantic decomposition through fuzzy preference structures and the specification of meaningful opposites. In particular, we introduce the Preference-Aversion (P-A) model, which allows analyzing, under an independent aggregation methodology, the possible gains and losses, like pros and cons, towards a given set of alternatives. As an attractive feature of this proposal, we show that the P-A model allows distinguishing between need and desire, contrary to common preference models where both notions are indistinguishable. © 2012 Elsevier B.V.","Decision making; Preference representation","Fuzzy preference structure; Human knowledge; Linguistic approach; Preference aversions; Preference modeling; Preference models; Preference relation; Preference representation; Artificial intelligence; Decision making; Fuzzy sets; Semantics",Article,Scopus,2-s2.0-84872145723
"Lee K., Zhu J., Shum J., Zhang Y., Muluk S.C., Chandra A., Eskandari M.K., Finol E.A.","Surface curvature as a classifier of abdominal aortic aneurysms: A comparative analysis",2013,"Annals of Biomedical Engineering",11,10.1007/s10439-012-0691-4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876168686&doi=10.1007%2fs10439-012-0691-4&partnerID=40&md5=925fb6de128554a99fc19ae0b1832b7c","An abdominal aortic aneurysm (AAA) carries one of the highest mortality rates among vascular diseases when it ruptures. To predict the role of surface curvature in rupture risk assessment, a discriminatory analysis of aneurysm geometry characterization was conducted. Data was obtained from 205 patient-specific computed tomography image sets corresponding to three AAA population subgroups: patients under surveillance, those that underwent elective repair of the aneurysm, and those with an emergent repair. Each AAA was reconstructed and their surface curvatures estimated using the biquintic Hermite finite element method. Local surface curvatures were processed into ten global curvature indices. Statistical analysis of the data revealed that the L2-norm of the Gaussian and Mean surface curvatures can be utilized as classifiers of the three AAA population subgroups. The application of statistical machine learning on the curvature features yielded 85.5% accuracy in classifying electively and emergent repaired AAAs, compared to a 68.9% accuracy obtained by using maximum aneurysm diameter alone. Such combination of non-invasive geometric quantification and statistical machine learning methods can be used in a clinical setting to assess the risk of rupture of aneurysms during regular patient follow-ups. © 2012 Biomedical Engineering Society.","Abdominal aortic aneurysm; Finite element method; Geometry quantification; Machine learning; Reconstruction; Rupture risk; Surface curvature","Abdominal aortic aneurysms; Comparative analysis; Computed tomography images; Discriminatory analysis; Hermite finite element method; Rupture risk; Statistical machine learning; Surface curvatures; Blood vessels; Computerized tomography; Finite element method; Geometry; Image reconstruction; Learning systems; Medical image processing; Population statistics; Repair; Risk assessment; abdominal aorta aneurysm; angiography; aorta rupture; article; artificial intelligence; biological model; biomedical engineering; classification; comparative study; computer assisted diagnosis; computer assisted tomography; computer simulation; finite element analysis; human; pathology; pathophysiology; three dimensional imaging; Angiography; Aortic Aneurysm, Abdominal; Aortic Rupture; Artificial Intelligence; Biomedical Engineering; Computer Simulation; Finite Element Analysis; Humans; Imaging, Three-Dimensional; Models, Cardiovascular; Radiographic Image Interpretation, Computer-Assisted; Tomography, X-Ray Computed",Article,Scopus,2-s2.0-84876168686
"Katić D., Wekerle A.-L., Görtler J., Spengler P., Bodenstedt S., Röhl S., Suwelack S., Kenngott H.G., Wagner M., Müller-Stich B.P., Dillmann R., Speidel S.","Context-aware Augmented Reality in laparoscopic surgery",2013,"Computerized Medical Imaging and Graphics",11,10.1016/j.compmedimag.2013.03.003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876995734&doi=10.1016%2fj.compmedimag.2013.03.003&partnerID=40&md5=311a94aa041741059b8ab264d1626cfa","Augmented Reality is a promising paradigm for intraoperative assistance. Yet, apart from technical issues, a major obstacle to its clinical application is the man-machine interaction. Visualization of unnecessary, obsolete or redundant information may cause confusion and distraction, reducing usefulness and acceptance of the assistance system.We propose a system capable of automatically filtering available information based on recognized phases in the operating room. Our system offers a specific selection of available visualizations which suit the surgeon's needs best. The system was implemented for use in laparoscopic liver and gallbladder surgery and evaluated in phantom experiments in conjunction with expert interviews. © 2013 Elsevier Ltd.","Augmented Reality; Context-awareness; Knowledge representation; Laparoscopy","Clinical application; Context-Aware; Context-awareness; Intra-operative; Laparoscopic surgery; Man-machine interaction; Phantom experiment; Redundant informations; Augmented reality; Knowledge representation; Laparoscopy; Visualization; Information filtering; article; biliary tract surgery; filtration; human; interview; laparoscopic surgery; operating room; phantom; priority journal; Algorithms; Animals; Artificial Intelligence; Hepatectomy; Humans; Laparoscopy; Liver; Surgery, Computer-Assisted; Swine; User-Computer Interface",Article,Scopus,2-s2.0-84876995734
"Gundersen O.E., Sørmo F., Aamodt A., Skalle P.","A real-time decision support system for high cost oil well drilling operations",2013,"AI Magazine",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876176962&partnerID=40&md5=365eb3283eadcf47e89a33b598a6b902","In this paper we present DrillEdge - a commercial and award-winning software system that monitors oil well drilling operations in order to reduce nonproductive time (NPT). DrillEdge utilizes case-based reasoning with temporal representations on streaming realtime data, pattern matching, and agent systems to predict problems and give advice on how to mitigate the problems. We document the methods utilized, the architecture, the GUI. and the development cost in addition to two case studies. Copyright © 2013, Association for the Advancement of Artificial Intelligence.",,"Agent systems; Development costs; Drilling operation; Non-productive time; Real-time data; Real-time decision support systems; Software systems; Temporal representations; Artificial intelligence; Decision support systems; Pattern matching; Oil well drilling",Conference Paper,Scopus,2-s2.0-84876176962
"Grundmann J., Schütze N., Lennartz F.","Sustainable management of a coupled groundwater-agriculture hydrosystem using multi-criteria simulation based optimisation",2013,"Water Science and Technology",11,10.2166/wst.2012.602,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874029300&doi=10.2166%2fwst.2012.602&partnerID=40&md5=e655b20dbb3d4dd8e62e8175c00c68d5","In this paper we present a new simulation-based integrated water management tool for sustainable water resources management in arid coastal environments. This tool delivers optimised groundwater withdrawal scenarios considering saltwater intrusion as a result of agricultural and municipal water abstraction. It also yields a substantially improved water use efficiency of irrigated agriculture. To allow for a robust and fast operation we unified process modelling with artificial intelligence tools and evolutionary optimisation techniques. The aquifer behaviour is represented using an artificial neural network (ANN) which emulates a numerical density-dependent groundwater flow model. The impact of agriculture is represented by stochastic crop water production functions (SCWPF). Simulation-based optimisation techniques together with the SCWPF and ANN deliver optimal groundwater abstraction and cropping patterns. To address contradicting objectives, e.g. profit-oriented agriculture vs. sustainable abstraction scenarios, we performed multi-objective optimisations using a multi-criteria optimisation algorithm. © IWA Publishing 2013.","Artificial intelligence; Density-dependent groundwater flow modelling; Integrated water resources management; Irrigation; Multi-criteria optimisation","Artificial intelligence tools; Coastal environments; Crop water production function; Cropping patterns; Density-dependent; Evolutionary optimisation; Fast operation; Groundwater abstraction; Groundwater Flow Model; Groundwater flow modelling; Groundwater withdrawal; Hydro-system; Integrated water management tools; Integrated Water Resources Management; Irrigated agriculture; Multi objective; Multi-criteria; Multi-criteria optimisation; Municipal water; Optimisation techniques; Optimisations; Simulation based optimisation; Sustainable management; Sustainable water resources; Unified process; Water use efficiency; Abstracting; Algorithms; Aquifers; Artificial intelligence; Groundwater flow; Groundwater resources; Irrigation; Multiobjective optimization; Neural networks; Profitability; Water conservation; Water supply; Agriculture; ground water; agricultural production; algorithm; arid region; artificial intelligence; artificial neural network; coastal zone; flow modeling; groundwater abstraction; groundwater flow; irrigation; multicriteria analysis; multiobjective programming; optimization; sustainable development; water management; water resource; water use; aquifer; article; artificial intelligence; artificial neural network; coastal waters; cropping system; irrigation (agriculture); simulation; sustainable agriculture; water flow; water management; water quality; water supply; Agriculture; Algorithms; Biomass; Computer Simulation; Groundwater; Neural Networks (Computer); Salinity; Water Movements; Water Supply",Article,Scopus,2-s2.0-84874029300
"Waters B.","Functional encryption: Origins and recent developments",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-36362-7_4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873964985&doi=10.1007%2f978-3-642-36362-7_4&partnerID=40&md5=ceaf18220eb5e688b763124a2f945d46","In this talk, we will present the notion of functional encryption and recent progress in the area. We will begin by describing the concept and origins of functional encryption. Next, we will describe intuitively why current bilinear map based constructions appear to be ""stuck"" with boolean formula type functionality even in the public index setting. Finally, we will see some very recent work that uses multilinear forms to move beyond these barriers and achieve functionality for any circuit. © 2013 International Association for Cryptologic Research.",,"Bilinear map; Boolean formulae; Functional encryptions; Multilinear forms; Recent progress; Artificial intelligence; Public key cryptography",Conference Paper,Scopus,2-s2.0-84873964985
"Gao J.-Q., Fan L.-Y., Xu L.-Z.","Median null(Sw)-based method for face feature recognition",2013,"Applied Mathematics and Computation",11,10.1016/j.amc.2013.01.005,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873970661&doi=10.1016%2fj.amc.2013.01.005&partnerID=40&md5=cf431212d2d932f2d6a6e9a5749cff93","With the progress of science and technology artificial intelligence is being paid more and more attention. People want to use computers to deal with complex practical problems. So, linear discriminant analysis (LDA) is widely used as a dimensionality reduction technique in image and text recognition classification tasks. However, a weakness of LDA model is that the class average vector in the formula completely depends on class sample average. Under special circumstances such as noise, bright light, some outliers will appear in the practical input databases. Therefore, by employing several given practical samples, the class sample average is not enough to estimate the class average accurately. So, the recognition performance of LDA model will decline. Compared to human intelligence, computers are far short of necessary fundamental knowledge of judgment which people normally acquire during the formative years of their lives. In order to solve the problem and also to render LDA model more robust, we propose a within-class scatter matrix null space median method (M-N(Sw)), which first transforms the original space by employing a basis of within-class scatter matrix null space, and then in the transformed space the maximum of between-class scatter matrix is pursued. In the second stage, within-class median vector is used in the traditional LDA model. Experiments on ORL, FERET and Yale face data sets are performed to test and evaluate the effectiveness of the proposed method. © 2013 Elsevier Inc. All rights reserved.","Face recognition; Linear discriminant analysis (LDA); M-N(Sw); Null space; Within-class median","Bright lights; Classification tasks; Dimensionality reduction techniques; Face data; Face features; Human intelligence; Lda models; Linear discriminant analysis; M-N(S<sub>w</sub>); Null space; Practical problems; Recognition performance; Sample average; Scatter matrix; Science and Technology; Text recognition; Within class; Within-class scatter matrix; Artificial intelligence; Character recognition; Text processing; Face recognition",Article,Scopus,2-s2.0-84873970661
"Zhao D., Wang Y., Lin Z., Sheng S.","An effective quality assessment method for small scale resistance spot welding based on process parameters",2013,"NDT and E International",11,10.1016/j.ndteint.2013.01.008,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873633123&doi=10.1016%2fj.ndteint.2013.01.008&partnerID=40&md5=0cc16ddb65137a3d7291958312cb2ef1","With the rapid development of microelectromechanical system technology, small scale resistance spot welding (SSRSW) is ever-increasingly used in electronic and medical devices. Whereas there is limited research work dealing with quality control of SSRSW. This paper treated the U-I (voltage-current) curve as the monitoring signature to explore a real-time and in-situ SSRSW quality monitoring method. First a systematic research on the U-I curve was performed and then five factors extracted from the U-I curve to estimate the weld quality through an artificial intelligence algorithm was proposed. The entire predictions match with their actual results well, showing that the U-I curve is a reliable quality monitoring signature for SSRSW. It can assess the weld strength and nugget diameter in small error on condition that the resistance welding power supply used during the whole welding process can provide linear direct current (DC) or high frequency (HF) current, which is commonly employed in SSRSW. © 2013 Elsevier Ltd. All rights reserved.","Process parameters; Quality assessment; Small scale resistance spot welding; U-I curve","Artificial intelligence algorithms; Direct current; High frequency; Medical Devices; Nugget diameters; Process parameters; Quality assessment; Quality monitoring; Small-scale resistance spot welding; Systematic research; U-I curve; Weld quality; Weld strength; Welding process; Artificial intelligence; Biomedical equipment; Electromechanical devices; MEMS; Welding; Welds; Resistance welding",Article,Scopus,2-s2.0-84873633123
"Jayadeva, Shah S., Bhaya A., Kothari R., Chandra S.","Ants find the shortest path: A mathematical proof",2013,"Swarm Intelligence",11,10.1007/s11721-013-0076-9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874677125&doi=10.1007%2fs11721-013-0076-9&partnerID=40&md5=07a89680448339e7c81c5890d9b1fd86","In the most basic application of Ant Colony Optimization (ACO), a set of artificial ants find the shortest path between a source and a destination. Ants deposit pheromone on paths they take, preferring paths that have more pheromone on them. Since shorter paths are traversed faster, more pheromone accumulates on them in a given time, attracting more ants and leading to reinforcement of the pheromone trail on shorter paths. This is a positive feedback process that can also cause trails to persist on longer paths, even when a shorter path becomes available. To counteract this persistence on a longer path, ACO algorithms employ remedial measures, such as using negative feedback in the form of uniform evaporation on all paths. Obtaining high performance in ACO algorithms typically requires fine tuning several parameters that govern pheromone deposition and removal. This paper proposes a new ACO algorithm, called EigenAnt, for finding the shortest path between a source and a destination, based on selective pheromone removal that occurs only on the path that is actually chosen for each trip. We prove that the shortest path is the only stable equilibrium for EigenAnt, which means that it is maintained for arbitrary initial pheromone concentrations on paths, and even when path lengths change with time. The EigenAnt algorithm uses only two parameters and does not require them to be finely tuned. Simulations that illustrate these properties are provided. © 2013 Springer Science+Business Media New York.","Ant Colony Optimization; Collective foraging; Distributed optimization; Optimization; Pheromone; Selective removal of pheromone; Self-organization; Stability analysis; Stagnation; Swarm intelligence","Collective foraging; Distributed optimization; Pheromone; Selective removal; Self organizations; Stability analysis; Stagnation; Swarm Intelligence; Ant colony optimization; Artificial intelligence; Constraint theory; Feedback; Optimization; Algorithms",Article,Scopus,2-s2.0-84874677125
"Avoine G., Carpent X.","Yet another ultralightweight authentication protocol that is broken",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-36140-1-2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873329224&doi=10.1007%2f978-3-642-36140-1-2&partnerID=40&md5=c5479f898cf79ecbdc5c27817d9d4dc7","Eghdamian and Samsudin published at ICIEIS 2011 an ultralightweight mutual authentication protocol that requires few bitwise operations. The simplicity of the design makes the protocol very suitable to low-cost RFID tags. However, we demonstrate in this paper that the long-term key shared by the reader and the tag can be recovered by an adversary with a few eavesdropped sessions only. Additionally, we provide the backbone of some attacks on a series of similar recent protocols, and highlight important common weaknesses in the design of ultralightweight protocols. © 2013 Springer-Verlag Berlin Heidelberg.","Authentication; RFID; Ultralightweight protocol","Authentication protocols; Mutual authentication; RF-ID tags; Artificial intelligence; Authentication; Radio frequency identification (RFID)",Conference Paper,Scopus,2-s2.0-84873329224
"Roy A., Mackin P.D., Mukhopadhyay S.","Methods for pattern selection, class-specific feature selection and classification for automated learning",2013,"Neural Networks",11,10.1016/j.neunet.2012.12.007,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875883516&doi=10.1016%2fj.neunet.2012.12.007&partnerID=40&md5=23002a85bcaa300eaec178660f0537e1","This paper presents methods for training pattern (prototype) selection, class-specific feature selection and classification for automated learning. For training pattern selection, we propose a method of sampling that extracts a small number of representative training patterns (prototypes) from the dataset. The idea is to extract a set of prototype training patterns that represents each class region in a classification problem. In class-specific feature selection, we try to find a separate feature set for each class such that it is the best one to separate that class from the other classes. We then build a separate classifier for that class based on its own feature set. The paper also presents a new hypersphere classification algorithm. Hypersphere nets are similar to radial basis function (RBF) nets and belong to the group of kernel function nets. Polynomial time complexity of the methods is proven. Polynomial time complexity of learning algorithms is important to the field of neural networks. Computational results are provided for a number of well-known datasets. None of the parameters of the algorithm were fine tuned for any of the problems solved and this supports the idea of automation of learning methods. Automation of learning is crucial to wider deployment of learning technologies. © 2012 Elsevier Ltd.","Automated learning; Classification algorithm; Complexity of learning; Feature selection; Hypersphere net; Polynomial time complexity; Training pattern selection","Automated learning; Classification algorithm; Complexity of learning; Hypersphere; Polynomial time complexity; Training patterns; Automation; Feature extraction; Learning algorithms; Polynomial approximation; Radial basis function networks; Separation; Computational complexity; article; artificial neural network; automation; classification algorithm; classifier; computer program; data analysis; hypersphere net; kernel method; learning algorithm; mathematical computing; mathematical parameters; online analysis; polynomial time complexity; priority journal; process development; sampling; statistical distribution; systematic error; Algorithms; Artificial Intelligence; Neural Networks (Computer); Pattern Recognition, Automated",Article,Scopus,2-s2.0-84875883516
"Andersson M., Gustafsson F., Prevost D., St-Laurent L.","Recognition of anomalous motion patterns in urban surveillance",2013,"IEEE Journal on Selected Topics in Signal Processing",11,10.1109/JSTSP.2013.2237882,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873816932&doi=10.1109%2fJSTSP.2013.2237882&partnerID=40&md5=b27fb7786ec92469d59b557f1230003e","We investigate the unsupervised K -means clustering and the semi-supervised hidden Markov model (HMM) to automatically detect anomalous motion patterns in groups of people (crowds). Anomalous motion patterns are typically people merging into a dense group, followed by disturbances or threatening situations within the group. The application of K-means clustering and HMM are illustrated with datasets from four surveillance scenarios. The results indicate that by investigating the group of people in a systematic way with different K values, analyze cluster density, cluster quality and changes in cluster shape we can automatically detect anomalous motion patterns. The results correspond well with the events in the datasets. The results also indicate that very accurate detections of the people in the dense group would not be necessary. The clustering and HMM results will be very much the same also with some increased uncertainty in the detections. © 2007-2012 IEEE.","Clustering algorithms; decision support systems; hidden Markov models; machine learning; machine vision; object segmentation; pattern recognition","Cluster densities; Data sets; K-means clustering; K-values; Motion pattern; Object segmentation; Semi-supervised; Urban surveillance; Artificial intelligence; Computer vision; Decision support systems; Hidden Markov models; Learning systems; Pattern recognition; Time and motion study; Clustering algorithms",Article,Scopus,2-s2.0-84873816932
"Liberman G., Louzoun Y., Aizenstein O., Blumenthal D.T., Bokstein F., Palmon M., Corn B.W., Ben Bashat D.","Automatic multi-modal MR tissue classification for the assessment of response to bevacizumab in patients with glioblastoma",2013,"European Journal of Radiology",11,10.1016/j.ejrad.2012.09.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871920938&doi=10.1016%2fj.ejrad.2012.09.001&partnerID=40&md5=a76dfb383ef7166932068210962ea408","Background: Current methods for evaluation of treatment response in glioblastoma are inaccurate, limited and time-consuming. This study aimed to develop a multi-modal MRI automatic classification method to improve accuracy and efficiency of treatment response assessment in patients with recurrent glioblastoma (GB). Materials and methods: A modification of the k-Nearest-Neighbors (kNN) classification method was developed and applied to 59 longitudinal MR data sets of 13 patients with recurrent GB undergoing bevacizumab (anti-angiogenic) therapy. Changes in the enhancing tumor volume were assessed using the proposed method and compared with Macdonald's criteria and with manual volumetric measurements. The edema-like area was further subclassified into peri- and non-peri-tumoral edema, using both the kNN method and an unsupervised method, to monitor longitudinal changes. Results: Automatic classification using the modified kNN method was applicable in all scans, even when the tumors were infiltrative with unclear borders. The enhancing tumor volume obtained using the automatic method was highly correlated with manual measurements (N = 33, r = 0.96, p < 0.0001), while standard radiographic assessment based on Macdonald's criteria matched manual delineation and automatic results in only 68% of cases. A graded pattern of tumor infiltration within the edema-like area was revealed by both automatic methods, showing high agreement. All classification results were confirmed by a senior neuro-radiologist and validated using MR spectroscopy. Conclusion: This study emphasizes the important role of automatic tools based on a multi-modal view of the tissue in monitoring therapy response in patients with high grade gliomas specifically under anti-angiogenic therapy. © 2012 Elsevier Ireland Ltd.","Anti-angiogenic treatment; Automatic classification; Glioblastoma; MRI","bevacizumab; adult; aged; antiangiogenic therapy; article; automation; cancer chemotherapy; cancer invasion; cancer patient; cancer survival; cancer tissue; classification algorithm; clinical article; comparative effectiveness; controlled study; diagnostic accuracy; female; glioblastoma; human; human tissue; intermethod comparison; male; nuclear magnetic resonance imaging; nuclear magnetic resonance spectroscopy; outcome assessment; patient monitoring; priority journal; progression free survival; treatment response; tumor recurrence; tumor volume; Adult; Aged; Algorithms; Angiogenesis Inhibitors; Antibodies, Monoclonal, Humanized; Artificial Intelligence; Brain Neoplasms; Female; Glioblastoma; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Male; Middle Aged; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Treatment Outcome",Article,Scopus,2-s2.0-84871920938
"Li D., Yang H.Z., Liang X.F.","Prediction analysis of a wastewater treatment system using a Bayesian network",2013,"Environmental Modelling and Software",11,10.1016/j.envsoft.2012.08.011,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871727347&doi=10.1016%2fj.envsoft.2012.08.011&partnerID=40&md5=ce6d9680e75a6d11d03509d476662e53","Wastewater treatment is a complicated dynamic process, the effectiveness of which is affected by microbial, chemical, and physical factors. At present, predicting the effluent quality of wastewater treatment systems is difficult because of complex biological reaction mechanisms that vary with both time and the physical attributes of the system. Bayesian networks are useful for addressing uncertainties in artificial intelligence applications. Their powerful inferential capability and convenient decision support mechanisms provide flexibility and applicability for describing and analyzing factors affecting wastewater treatment systems. In this study, a Bayesian network-based approach for modeling and predicting a wastewater treatment system based on Modified Sequencing Batch Reactor (MSBR) was proposed. Using the presented approach, a Bayesian network model for MSBR can be constructed using experiential information and physical data relating to influent loads, operating conditions, and effluent concentrations. Additionally, MSBR prediction analysis, wherein effluent concentration can be predicted from influent loads and operational conditions, can be performed. This approach can be applied, with minimal modifications, to other types of wastewater treatment plants. © 2012 Elsevier Ltd.","Bayesian network; Inference; Modified sequencing batch reactor; Prediction analysis","Bayesian network models; Biological reaction; Decision supports; Dynamic process; Effluent concentrations; Effluent quality; Inference; Influent loads; Modified sequencing batch reactors; Network-based approach; Operating condition; Operational conditions; Physical data; Physical factors; Wastewater treatment plants; Wastewater treatment system; Artificial intelligence; Decision support systems; Forecasting; Reclamation; Wastewater treatment; Water quality; Bayesian networks; Bayesian analysis; concentration (composition); data set; decision making; effluent; prediction; wastewater; water quality; water treatment",Article,Scopus,2-s2.0-84871727347
"Das D.K., Chakraborty C., Mitra B., Maiti A.K., Ray A.K.","Quantitative microscopy approach for shape-based erythrocytes characterization in anaemia",2013,"Journal of Microscopy",11,10.1111/jmi.12002,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872815658&doi=10.1111%2fjmi.12002&partnerID=40&md5=e2e57e72aedc916b7280c5d78f777760","Anaemia is one of the most common diseases in the world population. Primarily anaemia is identified based on haemoglobin level; and then microscopically examination of peripheral blood smear is required for characterizing and confirmation of anaemic stages. In conventional approach, experts visually characterize abnormality present in the erythrocytes under light microscope, and this evaluation process is subjective in nature and error prone. In this study, we have proposed a methodology using machine learning techniques for characterizing erythrocytes in anaemia associated with anaemia using microscopic images of peripheral blood smears. First, peripheral blood smear images are preprocessed based on grey world assumption technique and geometric mean filter for reducing unevenness of background illumination and noise reduction. Then erythrocyte cells are segmented using marker-controlled watershed segmentation technique. The erythrocytes in anaemia, such as, tear drop, echinocyte, acanthocyte, elliptocyte, sickle cells and normal erythrocytes cells have been characterized and classified based on their morphological changes. Optimal subset of features, ranked by information gain measure provides highest classification performance using logistic regression classifier in comparison with other standard classifiers. © 2012 The Authors Journal of Microscopy © 2012 Royal Microscopical Society.","Anaemia; Information gain; Logistic regression; Marker-controlled watershed; Zernike moments","hemoglobin; acanthocytosis; accuracy; anemia; article; blood smear; classification algorithm; controlled study; echinocyte; erythrocyte; erythrocyte shape; human; human cell; human tissue; illumination; image analysis; machine learning; major clinical study; mathematical model; microscopy; noise reduction; perceptron; predictive value; priority journal; quantitative analysis; sensitivity and specificity; sickle cell; signal noise ratio; watershed segmentation technique; Anemia; Artificial Intelligence; Automation; Biometry; Clinical Laboratory Techniques; Erythrocytes; Humans; Image Processing, Computer-Assisted; Microscopy",Article,Scopus,2-s2.0-84872815658
"Basin D., Klaedtke F., Marinovic S., Zǎlinescu E.","Monitoring compliance policies over incomplete and disagreeing logs",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-35632-2-17,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872839668&doi=10.1007%2f978-3-642-35632-2-17&partnerID=40&md5=5efa6cce3e634dbb5e10650dbd5d3108","When monitoring system behavior to check compliance against a given policy, one is sometimes confronted with incomplete knowledge about system events. In IT systems, such incompleteness may arise from logging infrastructure failures and corrupted log files, or when the logs produced by different system components disagree on whether actions took place. In this paper, we present a policy language with a three-valued semantics that allows one to explicitly reason about incomplete knowledge and handle disagreements. Furthermore, we present a monitoring algorithm for an expressive fragment of our policy language. We illustrate through examples how our approach extends compliance monitoring to systems with logging failures and disagreements. © 2013 Springer-Verlag Berlin Heidelberg.",,"Compliance monitoring; Incomplete knowledge; IT system; Log file; Logging infrastructure; Monitoring algorithms; Monitoring system; Policy language; System components; Three-valued; Artificial intelligence; Semantics",Conference Paper,Scopus,2-s2.0-84872839668
"Vasirani M., Ossowski S.","Smart consumer load balancing: State of the art and an empirical evaluation in the Spanish electricity market",2013,"Artificial Intelligence Review",11,10.1007/s10462-012-9391-6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878578199&doi=10.1007%2fs10462-012-9391-6&partnerID=40&md5=799cebb3e9358feda85652213f15384f","The basis of an efficient functioning of a power grid is an accurate balancing of the electricity demand of all the consumers at any instant with supply. Nowadays, this task involves only the grid operator and retail electricity providers. One of the facets of the Smart Grid vision is that consumers may have a more active role in the problem of balancing demand with supply. With the deployment of intelligent information and communication technologies in domestic environments, homes are becoming smarter and able to play a more active role in the management of energy. We use the term Smart Consumer Load Balancing to refer to algorithms that are run by energy management systems of homes in order to optimise the electricity consumption, to minimise costs and/or meet supply constraints. In this work, we analyse different approaches to Smart Consumer Load Balancing based on (distributed) artificial intelligence. We also put forward a new model of Smart Consumer Load Balancing, where consumers actively participate in the balancing of demand with supply by forming groups that agree on a joint demand profile to be contracted in the market with the mediation of an aggregator. We specify the business model as well as the optimisation model for load balancing, showing the economic benefits for the consumers in a realistic scenario based on the Spanish electricity market. © 2013 Springer Science+Business Media Dordrecht.","Coalitions; Load balancing; Optimisation; Smart grid","Coalitions; Domestic environments; Electricity-consumption; Empirical evaluations; Intelligent information; Optimisations; Smart grid; Spanish electricity markets; Artificial intelligence; Electric industry; Energy management systems; Information technology; Optimization; Resource allocation; Smart power grids; Parallel architectures",Conference Paper,Scopus,2-s2.0-84878578199
"Anifowose F., Labadin J., Abdulraheem A.","A least-square-driven functional networks type-2 fuzzy logic hybrid model for efficient petroleum reservoir properties prediction",2013,"Neural Computing and Applications",11,10.1007/s00521-012-1298-2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888841231&doi=10.1007%2fs00521-012-1298-2&partnerID=40&md5=a95655e08c46d9bfefa6bdc200d7cfed","Various computational intelligence techniques have been used in the prediction of petroleum reservoir properties. However, each of them has its limitations depending on different conditions such as data size and dimensionality. Hybrid computational intelligence has been introduced as a new paradigm to complement the weaknesses of one technique with the strengths of another or others. This paper presents a computational intelligence hybrid model to overcome some of the limitations of the standalone type-2 fuzzy logic system (T2FLS) model by using a least-square-fitting-based model selection algorithm to reduce the dimensionality of the input data while selecting the best variables. This novel feature selection procedure resulted in the improvement of the performance of T2FLS whose complexity is usually increased and performance degraded with increased dimensionality of input data. The iterative least-square-fitting algorithm part of functional networks (FN) and T2FLS techniques were combined in a hybrid manner to predict the porosity and permeability of North American and Middle Eastern oil and gas reservoirs. Training and testing the T2FLS block of the hybrid model with the best and dimensionally reduced input variables caused the hybrid model to perform better with higher correlation coefficients, lower root mean square errors, and less execution times than the standalone T2FLS model. This work has demonstrated the promising capability of hybrid modelling and has given more insight into the possibility of more robust hybrid models with better functionality and capability indices. © 2013 Springer-Verlag London.","Functional networks; Hybrid artificial intelligence; Least-square-fitting algorithm; Petroleum reservoir; Type-2 fuzzy logic","Computational intelligence techniques; Correlation coefficient; Functional network; Hybrid artificial intelligences; Hybrid computational intelligence; Root mean square errors; Type-2 fuzzy logic; Type-2 fuzzy logic system; Artificial intelligence; Complex networks; Fuzzy logic; Input output programs; Iterative methods; Mean square error; Petroleum reservoirs",Article,Scopus,2-s2.0-84888841231
"Segal G.","Identification of legionella effectors using bioinformatic approaches",2013,"Methods in Molecular Biology",11,10.1007/978-1-62703-161-5-37,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872091657&doi=10.1007%2f978-1-62703-161-5-37&partnerID=40&md5=c28f0098e896b8a220e606ece25334b6","Legionella pneumophila the causative agent of Legionnaires' disease, actively manipulates host cell processes to establish a replication niche inside host cells. The establishment of its replication niche requires a functional Icm/Dot type IV secretion system which translocates about 300 effector proteins into host cells during infection. Many of these effectors were first identified as effector candidates by several bioinformatic approaches, and these predicted effectors were later examined experimentally for translocation and a large number of which were validated as effector proteins. Here, I summarized the bioinformatic approaches that were used to identify these effectors. © 2013 Springer Science+Business Media New York.",,"bacterial protein; article; artificial intelligence; bacterial secretion system; biology; gene expression regulation; genetics; Legionella pneumophila; metabolism; methodology; protein transport; regulatory sequence; reproducibility; signal transduction; Artificial Intelligence; Bacterial Proteins; Bacterial Secretion Systems; Computational Biology; Gene Expression Regulation, Bacterial; Legionella pneumophila; Protein Transport; Regulatory Sequences, Nucleic Acid; Reproducibility of Results; Signal Transduction; Legionella; Legionella pneumophila",Article,Scopus,2-s2.0-84872091657
"Sánchez-Anguix V., Julián V., Botti V., García-Fornes A.","Studying the impact of negotiation environments on negotiation teams' performance",2013,"Information Sciences",11,10.1016/j.ins.2012.07.017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867582175&doi=10.1016%2fj.ins.2012.07.017&partnerID=40&md5=6666193271a791c149b83d3172c7a59b","In this article we study the impact of the negotiation environment on the performance of several intra-team strategies (team dynamics) for agent-based negotiation teams that negotiate with an opponent. An agent-based negotiation team is a group of agents that joins together as a party because they share common interests in the negotiation at hand. It is experimentally shown how negotiation environment conditions like the deadline of both parties, the concession speed of the opponent, similarity among team members, and team size affect performance metrics like the minimum utility of team members, the average utility of team members, and the number of negotiation rounds. Our goal is identifying which intra-team strategies work better in different environmental conditions in order to provide useful knowledge for team members to select appropriate intra-team strategies according to environmental conditions. © 2012 Elsevier Inc. All rights reserved.","Agreement technology; Automated negotiation; Collective decision making; Multi-agent system; Negotiation team","Agent based; Automated negotiations; Collective decision making; Environment conditions; Environmental conditions; Multi agent system (MAS); Negotiation team; Performance metrics; Team members; Team size; Artificial intelligence; Software engineering; Multi agent systems",Article,Scopus,2-s2.0-84867582175
"Sternberg M.J.E., Tamaddoni-Nezhad A., Lesk V.I., Kay E., Hitchen P.G., Cootes A., Van Alphen L.B., Lamoureux M.P., Jarrell H.C., Rawlings C.J., Soo E.C., Szymanski C.M., Dell A., Wren B.W., Muggleton S.H.","Gene Function Hypotheses for the Campylobacter jejuni Glycome Generated by a Logic-Based Approach",2013,"Journal of Molecular Biology",11,10.1016/j.jmb.2012.10.014,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871772041&doi=10.1016%2fj.jmb.2012.10.014&partnerID=40&md5=85e44b77a043516cb61a087f51a6b1bc","Increasingly, experimental data on biological systems are obtained from several sources and computational approaches are required to integrate this information and derive models for the function of the system. Here, we demonstrate the power of a logic-based machine learning approach to propose hypotheses for gene function integrating information from two diverse experimental approaches. Specifically, we use inductive logic programming that automatically proposes hypotheses explaining the empirical data with respect to logically encoded background knowledge. We study the capsular polysaccharide biosynthetic pathway of the major human gastrointestinal pathogen Campylobacter jejuni. We consider several key steps in the formation of capsular polysaccharide consisting of 15 genes of which 8 have assigned function, and we explore the extent to which functions can be hypothesised for the remaining 7. Two sources of experimental data provide the information for learning - the results of knockout experiments on the genes involved in capsule formation and the absence/presence of capsule genes in a multitude of strains of different serotypes. The machine learning uses the pathway structure as background knowledge. We propose assignments of specific genes to five previously unassigned reaction steps. For four of these steps, there was an unambiguous optimal assignment of gene to reaction, and to the fifth, there were three candidate genes. Several of these assignments were consistent with additional experimental results. We therefore show that the logic-based methodology provides a robust strategy to integrate results from different experimental approaches and propose hypotheses for the behaviour of a biological system. © 2012 Elsevier Ltd.","Campylobacter jejuni; capsular polysaccharide; machine learning; pathway modelling; systems biology","glucuronosyltransferase; heptose guanosyltransferase; heptose kinase; methyltransferase; sedoheptulose isomerase; sugar transferase; unclassified drug; article; Campylobacter jejuni; DNA microarray; gene expression; gene function; knockout gene; machine learning; nonhuman; priority journal; serotype; Artificial Intelligence; Bacterial Capsules; Biosynthetic Pathways; Campylobacter jejuni; Gene Knockout Techniques; Genes, Bacterial; Glycomics; Logic; Metabolomics; Models, Biological; Molecular Sequence Annotation; Mutation; Oligonucleotide Array Sequence Analysis; Phenotype; Polysaccharides, Bacterial; Systems Biology; Campylobacter jejuni",Article,Scopus,2-s2.0-84871772041
"Lim M.-H., Teoh A.B.J.","A novel encoding scheme for effective biometric discretization: Linearly separable subcode",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",11,10.1109/TPAMI.2012.122,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871740805&doi=10.1109%2fTPAMI.2012.122&partnerID=40&md5=f2d245a2fb8991df4d0422ce38782509","Separability in a code is crucial in guaranteeing a decent Hamming-distance separation among the codewords. In multibit biometric discretization where a code is used for quantization-intervals labeling, separability is necessary for preserving distance dissimilarity when feature components are mapped from a discrete space to a Hamming space. In this paper, we examine separability of Binary Reflected Gray Code (BRGC) encoding and reveal its inadequacy in tackling interclass variation during the discrete-to-binary mapping, leading to a tradeoff between classification performance and entropy of binary output. To overcome this drawback, we put forward two encoding schemes exhibiting full-ideal and near-ideal separability capabilities, known as Linearly Separable Subcode (LSSC) and Partially Linearly Separable Subcode (PLSSC), respectively. These encoding schemes convert the conventional entropy-performance tradeoff into an entropy-redundancy tradeoff in the increase of code length. Extensive experimental results vindicate the superiority of our schemes over the existing encoding schemes in discretization performance. This opens up possibilities of achieving much greater classification performance with high output entropy. © 2012 IEEE.","Biometric discretization; encoding; linearly separable subcode; quantization","Binary reflected gray codes; Classification performance; Code length; Code-words; Discrete spaces; Discretizations; Encoding schemes; Hamming space; High output; Linearly separable; Multi-bits; quantization; Biometrics; Commerce; Entropy; Hamming distance; Encoding (symbols); algorithm; article; artificial intelligence; automated pattern recognition; biometry; computer assisted diagnosis; computer simulation; face; histology; human; image enhancement; image subtraction; information retrieval; methodology; photography; reproducibility; sensitivity and specificity; signal processing; statistical model; Algorithms; Artificial Intelligence; Biometry; Computer Simulation; Face; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Information Storage and Retrieval; Linear Models; Pattern Recognition, Automated; Photography; Reproducibility of Results; Sensitivity and Specificity; Signal Processing, Computer-Assisted; Subtraction Technique",Article,Scopus,2-s2.0-84871740805
"Panagopoulos A., Wang C., Samaras D., Paragios N.","Simultaneous cast shadows, illumination and geometry inference using hypergraphs",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence",11,10.1109/TPAMI.2012.110,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871767528&doi=10.1109%2fTPAMI.2012.110&partnerID=40&md5=f0feb6e58d97b47b1e7bf4dfc8d9f52a","The cast shadows in an image provide important information about illumination and geometry. In this paper, we utilize this information in a novel framework in order to jointly recover the illumination environment, a set of geometry parameters, and an estimate of the cast shadows in the scene given a single image and coarse initial 3D geometry. We model the interaction of illumination and geometry in the scene and associate it with image evidence for cast shadows using a higher order Markov Random Field (MRF) illumination model, while we also introduce a method to obtain approximate image evidence for cast shadows. Capturing the interaction between light sources and geometry in the proposed graphical model necessitates higher order cliques and continuous-valued variables, which make inference challenging. Taking advantage of domain knowledge, we provide a two-stage minimization technique for the MRF energy of our model. We evaluate our method in different datasets, both synthetic and real. Our model is robust to rough knowledge of geometry and inaccurate initial shadow estimates, allowing a generic coarse 3D model to represent a whole class of objects for the task of illumination estimation, or the estimation of geometry parameters to refine our initial knowledge of scene geometry, simultaneously with illumination estimation. © 2012 IEEE.","image models; Markov random fields; photometry; shading","3D geometry; 3D models; Cast shadow; Data sets; Domain knowledge; Geometry parameter; GraphicaL model; Hyper graph; Illumination estimation; Illumination models; Image models; Markov Random Fields; shading; Single images; Data processing; Estimation; Geometry; Image segmentation; Light sources; Photometry; Three dimensional computer graphics; Three dimensional; algorithm; article; artificial intelligence; automated pattern recognition; computer assisted diagnosis; illumination; image enhancement; methodology; reproducibility; sensitivity and specificity; three dimensional imaging; Algorithms; Artificial Intelligence; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Lighting; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity",Article,Scopus,2-s2.0-84871767528
"Roy S.S., Madhu Viswanatham V., Venkata Krishna P., Saraf N., Gupta A., Mishra R.","Applicability of rough set technique for data investigation and optimization of intrusion detection system",2013,"Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering, LNICST",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982261651&partnerID=40&md5=df966a95f3cc4cc19732fc7d632385e4","The very idea of intrusion detection can be perceived through the hasty advancement following the expansion and revolution of artificial intelligence and soft computing. Thus, in order to analyze, detect, identify and hold up network attacks a network intrusion detection system based on rough set theory has been proposed in this article. In this paper we have shown how the rough set technique can be applied to reduce the redundancies in the dataset and optimize the Intrusion Detection System (IDS). © Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2013.","Intrusion Detection; Network Attacks; Rough Set Theory","Artificial intelligence; Computation theory; Computer crime; Heterogeneous networks; Mercury (metal); Rough set theory; Set theory; Soft computing; Hold up; Intrusion Detection Systems; Network attack; Network intrusion detection systems; Intrusion detection",Conference Paper,Scopus,2-s2.0-84982261651
"Osoba O., Kosko B.","Noise-enhanced clustering and competitive learning algorithms",2013,"Neural Networks",11,10.1016/j.neunet.2012.09.012,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870469359&doi=10.1016%2fj.neunet.2012.09.012&partnerID=40&md5=8a65b848b51266f6a872ee4155898e97","Noise can provably speed up convergence in many centroid-based clustering algorithms. This includes the popular k-means clustering algorithm. The clustering noise benefit follows from the general noise benefit for the expectation-maximization algorithm because many clustering algorithms are special cases of the expectation-maximization algorithm. Simulations show that noise also speeds up convergence in stochastic unsupervised competitive learning, supervised competitive learning, and differential competitive learning. © 2012 Elsevier Ltd.","Clustering; Competitive learning; EM algorithm; K-means clustering; Noise injection; Stochastic resonance","Clustering; Competitive learning; EM algorithms; K-means clustering; Noise injection; Stochastic resonances; Learning algorithms; Magnetic resonance; Clustering algorithms; algorithm; article; cluster analysis; competition; learning; mathematical analysis; mathematical computing; mathematical model; mathematical parameters; noise; priority journal; process optimization; simulation; statistical analysis; statistics; artifact; artificial intelligence; biological model; cluster analysis; computer simulation; human; competitive learning; learning; learning algorithm; normal distribution; Algorithms; Artifacts; Artificial Intelligence; Cluster Analysis; Computer Simulation; Humans; Models, Neurological; Stochastic Processes",Article,Scopus,2-s2.0-84870469359
"Alcuier P., Biau G.","Sparse single-index model",2013,"Journal of Machine Learning Research",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873482408&partnerID=40&md5=c3afdb48a9c36bd925d73727efb77c24","Let (X,Y) be a random pair taking values in Rp R. In the so-called single-index model, one has Y = f*(θ*TX)+W, where f* is an unknown univariate measurable function, c? is an unknown vector in Rd, andW denotes a random noise satisfying E[W|X] = 0. The single-index model is known to offer a flexible way to model a variety of high-dimensional real-world phenomena. However, despite its relative simplicity, this dimension reduction scheme is faced with severe complications as soon as the underlying dimension becomes larger than the number of observations ('p larger than n' paradigm). To circumvent this difficulty, we consider the single-index model estimation problem from a sparsity perspective using a PAC-Bayesian approach. On the theoretical side, we offer a sharp oracle inecuality, which is more powerful than the best known oracle inecualities for other common procedures of single-index recovery. The proposed method is implemented by means of the reversible jump Markov chain Monte Carlo technicue and its performance is compared with that of standard procedures. © 2013 Pierre Alcuier and Gerard Biau.","Oracle inecuality; PAC-Bayesian; Regression estimation; Reversible jump Markov chain Monte Carlo method; Single-index model; Sparsity","Oracle inecuality; PAC-Bayesian; Regression estimation; Reversible jump Markov chain Monte Carlo; Single index models; Sparsity; Artificial intelligence; Software engineering; Bayesian networks",Article,Scopus,2-s2.0-84873482408
"Jula A., Othman Z., Sundararajan E.","A hybrid imperialist competitive-gravitational attraction search algorithm to optimize cloud service composition",2013,"Proceedings of the 2013 IEEE Workshop on Memetic Computing, MC 2013 - 2013 IEEE Symposium Series on Computational Intelligence, SSCI 2013",11,10.1109/MC.2013.6608205,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886471801&doi=10.1109%2fMC.2013.6608205&partnerID=40&md5=fe464f1dfb8c156f85adee5aa3f45c4d","Service composition is among the most important challenges that cloud providers have ever faced. Optimization of QoS attributes when composing simple atomic services to obtain a complex service can be considered to be an NP-hard problem, which could be solved properly by using Hybrid optimization algorithms. In this research, the hybridization of an improved Gravitational Attraction Search (as a local search algorithm) with an Imperialist Competitive Algorithm has led us to introduce and apply a new memetic algorithm for gaining optimal or near optimal response time and execution fees simultaneously, for cloud computing service composition. Using a roulette wheel selection algorithm to make well-advised and non-blind decisions to choose the number of countries in each empire that should be selected to apply a local search to has assisted the hybrid algorithm at achieving better solutions. Introducing a new equation to calculate the QoS eligibility of the solutions that were generated based on the normalization of the response time and execution fee has also led us to compute the results fairly and in a scientifically based manner. © 2013 IEEE.","cloud computing; gravitational attraction search; imperialist competitive search; QoS attributes; service composition","Artificial intelligence; Cloud computing; Computational complexity; Learning algorithms; Optimization; Cloud computing services; Cloud service compositions; Gravitational attraction; Hybrid optimization algorithm; Imperialist competitive; Imperialist competitive algorithms; QoS attributes; Service compositions; Quality of service",Conference Paper,Scopus,2-s2.0-84886471801
"Manna C., Nanni L., Lumini A., Pappalardo S.","Artificial intelligence techniques for embryo and oocyte classification",2013,"Reproductive BioMedicine Online",11,10.1016/j.rbmo.2012.09.015,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874117575&doi=10.1016%2fj.rbmo.2012.09.015&partnerID=40&md5=fc627dcb2b2d55c551ef4dbb2b4bff58","One of the most relevant aspects in assisted reproduction technology is the possibility of characterizing and identifying the most viable oocytes or embryos. In most cases, embryologists select them by visual examination and their evaluation is totally subjective. Recently, due to the rapid growth in the capacity to extract texture descriptors from a given image, a growing interest has been shown in the use of artificial intelligence methods for embryo or oocyte scoring/selection in IVF programmes. This work concentrates the efforts on the possible prediction of the quality of embryos and oocytes in order to improve the performance of assisted reproduction technology, starting from their images. The artificial intelligence system proposed in this work is based on a set of Levenberg-Marquardt neural networks trained using textural descriptors (the local binary patterns). The proposed system was tested on two data sets of 269 oocytes and 269 corresponding embryos from 104 women and compared with other machine learning methods already proposed in the past for similar classification problems. Although the results are only preliminary, they show an interesting classification performance. This technique may be of particular interest in those countries where legislation restricts embryo selection. © 2012, Reproductive Healthcare Ltd.","Assisted reproduction technology; Embryo selection; Machine learning techniques; Neural networks; Oocyte selection; Texture analysis","article; artificial intelligence; clinical protocol; embryo cell; embryo development; female; human; human cell; medicolegal aspect; oocyte; quality control; reproduction",Article,Scopus,2-s2.0-84874117575
"Bartoli A., Pizarro D., Loog M.","Stratified generalized procrustes analysis",2013,"International Journal of Computer Vision",11,10.1007/s11263-012-0565-0,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873164866&doi=10.1007%2fs11263-012-0565-0&partnerID=40&md5=1371dc2de27df2d288b317f4e1753f2d","Generalized procrustes analysis computes the best set of transformations that relate matched shape data. In shape analysis the transformations are usually chosen as similarities, while in general statistical data analysis other types of transformation groups such as the affine group may be used. Generalized procrustes analysis has a nonlinear and nonconvex formulation. The classical approach alternates the computation of a so-called reference shape and the computation of transformations relating this reference shape to each shape datum in turn. We propose the stratified approach to generalized procrustes analysis. It first uses the affine transformation group to analyze the data and then upgrades the solution to the sought after group, whether Euclidean or similarity. We derive a convex formulation for each of these two steps, and efficient practical algorithms that gracefully handle missing data (incomplete shapes). Extensive experimental results show that our approaches perform well on simulated and real data. In particular our closed-form solution gives very accurate results for generalized procrustes analysis of Euclidean data. © 2012 Springer Science+Business Media, LLC.","Generalized; Procrustes; Registration; Shape; Theseus","Generalized; Procrustes; Registration; Shape; THESEUS; Artificial intelligence; Software engineering; Nonbibliographic retrieval systems",Article,Scopus,2-s2.0-84873164866
"Eid H.F., Hassanien A.E., Kim T.-H., Banerjee S.","Linear Correlation-Based Feature Selection for Network Intrusion Detection Model",2013,"Communications in Computer and Information Science",11,10.1007/978-3-642-40597-6_21,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904659464&doi=10.1007%2f978-3-642-40597-6_21&partnerID=40&md5=474909144ff7f6a122ff7bb24d922b8e","Feature selection is a preprocessing phase to machine learning, which leads to increase the classification accuracy and reduce its complexity. However, the increase of data dimensionality poses a challenge to many existing feature selection methods. This paper formulates and validates a method for selecting optimal feature subset based on the analysis of the Pearson correlation coefficients. We adopt the correlation analysis between two variables as a feature goodness measure. Where, a feature is good if it is highly correlated to the class and is low correlated to the other features. To evaluate the proposed Feature selection method, experiments are applied on NSL-KDD dataset. The experiments shows that, the number of features is reduced from 41 to 17 features, which leads to improve the classification accuracy to 99.1%. Also,The efficiency of the proposed linear correlation feature selection method is demonstrated through extensive comparisons with other well known feature selection methods. © Springer-Verlag Berlin Heidelberg 2013.","Data Reduction; Feature selection; Intrusion detection; Linear Correlation; Network security","Artificial intelligence; Correlation methods; Data processing; Data reduction; Experiments; Feature extraction; Frequency hopping; Intrusion detection; Network security; Classification accuracy; Correlation analysis; Data dimensionality; Feature selection methods; Linear correlation; Network intrusion detection; Pearson correlation coefficients; Preprocessing phase; Complex networks",Conference Paper,Scopus,2-s2.0-84904659464
"González-Álvarez D.L., Vega-Rodríguez M.A., Gómez-Pulido J.A., Sánchez-Pérez J.M.","Comparing multiobjective swarm intelligence metaheuristics for DNA motif discovery",2013,"Engineering Applications of Artificial Intelligence",11,10.1016/j.engappai.2012.06.014,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870061263&doi=10.1016%2fj.engappai.2012.06.014&partnerID=40&md5=dfc07096b60c6996c7b4f35f5e5d77e2","In recent years, a huge number of biological problems have been successfully addressed through computational techniques, among all these computational techniques we highlight metaheuristics. Also, most of these biological problems are directly related to genomic, studying the microorganisms, plants, and animals genomes. In this work, we solve a DNA sequence analysis problem called Motif Discovery Problem (MDP) by using two novel algorithms based on swarm intelligence: Artificial Bee Colony (ABC) and Gravitational Search Algorithm (GSA). To guide the pattern search to solutions that have a better biological relevance, we have redefined the problem formulation and incorporated several biological constraints that should be satisfied by each solution. One of the most important characteristics of the problem definition is the application of multiobjective optimization (MOO), maximizing three conflicting objectives: motif length, support, and similarity. So, we have adapted our algorithms to the multiobjective context. This paper presents an exhaustive comparison of both multiobjective proposals on instances of different nature: real instances, generic instances, and instances generated according to a Markov chain. To analyze their operations we have used several indicators and statistics, comparing their results with those obtained by standard algorithms in multiobjective computation, and by 14 well-known biological methods. © 2012 Elsevier Ltd. All rights reserved.","Artificial bee colony; Deoxyribonucleic acid (DNA); Gravitational search algorithm; Motif discovery; Multiobjective optimization; Swarm intelligence","Artificial bee colonies; Biological methods; Biological problems; Computational technique; DNA motif; DNA sequence analysis; Gravitational search algorithms; Meta heuristics; Motif discovery; Multi objective; Novel algorithm; Pattern search; Problem definition; Problem formulation; Standard algorithms; Swarm Intelligence; Artificial intelligence; DNA; Genes; Heuristic algorithms; Learning algorithms; Markov processes; Multiobjective optimization; Nucleic acids",Article,Scopus,2-s2.0-84870061263
"Wang H., Shkjezi F., Hoxha E.","Distance metric learning for multi-camera people matching",2013,"2013 6th International Conference on Advanced Computational Intelligence, ICACI 2013 - Proceedings",11,10.1109/ICACI.2013.6748490,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896350254&doi=10.1109%2fICACI.2013.6748490&partnerID=40&md5=c808f0e012a7a9a619c88921ee035fcc","In this paper, we propose a supervised distance metric learning method for the problem of matching people in different but non-overlapping camera pictures, which is an important and challenging problem for behavior understanding. Different from previous methods, which try to extract good visual features, in this paper, we try to model it as a distance metric learning problem. We formulate the problem so that the learned distance between the a pair of true matched people' image is smaller than that of a wrong matched pair. We conducted experiments on one benchmarking dataset, and demonstrate the advantage of the proposed distance learning models over state-of-the-art multi-camera people matching techniques. © 2013 IEEE.",,"Artificial intelligence; Behavior understanding; Distance Metric Learning; Matching techniques; Multi-cameras; Non-overlapping cameras; Visual feature; Distance education",Conference Paper,Scopus,2-s2.0-84896350254
"Chaganty A.T., Nori A.V., Rajamani S.K.","Efficiently sampling probabilistic programs via program analysis",2013,"Journal of Machine Learning Research",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954244367&partnerID=40&md5=ea391a88efa05a20058ff39e5166e24c","Probabilistic programs are intuitive and succinct representations of complex probability distributions. A natural approach to performing inference over these programs is to execute them and compute statistics over the resulting samples. Indeed, this approach has been taken before in a number of probabilistic programming tools. In this paper, we address two key challenges of this paradigm: (i) ensuring samples are well distributed in the combinatorial space of the program, and (ii) efficiently generating samples with minimal rejection. We present a new sampling algorithm Qi that addresses these challenges using concepts from the field of program analysis. To solve the first challenge (getting diverse samples), we use a technique called symbolic execution to systematically explore all the paths in a program. In the case of programs with loops, we systematically explore all paths up to a given depth, and present theorems on error bounds on the estimates as a function of the path bounds used. To solve the second challenge (efficient samples with minimal rejection), we propagate observations backward through the program using the notion of Dijkstra's weakest preconditions and hoist these propagated conditions to condition elementary distributions during sampling. We present theorems explaining the mathematical properties of Qi, as well as empirical results from an implementation of the algorithm. Copyright 2013 by the authors.",,"Artificial intelligence; Error analysis; Sampling; Generating samples; Mathematical properties; Natural approaches; Probabilistic programming; Probabilistic programs; Succinct representation; Symbolic execution; Weakest precondition; Probability distributions",Conference Paper,Scopus,2-s2.0-84954244367
"Ting I.-W., Chang Y.-K.","Improved group-based cooperative caching scheme for mobile ad hoc networks",2013,"Journal of Parallel and Distributed Computing",11,10.1016/j.jpdc.2012.12.013,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893666877&doi=10.1016%2fj.jpdc.2012.12.013&partnerID=40&md5=2412d80888b7f7f44ac4057c1d31c645","Data caching is a popular technique that improves data accessibility in wired or wireless networks. However, in mobile ad hoc networks, improvement in access latency and cache hit ratio may diminish because of the mobility and limited cache space of mobile hosts (MHs). In this paper, an improved cooperative caching scheme called group-based cooperative caching (GCC) is proposed to generalize and enhance the performance of most group-based caching schemes. GCC allows MHs and their neighbors to form a group, and exchange a bitmap data directory periodically used for proposed algorithms, such as the process of data discovery, and cache placement and replacement. The goal is to reduce the access latency of data requests and efficiently use available caching space among MH groups. Two optimization techniques are also developed for GCC to reduce computation and communication overheads. The first technique compresses the directories using an aggregate bitmap. The second employs multi-point relays to develop a forwarding node selection scheme to reduce the number of broadcast messages inside the group. Our simulation results show that the optimized GCC yields better results than existing cooperative caching schemes in terms of cache hit ratio, access latency, and average hop count. © 2013 Elsevier Inc. All rights reserved.","Cooperative caching; Mobile ad hoc networks; Multi-point relays","Artificial intelligence; Computer programming; Broadcast messages; Cache hit ratio; Communication overheads; Cooperative caching; Data accessibility; Forwarding nodes; Multi-points; Optimization techniques; Mobile ad hoc networks",Article,Scopus,2-s2.0-84893666877
"Borkowski P.","Ship course stabilization by feedback linearization with adaptive object model",2013,"Polish Maritime Research",11,10.2478/pomr-2014-0003,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899028764&doi=10.2478%2fpomr-2014-0003&partnerID=40&md5=2c6ca38925054ec7ab2b9822f30d9263","The algorithm of ship course stabilization herein presented is based on a feedback linearization controller with adaptive object model. The described method, consisting in current approximation of unknown object model functions by neuro-fuzzy approximators, represents a new generation of adaptive control methods. The implementation of this algorithm, which may constitute an executive module of a navigational decision support system, will contribute to a higher degree of automation and navigational safety improvement.","feedback linearization controller; GRBF; safety of navigation; ship course stabilization","Algorithms; Artificial intelligence; Decision support systems; Feedback linearization; Navigation; Ships; Adaptive control methods; Adaptive object modeling; Degree of automation; Feedback linearization controllers; GRBF; Navigational safeties; Neuro-fuzzy approximators; Safety of navigation; Stabilization",Article,Scopus,2-s2.0-84899028764
"Wang Y., Wang L., Li Y., He D., Chen W., Liu T.-Y.","A theoretical analysis of NDCG ranking measures",2013,"Journal of Machine Learning Research",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898043454&partnerID=40&md5=11f5570fa0ee2a03fdb23fef31e6824b","A central problem in ranking is to design a measure for evaluation of ranking functions. In this paper we study, from a theoretical perspective, the Normalized Discounted Cumulative Gain (NDCG) which is a family of ranking measures widely used in practice. Although there are extensive empirical studies of the NDCG family, little is known about its theoretical properties. We first show that, whatever the ranking function is, the standard NDCG which adopts a logarithmic discount, converges to 1 as the number of items to rank goes to infinity. On the first sight, this result seems to imply that the standard NDCG cannot differentiate good and bad ranking functions on large datasets, contradicting to its empirical success in many applications. In order to have a deeper understanding of the general NDCG ranking measures, we propose a notion referred to as consistent distin-guishability. This notion captures the intuition that a ranking measure should have such a property: For every pair of substantially different ranking functions, the ranking measure can decide which one is better in a consistent manner on almost all datasets. We show that standard NDCG has consistent distinguishability although it converges to the same limit for all ranking functions. We next characterize the set of all feasible discount functions for NDCG according to the concept of consistent distinguishability. Specifically we show that whether an NDCG measure has consistent distinguishability depends on how fast the discount decays; and r-1 is a critical point. We then turn to the cut-off version of NDCG, i.e., NDCG@k. We analyze the distinguishability of NDCG@k for various choices of k and the discount functions. Experimental results on real Web search datasets agree well with the theory. © 2013 Y. Wang, L. Wang, Y. Li, D. He, W. Chen &amp; T.-Y. Liu.","Consistent Distinguishability; NDCG; Ranking; Ranking measures","Artificial intelligence; Software engineering; Central problems; Distinguishability; Empirical studies; Large datasets; NDCG; Ranking; Ranking functions; Ranking measures; Information retrieval",Conference Paper,Scopus,2-s2.0-84898043454
"Maurer A., Pontil M.","Excess risk bounds for multitask learning with trace norm regularization",2013,"Journal of Machine Learning Research",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898036145&partnerID=40&md5=a23d863cbe50fcea28d4bbf830db000f","Trace norm regularization is a popular method of multitask learning. We give excess risk bounds with explicit dependence on the number of tasks, the number of examples per task and properties of the data distribution. The bounds are independent of the dimension of the input space, which may be infinite as in the case of reproducing kernel Hilbert spaces. A byproduct of the proof are bounds on the expected norm of sums of random positive semidefinite matrices with subexponential moments. © 2013 A. Maurer & M. Pontil.","Multitask learning; Random matrices; Risk bounds; Trace norm regularization","Artificial intelligence; Software engineering; Data distribution; Explicit dependences; Multitask learning; Positive semidefinite matrices; Random matrices; Reproducing Kernel Hilbert spaces; Risk bounds; Trace-norms; Learning algorithms",Conference Paper,Scopus,2-s2.0-84898036145
"Mantere M., Sailio M., Noponen S.","Network traffic features for anomaly detection in specific industrial control system network",2013,"Future Internet",11,10.3390/5040460,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017328771&doi=10.3390%2f5040460&partnerID=40&md5=c4a4eb03dbdb469af641fcebfaeebc1b","The deterministic and restricted nature of industrial control system networks sets them apart from more open networks, such as local area networks in office environments. This improves the usability of network security, monitoring approaches that would be less feasible in more open environments. One of such approaches is machine learning based anomaly detection. Without proper customization for the special requirements of the industrial control system network environment, many existing anomaly or misuse detection systems will perform sub-optimally. A machine learning based approach could reduce the amount of manual customization required for different industrial control system networks. In this paper we analyze a possible set of features to be used in a machine learning based anomaly detection system in the real world industrial control system network environment under investigation. The network under investigation is represented by architectural drawing and results derived from network trace analysis. The network trace is captured from a live running industrial process control network and includes both control data and the data flowing between the control network and the office network. We limit the investigation to the IP traffic in the traces. © 2013 by the authors.","Anomaly detection; Industrial control systems; Machine learning; Network security","Architectural design; Artificial intelligence; Control systems; Feature extraction; Learning algorithms; Learning systems; Anomaly detection; Anomaly detection systems; Industrial control systems; Industrial process control; Misuse detection systems; Monitoring approach; Network environments; Office environments; Network security",Article,Scopus,2-s2.0-85017328771
"Ahmed A., Hong L., Smola A.J.","Nested Chinese Restaurant Franchise Processes: Applications to user tracking and document modeling",2013,"30th International Conference on Machine Learning, ICML 2013",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897526759&partnerID=40&md5=bce42aae0444505ac0b8ba3a44430ae2","Much natural data is hierarchical in nature. Moreover, this hierarchy is often shared between different instances. We introduce the nested Chinese Restaurant Franchise Process to obtain both hierarchical tree-structured representations for objects, akin to (but more general than) the nested Chinese Restaurant Process while sharing their structure akin to the Hierarchical Dirichlet Process. Moreover, by decoupling the structure generating part of the process from the components responsible for the observations, we are able to apply the same statistical approach to a variety of user generated data. In particular, we model the joint distribution of microblogs and locations for Twitter for users. This leads to a 40% reduction in location uncertainty relative to the best previously published results. Moreover, we model documents from the NIPS papers dataset, obtaining excellent perplexity relative to (hierarchical) Pachinko allocation and LDA. Copyright 2013 by the author(s).",,"Artificial intelligence; Software engineering; Document model; Hierarchical Dirichlet process; Joint distributions; Location uncertainty; Statistical approach; Tree-structured representation; User tracking; User-generated; Learning systems",Conference Paper,Scopus,2-s2.0-84897526759
"Mayilvaganan M., Sabitha M.","A cloud-based architecture for Big-Data analytics in smart grid: A proposal",2013,"2013 IEEE International Conference on Computational Intelligence and Computing Research, IEEE ICCIC 2013",11,10.1109/ICCIC.2013.6724168,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894247555&doi=10.1109%2fICCIC.2013.6724168&partnerID=40&md5=de5a790e41f602caf03663a70d52f69b","A Smart Grid is an enhanced version of electric grid in which the demand and supply are balanced to meet the customers need. The paper deals with the formation of a cloud-based Smart Grid for analyzing the Bid-Data and taking decisions to balance the demand of customer needs. The proposed formation of smart grid will deal with Big Data set which will contain the data regarding the power usage patterns of customers, historic weather data of the location, the current demand and supply details. The grid will operate on the data being fetched from the cloud storage. The paper also focuses on smart grid being framed with the renewable energy sources. © 2013 IEEE.","Big Data Analytics; Cassandra; Cloud Computing; Hadoop; Hive; Renewable Energy; Smart Grid","Artificial intelligence; Cloud computing; Digital storage; Renewable energy resources; Sales; Big datum; Cassandras; Hadoop; Hive; Renewable energies; Smart grid; Smart power grids",Conference Paper,Scopus,2-s2.0-84894247555
"Tarlow D., Swersky K., Charlin L., Sutskever I., Zemel R.S.","Stochastic k-neighborhood selection for supervised and unsupervised learning",2013,"30th International Conference on Machine Learning, ICML 2013",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897512006&partnerID=40&md5=138ae0b6b418ea49caa12e27685c5925","Neighborhood Components Analysis (NCA) is a popular method for learning a distance metric to be used within a k-nearest neighbors (kNN) classifier. A key assumption built into the model is that each point stochastically selects a single neighbor, which makes the model well-justified only for kNN with k = 1. However, kNN classifiers with k > 1 are more robust and usually preferred in practice. Here we present kNCA, which generalizes NCA by learning distance metrics that are appropriate for kNN with arbitrary k. The main technical contribution is showing how to efficiently compute and optimize the expected accuracy of a kNN classifier. We apply similar ideas in an unsupervised setting to yield kSNE and kt-SNE, generalizations of Stochastic Neighbor Embedding (SNE, t-SNE) that operate on neighborhoods of size k, which provide an axis of control over embeddings that allow for more homogeneous and interpretable regions. Empirically, we show that kNCA often improves classification accuracy over state of the art methods, produces qualitative differences in the embeddings as k is varied, and is more robust with respect to label noise. Copyright 2013 by the author(s).",,"Artificial intelligence; Software engineering; Classification accuracy; Distance metrics; K nearest neighbor (KNN); Qualitative differences; State-of-the-art methods; Stochastic neighbor embedding; Supervised and unsupervised learning; Technical contribution; Learning systems",Conference Paper,Scopus,2-s2.0-84897512006
"Robinson E.C., Jbabdi S., Andersson J., Smith S., Glasser M.F., Van Essen D.C., Burgess G., Harms M.P., Barch D.M., Barch D.M.","Multimodal surface matching: fast and generalisable cortical registration using discrete optimisation.",2013,"Information processing in medical imaging : proceedings of the ... conference",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887011429&partnerID=40&md5=b8672a740062124f870d501ab894f344","Group neuroimaging studies of the cerebral cortex benefit from accurate, surface-based, cross-subject alignment for investigating brain architecture, function and connectivity. There is an increasing amount of high quality data available. However, establishing how different modalities correlate across groups remains an open research question. One reason for this is that the current methods for registration, based on cortical folding, provide sub-optimal alignment of some functional subregions of the brain. A more flexible framework is needed that will allow robust alignment of multiple modalities. We adapt the Fast Primal-Dual (Fast-PD) approach for discrete Markov Random Field (MRF) optimisation to spherical registration by reframing the deformation labels as a discrete set of rotations and propose a novel regularisation term, derived from the geodesic distance between rotation matrices. This formulation allows significant flexibility in the choice of similarity metric. To this end we propose a new multivariate cost function based on the discretisation of a graph-based mutual information measure. Results are presented for alignment driven by scalar metrics of curvature and myelination, and multivariate features derived from functional task performance. These experiments demonstrate the potential of this approach for improving the integration of complementary brain data sets in the future.",,"algorithm; article; artificial intelligence; automated pattern recognition; brain cortex; computer assisted diagnosis; diffusion tensor imaging; histology; human; image enhancement; image subtraction; information retrieval; methodology; myelinated nerve; reproducibility; sensitivity and specificity; three dimensional imaging; ultrastructure; Algorithms; Artificial Intelligence; Cerebral Cortex; Diffusion Tensor Imaging; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Information Storage and Retrieval; Nerve Fibers, Myelinated; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-84887011429
"Tsai C.-W., Tseng S.-P., Yang C.-S., Chiang M.-C.","PREACO: A fast ant colony optimization for codebook generation",2013,"Applied Soft Computing Journal",11,10.1016/j.asoc.2013.01.017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881609828&doi=10.1016%2fj.asoc.2013.01.017&partnerID=40&md5=abcb1f4d914bbc7e70c3e192b61263ca","This paper presents an effective and efficient method for speeding up ant colony optimization (ACO) in solving the codebook generation problem. The proposed method is inspired by the fact that many computations during the convergence process of ant-based algorithms are essentially redundant and thus can be eliminated to boost their convergence speed, especially for large and complex problems. To evaluate the performance of the proposed method, we compare it with several state-of-the-art metaheuristic algorithms. Our simulation results indicate that the proposed method can significantly reduce the computation time of ACO-based algorithms evaluated in this paper while at the same time providing results that match or outperform those ACO by itself can provide. © 2013 Elsevier B.V. All rights reserved.","Ant colony optimization; Codebook generation problem; Pattern reduction","Artificial intelligence; Optimization; Aco-based algorithms; Ant based algorithms; Ant Colony Optimization (ACO); Codebook generation; Convergence process; Convergence speed; Meta heuristic algorithm; Pattern reductions; Ant colony optimization",Article,Scopus,2-s2.0-84881609828
"Karaarslan A.","The implementation of bee colony optimization algorithm to sheppard-taylor PFC converter",2013,"IEEE Transactions on Industrial Electronics",11,10.1109/TIE.2012.2204711,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875694705&doi=10.1109%2fTIE.2012.2204711&partnerID=40&md5=c0a06916dbf449528f61094d58232fa4","In this paper, a bee colony optimization (BCO) algorithm is adapted to the current control strategy of power factor (PF) correction for a Sheppard-Taylor converter. The control approach is developed to eliminate the input current harmonics in the converter. The BCO algorithm is a new population-based search algorithm which is one of the metaheuristic techniques based on swarm intelligence. This algorithm depends on modeling the natural behavior of real honey bees. In this approach, duty cycles are generated by the BCO algorithm for the Sheppard-Taylor converter switch to obtain unity PF (UPF) and lower total harmonic distortion of the input current. The duty cycles for half a line period are calculated and stored in a lookup table. By synchronizing the memory with the line, near-unity PFs can be achieved in an operating point. The feedforward is also used into the control algorithm with the maximum value of the input voltage. The implementation of feedforward improves the converter performance. The input current of the converter is operated in continuous conduction mode. The simulation and experimental results show that the proposed control strategy works well and the UPF can be achieved with wide input voltage, load, and parameter variation. The results are compatible with the International Electrotechnical Commission (IEC) 61000-3-2 current harmonic standard. © 1982-2012 IEEE.","AC-DC converter; bee colony optimization (BCO); power conversion; power factor (PF) correction (PFC); power quality; Sheppard-Taylor converter","Artificial intelligence; Electric power factor correction; Optimization; Power converters; Power quality; Ac-dc converters; Bee colony optimizations; Power conversion; Power factors; Sheppard-Taylor converter; Algorithms",Article,Scopus,2-s2.0-84875694705
"Xu Y., Gao X., Lin S., Wong D.W., Liu J., Xu D., Cheng C.Y., Cheung C.Y., Wong T.Y.","Automatic grading of nuclear cataracts from slit-lamp lens images using group sparsity regression.",2013,"Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention",11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897574054&partnerID=40&md5=a4c99515440dacfc8aeb03cb9a3c04a2","Cataracts, which result from lens opacification, are the leading cause of blindness worldwide. Current methods for determining the severity of cataracts are based on manual assessments that may be weakened by subjectivity. In this work, we propose a system to automatically grade the severity of nuclear cataracts from slit-lamp images. We introduce a new feature for cataract grading together with a group sparsity-based constraint for linear regression, which performs feature selection, parameter selection and regression model training simultaneously. In experiments on a large database of 5378 images, our system outperforms the state-of-the-art by yielding with respect to clinical grading a mean absolute error (epsilon) of 0.336, a 69.0% exact integral agreement ratio (R0), a 85.2% decimal grading error < or = 0.5 (Re0.5), and a 98.9% decimal grading error < or = 1.0 (Re1.0). Through a more objective grading of cataracts using our proposed system, there is potential for better clinical management of the disease.",,"algorithm; article; artificial intelligence; automated pattern recognition; cataract; computer assisted diagnosis; human; illumination; image enhancement; methodology; ophthalmoscopy; pathology; regression analysis; reproducibility; sensitivity and specificity; severity of illness index; statistical analysis; Algorithms; Artificial Intelligence; Cataract; Data Interpretation, Statistical; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Lighting; Ophthalmoscopy; Pattern Recognition, Automated; Regression Analysis; Reproducibility of Results; Sensitivity and Specificity; Severity of Illness Index",Article,Scopus,2-s2.0-84897574054
"Banyal R.K., Jain P., Jain V.K.","Multi-factor authentication framework for cloud computing",2013,"Proceedings of International Conference on Computational Intelligence, Modelling and Simulation",11,10.1109/CIMSim.2013.25,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892773341&doi=10.1109%2fCIMSim.2013.25&partnerID=40&md5=c47c1441cb0f947061ccf1f2fb3fe120","Cloud computing is a new paradigm to deliverservices over the Internet. Data Security is the most criticalissues in a cloud computing environment. Authentication is akey technology for information security, which is a mechanismto establish proof of identities to get access of information inthe system. Traditional password authentication does notprovide enough security for information in cloud computingenvironment to the most modern means of attacks. In thispaper, we propose a new multi-factor authenticationframework for cloud computing. The proposed frameworkprovides a feasible and a most efficient mechanism which canclosely integrate with the traditional authentication system.The proposed framework is verified by developing CloudAccess Management (CAM) system which authenticates theuser based on multiple factors. Also using secret-splitting andencrypted value of arithmetic captcha is innovative factor foruser authentication for cloud computing environment.Prototype model for cloud computing own cloud server isimplemented using open sources technology. The proposedframework shows the close agreement with the standardcriteria for security. © 2013 IEEE.","Arithmetic Captcha; CAM System; Multi-Factor Authentication; Out of Band Authentication","Artificial intelligence; Cams; Cloud computing; Computer systems; Electronic mail filters; Security of data; CAM systems; CAPTCHAs; Cloud computing environments; Multi-factor authentication; Multiple factors; Out of band; Password authentication; Secret-splitting; Authentication",Conference Paper,Scopus,2-s2.0-84892773341
"Delzanno G., Sangnier A., Traverso R.","Parameterized verification of broadcast networks of register automata",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,10.1007/978-3-642-41036-9_11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886391938&doi=10.1007%2f978-3-642-41036-9_11&partnerID=40&md5=984aa031e52c2c5e467bf63857d6742e","We study parameterized verification problems for networks of interacting register automata. We consider safety properties expressed in terms of reachability, from arbitrarily large initial configurations, of a configuration exposing some given control states and patterns. © 2013 Springer-Verlag.",,"Artificial intelligence; Computer science; Broadcast Networks; Control state; Initial configuration; Parameterized verifications; Reachability; Safety property; Automata theory",Conference Paper,Scopus,2-s2.0-84886391938
"Park T., Kim H.","A data warehouse-based decision support system for sewer infrastructure management",2013,"Automation in Construction",11,10.1016/j.autcon.2012.11.017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870835606&doi=10.1016%2fj.autcon.2012.11.017&partnerID=40&md5=5394437add122022a9ca6a087555f9e2","Since the inception of the Governmental Accounting Standards Board statement-34 (GASB 34) in the United States, local and state governing entities need to inspect sewer systems and collect general information about their properties. Application of the collected information in decision-making processes, however, is often problematic due to the lack of consistency and completeness of infrastructure data. In addition, most techniques involved in decision-making processes are relatively complicated and dif ficult to implement without a certain level of engineering experience and training. Consequently, the sharing and transferring of pertinent information among stakeholders is not smooth and is frequently limited. This study presents a decision support system (DSS) for the management of sewer infrastructure using data warehousing technology. The proposed decision support system automatically assigns appropriate inspection and renewal methods for each pipeline and estimates associated costs, resulting in effective and practical sewer infrastructure management from various perspectives, with corresponding levels of detail. © 2012 Elsevier B.V. All rights reserved.","Data warehouse; Decision support system; Expert system; Infrastructure management; Renewal","Artificial intelligence; Data warehouses; Decision making; Expert systems; Information management; Sewers; Decision making process; Decision support system (dss); General information; Governmental accounting standards boards; Infrastructure managements; Renewal; Sewer infrastructure; Sharing and transferring; Decision support systems",Article,Scopus,2-s2.0-84870835606
"Radecki P., Hencey B.","Online thermal estimation, control, and self-excitation of buildings",2013,"Proceedings of the IEEE Conference on Decision and Control",11,10.1109/CDC.2013.6760642,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902358188&doi=10.1109%2fCDC.2013.6760642&partnerID=40&md5=481c891fdfc547a8e27a80cf2a71bdc4","This paper investigates a method to improve building control performance via online identification and excitation (active learning process) that does not disrupt normal operations. In previous studies we have demonstrated scalable methods to acquire multi-zone thermal models of passive buildings using a gray-box approach that leverages building topology and measurement data. Here we extend the method to multi-zone actively controlled buildings and examine how to improve the thermal model estimation by using the controller to excite unknown portions of the building dynamics. Comparing against a baseline thermostat controller, we demonstrate the utility of both the initially acquired and improved models with a Model Predictive Control (MPC) framework, which includes weather uncertainty and timevarying temperature set-points. By coupling building topology, estimation, and control routines into a single online framework, we have demonstrated the potential for low-cost scalable methods to actively learn and control buildings for optimal occupant comfort and minimum energy usage, all while using the existing building's HVAC sensors and hardware. © 2013 IEEE.",,"Artificial intelligence; Controllers; Model predictive control; Predictive control systems; Scalability; Structural dynamics; Thermography (temperature measurement); Topology; Active-learning process; Building controls; Building dynamics; Normal operations; Occupant comforts; On-line identification; Passive buildings; Time-varying temperatures; Buildings",Conference Paper,Scopus,2-s2.0-84902358188
"Zhou Z.-G., Liu F., Jiao L.-C., Zhou Z.-J., Yang J.-B., Gong M.-G., Zhang X.-P.","A bi-level belief rule based decision support system for diagnosis of lymph node metastasis in gastric cancer",2013,"Knowledge-Based Systems",11,10.1016/j.knosys.2013.09.001,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901815465&doi=10.1016%2fj.knosys.2013.09.001&partnerID=40&md5=0b33501b906130191b2b5b189ee12655","Lymph Node Metastasis (LNM) in gastric cancer is an important prognostic factor regarding long-term survival. As it is difficult for doctors to combine multiple factors for a comprehensive analysis, Clinical Decision Support System (CDSS) is desired to help the analysis. In this paper, a novel Bi-level Belief Rule Based (BBRB) prototype CDSS is proposed. The CDSS consists of a two-layer Belief Rule Base (BRB) system. It can be used to handle uncertainty in both clinical data and specific domain knowledge. Initial BRBs are constructed by domain specific knowledge, which may not be accurate. Traditional methods for optimizing BRB are sensitive to initialization and are limited by their weak local searching abilities. In this paper, a new Clonal Selection Algorithm (CSA) is proposed to train a BRB system. Based on CSA, efficient global search can be achieved by reproducing individuals and selecting their improved maturated progenies after the affinity maturation process. The proposed prototype CDSS is validated using a set of real patient data and performs extremely well. In particular, BBRB is capable of providing more reliable and informative diagnosis than a single-layer BRB system in the case study. Compared with conventional optimization method, the new CSA could improve the diagnostic performance further by trying to avoid immature convergence to local optima. © 2013 Elsevier B.V. All rights reserved.","Belief rule base; Clinical decision support system; Clonal selection algorithm; Gastric cancer; Lymph node metastasis","Artificial intelligence; Body fluids; Diseases; Evolutionary algorithms; Hospital data processing; Optimization; Pathology; Belief rule base; Clinical decision support systems; Clonal selection algorithms; Gastric cancers; Lymph node metastasis; Decision support systems",Article,Scopus,2-s2.0-84901815465
"Tuia D., Munoz-Mari J.","Learning user's confidence for active learning",2013,"IEEE Transactions on Geoscience and Remote Sensing",11,10.1109/TGRS.2012.2203605,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872937450&doi=10.1109%2fTGRS.2012.2203605&partnerID=40&md5=36aaf3a7a5064eaa4e3fa641a1e23244","In this paper, we study the applicability of active learning (AL) in operative scenarios. More particularly, we consider the well-known contradiction between the AL heuristics, which rank the pixels according to their uncertainty, and the user's confidence in labeling, which is related to both the homogeneity of the pixel context and user's knowledge of the scene. We propose a filtering scheme based on a classifier that learns the confidence of the user in labeling, thus minimizing the queries where the user would not be able to provide a class for the pixel. The capacity of a model to learn the user's confidence is studied in detail, also showing that the effect of resolution in such a learning task. Experiments on two QuickBird images of different resolutions (with and without pansharpening) and considering committees of users prove the efficiency of the filtering scheme proposed, which maximizes the number of useful queries with respect to traditional AL. © 1980-2012 IEEE.","Active learning (AL); bad states; photointerpretation; SVM; user's confidence; very high resolution (VHR) imagery","Active Learning; bad states; SVM; user's confidence; Very high resolution; Electrical engineering; Geology; Photointerpretation; Pixels; algorithm; artificial intelligence; efficiency measurement; experimental study; heuristics; homogeneity; image classification; image processing; numerical model; pixel; QuickBird; ranking; satellite imagery; uncertainty analysis",Article,Scopus,2-s2.0-84872937450
"Ahmad T., Holil M., Wibisono W., Royyana Muslim I.","An improved Quad and RDE-based medical data hiding method",2013,"Proceeding - IEEE CYBERNETICSCOM 2013: IEEE International Conference on Computational Intelligence and Cybernetics",11,10.1109/CyberneticsCom.2013.6865798,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906911752&doi=10.1109%2fCyberneticsCom.2013.6865798&partnerID=40&md5=9a205d1a4354cf0e0cc890e0ae12f6ff","Difference Exapansion is one of popular reversible steganography methods which can be used for authenticating images. By using this method, the reconstructed image is exactly the same as the original one. However, the stego image generated is relatively different from the original. Based on Quad and RDE methods, we propose an algorithm which can improve the quality of the stego image, in terms of PSNR, in particular. The experimental results show that while the capacity of embedded message is slightly higher, the PSNR is significantly better. © 2013 IEEE.","data hiding; security; steganography medical image","Artificial intelligence; Data hiding; Embedded messages; Medical data; Reconstructed image; Reversible steganography; security; Stego image; Steganography",Conference Paper,Scopus,2-s2.0-84906911752
"Goil A., Derry M., Argall B.D.","Using machine learning to blend human and robot controls for assisted wheelchair navigation",2013,"IEEE International Conference on Rehabilitation Robotics",10,10.1109/ICORR.2013.6650454,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891091562&doi=10.1109%2fICORR.2013.6650454&partnerID=40&md5=84c87b182d3306f1e1df0936a9e5f918","This work presents an algorithm for collaborative control of an assistive semi-autonomous wheelchair. Our approach is based on a statistical machine learning technique to learn task variability from demonstration examples. The algorithm has been developed in the context of shared-control powered wheelchairs that provide assistance to individuals with impairments that affect their control in challenging driving scenarios, like doorway navigation. We validate our algorithm within a simulation environment, and find that with relatively few demonstrations, our approach allows for safe traversal of the doorway while maintaining a high level of user control. © 2013 IEEE.",,"Assistive; Collaborative control; Powered wheel chairs; Robot controls; Simulation environment; Statistical machine learning; User control; Wheelchair navigation; Algorithms; Doors; Learning systems; Robotics; Wheelchairs; Robots; algorithm; article; artificial intelligence; equipment; equipment design; man machine interaction; robotics; wheelchair; Algorithms; Artificial Intelligence; Equipment Design; Man-Machine Systems; Robotics; Wheelchairs",Conference Paper,Scopus,2-s2.0-84891091562
"Acampora G., Vitiello A.","Interoperable neuro-fuzzy services for emotion-aware ambient intelligence",2013,"Neurocomputing",10,10.1016/j.neucom.2013.01.046,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884206918&doi=10.1016%2fj.neucom.2013.01.046&partnerID=40&md5=ad09da117f248e24d410cdeffee8745a","Ambient Intelligence (AmI) carries out a futuristic vision of living environments which are sensitive and responsive to the presence of people and, by taking care of their desires, intelligently respond to their actions improving their comfort and well-being. Typically, AmI frameworks are based on distributed context-aware approaches that, by using collections of invisible and interconnected devices, elicit and analyze environmental knowledge for delivering an appropriate set of services. Emotion-aware AmI (AmE) enhances the conventional idea of intelligent environment by exploiting theories from psychology and social sciences for suitably analyzing human emotional status and achieving a higher users' satisfaction. This work proposes a novel approach of combining emotion-aware idea with a neuro-fuzzy framework to train a collection of intelligent FML-based agents aimed at delivering efficient, personalized and interoperable emotion services in an AmE environment. As will be shown in experimental results, where a usability study and a confirmation of expectations test have been performed, the proposed approach is capable of anticipating user's requirements and improving the performance of a conventional AmI framework. © 2013 Elsevier B.V.","Ambient intelligence; Fuzzy markup language; Fuzzy systems; Neural computing","Ambient intelligence; Context-aware approaches; Environmental knowledge; Fuzzy markup languages; Intelligent environment; Living environment; Neural computing; Users' satisfactions; Artificial intelligence; Fuzzy systems; Intelligent agents; Markup languages; Interoperability; article; artificial intelligence; conceptual framework; controlled study; emotion aware ambient intelligence; fuzzy system; intermethod comparison; machine learning; markup language; priority journal; process design",Article,Scopus,2-s2.0-84884206918
"Dechter R.","Reasoning with probabilistic and deterministic graphical models: Exact Algorithms",2013,"Synthesis Lectures on Artificial Intelligence and Machine Learning",10,10.2200/S00529ED1V01Y201308AIM023,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891808231&doi=10.2200%2fS00529ED1V01Y201308AIM023&partnerID=40&md5=5e94fd63d8f56e4427d8544e608dd024","Graphical models (e.g., Bayesian and constraint networks, influence diagrams, and Markov decision processes) have become a central paradigm for knowledge representation and reasoning in both artificial intelligence and computer science in general. These models are used to perform many reasoning tasks, such as scheduling, planning and learning, diagnosis and prediction, design, hardware and software verification, and bioinformatics. These problems can be stated as the formal tasks of constraint satisfaction and satisfiability, combinatorial optimization, and probabilistic inference. It is well known that the tasks are computationally hard, but research during the past three decades has yielded a variety of principles and techniques that significantly advanced the state of the art. In this book we provide comprehensive coverage of the primary exact algorithms for reasoning with such models. The main feature exploited by the algorithms is the model's graph. We present inference-based, message-passing schemes (e.g., variable-elimination) and search-based, conditioning schemes (e.g., cycle-cutset conditioning and AND/OR search). Each class possesses distinguished characteristics and in particular has different time vs. space behavior. We emphasize the dependence of both schemes on few graph parameters such as the treewidth, cycle-cutset, and (the pseudo-tree) height. We believe the principles outlined here would serve well in moving forward to approximation and anytime-based schemes. The target audience of this book is researchers and students in the artificial intelligence and machine learning area, and beyond. © 2013 by Morgan and Claypool.","AND/OR search; Bayesian networks; bucket-elimination; conditioning; constraint networks; cycle-cutset; graphical models; induced-width; inference; knowledge representation; loop-cutset; Markov networks; pseudo-tree; reasoning; treewidth; variable-elimination","AND/OR search; bucket-elimination; Constraint networks; cycle-cutset; GraphicaL model; induced-width; inference; loop-cutset; Markov networks; pseudo-tree; reasoning; Tree-width; variable-elimination; Artificial intelligence; Bayesian networks; Bioinformatics; Knowledge representation; Learning algorithms; Markov processes; Message passing; Natural gas conditioning; Verification; Graphic methods",Article,Scopus,2-s2.0-84891808231
"Giesel M., Zaidi Q.","Frequency-based heuristics for material perception",2013,"Journal of Vision",10,10.1167/13.14.7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890143382&doi=10.1167%2f13.14.7&partnerID=40&md5=2c0e381ebdd4bd7d2eee05691d072b9a","People often make rapid visual judgments of the properties of surfaces they are going to walk on or touch. How do they do this when the interactions of illumination geometry with 3-D material structure and object shape result in images that inverse optics algorithms cannot resolve without externally imposed constraints? A possibly effective strategy would be to use heuristics based on information that can be gleaned rapidly from retinal images. By using perceptual scaling of a large sample of images, combined with correspondence and canonical correlation analyses, we discovered that material properties, such as roughness, thickness, and undulations, are characterized by specific scales of luminance variations. Using movies, we demonstrate that observers' percepts of these 3-D qualities vary continuously as a function of the relative energy in corresponding 2-D frequency bands. In addition, we show that judgments of roughness, thickness, and undulations are predictably altered by adaptation to dynamic noise at the corresponding scales. These results establish that the scale of local 3-D structure is critical in perceiving material properties, and that relative contrast at particular spatial frequencies is important for perceiving the critical 3-D structure from shading cues, so that cortical mechanisms for estimating material properties could be constructed by combining the parallel outputs of sets of frequency-selective neurons. These results also provide methods for remote sensing of material properties in machine vision, and rapid synthesis, editing and transfer of material properties for computer graphics and animation.","Adaptation; Image statistics; Material perception; Spatial frequency","adaptation; algorithm; article; artificial intelligence; association; contrast sensitivity; depth perception; female; human; illumination; image statistics; male; material perception; pattern recognition; physiology; spatial frequency; adaptation; image statistics; material perception; spatial frequency; Algorithms; Artificial Intelligence; Contrast Sensitivity; Cues; Depth Perception; Female; Form Perception; Humans; Lighting; Male; Pattern Recognition, Visual",Article,Scopus,2-s2.0-84890143382
"Pota M., Esposito M., De Pietro G.","Transforming probability distributions into membership functions of fuzzy classes: A hypothesis test approach",2013,"Fuzzy Sets and Systems",10,10.1016/j.fss.2013.03.013,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887484424&doi=10.1016%2fj.fss.2013.03.013&partnerID=40&md5=351126905b21966e9205a1b2d50245fb","In fuzzy Decision Support Systems, methods are strongly required for eliciting knowledge in the form of interpretable fuzzy sets from numerical data. In medical settings, statistical data are often available, or can be obtained from rough data, typically in the form of probability distributions. Moreover, since physicians are used to think and work according to a statistical interpretation of medical knowledge, the definition of fuzzy sets starting from statistical data is thought to be able to significantly reduce the existing lack of familiarity of physicians with fuzzy set theory, with respect to the classical statistical methods. Some methods based on different assumptions transform probability distributions into fuzzy sets. However, no theoretical approach was proposed up to now, for extracting fuzzy knowledge according to a fuzzy class interpretation, which can be used for inference purposes in fuzzy rule based systems. In this paper, a method for transforming probability distributions into fuzzy sets is shown, which generalizes some existing approaches and gives them a justification. It is based on the application of statistical test of hypothesis, and the resulting fuzzy sets are interpretable as fuzzy classes. The method enables the construction of normal fuzzy sets, which can be adapted to have pseudo-triangular or pseudo-trapezoidal shape, both coherently with the corresponding probability distributions, by tuning the method parameters. The properties of this method are illustrated by applying it to simulated probability distributions and its experimental comparison with existing methods is shown. Moreover, an application is performed on a real case study involving the detection of Multiple Sclerosis lesions. © 2013 Elsevier B.V.","Classification; Decision support systems; Fuzzy system models; Possibility theory; Probability-possibility transformations","Experimental comparison; Fuzzy decision support system; Fuzzy system models; Multiple sclerosis lesions; Possibility theory; Probability-possibility transformation; Statistical interpretation; Theoretical approach; Artificial intelligence; Classification (of information); Decision support systems; Fuzzy inference; Fuzzy sets; Probability distributions; Statistical tests; Statistics; Numerical methods",Article,Scopus,2-s2.0-84887484424
"Swan A.L., Hillier K.L., Smith J.R., Allaway D., Liddell S., Bacardit J., Mobasheri A.","Analysis of mass spectrometry data from the secretome of an explant model of articular cartilage exposed to pro-inflammatory and anti-inflammatory stimuli using machine learning",2013,"BMC Musculoskeletal Disorders",10,10.1186/1471-2474-14-349,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889971850&doi=10.1186%2f1471-2474-14-349&partnerID=40&md5=f24c972993626c3bc97a02ad973b11c5","Background: Osteoarthritis (OA) is an inflammatory disease of synovial joints involving the loss and degeneration of articular cartilage. The gold standard for evaluating cartilage loss in OA is the measurement of joint space width on standard radiographs. However, in most cases the diagnosis is made well after the onset of the disease, when the symptoms are well established. Identification of early biomarkers of OA can facilitate earlier diagnosis, improve disease monitoring and predict responses to therapeutic interventions. Methods. This study describes the bioinformatic analysis of data generated from high throughput proteomics for identification of potential biomarkers of OA. The mass spectrometry data was generated using a canine explant model of articular cartilage treated with the pro-inflammatory cytokine interleukin 1 β (IL-1β). The bioinformatics analysis involved the application of machine learning and network analysis to the proteomic mass spectrometry data. A rule based machine learning technique, BioHEL, was used to create a model that classified the samples into their relevant treatment groups by identifying those proteins that separated samples into their respective groups. The proteins identified were considered to be potential biomarkers. Protein networks were also generated; from these networks, proteins pivotal to the classification were identified. Results: BioHEL correctly classified eighteen out of twenty-three samples, giving a classification accuracy of 78.3% for the dataset. The dataset included the four classes of control, IL-1β, carprofen, and IL-1β and carprofen together. This exceeded the other machine learners that were used for a comparison, on the same dataset, with the exception of another rule-based method, JRip, which performed equally well. The proteins that were most frequently used in rules generated by BioHEL were found to include a number of relevant proteins including matrix metalloproteinase 3, interleukin 8 and matrix gla protein. Conclusions: Using this protocol, combining an in vitro model of OA with bioinformatics analysis, a number of relevant extracellular matrix proteins were identified, thereby supporting the application of these bioinformatics tools for analysis of proteomic data from in vitro models of cartilage degradation. © 2013 Swan et al.; licensee BioMed Central Ltd.","Bioinformatics; Biomarker; Carprofen; Cartilage; Interleukin 1 β; Machine learning; Osteoarthritis","biological marker; carprofen; cytokine; interleukin 1beta; interleukin 8; osteocalcin; protein; proteome; secretome; stromelysin; unclassified drug; animal tissue; antiinflammatory activity; article; articular cartilage; bioinformatics; classification algorithm; controlled study; cytokine production; explant; in vitro study; machine learning; mass spectrometry; network learning; nonhuman; osteoarthritis; polyacrylamide gel electrophoresis; protein analysis; protein expression; protein function; proteomics; Animals; Artificial Intelligence; Cartilage, Articular; Dogs; Interleukin-1beta; Male; Mass Spectrometry; Osteoarthritis; Proteins; Proteome",Article,Scopus,2-s2.0-84889971850
"Green R.M., Sheppard J.W.","Comparing frequency- and style-based features for Twitter author identification",2013,"FLAIRS 2013 - Proceedings of the 26th International Florida Artificial Intelligence Research Society Conference",10,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889857496&partnerID=40&md5=521071203556869de8ea3216330d70bd","Author identification is a subfield of Natural Language Processing (NLP) that uses machine learning techniques to identify the author of a text. Most previous research focused on long texts with the assumption that a minimum text length threshold exists under which author identification would no longer be effective. This paper examines author identification in short texts far below this threshold, focusing on messages retrieved from Twitter (maximum length: 140 characters) to determine the most effective feature set for author identification. Both Bag-of-Words (BOW) and Style Marker feature sets were extracted and evaluated through a series of 15 experiments involving up to 12 authors with large and small dataset sizes. Support Vector Machines (SVM) were used for all experiments. Our results achieve classification accuracies approaching that of longer texts, even for small dataset sizes of 60 training instances per author. Style Marker feature sets were found to be significantly more useful than BOW feature sets as well as orders of magnitude faster, and are therefore suggested for potential applications in future research. Copyright © 2013, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Author identification; Classification accuracy; Data set size; Feature sets; Machine learning techniques; NAtural language processing; Orders of magnitude; Style markers; Artificial intelligence; Experiments; Learning algorithms; Natural language processing systems; Social networking (online); Support vector machines; Classification (of information)",Conference Paper,Scopus,2-s2.0-84889857496
"Ishibashi R., Júnior C.L.N.","GFRBS-PHM: A genetic fuzzy rule-based system for phm with improved interpretability",2013,"PHM 2013 - 2013 IEEE International Conference on Prognostics and Health Management, Conference Proceedings",10,10.1109/ICPHM.2013.6621419,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888871411&doi=10.1109%2fICPHM.2013.6621419&partnerID=40&md5=4fa40ff993275eb7bdadc4a716656c69","This paper presents an approach to predict the Remaining Useful Life (RUL) of a generic system when a higher level of interpretability of the prediction model is desired. A set of well known computational intelligence techniques such as Decision Trees, Fuzzy Logic, and Genetic Algorithms is used to generate a hybrid model which is called Genetic Fuzzy Rule-Based System (GFRBS) supported by a Decision Tree. The proposed method automatically generates fuzzy rules and tunes the associated membership functions. Accuracy and improved interpretability are achieved during training since they are coded in the fitness function used by the genetic algorithm. The proposed approach is applied to a case study of degradation of aeronautical engines. The task is to estimate the remaining useful life of a commercial aircraft engine using only historical data gathered by the sensors embedded in the engine. © 2013 IEEE.","Genetic Fuzzy Rule Based System; Interpretability; Knowledge Extraction; Prognostic and Health Management","Aeronautical engines; Commercial aircraft; Computational intelligence techniques; Genetic-fuzzy; Interpretability; Knowledge extraction; Prognostic and health management; Remaining useful lives; Aircraft engines; Artificial intelligence; Decision trees; Fuzzy logic; Genetic algorithms; Systems engineering; Fuzzy rules",Conference Paper,Scopus,2-s2.0-84888871411
"Kashef S., Nezamabadi-Pour H.","A new feature selection algorithm based on binary ant colony optimization",2013,"IKT 2013 - 2013 5th Conference on Information and Knowledge Technology",10,10.1109/IKT.2013.6620037,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888580579&doi=10.1109%2fIKT.2013.6620037&partnerID=40&md5=7aa1c3161854583d69552585151ee170","Feature selection is an indispensable preprocessing step for effective analysis of high dimensional data. In this paper a novel feature selection algorithm based on Ant Colony Optimization (ACO), called Advanced Binary ACO (ABACO), is presented. Features are treated as graph nodes to construct a graph model. In this graph, each feature has two nodes, one for selecting that feature and the other for deselecting. Ant colony algorithm is used to select nodes while ants should visit all features. At the end of a tour, each ant has a binary vector with the same length as the number of features where 1 implies selecting and 0 implies deselecting the corresponding feature. The experimental comparison verifies that the algorithm has a good classification accuracy using a smaller feature set than another existing ACO-based feature selection method. © 2013 IEEE.","Ant colony optimization; Classification; Dimensionality reduction; Feature selection","Ant colony algorithms; Ant Colony Optimization (ACO); Binary ant colony optimizations; Classification accuracy; Dimensionality reduction; Experimental comparison; Feature selection algorithm; Feature selection methods; Ant colony optimization; Artificial intelligence; Classification (of information); Data mining; Feature extraction; Graph theory; Algorithms",Conference Paper,Scopus,2-s2.0-84888580579
"Eisner R., Greiner R., Tso V., Wang H., Fedorak R.N.","A machine-learned predictor of colonic polyps based on urinary metabolomics",2013,"BioMed Research International",10,10.1155/2013/303982,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888610357&doi=10.1155%2f2013%2f303982&partnerID=40&md5=54e4c0f73ac777eafa1361575497e3d0","We report an automated diagnostic test that uses the NMR spectrum of a single spot urine sample to accurately distinguish patients who require a colonoscopy from those who do not. Moreover, our approach can be adjusted to tradeoff between sensitivity and specificity. We developed our system using a group of 988 patients (633 normal and 355 who required colonoscopy) who were all at average or above-average risk for developing colorectal cancer. We obtained a metabolic profile of each subject, based on the urine samples collected from these subjects, analyzed via 1H-NMR and quantified using targeted profiling. Each subject then underwent a colonoscopy, the gold standard to determine whether he/she actually had an adenomatous polyp, a precursor to colorectal cancer. The metabolic profiles, colonoscopy outcomes, and medical histories were then analysed using machine learning to create a classifier that could predict whether a future patient requires a colonoscopy. Our empirical studies show that this classifier has a sensitivity of 64% and a specificity of 65% and, unlike the current fecal tests, allows the administrators of the test to adjust the tradeoff between the two. © 2013 Roman Eisner et al.",,"ketone body; nicotinamide; nicotinic acid; trigonelline; tyrosine; adult; article; cancer risk; cancer screening; colon adenoma; colon polyp; colon polyposis; colonoscopy; colorectal cancer; controlled study; diagnostic test accuracy study; family history; feces analysis; female; gastrointestinal hemorrhage; gold standard; human; learning algorithm; machine learning; major clinical study; male; metabolomics; occult blood test; predictive value; proton nuclear magnetic resonance; sensitivity and specificity; support vector machine; urinalysis; aged; artificial intelligence; clinical trial; colon polyp; colon tumor; middle aged; nuclear magnetic resonance spectroscopy; pathology; prognosis; urine; Adult; Aged; Artificial Intelligence; Colonic Neoplasms; Colonic Polyps; Colonoscopy; Female; Humans; Magnetic Resonance Spectroscopy; Male; Metabolomics; Middle Aged; Prognosis",Article,Scopus,2-s2.0-84888610357
"Pappas V., Kemerlis V.P., Zavou A., Polychronakis M., Keromytis A.D.","CloudFence: Data flow tracking as a cloud service",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",10,10.1007/978-3-642-41284-4_21,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888310971&doi=10.1007%2f978-3-642-41284-4_21&partnerID=40&md5=a12cc73f3f8f5bf9b44079f63c7167f4","The risk of unauthorized private data access is among the primary concerns for users of cloud-based services. For the common setting in which the infrastructure provider and the service provider are different, users have to trust their data to both parties, although they interact solely with the latter. In this paper we propose CloudFence, a framework for cloud hosting environments that provides transparent, fine-grained data tracking capabilities to both service providers, as well as their users. CloudFence allows users to independently audit the treatment of their data by third-party services, through the intervention of the infrastructure provider that hosts these services. CloudFence also enables service providers to confine the use of sensitive data in well-defined domains, offering additional protection against inadvertent information leakage and unauthorized access. The results of our evaluation demonstrate the ease of incorporating CloudFence on existing real-world applications, its effectiveness in preventing a wide range of security breaches, and its modest performance overhead on real settings. © 2013 Springer-Verlag.","data auditing; data flow tracking; information confinement","Data auditing; Data flow tracking; Information leakage; Infrastructure providers; Security breaches; Sensitive datas; Service provider; Unauthorized access; Artificial intelligence; Computer science; Data transfer",Conference Paper,Scopus,2-s2.0-84888310971
"Shieh E., Jain M., Jiang A.X., Tambe M.","Efficiently solving joint activity based security games",2013,"IJCAI International Joint Conference on Artificial Intelligence",10,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062446&partnerID=40&md5=08e54c498428c686cf9b8431698aaf09","Despite recent successful real-world deployments of Stackelberg Security Games (SSGs), scale-up remains a fundamental challenge in this field. The latest techniques do not scale-up to domains where multiple defenders must coordinate time-dependent joint activities. To address this challenge, this paper presents two branch-and-price algorithms for solving SSGs, SMARTO and SMARTH, with three novel features: (i) a column-generation approach that uses an ordered network of nodes (determined by solving the traveling salesman problem) to generate individual defender strategies; (ii) exploitation of iterative reward shaping of multiple coordinating defender units to generate coordinated strategies; (iii) generation of tighter upper-bounds for pruning by solving security games that only abide by key scheduling constraints. We provide extensive experimental results and formal analyses.",,"Branch-and-price algorithms; Coordinated strategies; Formal analysis; Joint activity; Key scheduling; Real-world; Security games; Stackelberg; Artificial intelligence; Traveling salesman problem; Iterative methods",Conference Paper,Scopus,2-s2.0-84896062446
"Eiter T., Fink M., Krennwallner T., Redl C.","Liberal safety for answer set programs with external sources",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",10,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893350659&partnerID=40&md5=87838daa6bbdea425491ea2983aeb6d6","Answer set programs with external source access may introduce new constants that are not present in the program, which is known as value invention. As naive value invention leads to programs with infinite grounding and answer sets, syntactic safety criteria are imposed on programs. However, traditional criteria are in many cases unnecessarily strong and limit expressiveness. We present liberal domain-expansion (de-) safe programs, a novel generic class of answer set programs with external source access that has a finite grounding and allows for value invention. De-safe programs use so-called term bounding functions as a parameter for modular instantiation with concrete-e.g., syntactic or semantic or both-safety criteria. This ensures extensibility of the approach in the future. We provide concrete instances of the framework and develop an operator that can be used for computing a finite grounding. Finally, we discuss related notions of safety from the literature, and show that our approach is strictly more expressive. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Answer set; Bounding functions; External sources; Generic class; Safety criterion; Artificial intelligence; Concretes; Semantics; Syntactics; Patents and inventions",Conference Paper,Scopus,2-s2.0-84893350659
"Carvalho A., Larson K.","A consensual linear opinion pool",2013,"IJCAI International Joint Conference on Artificial Intelligence",10,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062510&partnerID=40&md5=8e7f2ca9bf5271ee267e0424b711c295","An important question when eliciting opinions from experts is how to aggregate the reported opinions. In this paper, we propose a pooling method to aggregate expert opinions. Intuitively, it works as if the experts were continuously updating their opinions in order to accommodate the expertise of others. Each updated opinion takes the form of a linear opinion pool, where the weight that an expert assigns to a peer's opinion is inversely related to the distance between their opinions. In other words, experts are assumed to prefer opinions that are close to their own opinions. We prove that such an updating process leads to consensus, i.e., the experts all converge towards the same opinion. Further, we show that if rational experts are rewarded using the quadratic scoring rule, then the assumption that they prefer opinions that are close to their own opinions follows naturally. We empirically demonstrate the efficacy of the proposed method using real-world data.",,"Expert opinion; Real-world; Scoring rules; Artificial intelligence; Lakes; Aggregates",Conference Paper,Scopus,2-s2.0-84896062510
"Wu F., Zilberstein S., Jennings N.R.","Monte-Carlo expectation maximization for decentralized POMDPs",2013,"IJCAI International Joint Conference on Artificial Intelligence",10,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896061161&partnerID=40&md5=cc973db86546d32e1eaf5ab006ec06b5","We address two significant drawbacks of state-of-the-art solvers of decentralized POMDPs (DECPOMDPs): the reliance on complete knowledge of the model and limited scalability as the complexity of the domain grows. We extend a recently proposed approach for solving DEC-POMDPs via a reduction to the maximum likelihood problem, which in turn can be solved using EM. We introduce a model-free version of this approach that employs Monte-Carlo EM (MCEM). While a naïve implementation of MCEM is inadequate in multiagent settings, we introduce several improvements in sampling that produce high-quality results on a variety of DEC-POMDP benchmarks, including large problems with thousands of agents.",,"Expectation - maximizations; High quality; Large problems; Model free; Multi-agent setting; Artificial intelligence; Multi agent systems",Conference Paper,Scopus,2-s2.0-84896061161
"González C., López D.M., Blobel B.","Case-based reasoning in intelligent health decision support systems",2013,"Studies in Health Technology and Informatics",10,10.3233/978-1-61499-268-4-44,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894252570&doi=10.3233%2f978-1-61499-268-4-44&partnerID=40&md5=832fdaea017d988cd674adbfb8e2898c","Decision-making is a crucial task for decision makers in healthcare, especially because decisions have to be made quickly, accurately and under uncertainty. Taking into account the importance of providing quality decisions, offering assistance in this complex process has been one of the main challenges of Artificial Intelligence throughout history. Decision Support Systems (DSS) have gained popularity in the medical field for their efficacy to assist decision-making. In this sense, many DSS have been developed, but only few of them consider processing and analysis of information contained in electronic health records, in order to identify individual or population health risk factors. This paper deals with Intelligent Decision Support Systems that are integrated into Electronic Health Records Systems (EHRS) or Public Health Information Systems (PHIS). It provides comprehensive support for a wide range of decisions with the purpose of improving quality of care delivered to patients or public health planning, respectively. © 2013 The authors and IOS Press. All rights reserved.","case-based reasoning; Decision Support Systems; health information management; intelligence; public health","artificial intelligence; case control study; conference paper; decision support system; electronic medical record; medical record; system analysis; Artificial Intelligence; Case-Control Studies; Decision Support Systems, Clinical; Electronic Health Records; Health Records, Personal; Medical Record Linkage; Systems Integration",Conference Paper,Scopus,2-s2.0-84894252570
"Ågotnes T., Harrenstein P., Van Der Hoek W., Wooldridge M.","Verifiable equilibria in Boolean games",2013,"IJCAI International Joint Conference on Artificial Intelligence",10,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896064097&partnerID=40&md5=0d793b42a0df4e5f7cd0cf6c674b3d4e","This work is motivated by the following concern. Suppose we have a game exhibiting multiple Nash equilibria, with little to distinguish them except that one of them can be verified while the others cannot. That is, one of these equilibria carries sufficient information that, if this is the outcome, then the players can tell that an equilibrium has been played. This provides an argument for this equilibrium being played, instead of the alternatives. Verifiability can thus serve to make an equilibrium a focal point in the game. We formalise and investigate this concept using a model of Boolean games with incomplete information. We define and investigate three increasingly strong types of verifiable equilibria, characterise the complexity of checking these, and show how checking their existence can be captured in a variant of modal epistemic logic.",,"Boolean games; Epistemic logic; Focal points; Nash equilibria; Verifiability; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84896064097
"Toman D., Weddell G.","Conjunctive query answering in CFDnc: A PTIME description logic with functional constraints and disjointness",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",10,10.1007/978-3-319-03680-9_36,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893812382&doi=10.1007%2f978-3-319-03680-9_36&partnerID=40&md5=934342a25db5d702af63a8a806c14c7a","We consider conjunctive query answering and other basic reasoning services in CFDnc, an alternative to the description logic CFD that retains the latter's ability to support PTIME reasoning in the presence of terminological cycles with universal restrictions over functional roles and also in the presence of functional constraints over functional role paths. In contrast, CFD nc replaces the ability to have conjunction on left-hand-sides of inclusion dependencies with the ability to have primitive negation on right-hand-sides. This makes it possible to say that primitive concepts must denote disjoint sets of individuals, a common requirement with many information sources. © Springer International Publishing 2013.",,"Conjunctive queries; Description logic; Disjoint sets; Disjointness; Functional constraints; Inclusion dependencies; Information sources; Artificial intelligence; Formal languages; Query languages; Data description",Conference Paper,Scopus,2-s2.0-84893812382
"Nishanth K.J., Ravi V.","A computational intelligence based online data imputation method: An application for banking",2013,"Journal of Information Processing Systems",10,10.3745/JIPS.2013.9.4.633,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892169160&doi=10.3745%2fJIPS.2013.9.4.633&partnerID=40&md5=6e4b6d747123b668be860eb544bb50fc","All the imputation techniques proposed so far in literaturefor data imputationare offline techniques asthey requirea number of iterations to learn the characteristics of data during training and they also consume a lot of computational time. Hence, these techniques are not suitable for applications that require the imputation to beperformed on demand and near real-time. The paper proposes a computational intelligence based architecture for online data imputation and extended versions of an existing offline data imputation method as well. The proposed online imputation technique has 2 stages. In stage 1, Evolving Clustering Method (ECM) is used to replace the missing values with cluster centers, as part of the local learning strategy. Stage 2 refines the resultant approximate values using a General Regression Neural Network (GRNN) as part of the global approximation strategy. We also propose extended versions of an existing offline imputation technique. The offline imputation techniques employ K-Meansor K-Medoids and Multi Layer Perceptron (MLP)or GRNN in Stage-1and Stage-2respectively. Several experiments wereconducted on 8benchmark datasets and 4 bank related datasets to assess the effectivenessof the proposed online and offline imputation techniques. In terms of Mean Absolute Percentage Error (MAPE), the resultsindicate that the difference between the proposed best offline imputation method viz., K-Medoids+GRNN and the proposed online imputation method viz., ECM+GRNN is statistically insignificant at a 1% level of significance. Consequently, the proposed online technique, being less expensive and faster, can be employed for imputation instead of the existing and proposed offline imputation techniques. This is the significant outcome of the study. Furthermore, GRNN in stage-2 uniformly reduced MAPE values in both offline and online imputation methods on all datasets. © 2013 KIPS.","Data imputation; Evolving Clustering Method (ECM); General regression neural network (grnn); Imputation; K-Means clustering; K-Medoids clustering; Mlp","Data imputation; Evolving clustering methods; General regression neural network; Imputation; K-means clustering; K-medoids clustering; Mlp; Artificial intelligence",Article,Scopus,2-s2.0-84892169160
"Bessiere C., Carbonnel C., Hebrard E., Katsirelos G., Walsh T.","Detecting and exploiting subproblem tractability",2013,"IJCAI International Joint Conference on Artificial Intelligence",10,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063291&partnerID=40&md5=640fb0365aba7c2d3ca57733c97d5cd4","Constraint satisfaction problems may be nearly tractable. For instance, most of the relations in a problem might belong to a tractable language. We introduce a method to take advantage of this fact by computing a backdoor to this tractable language. The method can be applied to many tractable classes for which the membership test is itself tractable. We introduce therefore two polynomial membership testing algorithms, to check if a language is closed under a majority or conservative Mal'tsev polymorphism, respectively. Then we show that computing a minimal backdoor for such classes is fixed parameter tractable (FPT) if the tractable subset of relations is given, and W[2]- complete otherwise. Finally, we report experimental results on the XCSP benchmark set. We identified a few promising problem classes where problems were nearly closed under a majority polymorphism and small backdoors could be computed.",,"Backdoors; Testing algorithm; Tractable class; Tractable subset; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84896063291
"Alcázar V., Borrajo D., Fernández S., Fuentetaja R.","Revisiting regression in planning",2013,"IJCAI International Joint Conference on Artificial Intelligence",10,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896059559&partnerID=40&md5=af073e5b8f3898cf0d19db777311880b","Heuristic search with reachability-based heuristics is arguably the most successful paradigm in Automated Planning to date. In its earlier stages of development, heuristic search was proposed as both forward and backward search. Due to the disadvantages of backward search, in the last decade researchers focused mainly on forward search, and backward search was aband oned for the most part as a valid alternative. In the last years, important advancements regarding both the theoretical understand ing and the performance of heuristic search have been achieved, applied mainly to forward search planners. In this work we revisit regression in planning with reachabilitybased heuristics, trying to extrapolate to backward search current lines of research that were not as well understood as they are now.",,"Automated planning; Current lines; Forward-and-backward; Heuristic search; Artificial intelligence; Heuristic algorithms; Modular robots",Conference Paper,Scopus,2-s2.0-84896059559
"Tsangaratos P., Ilia I., Rozos D.","Case event system for landslide susceptibility analysis",2013,"Landslide Science and Practice: Landslide Inventory and Susceptibility and Hazard Zoning",10,10.1007/978-3-642-31325-7-77,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898063097&doi=10.1007%2f978-3-642-31325-7-77&partnerID=40&md5=695c126c73f2e7b1fa57acb7479345bf","Landslides are considered as a geological disaster that has an unfavourable effect on lives and properties, generating both direct and indirect economic and human losses every year. Compared to other geological disasters, landslides are considerably smaller in scale, more dispersed, but more disastrous in many cases. The presented methodology is based on a case-event system, which uses spatial analysis functions and artificial intelligence techniques, to evaluate potential instability problems concerning natural or artificial slopes. The methodology allows the user to examine new cases or areas of interest and compares them to previously recorded cases of instability problems that occur in the research area. The effectiveness of the methodology is evaluated in Kimi, Euboea, Greece, an area experienced substantial landslide events, where a well documented database of previous studies existed. © Springer-Verlag Berlin Heidelberg 2013.","Case event system; Landslide; Susceptibility analysis","Artificial intelligence techniques; Geological disaster; Instability problems; Landslide susceptibility; New case; Potential instability; Spatial analysis; Susceptibility analysis; Artificial intelligence; Disasters; Flow control; Landslides",Conference Paper,Scopus,2-s2.0-84898063097
"Zelinsky G.J., Peng Y., Samaras D.","Eye can read your mind: Decoding gaze fixations to reveal categorical search targets",2013,"Journal of Vision",10,10.1167/13.14.10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891665672&doi=10.1167%2f13.14.10&partnerID=40&md5=7b1c2649571dc1217ffd6f9cafce9201","Is it possible to infer a person's goal by decoding their fixations on objects? Two groups of participants categorically searched for either a teddy bear or butterfly among random category distractors, each rated as high, medium, or low in similarity to the target classes. Target-similar objects were preferentially fixated in both search tasks, demonstrating information about target category in looking behavior. Different participants then viewed the searchers' scanpaths, superimposed over the target-absent displays, and attempted to decode the target category (bear/ butterfly). Bear searchers were classified perfectly; butterfly searchers were classified at 77%. Bear and butterfly Support Vector Machine (SVM) classifiers were also used to decode the same preferentially fixated objects and found to yield highly comparable classification rates. We conclude that information about a person's search goal exists in fixation behavior, and that this information can be behaviorally decoded to reveal a search target-essentially reading a person's mind by analyzing their fixations. © 2014 ARVO.","Categorical search; Classification; Computer vision; Decoding; Fixation duration","article; artificial intelligence; categorical search; classification; computer vision; decoding; eye fixation; eye movement; female; Fixation duration; human; male; pattern recognition; physiology; categorical search; classification; computer vision; decoding; fixation duration; Artificial Intelligence; Eye Movements; Female; Fixation, Ocular; Humans; Male; Pattern Recognition, Visual",Article,Scopus,2-s2.0-84891665672
"Gutierrez J., Harrenstein P., Wooldridge M.","Iterated Boolean games",2013,"IJCAI International Joint Conference on Artificial Intelligence",10,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896058994&partnerID=40&md5=5f477e08def695d86e482f20baec6991","Iterated games are well-known in the game theory literature. We study iterated Boolean games. These are games in which players repeatedly choose truth values for Boolean variables they have control over. Our model of iterated Boolean games assumes that players have goals given by formulae of Linear Temporal Logic (LTL), a formalism for expressing properties of state sequences. In order to model the strategies that players use in such games, we use a finite state machine model. After introducing and formally defining iterated Boolean games, we investigate the computational complexity of their associated game-theoretic decision problems as well as semantic conditions characterising classes of LTL properties that are preserved by pure strategy Nash equilibria whenever they exist.",,"Boolean variables; Decision problems; Finite state machine model; Game-theoretic; Iterated games; Linear temporal logic; Nash equilibria; State sequences; Game theory; Semantics; Temporal logic; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84896058994
"Fink D., Damoulas T., Dave J.","Adaptive Spatio-Temporal Exploratory Models: Hemisphere-wide species distributions from massively crowdsourced ebird data",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",10,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893383816&partnerID=40&md5=ece5a048db0935a1e259f4dea59e2fa5","Broad-scale spatiotemporal processes in conservation and sustainability science, such as continent-wide animal movement, occur across a range of spatial and temporal scales. Understanding these processes at multiple scales is crucial for developing and coordinating conservation strategies across national boundaries. In this paper we propose a general class of models we call AdaSTEM, for Adaptive Spatio-Temporal Exploratory Models, that are able to exploit variation in the density of observations while adapting to multiple scales in space and time. We show that this framework is able to efficiently discover multiscale structure when it is present, while retaining predictive performance when absent. We provide an empirical comparison and analysis, offer theoretical insights from the ensemble loss decomposition, and deploy AdaSTEM to estimate the spatiotemporal distribution of Barn Swallow (Hirundo rustica) across the Western Hemisphere using massively crowdsourced eBird data. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Conservation strategies; Empirical - comparisons; Multi-scale structures; Predictive performance; Spatial and temporal scale; Spatiotemporal distributions; Spatiotemporal process; Sustainability science; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84893383816
"Paseka J.","Operators on MV-algebras and their representations",2013,"Fuzzy Sets and Systems",10,10.1016/j.fss.2013.02.010,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886666498&doi=10.1016%2fj.fss.2013.02.010&partnerID=40&md5=3aa9a628a2ab6ff1a77cc43bb0b503a3","A crucial problem concerning (tense) operators on MV-algebras is their representations. Having an MV-algebra with (tense) operators, we can ask if there exists a frame yielding a representation of this MV-algebra. We solve this problem for semisimple MV-algebras. © 2013 Elsevier B.V.","Modal operator; Non-classical logic; Representation theory; Tense MV-algebras","Modal operators; MV-algebras; Non-classical logic; Representation theory; Artificial intelligence; Fuzzy sets",Conference Paper,Scopus,2-s2.0-84886666498
"Liu B.","A new definition of independence of uncertain sets",2013,"Fuzzy Optimization and Decision Making",10,10.1007/s10700-013-9164-y,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888200709&doi=10.1007%2fs10700-013-9164-y&partnerID=40&md5=b7dbcdb7f6ba7f346ff81598ad28bda6","Uncertain sets are an effective tool to describe unsharp concepts like ""young"", ""tall"" and ""most"". As a key concept in uncertain set theory, the independence was first defined in the paper (Liu in Fuzzy Optim Decis Mak 11(4):387-410, 2012b). However, the definition is somewhat weak to deal with uncertain sets completely. In order to overcome this disadvantage, this paper presents a stronger definition of independence of uncertain sets and discusses its mathematical properties. © 2013 Springer Science+Business Media New York.","Independence; Uncertain set; Uncertainty theory","Effective tool; Independence; Mathematical properties; Uncertain set; Uncertainty theory; Artificial intelligence; Decision making; Set theory",Article,Scopus,2-s2.0-84888200709
"Zhang W., Hansen K.M., Bellavista P.","A research roadmap for context-awareness-based self-managed systems",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",10,10.1007/978-3-642-37804-1_28,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892946723&doi=10.1007%2f978-3-642-37804-1_28&partnerID=40&md5=b6d2d519067e18193bbdd967280abc7d","Cloud computing, autonomic computing, pervasive and mobile computing tends to converge, maximizing the benefits from different computing paradigms. This convergence makes emerging applications such as search and rescue applications, smart city and smart planet applications, e.g., to minimize the power consumption of a full city to achieve green computing vision, more promising on the one hand, but more complex to manage on the other hand. Interesting research questions arise due to this convergence. For example, how to efficiently retrieve underlying contexts that are difficult to recognize especially with resource-limited handheld devices, how to make use of these contexts for achieving self-management, and how to process large-scale contexts. These challenges require that researchers from software engineering, artificial intelligence, pattern recognition, high-performance distributed systems, cloud and mobile computing, etc. collaborate in order to make systems work in an efficiently self-managed manner. © Springer-Verlag 2013.",,"Autonomic Computing; Computing paradigm; Emerging applications; Hand held device; High-performance distributed systems; Research questions; Research roadmap; Search-and-rescue applications; Artificial intelligence; Distributed computer systems; Mobile computing; Research; Software engineering; Pattern recognition systems",Conference Paper,Scopus,2-s2.0-84892946723
"Blundo C., Cimato S.","Constrained role mining",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",10,10.1007/978-3-642-38004-4_19,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893098631&doi=10.1007%2f978-3-642-38004-4_19&partnerID=40&md5=e54e9b0b1304a3a88999b4ad96d3379f","Role Based Access Control (RBAC) is a very popular access control model, for a long time investigated and widely deployed in the security architecture of different enterprises. To implement RBAC, roles have to be firstly identified within the considered organization. Usually the process of (automatically) defining the roles in a bottom up way, starting from the permissions assigned to each user, is called role mining. In literature, the role mining problem has been formally analyzed and several techniques have been proposed in order to obtain a set of valid roles. Recently, the problem of defining different kind of constraints on the number and the size of the roles included in the resulting role set has been addressed. In this paper we provide a formal definition of the role mining problem under the cardinality constraint, i.e. restricting the maximum number of permissions that can be included in a role. We discuss formally the computational complexity of the problem and propose a novel heuristic. Furthermore we present experimental results obtained after the application of the proposed heuristic on both real and synthetic datasets, and compare the resulting performance to previous proposals. © Springer-Verlag Berlin Heidelberg 2013.",,"Access control models; Bottom up; Cardinality constraints; Formal definition; Role minings; Role-based Access Control; Security Architecture; Synthetic datasets; Artificial intelligence; Computer science; Computers; Access control",Conference Paper,Scopus,2-s2.0-84893098631
"Zhou T., Tao D.","Shifted subspaces tracking on sparse outlier for motion segmentation",2013,"IJCAI International Joint Conference on Artificial Intelligence",10,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062233&partnerID=40&md5=826892d5747d8888f843eaa2cbf38f51","In low-rank &amp; sparse matrix decomposition, the entries of the sparse part are often assumed to be i.i.d. sampled from a random distribution. But the structure of sparse part, as the central interest of many problems, has been rarely studied. One motivating problem is tracking multiple sparse object flows (motions) in video. We introduce ""shifted subspaces tracking (SST)"" to segment the motions and recover their trajectories by exploring the low-rank property of background and the shifted subspace property of each motion. SST is composed of two steps, background modeling and flow tracking. In step 1, we propose ""semi-soft GoDec"" to separate all the motions from the low-rank background L as a sparse outlier S. Its soft-thresholding in updating S significantly speeds up GoDec and facilitates the parameter tuning. In step 2, we update X as S obtained in step 1 and develop ""SST algorithm"" further decomposing X as X = Σi=1k L(i)oτ (i)+ S+G, wherein L(i) is a low-rank matrix storing the ith flow after transformation τ (i). SST algorithm solves k sub-problems in sequel by alternating minimization, each of which recovers one L(i) and its τ (i) by randomized method. Sparsity of L(i) and between-frame affinity are leveraged to save computations. We justify the effectiveness of SST on surveillance video sequences.",,"Alternating minimization; Low-rank matrices; Low-rank properties; Motion segmentation; Random distribution; Randomized method; Subspace properties; Surveillance video; Algorithms; Artificial intelligence; Linear transformations; Video signal processing; Statistics",Conference Paper,Scopus,2-s2.0-84896062233
"Interdonato R., Romeo S., Tagarelli A., Karypis G.","A versatile graph-based approach to package recommendation",2013,"Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI",10,10.1109/ICTAI.2013.130,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897729813&doi=10.1109%2fICTAI.2013.130&partnerID=40&md5=9a4f86fd5d169f3fb34ec28927acff6f","An emerging trend in research on recommender systems is the design of methods capable of recommending packages instead of single items. The problem is challenging due to a variety of critical aspects, including context-based and user-provided constraints for the items constituting a package, but also the high sparsity and limited accessibility of the primary data used to solve the problem. Most existing works on the topic have focused on a specific application domain (e.g., travel package recommendation), thus often providing ad-hoc solutions that cannot be adapted to other domains. By contrast, in this paper we propose a versatile package recommendation approach that is substantially independent of the peculiarities of a particular application domain. A key aspect in our framework is the exploitation of prior knowledge on the content type models of the packages being generated that express what the users expect from the recommendation task. Packages are learned for each package model, while the recommendation stage is accomplished by performing a PageRank-style method personalized w.r.t. The target user's preferences, possibly including a limited budget. Our developed method has been tested on a TripAdvisor dataset and compared with a recently proposed method for learning composite recommendations. © 2013 IEEE.",,"Context-based; Emerging trends; Package modeling; Package recommendations; Primary data; Prior knowledge; Travel packages; User's preferences; Artificial intelligence; Tools",Conference Paper,Scopus,2-s2.0-84897729813
"Wachsmuth G.H., Konat G.D.P., Vergu V.A., Groenewegen D.M., Visser E.","A language independent task engine for incremental name and type analysis",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",10,10.1007/978-3-319-02654-1_15,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891309843&doi=10.1007%2f978-3-319-02654-1_15&partnerID=40&md5=5ec973bdddf138a199242e23eea04e22","IDEs depend on incremental name and type analysis for responsive feedback for large projects. In this paper, we present a language-independent approach for incremental name and type analysis. Analysis consists of two phases. The first phase analyzes lexical scopes and binding instances and creates deferred analysis tasks. A task captures a single name resolution or type analysis step. Tasks might depend on other tasks and are evaluated in the second phase. Incrementality is supported on file and task level. When a file changes, only this file is recollected and only those tasks are reevaluated, which are affected by the changes in the collected data. The analysis does neither re-parse nor re-traverse unchanged files, even if they are affected by changes in other files. We implemented the approach as part of the Spoofax Language Workbench and evaluated it for the WebDSL web programming language. © 2013 Springer International Publishing.",,"Language independents; Language workbenches; Large project; Name resolution; Second phase; Spoofax; Task levels; Type analysis; Artificial intelligence; Computer science; Computers; Computer programming languages",Conference Paper,Scopus,2-s2.0-84891309843
"Fernández-Navarro F., Campoy-Muñoz P., La Paz-Marín M.-De., Hervás-Martínez C., Yao X.","Addressing the EU sovereign ratings using an ordinal regression approach",2013,"IEEE Transactions on Cybernetics",10,10.1109/TSMCC.2013.2247595,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890120267&doi=10.1109%2fTSMCC.2013.2247595&partnerID=40&md5=d49d22636401727aad404b0d55430269","The current European debt crisis has drawn considerable attention to credit-rating agencies' news about sovereign ratings. From a technical point of view, credit rating constitutes a typical ordinal regression problem because credit-rating agencies generally present a scale of risk composed of several categories. This fact motivated the use of an ordinal regression approach to address the problem of sovereign credit rating in this paper. Therefore, the ranking of different classes will be taken into account for the design of the classifier. To do so, a novel model is introduced in order to replicate sovereign rating, based on the negative correlation learning framework. The methodology is fully described in this paper and applied to the classification of the 27 European countries' sovereign rating during the 2007-2010 period based on Standard and Poor's reports. The proposed technique seems to be competitive and robust enough to classify the sovereign ratings reported by this agency when compared with other existing well-known ordinal and nominal methods. © 2013 IEEE.","Country risk detection; Negative correlation learning (NCL); Neural networks; Ordinal regression","Country risks; Credit ratings; Different class; European Countries; European Debt Crisis; Negative correlation learning; Ordinal regression; Developing countries; Neural networks; Regression analysis; Rating; algorithm; article; artificial intelligence; automated pattern recognition; computer simulation; decision support system; economics; European Union; methodology; regression analysis; statistical model; Algorithms; Artificial Intelligence; Computer Simulation; Decision Support Techniques; European Union; Models, Economic; Models, Statistical; Pattern Recognition, Automated; Regression Analysis",Article,Scopus,2-s2.0-84890120267
"Zhang J., Zhao H.","Improving function word alignment with frequency and syntactic information",2013,"IJCAI International Joint Conference on Artificial Intelligence",10,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896061537&partnerID=40&md5=2be46e063307fae680ced40f7bbcdd55","In statistical word alignment for machine translation, function words usually cause poor aligning performance because they do not have clear correspondence between different languages. This paper proposes a novel approach to improve word alignment by pruning alignments of function words from an existing alignment model with high precision and recall. Based on monolingual and bilingual frequency characteristics, a language-independent function word recognition algorithm is first proposed. Then a group of carefully defined syntactic structures combined with content word alignments are used for further function word alignment pruning. The experimental results show that the proposed approach improves both the quality of word alignment and the performance of statistical machine translation on Chinese-to-English, Germanto- English and French-to-English language pairs.",,"Frequency characteristic; Language pairs; Machine translations; Precision and recall; Statistical machine translation; Syntactic information; Syntactic structure; Word alignment; Artificial intelligence; Translation (languages); Alignment",Conference Paper,Scopus,2-s2.0-84896061537
"Gori P., Colliot O., Worbe Y., Marrakchi-Kacem L., Lecomte S., Poupon C., Hartmann A., Ayache N., Durrleman S.","Bayesian atlas estimation for the variability analysis of shape complexes.",2013,"Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention",10,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894607261&partnerID=40&md5=894034980e7b9f6cfd71404603d672bf","In this paper we propose a Bayesian framework for multiobject atlas estimation based on the metric of currents which permits to deal with both curves and surfaces without relying on point correspondence. This approach aims to study brain morphometry as a whole and not as a set of different components, focusing mainly on the shape and relative position of different anatomical structures which is fundamental in neuro-anatomical studies. We propose a generic algorithm to estimate templates of sets of curves (fiber bundles) and closed surfaces (sub-cortical structures) which have the same ""form"" (topology) of the shapes present in the population. This atlas construction method is based on a Bayesian framework which brings to two main improvements with respect to previous shape based methods. First, it allows to estimate from the data set a parameter specific to each object which was previously fixed by the user: the trade-off between data-term and regularity of deformations. In a multi-object analysis these parameters balance the contributions of the different objects and the need for an automatic estimation is even more crucial. Second, the covariance matrix of the deformation parameters is estimated during the atlas construction in a way which is less sensitive to the outliers of the population.",,"algorithm; article; artificial intelligence; audiovisual equipment; automated pattern recognition; Bayes theorem; brain; computer assisted diagnosis; histology; human; image enhancement; image subtraction; methodology; nuclear magnetic resonance imaging; reproducibility; sensitivity and specificity; Algorithms; Artificial Intelligence; Bayes Theorem; Brain; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Models, Anatomic; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique",Article,Scopus,2-s2.0-84894607261
"Mahanand B.S., Savitha R., Suresh S.","Computer aided diagnosis of ADHD using brain magnetic resonance images",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",10,10.1007/978-3-319-03680-9_39,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893807134&doi=10.1007%2f978-3-319-03680-9_39&partnerID=40&md5=2b8424a83b095412bca4fc36784fba02","This paper presents a pilot study on the development of an automated diagnostic tool for Attention Deficiency Hyperactivity Disorder (ADHD) based on regional anatomy of the child brain. For the pilot study, amygdala and cerebellar vermis are chosen from magnetic resonance images obtained from ADHD-200 consortium data set. These regions play a vital role in the control of emotional response and behavior/locomotion, respectively. The images are preprocessed, registered by transforming each image to the space of the population average. The gray matter tissue probability values of amygdala and cerebellar vermis are obtained by applying a region-of-interest mask. These values are then used to train a Projection Based Learning algorithm for a Metacognitive Radial Basis Function Network (PBL-McRBFN) for the diagnosis of ADHD and prediction of its subtype. Performance results show that the PBL-McRBFN diagnoses ADHD and predicts its subtypes based on these regions with an accuracy of approx. 65% and 62%, respectively. © Springer International Publishing 2013.","Attention deficient hyperactivity disorder; Magnetic resonance imaging; Meta-cognitive radial basis function network; Projection based learning; Region-of-Interest","Automated diagnostics; Brain magnetic resonance images; Emotional response; Hyperactivity disorder; Metacognitives; Projection based learning; Region of interest; Regional anatomy; Artificial intelligence; Brain; Computer aided diagnosis; Radial basis function networks; Magnetic resonance imaging",Conference Paper,Scopus,2-s2.0-84893807134
"Blocki J., Christin N., Datta A., Procaccia A.D., Sinha A.","Audit games",2013,"IJCAI International Joint Conference on Artificial Intelligence",10,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062422&partnerID=40&md5=a5bb20faebb4e580bf45b462f6733536","Effective enforcement of laws and policies requires expending resources to prevent and detect offenders, as well as appropriate punishment schemes to deter violators. In particular, enforcement of privacy laws and policies in modern organizations that hold large volumes of personal information (e.g., hospitals, banks) relies heavily on internal audit mechanisms. We study economic considerations in the design of these mechanisms, focusing in particular on effective resource allocation and appropriate punishment schemes. We present an audit game model that is a natural generalization of a standard security game model for resource allocation with an additional punishment parameter. Computing the Stackelberg equilibrium for this game is challenging because it involves solving an optimization problem with non-convex quadratic constraints. We present an additive FPTAS that efficiently computes the solution.",,"Economic considerations; Internal audit; Natural generalization; Optimization problems; Personal information; Quadratic constraint; Security games; Stackelberg equilibrium; Artificial intelligence; Data privacy; Resource allocation; Computer games",Conference Paper,Scopus,2-s2.0-84896062422
"Rochlin I., Sarne D.","Information sharing under costly communication in joint exploration",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",10,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893428814&partnerID=40&md5=ba84bb107ba1a7b7ef6b7d18b4a31587","This paper studies distributed cooperative multi-agent exploration methods in settings where the exploration is costly and the overall performance measure is determined by the minimum performance achieved by any of the individual agents. Such an exploration setting is applicable to various multi-agent systems, e.g., in Dynamic Spectrum Access exploration. The goal in such problems is to optimize the process as a whole, considering the tradeoffs between the quality of the solution obtained and the cost associated with the exploration and coordination between the agents. Through the analysis of the two extreme cases where coordination is completely free and when entirely disabled, we manage to extract the solution for the general case where coordination is taken to be costly, modeled as a fee that needs to be paid for each additional coordinated agent. The strategy structure for the general case is shown to be threshold-based, and the thresholds which are analytically derived in this paper can be calculated offline, resulting in a very low online computational load. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Dynamic spectrum access; Exploration methods; Individual agent; Information sharing; Offline; On-line computational loads; Performance measure; Artificial intelligence; Multi agent systems",Conference Paper,Scopus,2-s2.0-84893428814
"Lecoutre C., Paparrizouy A., Stergiou K.","Extending STR to a higher-order consistency",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",10,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893420489&partnerID=40&md5=16ed483a9ff49a7a86de6d27e2b5cfed","One of the most widely studied classes of constraints in constraint programming (CP) is that of table constraints. Numerous specialized filtering algorithms, enforcing the well-known property called generalized arc consistency (GAC), have been developed for such constraints. Among the most successful GAC algorithms for table constraints, we find variants of simple tabular reduction (STR), like STR2. In this paper, we propose an extension of STR-based algorithms that achieves full pairwise consistency (FPWC), a consistency stronger than GAC and max restricted pairwise consistency (maxRPWC). Our approach involves counting the number of occurrences of specific combinations of values in constraint intersections. Importantly, the worst-case time complexity of one call to the basic filtering procedure at the heart of our new algorithm is quite close to that of STR algorithms. Experiments demonstrate that our method can outperform STR2 in many classes of problems, being significantly faster in some cases. Also, it is clearly superior to maxRPWC+, an algorithm that has been recently proposed. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Constraint programming; Filtering algorithm; Filtering procedures; Generalized arc consistencies; Higher-order; Table constraints; Time complexity; Artificial intelligence; Computer programming; Constraint theory; Algorithms",Conference Paper,Scopus,2-s2.0-84893420489
"Guo M., Deligkas A.","Revenue maximization via hiding item attributes",2013,"IJCAI International Joint Conference on Artificial Intelligence",10,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062967&partnerID=40&md5=233c90bf075e7eb5b02cffc8fa7e5535","We study probabilistic single-item second-price auctions where the item is characterized by a set of attributes. The auctioneer knows the actual instantiation of all the attributes, but he may choose to reveal only a subset of these attributes to the bidders. Our model is an abstraction of the following Ad auction scenario. The website (auctioneer) knows the demographic information of its impressions, and this information is in terms of a list of attributes (e.g., age, gender, country of location). The website may hide certain attributes from its advertisers (bidders) in order to create thicker market, which may lead to higher revenue. We study how to hide attributes in an optimal way. We show that it is NP-hard to compute the optimal attribute hiding scheme. We then derive a polynomial-time solvable upper bound on the optimal revenue. Finally, we propose two heuristic-based attribute hiding schemes. Experiments show that revenue achieved by these schemes is close to the upper bound.",,"Ad auctions; Demographic information; NP-hard; Polynomial-time; Revenue maximization; Second-price auction; Upper Bound; Optimization; Websites; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84896062967
"Xu B., Zhuge H.","A text scanning mechanism simulating human reading process",2013,"IJCAI International Joint Conference on Artificial Intelligence",10,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896060944&partnerID=40&md5=484679cb12e00369ffaa7d99eaef436d","Previous text processing techniques focus on text itself while neglecting human reading process. Therefore they are limited in special applications. This paper proposes a text scanning mechanism for generating the dynamic impressions of words in text by simulating recall, association and forget processes during reading. Experiments show that the mechanism is suitable for multiple text processing applications.",,"Processing applications; Processing technique; Scanning mechanisms; Special applications; Artificial intelligence; Text processing",Conference Paper,Scopus,2-s2.0-84896060944
"Pill I., Quaritsch T.","Behavioral diagnosis of LTL specifications at operator level",2013,"IJCAI International Joint Conference on Artificial Intelligence",10,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063674&partnerID=40&md5=6e84677306a4600a08dc79fd5831a730","Product defects and rework efforts due to flawed specifications represent major issues for a project's performance, so that there is a high motivation for providing effective means that assist designers in assessing and ensuring a specification's quality. Recent research in the context of formal specifications, e.g. on coverage and vacuity, offers important means to tackle related issues. In the currently underrepresented research direction of diagnostic reasoning on a specification, we propose a scenario-based diagnosis at a specification's operator level using weak or strong fault models. Drawing on efficient SAT encodings, we show in this paper how to achieve that effectively for specifications in LTL. Our experimental results illustrate our approach's validity and attractiveness.",,"Diagnostic reasoning; Encodings; Fault model; Formal Specification; Operator levels; Product defects; Recent researches; Scenario-based; Artificial intelligence; Specifications",Conference Paper,Scopus,2-s2.0-84896063674
"Pedrycz W., Al-Hmouz R., Morfeq A., Balamash A.","The design of free structure granular mappings: The use of the principle of justifiable granularity",2013,"IEEE Transactions on Cybernetics",10,10.1109/TCYB.2013.2240384,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890060968&doi=10.1109%2fTCYB.2013.2240384&partnerID=40&md5=5ae1dc1f64500017494616f382525a62","The study introduces a concept of mappings realized in presence of information granules and offers a design framework supporting the formation of such mappings. Information granules are conceptually meaningful entities formed on a basis of a large number of experimental input-output numeric data available for the construction of the model. We develop a conceptually and algorithmically sound way of forming information granules. Considering the directional nature of the mapping to be formed, this directionality aspect needs to be taken into account when developing information granules. The property of directionality implies that while the information granules in the input space could be constructed with a great deal of flexibility, the information granules formed in the output space have to inherently relate to those built in the input space. The input space is granulated by running a clustering algorithm; for illustrative purposes, the focus here is on fuzzy clustering realized with the aid of the fuzzy C-means algorithm. The information granules in the output space are constructed with the aid of the principle of justifiable granularity (being one of the underlying fundamental conceptual pursuits of Granular Computing). The construct exhibits two important features. First, the constructed information granules are formed in the presence of information granules already constructed in the input space (and this realization is reflective of the direction of the mapping from the input to the output space). Second, the principle of justifiable granularity does not confine the realization of information granules to a single formalism such as fuzzy sets but helps form the granules expressed any required formalism of information granulation. The quality of the granular mapping (viz. The mapping realized for the information granules formed in the input and output spaces) is expressed in terms of the coverage criterion (articulating how well the experimental data are 'covered' by information granules produced by the granular mapping for any input experimental data). Some parametric studies are reported by quantifying the performance of the granular mapping (expressed in terms of the coverage and specificity criteria) versus the values of a certain parameters utilized in the construction of output information granules through the principle of justifiable granularity. The plots of coverage-specificity dependency help determine a knee point and reach a sound compromise between these two conflicting requirements imposed on the quality of the granular mapping. Furthermore, quantified is the quality of the mapping with regard to the number of information granules (implying a certain granularity of the mapping). A series of experiments is reported as well. © 2013 IEEE.","Clustering; Free structure modeling; Granular mappings; Information granules; Principle of justifiable granularity","Clustering; Coverage criteria; Design frameworks; Free structures; Fuzzy C-means algorithms; Important features; Information granulation; Principle of justifiable granularities; Clustering algorithms; Copying; Fuzzy clustering; Granulation; Mapping; Structural design; Information granules; algorithm; article; artificial intelligence; automated pattern recognition; data mining; decision support system; fuzzy logic; methodology; Algorithms; Artificial Intelligence; Data Mining; Decision Support Techniques; Fuzzy Logic; Pattern Recognition, Automated",Article,Scopus,2-s2.0-84890060968
"Zhuang Z., Pagnucco M., Zhang Y.","Definability of Horn revision from Horn contraction",2013,"IJCAI International Joint Conference on Artificial Intelligence",10,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896061822&partnerID=40&md5=64557a11c5cc3bc141959c8a80dfb19b","In the AGM framework [Alchourrón and Makinson, 1985], a revision function can be defined directly through constructions like systems of spheres, epistemic entrenchment, etc., or indirectly through a contraction operation via the Levi identity. A recent trend is to construct AGM style contraction and revision functions that operate under Horn logic. A direct construction of Horn revision is given in [Delgrande and Peppas, 2011]. However, it is unknown whether Horn revision can be defined indirectly from Horn contraction. In this paper, we address this problem by obtaining a model-based Horn revision through the model-based Horn contraction studied in [Zhuang and Pagnucco, 2012]. Our result shows that, under proper restrictions, Horn revision is definable through Horn contraction via the Levi identity.",,"Definability; Horn logic; Levi identity; Recent trends; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84896061822
"Legg S., Veness J.","An approximation of the universal intelligence measure",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",10,10.1007/978-3-642-44958-1-18,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893176961&doi=10.1007%2f978-3-642-44958-1-18&partnerID=40&md5=97cb9315fb61a07eea4bd82210208142","The Universal Intelligence Measure is a recently proposed formal definition of intelligence. It is mathematically specified, extremely general, and captures the essence of many informal definitions of intelligence. It is based on Hutter's Universal Artificial Intelligence theory, an extension of Ray Solomonoff's pioneering work on universal induction. Since the Universal Intelligence Measure is only asymptotically computable, building a practical intelligence test from it is not straightforward. This paper studies the practical issues involved in developing a real-world UIM-based performance metric. Based on our investigation, we develop a prototype implementation which we use to evaluate a number of different artificial agents. © 2013 Springer-Verlag Berlin Heidelberg.",,"Artificial agents; Formal definition; Intelligence tests; Performance metrices; Practical issues; Prototype implementations; Real-world; Universal induction; Algorithms; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84893176961
"Governatori G., Rotolo A., Villata S., Gandon F.","One license to compose them all: A deontic logic approach to data licensing on the web of data",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",10,10.1007/978-3-642-41335-3_10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891930735&doi=10.1007%2f978-3-642-41335-3_10&partnerID=40&md5=de89573e96d648143d9266092a9e7ae1","In the domain of Linked Open Data a need is emerging for developing automated frameworks able to generate the licensing terms associated to data coming from heterogeneous distributed sources. This paper proposes and evaluates a deontic logic semantics which allows us to define the deontic components of the licenses, i.e., permissions, obligations, and prohibitions, and generate a composite license compliant with the licensing items of the composed different licenses. Some heuristics are proposed to support the data publisher in choosing the licenses composition strategy which better suits her needs w.r.t. the data she is publishing. © 2013 Springer-Verlag.",,"Deontic; Deontic Logic; Distributed sources; Linked open datum; Web of datum; Computer science; Computers; Artificial intelligence",Conference Paper,Scopus,2-s2.0-84891930735
"Oommen B.J., Hashem M.K.","Modeling the ""learning process"" of the teacher in a tutorial-like system using learning automata",2013,"IEEE Transactions on Cybernetics",10,10.1109/TSMCB.2013.2238230,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890111621&doi=10.1109%2fTSMCB.2013.2238230&partnerID=40&md5=b525e349bb2880b31026ee2735aea279","Unlike the field of tutorial systems, where a real-life student interacts and learns from a software system, our research focuses on a new philosophy in which no entity needs to be a real-life individual. Such systems are termed as tutorial-like systems, and research in this field endeavors to model every component of the system using an appropriate learning model [in our case, a learning automaton (LA)].1 While models for the student, the domain, the teacher, etc., have been presented elsewhere, the aim of this paper is to present a new approach to model how the teacher, in this paradigm, of our tutorial- like system ""learns"" and improves his ""teaching skills"" while being himself an integral component of the system. We propose to model the ""learning process"" of the teacher by using a higher level LA, referred to as the metateacher, whose task is to assist the teacher himself. Ultimately, the intention is that the latter can communicate the teaching material to the student(s) in a manner customized to the particular student""s ability and progress. In short, the teacher will infer the progress of the student and initiate a strategy by which he can ""custom-communicate"" the material to each individual student. The results that we present in a simulated environment validate the model for the teacher and for the metateacher. The use of the latter can be seen to significantly improve the teaching abilities of the teacher. © 2013 IEEE.","Learning automata (LAs); Modeling of adaptive systems; Teacher modeling; Tutorial-like systems","Integral components; Learning Automata; Learning automaton; Learning process; Simulated environment; Teacher models; Teaching materials; Tutorial-like systems; Automata theory; Computer simulation; Learning systems; Students; Teaching; algorithm; article; artificial intelligence; biomimetics; computer simulation; education; information retrieval; methodology; teaching; theoretical model; Algorithms; Artificial Intelligence; Biomimetics; Computer Simulation; Computer-Assisted Instruction; Educational Measurement; Information Storage and Retrieval; Models, Theoretical; Teaching",Article,Scopus,2-s2.0-84890111621
"Farinelli A., Bicego M., Ramchurn S., Zucchelli M.","C-Link: A hierarchical clustering approach to large-scale near-optimal coalition formation",2013,"IJCAI International Joint Conference on Artificial Intelligence",10,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896064022&partnerID=40&md5=26bc5d4f8f02a3c70b5550d78b4a394b","Coalition formation is a fundamental approach to multi-agent coordination. In this paper we address the specific problem of coalition structure generation, and focus on providing good-enough solutions using a novel heuristic approach that is based on data clustering methods. In particular, we propose a hierarchical agglomerative clustering approach (C-Link), which uses a similarity criterion between coalitions based on the gain that the system achieves if two coalitions merge. We empirically evaluate C-Link on a synthetic benchmark data-set as well as in collective energy purchasing settings. Our results show that the C-link approach performs very well against an optimal benchmark based on Mixed-Integer Programming, achieving solutions which are in the worst case about 80% of the optimal (in the synthetic data-set), and 98% of the optimal (in the energy data-set). Thus we show that C-Link can return solutions for problems involving thousands of agents within minutes.",,"Coalition formations; Coalition structure; Data clustering methods; Hierarchical agglomerative clustering; Hierarchical clustering approach; Mixed-Integer Programming; Multi-agent coordinations; Synthetic benchmark; Artificial intelligence; Clustering algorithms; Heuristic methods; Integer programming; Optimization",Conference Paper,Scopus,2-s2.0-84896064022
"Kwok I., Wang Y.","Locate the hate: Detecting tweets against blacks",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",10,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893355375&partnerID=40&md5=418aa6960a079007d4dca319a8f65aec","Although the social medium Twitter grants users freedom of speech, its instantaneous nature and retweeting features also amplify hate speech. Because Twitter has a sizeable black constituency, racist tweets against blacks are especially detrimental in the Twitter community, though this effect may not be obvious against a backdrop of half a billion tweets a day.1 We apply a supervised machine learning approach, employing inexpensively acquired labeled data from diverse Twitter accounts to learn a binary classifier for the labels ""racist"" and ""nonracist."" The classifier has a 76% average accuracy on individual tweets, suggesting that with further improvements, our work can contribute data on the sources of anti-black hate speech. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Binary classifiers; Freedom of speech; Labeled data; Supervised machine learning; Artificial intelligence; Social networking (online)",Conference Paper,Scopus,2-s2.0-84893355375
"Yin M., Chen Y., Sun Y.-A.","The effects of performance-contingent financial incentives in online labor markets",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",10,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893402805&partnerID=40&md5=9e01a06bae3f4ddb8fd2e15febabfd7c","Online labor markets such as Amazon Mechanical Turk (MTurk) have emerged as platforms that facilitate the allocation of productive effort across global economies. Many of these markets compensate workers with monetary payments. We study the effects of performance-contingent financial rewards on work quality and worker effort in MTurk via two experiments.We find that the magnitude of performance contingent financial rewards alone affects neither quality nor effort. However, when workers working on two tasks of the same type in a sequence, the change in the magnitude of the reward over the two tasks affects both. In particular, both work quality and worker effort increase (alternatively decrease) as the reward increases (alternatively decreases) for the second task. This suggests the existence of the anchoring effect on workers' perception of incentives in MTurk and that this effect can be leveraged in workflow design to increase the effectiveness of financial incentives. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Amazon mechanical turks; Anchoring effects; Financial incentives; Financial rewards; Global economies; Labor markets; Work quality; Workflow designs; Artificial intelligence; Economics; Employment; Commerce",Conference Paper,Scopus,2-s2.0-84893402805
"Xiong D., Ben G., Zhang M., Lü Y., Liu Q.","Modeling lexical cohesion for document-level machine translation",2013,"IJCAI International Joint Conference on Artificial Intelligence",10,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896062065&partnerID=40&md5=0a4e0d81e77cf91d7b0e766099506a80","Lexical cohesion arises from a chain of lexical items that establish links between sentences in a text. In this paper we propose three different models to capture lexical cohesion for document-level machine translation: (a) a direct reward model where translation hypotheses are rewarded whenever lexical cohesion devices occur in them, (b) a conditional probability model where the appropriateness of using lexical cohesion devices is measured, and (c) a mutual information trigger model where a lexical cohesion relation is considered as a trigger pair and the strength of the association between the trigger and the triggered item is estimated by mutual information. We integrate the three models into hierarchical phrase-based machine translation and evaluate their effectiveness on the NIST Chinese-English translation tasks with large-scale training data. Experiment results show that all three models can achieve substantial improvements over the baseline and that the mutual information trigger model performs better than the others.",,"Conditional probabilities; Lexical cohesion; Lexical items; Machine translations; Mutual informations; Phrase-based machine translations; Three models; Training data; Artificial intelligence; C (programming language); Computer aided language translation; Adhesion",Conference Paper,Scopus,2-s2.0-84896062065
